[
  {
    "paper": {
      "id": "2505.19147",
      "authors": [
        {
          "_id": "68353258d005e45149d2d384",
          "user": {
            "_id": "66a0caa1a7a6ed88ad1c0ddf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a0caa1a7a6ed88ad1c0ddf/WoOP24-ruuHy4ryNhRp0D.jpeg",
            "isPro": false,
            "fullname": "Xuyang Liu",
            "user": "xuyang-liu16",
            "type": "user"
          },
          "name": "Xuyang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T08:23:05.932Z",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d385",
          "user": {
            "_id": "653b8c3e97a4d71d950e2f20",
            "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
            "isPro": false,
            "fullname": "Zichen Wen",
            "user": "zichenwen",
            "type": "user"
          },
          "name": "Zichen Wen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:50:45.710Z",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d386",
          "user": {
            "_id": "66968099c952e09a4cb29f78",
            "avatarUrl": "/avatars/bd3a361fe5315e26e9ae328071704eed.svg",
            "isPro": false,
            "fullname": "Wang",
            "user": "Steven-Shaobo",
            "type": "user"
          },
          "name": "Shaobo Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:50:41.853Z",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d387",
          "user": {
            "_id": "652f8642338c761caf474169",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/mq5jjqqNaFxVboWGDEocJ.jpeg",
            "isPro": false,
            "fullname": "Junjie Chen",
            "user": "coderchen01",
            "type": "user"
          },
          "name": "Junjie Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:50:48.148Z",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d388",
          "user": {
            "_id": "679f280ffb07d74f084520b6",
            "avatarUrl": "/avatars/b378000f68c7faf8d4fee8074dd2db5b.svg",
            "isPro": false,
            "fullname": "Zhishan Tao",
            "user": "Pppeach33",
            "type": "user"
          },
          "name": "Zhishan Tao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:50:43.758Z",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d389",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d38a",
          "user": {
            "_id": "64abcbfde144ba0eb9bb8419",
            "avatarUrl": "/avatars/6ccea0e755bad384aaabd5c455bd962e.svg",
            "isPro": false,
            "fullname": "Xiangqi Jin",
            "user": "Lueci4er",
            "type": "user"
          },
          "name": "Xiangqi Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:21:21.961Z",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d38b",
          "name": "Chang Zou",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d38c",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d38d",
          "name": "Chenfei Liao",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d38e",
          "name": "Xu Zheng",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d38f",
          "name": "Honggang Chen",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d390",
          "name": "Weijia Li",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d391",
          "name": "Xuming Hu",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d392",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:22:15.329Z",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d393",
          "user": {
            "_id": "642ec9831d1737803dc1c30a",
            "avatarUrl": "/avatars/c9ded838bad09004c15a27200e66a108.svg",
            "isPro": false,
            "fullname": "linfeng zhang",
            "user": "linfengZ",
            "type": "user"
          },
          "name": "Linfeng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:22:07.787Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T13:51:17.000Z",
      "submittedOnDailyAt": "2025-05-27T02:06:05.849Z",
      "title": "Cambiamos la eficiencia del AI de un enfoque centrado en modelos a un enfoque centrado en datos, haciendo una compresión.",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "El rápido desarrollo de los modelos de lenguaje grande (LLMs) y los modelos de lenguaje grande multimodal (MLLMs) ha sido iniciado por un aumento de los parámetros debido a la escalación modelo-centrada, lo que ha llevado a un crecimiento en el número de parámetros a nivel de cientos de millones, liderando mejoras en el rendimiento. Sin embargo, con el aumento del tamaño del modelo, las limitaciones de la hardware se acercan, y el principal buffer de cálculo se convierte en una función de costo cuadrático de la atención automática para largos contextos, lo que impide el manejo de textos largos, imágenes de alta resolución y vídeos continuos de larga duración.\n\nEn este artículo, se argumenta que el enfoque de la investigación eficiente de IA ha cambiado de modelo-centrado a datos-centrado, con un enfoque de compresión de tokens como nueva línea de investigación. Se propone que la reducción del número de tokens del modelo puede mejorar la eficiencia de la IA. A través de un análisis detallado, se revisa el reciente desarrollo de la inteligencia artificial en varias áreas, se construye un marco matemático integral para la optimización de los modelos existentes, y se muestra cómo la compresión de tokens representa una transición paradigmática para resolver el problema de sobrecarga de contexto largo.\n\nA continuación, se investiga la situación actual de la compresión de tokens, se analizan sus beneficios básicos y sus ventajas únicas en diferentes escenarios. Además, se ofrece una profunda análisis de los problemas actuales en la investigación de compresión de tokens y se describen las direcciones prometedoras del futuro. Finalmente, nuestro trabajo proporciona una nueva perspectiva sobre la eficiencia de la IA, sintetiza la investigación existente y promueve el desarrollo innovador que ataja los problemas de largo contexto, fomentando el crecimiento de la comunidad de AI.",
      "upvotes": 75,
      "discussionId": "68353259d005e45149d2d3c0",
      "projectPage": "https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression",
      "githubRepo": "https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression",
      "ai_summary": "The focus in AI research is shifting from model-centric to data-centric compression, with token compression identified as key to improving efficiency in handling long-context scenarios.",
      "ai_keywords": [
        "large language models",
        "multi-modal LLMs",
        "self-attention",
        "token compression",
        "long-context AI",
        "mathematical framework",
        "model efficiency",
        "long-context overhead",
        "current challenges",
        "future directions"
      ]
    },
    "publishedAt": "2025-05-25T09:51:17.000Z",
    "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression",
    "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\nwe argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19147.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19457",
      "authors": [
        {
          "_id": "683536ec70d215849adfc236",
          "user": {
            "_id": "6440f70f1a80f6d83cadfd16",
            "avatarUrl": "/avatars/04790922837dac81747e80bd0ee0a1cf.svg",
            "isPro": false,
            "fullname": "luguilong",
            "user": "guilong",
            "type": "user"
          },
          "name": "Guilong Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:23:29.797Z",
          "hidden": false
        },
        {
          "_id": "683536ec70d215849adfc237",
          "user": {
            "_id": "672b138db4215fd3888e0a8f",
            "avatarUrl": "/avatars/e90fe671a1db66401db88429fae9a763.svg",
            "isPro": false,
            "fullname": "guo",
            "user": "xuntao",
            "type": "user"
          },
          "name": "Xuntao Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:23:39.616Z",
          "hidden": false
        },
        {
          "_id": "683536ec70d215849adfc238",
          "user": {
            "_id": "6555df426947208b7741b637",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6555df426947208b7741b637/b7ply-HyaPKXrPvRNh21K.jpeg",
            "isPro": false,
            "fullname": "Rongjunchen Zhang",
            "user": "Tinker250",
            "type": "user"
          },
          "name": "Rongjunchen Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-27T03:52:17.018Z",
          "hidden": false
        },
        {
          "_id": "683536ec70d215849adfc239",
          "user": {
            "_id": "648add6aff6123185eb185a8",
            "avatarUrl": "/avatars/e37dfa680c1bb86c721165f03eb79e97.svg",
            "isPro": false,
            "fullname": "WNQzhu",
            "user": "Qlisp",
            "type": "user"
          },
          "name": "Wenqiao Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:49:28.216Z",
          "hidden": false
        },
        {
          "_id": "683536ec70d215849adfc23a",
          "name": "Ji Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6555df426947208b7741b637/48jI0LlYjRwO4-0kHRV0V.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6555df426947208b7741b637/atuM30TNh72kJtm8zGxoc.png"
      ],
      "publishedAt": "2025-05-26T03:23:02.000Z",
      "submittedOnDailyAt": "2025-05-27T02:28:08.336Z",
      "title": "BizFinBench: Marcador de rendimiento financiero de la realidad empresarial para evaluar LLM\n\n注意：虽然要求不添加额外文本，但为了确保翻译的准确性和专业性，我添加了“注意”部分以提醒读者翻译的性质。",
      "submittedOnDailyBy": {
        "_id": "6555df426947208b7741b637",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6555df426947208b7741b637/b7ply-HyaPKXrPvRNh21K.jpeg",
        "isPro": false,
        "fullname": "Rongjunchen Zhang",
        "user": "Tinker250",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje general muestran excelente rendimiento en tareas comunes, pero su evaluación en aspectos lógicos y precisión es muy compleja. Para abordar esto, presentamos BizFinBench, el primer marco de referencia para evaluar modelos de lenguaje para aplicaciones financieras reales en el mundo. BizFinBench incluye 5 dimensiones: cálculo numérico, razonamiento, extracción de información, reconocimiento de predicciones y respuestas basadas en conocimiento, y está compuesto por 6,781 preguntas bien explicadas, clasificadas en 9 categorías detalladas. El marco de referencia incorpora métricas subjetivas y objetivas, y presenta IteraJudge, un nuevo método de evaluación de modelos de lenguaje que reduce la sesgo en la evaluación utilizando métricas objetivas. Se evaluaron 25 modelos, incluyendo tanto modelos de pago como sistemas abierto-source. Los experimentos ampliados demostraron excelencia en todos los aspectos. La evaluación reveló patrones claros: en cálculo numérico, Claude-3.5-Sonnet (63.18) y DeepSeek-R1 (64.04) son los mejores, mientras que Qwen2.5-VL-3B (15.92) se ve afectado significativamente. En razonamiento, los modelos de pago son superiores (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15) a los abierto-source (aproximadamente 19.49). En extracción de información, el rendimiento varió más, con DeepSeek-R1 (71.46) y Qwen3-1.7B (11.23). En reconocimiento de predicciones, la variación fue la menor, con el mejor modelo entre 39.16 y 50.00. Actualmente, los modelos de lenguaje general pueden manejar adecuadamente preguntas generales financieras, pero en casos que requieren razonamiento complejo basado en esquemas y conceptos críticos, encuentran dificultades. BizFinBench ofrece un estricto y relevante marco de referencia para futuras investigaciones. Los códigos y datasets están disponibles en https://github.com/HiThink-Research/BizFinBench.",
      "upvotes": 45,
      "discussionId": "683536f170d215849adfc35e",
      "projectPage": "https://hithink-research.github.io/BizFinBench/",
      "githubRepo": "https://github.com/HiThink-Research/BizFinBench",
      "ai_summary": "BizFinBench is a benchmark for evaluating large language models in financial applications, revealing distinct performance patterns across various tasks.",
      "ai_keywords": [
        "large language models",
        "BizFinBench",
        "numerical calculation",
        "reasoning",
        "information extraction",
        "prediction recognition",
        "knowledge-based question answering",
        "IteraJudge",
        "Claude-3.5-Sonnet",
        "DeepSeek-R1",
        "Qwen2.5-VL-3B",
        "ChatGPT-o3",
        "Gemini-2.0-Flash",
        "Qwen3-1.7B"
      ]
    },
    "publishedAt": "2025-05-25T23:23:02.000Z",
    "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for\n  Evaluating LLMs",
    "summary": "Large language models excel in general tasks, yet assessing their reliability\nin logic-heavy, precision-critical domains like finance, law, and healthcare\nremains challenging. To address this, we introduce BizFinBench, the first\nbenchmark specifically designed to evaluate LLMs in real-world financial\napplications. BizFinBench consists of 6,781 well-annotated queries in Chinese,\nspanning five dimensions: numerical calculation, reasoning, information\nextraction, prediction recognition, and knowledge-based question answering,\ngrouped into nine fine-grained categories. The benchmark includes both\nobjective and subjective metrics. We also introduce IteraJudge, a novel LLM\nevaluation method that reduces bias when LLMs serve as evaluators in objective\nmetrics. We benchmark 25 models, including both proprietary and open-source\nsystems. Extensive experiments show that no model dominates across all tasks.\nOur evaluation reveals distinct capability patterns: (1) In Numerical\nCalculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while\nsmaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,\nproprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with\nopen-source models trailing by up to 19.49 points; (3) In Information\nExtraction, the performance spread is the largest, with DeepSeek-R1 scoring\n71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,\nperformance variance is minimal, with top models scoring between 39.16 and\n50.00. We find that while current LLMs handle routine finance queries\ncompetently, they struggle with complex scenarios requiring cross-concept\nreasoning. BizFinBench offers a rigorous, business-aligned benchmark for future\nresearch. The code and dataset are available at\nhttps://github.com/HiThink-Research/BizFinBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6555df426947208b7741b637/48jI0LlYjRwO4-0kHRV0V.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6555df426947208b7741b637/atuM30TNh72kJtm8zGxoc.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19457.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6555df426947208b7741b637",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6555df426947208b7741b637/b7ply-HyaPKXrPvRNh21K.jpeg",
      "fullname": "Rongjunchen Zhang",
      "name": "Tinker250",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17894",
      "authors": [
        {
          "_id": "683577db7733c0f27e945847",
          "user": {
            "_id": "65276c7911a8a521c91bc10f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
            "isPro": false,
            "fullname": "Khalil Hennara",
            "user": "Hennara",
            "type": "user"
          },
          "name": "Khalil Hennara",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-27T09:23:49.563Z",
          "hidden": false
        },
        {
          "_id": "683577db7733c0f27e945848",
          "user": {
            "_id": "6496df4b3c64d75523a11973",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6496df4b3c64d75523a11973/I_Qn5-3Czngle-NsGmabO.jpeg",
            "isPro": false,
            "fullname": "Muhammad Hreden",
            "user": "hr99",
            "type": "user"
          },
          "name": "Muhammad Hreden",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T10:03:33.725Z",
          "hidden": false
        },
        {
          "_id": "683577db7733c0f27e945849",
          "user": {
            "_id": "63aa7667769a10efc404fbbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63aa7667769a10efc404fbbc/tn8ZxUmTEMS0Gze7_F7JL.jpeg",
            "isPro": false,
            "fullname": "Mohamed Motasim Hamed",
            "user": "Moatasem444",
            "type": "user"
          },
          "name": "Mohamed Motaism Hamed",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-27T08:29:16.168Z",
          "hidden": false
        },
        {
          "_id": "683577db7733c0f27e94584a",
          "user": {
            "_id": "65704741e1cfce1764ce652e",
            "avatarUrl": "/avatars/9189aaf417426af4ebe381ed364a6c0e.svg",
            "isPro": false,
            "fullname": "Zeina Aldallal",
            "user": "ZeinaD",
            "type": "user"
          },
          "name": "Zeina Aldallal",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-27T08:29:16.168Z",
          "hidden": false
        },
        {
          "_id": "683577db7733c0f27e94584b",
          "name": "Sara Chrouf",
          "hidden": false
        },
        {
          "_id": "683577db7733c0f27e94584c",
          "name": "Safwan AlModhayan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T13:42:21.000Z",
      "submittedOnDailyAt": "2025-05-27T07:07:04.517Z",
      "title": "Advancement of Bidirectional Arabic-English Translation Using Small-Scale Language Models",
      "submittedOnDailyBy": {
        "_id": "65276c7911a8a521c91bc10f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
        "isPro": false,
        "fullname": "Khalil Hennara",
        "user": "Hennara",
        "type": "user"
      },
      "summary": "En el modelo universal, se presenta Mutarjim, un pequeño pero potente modelo de lenguaje bidireccional árabe-inglés. Los grandes modelos de LLM han demostrado una sorprendente evolución en tareas de procesamiento del lenguaje, especialmente en la traducción automática, pero también existen modelos más pequeños. Utilizando este guia, se desarrolló Mutarjim basándose en un modelo de lenguaje adecuado para árabe y inglés, llamado Kuwain-1.5B. Mutarjim supera a los grandes modelos en los evaluaciones de rendimiento establecidas, incluso a pesar de su tamaño reducido. Esto se logró gracias a un enfoque de entrenamiento optimizado en dos etapas y la selección de un corpus de entrenamiento de alta calidad. Los resultados experimentales muestran que Mutarjim presenta un rendimiento similar a los modelos 20 veces más grandes, reduciendo significativamente los costos de cálculo y las demandas de entrenamiento. Además, se presenta Tarjama-25, un nuevo benchmark diseñado para superar las limitaciones actuales de los conjuntos de datos de benchmark árabe-inglés. Estas limitaciones incluyen un ámbito de negocios estrecho, consultas cortas y una inclinación hacia el inglés en la fuente. Tarjama-25 incluye 5,000 pares de consultas revisadas por expertos y ofrece una amplia extensión de negocios y un marco de evaluación más detallado y equilibrado. En particular, Mutarjim alcanza el mejor rendimiento en tareas árabe-inglés en Tarjama-25, superando a modelos como GPT-4o mini y a modelos más grandes propietarios. Tarjama-25 se publica para apoyar futuras investigaciones y el desarrollo de sistemas de traducción árabe-inglés.",
      "upvotes": 38,
      "discussionId": "683577dc7733c0f27e94588d",
      "ai_summary": "Mutarjim is a compact Arabic-English translation model that outperforms larger models on established benchmarks and achieves state-of-the-art performance on a new comprehensive Tarjama-25 benchmark.",
      "ai_keywords": [
        "language model",
        "bidirectional Arabic-English translation",
        "LLMs",
        "Kuwain-1.5B",
        "two-phase training",
        "high-quality training corpus",
        "Tarjama-25",
        "domain narrowness",
        "English-source bias",
        "GPT-4"
      ]
    },
    "publishedAt": "2025-05-23T09:42:21.000Z",
    "title": "Mutarjim: Advancing Bidirectional Arabic-English Translation with a\n  Small Language Model",
    "summary": "We introduce Mutarjim, a compact yet powerful language model for\nbidirectional Arabic-English translation. While large-scale LLMs have shown\nimpressive progress in natural language processing tasks, including machine\ntranslation, smaller models. Leveraging this insight, we developed Mutarjim\nbased on Kuwain-1.5B , a language model tailored for both Arabic and English.\nDespite its modest size, Mutarjim outperforms much larger models on several\nestablished benchmarks, achieved through an optimized two-phase training\napproach and a carefully curated, high-quality training corpus.. Experimental\nresults show that Mutarjim rivals models up to 20 times larger while\nsignificantly reducing computational costs and training requirements. We also\nintroduce Tarjama-25, a new benchmark designed to overcome limitations in\nexisting Arabic-English benchmarking datasets, such as domain narrowness, short\nsentence lengths, and English-source bias. Tarjama-25 comprises 5,000\nexpert-reviewed sentence pairs and spans a wide range of domains, offering a\nmore comprehensive and balanced evaluation framework. Notably, Mutarjim\nachieves state-of-the-art performance on the English-to-Arabic task in\nTarjama-25, surpassing even significantly larger and proprietary models like\nGPT-4o mini. We publicly release Tarjama-25 to support future research and\nadvance the evaluation of Arabic-English translation systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17894.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65276c7911a8a521c91bc10f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
      "fullname": "Khalil Hennara",
      "name": "Hennara",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16348",
      "authors": [
        {
          "_id": "6835365d2925bc8bb23a57c7",
          "user": {
            "_id": "636b529ef796304dd67d139c",
            "avatarUrl": "/avatars/7a64d5095fcb1da558b52ad48177ad76.svg",
            "isPro": false,
            "fullname": "Taeyoon Kwon",
            "user": "Connoriginal",
            "type": "user"
          },
          "name": "Taeyoon Kwon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:50:26.210Z",
          "hidden": false
        },
        {
          "_id": "6835365d2925bc8bb23a57c8",
          "name": "Dongwook Choi",
          "hidden": false
        },
        {
          "_id": "6835365d2925bc8bb23a57c9",
          "name": "Sunghwan Kim",
          "hidden": false
        },
        {
          "_id": "6835365d2925bc8bb23a57ca",
          "name": "Hyojun Kim",
          "hidden": false
        },
        {
          "_id": "6835365d2925bc8bb23a57cb",
          "user": {
            "_id": "6420f4f55bccaa42484496e5",
            "avatarUrl": "/avatars/4996ba26955f8423c946b1ecd3989964.svg",
            "isPro": false,
            "fullname": "Seung Jun Moon",
            "user": "Lune-Blue",
            "type": "user"
          },
          "name": "Seungjun Moon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:24:40.306Z",
          "hidden": false
        },
        {
          "_id": "6835365d2925bc8bb23a57cc",
          "user": {
            "_id": "64b72a408ba7d6c922c73054",
            "avatarUrl": "/avatars/6d9797430bc36f05fb950b84aa6a9374.svg",
            "isPro": false,
            "fullname": "Beong Woo Kwak",
            "user": "bwookwak",
            "type": "user"
          },
          "name": "Beong-woo Kwak",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:24:46.911Z",
          "hidden": false
        },
        {
          "_id": "6835365d2925bc8bb23a57cd",
          "user": {
            "_id": "658a57b4126b8d7eae07b983",
            "avatarUrl": "/avatars/8d908cb3da697793564d24206a333782.svg",
            "isPro": false,
            "fullname": "Kuan-Hao Huang",
            "user": "ej0cl6",
            "type": "user"
          },
          "name": "Kuan-Hao Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:24:53.502Z",
          "hidden": false
        },
        {
          "_id": "6835365d2925bc8bb23a57ce",
          "user": {
            "_id": "682e91865fa8c5df85b3d8e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XWTfZoOjCdMnqaFEBBYWe.png",
            "isPro": false,
            "fullname": "Jinyoung Yeo",
            "user": "jinyeo",
            "type": "user"
          },
          "name": "Jinyoung Yeo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:25:01.610Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T08:00:10.000Z",
      "submittedOnDailyAt": "2025-05-27T02:20:10.067Z",
      "title": "Embodied Agents está revisando el uso de la memoria para proporcionar asistencia personalizada.",
      "submittedOnDailyBy": {
        "_id": "636b529ef796304dd67d139c",
        "avatarUrl": "/avatars/7a64d5095fcb1da558b52ad48177ad76.svg",
        "isPro": false,
        "fullname": "Taeyoon Kwon",
        "user": "Connoriginal",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje general (LLMs) son soportados por agentes concretos que muestran un excelente rendimiento en tareas de cambio de orden de la hipótesis. Sin embargo, estas tareas se centran principalmente en simples comandos y una interacción, lo que no refleja adecuadamente las problemas en los que los usuarios necesitan ayuda significativa. Para proporcionar ayuda personalizada, los agentes concretos deben utilizar los registros de las interacciones previas para que los significados únicos que los usuarios proporcionan se entiendan en el mundo real. Sin embargo, el efecto de utilizar memoria para proporcionar ayuda personalizada de manera efectiva aún no ha sido ampliamente investigado. Para complementar esto, se presenta MEMENTO, un marco de evaluación de agentes concretos personalizados. Este marco introduce un proceso de evaluación de memoria en dos etapas para evaluar el efecto de la utilización de memoria. Este proceso enfatiza la evaluación de la comprensión personalizada en tareas de cambio de orden, destacando la capacidad de: 1) especificar objetos basados en significados personales y 2) inferir la posición de objetos a partir de patrones del usuario. Experimentos con diferentes tipos de LLMs han revelado significativas limitaciones en la utilización de memoria. En particular, modelos avanzados como GPT-4o experimentan una pérdida de 30.5% de rendimiento cuando se necesita referirse a múltiples memorias. Estos hallazgos y el análisis detallado, junto con estudios de casos, proporcionan consejos valiosos para el desarrollo de agentes concretos personalizados en futuras investigaciones. Sitio web del proyecto: https://connoriginal.github.io/MEMENTO",
      "upvotes": 38,
      "discussionId": "683536612925bc8bb23a58e1",
      "projectPage": "https://connoriginal.github.io/MEMENTO/",
      "githubRepo": "https://github.com/Connoriginal/MEMENTO",
      "ai_summary": "MEMENTO evaluates personalized memory utilization in embodied agents, revealing limitations in understanding user semantics and routines.",
      "ai_keywords": [
        "embodied agents",
        "large language models (LLMs)",
        "object rearrangement tasks",
        "user semantics",
        "prior interaction history",
        "memory utilization",
        "personalized assistance",
        "goal interpretation",
        "object semantics",
        "user patterns"
      ]
    },
    "publishedAt": "2025-05-22T04:00:10.000Z",
    "title": "Embodied Agents Meet Personalization: Exploring Memory Utilization for\n  Personalized Assistance",
    "summary": "Embodied agents empowered by large language models (LLMs) have shown strong\nperformance in household object rearrangement tasks. However, these tasks\nprimarily focus on single-turn interactions with simplified instructions, which\ndo not truly reflect the challenges of providing meaningful assistance to\nusers. To provide personalized assistance, embodied agents must understand the\nunique semantics that users assign to the physical world (e.g., favorite cup,\nbreakfast routine) by leveraging prior interaction history to interpret\ndynamic, real-world instructions. Yet, the effectiveness of embodied agents in\nutilizing memory for personalized assistance remains largely underexplored. To\naddress this gap, we present MEMENTO, a personalized embodied agent evaluation\nframework designed to comprehensively assess memory utilization capabilities to\nprovide personalized assistance. Our framework consists of a two-stage memory\nevaluation process design that enables quantifying the impact of memory\nutilization on task performance. This process enables the evaluation of agents'\nunderstanding of personalized knowledge in object rearrangement tasks by\nfocusing on its role in goal interpretation: (1) the ability to identify target\nobjects based on personal meaning (object semantics), and (2) the ability to\ninfer object-location configurations from consistent user patterns, such as\nroutines (user patterns). Our experiments across various LLMs reveal\nsignificant limitations in memory utilization, with even frontier models like\nGPT-4o experiencing a 30.5% performance drop when required to reference\nmultiple memories, particularly in tasks involving user patterns. These\nfindings, along with our detailed analyses and case studies, provide valuable\ninsights for future research in developing more effective personalized embodied\nagents. Project website: https://connoriginal.github.io/MEMENTO",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16348.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636b529ef796304dd67d139c",
      "avatarUrl": "/avatars/7a64d5095fcb1da558b52ad48177ad76.svg",
      "fullname": "Taeyoon Kwon",
      "name": "Connoriginal",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20258",
      "authors": [
        {
          "_id": "68352b5803548b71276c1a6f",
          "user": {
            "_id": "64f2a228f40f35cfa3e8edfd",
            "avatarUrl": "/avatars/0671cb4df8f3d3bcaaa95aad3d0a46c2.svg",
            "isPro": false,
            "fullname": "Siye Wu",
            "user": "Siye01",
            "type": "user"
          },
          "name": "Siye Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:25:16.627Z",
          "hidden": false
        },
        {
          "_id": "68352b5803548b71276c1a70",
          "user": {
            "_id": "62d65139667051e0a29bffe7",
            "avatarUrl": "/avatars/0252aa2bcd4cf1c8e4b87e5f164b6da5.svg",
            "isPro": false,
            "fullname": "Jian Xie",
            "user": "hsaest",
            "type": "user"
          },
          "name": "Jian Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:50:53.006Z",
          "hidden": false
        },
        {
          "_id": "68352b5803548b71276c1a71",
          "user": {
            "_id": "63e8b792ca4fc7d30de6975b",
            "avatarUrl": "/avatars/57237f54d61d479df15209497a3f531e.svg",
            "isPro": false,
            "fullname": "Yikai Zhang",
            "user": "Arist12",
            "type": "user"
          },
          "name": "Yikai Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:25:41.797Z",
          "hidden": false
        },
        {
          "_id": "68352b5803548b71276c1a72",
          "name": "Aili Chen",
          "hidden": false
        },
        {
          "_id": "68352b5803548b71276c1a73",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "68352b5803548b71276c1a74",
          "name": "Yu Su",
          "hidden": false
        },
        {
          "_id": "68352b5803548b71276c1a75",
          "name": "Yanghua Xiao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62d65139667051e0a29bffe7/W2uaapL3hKBPi-TA-KDef.mp4"
      ],
      "publishedAt": "2025-05-26T17:38:50.000Z",
      "submittedOnDailyAt": "2025-05-27T01:44:27.885Z",
      "title": "ARM: ARM (Neural Network Module)",
      "submittedOnDailyBy": {
        "_id": "62d65139667051e0a29bffe7",
        "avatarUrl": "/avatars/0252aa2bcd4cf1c8e4b87e5f164b6da5.svg",
        "isPro": false,
        "fullname": "Jian Xie",
        "user": "hsaest",
        "type": "user"
      },
      "summary": "Los modelos de inferencia a gran escala muestran un excelente rendimiento en tareas complejas, pero su capacidad para ajustar el uso de los tokenes de justificación depende de la dificultad de la tarea. Esto se conecta con el problema de \"exceso de explicación\" — explicaciones innecesarias — que, aunque humanos pueden controlar el borrado de tokenes, se encuentra fundamentalmente en contra de los objetivos de una información completamente automática. En este artículo, proponemos un modelo de razonamiento adaptativo (Adaptive Reasoning Model, ARM). El ARM puede seleccionar formas de justificación apropiadas de manera gradual. Estas formas incluyen Direct Answer, Short CoT, Code, entre otros 3 formas eficientes, y Long CoT, que proporciona una explicación más detallada. Para el entrenamiento del ARM, introducimos una versión adaptativa del Policy Optimization de Grupo Relativo (Group Relative Policy Optimization, GRPO), llamada Ada-GRPO. Esta solución aborda los problemas de destrucción de la estructura en la GRPO tradicional. Mediante Ada-GRPO, el ARM puede reducir el uso de tokens en un promedio del 30%, reduciendo este uso en un 70% en comparación con modelos Long CoT, mientras mantiene su rendimiento. Además, la reducción en la generación de tokens mejora la eficiencia de la inferencia y puede aumentar la velocidad de entrenamiento en un factor de 2. Además del modo adaptativo básico, el ARM ofrece dos modos de justificación adicionales: 1) Modo guía de instrucciones, donde el usuario puede especificar explícitamente la forma de justificación utilizando tokenes especiales, siendo ideal para formas de justificación conocidas para las tareas de una carga de trabajo. 2) Modo guía de consenso, que integra los resultados de 3 formas eficientes y utiliza Long CoT cuando hay diferencias, priorizando el uso de tokens en casos de alto uso.",
      "upvotes": 32,
      "discussionId": "68352b5903548b71276c1a9f",
      "projectPage": "https://team-arm.github.io/arm/",
      "githubRepo": "https://github.com/TEAM-ARM/arm",
      "ai_summary": "Adaptive Reasoning Model (ARM) uses Ada-GRPO to reduce token usage and improve efficiency across different reasoning modes.",
      "ai_keywords": [
        "Adaptive Reasoning Model",
        "ARM",
        "Ada-GRPO",
        "Group Relative Policy Optimization",
        "GRPO",
        "format collapse",
        "token efficiency",
        "inference efficiency",
        "Direct Answer",
        "Short CoT",
        "Code",
        "Long CoT",
        "Adaptive Mode",
        "Instruction-Guided Mode",
        "Consensus-Guided Mode"
      ]
    },
    "publishedAt": "2025-05-26T13:38:50.000Z",
    "title": "ARM: Adaptive Reasoning Model",
    "summary": "While large reasoning models demonstrate strong performance on complex tasks,\nthey lack the ability to adjust reasoning token usage based on task difficulty.\nThis often leads to the \"overthinking\" problem -- excessive and unnecessary\nreasoning -- which, although potentially mitigated by human intervention to\ncontrol the token budget, still fundamentally contradicts the goal of achieving\nfully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a\nreasoning model capable of adaptively selecting appropriate reasoning formats\nbased on the task at hand. These formats include three efficient ones -- Direct\nAnswer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To\ntrain ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy\nOptimization (GRPO), which addresses the format collapse issue in traditional\nGRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by\nan average of 30%, and up to 70%, while maintaining performance comparable to\nthe model that relies solely on Long CoT. Furthermore, not only does it improve\ninference efficiency through reduced token generation, but it also brings a 2x\nspeedup in training. In addition to the default Adaptive Mode, ARM supports two\nadditional reasoning modes: 1) Instruction-Guided Mode, which allows users to\nexplicitly specify the reasoning format via special tokens -- ideal when the\nappropriate format is known for a batch of tasks. 2) Consensus-Guided Mode,\nwhich aggregates the outputs of the three efficient formats and resorts to Long\nCoT in case of disagreement, prioritizing performance with higher token usage.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62d65139667051e0a29bffe7/W2uaapL3hKBPi-TA-KDef.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20258.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "62d65139667051e0a29bffe7",
      "avatarUrl": "/avatars/0252aa2bcd4cf1c8e4b87e5f164b6da5.svg",
      "fullname": "Jian Xie",
      "name": "hsaest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19914",
      "authors": [
        {
          "_id": "68353e41f995630ab88c198b",
          "user": {
            "_id": "606ed1884ffe81d1e03e81e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1639375346654-606ed1884ffe81d1e03e81e5.png",
            "isPro": false,
            "fullname": "Jiangjie Chen",
            "user": "jiangjiechen",
            "type": "user"
          },
          "name": "Jiangjie Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:49:21.006Z",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c198c",
          "user": {
            "_id": "636b36351340f879a2ec2bb1",
            "avatarUrl": "/avatars/260a1c15f9c14c967125469072020946.svg",
            "isPro": false,
            "fullname": "QianyuHe",
            "user": "Abbey4799",
            "type": "user"
          },
          "name": "Qianyu He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:49:23.290Z",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c198d",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c198e",
          "name": "Aili Chen",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c198f",
          "name": "Zhicheng Cai",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c1990",
          "name": "Weinan Dai",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c1991",
          "name": "Hongli Yu",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c1992",
          "name": "Qiying Yu",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c1993",
          "name": "Xuefeng Li",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c1994",
          "name": "Jiaze Chen",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c1995",
          "name": "Hao Zhou",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c1996",
          "name": "Mingxuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T12:40:31.000Z",
      "submittedOnDailyAt": "2025-05-27T02:57:13.989Z",
      "title": "Anigmata: Un puzzle compuesto para la expansión de la inferencia lógica en modelos de lenguaje de gran escala",
      "submittedOnDailyBy": {
        "_id": "62d62b333bf5e059f7d2b286",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668513815771-62d62b333bf5e059f7d2b286.jpeg",
        "isPro": false,
        "fullname": "Siyu Yuan",
        "user": "siyuyuan",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) como o1 de OpenAI y R1 de DeepSeek han demostrado excelentes resultados en tareas lógicas avanzadas, como matemáticas y programación, a través del aprendizaje por refuerzo y la compensación verificable (RLVR). Sin embargo, en problemas desconocidos que requieren conocimientos domésticos, los humanos enfrentan dificultades. Presentamos aquí una primera serie detallada para mejorar tareas lógicas en problemas desconocidos. Esta colección incluye 36 tareas en 7 categorías, con ejemplos infinitos estructuralmente y dificultades controlables, así como un sistema de evaluación automático basado en reglas. Este diseño de generador-evaluador proporciona escalabilidad, entrenamiento de RL multi-tarea, análisis detallado y integración continua de RLVR. Además, proponemos un estricto benchmark como Enigmata-Eval y desarrollamos RLVR stereo tipos optimizados. Nuestro modelo entrenado, Qwen2.5-32B-Enigmata, supera a o3-mini-high y o1 en el benchmark de problemas lógicos desconocidos, y muestra una performance consistente en ARC-AGI (32.8%) y ARC-AGI 2 (0.6%). Además, observamos una mejor generalización en problemas desconocidos de otros dominios y en la lógica matemática. Durante el entrenamiento, el uso de grandes modelos como Seed1.5-Thinking (20B parámetros activos y 200B parámetros totales) ha demostrado que los datos de Enigmata de problemas desconocidos pueden alcanzar el nivel de rendimiento superior en tareas lógicas avanzadas de matemáticas y STEM, como AIME (2024-2025), BeyondAIME y GPQA (Diamante). Este estudio proporciona un marco controlable para mejorar la evolución de tareas lógicas en los LLMs. Los recursos de este estudio se pueden encontrar en https://seed-enigmata.github.io.",
      "upvotes": 26,
      "discussionId": "68353e42f995630ab88c19dc",
      "ai_summary": "Enigmata is a comprehensive suite for improving LLMs in puzzle reasoning through scalable multi-task RL training, leading to better performance on benchmarks and advanced math tasks.",
      "ai_keywords": [
        "Large Language Models",
        "OpenAI's o1",
        "DeepSeek's R1",
        "Reinforcement Learning with Verifiable Rewards",
        "Enigmata",
        "Enigmata-Eval",
        "multi-task RL training",
        "puzzle reasoning",
        "rule-based verifier",
        "ARC-AGI",
        "Qwen2.5-32B-Enigmata",
        "Seed1.5-Thinking",
        "AIME",
        "BeyondAIME",
        "GPQA"
      ]
    },
    "publishedAt": "2025-05-26T08:40:31.000Z",
    "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with\n  Synthetic Verifiable Puzzles",
    "summary": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19914.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d62b333bf5e059f7d2b286",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668513815771-62d62b333bf5e059f7d2b286.jpeg",
      "fullname": "Siyu Yuan",
      "name": "siyuyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19297",
      "authors": [
        {
          "_id": "68354c05f7b44d5d505262c7",
          "user": {
            "_id": "63725a2eacef705233c62876",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63725a2eacef705233c62876/QlRm8oq7O8THzUhATYQlH.jpeg",
            "isPro": false,
            "fullname": "Valerii",
            "user": "sharfikeg",
            "type": "user"
          },
          "name": "Valerii Startsev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:51:23.082Z",
          "hidden": false
        },
        {
          "_id": "68354c05f7b44d5d505262c8",
          "name": "Alexander Ustyuzhanin",
          "hidden": false
        },
        {
          "_id": "68354c05f7b44d5d505262c9",
          "name": "Alexey Kirillov",
          "hidden": false
        },
        {
          "_id": "68354c05f7b44d5d505262ca",
          "name": "Dmitry Baranchuk",
          "hidden": false
        },
        {
          "_id": "68354c05f7b44d5d505262cb",
          "name": "Sergey Kastryulin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T20:08:20.000Z",
      "submittedOnDailyAt": "2025-05-27T07:55:09.983Z",
      "title": "El Alchemist: Crear un generador de oro generativo para la generación de imágenes a partir de textos públicos.",
      "submittedOnDailyBy": {
        "_id": "63725a2eacef705233c62876",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63725a2eacef705233c62876/QlRm8oq7O8THzUhATYQlH.jpeg",
        "isPro": false,
        "fullname": "Valerii",
        "user": "sharfikeg",
        "type": "user"
      },
      "summary": "El aprendizaje previo transmite ampliamente el conocimiento mundial de texto a modelos de imagen a partir de texto (T2I), pero esto solo es suficiente para alcanzar un alto nivel de artística y coherencia. Por lo tanto, la finejación (SFT) de entrenamiento es necesaria para mejorar gradualmente, pero su efectividad depende significativamente de la calidad del conjunto de datos de finejación. Los conjuntos de datos públicos actuales se centran principalmente en áreas restringidas (por ejemplo, animación o estilos artísticos específicos). La producción de conjuntos de datos de finejación de alta calidad para usos generales es un problema de gran importancia. Los métodos actuales de caracterización de desempeño son generalmente costosos y difíciles de aplicar para identificar muestras que tienen un impacto real. Este problema se complica aún más por la escasez de conjuntos de datos públicos para usos generales. Los modelos líderes dependen principalmente de datos internos de gran escala y propiedades, lo que impide avances más amplios en la investigación. En este artículo, se presenta un nuevo método para la producción de conjuntos de datos de finejación de uso general, extendiendo generadores de aprendizaje previo evaluados como evaluadores para identificar muestras de alto impacto. Este método se llama Alchemist (3,350 muestras) y se ha construido y publicado de manera reducida. Los experimentos muestran que Alchemist mejora significativamente la calidad de la generación de modelos de imagen a partir de texto a partir de cinco textos públicos, manteniendo diversidad y estilo. Además, se ha publicado el conjunto de pesos de los modelos de finejación.",
      "upvotes": 26,
      "discussionId": "68354c07f7b44d5d50526322",
      "ai_summary": "A new method using a pre-trained generative model helps construct a high-impact SFT dataset, Alchemist, which improves the generative quality of text-to-image models while maintaining diversity.",
      "ai_keywords": [
        "text-to-image",
        "fine-tuning",
        "pre-trained generative model",
        "general-purpose datasets",
        "aesthetic quality",
        "alignment",
        "curated datasets"
      ]
    },
    "publishedAt": "2025-05-25T16:08:20.000Z",
    "title": "Alchemist: Turning Public Text-to-Image Data into Generative Gold",
    "summary": "Pre-training equips text-to-image (T2I) models with broad world knowledge,\nbut this alone is often insufficient to achieve high aesthetic quality and\nalignment. Consequently, supervised fine-tuning (SFT) is crucial for further\nrefinement. However, its effectiveness highly depends on the quality of the\nfine-tuning dataset. Existing public SFT datasets frequently target narrow\ndomains (e.g., anime or specific art styles), and the creation of high-quality,\ngeneral-purpose SFT datasets remains a significant challenge. Current curation\nmethods are often costly and struggle to identify truly impactful samples. This\nchallenge is further complicated by the scarcity of public general-purpose\ndatasets, as leading models often rely on large, proprietary, and poorly\ndocumented internal data, hindering broader research progress. This paper\nintroduces a novel methodology for creating general-purpose SFT datasets by\nleveraging a pre-trained generative model as an estimator of high-impact\ntraining samples. We apply this methodology to construct and release Alchemist,\na compact (3,350 samples) yet highly effective SFT dataset. Experiments\ndemonstrate that Alchemist substantially improves the generative quality of\nfive public T2I models while preserving diversity and style. Additionally, we\nrelease the fine-tuned models' weights to the public.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19297.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63725a2eacef705233c62876",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63725a2eacef705233c62876/QlRm8oq7O8THzUhATYQlH.jpeg",
      "fullname": "Valerii",
      "name": "sharfikeg",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18545",
      "authors": [
        {
          "_id": "6835217ee759f596d018f72c",
          "user": {
            "_id": "6631fd5961a4305e5610d403",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6631fd5961a4305e5610d403/P1Dtxzn-KIbYDDsiw60nr.jpeg",
            "isPro": true,
            "fullname": "An Vo",
            "user": "anvo25",
            "type": "user"
          },
          "name": "An Vo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:52:08.994Z",
          "hidden": false
        },
        {
          "_id": "6835217ee759f596d018f72d",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Reza Taesiri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-27T02:21:46.082Z",
          "hidden": false
        },
        {
          "_id": "6835217ee759f596d018f72e",
          "name": "Daeyoung Kim",
          "hidden": false
        },
        {
          "_id": "6835217ee759f596d018f72f",
          "user": {
            "_id": "60e85b3fcd1cf4e418fff651",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645625108920-60e85b3fcd1cf4e418fff651.jpeg",
            "isPro": false,
            "fullname": "Anh (Totti) Nguyen",
            "user": "anhng8",
            "type": "user"
          },
          "name": "Anh Totti Nguyen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-27T02:20:46.958Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T06:23:52.000Z",
      "submittedOnDailyAt": "2025-05-27T00:50:49.797Z",
      "title": "B-score: Uso de registros de respuestas para la detección de sesgo en lenguajes similares",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) muestran muchos prejuicios, como sobre las mujeres o el número 7. Investigamos si LLMs pueden reducir estos prejuicios al examinar sus respuestas a la misma pregunta. Para entender el tipo de pregunta, expandimos 9 temas y probamos las LLMs con 3 tipos de preguntas: subjetivas, aleatorias y centradas en objetivos. Interesantemente, encontramos que LLMs pueden \"eliminar\" sus prejuicios cuando se les preguntan respuestas aleatorias sin prejuicios. Además, proponemos un nuevo métrico llamado B-score para detectar eficazmente prejuicios en preguntas subjetivas, aleatorias, fáciles y difíciles. Al utilizar B-score en MMLU, HLE y CSQA, la precisión en la verificación de respuestas de LLMs se mejoró significativamente en comparación con la frecuencia de respuestas en un solo turno o con puntuaciones de confianza lingüística. Los códigos y datos están disponibles en https://b-score.github.io.",
      "upvotes": 23,
      "discussionId": "6835217ee759f596d018f794",
      "projectPage": "https://b-score.github.io/",
      "githubRepo": "https://github.com/anvo25/b-score",
      "ai_summary": "LLMs can reduce biases in multi-turn conversations for certain types of questions, and a novel B-score metric improves the accuracy of verifying LLM answers.",
      "ai_keywords": [
        "large language models",
        "biases",
        "multi-turn conversation",
        "B-score",
        "MMLU",
        "HLE",
        "CSQA",
        "verification accuracy",
        "verbalized confidence scores"
      ]
    },
    "publishedAt": "2025-05-24T02:23:52.000Z",
    "title": "B-score: Detecting biases in large language models using response\n  history",
    "summary": "Large language models (LLMs) often exhibit strong biases, e.g, against women\nor in favor of the number 7. We investigate whether LLMs would be able to\noutput less biased answers when allowed to observe their prior answers to the\nsame question in a multi-turn conversation. To understand which types of\nquestions invite more biased answers, we test LLMs on our proposed set of\nquestions that span 9 topics and belong to three types: (1) Subjective; (2)\nRandom; and (3) Objective. Interestingly, LLMs are able to \"de-bias\" themselves\nin a multi-turn conversation in response to questions that seek an Random,\nunbiased answer. Furthermore, we propose B-score, a novel metric that is\neffective in detecting biases to Subjective, Random, Easy, and Hard questions.\nOn MMLU, HLE, and CSQA, leveraging B-score substantially improves the\nverification accuracy of LLM answers (i.e, accepting LLM correct answers and\nrejecting incorrect ones) compared to using verbalized confidence scores or the\nfrequency of single-turn answers alone. Code and data are available at:\nhttps://b-score.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18545.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 84
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19815",
      "authors": [
        {
          "_id": "683523b21a2911c0774a1dc5",
          "user": {
            "_id": "643d26979347842571bc9613",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3heFf7h3jbhhJWJ4JfGfh.jpeg",
            "isPro": false,
            "fullname": "Junnan Liu",
            "user": "jnanliu",
            "type": "user"
          },
          "name": "Junnan Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:51:56.495Z",
          "hidden": false
        },
        {
          "_id": "683523b21a2911c0774a1dc6",
          "name": "Hongwei Liu",
          "hidden": false
        },
        {
          "_id": "683523b21a2911c0774a1dc7",
          "name": "Linchen Xiao",
          "hidden": false
        },
        {
          "_id": "683523b21a2911c0774a1dc8",
          "user": {
            "_id": "654ce87af0b05673196a9f45",
            "avatarUrl": "/avatars/7b9c854eb98e487e3057479b1c7860ac.svg",
            "isPro": false,
            "fullname": "Shudong Liu",
            "user": "Sudanl",
            "type": "user"
          },
          "name": "Shudong Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T08:15:12.824Z",
          "hidden": false
        },
        {
          "_id": "683523b21a2911c0774a1dc9",
          "name": "Taolin Zhang",
          "hidden": false
        },
        {
          "_id": "683523b21a2911c0774a1dca",
          "name": "Zihan Ma",
          "hidden": false
        },
        {
          "_id": "683523b21a2911c0774a1dcb",
          "user": {
            "_id": "630716d11801ecc7d2595021",
            "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
            "isPro": false,
            "fullname": "Songyang Zhang",
            "user": "zsytony",
            "type": "user"
          },
          "name": "Songyang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:51:54.358Z",
          "hidden": false
        },
        {
          "_id": "683523b21a2911c0774a1dcc",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T10:52:17.000Z",
      "submittedOnDailyAt": "2025-05-27T01:00:26.595Z",
      "title": "Traducción al español:\n\nAnálisis de la interpretación de modelos de lenguaje profundo (LLM) con cooperación en proyectos desde la perspectiva de optimización",
      "submittedOnDailyBy": {
        "_id": "630716d11801ecc7d2595021",
        "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
        "isPro": false,
        "fullname": "Songyang Zhang",
        "user": "zsytony",
        "type": "user"
      },
      "summary": "Proponemos un nuevo marco para entender la capacidad de inferencia de modelos de lenguaje de gran escala (LLM) desde la perspectiva de la meta-aprendizaje. Consideramos los parámetros de un LLM bajo una actualización de descenso de gradientes pseudo, identificando así la similitud entre la inferencia del modelo y diferentes paradigmas de meta-aprendizaje. Formulamos el proceso de entrenamiento de tareas de inferencia como un entorno de meta-aprendizaje, tratando cada pregunta como una tarea individual y utilizando la tarea de inferencia como optimización de la bucle interna. Después de ser entrenados en diferentes conjuntos de preguntas, un LLM puede generalizar su capacidad de inferencia para preguntas que nunca han visto antes. Una amplia evaluación empírica demuestra la fuerte conexión entre la inferencia de un LLM y el meta-aprendizaje, explorando varios problemas importantes desde la perspectiva del meta-aprendizaje. Nuestro estudio ayuda a entender la inferencia de un LLM y ofrece una visión práctica de cómo mejorar modelos utilizando técnicas de meta-aprendizaje.",
      "upvotes": 22,
      "discussionId": "683523b41a2911c0774a1e78",
      "githubRepo": "https://github.com/open-compass/RaML",
      "ai_summary": "LLM reasoning is understood through a meta-learning framework, treating reasoning as pseudo-gradient descent and questions as individual tasks, which enhances generalization and provides practical insights for improvement.",
      "ai_keywords": [
        "large language models",
        "meta-learning",
        "pseudo-gradient descent",
        "inner loop optimization",
        "generalization",
        "fundamental reasoning capabilities"
      ]
    },
    "publishedAt": "2025-05-26T06:52:17.000Z",
    "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective",
    "summary": "We propose a novel framework for comprehending the reasoning capabilities of\nlarge language models (LLMs) through the perspective of meta-learning. By\nconceptualizing reasoning trajectories as pseudo-gradient descent updates to\nthe LLM's parameters, we identify parallels between LLM reasoning and various\nmeta-learning paradigms. We formalize the training process for reasoning tasks\nas a meta-learning setup, with each question treated as an individual task, and\nreasoning trajectories serving as the inner loop optimization for adapting\nmodel parameters. Once trained on a diverse set of questions, the LLM develops\nfundamental reasoning capabilities that can generalize to previously unseen\nquestions. Extensive empirical evaluations substantiate the strong connection\nbetween LLM reasoning and meta-learning, exploring several issues of\nsignificant interest from a meta-learning standpoint. Our work not only\nenhances the understanding of LLM reasoning but also provides practical\ninsights for improving these models through established meta-learning\ntechniques.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19815.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "630716d11801ecc7d2595021",
      "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
      "fullname": "Songyang Zhang",
      "name": "zsytony",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19250",
      "authors": [
        {
          "_id": "68355ce06a9c239ada09f97b",
          "user": {
            "_id": "66e0404662d6ab4f1107580f",
            "avatarUrl": "/avatars/ef71694fea5482078a637a3869e30d19.svg",
            "isPro": false,
            "fullname": "Yi Wang",
            "user": "Yi53",
            "type": "user"
          },
          "name": "Yi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:49:06.169Z",
          "hidden": false
        },
        {
          "_id": "68355ce06a9c239ada09f97c",
          "user": {
            "_id": "68356f5db243fb809813a715",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68356f5db243fb809813a715/grhHvANfDRp75rMJxWlQo.jpeg",
            "isPro": false,
            "fullname": "LiuJunxiao",
            "user": "master-lan",
            "type": "user"
          },
          "name": "Junxiao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:55:01.261Z",
          "hidden": false
        },
        {
          "_id": "68355ce06a9c239ada09f97d",
          "name": "Shimao Zhang",
          "hidden": false
        },
        {
          "_id": "68355ce06a9c239ada09f97e",
          "name": "Jiajun Chen",
          "hidden": false
        },
        {
          "_id": "68355ce06a9c239ada09f97f",
          "name": "Shujian Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T17:58:50.000Z",
      "submittedOnDailyAt": "2025-05-27T07:16:47.391Z",
      "title": "PATS: Proceso nivel de pensamiento adaptativo de cambio",
      "submittedOnDailyBy": {
        "_id": "66e0404662d6ab4f1107580f",
        "avatarUrl": "/avatars/ef71694fea5482078a637a3869e30d19.svg",
        "isPro": false,
        "fullname": "Yi Wang",
        "user": "Yi53",
        "type": "user"
      },
      "summary": "Actualmente, los grandes modelos de lenguaje (LLMs) aplican una estrategia lógica fija a todos los problemas, pero su complejidad no depende de la dificultad del problema. Omitir esta variabilidad en la complejidad de la lógica puede generar desbalances entre el rendimiento y la eficiencia. Los métodos actuales intentan convertirse en sistemas de pensamiento rápido y lento sin entrenamiento al cambiar la dificultad del problema, pero están limitados a un ajuste simple de estrategias. Para resolver estos problemas, proponemos un nuevo paradigma lógico: el Modo de Pensamiento Adaptativo de Nivel de Procesos (PATS). Este permitirá que los LLMs ajusten dinamicamente sus estrategias lógicas según la dificultad de cada etapa, optimizando así el equilibrio entre precisión y eficiencia de cálculo. Nuestro enfoque integra el Modelo de Recompensa de Procesos (PRMs) y la Búsqueda de Rayos, incluyendo también la transición de modos evolutivos y la estructura de penalizaciones para etapas maltratadas. Los resultados de experimentos en diferentes marcos de referencia matemáticos muestran que nuestro método mantiene una precisión fija mientras reduce el uso de tokens. Esta investigación destaca la importancia de la transición de estrategias lógicas en función del nivel de dificultad en el nivel de procesos, y ofrece consejos valiosos para la inferencia eficiente de los LLMs.",
      "upvotes": 22,
      "discussionId": "68355ce16a9c239ada09f9a9",
      "ai_summary": "PATS enhances LLM efficiency by dynamically adjusting reasoning strategies based on task difficulty, leveraging PRMs and Beam Search.",
      "ai_keywords": [
        "large-language models (LLMs)",
        "reasoning strategy",
        "task and reasoning process complexity",
        "training-free fast-slow thinking system switching",
        "Process-Level Adaptive Thinking Mode Switching (PATS)",
        "Process Reward Models (PRMs)",
        "Beam Search",
        "progressive mode switching",
        "bad-step penalty mechanisms",
        "mathematical benchmarks",
        "process-level",
        "difficulty-aware reasoning strategy adaptation"
      ]
    },
    "publishedAt": "2025-05-25T13:58:50.000Z",
    "title": "PATS: Process-Level Adaptive Thinking Mode Switching",
    "summary": "Current large-language models (LLMs) typically adopt a fixed reasoning\nstrategy, either simple or complex, for all questions, regardless of their\ndifficulty. This neglect of variation in task and reasoning process complexity\nleads to an imbalance between performance and efficiency. Existing methods\nattempt to implement training-free fast-slow thinking system switching to\nhandle problems of varying difficulty, but are limited by coarse-grained\nsolution-level strategy adjustments. To address this issue, we propose a novel\nreasoning paradigm: Process-Level Adaptive Thinking Mode Switching (PATS),\nwhich enables LLMs to dynamically adjust their reasoning strategy based on the\ndifficulty of each step, optimizing the balance between accuracy and\ncomputational efficiency. Our approach integrates Process Reward Models (PRMs)\nwith Beam Search, incorporating progressive mode switching and bad-step penalty\nmechanisms. Experiments on diverse mathematical benchmarks demonstrate that our\nmethodology achieves high accuracy while maintaining moderate token usage. This\nstudy emphasizes the significance of process-level, difficulty-aware reasoning\nstrategy adaptation, offering valuable insights into efficient inference for\nLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19250.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e0404662d6ab4f1107580f",
      "avatarUrl": "/avatars/ef71694fea5482078a637a3869e30d19.svg",
      "fullname": "Yi Wang",
      "name": "Yi53",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20259",
      "authors": [
        {
          "_id": "6835346b2fdc5f8e8ea1e3cf",
          "name": "Haoyu Wang",
          "hidden": false
        },
        {
          "_id": "6835346b2fdc5f8e8ea1e3d0",
          "name": "Zeyu Qin",
          "hidden": false
        },
        {
          "_id": "6835346b2fdc5f8e8ea1e3d1",
          "name": "Yifei Zhao",
          "hidden": false
        },
        {
          "_id": "6835346b2fdc5f8e8ea1e3d2",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "6835346b2fdc5f8e8ea1e3d3",
          "name": "Min Lin",
          "hidden": false
        },
        {
          "_id": "6835346b2fdc5f8e8ea1e3d4",
          "name": "Xueqian Wang",
          "hidden": false
        },
        {
          "_id": "6835346b2fdc5f8e8ea1e3d5",
          "name": "Tianyu Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T17:40:40.000Z",
      "submittedOnDailyAt": "2025-05-27T02:13:04.341Z",
      "title": "生命周期安全对齐的语言模型\n\n(这是更自然的翻译版本，保持了专业性和准确性。)",
      "submittedOnDailyBy": {
        "_id": "63d91b6d255ef6add20e1b38",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
        "isPro": false,
        "fullname": "Tianyu Pang",
        "user": "P2333",
        "type": "user"
      },
      "summary": "Los LLMs han logrado una sorprendente transición, pero su capacidad creciente hace que estén vulnerables a ataques jailbreaking flexibles que evitan la seguridad. La estrategia de defensa actual se centra en ataques conocidos, pero es crucial responder a ataques impredecibles que pueden surgir en el entorno. Para abordar este desafío, proponemos un marco de seguridad a lo largo de la vida para LLMs que puedan adaptarse continuamente a nuevas y evolucionando estrategias de jailbreaking. En este marco, introducimos dos elementos para la estrategia: Meta-Attacker se entrena para encontrar nuevas estrategias de jailbreaking de manera proactiva, mientras que Defender se entrena para resistir a ellas. Para activar Meta-Attacker eficazmente, primero extraemos puntos clave de grandes investigaciones sobre jailbreaking utilizando la API de GPT-4o. A través de entrenamientos repetitivos, en la primera iteración, Meta-Attacker logra un éxito del 73% en el Ataque de Sucesión de Retratos (RR) y un 57% de movimiento en el Ataque de Sucesión de Retratos en un turno (LAT). Por otro lado, Defender aumenta su resistencia, finalmente restringiendo el éxito de Meta-Attacker a un 7% y permitiendo que los LLMs puedan estar seguros y confiables en entornos abiertos. El código está disponible en https://github.com/sail-sg/LifelongSafetyAlignment.",
      "upvotes": 21,
      "discussionId": "6835346c2fdc5f8e8ea1e407",
      "githubRepo": "https://github.com/sail-sg/LifelongSafetyAlignment",
      "ai_summary": "A lifecycle safety alignment framework employs a Meta-Attacker and Defender to adapt LLMs to novel jailbreaking strategies, improving robustness in deployment.",
      "ai_keywords": [
        "LLMs",
        "jailbreaking attacks",
        "safety alignment",
        "lifelong safety alignment framework",
        "Meta-Attacker",
        "Defender",
        "GPT-4o API",
        "attack success rate",
        "transfer attack success rate",
        "single-turn attacks"
      ]
    },
    "publishedAt": "2025-05-26T13:40:40.000Z",
    "title": "Lifelong Safety Alignment for Language Models",
    "summary": "LLMs have made impressive progress, but their growing capabilities also\nexpose them to highly flexible jailbreaking attacks designed to bypass safety\nalignment. While many existing defenses focus on known types of attacks, it is\nmore critical to prepare LLMs for unseen attacks that may arise during\ndeployment. To address this, we propose a lifelong safety alignment framework\nthat enables LLMs to continuously adapt to new and evolving jailbreaking\nstrategies. Our framework introduces a competitive setup between two\ncomponents: a Meta-Attacker, trained to actively discover novel jailbreaking\nstrategies, and a Defender, trained to resist them. To effectively warm up the\nMeta-Attacker, we first leverage the GPT-4o API to extract key insights from a\nlarge collection of jailbreak-related research papers. Through iterative\ntraining, the first iteration Meta-Attacker achieves a 73% attack success rate\n(ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks.\nMeanwhile, the Defender progressively improves its robustness and ultimately\nreduces the Meta-Attacker's success rate to just 7%, enabling safer and more\nreliable deployment of LLMs in open-ended environments. The code is available\nat https://github.com/sail-sg/LifelongSafetyAlignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20259.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d91b6d255ef6add20e1b38",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
      "fullname": "Tianyu Pang",
      "name": "P2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18675",
      "authors": [
        {
          "_id": "68351dde0c0aff775f3933ee",
          "user": {
            "_id": "67a4a26d5e65aa63c6d30e68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
            "isPro": false,
            "fullname": "FENG SICHENG",
            "user": "FSCCS",
            "type": "user"
          },
          "name": "Sicheng Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:52:13.406Z",
          "hidden": false
        },
        {
          "_id": "68351dde0c0aff775f3933ef",
          "name": "Song Wang",
          "hidden": false
        },
        {
          "_id": "68351dde0c0aff775f3933f0",
          "name": "Shuyi Ouyang",
          "hidden": false
        },
        {
          "_id": "68351dde0c0aff775f3933f1",
          "name": "Lingdong Kong",
          "hidden": false
        },
        {
          "_id": "68351dde0c0aff775f3933f2",
          "name": "Zikai Song",
          "hidden": false
        },
        {
          "_id": "68351dde0c0aff775f3933f3",
          "name": "Jianke Zhu",
          "hidden": false
        },
        {
          "_id": "68351dde0c0aff775f3933f4",
          "user": {
            "_id": "62b624f3b52bef716e248fd7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b624f3b52bef716e248fd7/AllcccKH-eBWduA8KVnOQ.png",
            "isPro": false,
            "fullname": "Huan Wang",
            "user": "Huan-WhoRegisteredMyName",
            "type": "user"
          },
          "name": "Huan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:52:11.085Z",
          "hidden": false
        },
        {
          "_id": "68351dde0c0aff775f3933f5",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T12:33:52.000Z",
      "submittedOnDailyAt": "2025-05-27T00:35:52.585Z",
      "title": "MLLMs ¿están regresando? Benchmark de lógica visualizada desde la guía de transporte hasta los detalles específicos",
      "submittedOnDailyBy": {
        "_id": "67a4a26d5e65aa63c6d30e68",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
        "isPro": false,
        "fullname": "FENG SICHENG",
        "user": "FSCCS",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje multimodal de Damo (MLLMs) han logrado notables avances recientes en tareas visuales, comprendiendo mejor la escala significativa y el arreglo de texto y imagen. Para mejorar su rendimiento en tareas complejas relacionadas con matemáticas o lógica, se han introducido variables lógicas. Sin embargo, su capacidad para comprender de manera detallada la visión no ha sido suficientemente evaluada en tareas lógicas.\n\nPara abordar esta limitación, presentamos el ReasonMap benchmark para evaluar la comprensión visual detallada y el poder lógico espacial de los MLLMs. ReasonMap incluye altas resoluciones de mapas de transporte de 30 ciudades en 13 países, con 1,008 pares de preguntas y respuestas, utilizando dos tipos de preguntas y tres templates. Además, hemos diseñado un plug-in de evaluación en dos etapas para evaluar precisamente la exactitud y calidad de las respuestas. Evaluamos de manera detallada 15 MLLMs (que incluyen tanto modelos básicos como variables lógicas), descubiertas patrones intuitivos: los modelos de código abierto superan a los modelos básicos en lógica, mientras que los modelos de código cerrado muestran un comportamiento opuesto. Además, la pérdida de la visión visual conduce a una disminución de rendimiento en límites generales, lo que demuestra que, aunque los MLLMs pueden utilizar su conocimiento, necesitan una percepción visual real para mostrar un excelente rendimiento en tareas lógicas espaciales. Nuestro estudio de benchmark ofrece un nuevo enfoque en la lógica visual y investiga las diferencias entre modelos de código abierto y cerrado.",
      "upvotes": 21,
      "discussionId": "68351de10c0aff775f39347a",
      "projectPage": "https://fscdc.github.io/Reason-Map/",
      "githubRepo": "https://github.com/fscdc/ReasonMap",
      "ai_summary": "ReasonMap evaluates the fine-grained visual understanding and spatial reasoning abilities of multimodal large language models, revealing that base models often outperform reasoning variants and highlighting the importance of genuine visual perception for complex tasks.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "semantic scene understanding",
        "text-image alignment",
        "reasoning variants",
        "ReasonMap",
        "high-resolution transit maps",
        "question-answer pairs",
        "two-level evaluation pipeline",
        "open-source models",
        "closed-source models",
        "visual reasoning"
      ]
    },
    "publishedAt": "2025-05-24T08:33:52.000Z",
    "title": "Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual\n  Reasoning from Transit Maps",
    "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nprogress in visual tasks, including semantic scene understanding and text-image\nalignment, with reasoning variants enhancing performance on complex tasks\ninvolving mathematics and logic. However, their capacity for reasoning tasks\ninvolving fine-grained visual understanding remains insufficiently evaluated.\nTo address this gap, we introduce ReasonMap, a benchmark designed to assess the\nfine-grained visual understanding and spatial reasoning abilities of MLLMs.\nReasonMap encompasses high-resolution transit maps from 30 cities across 13\ncountries and includes 1,008 question-answer pairs spanning two question types\nand three templates. Furthermore, we design a two-level evaluation pipeline\nthat properly assesses answer correctness and quality. Comprehensive\nevaluations of 15 popular MLLMs, including both base and reasoning variants,\nreveal a counterintuitive pattern: among open-source models, base models\noutperform reasoning ones, while the opposite trend is observed in\nclosed-source models. Additionally, performance generally degrades when visual\ninputs are masked, indicating that while MLLMs can leverage prior knowledge to\nanswer some questions, fine-grained visual reasoning tasks still require\ngenuine visual perception for strong performance. Our benchmark study offers\nnew insights into visual reasoning and contributes to investigating the gap\nbetween open-source and closed-source models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18675.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67a4a26d5e65aa63c6d30e68",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
      "fullname": "FENG SICHENG",
      "name": "FSCCS",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19209",
      "authors": [
        {
          "_id": "683529e8ddbf19d1df9038fb",
          "user": {
            "_id": "646a11791556443f24b582e9",
            "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
            "isPro": false,
            "fullname": "Zonglin Yang",
            "user": "ZonglinY",
            "type": "user"
          },
          "name": "Zonglin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:50:55.453Z",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df9038fc",
          "name": "Wanhao Liu",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df9038fd",
          "name": "Ben Gao",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df9038fe",
          "name": "Yujie Liu",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df9038ff",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df903900",
          "name": "Tong Xie",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df903901",
          "name": "Lidong Bing",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df903902",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df903903",
          "name": "Erik Cambria",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df903904",
          "name": "Dongzhan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T16:13:46.000Z",
      "submittedOnDailyAt": "2025-05-27T01:30:07.083Z",
      "title": "MOOSE-Chem2: Exploración de hipótesis científicas sobre las limitaciones de los modelos de lenguaje de inteligencia artificial: por medio de búsquedas jerárquicas.",
      "submittedOnDailyBy": {
        "_id": "646a11791556443f24b582e9",
        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
        "isPro": false,
        "fullname": "Zonglin Yang",
        "user": "ZonglinY",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) muestran buenos resultados en la automatización de la generación de hipótesis científicas, pero el enfoque actual principalmente genera hipótesis breves, con pocas detalladas metodológicas y experimentales. Presentamos una nueva tarea de la detección de hipótesis científicas detalladas y la definimos formalmente. Esto implica generar hipótesis detalladas y operables experimentalmente desde direcciones iniciales breves. Esta se combina en un problema de optimización, investigando si se puede resolver con el máximo potencial de la capacidad de los LLMs. Explicamos específicamente cuatro problemas básicos: (1) ¿Cómo se utilizan las heurísticas internas de los LLMs para determinar la mejor hipótesis de todas aquellas posibles que generan; (2) ¿Si la hipótesis más valiosa evaluada por el LLM coincide fuertemente con la hipótesis real; (3) ¿Se obtienen mejores resultados formando un mapa de recompensas (REWARD Landscape) con un ensamble de varios LLMs con la misma capacidad que los mejores instancias iterativas; (4) ¿Se proporciona un mapa de recompensas más confiable con un ensamble de un solo LLM que con un solo LLM. Para resolver estos problemas, proponemos un método de búsqueda heurística que evoluciona de conceptos generales a configuraciones experimentales específicas, agregando detalles a las hipótesis y evolucionando de conceptos generales a configuraciones experimentales específicas. Este proceso jerárquico suaviza el mapa de recompensas y facilita una optimización más efectiva. Nuestros métodos se han evaluado experimentalmente en un nuevo benchmark de hipótesis detalladas basado en la literatura química, annotada por expertos, y se han demostrado consistentemente excelentes comparados con fuertes referencias.",
      "upvotes": 20,
      "discussionId": "683529e9ddbf19d1df903939",
      "ai_summary": "A method is proposed to generate detailed scientific hypotheses using LLMs by defining and optimizing a latent reward landscape, outperforming baselines in benchmark evaluations.",
      "ai_keywords": [
        "large language models",
        "fine-grained scientific hypothesis discovery",
        "combinatorial optimization",
        "latent reward landscape",
        "hierarchical search method"
      ]
    },
    "publishedAt": "2025-05-25T12:13:46.000Z",
    "title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis\n  Discovery via Hierarchical Search",
    "summary": "Large language models (LLMs) have shown promise in automating scientific\nhypothesis generation, yet existing approaches primarily yield coarse-grained\nhypotheses lacking critical methodological and experimental details. We\nintroduce and formally define the novel task of fine-grained scientific\nhypothesis discovery, which entails generating detailed, experimentally\nactionable hypotheses from coarse initial research directions. We frame this as\na combinatorial optimization problem and investigate the upper limits of LLMs'\ncapacity to solve it when maximally leveraged. Specifically, we explore four\nfoundational questions: (1) how to best harness an LLM's internal heuristics to\nformulate the fine-grained hypothesis it itself would judge as the most\npromising among all the possible hypotheses it might generate, based on its own\ninternal scoring-thus defining a latent reward landscape over the hypothesis\nspace; (2) whether such LLM-judged better hypotheses exhibit stronger alignment\nwith ground-truth hypotheses; (3) whether shaping the reward landscape using an\nensemble of diverse LLMs of similar capacity yields better outcomes than\ndefining it with repeated instances of the strongest LLM among them; and (4)\nwhether an ensemble of identical LLMs provides a more reliable reward landscape\nthan a single LLM. To address these questions, we propose a hierarchical search\nmethod that incrementally proposes and integrates details into the hypothesis,\nprogressing from general concepts to specific experimental configurations. We\nshow that this hierarchical process smooths the reward landscape and enables\nmore effective optimization. Empirical evaluations on a new benchmark of\nexpert-annotated fine-grained hypotheses from recent chemistry literature show\nthat our method consistently outperforms strong baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19209.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646a11791556443f24b582e9",
      "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
      "fullname": "Zonglin Yang",
      "name": "ZonglinY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18536",
      "authors": [
        {
          "_id": "68351f7a06b4dae20a214442",
          "name": "Haoyuan Sun",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a214443",
          "name": "Jiaqi Wu",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a214444",
          "name": "Bo Xia",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a214445",
          "name": "Yifu Luo",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a214446",
          "name": "Yifei Zhao",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a214447",
          "name": "Kai Qin",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a214448",
          "name": "Xufei Lv",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a214449",
          "name": "Tiantian Zhang",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a21444a",
          "name": "Yongzhe Chang",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a21444b",
          "name": "Xueqian Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T06:01:48.000Z",
      "submittedOnDailyAt": "2025-05-27T00:43:18.404Z",
      "title": "Reinforcing the inference capabilities of a multi-type multi-language model through reinforcement adjustment.",
      "submittedOnDailyBy": {
        "_id": "65e2d43f9fb58a5115253049",
        "avatarUrl": "/avatars/46bd4ae27eaa23802cef3d91626897b5.svg",
        "isPro": false,
        "fullname": "Haoyuan Sun",
        "user": "xiaonengmiao",
        "type": "user"
      },
      "summary": "En 2025, se encuentra en una época importante relacionada con la persecución de la AGI, y RFT muestra el potencial crucial para elevar la inteligencia artificial de los modelos de lenguaje de entrada (LLMs), asociándose con el desarrollo de modelos avanzados como OpenAI-o1 y DeepSeek-R1. Además, se ha recibido un amplio respeto en la comunidad por su eficiente aplicación para elevar la inteligencia artificial de los MLLMs. Este artículo afirma que RFT apoya la inteligencia artificial de los MLLMs y ofrece una introducción detallada de los conocimientos básicos necesarios para los investigadores interesados en este campo, sumarizando los puntos clave en los que RFT puede mejorar la inteligencia artificial de los MLLMs: módulos diversos, tareas y áreas diversas, algoritmos de aprendizaje más eficientes, marcos de referencia ricos y marcos de trabajo de ingeniería activos. Finalmente, propone cinco direcciones potenciales de investigación que la comunidad debe considerar. Espera que este artículo sea una etapa importante en el desarrollo de la AGI y brinde consejos valiosos a la comunidad. Un resumen del estudio de la aplicación de RFT en MLLMs se puede encontrar en: https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.",
      "upvotes": 16,
      "discussionId": "68351f7b06b4dae20a2144b5",
      "projectPage": "https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs",
      "githubRepo": "https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs",
      "ai_summary": "Reinforcement fine-tuning significantly enhances the reasoning capabilities of multimodal large language models through diverse modalities, tasks, algorithms, benchmarks, and frameworks.",
      "ai_keywords": [
        "reinforcement fine-tuning",
        "multimodal large language models",
        "OpenAI-o1",
        "DeepSeek-R1",
        "diverse modalities",
        "diverse tasks and domains",
        "better training algorithms",
        "abundant benchmarks",
        "thriving engineering frameworks"
      ]
    },
    "publishedAt": "2025-05-24T02:01:48.000Z",
    "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal\n  Large Language Models",
    "summary": "Standing in 2025, at a critical juncture in the pursuit of Artificial General\nIntelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated\nsignificant potential in enhancing the reasoning capability of large language\nmodels (LLMs) and has led to the development of cutting-edge AI models such as\nOpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to\nenhance the reasoning capability of multimodal large language models (MLLMs)\nhas attracted widespread attention from the community. In this position paper,\nwe argue that reinforcement fine-tuning powers the reasoning capability of\nmultimodal large language models. To begin with, we provide a detailed\nintroduction to the fundamental background knowledge that researchers\ninterested in this field should be familiar with. Furthermore, we meticulously\nsummarize the improvements of RFT in powering reasoning capability of MLLMs\ninto five key points: diverse modalities, diverse tasks and domains, better\ntraining algorithms, abundant benchmarks and thriving engineering frameworks.\nFinally, we propose five promising directions for future research that the\ncommunity might consider. We hope that this position paper will provide\nvaluable insights to the community at this pivotal stage in the advancement\ntoward AGI. Summary of works done on RFT for MLLMs is available at\nhttps://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18536.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65e2d43f9fb58a5115253049",
      "avatarUrl": "/avatars/46bd4ae27eaa23802cef3d91626897b5.svg",
      "fullname": "Haoyuan Sun",
      "name": "xiaonengmiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19439",
      "authors": [
        {
          "_id": "68355784bb7d114755346770",
          "name": "Rihui Xin",
          "hidden": false
        },
        {
          "_id": "68355784bb7d114755346771",
          "name": "Han Liu",
          "hidden": false
        },
        {
          "_id": "68355784bb7d114755346772",
          "name": "Zecheng Wang",
          "hidden": false
        },
        {
          "_id": "68355784bb7d114755346773",
          "name": "Yupeng Zhang",
          "hidden": false
        },
        {
          "_id": "68355784bb7d114755346774",
          "name": "Dianbo Sui",
          "hidden": false
        },
        {
          "_id": "68355784bb7d114755346775",
          "name": "Xiaolin Hu",
          "hidden": false
        },
        {
          "_id": "68355784bb7d114755346776",
          "name": "Bingning Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T02:56:22.000Z",
      "submittedOnDailyAt": "2025-05-27T04:41:46.037Z",
      "title": "\"Señales de reemplazo sobre formato y longitud: Aprendizaje por refuerzo para resolver el efecto de la adyacencia de la barra\"",
      "submittedOnDailyBy": {
        "_id": "62e52483a944e2a56cd2c6ca",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg",
        "isPro": false,
        "fullname": "Jiejun Tan",
        "user": "zstanjj",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje general han logrado un éxito notable en tareas de procesamiento del lenguaje natural, y el aprendizaje por refuerzo desempeña un papel importante cuando se aplican a aplicaciones específicas. Sin embargo, obtener datos de respuestas correctas para el entrenamiento de los modelos de lenguaje general en problemas de matemáticas es generalmente difícil y costoso, lo que hace que esto sea prácticamente imposible. En este estudio, se revisa la metodología de usar sintaxis y longitud como señales de retroalimentación para entrenar modelos de lenguaje general en problemas de matemáticas, evitando la dependencia de datos de respuestas correctas existente. Se observó un mejoramiento del rendimiento en las etapas iniciales al centrarse en la precisión de la sintaxis y configurar la función de recompensa. Reconocieron las limitaciones de la recompensa de la sintaxis y utilizaron una recompensa basada en longitud. De esta manera, el enfoque GRPO que usa sintaxis y longitud como señales de retroalimentación superó al enfoque GRPO basado en datos de respuestas correctas, y en ciertos escenarios, alcanzó una precisión del 40.0% en el AIME2024. Este estudio proporciona una solución práctica para el entrenamiento de modelos de lenguaje general en problemas de matemáticas, reduciendo la dependencia excesiva de la recopilación de datos de respuestas correctas. Además, ilumina por qué el enfoque sin etiquetas puede ser exitoso: el modelo básico es visto como un excelente estudiante que ya ha aprendido matemáticas y habilidades de inferencia lógica, pero su expresión en el examen puede ser insuficiente para obtener resultados excelentes, lo que significa que la formación en la resolución de problemas es crucial para lograr buenos resultados en el examen.",
      "upvotes": 15,
      "discussionId": "68355785bb7d1147553467b8",
      "ai_summary": "The research demonstrates that using format and length as surrogate signals can improve LLMs' performance in mathematical problem-solving, matching or surpassing traditional methods without extensive ground truth data.",
      "ai_keywords": [
        "Large Language Models",
        "Reinforcement Learning",
        "mathematical problem-solving",
        "GRPO algorithm",
        "format correctness",
        "length-based rewards",
        "AIME2024"
      ]
    },
    "publishedAt": "2025-05-25T22:56:22.000Z",
    "title": "Surrogate Signals from Format and Length: Reinforcement Learning for\n  Solving Mathematical Problems without Ground Truth Answers",
    "summary": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers.Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0\\% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: base model is like\nan excellent student who has already mastered mathematical and logical\nreasoning skills, but performs poorly on the test paper, it simply needs to\ndevelop good answering habits to achieve outstanding results in exams , in\nother words, to unlock the capabilities it already possesses.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19439.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e52483a944e2a56cd2c6ca",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg",
      "fullname": "Jiejun Tan",
      "name": "zstanjj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18601",
      "authors": [
        {
          "_id": "6835268212dd354d6acdacbf",
          "name": "Jongwoo Ko",
          "hidden": false
        },
        {
          "_id": "6835268212dd354d6acdacc0",
          "user": {
            "_id": "63f0c2ac9cf89c9ed1bdd25c",
            "avatarUrl": "/avatars/856b2cb482250fb83c6fe793e29dfd74.svg",
            "isPro": false,
            "fullname": "Sungnyun Kim",
            "user": "sungnyun",
            "type": "user"
          },
          "name": "Sungnyun Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T08:15:10.751Z",
          "hidden": false
        },
        {
          "_id": "6835268212dd354d6acdacc1",
          "name": "Sungwoo Cho",
          "hidden": false
        },
        {
          "_id": "6835268212dd354d6acdacc2",
          "name": "Se-Young Yun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T08:50:53.000Z",
      "submittedOnDailyAt": "2025-05-27T01:14:15.443Z",
      "title": "Flex-Judge: Piensa por un momento, puedes juzgar en cualquier lugar.",
      "submittedOnDailyBy": {
        "_id": "63f0c2ac9cf89c9ed1bdd25c",
        "avatarUrl": "/avatars/856b2cb482250fb83c6fe793e29dfd74.svg",
        "isPro": false,
        "fullname": "Sungnyun Kim",
        "user": "sungnyun",
        "type": "user"
      },
      "summary": "Los señales de recompensa generadas por seres humanos son cruciales para alinear los modelos generativos con las preferencias humanas, y pueden proporcionar guías en la evaluación durante el aprendizaje y la inferencia. Al utilizar modelos de lenguaje de gran escala (LLM) como evaluadores delegados, se puede reducir significativamente los costos asociados con los comentarios manuales. Sin embargo, estos modelos generalmente necesitan entrenamiento con datos de modelado amplio para generalizar a problemas de diversidad, y su efectividad práctica está limitada. En este artículo, se propone un modelo de evaluación de diversidad \"Flex-Judge\" que guia la evaluación con razones contextuales mínimas, y permite una generalización amplia de modelos y formas de evaluación. La conclusión central del artículo es que patrones de juicio que se pueden generalizar a contextos estructurados están incorporados, lo que permite un excelente rendimiento en evaluaciones de diversidad que incluyan imágenes o videos. Los resultados de los experimentos muestran que Flex-Judge logra rendimientos competitivos o excelentes, incluso cuando la cantidad de datos contextual es significativamente baja, comparado con los modelos de evaluación de diversidad ampliamente entrenados y los APIs comerciales más avanzados. En particular, también tiene un impacto significativo en modelos como los de moléculas, donde los marcos de evaluación detallados son escasos. Este marco es una solución potente y costo-eficiente que reemplaza la forma tradicional de comentarios, y significativamente mejora la evaluación de modelos de diversidad.",
      "upvotes": 15,
      "discussionId": "6835268312dd354d6acdad1e",
      "projectPage": "https://flex-judge.github.io/",
      "githubRepo": "https://github.com/jongwooko/flex-judge",
      "ai_summary": "Flex-Judge uses minimal textual reasoning data to generalize across multiple modalities and evaluation formats, outperforming state-of-the-art models in multimodal evaluations.",
      "ai_keywords": [
        "reasoning-guided multimodal judge model",
        "structured textual reasoning explanations",
        "generalizable decision-making patterns",
        "multimodal judgments",
        "molecule evaluations",
        "reasoning-based text supervision",
        "scalable multimodal model-as-a-judge"
      ]
    },
    "publishedAt": "2025-05-24T04:50:53.000Z",
    "title": "Flex-Judge: Think Once, Judge Anywhere",
    "summary": "Human-generated reward signals are critical for aligning generative models\nwith human preferences, guiding both training and inference-time evaluations.\nWhile large language models (LLMs) employed as proxy evaluators, i.e.,\nLLM-as-a-Judge, significantly reduce the costs associated with manual\nannotations, they typically require extensive modality-specific training data\nand fail to generalize well across diverse multimodal tasks. In this paper, we\npropose Flex-Judge, a reasoning-guided multimodal judge model that leverages\nminimal textual reasoning data to robustly generalize across multiple\nmodalities and evaluation formats. Our core intuition is that structured\ntextual reasoning explanations inherently encode generalizable decision-making\npatterns, enabling an effective transfer to multimodal judgments, e.g., with\nimages or videos. Empirical results demonstrate that Flex-Judge, despite being\ntrained on significantly fewer text data, achieves competitive or superior\nperformance compared to state-of-the-art commercial APIs and extensively\ntrained multimodal evaluators. Notably, Flex-Judge presents broad impact in\nmodalities like molecule, where comprehensive evaluation benchmarks are scarce,\nunderscoring its practical value in resource-constrained domains. Our framework\nhighlights reasoning-based text supervision as a powerful, cost-effective\nalternative to traditional annotation-intensive approaches, substantially\nadvancing scalable multimodal model-as-a-judge.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18601.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f0c2ac9cf89c9ed1bdd25c",
      "avatarUrl": "/avatars/856b2cb482250fb83c6fe793e29dfd74.svg",
      "fullname": "Sungnyun Kim",
      "name": "sungnyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19949",
      "authors": [
        {
          "_id": "68352aac38e5ca9eb5349c2f",
          "name": "Siqi Kou",
          "hidden": false
        },
        {
          "_id": "68352aac38e5ca9eb5349c30",
          "name": "Qingyuan Tian",
          "hidden": false
        },
        {
          "_id": "68352aac38e5ca9eb5349c31",
          "name": "Hanwen Xu",
          "hidden": false
        },
        {
          "_id": "68352aac38e5ca9eb5349c32",
          "name": "Zihao Zeng",
          "hidden": false
        },
        {
          "_id": "68352aac38e5ca9eb5349c33",
          "name": "Zhijie Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T13:15:26.000Z",
      "submittedOnDailyAt": "2025-05-27T01:30:37.130Z",
      "title": "¿Las características de los datos pueden impulsar la lógica de la matemática y el código? Investigación utilizando funciones de influencia.",
      "submittedOnDailyBy": {
        "_id": "654e330f350abceb30a1390b",
        "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg",
        "isPro": false,
        "fullname": "KouSiqi",
        "user": "karrykkk",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) desarrollan una capacidad lógica extraordinaria en las áreas de la matemática y el código a través de modelos de aprendizaje por refuerzo que generan pensamientos de la lógica (CoTs) y se adaptan posteriormente. Sin embargo, la estrategia de coloración de datos actual se basa principalmente en heurísticas, lo que limita su generalización y no captura los detalles delicados de los datos. Para resolver estos limitaciones, utilizamos funciones de influencia para asignar de manera sistemática las habilidades lógicas de matemática y código en los modelos de LLMs, dependiendo de los muestras de entrenamiento, secuencias y tokens. Este enfoque sistemático ha permitido que nuestro sistema de asignación lógica basado en funciones de influencia (Infra) revele efectos no lineales en las tareas de matemática y código: muestras de alto nivel de dificultad mejoran tanto la lógica matemática como la lógica del código, mientras que tareas de bajo nivel de dificultad del código maximizan la lógica del código. Con estas observaciones, presentamos una estrategia sencilla y efectiva para reemplazar el grado de dificultad en los conjuntos de datos: esto ha aumentado la precisión de AIME24 del 10% al 20% y la precisión de Qwen2.5-7B-Instruct en LiveCodeBench del 33.8% al 35.3%. Además, nuestro enfoque detallado mejora el desempeño lógico de matemática y código mediante acciones exploratorias a nivel de secuencia y patrones de influencia a nivel de token, que difieren entre matemática y código: el primero prefiere conectores lógicos del lenguaje natural, mientras que el segundo enfatiza la estructura.",
      "upvotes": 13,
      "discussionId": "68352aad38e5ca9eb5349c6f",
      "ai_summary": "Influence functions are used to attribute LLMs' reasoning in math and coding to individual training elements, revealing cross-domain effects and enabling a reweighting strategy that improves model accuracy.",
      "ai_keywords": [
        "Large language models (LLMs)",
        "chain-of-thoughts (CoTs)",
        "influence functions",
        "attribution",
        "data curation",
        "reasoning ability",
        "high-difficulty math examples",
        "low-difficulty code tasks",
        "dataset reweighting strategy",
        "AIME24 accuracy",
        "LiveCodeBench accuracy",
        "sequence-level exploratory behaviors",
        "token-level influence patterns",
        "natural language logic connectors",
        "structural syntax",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-05-26T09:15:26.000Z",
    "title": "Which Data Attributes Stimulate Math and Code Reasoning? An\n  Investigation via Influence Functions",
    "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities in math and coding, often bolstered by post-training on the\nchain-of-thoughts (CoTs) generated by stronger models. However, existing\nstrategies for curating such training data predominantly rely on heuristics,\nlimiting generalizability and failing to capture subtleties underlying in data.\nTo address these limitations, we leverage influence functions to systematically\nattribute LLMs' reasoning ability on math and coding to individual training\nexamples, sequences, and tokens, enabling deeper insights into effective data\ncharacteristics. Our Influence-based Reasoning Attribution (Infra) uncovers\nnontrivial cross-domain effects across math and coding tasks: high-difficulty\nmath examples improve both math and code reasoning, while low-difficulty code\ntasks most effectively benefit code reasoning. Based on these findings, we\nintroduce a simple yet effective dataset reweighting strategy by flipping task\ndifficulty, which doubles AIME24 accuracy from 10\\% to 20\\% and boosts\nLiveCodeBench accuracy from 33.8\\% to 35.3\\% for Qwen2.5-7B-Instruct. Moreover,\nour fine-grained attribution reveals that the sequence-level exploratory\nbehaviors enhance reasoning performance in both math and code, and the\ntoken-level influence patterns are distinct for math and code reasoning: the\nformer prefers natural language logic connectors and the latter emphasizes\nstructural syntax.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654e330f350abceb30a1390b",
      "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg",
      "fullname": "KouSiqi",
      "name": "karrykkk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20256",
      "authors": [
        {
          "_id": "68352e44c829f2ea1e0484b5",
          "name": "Hao Zhong",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484b6",
          "name": "Muzhi Zhu",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484b7",
          "name": "Zongze Du",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484b8",
          "name": "Zheng Huang",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484b9",
          "user": {
            "_id": "646efd223dd912a539e0bd46",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
            "isPro": false,
            "fullname": "Canyu Zhao",
            "user": "Canyu",
            "type": "user"
          },
          "name": "Canyu Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:50:50.579Z",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484ba",
          "name": "Mingyu Liu",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484bb",
          "name": "Wen Wang",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484bc",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484bd",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T17:34:06.000Z",
      "submittedOnDailyAt": "2025-05-27T01:47:33.260Z",
      "title": "Omni-R1: El aprendizaje de la inteligencia artificial en el modo Omni mediante colaboración de dos sistemas",
      "submittedOnDailyBy": {
        "_id": "632179745fc60c44fd91fc33",
        "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
        "isPro": false,
        "fullname": "zhumuzhi",
        "user": "Z-MU-Z",
        "type": "user"
      },
      "summary": "La inferencia de videos y audio a largo plazo y la comprensión pixel a pixel plantean diferentes requisitos para modelos visuales completos: una cobertura temporal densa requiere múltiples frames de baja resolución, mientras que una comprensión precisa del fondo necesita entradas de alta resolución. Resolvemos esta equilibrio mediante dos arquitecturas de sistema: el sistema de inferencia global selecciona frames clave ricos en información y adapta el costo espacial a la tarea, mientras que el sistema de comprensión de detalles realiza una comprensión de fondo pixel a pixel en fragmentos de alta resolución seleccionados. Debido a que la selección de \"mejores\" frames clave y la adaptación no son claras ni normativas, es difícil aprender. Por lo tanto, proponemos un problema de aprendizaje por refuerzo basado en el final (RL) y optimizamos el método utilizando el Policy Optimization Group Relative, denominado Omni-R1. Omni-R1 aprende el sistema de inferencia global utilizando recompensas paso a paso obtenidas en colaboración en línea por el sistema de comprensión de detalles, requiriendo solo una ronda de aprendizaje por refuerzo en cada fragmento de tarea pequeña.\n\nLos experimentos realizados en referencia a los estándares de referencia audiovisual segmentación (RefAVS) y segmentación de objetos en video por razonamiento (REVOS) muestran que Omni-R1 supera a los modelos de aprendizaje por fuerza de base y a modelos avanzados especializados, demostrando una mejora significativa al inhibir la expansión de la discriminación y la habilidad multimodal de Harlow. Nuestros resultados indican que por primera vez, el aprendizaje por refuerzo se ha aplicado exitosamente en la inferencia visual completa a gran escala, mostrando un camino de expansión.",
      "upvotes": 11,
      "discussionId": "68352e47c829f2ea1e048539",
      "projectPage": "https://aim-uofa.github.io/OmniR1/",
      "githubRepo": "https://github.com/aim-uofa/Omni-R1",
      "ai_summary": "An end-to-end reinforcement learning framework, Omni-R1, achieves superior performance in long-horizon video-audio reasoning and fine-grained pixel understanding tasks by combining global reasoning and detail understanding systems.",
      "ai_keywords": [
        "reinforcement learning",
        "Group Relative Policy Optimization",
        "hierarchical rewards",
        "Referring Audio-Visual Segmentation",
        "Reasoning Video Object Segmentation",
        "out-of-domain generalization",
        "multimodal hallucination",
        "universally foundation models"
      ]
    },
    "publishedAt": "2025-05-26T13:34:06.000Z",
    "title": "Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System\n  Collaboration",
    "summary": "Long-horizon video-audio reasoning and fine-grained pixel understanding\nimpose conflicting requirements on omnimodal models: dense temporal coverage\ndemands many low-resolution frames, whereas precise grounding calls for\nhigh-resolution inputs. We tackle this trade-off with a two-system\narchitecture: a Global Reasoning System selects informative keyframes and\nrewrites the task at low spatial cost, while a Detail Understanding System\nperforms pixel-level grounding on the selected high-resolution snippets.\nBecause ``optimal'' keyframe selection and reformulation are ambiguous and hard\nto supervise, we formulate them as a reinforcement learning (RL) problem and\npresent Omni-R1, an end-to-end RL framework built on Group Relative Policy\nOptimization. Omni-R1 trains the Global Reasoning System through hierarchical\nrewards obtained via online collaboration with the Detail Understanding System,\nrequiring only one epoch of RL on small task splits.\n  Experiments on two challenging benchmarks, namely Referring Audio-Visual\nSegmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show\nthat Omni-R1 not only surpasses strong supervised baselines but also\noutperforms specialized state-of-the-art models, while substantially improving\nout-of-domain generalization and mitigating multimodal hallucination. Our\nresults demonstrate the first successful application of RL to large-scale\nomnimodal reasoning and highlight a scalable path toward universally foundation\nmodels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20256.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632179745fc60c44fd91fc33",
      "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
      "fullname": "zhumuzhi",
      "name": "Z-MU-Z",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19788",
      "authors": [
        {
          "_id": "68352630363d6fd2fff5d07f",
          "name": "Zihao Zeng",
          "hidden": false
        },
        {
          "_id": "68352630363d6fd2fff5d080",
          "user": {
            "_id": "6721dacfc5309c08451d21d5",
            "avatarUrl": "/avatars/ac8be5ac8b8ee5b5533214e526b72dad.svg",
            "isPro": false,
            "fullname": "Huang Xuyao",
            "user": "ElysiaTrue",
            "type": "user"
          },
          "name": "Xuyao Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:51:44.216Z",
          "hidden": false
        },
        {
          "_id": "68352630363d6fd2fff5d081",
          "name": "Boxiu Li",
          "hidden": false
        },
        {
          "_id": "68352630363d6fd2fff5d082",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "68352630363d6fd2fff5d083",
          "name": "Zhijie Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T10:18:57.000Z",
      "submittedOnDailyAt": "2025-05-27T01:59:02.853Z",
      "title": "Lo mejor completamente: la liberación de la razón eficiente mediante la descomposición estructurada por etapas",
      "submittedOnDailyBy": {
        "_id": "6721dacfc5309c08451d21d5",
        "avatarUrl": "/avatars/ac8be5ac8b8ee5b5533214e526b72dad.svg",
        "isPro": false,
        "fullname": "Huang Xuyao",
        "user": "ElysiaTrue",
        "type": "user"
      },
      "summary": "Los modelos lógicos de largo rango (LRMs) se critican por usar una cadena de pensamiento (CoT) demasiado larga para obtener la respuesta final, con un inicio de token y un tiempo de retraso altos. Generalmente, el CoT de los LRMs mezcla múltiples unidades de memoria. Cada unidad genera candidatos a respuestas para una pregunta de base. Una idea natural para aumentar la eficiencia es reducir el número de unidades. Sin embargo, en el CoT de BERT, las unidades de memoria no se manejan claramente, lo que complica este proceso. Este artículo introduce la Decomposición Multi-Turn (MinD) para transformar un solo CoT en una secuencia estructurada y interactiva de turnos, con el objetivo de corregir este error. En MinD, el modelo proporciona respuestas multi-turn para una consulta, donde cada turno incluye unidades de memoria y permite la generación de respuestas. Los turnos posteriores pueden reflexionar, verificar, modificar o sustituir las respuestas anteriores. Esto permite proporcionar respuestas más rápidas y controlar mejor el proceso lógico (por ejemplo, el usuario puede detenerse o continuar en cualquier turno). Nosotros aplicamos el patrón de aprendizaje por refuerzo (RL) para implementar MinD a través de fine-tuning supervisado (SFT). Primero, reprogramamos un modelo de lenguaje de alto rendimiento (LLM) para generar respuestas en formato multi-turn, y ajustamos el LRM con este datos. El modelo ajustado puede usar más tokens que el modelo original, demostrando que el formato multi-turn puede crear respuestas más extensas. Utilizando algoritmos de aprendizaje por refuerzo como GRPO (Generalized Proximal Policy Optimization), priorizamos la generación de respuestas precisas en menos turnos. El modelo MinD entrenado con el conjunto de datos MATH, como R1-Distill, reduce la cantidad de tokens utilizados y el tiempo del primer token (TTFT) en aproximadamente el 70%, mientras mantiene excelentes resultados en benchmarks lógicos como MATH-500, AIME24, AMC23 y GPQA-Diamond.",
      "upvotes": 11,
      "discussionId": "68352631363d6fd2fff5d0b9",
      "ai_summary": "Multi-Turn Decomposition improves efficiency in large reasoning models by breaking down chain-of-thought into manageable turns, reducing token usage and latency while maintaining performance.",
      "ai_keywords": [
        "Chain-of-Thought",
        "large reasoning models",
        "multi-turn decomposition",
        "thinking units",
        "iterative reasoning process",
        "supervised fine-tuning",
        "reinforcement learning",
        "MATH dataset",
        "R1-Distill models",
        "MATH-500",
        "AIME24",
        "AMC23",
        "GPQA-Diamond"
      ]
    },
    "publishedAt": "2025-05-26T06:18:57.000Z",
    "title": "Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured\n  Multi-Turn Decomposition",
    "summary": "Large Reasoning Models (LRMs) are criticized for the excessively lengthy\nChain-of-Thought (CoT) to derive the final answer, suffering from high\nfirst-token and overall latency. Typically, the CoT of LRMs mixes multiple\nthinking units; each unit attempts to produce a candidate answer to the\noriginal query. Hence, a natural idea to improve efficiency is to reduce the\nunit number. Yet, the fact that the thinking units in vanilla CoT cannot be\nexplicitly managed renders doing so challenging. This paper introduces\nMulti-Turn Decomposition (MinD) to decode conventional CoT into a sequence of\nexplicit, structured, and turn-wise interactions to bridge the gap. In MinD,\nthe model provides a multi-turn response to the query, where each turn embraces\na thinking unit and yields a corresponding answer. The subsequent turns can\nreflect, verify, revise, or explore alternative approaches to both the thinking\nand answer parts of earlier ones. This not only makes the answer delivered more\nswiftly, but also enables explicit controls over the iterative reasoning\nprocess (i.e., users may halt or continue at any turn). We follow a supervised\nfine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We\nfirst rephrase the outputs of an LRM into multi-turn formats by prompting\nanother LLM, and then tune the LRM with such data. Observing that the tuned\nmodel tends to consume even more tokens than the original one (probably due to\nthat the multi-turn formats introduce additional answer tokens), we advocate\nleveraging RL algorithms like GRPO to prioritize correct outputs with fewer\nturns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up\nto ~70% reduction in both output token usage and time to first token (TTFT),\nwhile maintaining competitive performance on reasoning benchmarks such as\nMATH-500, AIME24, AMC23, and GPQA-Diamond.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19788.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6721dacfc5309c08451d21d5",
      "avatarUrl": "/avatars/ac8be5ac8b8ee5b5533214e526b72dad.svg",
      "fullname": "Huang Xuyao",
      "name": "ElysiaTrue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19752",
      "authors": [
        {
          "_id": "683527ba3762eb8b3ea1de34",
          "user": {
            "_id": "62649e2b1ed8d81e47ad9b4e",
            "avatarUrl": "/avatars/f33a0b727822fd2ea99dce37fbda3d17.svg",
            "isPro": false,
            "fullname": "Li",
            "user": "henry12348",
            "type": "user"
          },
          "name": "Hengli Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:51:37.403Z",
          "hidden": false
        },
        {
          "_id": "683527ba3762eb8b3ea1de35",
          "user": {
            "_id": "60b9e6837946aff342f734ae",
            "avatarUrl": "/avatars/a711a6aa35757dfd7b78b26098a964fc.svg",
            "isPro": false,
            "fullname": "Yuxuan Wang",
            "user": "ColorfulAI",
            "type": "user"
          },
          "name": "Yuxuan Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-27T02:47:22.991Z",
          "hidden": false
        },
        {
          "_id": "683527ba3762eb8b3ea1de36",
          "name": "Song-Chun Zhu",
          "hidden": false
        },
        {
          "_id": "683527ba3762eb8b3ea1de37",
          "name": "Ying Nian Wu",
          "hidden": false
        },
        {
          "_id": "683527ba3762eb8b3ea1de38",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-27T02:47:22.991Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T09:32:12.000Z",
      "submittedOnDailyAt": "2025-05-27T01:18:56.411Z",
      "title": "Discrete Markov Bridge",
      "submittedOnDailyBy": {
        "_id": "62649e2b1ed8d81e47ad9b4e",
        "avatarUrl": "/avatars/f33a0b727822fd2ea99dce37fbda3d17.svg",
        "isPro": false,
        "fullname": "Li",
        "user": "henry12348",
        "type": "user"
      },
      "summary": "La discretización diferencial ha aparecido recientemente como un paradigma deseable en el modelado de datos discretos. Sin embargo, los métodos actuales generalmente utilizan matrices de transición con velocidades fijas durante el entrenamiento. Este enfoque limita la capacidad de representación de las representaciones potenciales y también limita el espacio de diseño total. Para resolver estas limitaciones, se propone un nuevo marco de trabajo llamado Discrete Markov Bridge. Este marco está especialmente diseñado para el aprendizaje de representaciones discretas. Nuestro enfoque se basa en dos elementos esenciales: Learning de Matrices y Learning de Puntos. Hemos realizado un análisis teórico riguroso, presentamos una garantía formal del rendimiento del Learning de Matrices y demostramos la convergencia del marco de trabajo en su totalidad. Además, hemos abordado las restricciones prácticas identificadas en estudios previos y analizamos la complejidad espacial de este método. Una evaluación experimental ampliada demuestra la efectividad del Discrete Markov Bridge, alcanzando un ELBO (inferior límite de probabilidad) de 1.38 en el conjunto de datos Text8 y superando los baselines existentes. Además, el modelo propuesto muestra un rendimiento relativo en el conjunto de datos CIFAR-10, obteniendo resultados similares a los de un enfoque generativo especializado en imágenes.",
      "upvotes": 11,
      "discussionId": "683527bb3762eb8b3ea1de6c",
      "projectPage": "https://github.com/Henry839/Discrete-Markov-Bridge/tree/main",
      "githubRepo": "https://github.com/Henry839/Discrete-Markov-Bridge/tree/main",
      "ai_summary": "A novel framework, Discrete Markov Bridge, is introduced for discrete data modeling with Matrix Learning and Score Learning components, demonstrating superior performance compared to existing methods on Text8 and CIFAR-10 datasets.",
      "ai_keywords": [
        "discrete diffusion",
        "variational methods",
        "Discrete Markov Bridge",
        "Matrix Learning",
        "Score Learning",
        "Evidence Lower Bound",
        "ELBO"
      ]
    },
    "publishedAt": "2025-05-26T05:32:12.000Z",
    "title": "Discrete Markov Bridge",
    "summary": "Discrete diffusion has recently emerged as a promising paradigm in discrete\ndata modeling. However, existing methods typically rely on a fixed rate\ntransition matrix during training, which not only limits the expressiveness of\nlatent representations, a fundamental strength of variational methods, but also\nconstrains the overall design space. To address these limitations, we propose\nDiscrete Markov Bridge, a novel framework specifically designed for discrete\nrepresentation learning. Our approach is built upon two key components: Matrix\nLearning and Score Learning. We conduct a rigorous theoretical analysis,\nestablishing formal performance guarantees for Matrix Learning and proving the\nconvergence of the overall framework. Furthermore, we analyze the space\ncomplexity of our method, addressing practical constraints identified in prior\nstudies. Extensive empirical evaluations validate the effectiveness of the\nproposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)\nof 1.38 on the Text8 dataset, outperforming established baselines. Moreover,\nthe proposed model demonstrates competitive performance on the CIFAR-10\ndataset, achieving results comparable to those obtained by image-specific\ngeneration approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19752.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62649e2b1ed8d81e47ad9b4e",
      "avatarUrl": "/avatars/f33a0b727822fd2ea99dce37fbda3d17.svg",
      "fullname": "Li",
      "name": "henry12348",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20152",
      "authors": [
        {
          "_id": "6835385ebd4d4208167d15ac",
          "name": "Kai Sun",
          "hidden": false
        },
        {
          "_id": "6835385ebd4d4208167d15ad",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "6835385ebd4d4208167d15ae",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "6835385ebd4d4208167d15af",
          "name": "Jiajie Zhang",
          "hidden": false
        },
        {
          "_id": "6835385ebd4d4208167d15b0",
          "name": "Ji Qi",
          "hidden": false
        },
        {
          "_id": "6835385ebd4d4208167d15b1",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "6835385ebd4d4208167d15b2",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T15:55:28.000Z",
      "submittedOnDailyAt": "2025-05-27T02:29:13.927Z",
      "title": "Para entender los límites, se necesita un gran modelo multimodelo para entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita entender los límites. Para entender los límites, se necesita",
      "submittedOnDailyBy": {
        "_id": "66cdd285c51a915bd5f2d017",
        "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
        "isPro": false,
        "fullname": "Jiajie Zhang",
        "user": "NeoZ123",
        "type": "user"
      },
      "summary": "Utilizando un visión visual encontrado a través de entrenamiento contrastivo en imágenes de gran escala natural, los grandes modelos multimodal (LMMs) han demostrado un desempeño sorprendente en diversas tareas de reconocimiento visual. Sin embargo, el entrenamiento contrastivo tiene límites inherentes al resumen de explicaciones, que limitan la capacidad de inferencia minuciosa y, en particular, afectan significativamente a la resolución de problemas geométricos. Para mejorar la comprensión geométrica, proponemos un nuevo marco de entrenamiento contrastivo para casos de alto desafío, combinando entrenamiento contrastivo basado en imágenes y contexto. Esto se logra utilizando ejemplos negativos difíciles generados por códigos de generación de formas, ejemplos negativos basados en reglas de explicaciones geométricas cambiantes, y ejemplos negativos basados en similitudes de captiones. Utilizamos este método de entrenamiento fuerte para entrenar CLIP y un LMM para la resolución de problemas geométricos. Nuestras experimentaciones muestran que nuestro modelo entrenado, MMGeoLM, supera significativamente a otros modelos abiertos. Incluso a una escala de 7B, MMGeoLM puede competir con modelos potentes como GPT-4o. Además, hemos investigado la influencia de diferentes métodos de construcción de ejemplos negativos y el número de ejemplos negativos en el desempeño de los LMMs en la inferencia geométrica. Nuestro código y conjunto de datos están disponibles en https://github.com/THU-KEG/MMGeoLM.",
      "upvotes": 10,
      "discussionId": "6835385fbd4d4208167d15f0",
      "ai_summary": "A novel hard negative contrastive learning framework improves geometric reasoning in Large Multimodal Models, significantly enhancing their performance compared to existing models.",
      "ai_keywords": [
        "contrastively trained visual encoders",
        "Large Multimodal Models",
        "geometric problem-solving",
        "hard negative contrastive learning",
        "generation-based hard negatives",
        "rule-based negatives",
        "retrieval-based negatives",
        "CLIP",
        "MMCLIP",
        "multimodal math clip",
        "MMGeoLM",
        "geometric reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-05-26T11:55:28.000Z",
    "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric\n  Understanding in Large Multimodal Models",
    "summary": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20152.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66cdd285c51a915bd5f2d017",
      "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
      "fullname": "Jiajie Zhang",
      "name": "NeoZ123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18759",
      "authors": [
        {
          "_id": "683551c54f3166e8677b43bb",
          "name": "Ruichen Zhang",
          "hidden": false
        },
        {
          "_id": "683551c54f3166e8677b43bc",
          "name": "Rana Muhammad Shahroz Khan",
          "hidden": false
        },
        {
          "_id": "683551c54f3166e8677b43bd",
          "name": "Zhen Tan",
          "hidden": false
        },
        {
          "_id": "683551c54f3166e8677b43be",
          "user": {
            "_id": "6474e1afb68461d5cf7c41cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
            "isPro": false,
            "fullname": "Dawei Li",
            "user": "wjldw",
            "type": "user"
          },
          "name": "Dawei Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:49:14.749Z",
          "hidden": false
        },
        {
          "_id": "683551c54f3166e8677b43bf",
          "name": "Song Wang",
          "hidden": false
        },
        {
          "_id": "683551c54f3166e8677b43c0",
          "name": "Tianlong Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T15:54:19.000Z",
      "submittedOnDailyAt": "2025-05-27T04:17:34.631Z",
      "title": "Exploración de razones eficientes: combinación de marcos de referencia centrados en datos y CoT",
      "submittedOnDailyBy": {
        "_id": "6474e1afb68461d5cf7c41cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
        "isPro": false,
        "fullname": "Dawei Li",
        "user": "wjldw",
        "type": "user"
      },
      "summary": "El decaimiento de los centros de datos, la edición de datos, la selección y la mezcla incluyendo, potencialmente, métodos para crear más eficientes grandes modelos de lenguaje (LLMs) para estudiantes. Sin embargo, aun no existen marcos de referencia detallados para evaluar efectivamente cada método de decaimiento. En este artículo, presentamos el primer marco de referencia del centro de datos, DC-CoT, que investiga la manipulación de datos de decaimiento de pensamiento en cadena (CoT) desde tres aspectos: métodos, modelos y datos, con el objetivo de evaluar efectivamente. Utilizamos diferentes modelos de enseñanza (por ejemplo: o4-mini, Gemini-Pro, Claude-3.5) y arquitecturas de estudiantes (por ejemplo: 3B, 7B parámetros) para evaluar con rigurosidad la influencia de estas manipulaciones en el rendimiento de los modelos estudiantes. En particular, se centra en la generalización dentro (IID) y fuera de la distribución (OOD), y en la movilidad de los dominios de datos. El objetivo de este estudio es proporcionar una visión práctica y efectiva de la optimización de la decaimiento de CoT basado en datos, lo que promueve el desarrollo de modelos de enseñanza más accesibles y potentes. Los datos están disponibles en https://huggingface.co/datasets/rana-shahroz/DC-COT y el código en https://anonymous.4open.science/r/DC-COT-FF4C/.",
      "upvotes": 10,
      "discussionId": "683551c64f3166e8677b4424",
      "ai_summary": "DC-CoT provides a comprehensive benchmark for assessing data-centric distillation techniques in chain-of-thought distillation, focusing on performance and generalization across different models and datasets.",
      "ai_keywords": [
        "data-centric distillation",
        "data augmentation",
        "data selection",
        "data mixing",
        "chain-of-thought (CoT)",
        "in-distribution (IID)",
        "out-of-distribution (OOD)",
        "cross-domain transfer"
      ]
    },
    "publishedAt": "2025-05-24T11:54:19.000Z",
    "title": "The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT\n  Distillation",
    "summary": "Data-centric distillation, including data augmentation, selection, and\nmixing, offers a promising path to creating smaller, more efficient student\nLarge Language Models (LLMs) that retain strong reasoning abilities. However,\nthere still lacks a comprehensive benchmark to systematically assess the effect\nof each distillation approach. This paper introduces DC-CoT, the first\ndata-centric benchmark that investigates data manipulation in chain-of-thought\n(CoT) distillation from method, model and data perspectives. Utilizing various\nteacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student\narchitectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of\nthese data manipulations on student model performance across multiple reasoning\ndatasets, with a focus on in-distribution (IID) and out-of-distribution (OOD)\ngeneralization, and cross-domain transfer. Our findings aim to provide\nactionable insights and establish best practices for optimizing CoT\ndistillation through data-centric techniques, ultimately facilitating the\ndevelopment of more accessible and capable reasoning models. The dataset can be\nfound at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is\nshared in https://anonymous.4open.science/r/DC-COT-FF4C/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18759.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6474e1afb68461d5cf7c41cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
      "fullname": "Dawei Li",
      "name": "wjldw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19602",
      "authors": [
        {
          "_id": "6835239e7309025530c85ba3",
          "user": {
            "_id": "6540ef0e733c1ce6a6fc989a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6540ef0e733c1ce6a6fc989a/lyDLbmJ-h4nUmkWZCvWtg.jpeg",
            "isPro": false,
            "fullname": "Kunjun Li",
            "user": "stargazerx0",
            "type": "user"
          },
          "name": "Kunjun Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:51:58.885Z",
          "hidden": false
        },
        {
          "_id": "6835239e7309025530c85ba4",
          "name": "Zigeng Chen",
          "hidden": false
        },
        {
          "_id": "6835239e7309025530c85ba5",
          "name": "Cheng-Yen Yang",
          "hidden": false
        },
        {
          "_id": "6835239e7309025530c85ba6",
          "name": "Jenq-Neng Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T07:11:42.000Z",
      "submittedOnDailyAt": "2025-05-27T01:00:44.064Z",
      "title": "Memoria eficiente de modelado visualizado de auto-restauración con reducción de caché KV adaptativa a escala",
      "submittedOnDailyBy": {
        "_id": "65811eeaa2284a018e51f1ba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
        "isPro": true,
        "fullname": "Zigeng Chen",
        "user": "Zigeng",
        "type": "user"
      },
      "summary": "Modelo Visual Autoregresivo (VAR) ha traído significativas mejoras en eficiencia, escalabilidad y generación zero-shot, lo que ha llevado a una mayor atención. Sin embargo, en los modelos VAR, las técnicas específicas, tanto a gran escala como a detalle, generan un aumento exponencial de la caché KV, lo que implica una consumo significativo de memoria y una extensa cantidad de cálculos. Para enfrentar esta limitación, presentamos ScaleKV, un nuevo marco de trabajo de compresión de caché KV adecuado para el modelo VAR. ScaleKV se ha construido basándose en dos observaciones clave: la demanda de caché diferente entre capas de Transformer y los patrones de atención diferentes en diferentes escalas. Basándose en estos datos, ScaleKV divide las capas de Transformer en dos grupos funcionales: el Desintegrador y el Refinador. El Desintegrador muestra atención distribuida en múltiples escalas y requiere un mayor tamaño de caché. Por otro lado, el Refinador se centra en la mapeo de tokens actuales y procesa detalles locales, lo que requiere un caché reducido. ScaleKV identifica el Desintegrador y el Refinador de acuerdo con la escala y optimiza el flujo de trabajo de inferencia multiescala para realizar una gestión de caché diferenciada en cada escala. En la evaluación del familia de modelos VAR, Infinity, nuestro enfoque muestra que se puede reducir la memoria de caché KV necesaria para mantener la precisión a nivel de píxeles, lo que es un 10% del valor inicial.",
      "upvotes": 9,
      "discussionId": "683523a07309025530c85c45",
      "ai_summary": "ScaleKV compresses the KV cache in Visual Autoregressive models by differentiating drafters and refiners across transformer layers, reducing memory consumption while maintaining high fidelity.",
      "ai_keywords": [
        "Visual Autoregressive",
        "VAR",
        "KV cache",
        "transformer layers",
        "drafters",
        "refiners",
        "memory consumption",
        "Infinity",
        "pixel-level fidelity"
      ]
    },
    "publishedAt": "2025-05-26T03:11:42.000Z",
    "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression",
    "summary": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19602.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65811eeaa2284a018e51f1ba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
      "fullname": "Zigeng Chen",
      "name": "Zigeng",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19590",
      "authors": [
        {
          "_id": "683523bcb0f9c65224abd710",
          "user": {
            "_id": "6275a465597c70eb8949fce5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
            "isPro": false,
            "fullname": "Xuandong Zhao",
            "user": "Xuandong",
            "type": "user"
          },
          "name": "Xuandong Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:51:51.514Z",
          "hidden": false
        },
        {
          "_id": "683523bcb0f9c65224abd711",
          "name": "Zhewei Kang",
          "hidden": false
        },
        {
          "_id": "683523bcb0f9c65224abd712",
          "name": "Aosong Feng",
          "hidden": false
        },
        {
          "_id": "683523bcb0f9c65224abd713",
          "name": "Sergey Levine",
          "hidden": false
        },
        {
          "_id": "683523bcb0f9c65224abd714",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T07:01:06.000Z",
      "submittedOnDailyAt": "2025-05-27T01:00:44.089Z",
      "title": "El motivo de aprender no es basado en la recompensa externa.",
      "submittedOnDailyBy": {
        "_id": "6275a465597c70eb8949fce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
        "isPro": false,
        "fullname": "Xuandong Zhao",
        "user": "Xuandong",
        "type": "user"
      },
      "summary": "Los métodos de aprendizaje por refuerzo (RL) para modelos de lenguaje grandes (LLMs) son efectivos, pero costosos y limitados por la dependencia de estándares de desarrollo específicos. Examinamos un marco de trabajo de aprendizaje por refuerzo con retroalimentación interna (RLIF), que permite a los LLMs aprender de señales internas sin necesidad de datos externos o etiquetados. Proponemos Intuitor, un método de RLIF que utiliza la confianza en sí mismo, llamada \"autoconfianza\", como el único señal de nivel. Intuitor reemplaza el nivel externo con la puntuación de autoconfianza en el aprendizaje de políticas relativas de grupo (GRPO), facilitando un aprendizaje completo sin dependencia de estándares. Los experimentos muestran que Intuitor supera el rendimiento de GRPO en marcos de prueba matemáticos y logra una generalización mejorada en tareas extrañas, como generación de código, sin necesidad de soluciones de alto rendimiento o casos de prueba. Nuestro hallazgo demuestra que señales propias del modelo pueden impulsar un aprendizaje efectivo en el ampliamiento de dominio, y que Intuitor es una alternativa escalable y compatible con RLVR, ofreciendo una solución escalable y compatible en sistemas de inteligencia artificial para proyectos autónomos.",
      "upvotes": 8,
      "discussionId": "683523bcb0f9c65224abd736",
      "githubRepo": "https://github.com/sunblaze-ucb/Intuitor",
      "ai_summary": "Intuitor, a Reinforcement Learning from Internal Feedback method, uses self-certainty as a reward signal to enable unsupervised learning of large language models, achieving performance comparable to GRPO on benchmarks and superior generalization.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "Reinforcement Learning from Internal Feedback",
        "Group Relative Policy Optimization",
        "self-certainty",
        "unsupervised learning"
      ]
    },
    "publishedAt": "2025-05-26T03:01:06.000Z",
    "title": "Learning to Reason without External Rewards",
    "summary": "Training large language models (LLMs) for complex reasoning via Reinforcement\nLearning with Verifiable Rewards (RLVR) is effective but limited by reliance on\ncostly, domain-specific supervision. We explore Reinforcement Learning from\nInternal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic\nsignals without external rewards or labeled data. We propose Intuitor, an RLIF\nmethod that uses a model's own confidence, termed self-certainty, as its sole\nreward signal. Intuitor replaces external rewards in Group Relative Policy\nOptimization (GRPO) with self-certainty scores, enabling fully unsupervised\nlearning. Experiments demonstrate that Intuitor matches GRPO's performance on\nmathematical benchmarks while achieving superior generalization to\nout-of-domain tasks like code generation, without requiring gold solutions or\ntest cases. Our findings show that intrinsic model signals can drive effective\nlearning across domains, offering a scalable alternative to RLVR for autonomous\nAI systems where verifiable rewards are unavailable. Code is available at\nhttps://github.com/sunblaze-ucb/Intuitor",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6275a465597c70eb8949fce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
      "fullname": "Xuandong Zhao",
      "name": "Xuandong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16972",
      "authors": [
        {
          "_id": "68351e269f4e0a0f048ea664",
          "name": "Tianduo Wang",
          "hidden": false
        },
        {
          "_id": "68351e269f4e0a0f048ea665",
          "name": "Lu Xu",
          "hidden": false
        },
        {
          "_id": "68351e269f4e0a0f048ea666",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "68351e269f4e0a0f048ea667",
          "name": "Shanbo Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:51:05.000Z",
      "submittedOnDailyAt": "2025-05-27T00:36:56.992Z",
      "title": "Tensophorus Tensors Toutenso Phoenix: Escalado de Back-Translation para el Reconocimiento de Voces",
      "submittedOnDailyBy": {
        "_id": "6352aa7b6cfb8f149814de5e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666361939036-noauth.jpeg",
        "isPro": false,
        "fullname": "Tianduo Wang",
        "user": "Tianduo",
        "type": "user"
      },
      "summary": "El desarrollo reciente del reconocimiento de lenguaje (ASR) ha sido principalmente impulsado por el uso de grandes corpus de lenguaje. Sin embargo, expandir la cobertura para varios idiomas con recursos limitados es un desafío extremadamente estricto. En este artículo, se presenta un proceso escalable llamado \"Speech Back-Translation\" (SBT) que utiliza modelos de TTS (Text-to-Speech) para convertir grandes corpus de texto en audio sintetizado, mejorando así modelos de ASR multilingües. Demostramos que, al entrenar modelos de TTS con aproximadamente docenas de horas de audio real, se puede aumentar la cantidad de audio sintetizado en varias cientos de veces, manteniendo alta calidad. Se desarrolló un marco de evaluación basado en la lecturabilidad para evaluar la calidad de los audios sintetizados y se definieron etapas claras para determinar cuándo los datos sintetizados pueden ser útiles para el entrenamiento de un modelo de ASR. Usando SBT, se generaron más de 500,000 horas de audio sintetizado en 10 idiomas, lo que, al entrenar Whisper-large-v3 adicionalmente, resultó en una reducción del 30% o más en el error de lectura promedio. Estos resultados claramente demuestran que SBT demostra la escalabilidad y eficiencia en la mejora de sistemas de ASR multilingües.",
      "upvotes": 8,
      "discussionId": "68351e279f4e0a0f048ea689",
      "githubRepo": "https://github.com/TianduoWang/Speech-BT",
      "ai_summary": "Speech Back-Translation enhances multilingual ASR systems by generating high-quality synthetic speech from text corpora, significantly reducing transcription errors.",
      "ai_keywords": [
        "Automatic Speech Recognition",
        "Speech Back-Translation",
        "multilingual ASR",
        "text-to-speech",
        "synthetic speech",
        "intelligibility-based assessment",
        "Whisper-large-v3",
        "transcription error reduction"
      ]
    },
    "publishedAt": "2025-05-22T13:51:05.000Z",
    "title": "From Tens of Hours to Tens of Thousands: Scaling Back-Translation for\n  Speech Recognition",
    "summary": "Recent advances in Automatic Speech Recognition (ASR) have been largely\nfueled by massive speech corpora. However, extending coverage to diverse\nlanguages with limited resources remains a formidable challenge. This paper\nintroduces Speech Back-Translation, a scalable pipeline that improves\nmultilingual ASR models by converting large-scale text corpora into synthetic\nspeech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just\ntens of hours of real transcribed speech can effectively train TTS models to\ngenerate synthetic speech at hundreds of times the original volume while\nmaintaining high quality. To evaluate synthetic speech quality, we develop an\nintelligibility-based assessment framework and establish clear thresholds for\nwhen synthetic data benefits ASR training. Using Speech Back-Translation, we\ngenerate more than 500,000 hours of synthetic speech in ten languages and\ncontinue pre-training Whisper-large-v3, achieving average transcription error\nreductions of over 30\\%. These results highlight the scalability and\neffectiveness of Speech Back-Translation for enhancing multilingual ASR\nsystems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16972.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6352aa7b6cfb8f149814de5e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666361939036-noauth.jpeg",
      "fullname": "Tianduo Wang",
      "name": "Tianduo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13426",
      "authors": [
        {
          "_id": "682c641925f124206513d14d",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "682c641925f124206513d14e",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "682c641925f124206513d14f",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "682c641925f124206513d150",
          "name": "Zhiqi Huang",
          "hidden": false
        },
        {
          "_id": "682c641925f124206513d151",
          "name": "Flood Sung",
          "hidden": false
        },
        {
          "_id": "682c641925f124206513d152",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "682c641925f124206513d153",
          "name": "Yuxin Wu",
          "hidden": false
        },
        {
          "_id": "682c641925f124206513d154",
          "name": "Baobao Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T17:54:39.000Z",
      "submittedOnDailyAt": "2025-05-27T00:09:52.653Z",
      "title": "G1: El aprendizaje inicia a fortalecer las habilidades de reconocimiento y inferencia del modelo de lenguaje visual.",
      "submittedOnDailyBy": {
        "_id": "61b0a4ce1b3d95b3d1ed9251",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Wwjr26vdudX5KYVTb8Q0a.png",
        "isPro": false,
        "fullname": "Liang Chen",
        "user": "leonardPKU",
        "type": "user"
      },
      "summary": "Visión-Lenguaje Modelos (VLMs) muestran excelentes resultados en diversas tareas multimodal directas, pero en entornos interactivos y visualmente ricos, encuentran dificultades para tomar decisiones efectivamente. Esta diferencia significa que el potencial de los agentes autónomos de conducción se ve notablemente limitado. Los líderes VLMs también fallan en simples juegos. Por otro lado, se presenta VLM-Gym. VLM-Gym es un entorno de aprendizaje por refuerzo (RL) que adopta una interfaz unificada y dificultades ajustables y combinables en varios juegos visuales. Usando VLM-Gym, el modelo G0 se entrena mediante una autoevolución dirigida por RL, demostrando conocimientos visuales sorprendentes y patrones lógicos. Para mitigar los problemas generados por la diversidad de los juegos, se desarrolla el modelo G1. G1 adopta un estado inicial fortalecido con conocimientos visuales antes de la configuración de RL. Como resultado, el modelo G1 supera a todos los juegos y excede modelos líderes como Claude-3.7-Sonnet-Thinking. Se han descubierto interesantes hallazgos en un análisis sistemático. La capacidad de conocimientos visuales y lógica se refuerzan mutuamente durante el proceso de entrenamiento de RL. El código fuente que incluye VLM-Gym y el entrenamiento de RL está disponible en https://github.com/chenllliang/G1, y conecta con futuras investigaciones que transformen a los VLMs en agentes interactivos eficientes.",
      "upvotes": 7,
      "discussionId": "682c641a25f124206513d1d5",
      "githubRepo": "https://github.com/chenllliang/G1",
      "ai_summary": "VLM-Gym addresses the \"knowing-doing\" gap in Vision-Language Models by training them in a diverse RL environment, leading to enhanced perception and reasoning abilities that surpass existing models in interactive games.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLM-Gym",
        "reinforcement learning",
        "RL",
        "G0 models",
        "self-evolution",
        "G1 models",
        "perception-enhanced cold start",
        "RL fine-tuning"
      ]
    },
    "publishedAt": "2025-05-19T13:54:39.000Z",
    "title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language\n  Model via Reinforcement Learning",
    "summary": "Vision-Language Models (VLMs) excel in many direct multimodal tasks but\nstruggle to translate this prowess into effective decision-making within\ninteractive, visually rich environments like games. This ``knowing-doing'' gap\nsignificantly limits their potential as autonomous agents, as leading VLMs\noften performing badly in simple games. To address this, we introduce VLM-Gym,\na curated reinforcement learning (RL) environment featuring diverse visual\ngames with unified interfaces and adjustable, compositional difficulty,\nspecifically designed for scalable multi-game parallel training. Leveraging\nVLM-Gym, we train G0 models using pure RL-driven self-evolution, which\ndemonstrate emergent perception and reasoning patterns. To further mitigate\nchallenges arising from game diversity, we develop G1 models. G1 incorporates a\nperception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models\nconsistently surpass their teacher across all games and outperform leading\nproprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals\nan intriguing finding: perception and reasoning abilities mutually bootstrap\neach other throughout the RL training process. Source code including VLM-Gym\nand RL training are released at https://github.com/chenllliang/G1 to foster\nfuture research in advancing VLMs as capable interactive agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13426.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b0a4ce1b3d95b3d1ed9251",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Wwjr26vdudX5KYVTb8Q0a.png",
      "fullname": "Liang Chen",
      "name": "leonardPKU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19731",
      "authors": [
        {
          "_id": "683588a1650d51732cab05de",
          "user": {
            "_id": "6262880c5eb4fa93219f0064",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6262880c5eb4fa93219f0064/6yyBvRK4Oh7OhjaaweaVN.jpeg",
            "isPro": false,
            "fullname": "Daniil Tiapkin",
            "user": "dtiapkin",
            "type": "user"
          },
          "name": "Daniil Tiapkin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T10:03:29.430Z",
          "hidden": false
        },
        {
          "_id": "683588a1650d51732cab05df",
          "name": "Daniele Calandriello",
          "hidden": false
        },
        {
          "_id": "683588a1650d51732cab05e0",
          "name": "Denis Belomestny",
          "hidden": false
        },
        {
          "_id": "683588a1650d51732cab05e1",
          "name": "Eric Moulines",
          "hidden": false
        },
        {
          "_id": "683588a1650d51732cab05e2",
          "name": "Alexey Naumov",
          "hidden": false
        },
        {
          "_id": "683588a1650d51732cab05e3",
          "user": {
            "_id": "629f3b18ee05727ce328ccbe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669189789447-629f3b18ee05727ce328ccbe.jpeg",
            "isPro": false,
            "fullname": "Kashif Rasul",
            "user": "kashif",
            "type": "user"
          },
          "name": "Kashif Rasul",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-27T09:40:50.326Z",
          "hidden": false
        },
        {
          "_id": "683588a1650d51732cab05e4",
          "user": {
            "_id": "651e97156d92456bdf5ace6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e97156d92456bdf5ace6b/KKfdZGPAcWPdqycp9SulH.jpeg",
            "isPro": false,
            "fullname": "Michal Valko",
            "user": "misovalko",
            "type": "user"
          },
          "name": "Michal Valko",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-27T10:00:09.731Z",
          "hidden": false
        },
        {
          "_id": "683588a1650d51732cab05e5",
          "name": "Pierre Menard",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6262880c5eb4fa93219f0064/F7nWvm2sO5QXTLRnB1k6e.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6262880c5eb4fa93219f0064/Fuzzt-cOiMSOCevLVjkW-.png"
      ],
      "publishedAt": "2025-05-26T09:17:32.000Z",
      "submittedOnDailyAt": "2025-05-27T08:13:23.799Z",
      "title": "Mira proximos para acelerar el aprendizaje naturalmente a través de retroalimentación humana",
      "submittedOnDailyBy": {
        "_id": "6262880c5eb4fa93219f0064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6262880c5eb4fa93219f0064/6yyBvRK4Oh7OhjaaweaVN.jpeg",
        "isPro": false,
        "fullname": "Daniil Tiapkin",
        "user": "dtiapkin",
        "type": "user"
      },
      "summary": "La aprendizaje de valores mediante aprendizaje por refuerzo (RLHF) se centra en modelos de recompensa, asumiendo estructuras de preferencias como el modelo de Brady-Terry, pero es difícil precisar la compleja preferencia real que tienen los humanos (por ejemplo, no transmisible). El aprendizaje de valores basado en juegos de Nash (NLHF) aborda el equilibrio de juegos de preferencias complejas y resuelve esos problemas directamente. En este estudio, se presenta un algoritmo de aprendizaje en línea basado en Nash-MP utilizando el algoritmo de optimización de Nash-MP. La análisis teórico muestra que Nash-MP converge linealmente a la equilibrio de Nash beta-normalizado. Específicamente, la varianza KL respecto a la política óptima disminuye a una velocidad de (1+2beta)^{-N/2}. Además, se muestra una convergencia lineal uniforme en el error de cálculo y en el espanó de logaritmos de probabilidades, sin depender del tamaño del espacio de acciones. Se propone una versión aproximada de Nash-MP evaluada por métodos de gradiente descendente y se realizan pruebas de aplicación. Finalmente, se describen estrategias prácticas para la fine-tuning de modelos de lenguaje grandes y se realizan experimentos que demostraron la compatibilidad con métodos existentes y un rendimiento fuerte.",
      "upvotes": 5,
      "discussionId": "683588a2650d51732cab0612",
      "ai_summary": "Nash Mirror Prox is an online algorithm for Nash Learning from Human Feedback that achieves linear convergence to the Nash equilibrium and is applicable for fine-tuning language models.",
      "ai_keywords": [
        "Nash Learning from Human Feedback",
        "Nash Mirror Prox",
        "Mirror Prox",
        "KL-divergence",
        "Nash equilibrium",
        "exploitability gap",
        "span semi-norm",
        "log-probabilities",
        "stochastic policy gradients",
        "fine-tuning",
        "large language models"
      ]
    },
    "publishedAt": "2025-05-26T05:17:32.000Z",
    "title": "Accelerating Nash Learning from Human Feedback via Mirror Prox",
    "summary": "Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on\nreward models, frequently assuming preference structures like the Bradley-Terry\nmodel, which may not accurately capture the complexities of real human\npreferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF)\noffers a more direct alternative by framing the problem as finding a Nash\nequilibrium of a game defined by these preferences. In this work, we introduce\nNash Mirror Prox (Nash-MP), an online NLHF algorithm that leverages\nthe Mirror Prox optimization scheme to achieve fast and stable convergence to\nthe Nash equilibrium. Our theoretical analysis establishes that Nash-MP\nexhibits last-iterate linear convergence towards the beta-regularized Nash\nequilibrium. Specifically, we prove that the KL-divergence to the optimal\npolicy decreases at a rate of order (1+2beta)^{-N/2}, where N is a number\nof preference queries. We further demonstrate last-iterate linear convergence\nfor the exploitability gap and uniformly for the span semi-norm of\nlog-probabilities, with all these rates being independent of the size of the\naction space. Furthermore, we propose and analyze an approximate version of\nNash-MP where proximal steps are estimated using stochastic policy gradients,\nmaking the algorithm closer to applications. Finally, we detail a practical\nimplementation strategy for fine-tuning large language models and present\nexperiments that demonstrate its competitive performance and compatibility with\nexisting methods.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6262880c5eb4fa93219f0064/F7nWvm2sO5QXTLRnB1k6e.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6262880c5eb4fa93219f0064/Fuzzt-cOiMSOCevLVjkW-.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19731.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6262880c5eb4fa93219f0064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6262880c5eb4fa93219f0064/6yyBvRK4Oh7OhjaaweaVN.jpeg",
      "fullname": "Daniil Tiapkin",
      "name": "dtiapkin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19640",
      "authors": [
        {
          "_id": "68353e3f9f4e0a0f0496d0c6",
          "name": "Roy Xie",
          "hidden": false
        },
        {
          "_id": "68353e3f9f4e0a0f0496d0c7",
          "name": "David Qiu",
          "hidden": false
        },
        {
          "_id": "68353e3f9f4e0a0f0496d0c8",
          "name": "Deepak Gopinath",
          "hidden": false
        },
        {
          "_id": "68353e3f9f4e0a0f0496d0c9",
          "name": "Dong Lin",
          "hidden": false
        },
        {
          "_id": "68353e3f9f4e0a0f0496d0ca",
          "name": "Yanchao Sun",
          "hidden": false
        },
        {
          "_id": "68353e3f9f4e0a0f0496d0cb",
          "name": "Chong Wang",
          "hidden": false
        },
        {
          "_id": "68353e3f9f4e0a0f0496d0cc",
          "name": "Saloni Potdar",
          "hidden": false
        },
        {
          "_id": "68353e3f9f4e0a0f0496d0cd",
          "name": "Bhuwan Dhingra",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T07:58:17.000Z",
      "submittedOnDailyAt": "2025-05-27T02:56:39.316Z",
      "title": "Intralíquido Residencia de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesg",
      "submittedOnDailyBy": {
        "_id": "6555a124a6554059711b58a2",
        "avatarUrl": "/avatars/222bb6b8f252d6c2bbd4cf35a54fc1c9.svg",
        "isPro": false,
        "fullname": "Roy",
        "user": "RRoy233",
        "type": "user"
      },
      "summary": "La teoría de la lógica continua (CoT) mejora significativamente la capacidad de inferencia de los modelos de lenguaje grandes (LLM). Sin embargo, la teoría de la lógica continua aumenta la ineficiencia y el tiempo de respuesta inicial (TTFT), lo que afecta la eficiencia. Proponemos un nuevo enfoque de aprendizaje para modelos de lógica que combina la teoría de la lógica continua y la respuesta mediante aprendizaje por refuerzo (RL). Observamos que los modelos pueden realizar lógica de manera cruzada y confirmamos que pueden mejorarse aún más. Introducimos una recompensa basada en reglas sencilla y efectiva para guiar el modelo de política hacia una ruta lógica precisa utilizando señales intermedias generadas por lógica cruzada. Una investigación extensa con cinco conjuntos de datos diferentes y tres algoritmos de aprendizaje por refuerzo (PPO, GRPO, REINFORCE++) muestra un mejoramiento uniforme en la teoría de la lógica y la respuesta, sin necesidad de herramientas externas. Específicamente, nuestro enfoque reduce en promedio el TTFT en más del 80% y mejora la precisión de Pass@1 en 19.3%. Además, nuestro método muestra una fuerte capacidad de generalización en conjuntos de datos complejos de lógica (por ejemplo, MATH, GPQA, MMLU), ya que se ha entrenado solo en conjuntos de datos de respuesta y teoría de la lógica. Además, realizamos un análisis detallado sobre la modelización de recompensas condicionales y proporcionamos muchos feedbacks valiosos.",
      "upvotes": 5,
      "discussionId": "68353e409f4e0a0f0496d0fb",
      "ai_summary": "A reinforcement learning-guided training paradigm enhances large language models' reasoning efficiency and performance for multi-hop questions by interleaving thinking and answering.",
      "ai_keywords": [
        "chain-of-thought",
        "large language models",
        "reasoning capabilities",
        "time-to-first-token",
        "reinforcement learning",
        "interleaved reasoning",
        "rule-based reward",
        "reward modeling",
        "multi-hop questions",
        "think-answer reasoning",
        "Pass@1 accuracy",
        "MATH",
        "GPQA",
        "MMLU",
        "PPO",
        "GRPO",
        "REINFORCE++"
      ]
    },
    "publishedAt": "2025-05-26T03:58:17.000Z",
    "title": "Interleaved Reasoning for Large Language Models via Reinforcement\n  Learning",
    "summary": "Long chain-of-thought (CoT) significantly enhances large language models'\n(LLM) reasoning capabilities. However, the extensive reasoning traces lead to\ninefficiencies and an increased time-to-first-token (TTFT). We propose a novel\ntraining paradigm that uses reinforcement learning (RL) to guide reasoning LLMs\nto interleave thinking and answering for multi-hop questions. We observe that\nmodels inherently possess the ability to perform interleaved reasoning, which\ncan be further enhanced through RL. We introduce a simple yet effective\nrule-based reward to incentivize correct intermediate steps, which guides the\npolicy model toward correct reasoning paths by leveraging intermediate signals\ngenerated during interleaved reasoning. Extensive experiments conducted across\nfive diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)\ndemonstrate consistent improvements over traditional think-answer reasoning,\nwithout requiring external tools. Specifically, our approach reduces TTFT by\nover 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,\nour method, trained solely on question answering and logical reasoning\ndatasets, exhibits strong generalization ability to complex reasoning datasets\nsuch as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to\nreveal several valuable insights into conditional reward modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19640.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6555a124a6554059711b58a2",
      "avatarUrl": "/avatars/222bb6b8f252d6c2bbd4cf35a54fc1c9.svg",
      "fullname": "Roy",
      "name": "RRoy233",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19427",
      "authors": [
        {
          "_id": "683525ac1c31d709ba52273e",
          "name": "Sihan Chen",
          "hidden": false
        },
        {
          "_id": "683525ac1c31d709ba52273f",
          "name": "Dan Zhao",
          "hidden": false
        },
        {
          "_id": "683525ac1c31d709ba522740",
          "name": "Jongwoo Ko",
          "hidden": false
        },
        {
          "_id": "683525ac1c31d709ba522741",
          "name": "Colby Banbury",
          "hidden": false
        },
        {
          "_id": "683525ac1c31d709ba522742",
          "name": "Huiping Zhuang",
          "hidden": false
        },
        {
          "_id": "683525ac1c31d709ba522743",
          "name": "Luming Liang",
          "hidden": false
        },
        {
          "_id": "683525ac1c31d709ba522744",
          "user": {
            "_id": "64ad94f05a4a60156925ec96",
            "avatarUrl": "/avatars/643bdb076e703bfcc89cec6fccb756c6.svg",
            "isPro": false,
            "fullname": "Tianyi Chen",
            "user": "tianyic",
            "type": "user"
          },
          "name": "Tianyi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:51:46.088Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T02:37:32.000Z",
      "submittedOnDailyAt": "2025-05-27T01:10:19.909Z",
      "title": "WINA: Mejora de la velocidad de inferencia de modelos de lenguaje grandes basada en la activación de neuronas con información de pesos",
      "submittedOnDailyBy": {
        "_id": "64ad94f05a4a60156925ec96",
        "avatarUrl": "/avatars/643bdb076e703bfcc89cec6fccb756c6.svg",
        "isPro": false,
        "fullname": "Tianyi Chen",
        "user": "tianyic",
        "type": "user"
      },
      "summary": "Con el aumento del carga computacional de los modelos de lenguaje grandes (LLMs), la eficiencia en la inferencia y la estrategia de activación ha adquirido importancia. En los últimos métodos, se utilizan estrategias de activación selectiva como Mixture-of-Experts (MoE), pero estos requieren entrenamiento especial. Los métodos de activación rara, sin necesidad de entrenamiento, pueden ser ampliamente aplicados a través de diseños de plug-and-play y tienen una alta eficiencia en recursos. Sin embargo, la mayoría de los métodos actuales deciden la activación solo basándose en el tamaño de los estados ocultos, lo que lleva a altos errores de aproximación y precisión de inferencia insuficiente. Para resolver estas limitaciones, proponemos un nuevo marco de activación rara sin necesidad de entrenamiento llamado Weight Informed Neuron Activation (WINA). Este método considera tanto el tamaño de los estados ocultos como la norma ell_2 de las columnas de la matriz de pesos. De esta manera, se puede lograr una estrategia de rara activación con garantías teóricas que proporcione un límite superior más cercano a la mejor aproximación que la tecnología actual. Experimentalmente, WINA supera los métodos más avanzados (como TEAL) en el mismo nivel de rara activación y mejora la eficiencia promedio en un 2.94% o más. Estos resultados demuestran que WINA establece una nueva frontera de rendimiento para la activación rara sin necesidad de entrenamiento en la inferencia de LLMs, y establece un nuevo estándar para el desarrollo de métodos raros sin entrenamiento y la eficiencia en la inferencia. El código fuente está disponible en https://github.com/microsoft/wina.",
      "upvotes": 5,
      "discussionId": "683525ac1c31d709ba52277c",
      "ai_summary": "WINA, a training-free sparse activation framework for large language models, improves inference accuracy by considering hidden state magnitudes and weight matrix norms, outperforming existing methods.",
      "ai_keywords": [
        "Mixture-of-Experts (MoE)",
        "sparse activation",
        "hidden state magnitudes",
        "column-wise $\\ell_2$-norms",
        "weight matrices",
        "sparsification strategy",
        "approximation error bounds",
        "training-free sparse activation",
        "large language models"
      ]
    },
    "publishedAt": "2025-05-25T22:37:32.000Z",
    "title": "WINA: Weight Informed Neuron Activation for Accelerating Large Language\n  Model Inference",
    "summary": "The growing computational demands of large language models (LLMs) make\nefficient inference and activation strategies increasingly critical. While\nrecent approaches, such as Mixture-of-Experts (MoE), leverage selective\nactivation but require specialized training, training-free sparse activation\nmethods offer broader applicability and superior resource efficiency through\ntheir plug-and-play design. However, many existing methods rely solely on\nhidden state magnitudes to determine activation, resulting in high\napproximation errors and suboptimal inference accuracy. To address these\nlimitations, we propose WINA (Weight Informed Neuron Activation), a novel,\nsimple, and training-free sparse activation framework that jointly considers\nhidden state magnitudes and the column-wise ell_2-norms of weight matrices.\nWe show that this leads to a sparsification strategy that obtains optimal\napproximation error bounds with theoretical guarantees tighter than existing\ntechniques. Empirically, WINA also outperforms state-of-the-art methods (e.g.,\nTEAL) by up to 2.94% in average performance at the same sparsity levels,\nacross a diverse set of LLM architectures and datasets. These results position\nWINA as a new performance frontier for training-free sparse activation in LLM\ninference, advancing training-free sparse activation methods and setting a\nrobust baseline for efficient inference. The source code is available at\nhttps://github.com/microsoft/wina.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ad94f05a4a60156925ec96",
      "avatarUrl": "/avatars/643bdb076e703bfcc89cec6fccb756c6.svg",
      "fullname": "Tianyi Chen",
      "name": "tianyic",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18822",
      "authors": [
        {
          "_id": "683528d8c682e155a8b9a80f",
          "user": {
            "_id": "64ce05c631c655ff8a2e183c",
            "avatarUrl": "/avatars/f2de7f8a1348b05f46946085e3e9718e.svg",
            "isPro": false,
            "fullname": "Shijue Huang",
            "user": "JoeYing",
            "type": "user"
          },
          "name": "Shijue Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T10:03:37.508Z",
          "hidden": false
        },
        {
          "_id": "683528d8c682e155a8b9a810",
          "name": "Hongru Wang",
          "hidden": false
        },
        {
          "_id": "683528d8c682e155a8b9a811",
          "name": "Wanjun Zhong",
          "hidden": false
        },
        {
          "_id": "683528d8c682e155a8b9a812",
          "name": "Zhaochen Su",
          "hidden": false
        },
        {
          "_id": "683528d8c682e155a8b9a813",
          "name": "Jiazhan Feng",
          "hidden": false
        },
        {
          "_id": "683528d8c682e155a8b9a814",
          "name": "Bowen Cao",
          "hidden": false
        },
        {
          "_id": "683528d8c682e155a8b9a815",
          "name": "Yi R. Fung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T18:46:50.000Z",
      "submittedOnDailyAt": "2025-05-27T07:17:54.612Z",
      "title": "AdaCtrl: Adaptación a la Dificultad a través de la Teoría de Control y Adaptación de Buzz Dining",
      "submittedOnDailyBy": {
        "_id": "64ce05c631c655ff8a2e183c",
        "avatarUrl": "/avatars/f2de7f8a1348b05f46946085e3e9718e.svg",
        "isPro": false,
        "fullname": "Shijue Huang",
        "user": "JoeYing",
        "type": "user"
      },
      "summary": "Los modelos lógicos de gran escala modernos muestran la capacidad de resolver problemas impresionantes utilizando estrategias lógicas complejas. Sin embargo, a menudo, presentan dificultades para equilibrar eficiencia y efectividad, generando largas cadenas de razonamiento incluso para problemas sencillos. En este artículo, se propone un nuevo marco llamado AdaCtrl. Este marco apoya la balanza cognitiva de la dificultad lógica y permite que el profundo análisis lógico sea controlado explícitamente por el usuario. AdaCtrl ajusta la longitud de la razón lógica de manera dinámica según la dificultad del problema, permitiendo al usuario controlar la balanza lógica directamente para elegir entre eficiencia y efectividad. Este método se implementa mediante una piperínea de entrenamiento en dos etapas: la primera se centra en la micro-ajuste inicial, donde se entrena la capacidad de ajuste de dificultad cognitiva y la balanza lógica, y la segunda en la etapa de aprendizaje por refuerzo (RL), donde se refina la estrategia lógica adaptativa del modelo y se ajusta la evaluación de dificultad del usuario. Para fomentar un modo interactivo intuitivo del usuario, se diseñan etiquetas explícitas basadas en longitud. Los resultados de los experimentos muestran que AdaCtrl cambia la longitud de la razón lógica según la dificultad evaluada, mejorando el rendimiento comparado con líneas de entrenamiento basadas en estándares, reduciendo en 10.06% y 12.14% la longitud de las respuestas en los conjuntos de datos AIME2024 y AIME2025, respectivamente, y en 62.05% y 91.04% en los conjuntos de datos MATH500 y GSM8K. Además, AdaCtrl permite un control preciso de la balanza lógica por parte del usuario y proporciona respuestas adaptadas a las necesidades específicas.",
      "upvotes": 5,
      "discussionId": "683528d9c682e155a8b9a852",
      "ai_summary": "AdaCtrl, a novel framework, dynamically adjusts reasoning length based on problem difficulty and user control, improving performance and reducing response length across various datasets compared to standard training methods.",
      "ai_keywords": [
        "AdaCtrl",
        "adaptive reasoning budget allocation",
        "self-assessed problem difficulty",
        "difficulty-aware reinforcement learning",
        "two-stage training pipeline",
        "cold-start fine-tuning"
      ]
    },
    "publishedAt": "2025-05-24T14:46:50.000Z",
    "title": "AdaCtrl: Towards Adaptive and Controllable Reasoning via\n  Difficulty-Aware Budgeting",
    "summary": "Modern large reasoning models demonstrate impressive problem-solving\ncapabilities by employing sophisticated reasoning strategies. However, they\noften struggle to balance efficiency and effectiveness, frequently generating\nunnecessarily lengthy reasoning chains for simple problems. In this work, we\npropose AdaCtrl, a novel framework to support both difficulty-aware adaptive\nreasoning budget allocation and explicit user control over reasoning depth.\nAdaCtrl dynamically adjusts its reasoning length based on self-assessed problem\ndifficulty, while also allowing users to manually control the budget to\nprioritize either efficiency or effectiveness. This is achieved through a\ntwo-stage training pipeline: an initial cold-start fine-tuning phase to instill\nthe ability to self-aware difficulty and adjust reasoning budget, followed by a\ndifficulty-aware reinforcement learning (RL) stage that refines the model's\nadaptive reasoning strategies and calibrates its difficulty assessments based\non its evolving capabilities during online training. To enable intuitive user\ninteraction, we design explicit length-triggered tags that function as a\nnatural interface for budget control. Empirical results show that AdaCtrl\nadapts reasoning length based on estimated difficulty, compared to the standard\ntraining baseline that also incorporates fine-tuning and RL, it yields\nperformance improvements and simultaneously reduces response length by 10.06%\nand 12.14% on the more challenging AIME2024 and AIME2025 datasets, which\nrequire elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K\ndatasets, where more concise responses are sufficient. Furthermore, AdaCtrl\nenables precise user control over the reasoning budget, allowing for tailored\nresponses to meet specific needs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18822.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ce05c631c655ff8a2e183c",
      "avatarUrl": "/avatars/f2de7f8a1348b05f46946085e3e9718e.svg",
      "fullname": "Shijue Huang",
      "name": "JoeYing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20278",
      "authors": [
        {
          "_id": "68353261bc28496925a185c9",
          "name": "Hoyeon Chang",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185ca",
          "name": "Jinho Park",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185cb",
          "name": "Hanseul Cho",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185cc",
          "name": "Sohee Yang",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185cd",
          "name": "Miyoung Ko",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185ce",
          "name": "Hyeonbin Hwang",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185cf",
          "name": "Seungpil Won",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185d0",
          "name": "Dohaeng Lee",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185d1",
          "name": "Youbin Ahn",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185d2",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T17:55:15.000Z",
      "submittedOnDailyAt": "2025-05-27T02:04:40.615Z",
      "title": "El Principio de Caballejo: Marco de Comprensión de la Escalabilidad Composita",
      "submittedOnDailyBy": {
        "_id": "64d0d6684dfd5df70744b237",
        "avatarUrl": "/avatars/4ea57bfd407e8cb727c624f64af75478.svg",
        "isPro": false,
        "fullname": "Chang",
        "user": "Hoyeon",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje general son especializados en el matching de patrones, mientras que tienen una debilidad en la generalización estructural sistemática. Proponemos el principio de cobertura: en un marco de datos, los modelos que dependen principalmente del matching de patrones pueden fracasar en la generalización confiable al no reemplazar el marco que proporciona los mismos resultados en el mismo contexto. Demostramos que este marco fortalece la capacidad de generalización de los transformers. Primero, el tamaño de la colección de tokens necesario para la generalización en la segunda etapa aumenta al cuadrado mínimo, y aunque se escalan los parámetros en 20 veces, la eficiencia de los datos no se mejora. Además, en tareas estructurales donde hay caminos inciertos, una variable influye en el resultado a través de múltiples caminos de cálculo. Los transformers aprenden representaciones de estado contextuales que destruyen la intercambiabilidad y la eficiencia de ambos lados. Tercero, los subobjetos de la \"Coherence of Thought\" mejoran la eficiencia de los datos en tareas de múltiples pasos, pero enfrentan dificultades con la incertidumbre de los caminos. Finalmente, explicamos el algoritmo basado en estructura y distinguimos tres métodos de generalización de las redes neuronales: estructural (limitado por la cobertura), atributo-basado (utilizando la invarianza de algoritmos) y operadores compartidos (reutilización de funciones). En este calderón conceptual, nuestros resultados contextualizan los resultados y revelan la necesidad de una nueva arquitectura. En resumen, el principio de cobertura proporciona una visión integral para entender las razones estructurales, lo que requiere innovaciones fundamentales en la arquitectura o en el entrenamiento.",
      "upvotes": 4,
      "discussionId": "68353261bc28496925a185ef",
      "ai_summary": "The coverage principle highlights limitations in Transformers' compositional generalization, emphasizing the need for new architectures or training methods to achieve systematic compositionality by distinguishing different mechanisms of generalization.",
      "ai_keywords": [
        "coverage principle",
        "data-centric framework",
        "sequential application",
        "pattern matching",
        "compositional generalization",
        "Transformers",
        "two-hop generalization",
        "token set size",
        "training data efficiency",
        "context-dependent state representations",
        "performance",
        "interoperability",
        "Chain-of-Thought supervision",
        "multi-hop tasks",
        "path ambiguity",
        "structure-based",
        "property-based",
        "shared-operator",
        "architectural innovations"
      ]
    },
    "publishedAt": "2025-05-26T13:55:15.000Z",
    "title": "The Coverage Principle: A Framework for Understanding Compositional\n  Generalization",
    "summary": "Large language models excel at pattern matching, yet often fall short in\nsystematic compositional generalization. We propose the coverage principle: a\ndata-centric framework showing that models relying primarily on pattern\nmatching for compositional tasks cannot reliably generalize beyond substituting\nfragments that yield identical results when used in the same contexts. We\ndemonstrate that this framework has a strong predictive power for the\ngeneralization capabilities of Transformers. First, we derive and empirically\nconfirm that the training data required for two-hop generalization grows at\nleast quadratically with the token set size, and the training data efficiency\ndoes not improve with 20x parameter scaling. Second, for compositional tasks\nwith path ambiguity where one variable affects the output through multiple\ncomputational paths, we show that Transformers learn context-dependent state\nrepresentations that undermine both performance and interoperability. Third,\nChain-of-Thought supervision improves training data efficiency for multi-hop\ntasks but still struggles with path ambiguity. Finally, we outline a\nmechanism-based taxonomy that distinguishes three ways neural networks\ncan generalize: structure-based (bounded by coverage), property-based\n(leveraging algebraic invariances), and shared-operator (through function\nreuse). This conceptual lens contextualizes our results and highlights where\nnew architectural ideas are needed to achieve systematic compositionally.\nOverall, the coverage principle provides a unified lens for understanding\ncompositional reasoning, and underscores the need for fundamental architectural\nor training innovations to achieve truly systematic compositionality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d0d6684dfd5df70744b237",
      "avatarUrl": "/avatars/4ea57bfd407e8cb727c624f64af75478.svg",
      "fullname": "Chang",
      "name": "Hoyeon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20254",
      "authors": [
        {
          "_id": "683528109f968fc5c604495f",
          "user": {
            "_id": "5f12485c0c833276f61f1afb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1595033594228-noauth.jpeg",
            "isPro": false,
            "fullname": "Xiangchen Song",
            "user": "xiangchensong",
            "type": "user"
          },
          "name": "Xiangchen Song",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-27T02:48:53.929Z",
          "hidden": false
        },
        {
          "_id": "683528109f968fc5c6044960",
          "user": {
            "_id": "64755a83e0b188d3cb2579d8",
            "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
            "isPro": false,
            "fullname": "Aashiq Muhamed",
            "user": "aashiqmuhamed",
            "type": "user"
          },
          "name": "Aashiq Muhamed",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:51:25.328Z",
          "hidden": false
        },
        {
          "_id": "683528109f968fc5c6044961",
          "name": "Yujia Zheng",
          "hidden": false
        },
        {
          "_id": "683528109f968fc5c6044962",
          "name": "Lingjing Kong",
          "hidden": false
        },
        {
          "_id": "683528109f968fc5c6044963",
          "name": "Zeyu Tang",
          "hidden": false
        },
        {
          "_id": "683528109f968fc5c6044964",
          "name": "Mona T. Diab",
          "hidden": false
        },
        {
          "_id": "683528109f968fc5c6044965",
          "name": "Virginia Smith",
          "hidden": false
        },
        {
          "_id": "683528109f968fc5c6044966",
          "name": "Kun Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T17:31:36.000Z",
      "submittedOnDailyAt": "2025-05-27T01:20:40.055Z",
      "title": "La descripción del dispositivo debe priorizar la coincidencia de los vectores de características de SAEs.",
      "submittedOnDailyBy": {
        "_id": "64755a83e0b188d3cb2579d8",
        "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
        "isPro": false,
        "fullname": "Aashiq Muhamed",
        "user": "aashiqmuhamed",
        "type": "user"
      },
      "summary": "Los Autoencoders Esparsos (AEs) son una herramienta importante en la explicabilidad mecánica (MI), permitiendo la decomposición de la activación de la red neuronal en características interpretables. Sin embargo, se ha confirmado que las características aprendidas por un AE son discontinuas en otros conjuntos de entrenamiento, lo que afecta la confianza y eficiencia de la investigación en MI. En este artículo, se argumenta que la explicabilidad mecánica debe priorizar la consistencia de las características de un AE, con el objetivo de lograr una convergencia confiable de conjuntos de características equivalentes en entrenamientos independientes. Aquí se propone el uso práctico del Coeficiente de Correlación de la Media de la Diccionario Parcial (PW-MCC) como métrica, demostrando que se puede alcanzar un alto nivel de consistencia (0.80 en los AEs de TopK de activaciones de un LLM). Nuestro contribución destaca los beneficios de priorizar la consistencia y proporciona una base teórica y una validación sintética mediante la organización del modelo, probando la confiabilidad de PW-MCC como proxy para la recuperación de valores reales. Además, se extiende estos hallazgos a datos de un LLM de gran escala, mostrando que una alta consistencia de las características tiene una fuerte asociación con la semejanza significativa de la interpretación de las características aprendidas. Se anima a la comunidad a medir sistemáticamente la consistencia de las características y promover un desarrollo sólido y acumulativo de la MI.",
      "upvotes": 4,
      "discussionId": "683528159f968fc5c6044aff",
      "githubRepo": "https://github.com/xiangchensong/sae-feature-consistency",
      "ai_summary": "Prioritizing feature consistency in sparse autoencoders improves mechanistic interpretability of neural networks by ensuring reliable and interpretable features.",
      "ai_keywords": [
        "Sparse Autoencoders (SAEs)",
        "mechanistic interpretability (MI)",
        "feature consistency",
        "Pairwise Dictionary Mean Correlation Coefficient (PW-MCC)",
        "TopK SAEs",
        "LLM activations",
        "synthetic validation",
        "semantic similarity",
        "learned feature explanations"
      ]
    },
    "publishedAt": "2025-05-26T13:31:36.000Z",
    "title": "Position: Mechanistic Interpretability Should Prioritize Feature\n  Consistency in SAEs",
    "summary": "Sparse Autoencoders (SAEs) are a prominent tool in mechanistic\ninterpretability (MI) for decomposing neural network activations into\ninterpretable features. However, the aspiration to identify a canonical set of\nfeatures is challenged by the observed inconsistency of learned SAE features\nacross different training runs, undermining the reliability and efficiency of\nMI research. This position paper argues that mechanistic interpretability\nshould prioritize feature consistency in SAEs -- the reliable convergence to\nequivalent feature sets across independent runs. We propose using the Pairwise\nDictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to\noperationalize consistency and demonstrate that high levels are achievable\n(0.80 for TopK SAEs on LLM activations) with appropriate architectural choices.\nOur contributions include detailing the benefits of prioritizing consistency;\nproviding theoretical grounding and synthetic validation using a model\norganism, which verifies PW-MCC as a reliable proxy for ground-truth recovery;\nand extending these findings to real-world LLM data, where high feature\nconsistency strongly correlates with the semantic similarity of learned feature\nexplanations. We call for a community-wide shift towards systematically\nmeasuring feature consistency to foster robust cumulative progress in MI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20254.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64755a83e0b188d3cb2579d8",
      "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
      "fullname": "Aashiq Muhamed",
      "name": "aashiqmuhamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19955",
      "authors": [
        {
          "_id": "68354470650d51732c992a4e",
          "user": {
            "_id": "61166c4328c98bfd5b92e7c5",
            "avatarUrl": "/avatars/f4bb0f0cc2c5b84428c28bddaa479b61.svg",
            "isPro": false,
            "fullname": "Hui Chen",
            "user": "chchenhui",
            "type": "user"
          },
          "name": "Hui Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T08:47:12.560Z",
          "hidden": false
        },
        {
          "_id": "68354470650d51732c992a4f",
          "user": {
            "_id": "6530cf34e7535baecd9620a7",
            "avatarUrl": "/avatars/e6058a932d88e42b4957734f653cbcfd.svg",
            "isPro": false,
            "fullname": "Miao Xiong",
            "user": "happymio",
            "type": "user"
          },
          "name": "Miao Xiong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T10:03:35.714Z",
          "hidden": false
        },
        {
          "_id": "68354470650d51732c992a50",
          "name": "Yujie Lu",
          "hidden": false
        },
        {
          "_id": "68354470650d51732c992a51",
          "name": "Wei Han",
          "hidden": false
        },
        {
          "_id": "68354470650d51732c992a52",
          "name": "Ailin Deng",
          "hidden": false
        },
        {
          "_id": "68354470650d51732c992a53",
          "name": "Yufei He",
          "hidden": false
        },
        {
          "_id": "68354470650d51732c992a54",
          "name": "Jiaying Wu",
          "hidden": false
        },
        {
          "_id": "68354470650d51732c992a55",
          "name": "Yibo Li",
          "hidden": false
        },
        {
          "_id": "68354470650d51732c992a56",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "68354470650d51732c992a57",
          "name": "Bryan Hooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T13:18:37.000Z",
      "submittedOnDailyAt": "2025-05-27T07:30:37.766Z",
      "title": "MLR-Bench: Evaluación de Agentes AI en la Investigación de Aprendizaje Automático Abierto",
      "submittedOnDailyBy": {
        "_id": "61166c4328c98bfd5b92e7c5",
        "avatarUrl": "/avatars/f4bb0f0cc2c5b84428c28bddaa479b61.svg",
        "isPro": false,
        "fullname": "Hui Chen",
        "user": "chchenhui",
        "type": "user"
      },
      "summary": "El reciente desarrollo de los agentes AI ha demostrado potencial en las descubrimientos de la ciencia. En este estudio, se presenta MLR-Bench, una marca de referencia detallada para evaluar investigaciones de aprendizaje automático abierto. MLR-Bench incluye tres componentes principales: (1) Se recopilan 201 tareas de investigación provenientes de talleres de NeurIPS, ICLR y ICML, abarcando diversos temas de ML; (2) MLR-Judge, un marcador de evaluación automática basado en LLM que combina revisores y etiquetado de calificaciones cuidadosamente diseñado, para evaluar la calidad de la investigación; (3) MLR-Agent, un esquema de agente modular que completa tareas de investigación en cuatro etapas: generación de ideas, formulación de propuestas, experimentación y escritura de artículos. El marcador apoya la evaluación de estas etapas de investigación y la evaluación final del artículo. Además, se utiliza MLR-Bench para evaluar seis avanzados LLM y agentes de alta capacidad de codificación, investigando cómo los LLM efectivamente generan ideas conectadas y estructuradas de artículos, mientras que los actuales agentes de codificación generalmente (por ejemplo, en el 80% de los casos) producen resultados experimentales que se fabrican o son invalidados, constituyendo un grave obstáculo para la confianza científica. MLR-Judge ha sido evaluado con criterios humanos y muestra un alto acuerdo con los revisores profesionales, demostrando su posibilidad como herramienta escalable para la evaluación de investigación. MLR-Bench se abre como fuente de código para ser un marco de referencia, diagnóstico y mejora de agentes AI para la búsqueda de descubrimientos científicos confiables.",
      "upvotes": 4,
      "discussionId": "68354471650d51732c992a81",
      "githubRepo": "https://github.com/chchenhui/mlrbench",
      "ai_summary": "MLR-Bench evaluates AI agents in scientific research through modular stages, revealing that while LLMs perform well in ideation and writing, coding agents often produce unreliable experimental results.",
      "ai_keywords": [
        "MLR-Bench",
        "MLR-Judge",
        "MLR-Agent",
        "LLM-based reviewers",
        "review rubrics",
        "research evaluation",
        "idea generation",
        "proposal formulation",
        "experimentation",
        "paper writing"
      ]
    },
    "publishedAt": "2025-05-26T09:18:37.000Z",
    "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
    "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61166c4328c98bfd5b92e7c5",
      "avatarUrl": "/avatars/f4bb0f0cc2c5b84428c28bddaa479b61.svg",
      "fullname": "Hui Chen",
      "name": "chchenhui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19443",
      "authors": [
        {
          "_id": "683517bf6bb42c7e99bd3b5c",
          "user": {
            "_id": "67ddd80896ac367438d400a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
            "isPro": false,
            "fullname": "Ranjan Sapkota",
            "user": "RanjanSapkota",
            "type": "user"
          },
          "name": "Ranjan Sapkota",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:53:14.365Z",
          "hidden": false
        },
        {
          "_id": "683517bf6bb42c7e99bd3b5d",
          "name": "Konstantinos I. Roumeliotis",
          "hidden": false
        },
        {
          "_id": "683517bf6bb42c7e99bd3b5e",
          "name": "Manoj Karkee",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/ASTag4z8Os01guAbKpxI6.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/EgtU3Vsfc22Hko-FbRQ51.jpeg"
      ],
      "publishedAt": "2025-05-26T03:00:21.000Z",
      "submittedOnDailyAt": "2025-05-27T00:12:16.499Z",
      "title": "Bivboing vs. Azeneicoding: La base y el impacto práctico de la IA Azeneic",
      "submittedOnDailyBy": {
        "_id": "67ddd80896ac367438d400a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
        "isPro": false,
        "fullname": "Ranjan Sapkota",
        "user": "RanjanSapkota",
        "type": "user"
      },
      "summary": "Este review proporciona un análisis detallado de dos nuevos paradigmas en el desarrollo de software que apoyan el AI: el Beevboarding y el Agencyboarding. Ambos utilizan modelos de lenguaje de gran escala (LLMs), pero presentan diferencias fundamentales en autonomía, diseño arquitectónico y el papel del desarrollador. El Beevboarding es intuitivo y enfatiza la interacción humana dentro de un rope, apoyando ideación, experimentación y búsqueda creativa a través de flujos de conversación basados en prompts. Por otro lado, el Agencyboarding permite el desarrollo de software autónomo sin la participación humana mínima, utilizando objetivos para planificar, ejecutar, probar y repetir. Proponemos una técnica de trabajo detallada que incluye fundamentos conceptuales, modelos de ejecución, ciclos de retroalimentación, estructuras de seguridad, estrategias de depuración y la integración de la ecosistema de herramientas de la realidad. A través de un análisis de flujos de trabajo relativos y 20 casos de uso específicos, se observa que los sistemas Beevboarding están activos en la fabricación de prototipos iniciales y educación, mientras que los sistemas Agencyboarding excelen en la automatización empresarial, refactorización basada en código y la integración de CI/CD. También revisamos la tendencia emergente de una arquitectura híbrida que combina interfaces de lenguaje natural y flujos de ejecución autónomos. Finalmente, se establece una mapeo claro de la futura del Agencyboarding AI, explicando las infraestructuras necesarias para sistemas confiables, explicables y colaborativos. Nuestros hallazgos muestran que un éxito en el ingeniería de software AI no depende de elegir un solo paradigma, sino en la armonización de sus fortalezas en un ciclo de desarrollo humano centrado y unificado.",
      "upvotes": 4,
      "discussionId": "683517c06bb42c7e99bd3b92",
      "ai_summary": "A review contrasts vibe coding and agentic coding paradigms, highlighting their differences in interaction, autonomy, and application areas in AI-assisted software development.",
      "ai_keywords": [
        "large language models",
        "vibe coding",
        "agentic coding",
        "prompt-based",
        "conversational workflows",
        "goal-driven agents",
        "execution models",
        "feedback loops",
        "safety mechanisms",
        "debugging strategies",
        "tool ecosystems",
        "hybrid architectures",
        "autonomous execution pipelines",
        "trustworthy",
        "explainable",
        "collaborative systems",
        "unified",
        "human-centered development lifecycle"
      ]
    },
    "publishedAt": "2025-05-25T23:00:21.000Z",
    "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications\n  of Agentic AI",
    "summary": "This review presents a comprehensive analysis of two emerging paradigms in\nAI-assisted software development: vibe coding and agentic coding. While both\nleverage large language models (LLMs), they differ fundamentally in autonomy,\narchitectural design, and the role of the developer. Vibe coding emphasizes\nintuitive, human-in-the-loop interaction through prompt-based, conversational\nworkflows that support ideation, experimentation, and creative exploration. In\ncontrast, agentic coding enables autonomous software development through\ngoal-driven agents capable of planning, executing, testing, and iterating tasks\nwith minimal human intervention. We propose a detailed taxonomy spanning\nconceptual foundations, execution models, feedback loops, safety mechanisms,\ndebugging strategies, and real-world tool ecosystems. Through comparative\nworkflow analysis and 20 detailed use cases, we illustrate how vibe systems\nthrive in early-stage prototyping and education, while agentic systems excel in\nenterprise-grade automation, codebase refactoring, and CI/CD integration. We\nfurther examine emerging trends in hybrid architectures, where natural language\ninterfaces are coupled with autonomous execution pipelines. Finally, we\narticulate a future roadmap for agentic AI, outlining the infrastructure needed\nfor trustworthy, explainable, and collaborative systems. Our findings suggest\nthat successful AI software engineering will rely not on choosing one paradigm,\nbut on harmonizing their strengths within a unified, human-centered development\nlifecycle.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/ASTag4z8Os01guAbKpxI6.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/EgtU3Vsfc22Hko-FbRQ51.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19443.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ddd80896ac367438d400a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
      "fullname": "Ranjan Sapkota",
      "name": "RanjanSapkota",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17652",
      "authors": [
        {
          "_id": "6835264edf7cbb5c08ce28a5",
          "user": {
            "_id": "65a0aade5fafc248c2156e95",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a0aade5fafc248c2156e95/S9YjJMTuKc-U1cFizqUMA.jpeg",
            "isPro": false,
            "fullname": "DeyangKong",
            "user": "DeyangKong",
            "type": "user"
          },
          "name": "Deyang Kong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:51:42.197Z",
          "hidden": false
        },
        {
          "_id": "6835264edf7cbb5c08ce28a6",
          "name": "Qi Guo",
          "hidden": false
        },
        {
          "_id": "6835264edf7cbb5c08ce28a7",
          "user": {
            "_id": "63edb098679c2cc40abc6c2e",
            "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg",
            "isPro": false,
            "fullname": "Xiangyu",
            "user": "xixy",
            "type": "user"
          },
          "name": "Xiangyu Xi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:51:39.843Z",
          "hidden": false
        },
        {
          "_id": "6835264edf7cbb5c08ce28a8",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "6835264edf7cbb5c08ce28a9",
          "name": "Jingang Wang",
          "hidden": false
        },
        {
          "_id": "6835264edf7cbb5c08ce28aa",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "6835264edf7cbb5c08ce28ab",
          "name": "Shikun Zhang",
          "hidden": false
        },
        {
          "_id": "6835264edf7cbb5c08ce28ac",
          "name": "Wei Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T09:15:26.000Z",
      "submittedOnDailyAt": "2025-05-27T01:12:37.832Z",
      "title": "Pensamientos sobre la muestreo y crítica de Retiding Reining Learning\nTeoría centrada en el equilibrio entre habilidades y dificultad",
      "submittedOnDailyBy": {
        "_id": "65a0aade5fafc248c2156e95",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a0aade5fafc248c2156e95/S9YjJMTuKc-U1cFizqUMA.jpeg",
        "isPro": false,
        "fullname": "DeyangKong",
        "user": "DeyangKong",
        "type": "user"
      },
      "summary": "El aprendizaje por refuerzo se centra en la posibilidad de mejorar la capacidad de aprendizaje de modelos de lenguaje de gran escala, pero en la etapa de ejecución, la pérdida de eficiencia de muestras hace que el expansión sea difícil. Los métodos actuales intentan mejorar la eficiencia al programar la tarea en función de su dificultad, pero esta aproximación suele estar condicionada por estimaciones impredecibles y sesgos en la dificultad de la tarea. Además, no se considera que el aprendizaje por refuerzo puede no alcanzar resultados óptimos debido a la falta de comprensión de la consistencia entre la capacidad del modelo y la dificultad de la tarea. Para superar estas limitaciones, este artículo presenta la Introducción de la Sampling de Capacidad-Dificultad (CDAS), que utiliza la diferencia en el rendimiento de problemas con efectos acumulados de pasados para garantizar la consistencia entre la capacidad y la dificultad de los problemas. A continuación, se cuantifica la capacidad del modelo y se utiliza un sistema de puntos fijos para seleccionar de manera adaptativa problemas de dificultad correspondiente al nivel de capacidad del modelo actual. Los resultados de experimentos en diferentes marcadores de matemáticas muestran un gran incremento en precisión y eficiencia gracias a la CDAS. La CDAS alcanza una precisión promedio más alta que la referencia, y comparada con la estrategia competitiva de DAPO, Dynamic Sampling, demostra una ventaja de velocidad del 2.33 veces.",
      "upvotes": 4,
      "discussionId": "6835264fdf7cbb5c08ce28f9",
      "ai_summary": "CDAS addresses low sample efficiency in reinforcement learning by aligning model competence with problem difficulty, improving both accuracy and efficiency in mathematical benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "competence-difficulty alignment sampling",
        "CDAS",
        "historical performance discrepancies",
        "fixed-point system",
        "dynamic sampling",
        "DAPO"
      ]
    },
    "publishedAt": "2025-05-23T05:15:26.000Z",
    "title": "Rethinking the Sampling Criteria in Reinforcement Learning for LLM\n  Reasoning: A Competence-Difficulty Alignment Perspective",
    "summary": "Reinforcement learning exhibits potential in enhancing the reasoning\nabilities of large language models, yet it is hard to scale for the low sample\nefficiency during the rollout phase. Existing methods attempt to improve\nefficiency by scheduling problems based on problem difficulties. However, these\napproaches suffer from unstable and biased estimations of problem difficulty\nand fail to capture the alignment between model competence and problem\ndifficulty in RL training, leading to suboptimal results. To tackle these\nlimitations, this paper introduces Competence-Difficulty\nAlignment Sampling (CDAS), which enables accurate\nand stable estimation of problem difficulties by aggregating historical\nperformance discrepancies of problems. Then the model competence is quantified\nto adaptively select problems whose difficulty is in alignment with the model's\ncurrent competence using a fixed-point system. Experimental results across a\nrange of challenging mathematical benchmarks show that CDAS achieves great\nimprovements in both accuracy and efficiency. CDAS attains the highest average\naccuracy against baselines and exhibits significant speed advantages compared\nto Dynamic Sampling, a competitive strategy in DAPO, which is 2.33\ntimes slower than CDAS.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17652.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65a0aade5fafc248c2156e95",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a0aade5fafc248c2156e95/S9YjJMTuKc-U1cFizqUMA.jpeg",
      "fullname": "DeyangKong",
      "name": "DeyangKong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10887",
      "authors": [
        {
          "_id": "682b5387f1e88185bddb0643",
          "user": {
            "_id": "648a2042e8bee533291da413",
            "avatarUrl": "/avatars/83b918ddd7e2130a1c72ae74606068dc.svg",
            "isPro": false,
            "fullname": "Bin Lei",
            "user": "Bin12345",
            "type": "user"
          },
          "name": "Bin Lei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:22:15.514Z",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb0644",
          "name": "Weitai Kang",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb0645",
          "name": "Zijian Zhang",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb0646",
          "name": "Winson Chen",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb0647",
          "name": "Xi Xie",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb0648",
          "name": "Shan Zuo",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb0649",
          "name": "Mimi Xie",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb064a",
          "name": "Ali Payani",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb064b",
          "name": "Mingyi Hong",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb064c",
          "name": "Yan Yan",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb064d",
          "name": "Caiwen Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T05:43:27.000Z",
      "submittedOnDailyAt": "2025-05-27T01:07:44.773Z",
      "title": "InfantAgent-Next: Agent Multimodal Generalist para Interacción Automática con Computadoras",
      "submittedOnDailyBy": {
        "_id": "648a2042e8bee533291da413",
        "avatarUrl": "/avatars/83b918ddd7e2130a1c72ae74606068dc.svg",
        "isPro": false,
        "fullname": "Bin Lei",
        "user": "Bin12345",
        "type": "user"
      },
      "summary": "En este artículo se presenta InfantAgent-Next, un agente general. Este agente permite la interacción con la computadora a través de diferentes modelos de texto, imágenes, sonido y vídeo. A diferencia de los métodos existentes, no se centra en la construcción de flujos de trabajo complejos centrados en un gran modelo único o en la modularidad de los flujos de trabajo. Nuestro agente integra un agente basado en herramientas y un agente visual sencillo, basándose en una arquitectura modular de alto nivel, lo que permite la resolución de tareas separadas y resolubles en etapas mediante la conexión de diferentes modelos. Nuestra generalidad se demuestra tanto en evaluaciones basadas en el mundo real visual (por ejemplo, OSWorld) como en más generales evaluaciones que priorizan herramientas (por ejemplo, GAIA y SWE-Bench). En particular, en OSWorld alcanzó una precisión del 7.27%, superando a Claude-Computer-Use. El código y los scripts de evaluación están disponibles en https://github.com/bin123apple/InfantAgent.",
      "upvotes": 3,
      "discussionId": "682b5389f1e88185bddb070d",
      "githubRepo": "https://github.com/bin123apple/InfantAgent",
      "ai_summary": "InfantAgent-Next is a multimodal agent that integrates tool-based and vision models in a modular architecture to solve various benchmarks, including OSWorld, GAIA, and SWE-Bench.",
      "ai_keywords": [
        "multimodal agent",
        "tool-based agents",
        "pure vision agents",
        "modular architecture",
        "OSWorld",
        "GAIA",
        "SWE-Bench"
      ]
    },
    "publishedAt": "2025-05-16T01:43:27.000Z",
    "title": "InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer\n  Interaction",
    "summary": "This paper introduces InfantAgent-Next, a generalist agent capable\nof interacting with computers in a multimodal manner, encompassing text,\nimages, audio, and video. Unlike existing approaches that either build\nintricate workflows around a single large model or only provide workflow\nmodularity, our agent integrates tool-based and pure vision agents within a\nhighly modular architecture, enabling different models to collaboratively solve\ndecoupled tasks in a step-by-step manner. Our generality is demonstrated by our\nability to evaluate not only pure vision-based real-world benchmarks (i.e.,\nOSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and\nSWE-Bench). Specifically, we achieve 7.27% accuracy on OSWorld,\nhigher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced\nat https://github.com/bin123apple/InfantAgent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10887.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648a2042e8bee533291da413",
      "avatarUrl": "/avatars/83b918ddd7e2130a1c72ae74606068dc.svg",
      "fullname": "Bin Lei",
      "name": "Bin12345",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20294",
      "authors": [
        {
          "_id": "683552b7d34b8e5da4d9dfe3",
          "user": {
            "_id": "653cb25c394886efebf9971a",
            "avatarUrl": "/avatars/bca0a20c305e178a3f316581a2636cb6.svg",
            "isPro": false,
            "fullname": "Xiao Chen",
            "user": "Xiao-HF",
            "type": "user"
          },
          "name": "Xiao Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:49:12.484Z",
          "hidden": false
        },
        {
          "_id": "683552b7d34b8e5da4d9dfe4",
          "name": "Tai Wang",
          "hidden": false
        },
        {
          "_id": "683552b7d34b8e5da4d9dfe5",
          "name": "Quanyi Li",
          "hidden": false
        },
        {
          "_id": "683552b7d34b8e5da4d9dfe6",
          "name": "Tao Huang",
          "hidden": false
        },
        {
          "_id": "683552b7d34b8e5da4d9dfe7",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "683552b7d34b8e5da4d9dfe8",
          "name": "Tianfan Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T17:59:52.000Z",
      "submittedOnDailyAt": "2025-05-27T06:49:31.312Z",
      "title": "GLEAM: Policía de búsqueda aprendible para el mapeo activo en espacios 3D complejos de interiores",
      "submittedOnDailyBy": {
        "_id": "653cb25c394886efebf9971a",
        "avatarUrl": "/avatars/bca0a20c305e178a3f316581a2636cb6.svg",
        "isPro": false,
        "fullname": "Xiao Chen",
        "user": "Xiao-HF",
        "type": "user"
      },
      "summary": "Los métodos anteriores estuvieron limitados por una cantidad insuficiente de datos de entrenamiento y estrategias explorativas conservadoras, lo que los confinó a un apropiado mapeo activo en entornos 3D complejos con conexiones diversas. Para resolver estos problemas, se introduce el primer marco de referencia de gran escala, GLEAM-Bench, que permite entrenamiento escalable y evaluación confiable. Este marco de referencia está diseñado para abordar 1,152 tipos de escenas 3D diversas, tanto de datos sintéticos como de datos reales, para un mapeo activo generalizable. Basándose en este marco, se propone GLEAM, una política de exploración generalizable para el mapeo activo. Su rendimiento general se basa principalmente en la representación semántica, el alcance de exploración a largo plazo y estrategias randomizadas. En 128 escenas complejas nunca vistas antes, se alcanza un coveraje de situaciones del 66.50% (+9.49%), logrando proyectos eficientes y mejoras en la precisión del mapeo. Página del proyecto: https://xiao-chen.tech/gleam/",
      "upvotes": 2,
      "discussionId": "683552b9d34b8e5da4d9e050",
      "ai_summary": "A new benchmark and policy, GLEAM-Bench and GLEAM, improve the scalability and reliability of active mapping in complex environments through semantic representations and efficient exploration strategies.",
      "ai_keywords": [
        "active mapping",
        "generalizable exploration",
        "semantic representations",
        "navigable goals",
        "randomized strategies",
        "3D scenes",
        "synthetic datasets",
        "real-scan datasets",
        "benchmark",
        "mapping accuracy",
        "coverage",
        "trajectories"
      ]
    },
    "publishedAt": "2025-05-26T13:59:52.000Z",
    "title": "GLEAM: Learning Generalizable Exploration Policy for Active Mapping in\n  Complex 3D Indoor Scenes",
    "summary": "Generalizable active mapping in complex unknown environments remains a\ncritical challenge for mobile robots. Existing methods, constrained by\ninsufficient training data and conservative exploration strategies, exhibit\nlimited generalizability across scenes with diverse layouts and complex\nconnectivity. To enable scalable training and reliable evaluation, we introduce\nGLEAM-Bench, the first large-scale benchmark designed for generalizable active\nmapping with 1,152 diverse 3D scenes from synthetic and real-scan datasets.\nBuilding upon this foundation, we propose GLEAM, a unified generalizable\nexploration policy for active mapping. Its superior generalizability comes\nmainly from our semantic representations, long-term navigable goals, and\nrandomized strategies. It significantly outperforms state-of-the-art methods,\nachieving 66.50% coverage (+9.49%) with efficient trajectories and improved\nmapping accuracy on 128 unseen complex scenes. Project page:\nhttps://xiao-chen.tech/gleam/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20294.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653cb25c394886efebf9971a",
      "avatarUrl": "/avatars/bca0a20c305e178a3f316581a2636cb6.svg",
      "fullname": "Xiao Chen",
      "name": "Xiao-HF",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20139",
      "authors": [
        {
          "_id": "6835744884d4600675a4449c",
          "name": "Jialin Yang",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a4449d",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a4449e",
          "name": "Lipeng He",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a4449f",
          "name": "Sherman Siu",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444a0",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444a1",
          "name": "Disen Liao",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444a2",
          "name": "Zhuofeng Li",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444a3",
          "name": "Huaye Zeng",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444a4",
          "name": "Yiming Jia",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444a5",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444a6",
          "name": "Benjamin Schneider",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444a7",
          "name": "Chi Ruan",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444a8",
          "name": "Wentao Ma",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444a9",
          "name": "Zhiheng Lyu",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444aa",
          "name": "Yifei Wang",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444ab",
          "name": "Yi Lu",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444ac",
          "name": "Quy Duc Do",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444ad",
          "name": "Ziyan Jiang",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444ae",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "6835744884d4600675a444af",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T15:40:42.000Z",
      "submittedOnDailyAt": "2025-05-27T06:46:45.076Z",
      "title": "StructEval: Herramienta para evaluar la capacidad de generación de salidas estructuradas por un LLM",
      "submittedOnDailyBy": {
        "_id": "62567c86d444a9b5a0ec51c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
        "isPro": false,
        "fullname": "Dongfu Jiang",
        "user": "DongfuJiang",
        "type": "user"
      },
      "summary": "La modelo de Largo Rango de Jungle (LLMs) ha adquirido una importancia crucial en el flujo de trabajo de desarrollo de software. La capacidad de generar salidas estructuradas ha convertidose en una habilidad muy importante. StructureEval presenta un detallado benchmark para evaluar la capacidad de generación de formatos estructurados en LLMs. A diferencia de los benchmarks previos, StructureEval evalúa la precisión estructural de manera sistemática en dos paradigmas: 1) la generación de tareas desde un lenguaje natural a salidas estructuradas y 2) la traducción de formatos estructurados entre sí. El benchmark incluye 18 formatos y 44 tipos de tareas, utilizando nuevos métricas para la adecuación del formato y la precisión estructural. Finalmente, se observó una notable diferencia en el rendimiento. Por ejemplo, el modelo de estado de la arte, o1-mini, alcanzó un promedio de 75.58 puntos, mientras que sus sustitutos abierto-source caían en aproximadamente 10 puntos. Las tareas de generación son más difíciles que las de transformación, ya que crear solo la estructura de un texto es mucho más sencillo que crear contenido visual preciso.",
      "upvotes": 2,
      "discussionId": "6835744884d4600675a444d3",
      "ai_summary": "StructEval benchmarks Large Language Models for generating and converting structured outputs, highlighting performance gaps and challenges in producing visual content.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "StructEval",
        "structured outputs",
        "JSON",
        "YAML",
        "CSV",
        "HTML",
        "React",
        "SVG",
        "generation tasks",
        "conversion tasks",
        "format adherence",
        "structural correctness"
      ]
    },
    "publishedAt": "2025-05-26T11:40:42.000Z",
    "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural\n  Outputs",
    "summary": "As Large Language Models (LLMs) become integral to software development\nworkflows, their ability to generate structured outputs has become critically\nimportant. We introduce StructEval, a comprehensive benchmark for evaluating\nLLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and\nrenderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,\nStructEval systematically evaluates structural fidelity across diverse formats\nthrough two paradigms: 1) generation tasks, producing structured output from\nnatural language prompts, and 2) conversion tasks, translating between\nstructured formats. Our benchmark encompasses 18 formats and 44 types of task,\nwith novel metrics for format adherence and structural correctness. Results\nreveal significant performance gaps, even state-of-the-art models like o1-mini\nachieve only 75.58 average score, with open-source alternatives lagging\napproximately 10 points behind. We find generation tasks more challenging than\nconversion tasks, and producing correct visual content more difficult than\ngenerating text-only structures.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20139.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62567c86d444a9b5a0ec51c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
      "fullname": "Dongfu Jiang",
      "name": "DongfuJiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19706",
      "authors": [
        {
          "_id": "6835182873a16b09c94ac4d2",
          "name": "Tej Deep Pala",
          "hidden": false
        },
        {
          "_id": "6835182873a16b09c94ac4d3",
          "name": "Panshul Sharma",
          "hidden": false
        },
        {
          "_id": "6835182873a16b09c94ac4d4",
          "name": "Amir Zadeh",
          "hidden": false
        },
        {
          "_id": "6835182873a16b09c94ac4d5",
          "name": "Chuan Li",
          "hidden": false
        },
        {
          "_id": "6835182873a16b09c94ac4d6",
          "name": "Soujanya Poria",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/OPFuTq1oRiXqqwJPyKgUx.png",
        "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/VsJ0SH2BgYbBQTS55nWSB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/qreWH-gdINsiHnTwLQcOL.png"
      ],
      "publishedAt": "2025-05-26T08:56:36.000Z",
      "submittedOnDailyAt": "2025-05-27T00:29:18.345Z",
      "title": "Error typing to be more intelligently rewarded: Using layered hyper-asymmetry interested in errors to improve process reward models.",
      "submittedOnDailyBy": {
        "_id": "626b626405fe1cb65725aca1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/aa-Lata46I3fXOmMetvXH.jpeg",
        "isPro": false,
        "fullname": "Soujanya Poria",
        "user": "soujanyaporia",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) se destacan particularmente en tareas de tipo matemática, que son altamente estructuradas y lógicas. Los modelos de recompensa final (FRMs) solo verifican la respuesta final, mientras que los modelos de recompensa por etapa (PRMs) evaluan cada etapa y controlan la generación de soluciones uniformes. Presentamos PathFinder-PRM, un nuevo PRM de capa, que es sensible a los errores y responde de manera precisa a cada etapa. Este modelo clasifica errores matemáticos o inconsistencias en cada etapa y estima la precisión de la etapa a través de la integración de estos micro-señales. Para el entrenamiento de PathFinder-PRM, hemos construido un conjunto de datos de 400K muestras con etiquetas de nivel de etapa, utilizando el corpus PRM800K explicado por humanos y las trazas de RLHFlow Mistral. En PRMBench, PathFinder-PRM ha alcanzado un nuevo record de puntuación PRMScore (67.7), superando el anterior record (65.5) con un 3 veces menor cantidad de datos. Cuando se aplica a búsquedas greedy guiadas por recompensa, nuestro modelo alcanza un prm@8 de 48.3, con un aumento de 1.5 puntos sobre el mejor base de referencia. Estos resultados muestran que la detección de errores separados y la estimación de recompensa estimulan la detección de errores más precisa y mejoran significativamente la comprensión matemática guiada por recompensa en las etapas finales.",
      "upvotes": 2,
      "discussionId": "6835182973a16b09c94ac514",
      "githubRepo": "https://github.com/declare-lab/PathFinder-PRM",
      "ai_summary": "PathFinder-PRM, a hierarchical and error-aware Process Reward Model, improves mathematical problem-solving by fine-grained error classification and step correctness estimation, achieving state-of-the-art PRMScore with reduced data usage.",
      "ai_keywords": [
        "Large Language Models",
        "hallucination",
        "mathematical problem solving",
        "Outcome Reward Models",
        "Process Reward Models",
        "PathFinder-PRM",
        "hierarchical",
        "error-aware",
        "discriminative PRM",
        "math errors",
        "consistency errors",
        "step correctness",
        "PRMBench",
        "PRMScore",
        "reward guided greedy search",
        "prm@8",
        "data efficiency"
      ]
    },
    "publishedAt": "2025-05-26T04:56:36.000Z",
    "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with\n  Error-Aware Hierarchical Supervision",
    "summary": "Large Language Models (LLMs) are prone to hallucination, especially during\nmulti-hop and reasoning-intensive tasks such as mathematical problem solving.\nWhile Outcome Reward Models verify only final answers, Process Reward Models\n(PRMs) score each intermediate step to steer generation toward coherent\nsolutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware\ndiscriminative PRM that first classifies math and consistency errors at each\nstep, then combines these fine-grained signals to estimate step correctness. To\ntrain PathFinder-PRM, we construct a 400K-sample dataset by enriching the\nhuman-annotated PRM800K corpus and RLHFlow Mistral traces with\nthree-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new\nstate-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while\nusing 3 times less data. When applied to reward guided greedy search, our model\nyields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results\ndemonstrate that decoupled error detection and reward estimation not only boost\nfine-grained error detection but also substantially improve end-to-end,\nreward-guided mathematical reasoning with greater data efficiency.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/OPFuTq1oRiXqqwJPyKgUx.png",
      "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/VsJ0SH2BgYbBQTS55nWSB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/qreWH-gdINsiHnTwLQcOL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19706.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626b626405fe1cb65725aca1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/aa-Lata46I3fXOmMetvXH.jpeg",
      "fullname": "Soujanya Poria",
      "name": "soujanyaporia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19630",
      "authors": [
        {
          "_id": "683522abd68b329aeb799c46",
          "name": "Yichun Feng",
          "hidden": false
        },
        {
          "_id": "683522abd68b329aeb799c47",
          "user": {
            "_id": "64060b49a577649430bf6974",
            "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
            "isPro": false,
            "fullname": "Jiawei Wang",
            "user": "Jarvis1111",
            "type": "user"
          },
          "name": "Jiawei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:52:01.364Z",
          "hidden": false
        },
        {
          "_id": "683522abd68b329aeb799c48",
          "name": "Lu Zhou",
          "hidden": false
        },
        {
          "_id": "683522abd68b329aeb799c49",
          "name": "Yixue Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T07:48:14.000Z",
      "submittedOnDailyAt": "2025-05-27T00:56:34.903Z",
      "title": "DoctorAgent-RL: El sistema de aprendizaje por refuerzo colaborativo de agentes de ClinicalKDIALOG para la reinformación continua",
      "submittedOnDailyBy": {
        "_id": "64060b49a577649430bf6974",
        "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
        "isPro": false,
        "fullname": "Jiawei Wang",
        "user": "Jarvis1111",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) han demostrado capacidades excepcionales en el campo de la consulta de medicina, pero aún no resuelven los problemas esenciales en las conversaciones de tratamiento real. Los sistemas actuales requieren que los pacientes expliquen completamente sus síntomas en una sola ocasión, utilizan modos de transmisión de información y proporcionan recomendaciones de diagnóstico general cuando los síntomas son inciertos. Los métodos de diálogo tradicionales basados en aprendizaje supervisado están limitados por un paradigma dinámico de datos, lo que disminuye su capacidad de generalización y dificulta la extracción adecuada de información clínica. Para enfrentar estas limitaciones, proponemos un marco de colaboración de múltiples agentes basado en aprendizaje por refuerzo (RL), llamado \"DoctorAgent-RL\", que modela procesos de toma de decisiones dinámicos. Los agentes médicos optimizan estrategias de preguntas en un marco de RL a través de interacciones múltiples con los agentes de pacientes, ajustando la información de manera dinámica según recompensas detalladas de evaluadores de diagnóstico. Esta estructura de ajuste micro de RL permite que los LLMs desarrollen estrategias de interacción automáticamente adaptadas a la lógica clínica, sin necesidad de modelar patrones explícitos en los datos de diálogo actual. En particular, hemos construido el primer conjunto de datos de diálogo médico en inglés \"MTMedDialog\". Los experimentos muestran que DoctorAgent-RL supera a los modelos actuales en la capacidad de razonamiento de múltiples turnos y en el rendimiento final del diagnóstico. Demostramos así la valiosa utilidad práctica de apoyar conversaciones de tratamiento. https://github.com/JarvisUSTC/DoctorAgent-RL",
      "upvotes": 2,
      "discussionId": "683522add68b329aeb799cc4",
      "githubRepo": "https://github.com/JarvisUSTC/DoctorAgent-RL",
      "ai_summary": "DoctorAgent-RL, a reinforcement learning-based multi-agent framework, enhances multi-turn reasoning and diagnostic performance in medical consultations compared to existing systems.",
      "ai_keywords": [
        "reinforcement learning",
        "multi-agent collaborative framework",
        "dynamic decision-making",
        "uncertainty",
        "questioning strategy",
        "interaction strategy",
        "clinical reasoning",
        "multi-turn medical consultation dataset",
        "diagnostic performance"
      ]
    },
    "publishedAt": "2025-05-26T03:48:14.000Z",
    "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning\n  System for Multi-Turn Clinical Dialogue",
    "summary": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Existing systems rely on a\none-way information transmission mode where patients must fully describe their\nsymptoms in a single round, leading to nonspecific diagnostic recommendations\nwhen complaints are vague. Traditional multi-turn dialogue methods based on\nsupervised learning are constrained by static data-driven paradigms, lacking\ngeneralizability and struggling to intelligently extract key clinical\ninformation. To address these limitations, we propose DoctorAgent-RL, a\nreinforcement learning (RL)-based multi-agent collaborative framework that\nmodels medical consultations as a dynamic decision-making process under\nuncertainty. The doctor agent continuously optimizes its questioning strategy\nwithin the RL framework through multi-turn interactions with the patient agent,\ndynamically adjusting its information-gathering path based on comprehensive\nrewards from the Consultation Evaluator. This RL fine-tuning mechanism enables\nLLMs to autonomously develop interaction strategies aligned with clinical\nreasoning logic, rather than superficially imitating patterns in existing\ndialogue data. Notably, we constructed MTMedDialog, the first English\nmulti-turn medical consultation dataset capable of simulating patient\ninteractions. Experiments demonstrate that DoctorAgent-RL outperforms existing\nmodels in both multi-turn reasoning capability and final diagnostic\nperformance, demonstrating practical value in assisting clinical consultations.\nhttps://github.com/JarvisUSTC/DoctorAgent-RL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19630.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64060b49a577649430bf6974",
      "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
      "fullname": "Jiawei Wang",
      "name": "Jarvis1111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19223",
      "authors": [
        {
          "_id": "68357a21d0fbc64a8e829088",
          "name": "Fengqi Zhu",
          "hidden": false
        },
        {
          "_id": "68357a21d0fbc64a8e829089",
          "name": "Rongzhen Wang",
          "hidden": false
        },
        {
          "_id": "68357a21d0fbc64a8e82908a",
          "name": "Shen Nie",
          "hidden": false
        },
        {
          "_id": "68357a21d0fbc64a8e82908b",
          "user": {
            "_id": "67513d6d3b8586521cda5d76",
            "avatarUrl": "/avatars/0f95cc5c23a0a1da289aa785bd33b616.svg",
            "isPro": false,
            "fullname": "Xiaolu  Zhang",
            "user": "xiaolu0714",
            "type": "user"
          },
          "name": "Xiaolu Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:45:40.970Z",
          "hidden": false
        },
        {
          "_id": "68357a21d0fbc64a8e82908c",
          "name": "Chunwei Wu",
          "hidden": false
        },
        {
          "_id": "68357a21d0fbc64a8e82908d",
          "name": "Jun Hu",
          "hidden": false
        },
        {
          "_id": "68357a21d0fbc64a8e82908e",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "68357a21d0fbc64a8e82908f",
          "user": {
            "_id": "65fcad0ba0d7adc40b54fac2",
            "avatarUrl": "/avatars/7564b5642378fddb46ec3b5ae57c0402.svg",
            "isPro": false,
            "fullname": "Jianfei Chen",
            "user": "surfingtomchen",
            "type": "user"
          },
          "name": "Jianfei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:45:00.594Z",
          "hidden": false
        },
        {
          "_id": "68357a21d0fbc64a8e829090",
          "user": {
            "_id": "657a651e1433ea7d44de6397",
            "avatarUrl": "/avatars/ccfc76f94595a38ff4a80f77c911eabf.svg",
            "isPro": false,
            "fullname": "Yankai Lin",
            "user": "lyk423",
            "type": "user"
          },
          "name": "Yankai Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:44:53.835Z",
          "hidden": false
        },
        {
          "_id": "68357a21d0fbc64a8e829091",
          "user": {
            "_id": "64b8c89052b7353d8c6a1013",
            "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
            "isPro": false,
            "fullname": "Ji-Rong Wen",
            "user": "jrwen",
            "type": "user"
          },
          "name": "Ji-Rong Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:44:47.347Z",
          "hidden": false
        },
        {
          "_id": "68357a21d0fbc64a8e829092",
          "user": {
            "_id": "64c07b488e2612254361153b",
            "avatarUrl": "/avatars/ade0f783cc4c2d3e73f402637f595471.svg",
            "isPro": false,
            "fullname": "chongxuan li",
            "user": "zhenxuan00",
            "type": "user"
          },
          "name": "Chongxuan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-27T08:44:37.114Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63a369d98c0c89dcae3b8329/HQWTRZ5gL3-RFJ6PSJ3NC.jpeg"
      ],
      "publishedAt": "2025-05-25T16:36:20.000Z",
      "submittedOnDailyAt": "2025-05-27T07:14:06.300Z",
      "title": "LLaDA 1.5: Modelo de optimización de la reducción de la variación lingüística",
      "submittedOnDailyBy": {
        "_id": "63a369d98c0c89dcae3b8329",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png",
        "isPro": false,
        "fullname": "Adina Yakefu",
        "user": "AdinaY",
        "type": "user"
      },
      "summary": "El módulo de herencia genética de la máscara (MDM) como el modelo LLaDA propone un nuevo paradigma en el modelado de lenguaje, aunque no se deja de lado el esfuerzo relativamente menor en aprendizaje por refuerzo para adaptarse a las preferencias humanas. La principal cuestión es la alta varianza en la estimación de probabilidades basada en el ELBO (inferior límite de verificación) para optimizar las preferencias. Para abordar este problema, proponemos la Variance-Reduced Preference Optimization (VRPO). VRPO analiza formalmente la varianza en la estimación del ELBO y proporciona un marco que limita tanto la bias como la varianza en la optimización de preferencias. Basándonos en esta base teórica, introducimos estrategias para reducir la varianza y mejorar significativamente el rendimiento de la MDM, incluyendo la distribución óptima de Monte Carlo y la muestreo inverso. Demostramos los beneficios de VRPO aplicandolo en LLaDA, donde el modelo resultante, LLaDA 1.5, muestra resultados activamente mejores en áreas como matemáticas (GSM8K +4.7), código (HumanEval +3.0, MBPP +1.8) y benchmarking de ajuste (IFEval +4.0, Arena-Hard +4.3) frente a modelos SFT anteriores. Además, LLaDA 1.5 demuestra una competencia fuerte en el lenguaje MDM y ARM, mostrando una excelente performance matemática. Página del proyecto: https://ml-gsai.github.io/LLaDA-1.5-Demo/",
      "upvotes": 2,
      "discussionId": "68357a21d0fbc64a8e8290ba",
      "ai_summary": "VRPO is a variance-reduced preference optimization framework for Masked Diffusion Models that significantly enhances their alignment with human preferences and performance across various benchmarks.",
      "ai_keywords": [
        "Masked Diffusion Models",
        "LLaDA",
        "Variance-Reduced Preference Optimization",
        "VRPO",
        "Evidence Lower Bound",
        "ELBO",
        "bias",
        "variance",
        "preference optimization",
        "unbiased variance reduction",
        "optimal Monte Carlo budget allocation",
        "antithetic sampling",
        "GSM8K",
        "HumanEval",
        "MBPP",
        "IFEval",
        "Arena-Hard",
        "ARMs"
      ]
    },
    "publishedAt": "2025-05-25T12:36:20.000Z",
    "title": "LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language\n  Diffusion Models",
    "summary": "While Masked Diffusion Models (MDMs), such as LLaDA, present a promising\nparadigm for language modeling, there has been relatively little effort in\naligning these models with human preferences via reinforcement learning. The\nchallenge primarily arises from the high variance in Evidence Lower Bound\n(ELBO)-based likelihood estimates required for preference optimization. To\naddress this issue, we propose Variance-Reduced Preference Optimization (VRPO),\na framework that formally analyzes the variance of ELBO estimators and derives\nbounds on both the bias and variance of preference optimization gradients.\nBuilding on this theoretical foundation, we introduce unbiased variance\nreduction strategies, including optimal Monte Carlo budget allocation and\nantithetic sampling, that significantly improve the performance of MDM\nalignment. We demonstrate the effectiveness of VRPO by applying it to LLaDA,\nand the resulting model, LLaDA 1.5, outperforms its SFT-only predecessor\nconsistently and significantly across mathematical (GSM8K +4.7), code\n(HumanEval +3.0, MBPP +1.8), and alignment benchmarks (IFEval +4.0, Arena-Hard\n+4.3). Furthermore, LLaDA 1.5 demonstrates a highly competitive mathematical\nperformance compared to strong language MDMs and ARMs. Project page:\nhttps://ml-gsai.github.io/LLaDA-1.5-Demo/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63a369d98c0c89dcae3b8329/HQWTRZ5gL3-RFJ6PSJ3NC.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19223.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 702
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19084",
      "authors": [
        {
          "_id": "6835334e0c0aff775f3eb6e2",
          "user": {
            "_id": "640c64779e5247967ff1e0b2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678533946170-640c64779e5247967ff1e0b2.jpeg",
            "isPro": false,
            "fullname": "Yifeng Xu",
            "user": "xyfJASON",
            "type": "user"
          },
          "name": "Yifeng Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T08:15:08.170Z",
          "hidden": false
        },
        {
          "_id": "6835334e0c0aff775f3eb6e3",
          "name": "Zhenliang He",
          "hidden": false
        },
        {
          "_id": "6835334e0c0aff775f3eb6e4",
          "name": "Meina Kan",
          "hidden": false
        },
        {
          "_id": "6835334e0c0aff775f3eb6e5",
          "name": "Shiguang Shan",
          "hidden": false
        },
        {
          "_id": "6835334e0c0aff775f3eb6e6",
          "name": "Xilin Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/Mqa2jx5wM-f5Fc1pdd6sz.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/-ubqvKPbhrxAjIKnj7dPU.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/Y4ub2WOy0Adp1TfcT90R3.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/lvdM2FUHFI2ut7cgELTh9.jpeg"
      ],
      "publishedAt": "2025-05-25T10:40:52.000Z",
      "submittedOnDailyAt": "2025-05-27T07:40:01.653Z",
      "title": "Jordi: Modelar la generación y comprensión visual juntos",
      "submittedOnDailyBy": {
        "_id": "640c64779e5247967ff1e0b2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678533946170-640c64779e5247967ff1e0b2.jpeg",
        "isPro": false,
        "fullname": "Yifeng Xu",
        "user": "xyfJASON",
        "type": "user"
      },
      "summary": "La generación visual y la comprensión son dos aspectos profundamente relacionados de la inteligencia humana, y en el aprendizaje automático se han tratado de manera independiente. En este artículo, se presenta un marco de trabajo distribuido propuesto para la integración de generación visual y comprensión llamado \"Jodi\". Jodi modela el área de las imágenes y varios dominios de etiquetas para integrar la generación y comprensión de imágenes. En particular, Jodi introduce la transformador lineal distribuido y la función de intercambio de roles para realizar las siguientes tareas con características: la generación común de imágenes y etiquetas, la generación controlable basada en la combinación de etiquetas arbitrarias, y la predicción simultánea de múltiples etiquetas en una imagen. Además, se presenta el conjunto de datos \"Joint-1.6M\", que incluye 200,000 imágenes de alta calidad, etiquetas automáticas en 7 dominios visuales y captiones generadas por un modelo de lenguaje grande, para demostrar las funciones de Jodi. Debido a su extensibilidad, Jodi ha demostrado excelentes resultados en ambas tareas de generación visual y comprensión, y también en un amplio rango de dominios visuales. El código está disponible en https://github.com/VIPL-GENUN/Jodi.",
      "upvotes": 2,
      "discussionId": "683533510c0aff775f3eb7ab",
      "ai_summary": "Jodi, a diffusion framework using a linear diffusion transformer and role switch mechanism, unifies visual generation and understanding, performing joint, controllable, and perceptual tasks effectively across multiple visual domains.",
      "ai_keywords": [
        "diffusion framework",
        "linear diffusion transformer",
        "role switch mechanism",
        "joint generation",
        "controllable generation",
        "image perception",
        "Joint-1.6M dataset",
        "visual domains",
        "LLM-generated captions"
      ]
    },
    "publishedAt": "2025-05-25T06:40:52.000Z",
    "title": "Jodi: Unification of Visual Generation and Understanding via Joint\n  Modeling",
    "summary": "Visual generation and understanding are two deeply interconnected aspects of\nhuman intelligence, yet they have been traditionally treated as separate tasks\nin machine learning. In this paper, we propose Jodi, a diffusion framework that\nunifies visual generation and understanding by jointly modeling the image\ndomain and multiple label domains. Specifically, Jodi is built upon a linear\ndiffusion transformer along with a role switch mechanism, which enables it to\nperform three particular types of tasks: (1) joint generation, where the model\nsimultaneously generates images and multiple labels; (2) controllable\ngeneration, where images are generated conditioned on any combination of\nlabels; and (3) image perception, where multiple labels can be predicted at\nonce from a given image. Furthermore, we present the Joint-1.6M dataset, which\ncontains 200,000 high-quality images collected from public sources, automatic\nlabels for 7 visual domains, and LLM-generated captions. Extensive experiments\ndemonstrate that Jodi excels in both generation and understanding tasks and\nexhibits strong extensibility to a wider range of visual domains. Code is\navailable at https://github.com/VIPL-GENUN/Jodi.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/Mqa2jx5wM-f5Fc1pdd6sz.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/-ubqvKPbhrxAjIKnj7dPU.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/Y4ub2WOy0Adp1TfcT90R3.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/lvdM2FUHFI2ut7cgELTh9.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640c64779e5247967ff1e0b2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678533946170-640c64779e5247967ff1e0b2.jpeg",
      "fullname": "Yifeng Xu",
      "name": "xyfJASON",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18773",
      "authors": [
        {
          "_id": "6835727f9da2b91fb4e30473",
          "name": "Jamie Hayes",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e30474",
          "name": "Ilia Shumailov",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e30475",
          "name": "Christopher A. Choquette-Choo",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e30476",
          "name": "Matthew Jagielski",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e30477",
          "name": "George Kaissis",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e30478",
          "name": "Katherine Lee",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e30479",
          "name": "Milad Nasr",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e3047a",
          "name": "Sahra Ghalebikesabi",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e3047b",
          "name": "Niloofar Mireshghallah",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e3047c",
          "name": "Meenatchi Sundaram Mutu Selva Annamalai",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e3047d",
          "name": "Igor Shilov",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e3047e",
          "name": "Matthieu Meeus",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e3047f",
          "name": "Yves-Alexandre de Montjoye",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e30480",
          "name": "Franziska Boenisch",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e30481",
          "name": "Adam Dziedzic",
          "hidden": false
        },
        {
          "_id": "6835727f9da2b91fb4e30482",
          "user": {
            "_id": "663c3b587e7bc3d3e4a54ffb",
            "avatarUrl": "/avatars/681abf7e4a85184667015cefefa226c6.svg",
            "isPro": false,
            "fullname": "A. Feder Cooper",
            "user": "pasta41",
            "type": "user"
          },
          "name": "A. Feder Cooper",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-27T08:06:24.708Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T16:23:43.000Z",
      "submittedOnDailyAt": "2025-05-27T06:39:29.837Z",
      "title": "Grandes conjuntos de datos y ataques de membresía fuertes contra modelos de lenguaje grandes (intermediarios)",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "La mejor inferencia de miembros (MIA) avanzada requiere la entrenamiento de múltiples modelos de referencia, lo que es imposible de escalar para modelos de lenguaje predeterminado (LLM) de gran escala, lo que ha llevado a que los estudios anteriores evitaran el entrenamiento de modelos de referencia (por ejemplo, ataques de fine-tuning) o se centraran en aplicar fuertes ataques a pequeños modelos o conjuntos de datos. Sin embargo, estos ataques débiles han mostrado ser débiles y aproximadamente exitosos, y la evasión de fuertes ataques en configuraciones simplificadas no se mantiene en los LLM actuales. Esto ha generado una pregunta importante: ¿son los límites observados en los estudios anteriores consecuencias de las decisiones de diseño del ataque o es que la MIA es inherentemente inadecuada para los LLM? Para abordar esta cuestión, hemos escalado LiRA (uno de los más poderosos MIA) a la arquitectura de GPT-2 (con un número de parámetros entre 10M y 1B) y entrenamos modelos de referencia con 200 mil millones de tokens en el conjunto de datos C4. Nuestros resultados han llevado a una mejora en el entendimiento de la MIA en los LLM en tres puntos clave: (1) una MIA fuerte puede tener éxito con modelos LLM preentrenados; (2) su eficacia está limitada en configuraciones reales (por ejemplo, AUC < 0.7); (3) la relación entre el éxito del ataque y métricas de privacidad es diferente a lo que se ha presentado en estudios anteriores.",
      "upvotes": 2,
      "discussionId": "683572809da2b91fb4e30513",
      "ai_summary": "Scaling LiRA membership inference attacks to large pre-trained language models shows that while these attacks can succeed, their effectiveness is limited and does not definitively correlate with privacy metrics.",
      "ai_keywords": [
        "membership inference attacks",
        "MIAs",
        "reference models",
        "fine-tuning attacks",
        "pre-trained language models",
        "LLMs",
        "LiRA",
        "GPT-2",
        "tokens",
        "C4 dataset",
        "AUC",
        "privacy metrics"
      ]
    },
    "publishedAt": "2025-05-24T12:23:43.000Z",
    "title": "Strong Membership Inference Attacks on Massive Datasets and (Moderately)\n  Large Language Models",
    "summary": "State-of-the-art membership inference attacks (MIAs) typically require\ntraining many reference models, making it difficult to scale these attacks to\nlarge pre-trained language models (LLMs). As a result, prior research has\neither relied on weaker attacks that avoid training reference models (e.g.,\nfine-tuning attacks), or on stronger attacks applied to small-scale models and\ndatasets. However, weaker attacks have been shown to be brittle - achieving\nclose-to-arbitrary success - and insights from strong attacks in simplified\nsettings do not translate to today's LLMs. These challenges have prompted an\nimportant question: are the limitations observed in prior work due to attack\ndesign choices, or are MIAs fundamentally ineffective on LLMs? We address this\nquestion by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures\nranging from 10M to 1B parameters, training reference models on over 20B tokens\nfrom the C4 dataset. Our results advance the understanding of MIAs on LLMs in\nthree key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their\neffectiveness, however, remains limited (e.g., AUC<0.7) in practical settings;\nand, (3) the relationship between MIA success and related privacy metrics is\nnot as straightforward as prior work has suggested.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18773.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18384",
      "authors": [
        {
          "_id": "68354f30d795fadab0623699",
          "user": {
            "_id": "65319bd7f85995389d4f019c",
            "avatarUrl": "/avatars/657858b8435b220c9a29918c0dae9c6d.svg",
            "isPro": false,
            "fullname": "Boyi Wei",
            "user": "boyiwei",
            "type": "user"
          },
          "name": "Boyi Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:49:18.459Z",
          "hidden": false
        },
        {
          "_id": "68354f30d795fadab062369a",
          "name": "Benedikt Stroebl",
          "hidden": false
        },
        {
          "_id": "68354f30d795fadab062369b",
          "name": "Jiacen Xu",
          "hidden": false
        },
        {
          "_id": "68354f30d795fadab062369c",
          "name": "Joie Zhang",
          "hidden": false
        },
        {
          "_id": "68354f30d795fadab062369d",
          "name": "Zhou Li",
          "hidden": false
        },
        {
          "_id": "68354f30d795fadab062369e",
          "name": "Peter Henderson",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T21:18:59.000Z",
      "submittedOnDailyAt": "2025-05-27T04:06:40.688Z",
      "title": "Seguir el enfoque de la seguridad contraatacadora en evaluaciones dinámicas de riesgos",
      "submittedOnDailyBy": {
        "_id": "65319bd7f85995389d4f019c",
        "avatarUrl": "/avatars/657858b8435b220c9a29918c0dae9c6d.svg",
        "isPro": false,
        "fullname": "Boyi Wei",
        "user": "boyiwei",
        "type": "user"
      },
      "summary": "El modelo básico está mejorando su capacidad como programador automático, lo que aumenta la posibilidad de que las operaciones cibernéticas agresivas se automaticen. Las evaluaciones de los modelos más avanzados actualmente investigan el riesgo de seguridad cibernética de estas entidades de salida, pero la mayoría no consideran la libertad que el oponente tiene en el mundo real. En particular, si un verificador fuerte y una recompensa financiera son ofrecidos, las entidades de salida para la seguridad cibernética pueden ser mejoradas por el oponente a través de experimentos y errores. Argumentamos que debemos considerar un modelo de amenazas ampliado en el contexto de la seguridad cibernética y enfatizar la libertad del oponente en estados y entornos de emergencia. Demostramos que el oponente puede mejorar en exceso del 40% su capacidad de seguridad cibernética en un CTF de codificación interactiva con un tiempo fijo de 8 GPU H100. Estos resultados subrayan la necesidad de evaluar dinámicamente el riesgo de las entidades de salida y de representar de manera representativa la amenaza.",
      "upvotes": 2,
      "discussionId": "68354f30d795fadab06236fe",
      "githubRepo": "https://github.com/boyiwei/Dynamic-Risk-Assessment",
      "ai_summary": "Adversaries can significantly enhance foundation model capabilities in offensive cybersecurity with limited computational resources, underscoring the need for dynamic threat model assessments.",
      "ai_keywords": [
        "foundation models",
        "autonomous programmers",
        "offensive cybersecurity",
        "model audits",
        "cybersecurity risks",
        "verifiers",
        "financial incentives",
        "iterative improvement",
        "threat model",
        "stateful environments",
        "non-stateful environments",
        "compute budget",
        "InterCode CTF"
      ]
    },
    "publishedAt": "2025-05-23T17:18:59.000Z",
    "title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents",
    "summary": "Foundation models are increasingly becoming better autonomous programmers,\nraising the prospect that they could also automate dangerous offensive\ncyber-operations. Current frontier model audits probe the cybersecurity risks\nof such agents, but most fail to account for the degrees of freedom available\nto adversaries in the real world. In particular, with strong verifiers and\nfinancial incentives, agents for offensive cybersecurity are amenable to\niterative improvement by would-be adversaries. We argue that assessments should\ntake into account an expanded threat model in the context of cybersecurity,\nemphasizing the varying degrees of freedom that an adversary may possess in\nstateful and non-stateful environments within a fixed compute budget. We show\nthat even with a relatively small compute budget (8 H100 GPU Hours in our\nstudy), adversaries can improve an agent's cybersecurity capability on\nInterCode CTF by more than 40\\% relative to the baseline -- without any\nexternal assistance. These results highlight the need to evaluate agents'\ncybersecurity risk in a dynamic manner, painting a more representative picture\nof risk.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18384.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65319bd7f85995389d4f019c",
      "avatarUrl": "/avatars/657858b8435b220c9a29918c0dae9c6d.svg",
      "fullname": "Boyi Wei",
      "name": "boyiwei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16312",
      "authors": [
        {
          "_id": "6830109ea20ebb4738e76931",
          "user": {
            "_id": "6747d38098fe79433a8c4580",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/BrcsTfusqfu9p9uv1NeX6.png",
            "isPro": false,
            "fullname": "Jiawei Liu",
            "user": "Jiawei1222",
            "type": "user"
          },
          "name": "Jiawei Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:15:09.111Z",
          "hidden": false
        },
        {
          "_id": "6830109ea20ebb4738e76932",
          "name": "Qisi Chen",
          "hidden": false
        },
        {
          "_id": "6830109ea20ebb4738e76933",
          "name": "Jianshu Zhang",
          "hidden": false
        },
        {
          "_id": "6830109ea20ebb4738e76934",
          "name": "Quan Liu",
          "hidden": false
        },
        {
          "_id": "6830109ea20ebb4738e76935",
          "name": "Defu Lian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T07:07:43.000Z",
      "submittedOnDailyAt": "2025-05-27T05:04:30.217Z",
      "title": "EquivPruner: Optimización de la búsqueda basada en modelos de biblioteca y mejora de la consulta para el pruning de acciones",
      "submittedOnDailyBy": {
        "_id": "6747d38098fe79433a8c4580",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/BrcsTfusqfu9p9uv1NeX6.png",
        "isPro": false,
        "fullname": "Jiawei Liu",
        "user": "Jiawei1222",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) superan los algoritmos de exploración por complejos motivos pero, la estrategia actual implica explorar pasos significativamente equivalentes de manera ineficiente, lo que incrementa significativamente el consumo de etiquetas. Los métodos de similitud semántica existentes tienen dificultades para reconocer la equivalencia exacta en contextos específicos como la matemática o la lógica matemática. En este sentido, proponemos EquivPruner. EquivPruner es un enfoque sencillo y efectivo para identificar y eliminar acciones semánticamente equivalentes en la exploración de razonamiento de un LLM. Además, hemos generado por primera vez el dataset MathEquiv para entrenar un detector de equivalencia en expresiones matemáticas. Esto permite el entrenamiento de un detector de equivalencia ligero. La validación en una amplia gama de modelos y tareas demuestra que EquivPruner reduce significativamente el consumo de etiquetas, mejora la eficiencia de la exploración y fortalece la precisión de la razón. Por ejemplo, cuando se aplica a Qwen2.5-Math-7B-Instruct en GSM8K, EquivPruner reduce el consumo de etiquetas en un 48.1% y mejora la precisión. Nuestro código está disponible en https://github.com/Lolo1222/EquivPruner.",
      "upvotes": 2,
      "discussionId": "6830109fa20ebb4738e769a3",
      "githubRepo": "https://github.com/Lolo1222/EquivPruner",
      "ai_summary": "EquivPruner reduces token consumption and improves reasoning accuracy by pruning semantically equivalent actions in LLM searches, leveraging a new dataset for mathematical equivalence.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "semantic similarity",
        "semantically equivalent actions",
        "EquivPruner",
        "MathEquiv",
        "equivalence detector",
        "GSM8K"
      ]
    },
    "publishedAt": "2025-05-22T03:07:43.000Z",
    "title": "EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via\n  Action Pruning",
    "summary": "Large Language Models (LLMs) excel at complex reasoning through search\nalgorithms, yet current strategies often suffer from massive token consumption\ndue to redundant exploration of semantically equivalent steps. Existing\nsemantic similarity methods struggle to accurately identify such equivalence in\ndomain-specific contexts like mathematical reasoning. To address this, we\npropose EquivPruner, a simple yet effective approach that identifies and prunes\nsemantically equivalent actions during LLM reasoning search. We also introduce\nMathEquiv, the first dataset we created for mathematical statement equivalence,\nwhich enables the training of a lightweight equivalence detector. Extensive\nexperiments across various models and tasks demonstrate that EquivPruner\nsignificantly reduces token consumption, improving searching efficiency and\noften bolstering reasoning accuracy. For instance, when applied to\nQwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by\n48.1\\% while also improving accuracy. Our code is available at\nhttps://github.com/Lolo1222/EquivPruner.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16312.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6747d38098fe79433a8c4580",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/BrcsTfusqfu9p9uv1NeX6.png",
      "fullname": "Jiawei Liu",
      "name": "Jiawei1222",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19800",
      "authors": [
        {
          "_id": "68356e736bb42c7e99d2d266",
          "user": {
            "_id": "5f04bd384ec31d33a72116d1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1594145966049-noauth.jpeg",
            "isPro": false,
            "fullname": "Zaid Alyafeai",
            "user": "Zaid",
            "type": "user"
          },
          "name": "Zaid Alyafeai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T08:47:10.858Z",
          "hidden": false
        },
        {
          "_id": "68356e736bb42c7e99d2d267",
          "name": "Maged S. Al-Shaibani",
          "hidden": false
        },
        {
          "_id": "68356e736bb42c7e99d2d268",
          "name": "Bernard Ghanem",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T10:31:26.000Z",
      "submittedOnDailyAt": "2025-05-27T07:09:18.429Z",
      "title": "\"No se: Modelos de lenguaje basados en LLMs utilizando extracción y validación de bases de datos en artículos científicos\"",
      "submittedOnDailyBy": {
        "_id": "5f04bd384ec31d33a72116d1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1594145966049-noauth.jpeg",
        "isPro": false,
        "fullname": "Zaid Alyafeai",
        "user": "Zaid",
        "type": "user"
      },
      "summary": "La extracción de metadatos es crucial para la clasificación y almacenamiento de conjuntos de datos, y es necesaria para fomentar la efectividad en la investigación científica y su reproducibilidad. En el contexto del crecimiento exponencial de la investigación científica, este aspecto es particularmente importante. Además, Masader (Alyafeai et al., 2021) estableció una base fundamental para extraer amplias propiedades de metadatos de artículos académicos de conjuntos de datos de lenguas árabes, pero dependía de manualidad. En este artículo, se propone el framework MOLE para que se puedan extraer automáticamente propiedades de metadatos de artículos científicos que incluyen conjuntos de datos de otros idiomas, utilizando modelos de lenguaje grandes (LLMs). Se introduce una estructura de validación fuerte para procesar documentos de múltiples formatos y garantizar un output consistente. Además, se introduce un nuevo benchmark para evaluar el avance de esta tarea. A través de un análisis sistemático de la longitud de contexto, el aprendizaje con pocos ejemplos y la integración con el navegador, se demuestra que los actuales LLMs no muestran los resultados deseados para la automatización de esta tarea, lo que subraya la necesidad de mejoras futuras. Se proporciona el código para la comunidad de investigadores: https://github.com/IVUL-KAUST/MOLE, y se proporciona el conjunto de datos: https://huggingface.co/datasets/IVUL-KAUST/MOLE.",
      "upvotes": 1,
      "discussionId": "68356e746bb42c7e99d2d2af",
      "projectPage": "https://ivul-kaust.github.io/MOLE/blog",
      "githubRepo": "https://github.com/IVUL-KAUST/MOLE",
      "ai_summary": "A framework leveraging Large Language Models (LLMs) is introduced for automatic metadata extraction from scientific papers, with a focus on datasets from languages other than Arabic.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "schema-driven methodology",
        "benchmark",
        "few-shot learning",
        "web browsing integration"
      ]
    },
    "publishedAt": "2025-05-26T06:31:26.000Z",
    "title": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs",
    "summary": "Metadata extraction is essential for cataloging and preserving datasets,\nenabling effective research discovery and reproducibility, especially given the\ncurrent exponential growth in scientific research. While Masader (Alyafeai et\nal.,2021) laid the groundwork for extracting a wide range of metadata\nattributes from Arabic NLP datasets' scholarly articles, it relies heavily on\nmanual annotation. In this paper, we present MOLE, a framework that leverages\nLarge Language Models (LLMs) to automatically extract metadata attributes from\nscientific papers covering datasets of languages other than Arabic. Our\nschema-driven methodology processes entire documents across multiple input\nformats and incorporates robust validation mechanisms for consistent output.\nAdditionally, we introduce a new benchmark to evaluate the research progress on\nthis task. Through systematic analysis of context length, few-shot learning,\nand web browsing integration, we demonstrate that modern LLMs show promising\nresults in automating this task, highlighting the need for further future work\nimprovements to ensure consistent and reliable performance. We release the\ncode: https://github.com/IVUL-KAUST/MOLE and dataset:\nhttps://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19800.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f04bd384ec31d33a72116d1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1594145966049-noauth.jpeg",
      "fullname": "Zaid Alyafeai",
      "name": "Zaid",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 48
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19056",
      "authors": [
        {
          "_id": "68357c8ef0b7aba41a858b61",
          "name": "Harethah Abu Shairah",
          "hidden": false
        },
        {
          "_id": "68357c8ef0b7aba41a858b62",
          "name": "Hasan Abed Al Kader Hammoud",
          "hidden": false
        },
        {
          "_id": "68357c8ef0b7aba41a858b63",
          "name": "Bernard Ghanem",
          "hidden": false
        },
        {
          "_id": "68357c8ef0b7aba41a858b64",
          "name": "George Turkiyyah",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T09:18:24.000Z",
      "submittedOnDailyAt": "2025-05-27T07:21:18.247Z",
      "title": "「Defensas Sencillas contra los Ataques de LLM」",
      "submittedOnDailyBy": {
        "_id": "642b51385bf2355d02a23d15",
        "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
        "isPro": true,
        "fullname": "Hasan Abed Al Kader Hammoud",
        "user": "hammh0a",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) generalmente responden a instrucciones potencialmente dañinas siguiendo las guías de seguridad. En los ataques recientes, se ha identificado que el ataque llamado \"abliteration\" separa y restringe una sola dirección potencial, lo que ha sido responsable de la mayoría de las negaciones. Proponemos medidas de defensa para cambiar la forma en que los modelos generan negaciones. Hemos construido un conjunto de datos que explica por qué se negaron a respuestas completas y a mensajes potencialmente dañinos. Luego, utilizamos este conjunto de datos de negación para fines de aprendizaje para ajustar la versión de Llama-2-7B-Chat y Qwen2.5-Instruct (con 150M y 3B parámetros), y evaluamos los resultados con un conjunto de mensajes potencialmente dañinos. En nuestros experimentos, los modelos de negación expandidos mantienen una tasa de negación alta, con un descenso máximo del 10%, mientras que la tasa de negación de los modelos de referencia disminuye hasta el 70-80% después del abliteration. Según una evaluación amplia de seguridad y utilidad, el ajuste de los modelos de negación expandidos neutraliza el ataque abliteration y mantiene un rendimiento general.",
      "upvotes": 1,
      "discussionId": "68357c8ff0b7aba41a858b96",
      "ai_summary": "Modifying models to generate justified refusals through fine-tuning on an extended-refusal dataset mitigates ablation attacks while maintaining high refusal rates and general performance.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "ablation",
        "latent direction",
        "refusal behavior",
        "extended-refusal dataset",
        "Llama-2-7B-Chat",
        "Qwen2.5-Instruct",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-05-25T05:18:24.000Z",
    "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks",
    "summary": "Large language models (LLMs) are typically aligned to comply with safety\nguidelines by refusing harmful instructions. A recent attack, termed\nabliteration, isolates and suppresses the single latent direction most\nresponsible for refusal behavior, enabling the model to generate unethical\ncontent. We propose a defense that modifies how models generate refusals. We\nconstruct an extended-refusal dataset that contains harmful prompts with a full\nresponse that justifies the reason for refusal. We then fine-tune\nLlama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our\nextended-refusal dataset, and evaluate the resulting systems on a set of\nharmful prompts. In our experiments, extended-refusal models maintain high\nrefusal rates, dropping at most by 10%, whereas baseline models' refusal rates\ndrop by 70-80% after abliteration. A broad evaluation of safety and utility\nshows that extended-refusal fine-tuning neutralizes the abliteration attack\nwhile preserving general performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19056.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642b51385bf2355d02a23d15",
      "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
      "fullname": "Hasan Abed Al Kader Hammoud",
      "name": "hammh0a",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18323",
      "authors": [
        {
          "_id": "683573bc0830dfc67834f1b5",
          "name": "Nicolas Küchler",
          "hidden": false
        },
        {
          "_id": "683573bc0830dfc67834f1b6",
          "name": "Ivan Petrov",
          "hidden": false
        },
        {
          "_id": "683573bc0830dfc67834f1b7",
          "name": "Conrad Grobler",
          "hidden": false
        },
        {
          "_id": "683573bc0830dfc67834f1b8",
          "name": "Ilia Shumailov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T19:28:45.000Z",
      "submittedOnDailyAt": "2025-05-27T06:44:57.820Z",
      "title": "Basado en la arquitectura, mantenimiento de datos de ubicación de folios de borrador y operaciones de inferencia de modelos",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "En los últimos 10 años, la academia ha enfocado su atención en el \"backdoor\" de los redes neuronales, principalmente observando cómo los adversarios manipulan las predicciones del modelo. Estas ataques de cambio de predicción tienen un propósito malicioso claro, pero su impacto real no era claro. En este artículo, basado en los últimos desarrollos en la arquitectura de redes neuronales, proponemos una nueva clase más potente de \"backdoor\", que utiliza la arquitectura para manipular y dañar datos de usuarios en gran escala. Esta arquitectura permite la información de los usuarios que se procesan en conjuntos, lo que promueve la información de los otros usuarios en la misma instancia y permite a los atacantes controlar completamente la respuesta del modelo a otros usuarios en la misma instancia. Por lo tanto, un atacante que puede modificar la arquitectura del modelo puede configurar los entradas y salidas de otros usuarios en la misma instancia para causar daños. Estos ataques requieren la atención y son realmente amenazantes, pues pueden ser fácilmente introducidos en modelos actuales y representan una amenaza real a la privacidad de los usuarios y la estabilidad del sistema. Un punto clave es que para enfrentar estas nuevas vulnerabilidades, proponemos una estrategia de respuesta basada en la análisis del grafo del modelo y la implementación de una nueva estructura de control de flujo de información que asegure la no interferencia entre los usuarios en la misma instancia. Con esta estrategia, hemos realizado un análisis de gran escala en modelos hospedados en Hugging Face y hemos encontrado que más de 200 modelos pueden causar información de datos entre instancias.",
      "upvotes": 1,
      "discussionId": "683573bc0830dfc67834f212",
      "ai_summary": "A novel class of backdoors in neural network architectures exploits batched inference to enable large-scale data manipulation, demonstrating information leakage and control over user inputs and outputs, with a proposed mitigation strategy using Information Flow Control.",
      "ai_keywords": [
        "backdoors",
        "neural networks",
        "classification tasks",
        "batched inference",
        "hardware utilization",
        "information leakage",
        "mitagation strategy",
        "Information Flow Control",
        "Hugging Face",
        "dynamic quantization"
      ]
    },
    "publishedAt": "2025-05-23T15:28:45.000Z",
    "title": "Architectural Backdoors for Within-Batch Data Stealing and Model\n  Inference Manipulation",
    "summary": "For nearly a decade the academic community has investigated backdoors in\nneural networks, primarily focusing on classification tasks where adversaries\nmanipulate the model prediction. While demonstrably malicious, the immediate\nreal-world impact of such prediction-altering attacks has remained unclear. In\nthis paper we introduce a novel and significantly more potent class of\nbackdoors that builds upon recent advancements in architectural backdoors. We\ndemonstrate how these backdoors can be specifically engineered to exploit\nbatched inference, a common technique for hardware utilization, enabling\nlarge-scale user data manipulation and theft. By targeting the batching\nprocess, these architectural backdoors facilitate information leakage between\nconcurrent user requests and allow attackers to fully control model responses\ndirected at other users within the same batch. In other words, an attacker who\ncan change the model architecture can set and steal model inputs and outputs of\nother users within the same batch. We show that such attacks are not only\nfeasible but also alarmingly effective, can be readily injected into prevalent\nmodel architectures, and represent a truly malicious threat to user privacy and\nsystem integrity. Critically, to counteract this new class of vulnerabilities,\nwe propose a deterministic mitigation strategy that provides formal guarantees\nagainst this new attack vector, unlike prior work that relied on Large Language\nModels to find the backdoors. Our mitigation strategy employs a novel\nInformation Flow Control mechanism that analyzes the model graph and proves\nnon-interference between different user inputs within the same batch. Using our\nmitigation strategy we perform a large scale analysis of models hosted through\nHugging Face and find over 200 models that introduce (unintended) information\nleakage between batch entries due to the use of dynamic quantization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18323.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15957",
      "authors": [
        {
          "_id": "6830745670e219f5de8ad360",
          "user": {
            "_id": "646fa3016441111304fec68d",
            "avatarUrl": "/avatars/923629340f3785ae8c6e52cf3674d5c2.svg",
            "isPro": false,
            "fullname": "Chih-Kai Yang",
            "user": "zenyn",
            "type": "user"
          },
          "name": "Chih-Kai Yang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T13:21:28.397Z",
          "hidden": false
        },
        {
          "_id": "6830745670e219f5de8ad361",
          "name": "Neo S. Ho",
          "hidden": false
        },
        {
          "_id": "6830745670e219f5de8ad362",
          "name": "Hung-yi Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T19:17:29.000Z",
      "submittedOnDailyAt": "2025-05-27T01:41:21.824Z",
      "title": "Evaluación histórica de modelos de voz-lenguaje: Investigación consistente",
      "submittedOnDailyBy": {
        "_id": "646fa3016441111304fec68d",
        "avatarUrl": "/avatars/923629340f3785ae8c6e52cf3674d5c2.svg",
        "isPro": false,
        "fullname": "Chih-Kai Yang",
        "user": "zenyn",
        "type": "user"
      },
      "summary": "El desarrollo de los modelos de lenguaje de voz grande (LALM) espera que los modelos que añaden habilidades de voz demostren un excelente rendimiento en diversas tareas de voz. Para evaluar estos modelos, varios benchmarks han sido presentados, pero estos son separados y no tienen una estructura organizada. Para remediar esto, hemos realizado una investigación detallada y proponemos una tecnología sistemática adecuada para la evaluación de LALM. Esta tecnología se clasifica en cuatro dimensiones según el propósito: (1) reconocimiento y procesamiento general de voz, (2) conocimiento y lógica, (3) capacidades diafóricas, (4) equidad, seguridad y confianza. Se proporcionan resúmenes detallados en cada categoría para clarificar los problemas y dar consejos sobre las posibilidades futuras. Debido a nuestras limitaciones de conocimiento, esto es la primera investigación especializada en la evaluación de LALM y proporciona a la comunidad una guía clara. Publicamos el conjunto de artículos investigados y continuamos actualizando para apoyar el desarrollo de esta área.",
      "upvotes": 1,
      "discussionId": "6830745770e219f5de8ad38b",
      "projectPage": "https://github.com/ckyang1124/LALM-Evaluation-Survey",
      "githubRepo": "https://github.com/ckyang1124/LALM-Evaluation-Survey",
      "ai_summary": "A survey proposes a systematic taxonomy for evaluating large audio-language models across dimensions including auditory awareness, knowledge reasoning, dialogue ability, and fairness, to address fragmented benchmarks in the field.",
      "ai_keywords": [
        "large audio-language models",
        "LALMs",
        "large language models",
        "LLMs",
        "auditory capabilities",
        "general auditory awareness",
        "processing",
        "knowledge and reasoning",
        "dialogue-oriented ability",
        "fairness",
        "safety",
        "trustworthiness",
        "taxonomy",
        "evaluations",
        "benchmark",
        "survey",
        "guidelines"
      ]
    },
    "publishedAt": "2025-05-21T15:17:29.000Z",
    "title": "Towards Holistic Evaluation of Large Audio-Language Models: A\n  Comprehensive Survey",
    "summary": "With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15957.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646fa3016441111304fec68d",
      "avatarUrl": "/avatars/923629340f3785ae8c6e52cf3674d5c2.svg",
      "fullname": "Chih-Kai Yang",
      "name": "zenyn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]