[
  {
    "paper": {
      "id": "2506.06751",
      "authors": [
        {
          "_id": "6848fecf42e4f9106973f315",
          "user": {
            "_id": "62bd6c6baaf1480f1aa2222e",
            "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
            "isPro": false,
            "fullname": "Mikhail Salnikov",
            "user": "msalnikov",
            "type": "user"
          },
          "name": "Mikhail Salnikov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:34:47.630Z",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f316",
          "name": "Dmitrii Korzh",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f317",
          "user": {
            "_id": "657c4a8dfb0285d857d86e4c",
            "avatarUrl": "/avatars/17635a4c2c804dd3837ae01833bb940d.svg",
            "isPro": false,
            "fullname": "Ivan",
            "user": "IvanLazichny",
            "type": "user"
          },
          "name": "Ivan Lazichny",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-11T09:26:44.876Z",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f318",
          "name": "Elvir Karimov",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f319",
          "name": "Artyom Iudin",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31a",
          "name": "Ivan Oseledets",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31b",
          "name": "Oleg Y. Rogov",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31c",
          "user": {
            "_id": "605473729d7c1d4d81b7e52b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662046050710-605473729d7c1d4d81b7e52b.jpeg",
            "isPro": false,
            "fullname": "Alexander Panchenko",
            "user": "apanc",
            "type": "user"
          },
          "name": "Alexander Panchenko",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-11T09:27:06.218Z",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31d",
          "name": "Natalia Loukachevitch",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31e",
          "user": {
            "_id": "662f8d645c4db70c77a203b0",
            "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
            "isPro": false,
            "fullname": "Elena Tutubalina",
            "user": "tlenusik",
            "type": "user"
          },
          "name": "Elena Tutubalina",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-11T09:26:55.840Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-07T10:45:17.000Z",
      "submittedOnDailyAt": "2025-06-11T02:53:12.616Z",
      "title": "El prejuicio en la política internacional: los modelos de lenguaje modernos de \"buenas\" y \"malas\" naciones",
      "submittedOnDailyBy": {
        "_id": "62bd6c6baaf1480f1aa2222e",
        "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
        "isPro": false,
        "fullname": "Mikhail Salnikov",
        "user": "msalnikov",
        "type": "user"
      },
      "summary": "Este artículo evalúa las diferencias de visión internacional en la interpretación de eventos históricos (Estados Unidos, Reino Unido, Unión Estados Unidos-China, China) para evaluar la inclinación política local de los Modelos de Lenguaje de Gran Tamaño (LLMs). Presentamos un nuevo conjunto de datos que incluye explicaciones neutrales de los eventos y perspectivas diferentes de cada país. Nuestros hallazgos muestran una gran inclinación política local y confirman que los modelos tratan de manera más amigable la representación narrativa de ciertas naciones. Además, un simple prompt de control de sesgos puede reducir estas inclinaciones. Experimentos donde se manipulan los etiquetadores de los participantes demuestran la sensibilidad del modelo a las características y confirman que los modelos pueden ampliar o reconocer perspectivas inadecuadas. Este estudio claramente demuestra la inclinación narrativa política de los países en los LLMs, duda sobre el efecto de los métodos simples de control de sesgos y proporciona un marco y conjunto de datos para futuras investigaciones sobre la inclinación política local.",
      "upvotes": 34,
      "discussionId": "6848fed042e4f9106973f31f",
      "projectPage": "https://airi-institute.github.io/geopolitical_llm_bias",
      "githubRepo": "https://github.com/AIRI-Institute/geopolitical_llm_bias",
      "ai_summary": "LLMs exhibit significant geopolitical biases in their interpretation of historical events, and simple debiasing methods have limited effectiveness; a novel dataset for further research is provided.",
      "ai_keywords": [
        "LLMs",
        "geopolitical biases",
        "historical events",
        "national narratives",
        "debiasing prompts"
      ]
    },
    "publishedAt": "2025-06-07T06:45:17.000Z",
    "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models",
    "summary": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bd6c6baaf1480f1aa2222e",
      "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
      "fullname": "Mikhail Salnikov",
      "name": "msalnikov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08672",
      "authors": [
        {
          "_id": "684936e842e4f9106973f45e",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "684936e842e4f9106973f45f",
          "name": "Jiaqi Li",
          "hidden": false
        },
        {
          "_id": "684936e842e4f9106973f460",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-11T07:57:29.119Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T10:31:21.000Z",
      "submittedOnDailyAt": "2025-06-11T07:49:21.042Z",
      "title": "Rul Resíguar: Inferencia de Reglas Basada en Reino Dinámicamente Sampling del Dominio",
      "submittedOnDailyBy": {
        "_id": "63a95a6a7930fa8c7dd63d4e",
        "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
        "isPro": false,
        "fullname": "Zilong Zheng",
        "user": "zlzheng",
        "type": "user"
      },
      "summary": "La inferencia basada en reglas se reconoce como uno de los problemas fundamentales de la base lógica, pero en aplicaciones reales, la inclinación hacia el formato, la clase y la complejidad de las reglas representa un desafío estricto. Según recientes estudios, modelos lógicos de gran escala (LRMs) han demostrado un poder lógico sorprendente, y este rendimiento ha sido significativamente mejorado mediante aprendizaje por refuerzo (RL). Sin embargo, la capacidad de modelos lógicos de pequeña escala (SRMs) para aprender inferencias basadas en reglas de fuerte generalización en diversas tareas y campos es un problema que suscita debate. En respuesta a este desafío, presentamos una metodología sencilla y efectiva para inferencia basada en reglas reforzada, llamada RuleReasoner. Este método permite realizar inferencias basadas en reglas en diferentes tareas modificadas mediante una aproximación dinámica de muestreo en nuevos campos. En particular, RuleReasoner actualiza los pesos de muestreo en diferentes campos basándose en recompensas históricas, lo que permite expandir los campos y implementar un aprendizaje en línea flexible de RL. Esto elimina la necesidad de recetas de entrenamiento híbrida previamente humanamente diseñadas en métodos existentes. Las evaluaciones experimentales en marcos de referencia de distribución dentro (ID) y fuera de distribución (OOD) muestran que RuleReasoner supera significativamente a los LRMs avanzados (Delta promedio de 4.1% en 8 tareas ID, y 10.4% en 3 tareas OOD). En particular, nuestro enfoque muestra una mayor eficiencia computacional en comparación con métodos dinámicos de muestreo anteriores de RL.",
      "upvotes": 15,
      "discussionId": "684936e842e4f9106973f461",
      "githubRepo": "https://github.com/bigai-nlco/RuleReasoner",
      "ai_summary": "RuleReasoner enhances rule-based reasoning in small models through dynamic domain sampling, achieving superior performance and efficiency compared to large models.",
      "ai_keywords": [
        "reinforcement learning",
        "rule-based reasoning",
        "large reasoning models",
        "small reasoning models",
        "domain-aware dynamic sampling",
        "historical rewards",
        "in-distribution",
        "out-of-distribution",
        "computational efficiency"
      ]
    },
    "publishedAt": "2025-06-10T06:31:21.000Z",
    "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic\n  Sampling",
    "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin (Delta4.1%\naverage points on eight ID tasks and Delta10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a95a6a7930fa8c7dd63d4e",
      "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
      "fullname": "Zilong Zheng",
      "name": "zlzheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09040",
      "authors": [
        {
          "_id": "6848fff842e4f9106973f321",
          "name": "Dianyi Wang",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f322",
          "name": "Wei Song",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f323",
          "name": "Yikun Wang",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f324",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f325",
          "name": "Kaicheng Yu",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f326",
          "name": "Zhongyu Wei",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f327",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:57:50.000Z",
      "submittedOnDailyAt": "2025-06-11T05:55:58.221Z",
      "title": "La reconstrucción visual significativa de la regresión automática ayuda a mejorar la comprensión de los VLMs.",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "Los modelos grandes de lenguaje visual estándar (LVLMs) aplican solo soporte para la auto-reconstrucción de secuencias de texto y no integran completamente el modelo visual en el proceso de entrenamiento. Esto genera tres limitaciones principales: 1) la capacidad de uso de imágenes no existe cuando no se capturan, 2) el riesgo de omitir detalles visuales importantes en la captura, y 3) que el contenido visual puede ser más precisamente transmitido en lenguaje es un desafío. Por lo tanto, los LVLMs actuales priorizan la armonización de la lenguaje a partir de la visión, pero tienen la posibilidad de dejar de lado informaciones visuales detalladas. En contraste, los estudios previos han investigado la generación de imágenes por auto-reconstrucción y se han desarrollado desafíos para mejorar la comprensión de imágenes utilizando el soporte de auto-reconstrucción visual efectivomente. En este artículo, se presenta la auto-reconstrucción de lenguaje y visión (ASVR) y se describe cómo se pueden entrenar modelos de visión y lenguaje en un solo marco de auto-reconstrucción. La auto-reconstrucción de la apariencia original de la imagen no mejora la comprensión de varios tipos de imágenes, pero la auto-reconstrucción de la representación visual sí mejora la comprensión. Específicamente, cuando el modelo recibe características continuas de imágenes como entrada, puede reconstruir efectivamente estas características como tokens de significado dispersos, lo que se ha confirmado en mejoras estables en diferentes marcos de evaluación de tipos. Nuestro enfoque obtiene un gran mejoramiento en rendimiento, independientemente del tamaño de datos (556k-2M) y de la variedad de modelos de lenguaje llamados a servicio (LLM). En particular, ASVR mejora en un promedio de 5% en 14 marcos de evaluación de tipos respecto a LLaVA-1.5. El código está disponible en https://github.com/AlenjandroWang/ASVR.",
      "upvotes": 12,
      "discussionId": "6848fff842e4f9106973f328",
      "ai_summary": "Autoregressive Semantic Visual Reconstruction (ASVR) improves multimodal understanding by focusing on semantic reconstruction rather than raw visual appearance, enhancing performance across various benchmarks.",
      "ai_keywords": [
        "autoregressive supervision",
        "large vision-language models (LVLMs)",
        "visual modality",
        "image captions",
        "autoregressive image generation",
        "multimodal learning",
        "semantic representation",
        "discrete semantic tokens",
        "multimodal understanding benchmarks",
        "LLaVA-1.5"
      ]
    },
    "publishedAt": "2025-06-10T13:57:50.000Z",
    "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better",
    "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09040.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08009",
      "authors": [
        {
          "_id": "68485e5b4fe3b60e21b258bd",
          "name": "Xun Huang",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258be",
          "name": "Zhengqi Li",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258bf",
          "user": {
            "_id": "67492ee82ad3cfc108a41bbb",
            "avatarUrl": "/avatars/7ad03e55a8791c62f1271a5c9bf8cc60.svg",
            "isPro": false,
            "fullname": "Guande He",
            "user": "gdhe17",
            "type": "user"
          },
          "name": "Guande He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:40.950Z",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258c0",
          "name": "Mingyuan Zhou",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258c1",
          "name": "Eli Shechtman",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67492ee82ad3cfc108a41bbb/bEQLc--MCz7a-4ZBIBbaJ.mp4"
      ],
      "publishedAt": "2025-06-09T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-11T04:34:32.742Z",
      "title": "Auto-Reconstructing Video Diffusers Through the Lens of Overfitting Test-to-Test Wrapping",
      "submittedOnDailyBy": {
        "_id": "67492ee82ad3cfc108a41bbb",
        "avatarUrl": "/avatars/7ad03e55a8791c62f1271a5c9bf8cc60.svg",
        "isPro": false,
        "fullname": "Guande He",
        "user": "gdhe17",
        "type": "user"
      },
      "summary": "Self Forcing introduce una nueva paradigma de entrenamiento para modelos de difusores de video que generan automáticamente el siguiente frame. Este enfoque resuelve el problema de generar secuencias basadas en su propio output incompleto durante la inferencia, y aborda problemas de entrenamiento que duran mucho tiempo debido al contexto real. En comparación con métodos anteriores, en lugar de denoising futuros frames basados en contextos reales, Self Forcing utiliza un cache de KV para ejecutar rollouts automáticos durante el entrenamiento y genera cada frame basándose en el output generado en el pasado. Esta estrategia permite controlar el entrenamiento a nivel de video a través de un pérdida general y evaluar directamente la calidad de las secuencias generadas. Para garantizar la eficiencia del entrenamiento, se utilizan modelos de difusores con pocos pasos y la trimatización de tokens de random gandia en español, lo que equilibra efectivamente los costos computacionales y la eficiencia. Además, para ejecutar rollouts automáticos de manera eficiente, se introduce una estructura de cache KV. A través de experimentos extendidos, nuestro enfoque ha permitido la generación de videos en tiempo real, satisfaciendo la calidad de generación en segundos de latín en una sola tarjeta de procesador, sin mejorar en velocidad y superando la calidad de generación de los modelos difusores básicos. Sitio web del proyecto: http://self-forcing.github.io/",
      "upvotes": 9,
      "discussionId": "68485e5b4fe3b60e21b258c2",
      "projectPage": "https://self-forcing.github.io/",
      "githubRepo": "https://github.com/guandeh17/Self-Forcing",
      "ai_summary": "Self Forcing, a novel training method for autoregressive video diffusion models, reduces exposure bias and improves generation quality through holistic video-level supervision and efficient caching mechanisms.",
      "ai_keywords": [
        "Self Forcing",
        "autoregressive video diffusion models",
        "exposure bias",
        "denoising",
        "key-value (KV) caching",
        "autoregressive rollout",
        "holistic loss",
        "few-step diffusion model",
        "stochastic gradient truncation",
        "rolling KV cache mechanism",
        "video extrapolation"
      ]
    },
    "publishedAt": "2025-06-09T13:59:55.000Z",
    "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
    "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67492ee82ad3cfc108a41bbb/bEQLc--MCz7a-4ZBIBbaJ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67492ee82ad3cfc108a41bbb",
      "avatarUrl": "/avatars/7ad03e55a8791c62f1271a5c9bf8cc60.svg",
      "fullname": "Guande He",
      "name": "gdhe17",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07927",
      "authors": [
        {
          "_id": "684794003ec10bdd8ab4de11",
          "name": "Jiayi Sheng",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de12",
          "name": "Luna Lyu",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de13",
          "name": "Jikai Jin",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de14",
          "name": "Tony Xia",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de15",
          "name": "Alex Gu",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de16",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de17",
          "name": "Pan Lu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60f5f68fa7fd83d025749234/ahvR-ZmwDrUNm3-jcQ4o1.png"
      ],
      "publishedAt": "2025-06-09T16:43:38.000Z",
      "submittedOnDailyAt": "2025-06-11T04:15:25.994Z",
      "title": "Metodología de Demostración de Desigualdades utilizando Modelos de Lenguaje de Gran Escala",
      "submittedOnDailyBy": {
        "_id": "60f5f68fa7fd83d025749234",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
        "isPro": true,
        "fullname": "Pan Lu",
        "user": "lupantech",
        "type": "user"
      },
      "summary": "La prova de desigualtats es una área importante en diversos campos científicos y matemáticos, que evalúa la capacidad de inferencia alta a través de la detección de fronteras estrictas y la aplicación de teoremas estratégicos. Por lo tanto, los modelos de lenguaje de gran tamaño (LLMs) ofrecen mayores profundidades de comprensión que solucionan problemas matemáticos generales, especialmente en la configuración de fronteras difíciles. El desarrollo de esta área está limitado por conjuntos de datos raros, sintéticos o con formaciones estrictas. Para resolver esto, proponemos la formación de tareas no formales pero provablemente, reconstruyendo la prova de desigualtats en dos sub-tareas automáticamente verificables: estimación de fronteras y predicción de relaciones. Basándonos en esto, lanzamos el conjunto de datos \"IneqMath\", un conjunto de datos de alto nivel de Olimpiada de desigualtats personalizado. Este conjunto de datos incluye conjuntos de prueba y corpus de aprendizaje con soluciones paso a paso y resumenes. Además, desarrollamos un nuevo marco de evaluación \"LLM-as-judge\" que combina un juez de respuestas finales con un juez de cuatro etapas para detectar errores en la inferencia general. Los resultados sistemáticos de 29 modelos avanzados de LLM en \"IneqMath\" revelan sorprendentes hechos: por ejemplo, modelos como o1 alcanzan una precisión general inferior a 10% debido a la revisión paso a paso, lo que representa una pérdida del 65.5% en precisión comparativa con la consistencia de la respuesta final. Esta diferencia evidencia una falta crucial en los modelos de LLM para construir pruebas rigurosas. La expansión del tamaño del modelo y el aumento de la cantidad de cálculo en las pruebas tienen límites solo en la precisión de la prueba general, sin efecto significativo. En cambio, nuestra dirección de investigación muestra la posibilidad de inferencia guiada por teoremas y auto-revisión. Los códigos y datos están disponibles en https://ineqmath.github.io/.",
      "upvotes": 9,
      "discussionId": "684794013ec10bdd8ab4de18",
      "ai_summary": "The investigation into inequality proving using large language models uncovers significant challenges in constructing rigorous proofs, revealing gaps between finding answers and generating valid step-wise solutions.",
      "ai_keywords": [
        "LLMs",
        "IneqMath",
        "bound estimation",
        "relation prediction",
        "theorem-guided reasoning",
        "self-refinement"
      ]
    },
    "publishedAt": "2025-06-09T12:43:38.000Z",
    "title": "Solving Inequality Proofs with Large Language Models",
    "summary": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60f5f68fa7fd83d025749234/ahvR-ZmwDrUNm3-jcQ4o1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07927.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f5f68fa7fd83d025749234",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
      "fullname": "Pan Lu",
      "name": "lupantech",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04614",
      "authors": [
        {
          "_id": "684921e342e4f9106973f3e7",
          "name": "Yuyang Wanyan",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3e8",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3e9",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ea",
          "name": "Haowei Liu",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3eb",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ec",
          "name": "Jiabo Ye",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ed",
          "name": "Yutong Kou",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ee",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ef",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3f0",
          "name": "Xiaoshan Yang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3f1",
          "name": "Weiming Dong",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3f2",
          "name": "Changsheng Xu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/u6BK1EJr5-c6EUSfCkYqH.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/ySTNcOWpa_W2ZpbmkS50q.jpeg"
      ],
      "publishedAt": "2025-06-05T04:12:36.000Z",
      "submittedOnDailyAt": "2025-06-11T04:59:22.770Z",
      "title": "「Lo que el jugador puede ver al jugar: Diagnóstico de errores previos a la cirugía de la automatización de GUI mediante el modelo GUI-Critic-R1」",
      "submittedOnDailyBy": {
        "_id": "645b10e80c73ea27d13f7aca",
        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
        "isPro": false,
        "fullname": "xuhaiyang",
        "user": "xhyandwyy",
        "type": "user"
      },
      "summary": "Recientemente, los modelos de lenguaje multimodal (MLLMs) están siendo utilizados ampliamente en tareas de automatización de interfaz gráfica (GUI) y otras tareas lógicas. A diferencia de las tareas comunes de modelos en línea, la automatización de GUI se realiza en entornos interactivos en línea, donde las decisiones se toman de manera secuencial basadas en el estado real-time del entorno. Esta tarea tiene una baja tolerancia a errores en cada paso, ya que algunos errores pueden destruir continuamente el proceso, y existe la posibilidad de obtener resultados no mutables como eliminación o pago, por lo que se introduce una estructura de evaluación preliminar que proporcione retroalimentación válida antes de la ejecución real. En particular, se propone la construcción de un modelo de evaluación de interfaz gráfica (GUI-Critic-R1) que considera los resultados potenciales, basándose en las propuestas sugeridas, y se propone una estrategia de optimización de políticas relativas (S-GRPO). Además, se busca aumentar la confianza en el retroalimentación del modelo con la propuesta de nuevas recompensas. Además, se desarrolla un pipeline de recolección de datos basado en evidencia para resolver los defectos existentes en los datos de evaluación de GUI, y se crean GUI-Critic-Train y GUI-Critic-Test. Las pruebas estáticas en el dominio de móviles y web en GUI-Critic-Test muestran una gran ventaja en términos de precisión de evaluación comparada con los actuales MLLMs. La evaluación dinámica en el marco de referencia de automatización de GUI demuestra una mejoría en la tasa de éxito y la eficiencia de funcionamiento, lo que demuestra más claramente la eficacia y el excelente rendimiento del modelo.",
      "upvotes": 9,
      "discussionId": "684921e442e4f9106973f3f3",
      "githubRepo": "https://github.com/X-PLUG/MobileAgent",
      "ai_summary": "A pre-operative critic mechanism with Suggestion-aware Gradient Relative Policy Optimization enhances the reliability of multimodal reasoning tasks in GUI automation.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Suggestion-aware Gradient Relative Policy Optimization",
        "pre-operative critic mechanism",
        "reasoning-bootstrapping",
        "GUI automation",
        "GUI-Critic-R1",
        "GUI-Critic-Test",
        "GUI-Critic-Train"
      ]
    },
    "publishedAt": "2025-06-05T00:12:36.000Z",
    "title": "Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation",
    "summary": "In recent years, Multimodal Large Language Models (MLLMs) have been\nextensively utilized for multimodal reasoning tasks, including Graphical User\nInterface (GUI) automation. Unlike general offline multimodal tasks, GUI\nautomation is executed in online interactive environments, necessitating\nstep-by-step decision-making based on real-time status of the environment. This\ntask has a lower tolerance for decision-making errors at each step, as any\nmistakes may cumulatively disrupt the process and potentially lead to\nirreversible outcomes like deletions or payments. To address these issues, we\nintroduce a pre-operative critic mechanism that provides effective feedback\nprior to the actual execution, by reasoning about the potential outcome and\ncorrectness of actions. Specifically, we propose a Suggestion-aware Gradient\nRelative Policy Optimization (S-GRPO) strategy to construct our pre-operative\ncritic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance\nthe reliability of the model's feedback. Furthermore, we develop a\nreasoning-bootstrapping based data collection pipeline to create a\nGUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic\ndata. Static experiments on the GUI-Critic-Test across both mobile and web\ndomains reveal that our GUI-Critic-R1 offers significant advantages in critic\naccuracy compared to current MLLMs. Dynamic evaluation on GUI automation\nbenchmark further highlights the effectiveness and superiority of our model, as\nevidenced by improved success rates and operational efficiency.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/u6BK1EJr5-c6EUSfCkYqH.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/ySTNcOWpa_W2ZpbmkS50q.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08002",
      "authors": [
        {
          "_id": "6848f8c242e4f9106973f2f6",
          "name": "Aadarsh Sahoo",
          "hidden": false
        },
        {
          "_id": "6848f8c242e4f9106973f2f7",
          "name": "Vansh Tibrewal",
          "hidden": false
        },
        {
          "_id": "6848f8c242e4f9106973f2f8",
          "name": "Georgia Gkioxari",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/2iX6eNaXCKBiwTpAmSk2Z.qt",
        "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/EUswqANZ-bRURwRZ0sv3m.qt",
        "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/dLiQzQHFyFye4cIgj3ivo.png"
      ],
      "publishedAt": "2025-06-09T17:59:37.000Z",
      "submittedOnDailyAt": "2025-06-11T02:05:41.608Z",
      "title": "Texto, imágenes, estructuras 3D se corresponden token a token.",
      "submittedOnDailyBy": {
        "_id": "638e5fc6485360fbdfeb1301",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e5fc6485360fbdfeb1301/_8oqLMbkn_Ig-Jqa0fZCJ.png",
        "isPro": false,
        "fullname": "Aadarsh Sahoo",
        "user": "aadarsh99",
        "type": "user"
      },
      "summary": "Crear máquinas que comprendan la realidad 3D es una tarea importante que ayuda a los diseñadores a manejar ambientes y edición en 3D. Con el desarrollo de modelos de lenguaje y modelado de imágenes, es posible explorar la posibilidad de nuevos modelos estructurados de escenas 3D. Para ello, se propone un marco de trabajo de LLM que integra lenguaje, imágenes y escenas 3D, y ofrece una guía clara sobre las elecciones de diseño para lograr el mejor entrenamiento y rendimiento en temas como la representación de datos, la modelación de objetivos específicos y otras preguntas importantes. Se evalua el rendimiento en cuatro trabajos clave de 3D (renderización, reconocimiento, seguimiento de instrucciones, respuesta a preguntas) y cuatro conjuntos de datos de 3D (datos sintéticos y de la realidad). Se muestra el efecto del modelo en la tarea de reconocimiento de objetos 3D en la realidad añadiendo codificación de formas positivas. Página del proyecto: https://glab-caltech.github.io/kyvo/",
      "upvotes": 8,
      "discussionId": "6848f8c242e4f9106973f2f9",
      "projectPage": "https://glab-caltech.github.io/kyvo/",
      "githubRepo": "https://github.com/AadSah/kyvo",
      "ai_summary": "A unified language, image, and 3D scene model framework is proposed, achieving optimal training and performance across various 3D tasks and datasets.",
      "ai_keywords": [
        "autoregressive models",
        "LLM framework",
        "data representation",
        "modality-specific objectives",
        "3D rendering",
        "3D recognition",
        "instruction-following",
        "question-answering",
        "3D datasets",
        "quantized shape encodings"
      ]
    },
    "publishedAt": "2025-06-09T13:59:37.000Z",
    "title": "Aligning Text, Images, and 3D Structure Token-by-Token",
    "summary": "Creating machines capable of understanding the world in 3D is essential in\nassisting designers that build and edit 3D environments and robots navigating\nand interacting within a three-dimensional space. Inspired by advances in\nlanguage and image modeling, we investigate the potential of autoregressive\nmodels for a new modality: structured 3D scenes. To this end, we propose a\nunified LLM framework that aligns language, images, and 3D scenes and provide a\ndetailed ''cookbook'' outlining critical design choices for achieving optimal\ntraining and performance addressing key questions related to data\nrepresentation, modality-specific objectives, and more. We evaluate performance\nacross four core 3D tasks -- rendering, recognition, instruction-following, and\nquestion-answering -- and four 3D datasets, synthetic and real-world. We extend\nour approach to reconstruct complex 3D object shapes by enriching our 3D\nmodality with quantized shape encodings, and show our model's effectiveness on\nreal-world 3D object recognition tasks. Project webpage:\nhttps://glab-caltech.github.io/kyvo/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/2iX6eNaXCKBiwTpAmSk2Z.qt",
      "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/EUswqANZ-bRURwRZ0sv3m.qt",
      "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/dLiQzQHFyFye4cIgj3ivo.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08002.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638e5fc6485360fbdfeb1301",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e5fc6485360fbdfeb1301/_8oqLMbkn_Ig-Jqa0fZCJ.png",
      "fullname": "Aadarsh Sahoo",
      "name": "aadarsh99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07177",
      "authors": [
        {
          "_id": "6849036342e4f9106973f32a",
          "name": "Sangwon Jang",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32b",
          "user": {
            "_id": "66b57c77778c98d29446c8ec",
            "avatarUrl": "/avatars/63a7da38ee3808858f0f786a3a4a8dae.svg",
            "isPro": false,
            "fullname": "Taekyung Ki",
            "user": "tkkitkki",
            "type": "user"
          },
          "name": "Taekyung Ki",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:34:42.228Z",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32c",
          "name": "Jaehyeong Jo",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32d",
          "user": {
            "_id": "652066649004117947e46ed6",
            "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg",
            "isPro": false,
            "fullname": "Jaehong Yoon",
            "user": "jaehong31",
            "type": "user"
          },
          "name": "Jaehong Yoon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:34:44.146Z",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32e",
          "name": "Soo Ye Kim",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32f",
          "name": "Zhe Lin",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f330",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63bbf972d8d676a2299cdb44/n94aSArXRHEapWS5MwzkR.mp4"
      ],
      "publishedAt": "2025-06-08T14:54:41.000Z",
      "submittedOnDailyAt": "2025-06-11T02:49:49.936Z",
      "title": "Guía de marcos: Guía de distribución de modelos para el control del nivel de marcos de video sin aprendizaje.",
      "submittedOnDailyBy": {
        "_id": "63bbf972d8d676a2299cdb44",
        "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
        "isPro": false,
        "fullname": "Sangwon",
        "user": "agwmon",
        "type": "user"
      },
      "summary": "El desarrollo de modelos de expansión ha notado un gran aumento en la calidad de las imágenes y la posibilidad de control minucioso. Sin embargo, actualmente muchos métodos dependen de la fine-tuning de modelos de imágenes de gran escala para específicas tareas, pero esta aproximación no es efectiva cuando el tamaño del modelo sigue aumentando. En este artículo, se propone \"Frame Guidance\", una guía sin entrenamiento basada en señales de nivel de frame (por ejemplo, frame clave, imagen de estilo, esquema o mapa de profundidad, etc.) para la generación de imágenes controlables. Para implementar una guía sin entrenamiento efectiva, se propone una simple técnica de procesamiento de variables potenciales que reduce significativamente el uso de memoria, y se aplica una nueva estrategia de optimización de variables potenciales diseñada para la generación de imágenes coherentes. Frame Guidance permite un control efectivo en diversas tareas como guiar con un frame clave, estilizar o realizar loops, y genera imágenes de alta calidad sin necesidad de entrenamiento, y es compatible con cualquier modelo de imágenes. Los resultados experimentales muestran que Frame Guidance puede generar imágenes de alta calidad con control efectivo en una amplia variedad de tareas y señales de entrada.",
      "upvotes": 8,
      "discussionId": "6849036342e4f9106973f331",
      "ai_summary": "Frame Guidance offers a training-free method for controlling video generation using frame-level signals, reducing memory usage and enhancing globally coherent video output.",
      "ai_keywords": [
        "diffusion models",
        "frame-level signals",
        "keyframes",
        "style reference images",
        "sketches",
        "depth maps",
        "latent processing",
        "latent optimization",
        "globally coherent video generation",
        "video models",
        "keyframe guidance",
        "stylization",
        "looping"
      ]
    },
    "publishedAt": "2025-06-08T10:54:41.000Z",
    "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video\n  Diffusion Models",
    "summary": "Advancements in diffusion models have significantly improved video quality,\ndirecting attention to fine-grained controllability. However, many existing\nmethods depend on fine-tuning large-scale video models for specific tasks,\nwhich becomes increasingly impractical as model sizes continue to grow. In this\nwork, we present Frame Guidance, a training-free guidance for controllable\nvideo generation based on frame-level signals, such as keyframes, style\nreference images, sketches, or depth maps. For practical training-free\nguidance, we propose a simple latent processing method that dramatically\nreduces memory usage, and apply a novel latent optimization strategy designed\nfor globally coherent video generation. Frame Guidance enables effective\ncontrol across diverse tasks, including keyframe guidance, stylization, and\nlooping, without any training, compatible with any video models. Experimental\nresults show that Frame Guidance can produce high-quality controlled videos for\na wide range of tasks and input signals.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63bbf972d8d676a2299cdb44/n94aSArXRHEapWS5MwzkR.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07177.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbf972d8d676a2299cdb44",
      "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
      "fullname": "Sangwon",
      "name": "agwmon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05167",
      "authors": [
        {
          "_id": "68468cb23ec10bdd8ab4db5b",
          "user": {
            "_id": "645aedd221ab438e732bff43",
            "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
            "isPro": false,
            "fullname": "Yeonseok Jeong",
            "user": "yeonseokjeong",
            "type": "user"
          },
          "name": "Yeonseok Jeong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:11.745Z",
          "hidden": false
        },
        {
          "_id": "68468cb23ec10bdd8ab4db5c",
          "name": "Jinsu Kim",
          "hidden": false
        },
        {
          "_id": "68468cb23ec10bdd8ab4db5d",
          "name": "Dohyeon Lee",
          "hidden": false
        },
        {
          "_id": "68468cb23ec10bdd8ab4db5e",
          "name": "Seung-won Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T15:43:49.000Z",
      "submittedOnDailyAt": "2025-06-11T00:47:14.627Z",
      "title": "ECoRAG: Guía de Evidencias para Comprimir el Contexto Largo de RAG",
      "submittedOnDailyBy": {
        "_id": "645aedd221ab438e732bff43",
        "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
        "isPro": false,
        "fullname": "Yeonseok Jeong",
        "user": "yeonseokjeong",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) muestran un desempeño impresionante en respuestas a consultas en dominios abiertos (ODQA) a través de la utilización de documentos externos y la tecnología de Rechius Augur Decision (RAG). Para reducir el overhead de RAG, es necesario reducir el contexto largo mediante la configuración del contexto. Sin embargo, los métodos de configuración de reciente pasado no pueden filtrar información no probada, lo que limita el desempeño de RAG basado en LLMs. En consecuencia, proponemos el marco de referencia RAG basado en pruebas, ECoRAG. ECoRAG configura los documentos encontrados basándose en pruebas y verifica si la generación de respuesta es sostenida por pruebas probadas. En un paso adicional, ECoRAG verifica si las pruebas proporcionan contenidos suficientemente comprimidos y, si no, realiza una búsqueda adicional. Los experimentos demuestran que ECoRAG mejora el desempeño de los LLMs en ODQA, supera los métodos de configuración existentes y minimiza la cantidad de tokens utilizados, reduciendo el costo de la memoria y el código está disponible en https://github.com/ldilab/ECoRAG.",
      "upvotes": 6,
      "discussionId": "68468cb23ec10bdd8ab4db5f",
      "githubRepo": "https://github.com/ldilab/ECoRAG",
      "ai_summary": "ECoRAG framework enhances LLM performance in ODQA by compressing retrieved documents based on evidentiality, reducing latency and token usage.",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "context compression",
        "evidentiality",
        "LLM",
        "Open-Domain Question Answering (ODQA)"
      ]
    },
    "publishedAt": "2025-06-05T11:43:49.000Z",
    "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
    "summary": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\nECoRAG framework. ECoRAG improves LLM performance by compressing retrieved\ndocuments based on evidentiality, ensuring whether answer generation is\nsupported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645aedd221ab438e732bff43",
      "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
      "fullname": "Yeonseok Jeong",
      "name": "yeonseokjeong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08887",
      "authors": [
        {
          "_id": "6848ec1542e4f9106973f2ac",
          "user": {
            "_id": "6364b81b3e248b1e28a68b26",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6364b81b3e248b1e28a68b26/9pd6zfH3HGx1gQkYuL3pR.png",
            "isPro": false,
            "fullname": "LeqiShen",
            "user": "lunar677",
            "type": "user"
          },
          "name": "Leqi Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:05.097Z",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2ad",
          "name": "Guoqiang Gong",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2ae",
          "name": "Tianxiang Hao",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2af",
          "name": "Tao He",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b0",
          "name": "Yifeng Zhang",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b1",
          "name": "Pengzhang Liu",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b2",
          "name": "Sicheng Zhao",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b3",
          "name": "Jungong Han",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b4",
          "name": "Guiguang Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T15:16:40.000Z",
      "submittedOnDailyAt": "2025-06-11T01:17:30.703Z",
      "title": "Discovery: Visión, lenguaje y eliminación de errores en arrays para una búsqueda de vídeos de texto eficiente en parámetros",
      "submittedOnDailyBy": {
        "_id": "6364b81b3e248b1e28a68b26",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6364b81b3e248b1e28a68b26/9pd6zfH3HGx1gQkYuL3pR.png",
        "isPro": false,
        "fullname": "LeqiShen",
        "user": "lunar677",
        "type": "user"
      },
      "summary": "El artículo - investigación sobre la aplicación eficiente de los parámetros del modelo CLIP ante el aprendizaje profundo en la búsqueda vídeo-texto es una de las áreas más importantes. Aunque CLIP se centra en el enfoque de correspondencia visuolingüística a nivel de imágenes, la búsqueda vídeo-texto requiere una comprensión integral a nivel de vídeo. Al pasar de imágenes a vídeo, surgen tres diferencias: visión, lenguaje y alineación. Sin embargo, actualmente, los métodos se centran principalmente en la visión, excluyendo la lenguaje y la alineación. En este artículo, se propone una reducción de las discrepancias entre visión, lenguaje y alineación llamada Discrepancy Reduction for Vision, Language, and Alignment (DiscoVLA). Específicamente, se introduce la integración de características de imágenes y vídeo para combinar las características de ambos niveles, y se aborda efectivamente las diferencias entre visión y lenguaje. Además, se generan imágenes falsas para entrenar la alineación a nivel de imágenes. Se propone la distillación de alineación de imágenes a vídeo para reducir las diferencias en la alineación, utilizando el conocimiento de alineación a nivel de imágenes. Los experimentos extensos muestran la excelente performance de DiscoVLA, con un mejoramiento del 1.5% en R@1 en comparación con los métodos anteriores, alcanzando un R@1 final de 50.5% cuando se utiliza CLIP (ViT-B/16) en MSRVTT. El código está disponible en https://github.com/LunarShen/DsicoVLA.",
      "upvotes": 4,
      "discussionId": "6848ec1642e4f9106973f2b5",
      "githubRepo": "https://github.com/LunarShen/DsicoVLA",
      "ai_summary": "The paper proposes DiscoVLA to improve video-text retrieval using CLIP by addressing vision, language, and alignment discrepancies, achieving superior performance.",
      "ai_keywords": [
        "parameter-efficient adaptation",
        "image-text pretraining model",
        "CLIP",
        "video-text retrieval",
        "vision",
        "language",
        "alignment",
        "Image-Video Features Fusion",
        "pseudo image captions",
        "Image-to-Video Alignment Distillation",
        "MSRVTT",
        "R@1"
      ]
    },
    "publishedAt": "2025-06-10T11:16:40.000Z",
    "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for\n  Parameter-Efficient Video-Text Retrieval",
    "summary": "The parameter-efficient adaptation of the image-text pretraining model CLIP\nfor video-text retrieval is a prominent area of research. While CLIP is focused\non image-level vision-language matching, video-text retrieval demands\ncomprehensive understanding at the video level. Three key discrepancies emerge\nin the transfer from image-level to video-level: vision, language, and\nalignment. However, existing methods mainly focus on vision while neglecting\nlanguage and alignment. In this paper, we propose Discrepancy Reduction in\nVision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all\nthree discrepancies. Specifically, we introduce Image-Video Features Fusion to\nintegrate image-level and video-level features, effectively tackling both\nvision and language discrepancies. Additionally, we generate pseudo image\ncaptions to learn fine-grained image-level alignment. To mitigate alignment\ndiscrepancies, we propose Image-to-Video Alignment Distillation, which\nleverages image-level alignment knowledge to enhance video-level alignment.\nExtensive experiments demonstrate the superiority of our DiscoVLA. In\nparticular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous\nmethods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is\navailable at https://github.com/LunarShen/DsicoVLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08887.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6364b81b3e248b1e28a68b26",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6364b81b3e248b1e28a68b26/9pd6zfH3HGx1gQkYuL3pR.png",
      "fullname": "LeqiShen",
      "name": "lunar677",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07932",
      "authors": [
        {
          "_id": "68487f6342e4f9106973f17a",
          "user": {
            "_id": "60796959c59d9e1697fa2324",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
            "isPro": false,
            "fullname": "Rishit Dagli",
            "user": "rishitdagli",
            "type": "user"
          },
          "name": "Rishit Dagli",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:32.138Z",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17b",
          "name": "Yushi Guan",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17c",
          "name": "Sankeerth Durvasula",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17d",
          "name": "Mohammadreza Mofayezi",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17e",
          "name": "Nandita Vijaykumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T16:52:10.000Z",
      "submittedOnDailyAt": "2025-06-11T02:25:06.388Z",
      "title": "Squeeze3D: Tu modelo de generación 3D es en realidad un potente nueral compactor oculto.",
      "submittedOnDailyBy": {
        "_id": "60796959c59d9e1697fa2324",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
        "isPro": false,
        "fullname": "Rishit Dagli",
        "user": "rishitdagli",
        "type": "user"
      },
      "summary": "Squeeze3D es un nuevo marco de trabajo que utiliza los espacios de vectores ocultos aprendidos por modelos de generación 3D previamente entrenados para comprimir datos 3D de una manera muy eficiente. Nuestro enfoque utiliza una red de mapeo aprendible para conectar los espacios de vectores ocultos de un codificador y un modelo de generación previamente entrenados. La red de mapeo crea vectores ocultos que representan a modelos 3D (como modelos de puntos, clusters de puntos o campos radiales) y los transforma a espacios de vectores ocultos del modelo de generación. Estos vectores ocultos pueden ser utilizados como una representación muy comprimida de modelos de puntos o clusters de puntos. Squeeze3D se entrena solo con datos de síntesis generado, sin necesidad de un conjunto completo de datos 3D. La estructura de Squeeze3D puede ser adaptada a codificadores y modelos de generación previamente entrenados, y es flexible para soportar formatos como modelos de puntos, clusters de puntos o campos radiales. Según los experimentos, Squeeze3D logra tasas de compresión de 2187 veces en modelos de texto-as-point, 55 veces en clusters de puntos y 619 veces en campos radiales, manteniendo la calidad visual comparativa con muchos métodos existentes. Squeeze3D no requiere entrenar una red especializada para cada objeto, lo que reduce la latencia de compresión y descompresión.",
      "upvotes": 2,
      "discussionId": "68487f6442e4f9106973f17f",
      "projectPage": "https://squeeze3d.github.io/",
      "ai_summary": "A novel framework called Squeeze3D uses pre-trained models to compress 3D data efficiently, achieving high compression ratios while maintaining visual quality.",
      "ai_keywords": [
        "pre-trained 3D generative models",
        "latent spaces",
        "encode",
        "latent code",
        "mapping networks",
        "radiance fields",
        "synthetic data",
        "compression ratios",
        "visual quality",
        "compression latency",
        "decompression latency"
      ]
    },
    "publishedAt": "2025-06-09T12:52:10.000Z",
    "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor",
    "summary": "We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07932.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60796959c59d9e1697fa2324",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
      "fullname": "Rishit Dagli",
      "name": "rishitdagli",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05928",
      "authors": [
        {
          "_id": "6847b3393ec10bdd8ab4df20",
          "user": {
            "_id": "65ea90741b0d7e029a3a1fb0",
            "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
            "isPro": false,
            "fullname": "cj",
            "user": "cajie",
            "type": "user"
          },
          "name": "Jie Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:42:50.209Z",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df21",
          "name": "Tianwei Lin",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df22",
          "name": "Hongyang He",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df23",
          "name": "Rolan Yan",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df24",
          "name": "Wenqiao Zhang",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df25",
          "name": "Juncheng Li",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df26",
          "name": "Dongping Zhang",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df27",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df28",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T09:54:19.000Z",
      "submittedOnDailyAt": "2025-06-11T01:28:46.209Z",
      "title": "Moa: Efectividad de los parámetros en la micro-ajuste de modelos híbridos de lenguaje de gran escala",
      "submittedOnDailyBy": {
        "_id": "65ea90741b0d7e029a3a1fb0",
        "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
        "isPro": false,
        "fullname": "cj",
        "user": "cajie",
        "type": "user"
      },
      "summary": "Recientes investigaciones están trabajando en mejorar el rendimiento de métodos de adaptación eficiente de parámetros (PEFT) en aplicaciones de modelos de lenguaje grandes (LLM) mediante la integración de Low-Rank Adaptation (LoRA) y Mixture-of-Experts (MoE). Los métodos actuales están compuestos por una arquitectura MoE-LoRA uniforme con LoRA Experts de similares estructuras y capacidades. Sin embargo, estas aproximaciones son afectadas por la pérdida de representación y la desbalanceo de cargas de los Experts, lo que puede reducir el potencial de los LLM. Para enfrentar estos desafíos, proponemos un enfoque heterogéneo de Mixture-of-Adapters (MoA).\n\nEste método integra de manera dinámica PEFT Adapter Experts de diferentes estructuras y utiliza su capacidad de interpolación para promover la especialización de los Experts y transmitir de manera eficiente el conocimiento pre-entrenado para mejorar las tareas posteriores. La MoA ofrece dos versiones:\n(i) MoA suave integra de manera fina todos los Experts mediante una combinación ponderada de sus salidas.\n(ii) MoA esparsa activa de manera esparsa a los Experts según su contribución, lo que permite realizar ajustes sin perder el rendimiento.\n\nLos resultados de los experimentos muestran que la MoA heterogénea supera a la arquitectura homogénea MoE-LoRA en términos de rendimiento y eficiencia de parámetros. Nuestro proyecto está disponible en: https://github.com/DCDmllm/MoA.",
      "upvotes": 2,
      "discussionId": "6847b3393ec10bdd8ab4df29",
      "githubRepo": "https://github.com/DCDmllm/MoA",
      "ai_summary": "A heterogeneous Mixture-of-Adapters (MoA) approach enhances parameter-efficient fine-tuning in LLMs by integrating diverse adapter experts, outperforming homogeneous MoE-LoRA methods.",
      "ai_keywords": [
        "Low-Rank Adaptation",
        "Mixture-of-Experts",
        "parameter-efficient fine-tuning",
        "Large Language Model",
        "homogeneous",
        "representation collapse",
        "expert load imbalance",
        "heterogeneous",
        "Mixture-of-Adapters",
        "soft MoA",
        "sparse MoA"
      ]
    },
    "publishedAt": "2025-06-06T05:54:19.000Z",
    "title": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient\n  Fine-Tuning of Large Language Models",
    "summary": "Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts\n(MoE) to further enhance the performance of parameter-efficient fine-tuning\n(PEFT) methods in Large Language Model (LLM) applications. Existing methods\nemploy homogeneous MoE-LoRA architectures composed of LoRA experts with\neither similar or identical structures and capacities. However, these\napproaches often suffer from representation collapse and expert load imbalance,\nwhich negatively impact the potential of LLMs. To address these challenges, we\npropose a heterogeneous Mixture-of-Adapters (MoA) approach.\nThis method dynamically integrates PEFT adapter experts with diverse\nstructures, leveraging their complementary representational capabilities to\nfoster expert specialization, thereby enhancing the effective transfer of\npre-trained knowledge to downstream tasks. MoA supports two variants:\n(i) Soft MoA achieves fine-grained integration by performing\na weighted fusion of all expert outputs; (ii) Sparse MoA\nactivates adapter experts sparsely based on their contribution, achieving this\nwith negligible performance degradation. Experimental results demonstrate that\nheterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance\nand parameter efficiency. Our project is available at\nhttps://github.com/DCDmllm/MoA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05928.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ea90741b0d7e029a3a1fb0",
      "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
      "fullname": "cj",
      "name": "cajie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08300",
      "authors": [
        {
          "_id": "684954493614057188acbf5a",
          "name": "Matteo Cargnelutti",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5b",
          "name": "Catherine Brobston",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5c",
          "name": "John Hess",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5d",
          "name": "Jack Cushman",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5e",
          "name": "Kristi Mukk",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5f",
          "name": "Aristana Scourtas",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf60",
          "name": "Kyle Courtney",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf61",
          "name": "Greg Leppert",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf62",
          "name": "Amanda Watson",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf63",
          "name": "Martha Whitehead",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf64",
          "name": "Jonathan Zittrain",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T00:11:30.000Z",
      "submittedOnDailyAt": "2025-06-11T08:37:11.451Z",
      "title": "Institucional Bug 1.0: Colección de la Biblioteca Harvard de 242B tokens, adecuado para la precisión y la posibilidad de uso.",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "Los grandes modelos de lenguaje (LLMs) tienen como objetivo aprender sobre el mundo utilizando datos y generar relaciones significativas y predicciones. Por lo tanto, las características, la escala, la calidad y la diversidad de los conjuntos de datos que se utilizan para entrenar o inferir estos modelos tienen un impacto directo en su calidad. El rápido desarrollo y la introducción de modelos de alta calidad han resaltado la falta de alta calidad de datos de entrenamiento disponibles públicamente, lo que ha llevado a una necesidad urgente de que los administradores de estos conjuntos de datos adopten prácticas continuas. Sin embargo, este informe técnico presenta, por primera vez, desde 2006, el servicio de Gitter Book, la biblioteca de Harvard ha digitalizado la colección de libros institucionales 1.0, una amplia colección de libros en el ámbito público. En colaboración con la biblioteca de Harvard, estos libros fueron extraídos, analizados y convertidos en conjuntos de datos registrados. Este análisis cubrió toda la colección de la biblioteca de Harvard y presentó por primera vez 1,075,899 libros escritos en más de 250 idiomas, superando aproximadamente 250 billones de tokens. Una parte de esta primera versión de lanzamiento incluyó el texto extraído por OCR (original y posteriormente procesado) y metadatos (dígitales de catálogo, fuentes, generados) de 983,004 libros (242B tokens). Este informe explica el objetivo y los métodos del proyecto, resume los resultados de los análisis realizados, y proporciona apoyo para hacer que esta colección histórica esté más accesible, ofreciendo métodos para filtrar, leer y usar tanto para personas como para máquinas.",
      "upvotes": 1,
      "discussionId": "684954493614057188acbf65",
      "ai_summary": "Institutional Books 1.0 provides a large dataset of public domain books from Harvard Library for training and inference of large language models, enhancing data accessibility and sustainability.",
      "ai_keywords": [
        "large language models",
        "datasets",
        "institutional books",
        "public domain",
        "harrass library",
        "google books project",
        "ocr",
        "metadata",
        "historic texts"
      ]
    },
    "publishedAt": "2025-06-09T20:11:30.000Z",
    "title": "Institutional Books 1.0: A 242B token dataset from Harvard Library's\n  collections, refined for accuracy and usability",
    "summary": "Large language models (LLMs) use data to learn about the world in order to\nproduce meaningful correlations and predictions. As such, the nature, scale,\nquality, and diversity of the datasets used to train these models, or to\nsupport their work at inference time, have a direct impact on their quality.\nThe rapid development and adoption of LLMs of varying quality has brought into\nfocus the scarcity of publicly available, high-quality training data and\nrevealed an urgent need to ground the stewardship of these datasets in\nsustainable practices with clear provenance chains. To that end, this technical\nreport introduces Institutional Books 1.0, a large collection of public domain\nbooks originally digitized through Harvard Library's participation in the\nGoogle Books project, beginning in 2006. Working with Harvard Library, we\nextracted, analyzed, and processed these volumes into an extensively-documented\ndataset of historic texts. This analysis covers the entirety of Harvard\nLibrary's collection scanned as part of that project, originally spanning\n1,075,899 volumes written in over 250 different languages for a total of\napproximately 250 billion tokens. As part of this initial release, the\nOCR-extracted text (original and post-processed) as well as the metadata\n(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,\nidentified as being in the public domain have been made available. This report\ndescribes this project's goals and methods as well as the results of the\nanalyses we performed, all in service of making this historical collection more\naccessible and easier for humans and machines alike to filter, read and use.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08300.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2742
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07047",
      "authors": [
        {
          "_id": "68492bf142e4f9106973f411",
          "name": "Yu Xuejun",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f412",
          "user": {
            "_id": "6608fa4f5baec84322ec85ea",
            "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
            "isPro": false,
            "fullname": "Zhong",
            "user": "Jianyuan1",
            "type": "user"
          },
          "name": "Jianyuan Zhong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:34:08.719Z",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f413",
          "name": "Zijin Feng",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f414",
          "name": "Pengyi Zhai",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f415",
          "name": "Roozbeh Yousefzadeh",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f416",
          "name": "Wei Chong Ng",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f417",
          "name": "Haoxiong Liu",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f418",
          "name": "Ziyi Shou",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f419",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41a",
          "name": "Yudong Zhou",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41b",
          "name": "Claudia Beth Ong",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41c",
          "name": "Austen Jeremy Sugiarto",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41d",
          "name": "Yaoxi Zhang",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41e",
          "name": "Wai Ming Tai",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41f",
          "name": "Huan Cao",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f420",
          "name": "Dongcai Lu",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f421",
          "name": "Jiacheng Sun",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f422",
          "name": "Qiang Xu",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f423",
          "name": "Shen Xin",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f424",
          "name": "Zhenguo Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-08T09:04:14.000Z",
      "submittedOnDailyAt": "2025-06-11T05:45:24.452Z",
      "title": "Maestro: Investigación sobre demostraciones formales a partir de lenguaje natural",
      "submittedOnDailyBy": {
        "_id": "6608fa4f5baec84322ec85ea",
        "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
        "isPro": false,
        "fullname": "Zhong",
        "user": "Jianyuan1",
        "type": "user"
      },
      "summary": "El reciente desarrollo de modelos de lenguaje grandes muestra una gran posibilidad debido a su capacidad formal. Sin embargo, la mayoría de los sistemas de demostración basados en LLM requieren entradas de explicaciones formales escritas por expertos, lo que restringe su aplicación a problemas expresados en naturaleza. Hemos intentado remediar este vacío introduciendo un proceso completo de demostración formal llamado \"Mathesis\". Este sistema proporciona un primer generador automático de formalización, \"Mathesis-Autoformalizer\", utilizando aprendizaje por refuerzo para fortalecer la capacidad de formalización de las explicaciones en naturaleza. Además, nuestro nuevo marco de evaluación LeanScorer apoya la evaluación de la calidad de la formalización compleja. También proponemos \"Mathesis-Prover\", un sistema que genera demostraciones formales a partir de explicaciones formales. Para evaluar la posibilidad de aplicación de demostraciones formales, hemos introducido el benchmark Gaokao-Formal, compuesto por 488 problemas complejos. Nuestro enfoque se ha diseñado cuidadosamente a través de la investigación detallada de cada componente. Las experimentaciones demuestran el efecto de Mathesis y alcanzan un mejor resultado que el 22% del mejor base de referencia en Gaokao-Formal. El sistema completo alcanza una precisión del 64% en MiniF2F y el 18% más avanzado en Gaokao-Formal.",
      "upvotes": 1,
      "discussionId": "68492bf142e4f9106973f425",
      "githubRepo": "https://github.com/Huawei-AI4Math/Mathesis"
    },
    "publishedAt": "2025-06-08T05:04:14.000Z",
    "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages",
    "summary": "Recent advances in large language models show strong promise for formal\nreasoning. However, most LLM-based theorem provers have long been constrained\nby the need for expert-written formal statements as inputs, limiting their\napplicability to real-world problems expressed in natural language. We tackle\nthis gap with Mathesis, the first end-to-end theorem proving pipeline\nprocessing informal problem statements. It contributes Mathesis-Autoformalizer,\nthe first autoformalizer using reinforcement learning to enhance the\nformalization ability of natural language problems, aided by our novel\nLeanScorer framework for nuanced formalization quality assessment. It also\nproposes a Mathesis-Prover, which generates formal proofs from the formalized\nstatements. To evaluate the real-world applicability of end-to-end formal\ntheorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex\nproblems from China's national college entrance exam. Our approach is carefully\ndesigned, with a thorough study of each component. Experiments demonstrate\nMathesis's effectiveness, with the autoformalizer outperforming the best\nbaseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other\nmodel combinations, achieving 64% accuracy on MiniF2F with pass@32 and a\nstate-of-the-art 18% on Gaokao-Formal.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07047.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6608fa4f5baec84322ec85ea",
      "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
      "fullname": "Zhong",
      "name": "Jianyuan1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05700",
      "authors": [
        {
          "_id": "6848de6e42e4f9106973f273",
          "user": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "name": "Yan Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-11T01:39:59.328Z",
          "hidden": false
        },
        {
          "_id": "6848de6e42e4f9106973f274",
          "name": "Yueru He",
          "hidden": false
        },
        {
          "_id": "6848de6e42e4f9106973f275",
          "name": "Ruoyu Xiang",
          "hidden": false
        },
        {
          "_id": "6848de6e42e4f9106973f276",
          "name": "Jeff Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T03:02:52.000Z",
      "submittedOnDailyAt": "2025-06-11T00:15:04.904Z",
      "title": "Modelo de lenguaje grande fortalecido con conocimiento de reglas",
      "submittedOnDailyBy": {
        "_id": "65d76cc5b9b7b8bf88faa916",
        "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
        "isPro": true,
        "fullname": "Yan Wang",
        "user": "YanAdjeNole",
        "type": "user"
      },
      "summary": "El reciente desarrollo de grandes modelos de lenguaje (LLMs) ofrece gran potencial en el campo financiero, sin embargo, los problemas de precisión y violaciones en los informes de regulación digital (DRR) han sido un tema de gran importancia. Para enfrentar estos desafíos, proponemos RKEFino1, un modelo de lógica financiera basado en Fino1 que se ha fortalecido con conocimientos regulatorios. Este modelo se ha ajustado a conocimientos de áreas específicas obtenidos de XBRL, CDM y MOF. Hemos configurado dos tareas de preguntas y respuestas basadas en conocimiento y matemática, y hemos introducido una nueva tarea de reconocimiento de entidades numéricas (NER) que cubre entidades financieras en oraciones y tablas. A través de los resultados experimentales, demostramos que RKEFino1 muestra eficacia y capacidad de generalización en tareas financieras importantes relacionadas con violaciones. Nuestro modelo está disponible en Hugging Face.",
      "upvotes": 1,
      "discussionId": "6848de6e42e4f9106973f277",
      "ai_summary": "RKEFino1, a regulation-aware LLM fine-tuned with financial domain knowledge, effectively handles compliance-critical tasks including QA and numerical NER.",
      "ai_keywords": [
        "Large language models",
        "financial reasoning",
        "Digital Regulatory Reporting",
        "regulation knowledge-enhanced",
        "fine-tuning",
        "domain knowledge",
        "XBRL",
        "CDM",
        "MOF",
        "QA tasks",
        "knowledge-based reasoning",
        "mathematical reasoning",
        "Numerical NER",
        "financial entities"
      ]
    },
    "publishedAt": "2025-06-05T23:02:52.000Z",
    "title": "RKEFino1: A Regulation Knowledge-Enhanced Large Language Model",
    "summary": "Recent advances in large language models (LLMs) hold great promise for\nfinancial applications but introduce critical accuracy and compliance\nchallenges in Digital Regulatory Reporting (DRR). To address these issues, we\npropose RKEFino1, a regulation knowledge-enhanced financial reasoning model\nbuilt upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We\nformulate two QA tasks-knowledge-based and mathematical reasoning-and introduce\na novel Numerical NER task covering financial entities in both sentences and\ntables. Experimental results demonstrate the effectiveness and generalization\ncapacity of RKEFino1 in compliance-critical financial tasks. We have released\nour model on Hugging Face.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05700.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d76cc5b9b7b8bf88faa916",
      "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
      "fullname": "Yan Wang",
      "name": "YanAdjeNole",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07976",
      "authors": [
        {
          "_id": "68487bd642e4f9106973f16d",
          "name": "Junhong Shen",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f16e",
          "user": {
            "_id": "62927c2e56fedc76e396b3ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678105603200-62927c2e56fedc76e396b3ca.jpeg",
            "isPro": false,
            "fullname": "HAO BAI",
            "user": "JackBAI",
            "type": "user"
          },
          "name": "Hao Bai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T10:08:42.049Z",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f16f",
          "name": "Lunjun Zhang",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f170",
          "name": "Yifei Zhou",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f171",
          "name": "Amrith Setlur",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f172",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f173",
          "name": "Diego Caples",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f174",
          "name": "Nan Jiang",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f175",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f176",
          "name": "Ameet Talwalkar",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f177",
          "name": "Aviral Kumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:50:02.000Z",
      "submittedOnDailyAt": "2025-06-11T08:38:04.852Z",
      "title": "Thinking vs. Doing: Escalado de Interacciones Interactivas en Tiempo de Prueba por Razones de Escalado",
      "submittedOnDailyBy": {
        "_id": "62927c2e56fedc76e396b3ca",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678105603200-62927c2e56fedc76e396b3ca.jpeg",
        "isPro": false,
        "fullname": "HAO BAI",
        "user": "JackBAI",
        "type": "user"
      },
      "summary": "Actualmente, el paradigma de escalado temporal de pruebas implica la generación de trazas lógicas complejas antes de producir una respuesta. En problemas de intercambio, estas trazas lógicas permiten pensar en acciones a realizar en un mundo antes de actuar. Sin embargo, este proceso no puede ser ejecutado dentro del interior de la carga de salida, que puede realizar diversas acciones como exploración, retroceso y reorganización dinámica. Este artículo propone un escalado temporal de intercambio de tiempos de prueba. Este escalado temporal permite que dentro de la carga de salida se realicen diversas acciones como exploración, retroceso y reorganización dinámica, lo cual no es posible en el escalado temporal de pruebas actuales. Para demostrar la posibilidad de este nuevo escalado, se centra en el área de la carga de salida web. Primero, se muestra que la escalado de intercambio basado en prompts sin aprendizaje aumenta simplemente la tasa de éxito. A través de esto, se introduce el TTI (Interacción de Tiempos de Prueba) y se utiliza un enfoque basado en el aprendizaje de RL (Reinforcement Learning) con un clérivo para ajustar de manera adaptativa la longitud de la carga de salida. Utilizando el modelo de ratón 3 12B, el TTI genera la mejor carga de salida web abierta y con datos abiertos en los benchmark de webloader y webarena. Además, el TTI muestra que puede mantener un equilibrio adaptativo entre exploración y uso dentro de la carga de salida. Nuestros resultados demuestran que el escalado de intercambio es un poderoso complemento para la escalabilidad de la cantidad de cálculo en cada etapa, y ofrece una nueva ruta para entrenar la carga de salida.",
      "upvotes": 0,
      "discussionId": "68487bd742e4f9106973f178",
      "ai_summary": "Test-Time Interaction (TTI) improves web agent performance by scaling interaction, enabling adaptive behavior and balancing exploration and exploitation without adding per-step compute.",
      "ai_keywords": [
        "test-time scaling",
        "thinking traces",
        "agent interaction",
        "interaction horizon",
        "exploration",
        "backtracking",
        "dynamic re-planning",
        "rollout",
        "curriculum-based online reinforcement learning (RL)",
        "Gemma 3 12B model",
        "WebVoyager",
        "WebArena",
        "adaptive agents"
      ]
    },
    "publishedAt": "2025-06-09T13:50:02.000Z",
    "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
    "summary": "The current paradigm of test-time scaling relies on generating long reasoning\ntraces (\"thinking\" more) before producing a response. In agent problems that\nrequire interaction, this can be done by generating thinking traces before\nacting in the world. However, this process does not allow agents to acquire new\ninformation from the environment or adapt their behavior over time. In this\nwork, we propose to scale test-time interaction, an untapped dimension of\ntest-time scaling that increases the agent's interaction horizon to enable\nrunning rich behaviors such as exploration, backtracking, and dynamic\nre-planning within a single rollout. To demonstrate the promise of this scaling\ndimension, we study the domain of web agents. We first show that even\nprompting-based interaction scaling without any training can improve task\nsuccess on web benchmarks non-trivially. Building on this, we introduce TTI\n(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)\napproach that trains agents by adaptively adjusting their rollout lengths.\nUsing a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data\nweb agents on WebVoyager and WebArena benchmarks. We further show that TTI\nenables agents to balance exploration and exploitation adaptively. Our results\nestablish interaction scaling as a powerful, complementary axis to scaling\nper-step compute, offering new avenues for training adaptive agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07976.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62927c2e56fedc76e396b3ca",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678105603200-62927c2e56fedc76e396b3ca.jpeg",
      "fullname": "HAO BAI",
      "name": "JackBAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]