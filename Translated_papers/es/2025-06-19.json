[
  {
    "paper": {
      "id": "2506.15675",
      "authors": [
        {
          "_id": "6853946599bf39f9665c79e0",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e1",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e2",
          "name": "Xiaofeng Mao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e3",
          "name": "Shaoheng Lin",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e4",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e5",
          "name": "Shitian Zhao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e6",
          "name": "Zhaopan Xu",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e7",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e8",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e9",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ea",
          "name": "Zizhen Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79eb",
          "name": "Fanrui Zhang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ec",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ed",
          "name": "Zhixiang Wang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ee",
          "name": "Yuwei Wu",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ef",
          "name": "Tong He",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f0",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f1",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f2",
          "name": "Yunde Jia",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f3",
          "user": {
            "_id": "63527f4e7d071f23d085ad45",
            "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
            "isPro": false,
            "fullname": "KAIPENG ZHANG",
            "user": "kpzhang",
            "type": "user"
          },
          "name": "Kaipeng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-19T10:09:38.100Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/GFrjhPvZnILgeTd_w2kvc.mp4"
      ],
      "publishedAt": "2025-06-18T17:57:06.000Z",
      "submittedOnDailyAt": "2025-06-19T03:16:13.113Z",
      "title": "World: Dataset for Video Exploration of the World",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "La tecnología de generación de vídeo ha experimentado un desarrollo sorprendente, y debe convertirse en la base para explorar el mundo interactivo. Sin embargo, los conjuntos de datos de vídeo actuales no son adecuados para el entrenamiento de exploración del mundo. Estos presentan limitaciones como lugares limitados, tiempo corto, escenas estáticas y una falta de explicaciones sobre la exploración y el mundo. En este artículo, se presenta el primer conjunto de vídeo de punto de vista primero de un mundo (Sekai), de alta calidad, que significa \"mundo\". Este conjunto de datos recopiló más de 5,000 horas de vídeo desde la perspectiva de caminantes y pilotes (FPV y UVA) en más de 750 ciudades de más de 100 países y regiones, incluyendo una amplia descripción de exploración del mundo. Se desarrollaron herramientas eficientes y efectivas para la recopilación, preprocesamiento y anotación basadas en descripciones del conjunto de datos. Las experimentaciones que muestran la calidad del conjunto de datos utilizaron parte de ellos para entrenar modelos de exploración del mundo interactivo llamados \"YUME\" (que significa \"sueño\" en japonés). Se cree que Sekai ayudará a la generación de vídeo y la exploración del mundo, y promoverá aplicaciones valiosas.",
      "upvotes": 26,
      "discussionId": "6853946599bf39f9665c79f4",
      "projectPage": "https://lixsp11.github.io/sekai-project/",
      "githubRepo": "https://github.com/Lixsp11/sekai-codebase",
      "ai_summary": "Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.",
      "ai_keywords": [
        "first-person view",
        "worldwide video dataset",
        "rich annotations",
        "FPV",
        "UVA",
        "video collection",
        "pre-processing",
        "camera trajectories",
        "interactive video world exploration model"
      ]
    },
    "publishedAt": "2025-06-18T13:57:06.000Z",
    "title": "Sekai: A Video Dataset towards World Exploration",
    "summary": "Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/GFrjhPvZnILgeTd_w2kvc.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15675.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15681",
      "authors": [
        {
          "_id": "68536fc899bf39f9665c7961",
          "user": {
            "_id": "657152eb12f162153b50ec9d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
            "isPro": false,
            "fullname": "Byung-Kwan Lee",
            "user": "BK-Lee",
            "type": "user"
          },
          "name": "Byung-Kwan Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-19T09:10:25.415Z",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7962",
          "user": {
            "_id": "65b33e5f7cd0069ad648c4e8",
            "avatarUrl": "/avatars/1a746ea535cffa92ea08006e05ea414a.svg",
            "isPro": false,
            "fullname": "Ryo Hachiuma",
            "user": "rhachiuma",
            "type": "user"
          },
          "name": "Ryo Hachiuma",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-19T02:02:49.546Z",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7963",
          "name": "Yong Man Ro",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7964",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7965",
          "name": "Yueh-Hua Wu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/2xNUivkkqkJWJLWtIPNvb.mp4"
      ],
      "publishedAt": "2025-06-18T17:59:49.000Z",
      "submittedOnDailyAt": "2025-06-19T00:36:10.331Z",
      "title": "GenRecal: Generación de modelos de visión-lengua con ajuste hacia el lado más pequeño después de reescalado",
      "submittedOnDailyBy": {
        "_id": "657152eb12f162153b50ec9d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
        "isPro": false,
        "fullname": "Byung-Kwan Lee",
        "user": "BK-Lee",
        "type": "user"
      },
      "summary": "El reciente desarrollo de modelos de lenguaje visual (VLMs) ha permitido alcanzar performances similares a los sistemas cerrados como GPT-4V, utilizando grandes modelos de lenguaje (LLMs). Sin embargo, el funcionamiento de estos modelos en entornos reales, especialmente en dispositivos limitados en recursos, se ha complicado debido a las exigencias de cálculo grandes. Esto ha llevado a interesar en reducir de manera eficiente los VLMs de gran escala a pequeñas escalas. La diversidad estructural de los VLMs es un problema importante en este contexto. Estos modelos se construyen sobre diferentes LLMs y presentan cambios en el tamaño de palabra, la división de palabras y el orden de los índices de palabras, además de tipos de tareas diferentes. Para resolver las limitaciones específicas de un tipo de VLM, presentamos GenRecal. GenRecal es un nuevo marco de diseño general que incluye un re-calibrador para ajustar y adaptar las representaciones de características entre diferentes VLMs. GenRecal ha mejorado significativamente la performance en difíciles benchmarks a través de experimentos amplios, y ha demostrado finalmente superar a grandes VLMs abiertos y cerrados.",
      "upvotes": 13,
      "discussionId": "68536fc899bf39f9665c7966",
      "projectPage": "https://byungkwanlee.github.io/GenRecal-page/",
      "ai_summary": "GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.",
      "ai_keywords": [
        "vision-language models",
        "large language models",
        "distillation framework",
        "GenerRecal",
        "recalibration",
        "feature representations",
        "heterogeneous VLMs"
      ]
    },
    "publishedAt": "2025-06-18T13:59:49.000Z",
    "title": "GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models",
    "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/2xNUivkkqkJWJLWtIPNvb.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657152eb12f162153b50ec9d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
      "fullname": "Byung-Kwan Lee",
      "name": "BK-Lee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.15677",
      "authors": [
        {
          "_id": "68536b2399bf39f9665c794c",
          "name": "Yining Hong",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794d",
          "name": "Rui Sun",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794e",
          "name": "Bingxuan Li",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794f",
          "name": "Xingcheng Yao",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7950",
          "name": "Maxine Wu",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7951",
          "name": "Alexander Chien",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7952",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7953",
          "name": "Ying Nian Wu",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7954",
          "name": "Zhecan James Wang",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7955",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T17:58:17.000Z",
      "submittedOnDailyAt": "2025-06-19T00:14:22.240Z",
      "title": "Inteligencia Artificial Integrada que Conecta el Mundo Físico y el Digital",
      "submittedOnDailyBy": {
        "_id": "6431b64df76c34519e93d1ba",
        "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
        "isPro": true,
        "fullname": "Yining Hong",
        "user": "evelynhong",
        "type": "user"
      },
      "summary": "Los actuales agentes de IA están principalmente codificados: esto incluye la capacidad de buscar y explicar lógicamente grandes cantidades de información y conocimiento digitales disponibles en línea, así como la capacidad de reconocer experiencialmente, planificar y actuar para interactuar con el mundo físico. Sin embargo, es poco frecuente que puedan realizar ambas tareas simultáneamente. Esta separación limita su capacidad para integrar inteligencia física y digital para resolver tareas. Por ejemplo, preparar un plato de cocina basado en recetas en línea, navegar utilizando datos dinámicos de mapas, o interpretar indicaciones de realidad aumentada a partir de datos web. Introducimos un nuevo paradigma que naturalmente conecta conocimientos experienciales y razones de escala de vendedores. Para implementar este concepto, primero desarrollamos un plataforma de simulación integrada que se combina estrechamente con entornos 3D realistas de interiores y exteriores y con una interfaz funcional de vendedores. Con esta plataforma, construimos y lanzamos diversas tareas como cocinar, navegar, comprar, turismo y localización. Estas tareas requieren la integración de ambos dominios físico y digital, lo que necesita una evaluación sistemática de la inteligencia en áreas cruzadas. Los resultados de los experimentos muestran una gran diferencia de rendimiento entre los sistemas de IA más avanzados y las capacidades humanas, revelando tanto problemas como oportunidades en el punto de cruce de la reconocimiento experiencial y la acceso al conocimiento de escala de vendedores. Todos los datasets, códigos y sitios web están disponibles en nuestra página de proyecto (https://embodied-web-agent.github.io/).",
      "upvotes": 7,
      "discussionId": "68536b2399bf39f9665c7956",
      "ai_summary": "Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.",
      "ai_keywords": [
        "Embodied Web Agents",
        "task environments",
        "simulation platform",
        "3D indoor and outdoor environments",
        "functional web interfaces",
        "Embodied Web Agents Benchmark",
        "systematic assessment",
        "cross-domain intelligence",
        "embodied cognition"
      ]
    },
    "publishedAt": "2025-06-18T13:58:17.000Z",
    "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence",
    "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15677.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6431b64df76c34519e93d1ba",
      "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
      "fullname": "Yining Hong",
      "name": "evelynhong",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15068",
      "authors": [
        {
          "_id": "68536bf399bf39f9665c7958",
          "name": "Zongxia Li",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c7959",
          "name": "Yapei Chang",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795a",
          "name": "Yuhang Zhou",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795b",
          "name": "Xiyang Wu",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795c",
          "name": "Zichao Liang",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795d",
          "name": "Yoo Yeon Sung",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795e",
          "name": "Jordan Lee Boyd-Graber",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T02:16:53.000Z",
      "submittedOnDailyAt": "2025-06-19T04:04:25.155Z",
      "title": "Interés en la Semántica y configuración de recompensas, así como respuestas para el entrenamiento R1 de forma abierta para el aprendizaje autónomo.",
      "submittedOnDailyBy": {
        "_id": "64ea62f918d79efd533c93fe",
        "avatarUrl": "/avatars/9985a789ce11b788de2cba12adfb72fc.svg",
        "isPro": false,
        "fullname": "Xiyang Wu",
        "user": "wuxiyang",
        "type": "user"
      },
      "summary": "La evaluación de largas oraciones en terminales abiertos es difícil. Esto es debido a que es difícil distinguir claramente entre buenas y malas salidas. Los métodos de evaluación actuales mal valoran principales elementos como consistencia, estilo y relevancia, además de ser sesgados en función de los datos de entrenamiento. Por estas razones, la evaluación de largas oraciones en terminales abiertos es un problema de falta de investigación. Para resolver esto, proponemos PrefBERT. PrefBERT es un modelo de puntuación para evaluar la generación de largas oraciones en terminales abiertos en GRPO, proporcionando diferentes recompensas para buenas y malas salidas para guiar el aprendizaje. PrefBERT se entrenó con dos conjuntos de datos de respuestas con diferentes estilos de largas oraciones y con la calidad de la evaluación Likert. De este modo, PrefBERT proporciona una mejor retroalimentación significativa que Local o BERTScore, y apoya efectivamente a GRPO. A través de la evaluación como un juez (LLM-as-a-judge), evaluación humana y análisis cualitativo, PrefBERT se entrenó para varias oraciones y longitudes, ofreciendo retroalimentación estable y verificable necesaria para GRPO, incluso en casos largos. La evaluación humana confirmó que utilizando PrefBERT como señal de recompensa, se pueden obtener respuestas bien ajustadas a las preferencias humanas. El código está disponible en https://github.com/zli12321/long_form_rl.",
      "upvotes": 7,
      "discussionId": "68536bf399bf39f9665c795f",
      "ai_summary": "PrefBERT, a scoring model, improves open-ended long-form generation by providing better semantic reward feedback than traditional metrics.",
      "ai_keywords": [
        "PrefBERT",
        "GRPO",
        "multi-sentence responses",
        "paragraph-length responses",
        "Likert-rated quality",
        "LLM-as-a-judge",
        "human ratings",
        "qualitative analysis",
        "verifiable rewards"
      ]
    },
    "publishedAt": "2025-06-17T22:16:53.000Z",
    "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation",
    "summary": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15068.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ea62f918d79efd533c93fe",
      "avatarUrl": "/avatars/9985a789ce11b788de2cba12adfb72fc.svg",
      "fullname": "Xiyang Wu",
      "name": "wuxiyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15569",
      "authors": [
        {
          "_id": "68537ca999bf39f9665c799a",
          "name": "Chengye Wang",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799b",
          "name": "Yifei Shen",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799c",
          "name": "Zexi Kuang",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799d",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799e",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun Zhao",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-19T09:10:23.732Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T15:43:26.000Z",
      "submittedOnDailyAt": "2025-06-19T01:28:40.934Z",
      "title": "SciVer: Modelo de Evaluación de la Estructura Doble de la Protesta Científica",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "SciVer es el primer benchmark diseñado para evaluar la capacidad de probar argumentos científicos en el contexto científico. SciVer incluye 3,000 anotaciones de expertos en 1,113 artículos y contiene 4 subconjuntos comunes de lógica para la probación de argumentos científicos. Se incluye evidencia auxiliar anotada por expertos para ayudar en la evaluación de cada ejemplo. Se evalúan los rendimientos de 21 modelos de vanguardia, incluyendo o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision y Qwen2.5-VL. Se puede observar una gran diferencia en el rendimiento entre los modelos y los expertos humanos en SciVer. El análisis detallado de la generación de agresiones de búsqueda (RAG) y los errores evaluados por personas humanos permiten identificar limitaciones importantes de los modelos abiertos de código, proporcionando claves insights sobre el entendimiento de los modelos y el desarrollo de lógica en el trabajo con artículos científicos multimodelo.",
      "upvotes": 6,
      "discussionId": "68537ca999bf39f9665c799f",
      "githubRepo": "https://github.com/QDRhhhh/SciVer",
      "ai_summary": "A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.",
      "ai_keywords": [
        "retrieval-augmented generation (RAG)"
      ]
    },
    "publishedAt": "2025-06-18T11:43:26.000Z",
    "title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification",
    "summary": "We introduce SciVer, the first benchmark specifically designed to evaluate\nthe ability of foundation models to verify claims within a multimodal\nscientific context. SciVer consists of 3,000 expert-annotated examples over\n1,113 scientific papers, covering four subsets, each representing a common\nreasoning type in multimodal scientific claim verification. To enable\nfine-grained evaluation, each example includes expert-annotated supporting\nevidence. We assess the performance of 21 state-of-the-art multimodal\nfoundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and\nQwen2.5-VL. Our experiment reveals a substantial performance gap between these\nmodels and human experts on SciVer. Through an in-depth analysis of\nretrieval-augmented generation (RAG), and human-conducted error evaluations, we\nidentify critical limitations in current open-source models, offering key\ninsights to advance models' comprehension and reasoning in multimodal\nscientific literature tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15569.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.15050",
      "authors": [
        {
          "_id": "68539c6199bf39f9665c79f6",
          "name": "Tiantian Fan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f7",
          "name": "Lingjun Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f8",
          "name": "Yu Yue",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f9",
          "name": "Jiaze Chen",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fa",
          "name": "Chengyi Wang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fb",
          "name": "Qiying Yu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fc",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fd",
          "name": "Zhiqi Lin",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fe",
          "name": "Ruofei Zhu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79ff",
          "name": "Yufeng Yuan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a00",
          "name": "Xiaochen Zuo",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a01",
          "name": "Bole Ma",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a02",
          "name": "Mofan Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a03",
          "name": "Gaohong Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a04",
          "name": "Ru Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a05",
          "name": "Haotian Zhou",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a06",
          "name": "Cong Xie",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a07",
          "name": "Ruidong Zhu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a08",
          "name": "Zhi Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a09",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0a",
          "name": "Mingxuan Wang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0b",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0c",
          "name": "Yonghui Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T01:21:38.000Z",
      "submittedOnDailyAt": "2025-06-19T03:43:49.620Z",
      "title": "Truncated Proximal Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recientemente, modelos de escalado de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la raíz de la ra",
      "upvotes": 4,
      "discussionId": "68539c6199bf39f9665c7a0d",
      "ai_summary": "T-PPO, an extension of PPO, improves training efficiency for Large Language Models by optimizing policy updates and utilizing hardware resources more effectively.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "chains-of-thought (CoT)",
        "reinforcement learning (RL)",
        "Proximal Policy Optimization (PPO)",
        "Truncated Proximal Policy Optimization (T-PPO)",
        "Extended Generalized Advantage Estimation (EGAE)",
        "advantage estimation",
        "policy and value models",
        "independent optimization",
        "prompt and truncated tokens",
        "AIME 2024",
        "base model"
      ]
    },
    "publishedAt": "2025-06-17T21:21:38.000Z",
    "title": "Truncated Proximal Policy Optimization",
    "summary": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated\nexceptional reasoning capabilities across scientific and professional tasks by\ngenerating long chains-of-thought (CoT). As a crucial component for developing\nthese reasoning models, reinforcement learning (RL), exemplified by Proximal\nPolicy Optimization (PPO) and its variants, allows models to learn through\ntrial and error. However, PPO can be time-consuming due to its inherent\non-policy nature, which is further exacerbated by increasing response lengths.\nIn this work, we propose Truncated Proximal Policy Optimization (T-PPO), a\nnovel extension to PPO that improves training efficiency by streamlining policy\nupdate and length-restricted response generation. T-PPO mitigates the issue of\nlow hardware utilization, an inherent drawback of fully synchronized\nlong-generation procedures, where resources often sit idle during the waiting\nperiods for complete rollouts. Our contributions are two-folds. First, we\npropose Extended Generalized Advantage Estimation (EGAE) for advantage\nestimation derived from incomplete responses while maintaining the integrity of\npolicy learning. Second, we devise a computationally optimized mechanism that\nallows for the independent optimization of the policy and value models. By\nselectively filtering prompt and truncated tokens, this mechanism reduces\nredundant computations and accelerates the training process without sacrificing\nconvergence performance. We demonstrate the effectiveness and efficacy of T-PPO\non AIME 2024 with a 32B base model. The experimental results show that T-PPO\nimproves the training efficiency of reasoning LLMs by up to 2.5x and\noutperforms its existing competitors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15050.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 88
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06279",
      "authors": [
        {
          "_id": "6850e0285e07650ecce890f3",
          "user": {
            "_id": "637f22d27119bd030dfd4af8",
            "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
            "isPro": false,
            "fullname": "Shi Liu",
            "user": "CLLBJ16",
            "type": "user"
          },
          "name": "Shi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-18T12:16:48.071Z",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f4",
          "user": {
            "_id": "63e4562f9db5da2dc1f3b520",
            "avatarUrl": "/avatars/f4eecf1396b05e1c72436e7026d85cef.svg",
            "isPro": false,
            "fullname": "Weijie Su",
            "user": "jackroos",
            "type": "user"
          },
          "name": "Weijie Su",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-19T09:10:33.557Z",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f5",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f6",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f7",
          "name": "Jifeng Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:59:06.000Z",
      "submittedOnDailyAt": "2025-06-19T00:24:25.743Z",
      "title": "CoMemo: LVLMs necesitan contexto de imagen y memoria de imagen.",
      "submittedOnDailyBy": {
        "_id": "637f22d27119bd030dfd4af8",
        "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
        "isPro": false,
        "fullname": "Shi Liu",
        "user": "CLLBJ16",
        "type": "user"
      },
      "summary": "Recientemente, los grandes modelos de lenguaje visual y lingüístico (LVLM) han desarrollado un paradigma principal basado en modelos de lenguaje grande (LLM), centrado en la sincronización de características visuales y las representaciones de LLM. Sin embargo, estos LVLM tienden a no tener una estructura de procesamiento multiform optimizada. Primero, los LVLM muestran dos distribuciones que influyen en la asignación de atención, y pueden dejar de considerar la información visual mediante el contenido visual a medida que el contexto se expande. Además, la simple técnica de codificación de posición simple falla en mantener las relaciones estructurales bidimensionales en procesamiento de imágenes de alta resolución dinámica. Para resolver estas limitaciones, proponemos la estructura dual CoMemo. Esta combina imágenes de contexto y memoria de imágenes. CoMemo efectivamente reduce la ignorancia de la información visual. Además, introducimos RoPE-DHR, una nueva técnica de codificación de posición que utiliza agresión de posición basada en imágenes superficiales para mitigar la desaceleración distancial durante secuencias largas, mientras mantiene la percepción en el espacio bidimensional. En evaluaciones en 7 marcos de referencia (comprensión de contexto extenso, inferencia de imágenes multiescala, pruebas de respuesta a preguntas visuales, entre otros), CoMemo muestra un rendimiento superior en comparación con las estructuras tradicionales de LVLM. Para obtener más información sobre el proyecto, visite la página del proyecto en https://lalbj.github.io/projects/CoMemo/.",
      "upvotes": 4,
      "discussionId": "6850e0295e07650ecce890f8",
      "projectPage": "https://lalbj.github.io/projects/CoMemo/",
      "githubRepo": "https://github.com/LALBJ/CoMemo",
      "ai_summary": "CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.",
      "ai_keywords": [
        "Large Vision-Language Models",
        "Large Language Models",
        "multimodal processing",
        "bimodal distribution",
        "attention allocation",
        "middle visual content",
        "positional encoding",
        "visual processing",
        "image Memory path",
        "RoPE-DHR",
        "positional aggregation",
        "2D structural relationships",
        "spatial awareness",
        "remote decay",
        "long-context comprehension",
        "multi-image reasoning",
        "visual question answering"
      ]
    },
    "publishedAt": "2025-06-06T13:59:06.000Z",
    "title": "CoMemo: LVLMs Need Image Context with Image Memory",
    "summary": "Recent advancements in Large Vision-Language Models built upon Large Language\nModels have established aligning visual features with LLM representations as\nthe dominant paradigm. However, inherited LLM architectural designs introduce\nsuboptimal characteristics for multimodal processing. First, LVLMs exhibit a\nbimodal distribution in attention allocation, leading to the progressive\nneglect of middle visual content as context expands. Second, conventional\npositional encoding schemes fail to preserve vital 2D structural relationships\nwhen processing dynamic high-resolution images. To address these limitations,\nwe propose CoMemo - a dual-path architecture that combines a Context image path\nwith an image Memory path for visual processing, effectively alleviating visual\ninformation neglect. Additionally, we introduce RoPE-DHR, a novel positional\nencoding mechanism that employs thumbnail-based positional aggregation to\nmaintain 2D spatial awareness while mitigating remote decay in extended\nsequences. Evaluations across seven benchmarks,including long-context\ncomprehension, multi-image reasoning, and visual question answering,\ndemonstrate CoMemo's superior performance compared to conventional LVLM\narchitectures. Project page is available at\nhttps://lalbj.github.io/projects/CoMemo/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f22d27119bd030dfd4af8",
      "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
      "fullname": "Shi Liu",
      "name": "CLLBJ16",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.15672",
      "authors": [
        {
          "_id": "6853bbcc99bf39f9665c7a50",
          "name": "Yao Zhang",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a51",
          "name": "Chenyang Lin",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a52",
          "name": "Shijie Tang",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a53",
          "name": "Haokun Chen",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a54",
          "name": "Shijie Zhou",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a55",
          "name": "Yunpu Ma",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a56",
          "name": "Volker Tresp",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T17:54:55.000Z",
      "submittedOnDailyAt": "2025-06-19T06:26:51.897Z",
      "title": "Creación de un sistema completamente automatizado ágnico basado en Sewerum Intelligence",
      "submittedOnDailyBy": {
        "_id": "648cbea3dee03837c823cbf2",
        "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
        "isPro": false,
        "fullname": "Shuo Chen",
        "user": "ShuoChen99",
        "type": "user"
      },
      "summary": "El rápido desarrollo de los modelos de lenguaje generalizados ha impulsado el progreso de los sistemas de salida para la decisión, la colaboración y la ejecución de tareas. Sin embargo, los marcos de creación actuales de los sistemas de salida no tienen completa autonomía y limitan la adaptabilidad y escalabilidad debido a la escasez de escritura, la generación de salidas, la optimización automática y la falta de colaboración. Proponemos el marco SwarmAgentic para lograr la creación de un sistema de salidas completamente automatizado. Este marco realiza la construcción de sistemas de salida desde la escritura hasta la salida, ejecutando un proceso de optimización conjunto que depende de las funciones de salida y la colaboración. Para permitir una exploración eficiente de la estructura del sistema, SwarmAgentic mantiene un grupo de sistemas candidatos, actualiza los mismos mediante retroalimentación y inspiración en la Optimización de Partículas (PSO). Nuestro método se evalua en seis tareas reales, abiertas y exploratorias, demostrando altos niveles de planificación, colaboración a nivel de sistema y creatividad. Con solo la descripción de las tareas y funciones objetivo, SwarmAgentic supera todos los baselines y alcanza un mejoramiento relativo del +261.8% en el benchmark TravelPlanner, destacando el efecto de la completa automatización en tareas no estructuradas. Este marco muestra un paso importante en el diseño de sistemas de salida con intercambiabilidad y autonomía, conectando el conocimiento colectivo con la generación de salidas de sistema. Nuestro código está disponible en https://yaoz720.github.io/SwarmAgentic/.",
      "upvotes": 2,
      "discussionId": "6853bbcc99bf39f9665c7a57",
      "ai_summary": "SwarmAgentic is a framework for automated agentic system generation that optimize agent functionality and collaboration through language-driven exploration, outperforming existing baselines in unconstrained tasks.",
      "ai_keywords": [
        "Large Language Models",
        "agentic systems",
        "from-scratch agent generation",
        "self-optimizing agent functionality",
        "collaboration",
        "Particle Swarm Optimization (PSO)",
        "TravelPlanner benchmark",
        "system-level coordination",
        "creative reasoning"
      ]
    },
    "publishedAt": "2025-06-18T13:54:55.000Z",
    "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via\n  Swarm Intelligence",
    "summary": "The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648cbea3dee03837c823cbf2",
      "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
      "fullname": "Shuo Chen",
      "name": "ShuoChen99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14435",
      "authors": [
        {
          "_id": "68537b2a99bf39f9665c7990",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7991",
          "name": "Jiayu Xu",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7992",
          "name": "Ruiping Wang",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7993",
          "name": "Yan Feng",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7994",
          "name": "Yitao Zhai",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7995",
          "name": "Peng Pei",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7996",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7997",
          "name": "Xilin Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T11:53:49.000Z",
      "submittedOnDailyAt": "2025-06-19T01:22:11.793Z",
      "title": "MoTE: Modelo Mixto de Expertos Triádico para Memoria Efectiva en Grandes Modelos Multimodales",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "Los grandes modelos de MoEs (Mixture of Experts) aumentan su tamaño y mejoran su rendimiento al fijar los parámetros activos. Sin embargo, en estudios previos, se utilizaban principalmente expertos de precisión total durante la escalación esparsa. Estos mostraron altos rendimientos en tareas de fin de conjunto, pero elevanba muchos expertos en la impresión de memoria y presentaban problemas significativos con el introducción de dispositivos de borde. En este artículo, proponemos MoTE (Mixture of Ternary Experts), que es un enfoque escalable y eficiente en memoria para el entrenamiento con chequeos de densidad. En lugar de entrenar expertos de alta precisión, incrementamos a los expertos de baja precisión y los utilizamos en la escalación esparsa. Específicamente, compartimos un FFN preentrenado como experto común y entrenamos expertos ternarios con tres valores {-1, 0, 1}. Los experimentos ampliados muestran que nuestro enfoque demuestra un rendimiento comparable al baseline MoE-LLaVA basado en precisión total, pero con impresiones de memoria más pequeñas. Además, nuestro enfoque es compatible con métodos de reducción posterior y mejora significativamente cuando las restricciones de memoria se reducen. Al mantener el mismo impresionado de memoria por experto (3.4GB), la combinación de reducción posterior y MoTE aumentó la precisión promedio en tareas de fin de conjunto en un 4.3%, demostrando su eficacia en dispositivos con restricciones de memoria.",
      "upvotes": 2,
      "discussionId": "68537b2b99bf39f9665c7998",
      "ai_summary": "MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "MoEs",
        "sparse up-cycling",
        "low-precision",
        "ternary experts",
        "shared expert",
        "FFN",
        "pre-trained",
        "post-training quantization",
        "memory-constrained",
        "end tasks"
      ]
    },
    "publishedAt": "2025-06-17T07:53:49.000Z",
    "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models",
    "summary": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size\nto boost performance while maintaining fixed active parameters. However,\nprevious works primarily utilized full-precision experts during sparse\nup-cycling. Despite they show superior performance on end tasks, the large\namount of experts introduces higher memory footprint, which poses significant\nchallenges for the deployment on edge devices. In this work, we propose MoTE, a\nscalable and memory-efficient approach to train Mixture-of-Ternary-Experts\nmodels from dense checkpoint. Instead of training fewer high-precision experts,\nwe propose to train more low-precision experts during up-cycling. Specifically,\nwe use the pre-trained FFN as a shared expert and train ternary routed experts\nwith parameters in {-1, 0, 1}. Extensive experiments show that our approach has\npromising scaling trend along model size. MoTE achieves comparable performance\nto full-precision baseline MoE-LLaVA while offering lower memory footprint.\nFurthermore, our approach is compatible with post-training quantization methods\nand the advantage further amplifies when memory-constraint goes lower. Given\nthe same amount of expert memory footprint of 3.4GB and combined with\npost-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3%\naverage accuracy on end tasks, demonstrating its effectiveness and potential\nfor memory-constrained devices.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14866",
      "authors": [
        {
          "_id": "6853db4199bf39f9665c7ae5",
          "name": "Thomas Kuntz",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae6",
          "name": "Agatha Duzan",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae7",
          "name": "Hao Zhao",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae8",
          "name": "Francesco Croce",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae9",
          "name": "Zico Kolter",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7aea",
          "name": "Nicolas Flammarion",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7aeb",
          "name": "Maksym Andriushchenko",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T17:59:31.000Z",
      "submittedOnDailyAt": "2025-06-19T08:15:06.478Z",
      "title": "OS-Harm: Marcador de seguridad para medir la seguridad de los agentes de uso de computadoras",
      "submittedOnDailyBy": {
        "_id": "64c225f0129617dbaba5ae88",
        "avatarUrl": "/avatars/b12db433cd6b37c8e5299e575bdf98f9.svg",
        "isPro": false,
        "fullname": "Maksym Andriushchenko",
        "user": "MaksymAndriushchenko",
        "type": "user"
      },
      "summary": "Los agentes de uso de computadoras son agentes basados en modelos de lenguaje de alto nivel que interactúan directamente con la interfaz gráfica de usuario mediante el procesamiento de capturas de pantalla o árboles de acceso. Estos sistemas están comenzando a expandirse, pero su seguridad ha sido significativamente comprometida, lo que hace necesario evaluar su posibilidad de comportamiento nocivo. Para abordar este tema, se presenta un nuevo marco de referencia llamado OS-Harm. OS-Harm se construye en el entorno OSWorld y valida los modelos en tres categorías: uso no-positivo de usuarios intencionales, ataques de inyección de prompts y comportamiento no-positivo del modelo. Para ello, se crean 150 tareas y se exigen interacciones con agentes de sitios web (clientes de correo electrónico, editores de código, navegadores, etc.). Además, se propone un sistema de checkboxes automáticos para evaluar precisión y seguridad, logrando altos niveles de coincidencia con análisis humanos (F1 score de 0.76 y 0.79). OS-Harm evalúa la seguridad de modelos avanzados como o4-mini, Claude 3.7 Sonnet y Gemini 2.5 Pro, destacando que todos responden directamente a consultas de uso no-positivo intencional, siendo relativamente vulnerables a la inyección de prompts dinámicas y a veces mostrando comportamientos inestables. El marco de referencia OS-Harm está disponible en https://github.com/tml-epfl/os-harm.",
      "upvotes": 1,
      "discussionId": "6853db4199bf39f9665c7aec",
      "githubRepo": "https://github.com/tml-epfl/os-harm",
      "ai_summary": "A new benchmark called OS-Harm measures the safety of computer use agents interacting with GUIs, evaluating their susceptibility to misuse, prompt injection attacks, and misbehavior across various safety violations and applications.",
      "ai_keywords": [
        "LLM-based agents",
        "OS-Harm",
        "OSWorld environment",
        "deliberate user misuse",
        "prompt injection attacks",
        "model misbehavior",
        "harassment",
        "copyright infringement",
        "disinformation",
        "data exfiltration",
        "automated judge",
        "F1 score",
        "GUI"
      ]
    },
    "publishedAt": "2025-06-17T13:59:31.000Z",
    "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents",
    "summary": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14866.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c225f0129617dbaba5ae88",
      "avatarUrl": "/avatars/b12db433cd6b37c8e5299e575bdf98f9.svg",
      "fullname": "Maksym Andriushchenko",
      "name": "MaksymAndriushchenko",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14824",
      "authors": [
        {
          "_id": "6853c3af99bf39f9665c7a89",
          "name": "Yao Zhang",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8a",
          "name": "Hewei Gao",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8b",
          "name": "Haokun Chen",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8c",
          "name": "Weiguo Li",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8d",
          "name": "Yunpu Ma",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8e",
          "name": "Volker Tresp",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:50:50.000Z",
      "submittedOnDailyAt": "2025-06-19T06:31:06.085Z",
      "title": "FedNano: Aprendizaje Federado Ligero para Modelos Pre-entrenados - Avances en Progreso",
      "submittedOnDailyBy": {
        "_id": "648cbea3dee03837c823cbf2",
        "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
        "isPro": false,
        "fullname": "Shuo Chen",
        "user": "ShuoChen99",
        "type": "user"
      },
      "summary": "Los modelos de multimodal (MLLMs) muestran excelente rendimiento en tareas complejas como la inferencia de modelos multimodal y la búsqueda cruzada. Sin embargo, en entornos reales, su funcionamiento suele estar limitado por la distribución de datos multimodal y los altos requisitos de privacidad, lo que impide su uso efectivo. El aprendizaje federado (FL) permite el aprendizaje colectivo del modelo sin la centralización de datos, proporcionando una solución al problema. Sin embargo, la implementación de FL en MLLMs presenta desafíos como altas demandas computacionales, limitaciones de capacidad de los clientes, altos costos de comunicación y problemas derivados de datos heterogéneos. Los métodos actuales de FL asumen que se realiza el aprendizaje de la red neuronal completa en el lado del cliente, un supuesto que se destruye debido a la gran tamaño de los MLLMs y las demandas de comunicación. Para resolver estos limitaciones, se propone FedNano, el primer framework de FL. FedNano centraliza el modelo de lenguaje largo de memoria (LLM) en el lado del servidor y introduce un módulo llamado NanoEdge en el lado del cliente, diseñado para adaptarse a las necesidades específicas. NanoEdge utiliza un encoder propio del modelo, un conector y un adaptador de bajo rango para eliminar la necesidad de entrenamiento de la red neuronal en el lado del cliente, reduciendo en 95% la memoria del cliente y restringiendo los costos de comunicación de los parámetros del modelo a menos de 0.01%. Transmitiendo solo el NanoAdapter, FedNano aborda los desafíos de datos heterogéneos y limitaciones de recursos, manteniendo la privacidad, y equilibra el tamaño del modelo y la posibilidad de FL, ofreciendo la posibilidad de un sistema AI distribuido multimodal. Los experimentos demuestran que FedNano supera los límites de los métodos de FL existentes, equilibra el tamaño del modelo y la posibilidad de FL, y ofrece la posibilidad de un sistema AI distribuido multimodal.",
      "upvotes": 1,
      "discussionId": "6853c3af99bf39f9665c7a8f",
      "ai_summary": "FedNano is a federated learning framework that centralizes large language models on servers and uses NanoEdge modules for client-specific adaptation, addressing scalability and privacy issues.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Federated Learning",
        "NanoEdge",
        "modality-specific encoders",
        "connectors",
        "NanoAdapters",
        "low-rank adaptation",
        "client-specific adaptation",
        "compact NanoAdapter updates",
        "decentralized multimodal AI systems"
      ]
    },
    "publishedAt": "2025-06-12T13:50:50.000Z",
    "title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal\n  Large Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal\nreasoning and cross-modal retrieval but face deployment challenges in\nreal-world scenarios due to distributed multimodal data and strict privacy\nrequirements. Federated Learning (FL) offers a solution by enabling\ncollaborative model training without centralizing data. However, realizing FL\nfor MLLMs presents significant challenges, including high computational\ndemands, limited client capacity, substantial communication costs, and\nheterogeneous client data. Existing FL methods assume client-side deployment of\nfull models, an assumption that breaks down for large-scale MLLMs due to their\nmassive size and communication demands. To address these limitations, we\npropose FedNano, the first FL framework that centralizes the LLM on the server\nwhile introducing NanoEdge, a lightweight module for client-specific\nadaptation. NanoEdge employs modality-specific encoders, connectors, and\ntrainable NanoAdapters with low-rank adaptation. This design eliminates the\nneed to deploy LLM on clients, reducing client-side storage by 95%, and\nlimiting communication overhead to only 0.01% of the model parameters. By\ntransmitting only compact NanoAdapter updates, FedNano handles heterogeneous\nclient data and resource constraints while preserving privacy. Experiments\ndemonstrate that FedNano outperforms prior FL baselines, bridging the gap\nbetween MLLM scale and FL feasibility, and enabling scalable, decentralized\nmultimodal AI systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14824.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648cbea3dee03837c823cbf2",
      "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
      "fullname": "Shuo Chen",
      "name": "ShuoChen99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]