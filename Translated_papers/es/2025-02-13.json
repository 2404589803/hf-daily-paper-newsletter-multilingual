[
  {
    "paper": {
      "id": "2502.08590",
      "authors": [
        {
          "_id": "67ad79552fdac6537b43f120",
          "name": "Yujie Zhou",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f121",
          "name": "Jiazi Bu",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f122",
          "name": "Pengyang Ling",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f123",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f124",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f125",
          "name": "Qidong Huang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f126",
          "name": "Jinsong Li",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f127",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f128",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:31.817Z",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f129",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12a",
          "name": "Anyi Rao",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12b",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12c",
          "name": "Li Niu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T17:24:19.000Z",
      "title": "Light-A-Video: Fusión progresiva de la luz para la re-encuadre de videos sin aprendizaje",
      "summary": "El desarrollo de modelos de reestructuración de iluminación en imágenes ha sido impulsado por el uso de grandes conjuntos de datos y modelos preentrenados de difusión. Este avance ha permitido establecer iluminaciones coherentes, pero el reestructurado de iluminación en imágenes se ha retrasado debido a costos de entrenamiento exagerados y la escasez de conjuntos de datos de iluminación reestructurada de alta calidad y diversidad. Al aplicar modelos de reestructuración de iluminación de manera frame-by-frame, se producen discontinuidades en la iluminación y en los bordes reestructurados, lo que genera artefactos en las imágenes resultantes.\n\nEn este artículo, se propone un enfoque sin necesidad de entrenamiento llamado Light-A-Video, que logra reestructurar la iluminación de imágenes de manera suave en el tiempo. La adaptación de modelos de reestructuración de iluminación a Light-A-Video introduce dos tecnologías cruciales para mejorar la coherencia de la iluminación: primero, se diseña el módulo de Atención de Luz Consistente (CLA) para fortalecer la interacción entre frames y estabilizar la generación de iluminación en el fondo. Luego, se utiliza la física de la propagación de la luz para combinar linealmente la iluminación original y la reestructurada mediante la estrategia de Fusión de Luz Progresiva (PLF), asegurando una iluminación suave en el tiempo. Las experimentaciones muestran que Light-A-Video mejora la coherencia temporal de las imágenes reestructuradas, mantiene la calidad de las imágenes y asegura la movilidad de la iluminación entre frames. Página del proyecto: https://bujiazi.github.io/light-a-video.github.io/",
      "upvotes": 25,
      "discussionId": "67ad79572fdac6537b43f189"
    },
    "publishedAt": "2025-02-12T23:47:56.223Z",
    "title": "Light-A-Video: Training-free Video Relighting via Progressive Light Fusion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07870",
      "authors": [
        {
          "_id": "67ad79cb60ec3f444b21cbcb",
          "name": "Alex Jinpeng Wang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcc",
          "name": "Dongxing Mao",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcd",
          "name": "Jiawei Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbce",
          "name": "Weiming Han",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcf",
          "name": "Zhuobai Dong",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd0",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd1",
          "name": "Yiqi Lin",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd2",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd3",
          "name": "Libo Qin",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd4",
          "name": "Fuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd5",
          "name": "Lijuan Wang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd6",
          "name": "Min Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:59:19.000Z",
      "title": "TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas",
      "summary": "Recientemente, la generación de imágenes con texto condicionado ha recibido atención, ya que se ha logrado procesar largos y complejos textos de prueba. En la vida diaria, aparecen adornos, gráficos de diseño y firmas, entre otros, con textos complejos. La integración de texto y visión es crucial para transmitir información compleja. Sin embargo, la generación de imágenes que incluyen largos textos sigue siendo un problema a largo plazo, y la limitación actual de los conjuntos de datos es la principal causa de este problema. Para resolver esto, se presenta el nuevo conjunto de datos TextAtlas5M. Este conjunto de datos ha sido diseñado específicamente para evaluar la visualización de largos textos. Consta de 5 millones de imágenes generadas con textos largos y se combina con diferentes tipos de datos, permitiendo una evaluación detallada de la generación de imágenes con largos textos en modelos de generación a gran escala. Además, se ha construido un conjunto de prueba humano-mejora TextAtlasEval, que recorre 3 áreas de datos y construye el marco de referencia más amplio de la generación condicionada por texto. Según los resultados de la evaluación, el marco de referencia TextAtlasEval plantea grandes problemas incluso para los modelos de vanguardia (por ejemplo, GPT4o y DallE-3), pero muestra un gran desempeño para el conjunto de datos abierto. Esta evidencia muestra que TextAtlas5M ocupará un lugar efectivo como conjunto de datos para la entrenamiento y evaluación de modelos de generación de imágenes condicionada por texto en el futuro.",
      "upvotes": 24,
      "discussionId": "67ad79d260ec3f444b21cd1f"
    },
    "publishedAt": "2025-02-12T23:50:07.130Z",
    "title": "TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08639",
      "authors": [
        {
          "_id": "67ad5f25cad644864b436186",
          "name": "Qinghe Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436187",
          "name": "Yawen Luo",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436188",
          "name": "Xiaoyu Shi",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436189",
          "name": "Xu Jia",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618a",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618b",
          "name": "Tianfan Xue",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618c",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618d",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618e",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618f",
          "name": "Kun Gai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T18:55:36.000Z",
      "title": "Reconocimiento 3D de imágenes y framework de controlable frames para la generación de vídeos a partir de texto.",
      "summary": "En este estudio se propone un nuevo marco de trabajo llamado CineMaster. Este marco de trabajo puede realizar reconocimiento 3D y generar animaciones a partir de texto. Nuestro objetivo es proporcionar a los usuarios la posibilidad de controlar de manera similar a un director de películas profesionales. Esto permite la posición precisa de objetos dentro de escenas, el manejo flexible de objetos y cámaras en el espacio 3D, y el control intuitivo de la secuencia de frames renderizados. Para lograrlo, CineMaster opera en dos etapas. En la primera etapa, se diseña un flujo de trabajo interactivo que permita a los usuarios construir señales condicionales de reconocimiento 3D de manera intuitiva. Se definen las posiciones de las cajas de bounding box de los objetos en el espacio 3D y se define el movimiento de la cámara. En la segunda etapa, las señales de control (mapas de profundidad renderizados, rutas de movimiento de la cámara, etiquetas de clase de los objetos) se convierten en guías para un modelo de difusión de animación a partir del texto, y se generan los contenidos de animación que los usuarios deseen. Además, para superar la escasez de datos explicados sobre el movimiento de objetos 3D y la postura de la cámara, se construye cuidadosamente un plug-in de explicación de datos automática para extraer las cajas de bounding box de objetos 3D y las rutas de movimiento de la cámara a partir de grandes cantidades de datos de animación. Los experimentos detallados y cuantitativos muestran que CineMaster supera significativamente los métodos actuales y logra realizar la generación de animación a partir de texto con una reconocimiento 3D claro. Página del proyecto: https://cinemaster-dev.github.io/",
      "upvotes": 22,
      "discussionId": "67ad5f26cad644864b4361cf"
    },
    "publishedAt": "2025-02-12T21:55:44.479Z",
    "title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6063
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08047",
      "authors": [
        {
          "_id": "67ad92bfbbf3810ab20595c2",
          "name": "Henry Hengyuan Zhao",
          "hidden": false
        },
        {
          "_id": "67ad92bfbbf3810ab20595c3",
          "name": "Difei Gao",
          "hidden": false
        },
        {
          "_id": "67ad92bfbbf3810ab20595c4",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T01:06:10.000Z",
      "title": "WorldGUI: Herramienta de Interfaz Gráfica Desktop para la Automática de Pruebas de Overflow\n\n**Nota:** La traducción se ha realizado manteniendo la estructura y formato del texto original, asegurando la precisión en los términos técnicos.",
      "summary": "Actualmente, los agentes de GUI basados en la gestión de elementos de GUI logran un rendimiento excepcional. Sin embargo, dejan problemas muy complejos sin resolver, especialmente sensibilidad a la estado inicial de la entorno, lo que los afecta significativamente. Concretamente, las pequeñas diferencias en el estado inicial, como si el software objetivo no está abierto o si la interfaz no está en su estado por defecto, a menudo provocan errores en la planificación. Este problema es ampliamente presente en los escenarios de usuario real, pero los actuales benchmarks no pueden evaluarlo. En este artículo, se presenta WorldGUI, un nuevo benchmark de GUI. Este benchmark diseña tareas de GUI con diferentes estados iniciales para mimetizar la interacción real entre usuarios y computadoras. Incluye una amplia gama de tareas de software populares como PowerPoint, VSCode y Adobe Acrobat. Además, para responder a las tareas de automatización de GUI dinámicas, se propone un marco de trabajo generalizado llamado GUI-Thinker. Este marco de trabajo utiliza estructuras de evaluación para gestionar eficazmente la incertidumbre y la complejidad de la interacción de GUI. A través de los resultados de los experimentos, GUI-Thinker registra un aumento del 14.9% en la tasa de éxito en tareas de WorldGUI en comparación con Claude-3.5 (Uso de Computadora), demostrando la eficacia de nuestro marco de trabajo basado en pensamiento crítico.",
      "upvotes": 19,
      "discussionId": "67ad92c1bbf3810ab205961c"
    },
    "publishedAt": "2025-02-13T01:39:08.775Z",
    "title": "WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08047.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647d7eb9770c299e56f5b39b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647d7eb9770c299e56f5b39b/CC5JJgkyLkXOxw-BeT4G5.jpeg",
      "fullname": "Hengyuan Zhao",
      "name": "hhenryz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07563",
      "authors": [
        {
          "_id": "67ad7929dc2968691c241147",
          "user": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "isPro": false,
            "fullname": "Weigao Sun",
            "user": "weigao266",
            "type": "user"
          },
          "name": "Weigao Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:37.445Z",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c241148",
          "user": {
            "_id": "66ea643899af9ac3463639b1",
            "avatarUrl": "/avatars/252d470e761a57834dee3dbc60dfefed.svg",
            "isPro": false,
            "fullname": "Disen Lan",
            "user": "landisen",
            "type": "user"
          },
          "name": "Disen Lan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:33.746Z",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c241149",
          "name": "Yiran Zhong",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c24114a",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c24114b",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T14:01:39.000Z",
      "title": "LASP-2: Revisión y híbrido de la paralelización secuencial de la atención lineal",
      "summary": "Un enfoque de modelado de secuencias lineales, como el atención lineal, ofrece entrenamiento en tiempo lineal y inferencia en memoria constante, independientemente de la longitud de la secuencia, proporcionando excelentes ventajas. Sin embargo, los métodos actuales de cálculo paralelo de secuencias (SP) no están optimizados para las características de multiplicación a la derecha del atención lineal, ni son capaces de manejar la escalabilidad en sistemas de dispersión de largas secuencias debido a la baja eficiencia de paralelismo computacional y a estrategias de comunicación como el \"Ring Style\". En este artículo, se presenta un nuevo método de SP llamado LASP-2, con el objetivo de mejorar tanto la paralelismo de comunicación como el de cálculo durante el entrenamiento de modelos de transformer con atención lineal y largas secuencias de entrada. En comparación con LASP, LASP-2 reconoce los mínimos requisitos de comunicación en la capa de atención lineal y reestructura todo el flujo de trabajo de comunicación y cálculo de LASP. Esto permite que solo sea necesario una sola comunicación AllGather en el estado de memoria indirecta, de tamaño no dependiente de la longitud, lo que implica un gran aumento en la paralelismo y el enlazamiento de comunicación y cálculo. Además, se propone una extensión de LASP-2 llamada LASP-2H, que aplica un re diseño de comunicación similar a los módulos de atención estándar para proporcionar una solución eficiente de SP para modelos mixtos que combinan atención lineal y estándar. En la evaluación del modelo Linear-Llama3, se demostró la eficacia de LASP-2 y LASP-2H, especialmente, LASP-2 logró un aumento del 15.2% en la velocidad de entrenamiento en comparación con LASP y un aumento del 36.6% en comparación con el atención de Ring, al procesar secuencias de longitud de 2048K en 64 GPUs. El código está disponible en https://github.com/OpenSparseLLMs/Linear-MoE.",
      "upvotes": 17,
      "discussionId": "67ad792adc2968691c241173"
    },
    "publishedAt": "2025-02-12T23:47:31.651Z",
    "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07563.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07346",
      "authors": [
        {
          "_id": "67ac4e046b8c86e0cc7988f0",
          "user": {
            "_id": "649d1d4c379eada9a580cf59",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d1d4c379eada9a580cf59/ucXv7KoJDEB3Phgn-Dn5E.png",
            "isPro": false,
            "fullname": "xuhuang",
            "user": "ggdcr",
            "type": "user"
          },
          "name": "Xu Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:25:17.555Z",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f1",
          "name": "Wenhao Zhu",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f2",
          "name": "Hanxu Hu",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f3",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f4",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f5",
          "name": "Shujian Huang",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f6",
          "name": "Fei Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T08:17:19.000Z",
      "title": "BenchMAX: Sistema de Evaluación Detallado Multilingüe para Modelos de Lenguaje de Grandes Escalas",
      "summary": "Anteriormente, los marcos de evaluación multilingües se centraban principalmente en tareas sencillas de comprensión, pero los modelos de lenguaje de gran escala (LLMs) enfatizan capacidades de alto nivel, como seguir palabras, razonar, entender contextos largos y generar código. Sin embargo, la medida de estas capacidades en varios idiomas ha sido poco investigada. Para abordar este problema, se presenta BenchMAX, un marco de evaluación multilingüe. Este marco permite una comparación equitativa de estas habilidades en varios idiomas. Para mantener la calidad, los datos se traducen de inglés a 16 idiomas diferentes y se registran independientemente por 3 hablantes nativos. Además, se presentan nuevos desafíos de traducción en la construcción del conjunto de datos. Las experimentaciones extendidas en BenchMAX revelan las diferencias efectivas de las habilidades clave en cada idioma y muestran que aunque el tamaño del modelo aumenta, la diferencia en el rendimiento persiste. BenchMAX ofrece un plataforma uniforme para la evaluación en varios idiomas y proporciona un test benchmark para fomentar el desarrollo de modelos de lenguaje en múltiples idiomas. El conjunto de datos y el código están accesibles públicamente.",
      "upvotes": 14,
      "discussionId": "67ac4e056b8c86e0cc798952"
    },
    "publishedAt": "2025-02-13T03:34:47.873Z",
    "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07346.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d1d4c379eada9a580cf59",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d1d4c379eada9a580cf59/ucXv7KoJDEB3Phgn-Dn5E.png",
      "fullname": "xuhuang",
      "name": "ggdcr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.08127",
      "authors": [
        {
          "_id": "67ad5ca29109885ce9b859e4",
          "name": "Lingfei Qian",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e5",
          "name": "Weipeng Zhou",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e6",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e7",
          "name": "Xueqing Peng",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e8",
          "user": {
            "_id": "63b58ed5889aa6707f0bb0f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
            "isPro": true,
            "fullname": "Jimin Huang",
            "user": "jiminHuang",
            "type": "user"
          },
          "name": "Jimin Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T02:44:52.979Z",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e9",
          "user": {
            "_id": "6479f4317c18dca75e9a9324",
            "avatarUrl": "/avatars/9aa709230b057f57ee4415c04a622c63.svg",
            "isPro": false,
            "fullname": "Xie",
            "user": "QianqianXie1994",
            "type": "user"
          },
          "name": "Qianqian Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:22:01.539Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T05:13:04.000Z",
      "title": "Teoría de la Aplicabilidad de los Modelos de Lenguaje Aumentados por Inferencia en Finanzas",
      "summary": "El reciente desarrollo de grandes modelos de lenguaje (LLMs) ha demostrado una potente capacidad de lógica general, pero su efectividad en el campo financiero ha sido poco investigada. En este estudio, se evaluaron tres tareas financieras complejas que incluyen textos financieros, datos de tablas y ecuaciones, utilizando lógica numérica, análisis de detalles de tablas, comprensión de términos financieros, procesamiento de largos contextos y resolución de problemas basados en ecuaciones. Se utilizaron 16 modelos de lógica fuerte y generales de LLMs. Los resultados confirmaron que la mejora de los conjuntos de datos y la mejora del entrenamiento previo pueden aumentar el efecto en el campo financiero. Sin embargo, la ajuste de la lógica de razonamiento basada en la Teoría de la Razón (CoT) no siempre produce efectos consistentes. Además, todos los estrategias de lógica tienen problemas para mejorar el rendimiento en tareas de largo contexto o múltiples tablas. Para resolver estos limitaciones, se desarrolló un modelo de mejora de lógica financiera basado en Llama-3.1-8B-Instruct, y se realizó un entrenamiento de reinforcement con ajustes de CoT y rutas lógicas específicas de razonamiento. Con solo un ajuste simple en un conjunto de datos financiero, el modelo logró un aumento de rendimiento positivo del 10% en todas las tareas, superando a todos los modelos de 8B y, aún más, superando los promedios de Llama3-70B-Instruct y Llama3.1-70B-Instruct. Estos resultados subrayan la necesidad de cambios específicos de razonamiento en tareas financieras y proponen futuras direcciones como la lógica multitabla, el procesamiento de largo contexto y la comprensión de términos financieros. Todos los conjuntos de datos, modelos y códigos están disponibles públicamente. Además, se introdujo un rincón de línea base para los conjuntos de datos y modelos de prueba futuros.",
      "upvotes": 13,
      "discussionId": "67ad5ca59109885ce9b85a5b"
    },
    "publishedAt": "2025-02-12T21:45:28.944Z",
    "title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08127.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63b58ed5889aa6707f0bb0f4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
      "fullname": "Jimin Huang",
      "name": "jiminHuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07864",
      "authors": [
        {
          "_id": "67ad5b3a007d78b391946a57",
          "user": {
            "_id": "643f55d4ec817b766686438a",
            "avatarUrl": "/avatars/0feb460432c92ab9ada0d417a7a38f6a.svg",
            "isPro": false,
            "fullname": "mengfanxu",
            "user": "fxmeng",
            "type": "user"
          },
          "name": "Fanxu Meng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:22:03.808Z",
          "hidden": false
        },
        {
          "_id": "67ad5b3a007d78b391946a58",
          "name": "Zengwei Yao",
          "hidden": false
        },
        {
          "_id": "67ad5b3a007d78b391946a59",
          "name": "Muhan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:20:18.000Z",
      "title": "TransMLA: El potencial de atención multi-headed te será útil para ti.",
      "summary": "Los grandes modelos de lenguaje modernos (LLMs) enfrentan actualmente problemas de congestión de comunicación y frecuentemente están limitados por restricciones de cálculo en la hardware disponible. La Multi-Layer Entity (MLA) es una solución que aborda estas desafíos utilizando matrices de bloqueo en las capas clave-valor (KV), permitiendo que estas estados potenciales compresos se almacenen en un caché. Este enfoque reduce significativamente el tamaño del caché KV en comparación con los modelos tradicionales, mejorando tanto la velocidad de inferencia como la eficiencia. Además, MLA mejora la expresividad utilizando matrices de proyección, contribuyendo a reducir la congestión de comunicación. Sin embargo, MLA ha demostrado su eficiencia y eficacia en modelos como Deepseek V2/V3/R1, pero muchos de los principales proporcionadores de modelos dependen de GQA y no tienen planes para integrarlo, lo que limita su uso más amplio. En este artículo, demostramos que GQA puede mantener la expresividad similar a MLA, mientras que MLA no es posible en el caso contrario. Para fomentar el uso más amplio de MLA, presentamos **TransMLA**, un método posterior de entrenamiento que permite transformar modelos pre-entrenados basados en GQA (como LLaMA, Qwen, Mixtral) en modelos basados en MLA. Después de la transformación, los modelos pueden mejorar su expresividad mediante entrenamiento adicional sin aumentar el tamaño del caché KV. Además, desarrollamos técnicas de velocidad de inferencia optimizadas para MLA que mantienen bajos tiempos de respuesta y permiten un diseño más eficiente del Deepseek R1.",
      "upvotes": 13,
      "discussionId": "67ad5b3b007d78b391946a79"
    },
    "publishedAt": "2025-02-12T21:41:19.791Z",
    "title": "TransMLA: Multi-head Latent Attention Is All You Need",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643f55d4ec817b766686438a",
      "avatarUrl": "/avatars/0feb460432c92ab9ada0d417a7a38f6a.svg",
      "fullname": "mengfanxu",
      "name": "fxmeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.08606",
      "authors": [
        {
          "_id": "67ad77f9cd8de299e5049c05",
          "name": "Dan Busbridge",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c06",
          "name": "Amitis Shidani",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c07",
          "name": "Floris Weers",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c08",
          "name": "Jason Ramapuram",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c09",
          "name": "Etai Littwin",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c0a",
          "name": "Russ Webb",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T17:52:47.000Z",
      "title": "Distiriusion Scaling Las",
      "summary": "Nosotros ofrecemos herramientas de escalado de resumen basadas en la asignación de cálculo entre modelos de profesor y estudiante para predecir el rendimiento de modelos de resumen basados en asignación de cálculo. Lo que hemos encontrado nos ayuda a reducir los riesgos asociados con el uso del resumen en función de la escalado. La asignación de cálculo entre modelos de profesor y estudiante tiene como objetivo maximizar el rendimiento del modelo estudiante. Ofrecemos recetas óptimas de cálculo para el resumen cuando el modelo de profesor existe o necesita entrenamiento. Cuando el estudiante es múltiple o el modelo de profesor ya existe, el resumen mejora en términos de cálculo antes de que el nivel de cálculo de entrenamiento predictivo aumente en relación con el tamaño del modelo estudiante. En cambio, cuando el estudiante es solo uno o el modelo de profesor necesita entrenamiento, se debe priorizar el entrenamiento. Además, proporcionamos retroalimentación basada en los resultados de nuestras grandes investigaciones sobre el resumen para profundizar la comprensión del resumen y proporcionar información para el diseño de experimentos.",
      "upvotes": 8,
      "discussionId": "67ad77fccd8de299e5049d06"
    },
    "publishedAt": "2025-02-12T23:41:41.281Z",
    "title": "Distillation Scaling Laws",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08606.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6063
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08168",
      "authors": [
        {
          "_id": "67ad5f32d1a5243cc4fa38ad",
          "user": {
            "_id": "64a0ed5ed5374ca472cfb0ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
            "isPro": false,
            "fullname": "ZhimingMa",
            "user": "JimmyMa99",
            "type": "user"
          },
          "name": "Zhiming Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:57.239Z",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38ae",
          "name": "Xiayang Xiao",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38af",
          "name": "Sihao Dong",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b0",
          "name": "Peidong Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b1",
          "name": "HaiPeng Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b2",
          "name": "Qingyun Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T07:19:36.000Z",
      "title": "SARChat-Bench-2M: Interpretación de un benchmark de lenguaje visión multi-tarea para imágenes SAR",
      "summary": "En el campo de la interpretación de imágenes de radar de apertura sintética (SAR), los modelos de lenguaje de visión (VLMs) han logrado una sorprendente revolución en el procesamiento de lenguaje natural y la comprensión de imágenes, pero su aplicación en el ámbito de proyectos está limitada debido a la falta de conocimientos domésticos. En este artículo se propone un primer conjunto de datos de diálogos de tipos múltiples de gran escala. Este conjunto de datos incluye aproximadamente 2 millones de pares de imágenes de alta calidad y texto, con descripciones detalladas de los objetivos y una variedad de escenarios. Este conjunto de datos apoya tareas importantes como la comprensión visual y la detección de objetos, y presenta una innovación única: se desarrolla en este artículo un conjunto de datos de lenguaje de visión para el campo de SAR y un benchmark, lo que permite evaluar la capacidad de interpretación de imágenes SAR de los VLMs, y proporciona un marco para la construcción de conjuntos de datos de tipos múltiples en diversas áreas de observación remota. Mediante experimentos con 16 VLMs principales, se demuestra completamente el efecto del conjunto de datos, y se establece exitosamente el primer benchmark de diálogo de tareas de tipos múltiples en el campo de SAR. Este proyecto se lanza en https://github.com/JimmyMa99/SARChat, con el objetivo de promover el desarrollo profundo y la amplia aplicación de modelos de lenguaje de visión para el campo de SAR.",
      "upvotes": 8,
      "discussionId": "67ad5f37d1a5243cc4fa399c"
    },
    "publishedAt": "2025-02-12T21:57:30.420Z",
    "title": "SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/LvHzRQCttMAvKS-LM0ZDH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08168.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64a0ed5ed5374ca472cfb0ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
      "fullname": "ZhimingMa",
      "name": "JimmyMa99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.08524",
      "authors": [
        {
          "_id": "67ad783da2808b57a3cd3316",
          "name": "Jihoon Tack",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3317",
          "name": "Jack Lanchantin",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3318",
          "name": "Jane Yu",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3319",
          "name": "Andrew Cohen",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331a",
          "name": "Ilia Kulikov",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331b",
          "name": "Janice Lan",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331c",
          "name": "Shibo Hao",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331d",
          "name": "Yuandong Tian",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331e",
          "name": "Jason Weston",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331f",
          "user": {
            "_id": "659a395421a7431643caedda",
            "avatarUrl": "/avatars/c1e0bbcedce68fe3b4fe39e0cf01c65c.svg",
            "isPro": false,
            "fullname": "Xian Li",
            "user": "xlxxl",
            "type": "user"
          },
          "name": "Xian Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T04:42:38.302Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T16:00:11.000Z",
      "title": "Certainly! Here is the translation of the provided text into Spanish, maintaining professionalism and accuracy:\n\n**Traducción al Español:**\n\"Ejercicios de entrenamiento de conceptos continuos para LLM\"\n\n**Nota:**\n- \"LLM\" se mantiene como \"LLM\" para preservar la acronomía común en el campo de la inteligencia artificial.\n- \"Continuos\" se traduce como \"continuos\" para mantener la precisión técnica.\n- \"Conceptos\" se traduce como \"conceptos\" para reflejar el significado de \"concepts\" en español.\n\nSi necesitas la traducción al español de otro texto o tienes alguna otra pregunta, no dudes en decírmelo.",
      "summary": "El siguiente predicción de tokens ha sido utilizada consistentemente como un objetivo de entrenamiento estándar en el aprendizaje previo de modelos de lenguaje grandes. Se optimizan errores estructurales a nivel de token para aprender representaciones. Proponemos un nuevo marco de aprendizaje previo \"CoCoMix\". Este marco predice conceptos continuos en un autoencoder esparso previamente entrenado y mezcla las representaciones ocultas de los tokens con el estado oculto del modelo. Según varios benchmarks (modelado de lenguaje y tareas de teoría de razonamiento posterior), CoCoMix muestra una eficiencia de muestra alta y supera siempre la predicción de siguiente token, la propagación de conocimiento y la inserción de tokens posteriores. Hemos encontrado que la combinación de aprendizaje de conceptos y cruzados en marcos de aprendizaje es crucial para mejorar el rendimiento. Además, CoCoMix permite la revisión y el ajuste directos de los conceptos predecidos, y proporciona transparencia a los procesos de razonamiento interno del modelo, mejorando su interpretabilidad y manipulabilidad.",
      "upvotes": 6,
      "discussionId": "67ad783ea2808b57a3cd3361"
    },
    "publishedAt": "2025-02-12T23:42:44.287Z",
    "title": "LLM Pretraining with Continuous Concepts",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08524.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6063
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06533",
      "authors": [
        {
          "_id": "67accc647e1fcf03e14b1033",
          "user": {
            "_id": "6637cd3e691043ccb248d0fd",
            "avatarUrl": "/avatars/94cf09cf817327be50ecba75f7f60fa1.svg",
            "isPro": false,
            "fullname": "Jean Vassoyan",
            "user": "supertardigrade",
            "type": "user"
          },
          "name": "Jean Vassoyan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:24:24.993Z",
          "hidden": false
        },
        {
          "_id": "67accc647e1fcf03e14b1034",
          "user": {
            "_id": "63da60458658cbc1cc489bd7",
            "avatarUrl": "/avatars/620ce7ea229de7abe4dc9ea93021f0e4.svg",
            "isPro": false,
            "fullname": "Nathanaël Beau",
            "user": "Nbeau",
            "type": "user"
          },
          "name": "Nathanaël Beau",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-12T16:29:25.829Z",
          "hidden": false
        },
        {
          "_id": "67accc647e1fcf03e14b1035",
          "user": {
            "_id": "66470e227d73a39a342866e4",
            "avatarUrl": "/avatars/cb0746295492044c483a470692b9637c.svg",
            "isPro": false,
            "fullname": "Roman Plaud",
            "user": "lecraquito",
            "type": "user"
          },
          "name": "Roman Plaud",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:24:27.330Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T14:56:25.000Z",
      "title": "Ignora KL Penalty y fortalezca la búsqueda de tokens importantes para mejorar el ajuste micro de RL.",
      "summary": "La capacidad de alcanzar objetivos a largo plazo es un problema importante en el desarrollo de los grandes modelos de lenguaje (LLMs). Para resolverlo, se puede realizar aprendizaje por refuerzo (RL) para ajustar los modelos. Sin embargo, el explorado de los LLMs es muy difícil, lo que hace que, a pesar de encontrar nuevas soluciones, no se pueda reducir significativamente la distancia entre los modelos ajustados y los objetivos. Por lo general, se controla con una penalización de Kullback-Leibler (KL). En este artículo, se investiga la dinámica de exploración de un pequeño modelo de lenguaje que realiza tareas aritméticas simples, y se muestra cómo el aprendizaje por refuerzo afecta la exploración y la importancia de los \"tokens KL\" que tienen un gran impacto en los resultados finales. Como resultado, se propone una simple mejora en la penalización de KL para optimizar el proceso de aprendizaje por refuerzo durante la etapa de ajuste.",
      "upvotes": 5,
      "discussionId": "67accc657e1fcf03e14b109e"
    },
    "publishedAt": "2025-02-13T03:47:28.654Z",
    "title": "Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06533.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66470e227d73a39a342866e4",
      "avatarUrl": "/avatars/cb0746295492044c483a470692b9637c.svg",
      "fullname": "Roman Plaud",
      "name": "lecraquito",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06145",
      "authors": [
        {
          "_id": "67ad9fb9731ff0d7da9f40e9",
          "user": {
            "_id": "67ad9f06040354c9105b00bc",
            "avatarUrl": "/avatars/39e9f4c48c93bb33f155390653936fc1.svg",
            "isPro": false,
            "fullname": "LiHu",
            "user": "Hookszdp",
            "type": "user"
          },
          "name": "Li Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:24.286Z",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ea",
          "name": "Guangyuan Wang",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40eb",
          "name": "Zhen Shen",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ec",
          "name": "Xin Gao",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ed",
          "name": "Dechao Meng",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ee",
          "name": "Lian Zhuo",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ef",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40f0",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40f1",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T04:20:11.000Z",
      "title": "Animat・Nyanny 2: Animación de imágenes de personajes de alta calidad utilizando el complemento ambiental",
      "summary": "Recientemente, se ha observado el desarrollo de técnicas como Animate Anyone en el campo de la animación de mapas de características basadas en modelos de difusión. Sin embargo, estas tecnologías a veces presentan dificultades en la creación de conexiones lógicas entre los mapas de características y el entorno. Para abordar este problema, presentamos Animate Anyone 2, un método que se centra en animar mapas de características de acuerdo con las funciones del entorno. Para lograr esto, extraemos señales de movimiento desde la video fuente, pero también incluimos la representación del entorno como entrada condicional. El entorno se configura como un área que no incluye los mapas de características, permitiendo la creación de mapas de características que mantengan la coherencia con el contexto del entorno. Para expresar efectivamente la relación entre los mapas de características y el entorno, proponemos una estrategia de mascaras sin restricciones de forma. Además, para mejorar la precisión de la interacción entre objetos, utilizamos guías de objetos para extraer características de objetos que interactúan y, mediante espectral branding, inyectamos estas características. También proponemos una estrategia para ajustar la postura de objetos que pueda manejar diferentes patrones de movimiento. Nuestros resultados experimentales demuestran la excelente performance de nuestro método.",
      "upvotes": 3,
      "discussionId": "67ad9fbb731ff0d7da9f4145"
    },
    "publishedAt": "2025-02-13T03:45:43.646Z",
    "title": "Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06145.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ad9f06040354c9105b00bc",
      "avatarUrl": "/avatars/39e9f4c48c93bb33f155390653936fc1.svg",
      "fullname": "LiHu",
      "name": "Hookszdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06872",
      "authors": [
        {
          "_id": "67ad7da995ff670869168209",
          "name": "Bo Ni",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820a",
          "name": "Zheyuan Liu",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820b",
          "name": "Leyao Wang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820c",
          "name": "Yongjia Lei",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820d",
          "name": "Yuying Zhao",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820e",
          "name": "Xueqi Cheng",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820f",
          "name": "Qingkai Zeng",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168210",
          "name": "Luna Dong",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168211",
          "name": "Yinglong Xia",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168212",
          "name": "Krishnaram Kenthapadi",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168213",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168214",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:27.740Z",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168215",
          "name": "Md Mehrab Tanjim",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168216",
          "name": "Nesreen Ahmed",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168217",
          "name": "Xiaorui Liu",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168218",
          "name": "Wenqi Fan",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168219",
          "name": "Erik Blasch",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821a",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821b",
          "name": "Meng Jiang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821c",
          "name": "Tyler Derr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T06:50:47.000Z",
      "title": "Trastorrería Stretchiauwa Garshion Pollarr Jowng Ya Iant Rng Jue Esion Modulo: Asid",
      "summary": "El Retórico de la Revolución de la Agricultura (RAG) es una tecnología avanzada diseñada para resolver los problemas de contenido generado por la inteligencia artificial (AIGC). Integrando la búsqueda por categorías en la generación de contenido, el RAG proporciona conocimientos externos actuales y confiables, reduce el ruido irrelevante y garantiza la relevancia en una amplia gama de tareas. Sin embargo, el paradigma del RAG ha encontrado nuevos riesgos, incluyendo problemas de robustez, preocupaciones de privacidad, ataques en contra, y cuestiones de responsabilidad, independientemente de su éxito y posibilidades. La resolución de estos riesgos es crucial para la aplicación futura del RAG y su confianza. Métodos para aumentar la confianza en el RAG han sido desarrollados, pero falta una visión y un marco estructurado uniformes en esta área de investigación. Por lo tanto, este artículo se esfuerza por proporcionar una guía detallada para el desarrollo de sistemas RAG confiables. El discurso se centra en cinco aspectos principales: confianza, privacidad, seguridad, equidad, explicabilidad y problemas de responsabilidad. En cada aspecto, se proporciona un marco general y tecnologías para comprender los problemas actuales, evaluar soluciones existentes y determinar direcciones de investigación futuras. Además, se menciona específicamente las aplicaciones que tienen un gran impacto en la introducción y innovación de sistemas RAG confiables.",
      "upvotes": 3,
      "discussionId": "67ad7daa95ff670869168251"
    },
    "publishedAt": "2025-02-13T00:06:04.056Z",
    "title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05167",
      "authors": [
        {
          "_id": "67aa583c3a878652daeae02e",
          "user": {
            "_id": "60e4738a8c0ddd18fc27ff88",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e4738a8c0ddd18fc27ff88/lpLeeIW8r85RTY4fGZTva.jpeg",
            "isPro": false,
            "fullname": "Ali Modarressi",
            "user": "amodaresi",
            "type": "user"
          },
          "name": "Ali Modarressi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:42.560Z",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae02f",
          "name": "Hanieh Deilamsalehy",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae030",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:26:01.327Z",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae031",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae032",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae033",
          "name": "Seunghyun Yoon",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae034",
          "name": "Hinrich Schütze",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:49:46.000Z",
      "title": "NoLiMa: Evaluación del contexto más que respuesta directa textual",
      "summary": "Recientemente, los grandes modelos de lenguaje (LLMs) soportan contextos largos de 128K a 1M tokens. Para evaluar estas funciones, un método popular es el \"Nail-In-Up-Lion\" (NIAH). Este método consiste en buscar información relacionada (「nail」) en un largo contexto irrelevante (「sack」). Se ha expandido este método para incluir intervenciones, ciclos de verdad y lógica dentro del texto. Sin embargo, los modelos pueden simplificar el problema utilizando las correspondencias contextuales entre el \"nail\" y el \"sack\". Para resolver esto, hemos introducido el benchmark \"NoLiMa\", que es una extensión del NIAH. Este benchmark se diseñó de manera que los problemas y el \"nail\" tengan un mínimo de repetición de palabras, y el modelo debe inferir la posición del \"nail\" dentro del \"sack\" utilizando ciclos potenciales. Estamos evaluando 12 modelos LLMs populares. Estos modelos funcionan bien con contextos cortos (＜1K), pero su rendimiento disminuye significativamente cuando el contexto es más largo. Por ejemplo, solo 5 de 10 modelos superan el 50% de una línea de base fuerte basada en contextos cortos en un contexto de 32K. Además, GPT-4o es una excepción notable, y su rendimiento cae de casi el 100% (99.3%) a 69.7%. Nuestro análisis sugiere que la estructura de acción que utiliza el contexto se vuelve más difícil y que la búsqueda de información relevante se vuelve más complicada cuando el contexto es más largo.",
      "upvotes": 3,
      "discussionId": "67aa583d3a878652daeae06c"
    },
    "publishedAt": "2025-02-13T00:04:29.194Z",
    "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07737",
      "authors": [
        {
          "_id": "67ad5d2f8436e8ea7abb7a15",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a16",
          "name": "Shuming Ma",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a17",
          "name": "Xu Sun",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a18",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T17:57:53.000Z",
      "title": "Predicción de Bloques: Generación de Video por Modelado de Regresión Automática Semi-Manual",
      "summary": "El método de Predicción de Tokens Futuros (NTP) se ha utilizado en la generación automática de vídeos, pero presenta problemas como una relación de dependencia unidireccional y un lento velocismo de inferencia. En este artículo, se propone un marco semi-automático de recuperación (semi-AR) para la generación de vídeos, llamado Next-Block Prediction (NBP). El contenido de vídeo se divide en bloques de la misma dimensión y se cambia la unidad de generación para que cada token en un bloque pueda predecir el token correspondiente en el siguiente bloque simultáneamente. A diferencia de los modelos tradicionales de AR, se utiliza atención bidireccional dentro de cada bloque, permitiendo capturar una fuerte dependencia espacial. Prediciendo múltiples tokens en paralelo, el modelo NBP reduce significativamente el proceso de generación y logra una inferencia rápida y eficiente. En UCF101, el modelo alcanza un FVD de 103.3 y en K600 un FVD de 25.5, superando los modelos NTP en un promedio de 4.4 puntos. Además, al reducir el tiempo de inferencia, el modelo NBP genera 8.89 frames por segundo a una resolución de 128x128, alcanzando una velocidad de trabajo 11 veces más rápida. Además, la escalabilidad del modelo se demuestra al examinar un rango de parámetros de 700M a 3B, mostrando un gran aumento en la calidad de generación, con un FVD de 55.3 en UCF101 y de 19.5 en K600, demostrando su capacidad de escalabilidad.",
      "upvotes": 3,
      "discussionId": "67ad5d308436e8ea7abb7a3d"
    },
    "publishedAt": "2025-02-12T21:48:00.325Z",
    "title": "Next Block Prediction: Video Generation via Semi-Autoregressive Modeling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07737.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d2e681b8448e1785bbda06",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
      "fullname": "Shuhuai Ren",
      "name": "ShuhuaiRen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07599",
      "authors": [
        {
          "_id": "67ad5bd2ac32a8e230fc8996",
          "name": "Xiliang Yang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8997",
          "name": "Feng Jiang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8998",
          "name": "Qianen Zhang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8999",
          "name": "Lei Zhao",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc899a",
          "name": "Xiao Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T14:49:44.000Z",
      "title": "DPO-Shift: Optimización de la Distribución de la Preferencia Directa",
      "summary": "La optimización directa de preferencias (Direct Preference Optimization, DPO) y sus variantes están ganando popularidad gradualmente para ajustar modelos de lenguaje con las preferencias humanas. Estos métodos tienen como objetivo mejorar la diferenciación entre lo escogido (o preferido) y lo rechazado (o no preferido) por el modelo. Sin embargo, se ha observado en investigaciones previas que la probabilidad de selección disminuye durante el entrenamiento, un fenómeno conocido como \"cambio de frecuencia\". Para resolver este problema, este estudio utiliza una metodología para deformar la distribución de la probabilidad de selección. Este enfoque, analizado tanto teóricamente como experimentalmente, ha demostrado ser una transformación fundamental que mejora la probabilidad de selección sin perder el recompensa, revelando así su importancia. Además, este método supera a DPO en tareas como MT-Bench, que evalúan probabilidades diseñadas. Creemos que este estudio demuestra que se puede mitigar eficazmente el problema de cambio de frecuencia en DPO mediante una solución sencilla basada en teoría. El código está disponible en https://github.com/Meaquadddd/DPO-Shift.",
      "upvotes": 3,
      "discussionId": "67ad5bd3ac32a8e230fc89a7"
    },
    "publishedAt": "2025-02-12T21:43:42.404Z",
    "title": "DPO-Shift: Shifting the Distribution of Direct Preference Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07599.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66270fcef7cf69d4223a8a3f",
      "avatarUrl": "/avatars/115db0326737e65318c92a7b8dc5ed6a.svg",
      "fullname": "Xiao Li",
      "name": "xli0982",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04411",
      "authors": [
        {
          "_id": "67adad972883187d78409a7a",
          "name": "Kunfeng Lai",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7b",
          "name": "Zhenheng Tang",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7c",
          "name": "Xinglin Pan",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7d",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7e",
          "user": {
            "_id": "63024676056ec3a2a8714b24",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
            "isPro": false,
            "fullname": "Xiang Liu",
            "user": "Dominic789654",
            "type": "user"
          },
          "name": "Xiang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:45:17.030Z",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7f",
          "name": "Haolan Chen",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a80",
          "name": "Li Shen",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a81",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a82",
          "name": "Xiaowen Chu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T11:26:30.000Z",
      "title": "Media Tar: Conflictos de Parámetros Reducidos y Memoria Eficiente Basada en la Confianza de los LLM Integrados",
      "summary": "La integración de modelos se realiza al fortalecer modelos grandes de lenguaje (LLMs) fine-tunados para diversas tareas. Sin embargo, el colisionamiento de parámetros está relacionado con una pérdida de rendimiento promedio. La rutina de modelos selecciona individualmente modelos durante la inferencia para resolver este problema, pero esto genera costos de estoreo y cálculos excesivos y no se aprovechan conocimientos compartidos de otros modelos. En este estudio, se observa que el grado de colisionamiento de parámetros varía según las capas, y se basa en esto para promediar capas con menores colisiones y utilizar rutinas de expertos de nivel de tarea para capas con mayores colisiones. Para reducir aún más los costos de estoreo, se utiliza la esparsidad matemática de las tareas para separar a varios expertos fine-tunados en un experto perfecto y en múltiples expertos esparsos. Se consideran muestras fuera de la distribución y se selecciona un experto adecuado basándose en la incertidumbre de las tareas de entrada para su integración. Se prueban en tareas de inferencia real de LLaMA y Qwen con diferentes tamaños de parámetros y se evalúan en tareas de inferencia real. Los resultados muestran que se puede obtener una mejora del rendimiento con un costo del sistema menor comparado con los métodos actuales.",
      "upvotes": 2,
      "discussionId": "67adad992883187d78409aa8"
    },
    "publishedAt": "2025-02-13T03:30:35.137Z",
    "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07985",
      "authors": [
        {
          "_id": "67ad9577b469222e0df18134",
          "user": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "name": "Víctor Gallego",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T06:47:20.731Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T22:06:25.000Z",
      "title": "MetaSC: Modelo de lenguaje para optimizar el espectro de seguridad en el testeo",
      "summary": "Proponemos un nuevo marco dinámico de seguridad para optimizar la lógica de seguridad de modelos de lenguaje (LM) sin cambiar los pesos del modelo. Basado en el desarrollo reciente de métodos de autoevaluación, nuestro enfoque utiliza una estructura de metaevaluación que actualiza repetidamente los Prompts de seguridad (un poco como 'reglas') de manera adaptativa, permitiendo que el proceso de autoevaluación y modificación se desarrolle de manera dinámica. Este optimizado proceso mejora el rendimiento frente a solicitudes adversas como 'brakes de zorro', evita daños morales en tareas generales de seguridad, y exige respuestas verdaderas para mejorar la seguridad. Los experimentos en diversos modelos de lenguaje muestran que los Prompts de seguridad optimizados dinámicamente muestran un nivel significativo de puntuación de seguridad más alto que los Prompts fijos del sistema y las defensas de autoevaluación estáticas. El código está disponible en https://github.com/vicgalle/meta-self-critique.git.",
      "upvotes": 1,
      "discussionId": "67ad9578b469222e0df18162"
    },
    "publishedAt": "2025-02-13T01:47:30.377Z",
    "title": "MetaSC: Test-Time Safety Specification Optimization for Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07985.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fad8602b8423e1d80b8a965",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
      "fullname": "Victor Gallego",
      "name": "vicgalle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 117
    },
    "isAuthorParticipating": true
  }
]