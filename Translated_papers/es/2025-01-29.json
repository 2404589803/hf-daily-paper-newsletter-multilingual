[
  {
    "paper": {
      "id": "2501.17161",
      "authors": [
        {
          "_id": "6799b39b15f4661561c22968",
          "name": "Tianzhe Chu",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c22969",
          "name": "Yuexiang Zhai",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296a",
          "name": "Jihan Yang",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296b",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296c",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296d",
          "name": "Dale Schuurmans",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296e",
          "name": "Quoc V. Le",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296f",
          "name": "Sergey Levine",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c22970",
          "name": "Yi Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T18:59:44.000Z",
      "title": "Memoria de SFT, Generalización de RL: Investigación después de la mejora de los modelos básicos",
      "summary": "El aprendizaje inicial (SFT) y el aprendizaje por refuerzo (RL) son técnicas ampliamente utilizadas después del entrenamiento de modelos básicos, pero no está claro su papel en mejorar la capacidad de generalización del modelo. En este artículo, investigamos las diferencias en la generalización y la memoria entre SFT y RL, centrandonos en los cambios de reglas y los cambios visuales basados en contexto. Evaluamos cómo modelos entrenados con SFT y RL generalizan ante variables no vistas contextuales e visuales utilizando el juego de aritmética GeneralPoints y el entorno de navegación V-IRL. Mostramos que, con una recompensa basada en resultados, RL, especialmente, logra generalizar en contextos y variables visuales basadas en reglas, mientras que SFT tiende a memorizar los datos de entrenamiento y es más difícil de generalizar en escenarios fuera de la distribución. En un análisis avanzado, RL mejora la capacidad de reconocimiento visual del modelo y promueve la generalización visual. A pesar de su excelente capacidad de generalización, SFT es crucial para un entrenamiento efectivo de RL, ya que establece el formato de salida del modelo y permite a RL mejorar su rendimiento. Estos hallazgos demuestran la capacidad de RL para aprender conocimientos generalizables en tareas multimodal complejas.",
      "upvotes": 11,
      "discussionId": "6799b39d15f4661561c229e6"
    },
    "publishedAt": "2025-01-28T23:50:56.664Z",
    "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17161.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.17116",
      "authors": [
        {
          "_id": "6799b367d30dc065a2d51592",
          "name": "Ruizhe Wang",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51593",
          "name": "Yeyun Gong",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51594",
          "name": "Xiao Liu",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51595",
          "name": "Guoshuai Zhao",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51596",
          "name": "Ziyue Yang",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51597",
          "name": "Baining Guo",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51598",
          "name": "Zhengjun Zha",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51599",
          "user": {
            "_id": "653feb7ccf1f9c88f4928910",
            "avatarUrl": "/avatars/23a6a6818116683ea9485e1470a0062f.svg",
            "isPro": false,
            "fullname": "Peng Cheng",
            "user": "cp5555",
            "type": "user"
          },
          "name": "Peng Cheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-29T04:49:44.372Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T18:04:50.000Z",
      "title": "Utilizando el tipo de datos FP4 para optimizar el entrenamiento de modelos de lenguaje grandes",
      "summary": "La necesidad de métodos más eficientes para afrontar el aumento del carga de cálculo en el entrenamiento de LLM. El entrenamiento cuantizado permite operaciones en bajos bits, lo que resulta en una solución adecuada para reducir estas costos. La precisión FP8 muestra la posibilidad de ejecución, pero la utilización de FP4 presenta desafíos debido a errores significativos y limitaciones de representación. En este artículo, se propone un primer marco de entrenamiento FP4 de LLM para resolver estos problemas, proponiendo dos innovaciones principales: la actualización de pesos utilizando evaluadores de precisión cuadrática diferenciable y la protección de la activación mediante estrategias de clipping y corrección de valores anormales. Para garantizar la estabilidad, el marco combina el aprendizaje de precisión mixta y el formato vectorial de Wiz. De acuerdo con los resultados de los experimentos, nuestro marco FP4 logra alcanzar la misma precisión que BF16 y FP8 con un mínimo de pérdida, y se puede entrenar un LLM de 13B parámetros con 100B tokens. En caso de que los próximos generadores de hardware apoyen FP4, nuestro marco proporcionará la base para un aprendizaje de precisión ultrabaja eficiente.",
      "upvotes": 9,
      "discussionId": "6799b368d30dc065a2d515bf"
    },
    "publishedAt": "2025-01-28T23:50:12.472Z",
    "title": "Optimizing Large Language Model Training Using FP4 Quantization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17116.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16975",
      "authors": [
        {
          "_id": "6799b345a66ae6b357bef986",
          "name": "Hongzhi Huang",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef987",
          "name": "Defa Zhu",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef988",
          "name": "Banggu Wu",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef989",
          "name": "Yutao Zeng",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef98a",
          "name": "Ya Wang",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef98b",
          "name": "Qiyang Min",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef98c",
          "name": "Xun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T14:15:42.000Z",
      "title": "Transformer: Vocabulario Generalmente Valuable para Ampliación",
      "summary": "El tokenizado es un componente fundamental de los modelos de lenguaje de gran escala (LLMs) y su impacto no ha sido completamente investigado. En este artículo, se presenta un nuevo marco llamado \"Transformers Over-Tokenized\" para separar los vectores de palabras de entrada y salida y mejorar el rendimiento de la modelización del lenguaje. Específicamente, nuestro enfoque amplía los vectores de palabras de entrada y utiliza bigramas como tokens. A través de experimentos extensos, se ha demostrado que hay una relación logarítmica entre el tamaño de los vectores de palabras de entrada y el pérdida de entrenamiento. Estos tamaños mejoran consistentemente el rendimiento del modelo, independientemente de su tamaño. Al usar vectores de palabras de entrada grandes, se logra alcanzar un rendimiento equivalente al de un modelo de doble tamaño sin aumentar los costos adicionales. Nuestros hallazgos subrayan la importancia de las leyes de escalabilidad y proporcionan una guía práctica para el diseño del tokenizado, abriéndol caminos para modelos de lenguaje de mayor eficiencia y potencia.",
      "upvotes": 6,
      "discussionId": "6799b346a66ae6b357bef9e3"
    },
    "publishedAt": "2025-01-28T23:49:26.959Z",
    "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16975.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16764",
      "authors": [
        {
          "_id": "6799aa5a311dbfe3c96724cd",
          "name": "Chenguo Lin",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724ce",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724cf",
          "name": "Bangbang Yang",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724d0",
          "name": "Zeming Li",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724d1",
          "name": "Yadong Mu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T07:38:59.000Z",
      "title": "DiffSplat: Método de reutilización de modelos de difusión de imágenes para la generación escalable de Gaussian Splats",
      "summary": "El desarrollo reciente del contenido 3D ha caído debido a limitaciones en los conjuntos de datos de alta calidad 3D y a la incertidumbre en la generación de poliedros 2D. En este contexto, se presenta un nuevo marco de trabajo de generación 3D llamado DiffSplat. DiffSplat utiliza modelos que expanden grandes textos a imágenes para generar gaussianos 3D de manera inédita. A diferencia de los modelos de generación 3D anteriores, DiffSplat efectivamente utiliza 2D proyectores de escala web, manteniendo una consistencia 3D a través de un modelo único. Se propone un modelo de reconstrucción ligero para iniciar el entrenamiento, que permite la preparación de conjuntos de datos escalables, así como la generación inmediata de grillas de gaussianos 3D para múltiples ángulos. Combinando estas grillas con el pérdida generalizada, se introduce un pérdida de renderización 3D para fomentar la consistencia 3D. Esta estrategia se alinea bien con los modelos de expansión de imágenes, permitiendo fácilmente la aplicación de muchas tecnologías de generación de imágenes en el dominio 3D. Los experimentos expandidos demuestran excelentes resultados en la generación basada en texto y imágenes, así como en las aplicaciones posteriores. La investigación detallada demuestra la efectividad de cada decisión diseñadora y proporciona una comprensión profunda de la tecnología.",
      "upvotes": 4,
      "discussionId": "6799aa5c311dbfe3c9672542"
    },
    "publishedAt": "2025-01-29T01:12:02.839Z",
    "title": "DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/654866e8cd0a5621395f8287/TFJMeKzXxMLOnq8NH8ltZ.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/654866e8cd0a5621395f8287/6kn1RLEUsUV-W6S0Taylo.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16764.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "654866e8cd0a5621395f8287",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654866e8cd0a5621395f8287/4Bccwd1ehn-Ee4T1rId5S.jpeg",
      "fullname": "Panwang Pan",
      "name": "paulpanwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16496",
      "authors": [
        {
          "_id": "6799b2fbfe3c29ec219d7d99",
          "name": "Lee Sharkey",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9a",
          "user": {
            "_id": "64ad563f4beffa272de6efac",
            "avatarUrl": "/avatars/f1a4902a95830cc3936058449626f8e4.svg",
            "isPro": false,
            "fullname": "Bilal Chughtai",
            "user": "bilalchughtai",
            "type": "user"
          },
          "name": "Bilal Chughtai",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-29T04:47:56.702Z",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9b",
          "name": "Joshua Batson",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9c",
          "name": "Jack Lindsey",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9d",
          "name": "Jeff Wu",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9e",
          "name": "Lucius Bushnaq",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9f",
          "name": "Nicholas Goldowsky-Dill",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da0",
          "name": "Stefan Heimersheim",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da1",
          "name": "Alejandro Ortega",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da2",
          "name": "Joseph Bloom",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da3",
          "name": "Stella Biderman",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da4",
          "name": "Adria Garriga-Alonso",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da5",
          "name": "Arthur Conmy",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da6",
          "name": "Neel Nanda",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da7",
          "name": "Jessica Rumbelow",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da8",
          "name": "Martin Wattenberg",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da9",
          "name": "Nandi Schoots",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7daa",
          "name": "Joseph Miller",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dab",
          "name": "Eric J. Michaud",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dac",
          "name": "Stephen Casper",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dad",
          "name": "Max Tegmark",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dae",
          "name": "William Saunders",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7daf",
          "name": "David Bau",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db0",
          "name": "Eric Todd",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db1",
          "name": "Atticus Geiger",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db2",
          "name": "Mor Geva",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db3",
          "name": "Jesse Hoogland",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db4",
          "name": "Daniel Murfet",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db5",
          "name": "Tom McGrath",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T20:57:18.000Z",
      "title": "Descripción de problemas abiertos en la explicación de la máquina",
      "summary": "La descripción de la máquina tiene como objetivo comprender las funciones de las redes neuronales y definir claramente la estructura computacional para alcanzar objetivos científicos y técnicos específicos. El desarrollo de esta área puede elevar la confianza en el comportamiento de los sistemas de IA y iluminar interesantes problemas científicos sobre la naturaleza del conocimiento. Con los últimos avances, es posible avanzar hacia este objetivo. Sin embargo, antes de poder lograrlo, no se pueden lograr muchos beneficios científicos y prácticos, ya que se requieren la resolución de muchas problemáticas abiertas. Nuestro enfoque necesita mejoras conceptuales y prácticas, así como una profunda comprensión. Además, es necesario clarificar cómo nuestro enfoque se aplica de manera óptima para alcanzar ciertos objetivos. Además, nuestro trabajo no solo se afecta, sino que también afecta a las problemáticas tecnológicas sociales que lo rodean. Esta revisión avanzada discute el estado de la arte de la descripción de la máquina y los problemas abiertos que la disciplina debe abordar.",
      "upvotes": 4,
      "discussionId": "6799b2fcfe3c29ec219d7dca"
    },
    "publishedAt": "2025-01-28T23:48:30.888Z",
    "title": "Open Problems in Mechanistic Interpretability",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16496.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.15747",
      "authors": [
        {
          "_id": "6799946c18cb282841d42639",
          "name": "Sankalp KJ",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263a",
          "name": "Ashutosh Kumar",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263b",
          "user": {
            "_id": "66707b60405252abeefd4c50",
            "avatarUrl": "/avatars/ee2728f115376e234e96820b8b376849.svg",
            "isPro": false,
            "fullname": "Laxmaan Balaji",
            "user": "laxmaanb",
            "type": "user"
          },
          "name": "Laxmaan Balaji",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-29T02:37:34.240Z",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263c",
          "name": "Nikunj Kotecha",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263d",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263e",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-29T08:55:14.312Z",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263f",
          "name": "Sreyoshi Bhaduri",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T03:19:03.000Z",
      "title": "IndicMMLU-Pro: Benchmark para el Modelo de Lenguaje Indic en Multi-task Comprensión de Lenguaje",
      "summary": "La población de más de 1,500 millones de personas que hablan lenguas indígenas en India ofrece a los estudios de procesamiento del lenguaje natural (NLP) problemas característicos y oportunidades, debido a la rica tradición cultural, la diversidad y la complejidad estructural de las lenguas. IndicMMLU-Pro es un marco de referencia detallado diseñado para evaluar grandes modelos de lenguaje (LLMs) sobre una amplia gama de lenguas indígenas. Basado en el marco de comprensión multitarea mágica (Magic Multitask Language Understanding), se utiliza para abordar problemas específicos de la diversidad lingüística de India, incluyendo principales lenguas como el Handi, el Bengalí, el Gujarati, el Marathi, el Kannada, el Punjabí, el Tamil, el Telugu y el Urdu. Este marco de referencia detecta la complejidad de la comprensión del lenguaje, la lógica y la generación, y maximiza las características de las lenguas indígenas. IndicMMLU-Pro proporciona un marco de evaluación estándar para impulsar la investigación en lenguajes indígenas con AI, promoviendo el desarrollo de modelos precisos, eficientes y culturalmente sensibles. Este artículo explica los principios de diseño del marco de referencia, las técnicas de tareas y los métodos de recolección de datos, ofreciendo resultados estándar basados en los modelos multilingües más recientes.",
      "upvotes": 3,
      "discussionId": "6799946e18cb282841d426d6"
    },
    "publishedAt": "2025-01-28T21:38:17.182Z",
    "title": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15747.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.16372",
      "authors": [
        {
          "_id": "67999c3dc1e34886f90320ee",
          "name": "J. Pablo Muñoz",
          "hidden": false
        },
        {
          "_id": "67999c3dc1e34886f90320ef",
          "name": "Jinjie Yuan",
          "hidden": false
        },
        {
          "_id": "67999c3dc1e34886f90320f0",
          "name": "Nilesh Jain",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-23T02:14:08.000Z",
      "title": "Los inferiores ordenadores y la búsqueda de arquitecturas neuronales se unen a la compresión de modelos grandes.",
      "summary": "El rápido crecimiento de los modelos de lenguaje grandes (LLM) revela importantes problemas relacionados con los recursos computacionales necesarios para fine-tuning y deployment. El reciente avance en low-rank adapters muestra su eficacia en fine-tuning paramétricomente eficiente (PEFT) de estos modelos. En este artículo, se discuten exhaustivamente un enfoque innovador basado en la representación de low-rank y la búsqueda de arquitecturas neuronales (NAS), especialmente en la utilización de redes super-red de pesos y interacciones. Al integrar estas metodologías, se desarrollaron fuertes soluciones para la compresión y fine-tuning de modelos pre-entrenados grandes. Nuestro análisis muestra que esta combinación puede democratizar el uso de LLM y hacer más accesible su deployment en entornos con limitaciones de recursos. Finalmente, los modelos obtenidos reducen el consumo de memoria, cortan el tiempo de inferencia y permiten la implementación de aplicaciones prácticas y escalables de LLM. Los modelos y código están disponibles en la siguiente URL.\nhttps://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning",
      "upvotes": 2,
      "discussionId": "67999c3dc1e34886f9032140"
    },
    "publishedAt": "2025-01-28T22:11:04.472Z",
    "title": "Low-Rank Adapters Meet Neural Architecture Search for LLM Compression",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16372.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.17117",
      "authors": [
        {
          "_id": "6799e5f9121155210e4fa48c",
          "name": "Thibaud Leteno",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa48d",
          "name": "Irina Proskurina",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa48e",
          "name": "Antoine Gourru",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa48f",
          "name": "Julien Velcin",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa490",
          "name": "Charlotte Laclau",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa491",
          "name": "Guillaume Metzler",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa492",
          "name": "Christophe Gravier",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T18:07:30.000Z",
      "title": "\"Evaluación de Moraal Alimentación en el Dataset Francés 'Moral Nomiti'\"",
      "summary": "Modelos deben aumentar su capacidad en función de la vida cotidiana, pero es crucial ajustar su lenguaje de acuerdo con los valores humanos. Aunque modelos pueden ser ajustados de acuerdo con las preferencias de los usuarios, es fundamental que se ajusten a los normas éticas y comportamientos de la sociedad real. Mientras el desarrollo en inglés y chino ha sido considerable, el francés ha mostrado menos interés en este campo, y existe una deficiencia en la comprensión de cómo los modelos de lenguaje grande (LLM) tratan la lógica moral en francés. Para abordar esta deficiencia, presentamos el conjunto de datos de \"Histoires Morales\" en francés. Este conjunto de datos incluye anotaciones de valores morales y está escrito en el contexto de la cultura francesa. \"Histoires Morales\" cubre diversas situaciones sociales, como la desigualdad de oportunidades, la honestidad en las relaciones y la responsabilidad hacia los animales. En el futuro, realizaremos experimentos iniciales para combinar modelos multilingües en francés y inglés, así como para evaluar su robustez. Según estos resultados, los modelos de lenguaje grande suelen ser fácilmente influenciados por datos morales o inmorales según las preferencias del usuario.",
      "upvotes": 0,
      "discussionId": "6799e5fb121155210e4fa500"
    },
    "publishedAt": "2025-01-29T03:32:09.927Z",
    "title": "Histoires Morales: A French Dataset for Assessing Moral Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17117.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "629a3dbcd496c6dcdebf41cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655113762275-629a3dbcd496c6dcdebf41cc.jpeg",
      "fullname": "Irina Proskurina",
      "name": "iproskurina",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]