[
  {
    "paper": {
      "id": "2505.15277",
      "authors": [
        {
          "_id": "682e854551706f69070aca6b",
          "user": {
            "_id": "64c8f4cec547ed5243ebd0a8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
            "isPro": false,
            "fullname": "Hyungjoo Chae",
            "user": "hyungjoochae",
            "type": "user"
          },
          "name": "Hyungjoo Chae",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:37.301Z",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6c",
          "user": {
            "_id": "646a0897c37ca1e12308b026",
            "avatarUrl": "/avatars/6d720a9e366db9bec15c8c10878c0c75.svg",
            "isPro": false,
            "fullname": "Sunghwan Kim",
            "user": "KimSHine",
            "type": "user"
          },
          "name": "Sunghwan Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:32.322Z",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6d",
          "name": "Junhee Cho",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6e",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6f",
          "name": "Seungjun Moon",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca70",
          "name": "Gyeom Hwangbo",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca71",
          "user": {
            "_id": "6683b8680b72be136701de35",
            "avatarUrl": "/avatars/0c135e570b16b81ee2fb81ad65b01ba8.svg",
            "isPro": false,
            "fullname": "Dongha Lim",
            "user": "donghalim",
            "type": "user"
          },
          "name": "Dongha Lim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:34.741Z",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca72",
          "name": "Minjin Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca73",
          "name": "Yeonjun Hwang",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca74",
          "name": "Minju Gwak",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca75",
          "name": "Dongwook Choi",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca76",
          "name": "Minseok Kang",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca77",
          "name": "Gwanhoon Im",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca78",
          "name": "ByeongUng Cho",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca79",
          "name": "Hyojun Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7a",
          "name": "Jun Hee Han",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7b",
          "name": "Taeyoon Kwon",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7c",
          "name": "Minju Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7d",
          "name": "Beong-woo Kwak",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7e",
          "name": "Dongjin Kang",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7f",
          "name": "Jinyoung Yeo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64c8f4cec547ed5243ebd0a8/hXqaaoJTvW35xMW1lPVv0.png"
      ],
      "publishedAt": "2025-05-21T08:56:55.000Z",
      "submittedOnDailyAt": "2025-05-22T00:31:53.858Z",
      "title": "Web-Superd: Mejora de la gestión de cuentas de web con el desarrollo de PRM",
      "submittedOnDailyBy": {
        "_id": "64c8f4cec547ed5243ebd0a8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
        "isPro": false,
        "fullname": "Hyungjoo Chae",
        "user": "hyungjoochae",
        "type": "user"
      },
      "summary": "Web Mapping es una especialización destinada principalmente a la automatización de tareas real-time repetitivas, que son más complejas que las tareas generales de modelos multimodales de lenguaje (MLLM) debido a la necesidad de tomar decisiones secuenciales de largo plazo. Sin embargo, aún no existen modelos de recompensa especializados para el entrenamiento y prueba de web mapping. En estudios previos, se utilizaron MLLM como modelos de recompensa debido a la importancia de velocidad y eficiencia de costo, pero encontraron limitaciones para su introducción práctica. En este artículo, se propone el primer modelo de recompensa de proceso (PRM) para web mapping: Web-Shepherd. Web-Shepherd puede evaluar cada etapa del proceso de web mapping. Para ello, se construye primero la colección de PRM web. Esta colección incluye un conjunto de datos grande con 40K pares de preferencia y diferentes niveles de área y dificultad. A continuación, se presenta el benchmark de recompensa web (WebRewardBench), el primer meta-benchmark para evaluar PRM. En las experimentaciones, Web-Shepherd mejoró la precisión de GPT-4o en el benchmark de recompensa web en aproximadamente 30 puntos. Además, al usar GPT-4o-mini como política, se logró un aumento de 10.9 puntos de rendimiento y una reducción del costo en un factor de 10 en la arena web-lite. Los modelos, conjuntos de datos y código están disponibles públicamente a través de enlaces.",
      "upvotes": 75,
      "discussionId": "682e854951706f69070acbf0",
      "githubRepo": "https://github.com/kyle8581/Web-Shepherd",
      "ai_summary": "The paper introduces Web-Shepherd, a process reward model for web navigation, which improves accuracy and cost-effectiveness in step-level trajectory assessment compared to existing multimodal large language models.",
      "ai_keywords": [
        "multimodal large language model",
        "process reward model",
        "web navigation",
        "webPRM collection",
        "webrewardbench",
        "long-horizon sequential decision making",
        "preference pairs",
        "annotated checklists",
        "step-level assessment",
        "webarena-lite",
        "policy",
        "verifier"
      ]
    },
    "publishedAt": "2025-05-21T04:56:55.000Z",
    "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents",
    "summary": "Web navigation is a unique domain that can automate many repetitive real-life\ntasks and is challenging as it requires long-horizon sequential decision making\nbeyond typical multimodal large language model (MLLM) tasks. Yet, specialized\nreward models for web navigation that can be utilized during both training and\ntest-time have been absent until now. Despite the importance of speed and\ncost-effectiveness, prior works have utilized MLLMs as reward models, which\nposes significant constraints for real-world deployment. To address this, in\nthis work, we propose the first process reward model (PRM) called Web-Shepherd\nwhich could assess web navigation trajectories in a step-level. To achieve\nthis, we first construct the WebPRM Collection, a large-scale dataset with 40K\nstep-level preference pairs and annotated checklists spanning diverse domains\nand difficulty levels. Next, we also introduce the WebRewardBench, the first\nmeta-evaluation benchmark for evaluating PRMs. In our experiments, we observe\nthat our Web-Shepherd achieves about 30 points better accuracy compared to\nusing GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by\nusing GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve\n10.9 points better performance, in 10 less cost compared to using GPT-4o-mini\nas the verifier. Our model, dataset, and code are publicly available at LINK.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64c8f4cec547ed5243ebd0a8/hXqaaoJTvW35xMW1lPVv0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15277.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64c8f4cec547ed5243ebd0a8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
      "fullname": "Hyungjoo Chae",
      "name": "hyungjoochae",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14302",
      "authors": [
        {
          "_id": "682e887e866a44f6a81409b1",
          "user": {
            "_id": "64aea082704210bf815e7551",
            "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
            "isPro": false,
            "fullname": "Mengzhao Chen",
            "user": "ChenMnZ",
            "type": "user"
          },
          "name": "Mengzhao Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:29.873Z",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b2",
          "name": "Chaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b3",
          "name": "Jing Liu",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b4",
          "name": "Yutao Zeng",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b5",
          "name": "Zeyue Xue",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b6",
          "name": "Zhiheng Liu",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b7",
          "name": "Yunshui Li",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b8",
          "name": "Jin Ma",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b9",
          "name": "Jie Huang",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409ba",
          "name": "Xun Zhou",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409bb",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T12:54:43.000Z",
      "submittedOnDailyAt": "2025-05-22T00:44:46.277Z",
      "title": "Regla de escalabilidad para entrenamiento de reconocimiento de patrones probabilísticos",
      "submittedOnDailyBy": {
        "_id": "64aea082704210bf815e7551",
        "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
        "isPro": false,
        "fullname": "Mengzhao Chen",
        "user": "ChenMnZ",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) asocian una demanda significativa de cálculo y recursos de memoria, lo que puede generar problemas al introducirlos. El entrenamiento con conciencia de la cuantificación (QAT) resuelve estos problemas manteniendo la eficiencia del modelo a costas de una reducción en su precisión. Sin embargo, el comportamiento de escalado en QAT, especialmente en la precisión de 4 bits (W4A4), es poco comprensible. Los métodos actuales de escalado en QAT ignoran factores importantes como el número de tokens de entrenamiento y la intensidad de la cuantificación, y su aplicación se vuelve más limitada. En este artículo, proponemos un método de escalado para medir los errores de cuantificación en QAT, considerando como componentes el tamaño del modelo, la cantidad de datos de entrenamiento y el tamaño de los grupos de cuantificación. A través de 268 experimentos de QAT, mostramos que el error de cuantificación disminuye al aumentar el tamaño del modelo, mientras que aumenta con el número de tokens de entrenamiento y la intensidad de la cuantificación. Para identificar la causa de los errores de cuantificación en W4A4, los errores se decomponen en pesos y componentes activos. Ambos comportamientos siguen la tendencia general de los errores de cuantificación, pero presentan diferentes sensibilidades. En particular, el error de cuantificación de los pesos aumenta drásticamente con el número de tokens de entrenamiento. Un análisis más profundo muestra que el error de cuantificación de los componentes activos en la capa FC2 es principalmente causado por outliers, lo que se convierte en un \"cuerda de nariz\" clave en la cuantificación de W4A4 en QAT. Para resolver este \"cuerda de nariz\", se aplica la cuantificación de precisión mixta, lo que permite alcanzar un nivel de error de cuantificación similar para pesos y componentes activos. Además, al aumentar la cantidad de datos de entrenamiento, el error de cuantificación de los pesos supera al de los componentes activos, demostrando que la reducción del error de cuantificación de los pesos sigue siendo crucial. Estos hallazgos ofrecen perspectivas clave para el avance en la investigación y desarrollo de QAT.",
      "upvotes": 46,
      "discussionId": "682e887e866a44f6a81409f0",
      "ai_summary": "A unified scaling law for quantization-aware training (QAT) identifies key factors affecting quantization error, leading to improvements through mixed-precision quantization.",
      "ai_keywords": [
        "quantization-aware training",
        "QAT",
        "quantization error",
        "model size",
        "training tokens",
        "quantization granularity",
        "weight quantization",
        "activation quantization",
        "mixed-precision quantization",
        "FC2 layer"
      ]
    },
    "publishedAt": "2025-05-20T08:54:43.000Z",
    "title": "Scaling Law for Quantization-Aware Training",
    "summary": "Large language models (LLMs) demand substantial computational and memory\nresources, creating deployment challenges. Quantization-aware training (QAT)\naddresses these challenges by reducing model precision while maintaining\nperformance. However, the scaling behavior of QAT, especially at 4-bit\nprecision (W4A4), is not well understood. Existing QAT scaling laws often\nignore key factors such as the number of training tokens and quantization\ngranularity, which limits their applicability. This paper proposes a unified\nscaling law for QAT that models quantization error as a function of model size,\ntraining data volume, and quantization group size. Through 268 QAT experiments,\nwe show that quantization error decreases as model size increases, but rises\nwith more training tokens and coarser quantization granularity. To identify the\nsources of W4A4 quantization error, we decompose it into weight and activation\ncomponents. Both components follow the overall trend of W4A4 quantization\nerror, but with different sensitivities. Specifically, weight quantization\nerror increases more rapidly with more training tokens. Further analysis shows\nthat the activation quantization error in the FC2 layer, caused by outliers, is\nthe primary bottleneck of W4A4 QAT quantization error. By applying\nmixed-precision quantization to address this bottleneck, we demonstrate that\nweight and activation quantization errors can converge to similar levels.\nAdditionally, with more training data, weight quantization error eventually\nexceeds activation quantization error, suggesting that reducing weight\nquantization error is also important in such scenarios. These findings offer\nkey insights for improving QAT research and development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14302.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64aea082704210bf815e7551",
      "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
      "fullname": "Mengzhao Chen",
      "name": "ChenMnZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14231",
      "authors": [
        {
          "_id": "682db25d265177367e35d5b1",
          "name": "Sule Bai",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b2",
          "name": "Mingxing Li",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b3",
          "name": "Yong Liu",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b4",
          "name": "Jing Tang",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b5",
          "name": "Haoji Zhang",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b6",
          "name": "Lei Sun",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b7",
          "user": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "name": "Xiangxiang Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T12:27:07.959Z",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b8",
          "name": "Yansong Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T11:40:43.000Z",
      "submittedOnDailyAt": "2025-05-22T00:13:45.780Z",
      "title": "UniVG-R1: Aprendizaje por Reforzamiento para la Gestión de Grupos Visuais Generales de Lógica",
      "submittedOnDailyBy": {
        "_id": "6513a0f14f1682e4407758a9",
        "avatarUrl": "/avatars/b2a6886114492944cfa235363817565f.svg",
        "isPro": false,
        "fullname": "Mingxing Li",
        "user": "MingxingLi",
        "type": "user"
      },
      "summary": "Los métodos tradicionales de visualización de ubicación se realizan principalmente en entornos de solo imagen y utilizan referencias de texto simples. Sin embargo, presentan un gran desafío al aplicar estas técnicas en entornos reales que incluyen múltiples imágenes con instrucciones complejas. Esto se debe a la falta de capacidades de inferencia avanzadas para procesar múltiples modalidades de contexto. Este estudio propone UniVG-R1 para resolver de manera más práctica el trabajo general de ubicación. UniVG-R1 es un modelo de lenguaje multimodal de lenguaje (MLLM) que mejora su capacidad de inferencia utilizando aprendizaje por refuerzo (RL) y aumento de datos por enfriamiento inicial. Específicamente, primero construimos un conjunto de datos de ubicación de alta calidad que incluye conexiones de inferencia detalladas, y entrenamos el modelo utilizando aprendizaje supervisado para orientarlo hacia rutas de inferencia precisas. Luego, realizamos entrenamiento por refuerzo basado en reglas para inducir al modelo a reconocer las conexiones de inferencia correctas y mejorar su capacidad de inferencia. Además, identificamos la sesgo de dificultad que puede surgir durante el entrenamiento por refuerzo y proponemos una estrategia de ajuste de pesos para mejorar la eficiencia. Los resultados de los experimentos demuestran la efectividad de UniVG-R1, logrando un aumento del 9.1% en el MIG-Bench y alcanzar el rendimiento más reciente. Además, nuestro modelo muestra un aumento promedio del 23.4% en la eficiencia de 0-shot en cuatro bases de datos de prueba de ubicación de imágenes y videos. La página del proyecto está disponible en https://amap-ml.github.io/UniVG-R1-page/.",
      "upvotes": 39,
      "discussionId": "682db25e265177367e35d638",
      "projectPage": "https://amap-ml.github.io/UniVG-R1-page/",
      "githubRepo": "https://github.com/AMAP-ML/UniVG-R1",
      "ai_summary": "UniVG-R1, a reasoning-guided multimodal large language model, enhances visual grounding by leveraging reinforcement learning and a difficulty-aware strategy, achieving state-of-the-art results and strong generalizability.",
      "ai_keywords": [
        "multimodal large language model",
        "reasoning guided",
        "reinforcement learning",
        "cold-start data",
        "Chain-of-Thought dataset",
        "supervised fine-tuning",
        "rule-based reinforcement learning",
        "difficulty bias",
        "difficulty-aware weight adjustment"
      ]
    },
    "publishedAt": "2025-05-20T07:40:43.000Z",
    "title": "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement\n  Learning",
    "summary": "Traditional visual grounding methods primarily focus on single-image\nscenarios with simple textual references. However, extending these methods to\nreal-world scenarios that involve implicit and complex instructions,\nparticularly in conjunction with multiple images, poses significant challenges,\nwhich is mainly due to the lack of advanced reasoning ability across diverse\nmulti-modal contexts. In this work, we aim to address the more practical\nuniversal grounding task, and propose UniVG-R1, a reasoning guided multimodal\nlarge language model (MLLM) for universal visual grounding, which enhances\nreasoning capabilities through reinforcement learning (RL) combined with\ncold-start data. Specifically, we first construct a high-quality\nChain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning\nchains, to guide the model towards correct reasoning paths via supervised\nfine-tuning. Subsequently, we perform rule-based reinforcement learning to\nencourage the model to identify correct reasoning chains, thereby incentivizing\nits reasoning capabilities. In addition, we identify a difficulty bias arising\nfrom the prevalence of easy samples as RL training progresses, and we propose a\ndifficulty-aware weight adjustment strategy to further strengthen the\nperformance. Experimental results demonstrate the effectiveness of UniVG-R1,\nwhich achieves state-of-the-art performance on MIG-Bench with a 9.1%\nimprovement over the previous method. Furthermore, our model exhibits strong\ngeneralizability, achieving an average improvement of 23.4% in zero-shot\nperformance across four image and video reasoning grounding benchmarks. The\nproject page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14231.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6513a0f14f1682e4407758a9",
      "avatarUrl": "/avatars/b2a6886114492944cfa235363817565f.svg",
      "fullname": "Mingxing Li",
      "name": "MingxingLi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15809",
      "authors": [
        {
          "_id": "682e7e061d7637a25846bf52",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf53",
          "user": {
            "_id": "64e357dd825f4133e7427bf8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e357dd825f4133e7427bf8/HwaWhINrzkbXG6SHG2oyf.jpeg",
            "isPro": false,
            "fullname": "tyfeld",
            "user": "tyfeld",
            "type": "user"
          },
          "name": "Ye Tian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:11.786Z",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf54",
          "name": "Bowen Li",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf55",
          "user": {
            "_id": "653e5d31ffd60206c8b64bb5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653e5d31ffd60206c8b64bb5/JrfOSCunFSW39vdW7E59y.png",
            "isPro": false,
            "fullname": "Xinchen Zhang",
            "user": "comin",
            "type": "user"
          },
          "name": "Xinchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:14.114Z",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf56",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf57",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf58",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:59:05.000Z",
      "submittedOnDailyAt": "2025-05-22T00:24:15.122Z",
      "title": "MMaDA: Monomodal Raziri Diffusion Landraced Mop",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "MMaDA es un nuevo tipo de modelo basado en multi-modal DPI diseñado para alcanzar altos rendimientos en diversas áreas como la inferencia de texto, comprensión multi-modal, y la generación de imágenes a partir de texto. Este enfoque se caracteriza por tres puntos innovadores: (i) MMaDA adopta una representación probabilística común y un diseño independiente de modelo para utilizar una arquitectura de DPI multi-modal uniforme, reduciendo la necesidad de componentes específicos dependientes del modelo. Esta arquitectura garantiza una interacción y procesamiento uniforme entre diferentes tipos de datos. (ii) Para promover la entrenamiento inicial en la fase de aprendizaje por refuerzo (RL) del modelo, MMaDA integra un enfoque de razonamiento mixto de cadenas de longitud variable (CoT) para texto y visión, implementando una estrategia de ajuste micro de CoT. Esta estrategia mejora la capacidad del modelo para procesar tareas complejas desde el principio, promoviendo un entrenamiento inicial en la fase de RL. (iii) Propone una RL basada en políticas de gradiente especializada llamada UniGRPO para realizar un aprendizaje posterior unificado de razonamiento y generación. Utilizando diferentes modelos de recompensa, UniGRPO garantiza un mejoramiento uniforme en ambas tareas y una mejora en el rendimiento. Los resultados de los experimentos muestran que MMaDA-8B muestra una fuerte capacidad de generalización como un modelo basado en multi-modal uniforme, presentando mejores resultados en inferencia de texto que LLaMA-3-7B y Qwen2-7B, en comprensión multi-modal superior a Show-o y SEED-X, y en generación de imágenes a partir de texto mejor que SDXL y Janus. Estos resultados demuestran la efectividad de MMaDA en integrar el aprendizaje previo y posterior de una arquitectura de DPI multi-modal uniforme, proporcionando un marco consistente para futuras investigaciones y desarrollos. Publicamos los códigos y modelos entrenados en el siguiente URL: https://github.com/Gen-Verse/MMaDA",
      "upvotes": 35,
      "discussionId": "682e7e0a1d7637a25846c03b",
      "projectPage": "https://huggingface.co/spaces/Gen-Verse/MMaDA",
      "githubRepo": "https://github.com/Gen-Verse/MMaDA",
      "ai_summary": "MMaDA, a multimodal diffusion foundation model, achieves superior performance through a unified architecture, mixed long chain-of-thought fine-tuning, and a unified policy-gradient-based RL algorithm.",
      "ai_keywords": [
        "multimodal diffusion foundation models",
        "unified diffusion architecture",
        "modality-agnostic design",
        "mixed long chain-of-thought fine-tuning",
        "cold-start training",
        "reinforcement learning",
        "UniGRPO",
        "policy-gradient-based RL algorithm",
        "diversified reward modeling",
        "generalization capabilities",
        "textual reasoning",
        "multimodal understanding",
        "text-to-image generation"
      ]
    },
    "publishedAt": "2025-05-21T13:59:05.000Z",
    "title": "MMaDA: Multimodal Large Diffusion Language Models",
    "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15809.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13909",
      "authors": [
        {
          "_id": "682d525994ed89a9b2aec35d",
          "user": {
            "_id": "661b9ac57cfb7bcb3057a578",
            "avatarUrl": "/avatars/f8afaa8eaad3a1e5963a4feebec3f7ab.svg",
            "isPro": false,
            "fullname": "Yanheng He",
            "user": "henryhe0123",
            "type": "user"
          },
          "name": "Yanheng He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:46.371Z",
          "hidden": false
        },
        {
          "_id": "682d525994ed89a9b2aec35e",
          "user": {
            "_id": "663f5e959e6f865ec6d4fb62",
            "avatarUrl": "/avatars/eb0a908562b2c57335ae8bb949220430.svg",
            "isPro": false,
            "fullname": "Jiahe Jin",
            "user": "zizi-0123",
            "type": "user"
          },
          "name": "Jiahe Jin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:43.438Z",
          "hidden": false
        },
        {
          "_id": "682d525994ed89a9b2aec35f",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T04:20:18.000Z",
      "submittedOnDailyAt": "2025-05-22T00:48:35.836Z",
      "title": "Eficiente entrenamiento de un outlook para el uso de computadoras",
      "submittedOnDailyBy": {
        "_id": "663f5e959e6f865ec6d4fb62",
        "avatarUrl": "/avatars/eb0a908562b2c57335ae8bb949220430.svg",
        "isPro": false,
        "fullname": "Jiahe Jin",
        "user": "zizi-0123",
        "type": "user"
      },
      "summary": "El desarrollo de datos de tarot de alta calidad ha sido una fuerte base a largo plazo para el desarrollo de agentes computacionales que funcionan de manera humana. Presentamos el marco de entrenamiento eficiente de agentes PC Agent-E para reducir la dependencia de demostraciones humanas. Inicialmente, solo teníamos 312 datos de tarot de uso de computadora explicados por humanos, pero utilizando Claude 3.7 Sonnet, sintetizamos decisiones de acciones variadas y mejoramos la calidad de los datos. El modelo de PC Agent-E entrenado con este ampliado conjunto de datos superó a Claude 3.7 Sonnet en WindowsAgentArena-V2, logrando un aumento relativo del 141%. Además, PC Agent-E demostró una fuerte generalización en OSWorld para diferentes sistemas operativos. Nuestro hallazgo muestra que se puede estimular una fuerte capacidad de uso de computadora a partir de un pequeño conjunto de datos de alta calidad de tarot.",
      "upvotes": 25,
      "discussionId": "682d525a94ed89a9b2aec397",
      "githubRepo": "https://github.com/GAIR-NLP/PC-Agent-E",
      "ai_summary": "PC Agent-E framework improves data efficiency and achieves superior performance on human-like computer use tasks through enhanced trajectory synthesis and training.",
      "ai_keywords": [
        "agent training framework",
        "human-annotated trajectories",
        "action decisions",
        "Claude 3.7 Sonnet",
        "WindowsAgentArena-V2",
        "OSWorld",
        "generalizability"
      ]
    },
    "publishedAt": "2025-05-20T00:20:18.000Z",
    "title": "Efficient Agent Training for Computer Use",
    "summary": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13909.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "663f5e959e6f865ec6d4fb62",
      "avatarUrl": "/avatars/eb0a908562b2c57335ae8bb949220430.svg",
      "fullname": "Jiahe Jin",
      "name": "zizi-0123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15045",
      "authors": [
        {
          "_id": "682e9672d28d9650c90db133",
          "user": {
            "_id": "638f1803c67af472d317a922",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638f1803c67af472d317a922/9BMVXqHa-AsdZPmBprcbd.jpeg",
            "isPro": false,
            "fullname": "siyue zhang",
            "user": "siyue",
            "type": "user"
          },
          "name": "Siyue Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:04.722Z",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db134",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:02.079Z",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db135",
          "user": {
            "_id": "65457e29bd25cef7d118122c",
            "avatarUrl": "/avatars/67777b3f4584b8ba92f12e95dbf93482.svg",
            "isPro": false,
            "fullname": "Liyuan Geng",
            "user": "LYGeng",
            "type": "user"
          },
          "name": "Liyuan Geng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:59.203Z",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db136",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db137",
          "name": "Anh Tuan Luu",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db138",
          "name": "Chen Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638f1803c67af472d317a922/2-30it6-2JiegJOHJEfOy.png"
      ],
      "publishedAt": "2025-05-21T02:59:14.000Z",
      "submittedOnDailyAt": "2025-05-22T01:48:05.988Z",
      "title": "Modelo de lenguaje difusivo vs. Modelo de lenguaje automático de regresión: Comparación entre el modelo de lenguaje difusivo y el modelo de lenguaje automático de regresión - Comparación desde la perspectiva de la inserción de texto.",
      "submittedOnDailyBy": {
        "_id": "638f1803c67af472d317a922",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638f1803c67af472d317a922/9BMVXqHa-AsdZPmBprcbd.jpeg",
        "isPro": false,
        "fullname": "siyue zhang",
        "user": "siyue",
        "type": "user"
      },
      "summary": "Los modelos de adición basados en modelos de lenguaje grande (LLM) han superado a los modelos como BERT y T5 en tareas generales de búsqueda de documentos a través de un aprendizaje previo a gran escala y un aprendizaje posterior. Sin embargo, una de las limitaciones básicas de la adición basada en LLM es la función de atención unidireccional, diseñada para el aprendizaje automático de regresión en el aprendizaje previo, que no se adapta adecuadamente a las características bidireccionales de las tareas de adición de texto. En respuesta a esto, proponemos la introducción de modelos de lenguaje grande en la adición de texto, inspirados en la arquitectura bidireccional que ha demostrado éxito recientemente. Hemos realizado una investigación sistemática inicial y hemos logrado mejoras significativas: un 20% en búsquedas de documentos largos, un 8% en búsquedas lógicas y un 2% en búsquedas según instrucciones, mostrando también un rendimiento relativo en los benchmarks tradicionales de adición de texto. Nuestro análisis demuestra la importancia de la función de atención bidireccional para la adición global de contexto en documentos largos y complejos.",
      "upvotes": 24,
      "discussionId": "682e9673d28d9650c90db154",
      "ai_summary": "Diffusion language models outperform large language model embeddings in text retrieval tasks due to their bidirectional architecture.",
      "ai_keywords": [
        "large language model (LLM)",
        "diffusion language models",
        "unidirectional attention",
        "bidirectional attention",
        "document retrieval",
        "reasoning-intensive retrieval",
        "instruction-following retrieval",
        "text embedding benchmarks"
      ]
    },
    "publishedAt": "2025-05-20T22:59:14.000Z",
    "title": "Diffusion vs. Autoregressive Language Models: A Text Embedding\n  Perspective",
    "summary": "Large language model (LLM)-based embedding models, benefiting from large\nscale pre-training and post-training, have begun to surpass BERT and T5-based\nmodels on general-purpose text embedding tasks such as document retrieval.\nHowever, a fundamental limitation of LLM embeddings lies in the unidirectional\nattention used during autoregressive pre-training, which misaligns with the\nbidirectional nature of text embedding tasks. To this end, We propose adopting\ndiffusion language models for text embeddings, motivated by their inherent\nbidirectional architecture and recent success in matching or surpassing LLMs\nespecially on reasoning tasks. We present the first systematic study of the\ndiffusion language embedding model, which outperforms the LLM-based embedding\nmodel by 20% on long-document retrieval, 8% on reasoning-intensive retrieval,\n2% on instruction-following retrieval, and achieve competitive performance on\ntraditional text embedding benchmarks. Our analysis verifies that bidirectional\nattention is crucial for encoding global context in long and complex text.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638f1803c67af472d317a922/2-30it6-2JiegJOHJEfOy.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15045.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638f1803c67af472d317a922",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638f1803c67af472d317a922/9BMVXqHa-AsdZPmBprcbd.jpeg",
      "fullname": "siyue zhang",
      "name": "siyue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15612",
      "authors": [
        {
          "_id": "682e97bf9c1b77a503087f4f",
          "user": {
            "_id": "6458af46f4d212d780bd7c68",
            "avatarUrl": "/avatars/832fd34bcc041b0b7b551873a459fc3c.svg",
            "isPro": false,
            "fullname": "Wei Liu",
            "user": "PeterV09",
            "type": "user"
          },
          "name": "Wei Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:57.023Z",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f50",
          "name": "Ruochen Zhou",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f51",
          "name": "Yiyun Deng",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f52",
          "name": "Yuzhen Huang",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f53",
          "name": "Junteng Liu",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f54",
          "user": {
            "_id": "63081e15a670ed10f9d44229",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
            "isPro": true,
            "fullname": "Yuntian Deng",
            "user": "yuntian-deng",
            "type": "user"
          },
          "name": "Yuntian Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:54.170Z",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f55",
          "name": "Yizhe Zhang",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f56",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T15:03:26.000Z",
      "submittedOnDailyAt": "2025-05-22T01:49:54.677Z",
      "title": "Aprenda a aplicar ajustes de corrección en función de la longitud y a aprender métodos eficientes para explicar razones.",
      "submittedOnDailyBy": {
        "_id": "6458af46f4d212d780bd7c68",
        "avatarUrl": "/avatars/832fd34bcc041b0b7b551873a459fc3c.svg",
        "isPro": false,
        "fullname": "Wei Liu",
        "user": "PeterV09",
        "type": "user"
      },
      "summary": "Los modelos lógicos (LRMs) han demostrado una capacidad sorprendente para resolver problemas complejos a través del aprendizaje por refuerzo (RL). En particular, en la generación de largos trazos lógicos. Sin embargo, estos largos trazos están llenos de información innecesaria, lo que limita la eficiencia de los LRMs. En este artículo, se investiga métodos para mejorar la eficiencia lógica basados en RL. Específicamente, se propone un marco unificado para formalizar varias formas eficientes de trazos lógicos utilizando el \"clipping de recompensas basado en longitud\". Se propone el método de recompensa basada en longitud de pasos (LASER). LASER utiliza una función de pasos como recompensa y se controla por la longitud objetivo. LASER supera a los métodos anteriores al lograr un excelente equilibrio entre rendimiento y eficiencia. Además, LASER se desarrolla en dos direcciones importantes: (1) los comportamientos lógicos del modelo evolucionan durante el entrenamiento y la definición de las recompensas debe ser adaptativa y dinámica; (2) en lugar de incentivar secuencias de pensamiento cortas o largas de manera uniforme, el clipping basado en longitud debe enfrentar el desafío de evaluar más severamente largos trazos lógicos en preguntas simples. Esta aproximación promete mejorar la combinación rápida y lenta de pensamiento y mejorar el equilibrio general. Como resultado, se denomina LASER-D (dinámico y desafío). Se realizaron experimentos con DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B y DeepSeek-R1-Distill-Qwen-32B, demostrando que nuestra aproximación significativamente mejora el rendimiento lógico y la eficiencia en la longitud de respuesta. Por ejemplo, LASER-D y sus variantes mejoraron +6.1 en AIME2024 y reducieron el uso de tokens en un 63%. Un análisis profundo muestra que nuestro aprendizaje por refuerzo basado se centra en reducir el \"juicio\" innecesario en favor de la generación de patrones lógicos claros. Los recursos están disponibles en https://github.com/hkust-nlp/Laser.",
      "upvotes": 20,
      "discussionId": "682e97bf9c1b77a503087f81",
      "githubRepo": "https://github.com/hkust-nlp/Laser",
      "ai_summary": "RL-based reward shaping methods, particularly LASER-D, enhance reasoning efficiency and performance in large reasoning models by dynamically adapting to difficulty and reducing redundancy.",
      "ai_keywords": [
        "reinforcement learning",
        "long reasoning traces",
        "length-based reward shaping",
        "LASER",
        "LASER-D",
        "reasoning behavior",
        "target length",
        "step function",
        "adaptive",
        "dynamic",
        "difficulty-aware",
        "chains of thought",
        "self-reflections"
      ]
    },
    "publishedAt": "2025-05-21T11:03:26.000Z",
    "title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping",
    "summary": "Large Reasoning Models (LRMs) have shown remarkable capabilities in solving\ncomplex problems through reinforcement learning (RL), particularly by\ngenerating long reasoning traces. However, these extended outputs often exhibit\nsubstantial redundancy, which limits the efficiency of LRMs. In this paper, we\ninvestigate RL-based approaches to promote reasoning efficiency. Specifically,\nwe first present a unified framework that formulates various efficient\nreasoning methods through the lens of length-based reward shaping. Building on\nthis perspective, we propose a novel Length-bAsed StEp Reward shaping method\n(LASER), which employs a step function as the reward, controlled by a target\nlength. LASER surpasses previous methods, achieving a superior Pareto-optimal\nbalance between performance and efficiency. Next, we further extend LASER based\non two key intuitions: (1) The reasoning behavior of the model evolves during\ntraining, necessitating reward specifications that are also adaptive and\ndynamic; (2) Rather than uniformly encouraging shorter or longer chains of\nthought (CoT), we posit that length-based reward shaping should be\ndifficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.\nThis approach is expected to facilitate a combination of fast and slow\nthinking, leading to a better overall tradeoff. The resulting method is termed\nLASER-D (Dynamic and Difficulty-aware). Experiments on\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and\nDeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both\nreasoning performance and response length efficiency. For instance, LASER-D and\nits variant achieve a +6.1 improvement on AIME2024 while reducing token usage\nby 63%. Further analysis reveals our RL-based compression produces more concise\nreasoning patterns with less redundant \"self-reflections\". Resources are at\nhttps://github.com/hkust-nlp/Laser.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15612.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6458af46f4d212d780bd7c68",
      "avatarUrl": "/avatars/832fd34bcc041b0b7b551873a459fc3c.svg",
      "fullname": "Wei Liu",
      "name": "PeterV09",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15400",
      "authors": [
        {
          "_id": "682ec5f6b16c79c271c8559e",
          "user": {
            "_id": "67f33b43ccb05db5f0190cf6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/c-UeSyHfO7QchwNXW9mgu.png",
            "isPro": false,
            "fullname": "ZhangXiaoyun",
            "user": "DadaCloud01",
            "type": "user"
          },
          "name": "Xiaoyun Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:16.038Z",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c8559f",
          "user": {
            "_id": "64872abb3fb9bc8f4994e014",
            "avatarUrl": "/avatars/9743b5a2b6f9b9990aa1cb06bd6eb5c6.svg",
            "isPro": false,
            "fullname": "Jingqing Ruan",
            "user": "Amanda2023",
            "type": "user"
          },
          "name": "Jingqing Ruan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:09.457Z",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a0",
          "user": {
            "_id": "663bc46ae14047f71026c2b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663bc46ae14047f71026c2b3/lo4zFH0aIkFUQSNuzJW4I.jpeg",
            "isPro": false,
            "fullname": "Maxing",
            "user": "Machine981",
            "type": "user"
          },
          "name": "Xing Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:12.313Z",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a1",
          "user": {
            "_id": "682ecaa39dd5ef944657bc16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/MGiOAd4-_0HpBfmXWsNmY.png",
            "isPro": false,
            "fullname": "ZHUYAWEN",
            "user": "Yaawennn",
            "type": "user"
          },
          "name": "Yawen Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T08:47:18.660Z",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a2",
          "name": "Haodong Zhao",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a3",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a4",
          "name": "Jiansong Chen",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a5",
          "name": "Ke Zeng",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a6",
          "name": "Xunliang Cai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T11:41:39.000Z",
      "submittedOnDailyAt": "2025-05-22T05:06:49.326Z",
      "title": "Un momento que se puede recordar: transición de modo de simulación de memoria adaptativa para razones eficientes",
      "submittedOnDailyBy": {
        "_id": "64872abb3fb9bc8f4994e014",
        "avatarUrl": "/avatars/9743b5a2b6f9b9990aa1cb06bd6eb5c6.svg",
        "isPro": false,
        "fullname": "Jingqing Ruan",
        "user": "Amanda2023",
        "type": "user"
      },
      "summary": "Los modelos lógicos largos (LRMs) logran un desempeño sorprendente en largas cadenas de razonamiento, pero particularmente en tareas simples, se produce un sobrecosto de cálculo innecesario debido a razonamientos lógicos innecesarios. En este artículo, se evaluan de manera sistemática los modos de Long-Thinking y No-Thinking de los LRMs y se descubre el fenómeno de \"función de auto-recuperación interna\". Desde esta perspectiva, se propone un marco de referencia para la lógica adaptativa de auto-recuperación (ASRR) para inhibir razonamientos innecesarios y facilitar la recuperación potencial. Se introduce una regulación de la ganancia por longitud para la precisión, lo que permite a ASRR asignar adecuadamente el esfuerzo lógico según la dificultad del problema, minimizando la carga de rendimiento y alcanzando altas eficiencias. A través de los resultados de experimentos en muchos benchmarks y modelos, se demuestra que, en comparación con GRPO, ASRR reduce el área lógica en un máximo de 32.5% (1.5B) o 25.7% (7B), minimiza la pérdida de precisión (1.2% y 0.6% en pass@1) y mejora significativamente la tasa de seguridad en los benchmarks de seguridad (aumento máximo de +21.7%). Nuestros resultados revelan el potencial de ASRR para hacer posible lógicas eficientes, adaptativas y más seguras en los LRMs.",
      "upvotes": 15,
      "discussionId": "682ec5f7b16c79c271c855df",
      "ai_summary": "ASRR framework optimizes reasoning efficiency in large models by suppressing redundant information processing without significantly impacting performance or safety.",
      "ai_keywords": [
        "Large reasoning models",
        "LRMs",
        "Long-Thinking modes",
        "No-Thinking modes",
        "Internal Self-Recovery Mechanism",
        "Adaptive Self-Recovery Reasoning",
        "accuracy-aware length reward regulation",
        "reasoning budget",
        "pass@1",
        "harmless rates",
        "safety benchmarks",
        "efficiency",
        "adaptive reasoning",
        "safety"
      ]
    },
    "publishedAt": "2025-05-21T07:41:39.000Z",
    "title": "When to Continue Thinking: Adaptive Thinking Mode Switching for\n  Efficient Reasoning",
    "summary": "Large reasoning models (LRMs) achieve remarkable performance via long\nreasoning chains, but often incur excessive computational overhead due to\nredundant reasoning, especially on simple tasks. In this work, we\nsystematically quantify the upper bounds of LRMs under both Long-Thinking and\nNo-Thinking modes, and uncover the phenomenon of \"Internal Self-Recovery\nMechanism\" where models implicitly supplement reasoning during answer\ngeneration. Building on this insight, we propose Adaptive Self-Recovery\nReasoning (ASRR), a framework that suppresses unnecessary reasoning and enables\nimplicit recovery. By introducing accuracy-aware length reward regulation, ASRR\nadaptively allocates reasoning effort according to problem difficulty,\nachieving high efficiency with negligible performance sacrifice. Experiments\nacross multiple benchmarks and models show that, compared with GRPO, ASRR\nreduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal\naccuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates\non safety benchmarks (up to +21.7%). Our results highlight the potential of\nASRR for enabling efficient, adaptive, and safer reasoning in LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15400.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64872abb3fb9bc8f4994e014",
      "avatarUrl": "/avatars/9743b5a2b6f9b9990aa1cb06bd6eb5c6.svg",
      "fullname": "Jingqing Ruan",
      "name": "Amanda2023",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15210",
      "authors": [
        {
          "_id": "682e84d83291b134b3370184",
          "name": "Jie Ma",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370185",
          "user": {
            "_id": "653d0136d6f7982a7a52054d",
            "avatarUrl": "/avatars/5d47d022dd5954a4c3d243a7f0292d32.svg",
            "isPro": false,
            "fullname": "QUNING",
            "user": "stillqu",
            "type": "user"
          },
          "name": "Ning Qu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:55.205Z",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370186",
          "name": "Zhitao Gao",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370187",
          "name": "Rui Xing",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370188",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370189",
          "name": "Hongbin Pei",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018a",
          "name": "Jiang Xie",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018b",
          "name": "Linyun Song",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018c",
          "name": "Pinghui Wang",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018d",
          "name": "Jing Tao",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018e",
          "name": "Zhou Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T07:38:45.000Z",
      "submittedOnDailyAt": "2025-05-22T00:31:54.508Z",
      "title": "「Revisión de la confianza inicial: la justificación de las razones por las cuales un modelo de lenguaje de gran escala de grafos de conocimiento puede ser confiable」",
      "submittedOnDailyBy": {
        "_id": "67a7099286a55d5569acb213",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rYISkvTtyraUdbgSsNfpC.png",
        "isPro": false,
        "fullname": "JieMa",
        "user": "JamesMile",
        "type": "user"
      },
      "summary": "El enfoque de generación de búsqueda en base a grafos de conocimiento tiene como objetivo reducir la confusión derivada de la falta de conocimiento y la información obsoleta en modelos de lenguaje de gran escala (LLMs). Sin embargo, los métodos actuales no utilizan de manera suficiente los conocimientos sorprendentes incluidos en grafos de conocimiento (KGs), especialmente las informaciones estructurales o restricciones explícitas o ocultas. Estas restricciones pueden mejorar la precisión de la razón de los modelos LLMs y aumentar la confianza en la generación de respuestas. Con ese objetivo, proponemos un marco de trabajo llamado \"Pasar lo sorprendente para revisar\" (DP), que combina aprendizaje supervisado y optimización de Kahneman-Tversky para integrar estructuras sorprendentes en los LLMs y mejorar la precisión de las relaciones. Además, nuestro marco utiliza una estrategia de reflexión basada en restricciones sorprendentes validadas para garantizar la confianza en la generación de respuestas. Los experimentos en tres conjuntos de datos de benchmark, demostraron que DP alcanzó nuevas mejoras en la eficiencia de distancia y, en particular, un aumento del 13% en el Hit@1 en el conjunto de datos ComplexWebQuestions, generando respuestas con alta confianza. Además, se confirmó su flexibilidad y aplicabilidad a través de diversas análisis. El código está disponible en https://github.com/reml-group/Deliberation-on-Priors.",
      "upvotes": 13,
      "discussionId": "682e84d93291b134b33701cb",
      "githubRepo": "https://github.com/reml-group/Deliberation-on-Priors",
      "ai_summary": "The Deliberation over Priors framework enhances the trustworthiness of LLMs by integrating structural and constraint priors from knowledge graphs through knowledge distillation and reasoning introspection.",
      "ai_keywords": [
        "knowledge graph-based retrieval-augmented generation",
        "Large Language Models (LLMs)",
        "knowledge graphs (KGs)",
        "structural information",
        "explicit constraints",
        "implicit constraints",
        "trustworthworthy reasoning framework",
        "Deliberation over Priors (DP)",
        "progressive knowledge distillation",
        "supervised fine-tuning",
        "Kahneman-Tversky optimization",
        "relation path generation",
        "reasoning-introspection strategy",
        "ComplexWebQuestions dataset",
        "Hit@1 improvement"
      ]
    },
    "publishedAt": "2025-05-21T03:38:45.000Z",
    "title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models\n  on Knowledge Graphs",
    "summary": "Knowledge graph-based retrieval-augmented generation seeks to mitigate\nhallucinations in Large Language Models (LLMs) caused by insufficient or\noutdated knowledge. However, existing methods often fail to fully exploit the\nprior knowledge embedded in knowledge graphs (KGs), particularly their\nstructural information and explicit or implicit constraints. The former can\nenhance the faithfulness of LLMs' reasoning, while the latter can improve the\nreliability of response generation. Motivated by these, we propose a\ntrustworthy reasoning framework, termed Deliberation over Priors (DP), which\nsufficiently utilizes the priors contained in KGs. Specifically, DP adopts a\nprogressive knowledge distillation strategy that integrates structural priors\ninto LLMs through a combination of supervised fine-tuning and Kahneman-Tversky\noptimization, thereby improving the faithfulness of relation path generation.\nFurthermore, our framework employs a reasoning-introspection strategy, which\nguides LLMs to perform refined reasoning verification based on extracted\nconstraint priors, ensuring the reliability of response generation. Extensive\nexperiments on three benchmark datasets demonstrate that DP achieves new\nstate-of-the-art performance, especially a Hit@1 improvement of 13% on the\nComplexWebQuestions dataset, and generates highly trustworthy responses. We\nalso conduct various analyses to verify its flexibility and practicality. The\ncode is available at https://github.com/reml-group/Deliberation-on-Priors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15210.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a7099286a55d5569acb213",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rYISkvTtyraUdbgSsNfpC.png",
      "fullname": "JieMa",
      "name": "JamesMile",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15779",
      "authors": [
        {
          "_id": "682e9d84a0db46260ccc15e1",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e2",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e3",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e4",
          "name": "Mingliang Zhai",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e5",
          "name": "Yifan Chang",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e6",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:31:49.000Z",
      "submittedOnDailyAt": "2025-05-22T02:37:20.764Z",
      "title": "IA-T2I: Internet cualquier texto a imagen generación",
      "submittedOnDailyBy": {
        "_id": "6794cd79b72b1721ea69f4f2",
        "avatarUrl": "/avatars/4e4fb9e9e127a0c031131ace705687cd.svg",
        "isPro": false,
        "fullname": "Ming Li",
        "user": "afdsafas",
        "type": "user"
      },
      "summary": "Actualmente, los modelos de generación de imágenes a partir del texto (T2I) han demostrado resultados estables pero presentan problemas cuando la información contextual es incierta. Por ejemplo, un modelo T2I lanzado en febrero tiene dificultades en crear posteres adecuados para una película que se estreno en abril. Esto se debe a que el modelo no está seguro sobre el diseño de los personajes o estilos. Para resolver estos problemas, proponemos un marco de trabajo para la generación de imágenes a partir del texto (IA-T2I). Este marco fortalece el modelo T2I mediante la provisión de imágenes de referencia basadas en un procesamiento de texto para clarificar la incertidumbre de conocimientos contextuales. En particular, el módulo de búsqueda activa decide si se necesita una imagen de referencia según el contexto, el módulo de selección de imágenes de niveles decide las imágenes óptimas que devuelve un motor de búsqueda de imágenes, y el módulo de reflexión evalúa continuamente las imágenes generadas para mejorarlas de acuerdo con el procesamiento de texto. Para evaluar el rendimiento del marco, preparamos un conjunto de datos para el procesamiento de texto que incluye 3 tipos de conocimientos inciertos: (1) raros pero conocidos, (2) desconocidos, y (3) inciertos. Además, creamos procesamientos complejos y realizamos evaluaciones de preferencia con GPT-4o, lo que demostró una precisión similar a la evaluación de preferencias humanas. Los resultados de los experimentos muestran que nuestro marco de trabajo demostró un rendimiento superior de al menos 30% más que GPT-4o en evaluaciones de preferencia humanas.",
      "upvotes": 12,
      "discussionId": "682e9d85a0db46260ccc161f",
      "ai_summary": "An Internet-Augmented text-to-image generation framework improves uncertain text prompt handling by integrating reference images, enhancing image quality and fidelity.",
      "ai_keywords": [
        "text-to-image generation",
        "IA-T2I framework",
        "reference images",
        "active retrieval module",
        "hierarchical image selection module",
        "self-reflection mechanism",
        "Img-Ref-T2I dataset",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-05-21T13:31:49.000Z",
    "title": "IA-T2I: Internet-Augmented Text-to-Image Generation",
    "summary": "Current text-to-image (T2I) generation models achieve promising results, but\nthey fail on the scenarios where the knowledge implied in the text prompt is\nuncertain. For example, a T2I model released in February would struggle to\ngenerate a suitable poster for a movie premiering in April, because the\ncharacter designs and styles are uncertain to the model. To solve this problem,\nwe propose an Internet-Augmented text-to-image generation (IA-T2I) framework to\ncompel T2I models clear about such uncertain knowledge by providing them with\nreference images. Specifically, an active retrieval module is designed to\ndetermine whether a reference image is needed based on the given text prompt; a\nhierarchical image selection module is introduced to find the most suitable\nimage returned by an image search engine to enhance the T2I model; a\nself-reflection mechanism is presented to continuously evaluate and refine the\ngenerated image to ensure faithful alignment with the text prompt. To evaluate\nthe proposed framework's performance, we collect a dataset named Img-Ref-T2I,\nwhere text prompts include three types of uncertain knowledge: (1) known but\nrare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt\nto guide GPT-4o in making preference evaluation, which has been shown to have\nan evaluation accuracy similar to that of human preference evaluation.\nExperimental results demonstrate the effectiveness of our framework,\noutperforming GPT-4o by about 30% in human evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15779.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6794cd79b72b1721ea69f4f2",
      "avatarUrl": "/avatars/4e4fb9e9e127a0c031131ace705687cd.svg",
      "fullname": "Ming Li",
      "name": "afdsafas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14357",
      "authors": [
        {
          "_id": "682d2b9ba006b93dbe79a333",
          "user": {
            "_id": "66b06fe4ce2224a4d066cc0a",
            "avatarUrl": "/avatars/3f89007c5c17e57e623a10d82637b5fc.svg",
            "isPro": false,
            "fullname": "knightnemo",
            "user": "knightnemo",
            "type": "user"
          },
          "name": "Siqiao Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T18:04:10.245Z",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a334",
          "user": {
            "_id": "643b866bff50448bcfc7d1d1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "manchery",
            "type": "user"
          },
          "name": "Jialong Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-22T05:44:25.954Z",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a335",
          "name": "Qixing Zhou",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a336",
          "name": "Shangchen Miao",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a337",
          "name": "Mingsheng Long",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T13:41:45.000Z",
      "submittedOnDailyAt": "2025-05-22T00:17:33.386Z",
      "title": "Vid2World: Crear un modelo de depresión vídeo y hablar sobre el modelo de mundo interactivo.",
      "submittedOnDailyBy": {
        "_id": "66b06fe4ce2224a4d066cc0a",
        "avatarUrl": "/avatars/3f89007c5c17e57e623a10d82637b5fc.svg",
        "isPro": false,
        "fullname": "knightnemo",
        "user": "knightnemo",
        "type": "user"
      },
      "summary": "El modelo mundial realiza predicciones basadas en observaciones históricas y secuencias de acciones, demostrando una gran expectativa para mejorar la eficiencia de datos en decisiones secuenciales. Sin embargo, actualmente, los modelos mundiales requieren entrenamiento muy especializado en dominios estrictos, producen predicciones de baja calidad y limitan la aplicabilidad en entornos complejos. Los modelos de difusión de video entrenados con grandes conjuntos de datos de internet muestran excelentes capacidades para generar altas calidades de video que detectan las acciones de diversos entornos reales. En este artículo, se presenta un enfoque general llamado Vid2World, que utiliza modelos de difusión de video en interacción con modelos de interacción. Vid2World diseña la arquitectura y el objetivo de entrenamiento de modelos de difusión de video que permiten generación automática de manera recursiva y causalidad, además de introducir una estructura de guía de acciones causales para mejorar la capacidad de control de acciones en modelos de interacción. Los experimentos expandidos en el campo de la manipulación mecánica y la simulación de juegos demuestran que nuestro método proporciona una aproximación efectiva y perjudicial para reutilizar modelos de difusión de video de alta calidad en modelos de interacción.",
      "upvotes": 12,
      "discussionId": "682d2b9da006b93dbe79a3af",
      "projectPage": "https://knightnemo.github.io/vid2world/",
      "ai_summary": "Vid2World repurposes pre-trained video diffusion models into interactive world models via causalization and action guidance, enhancing action controllability and scalability in complex environments.",
      "ai_keywords": [
        "world models",
        "transitions",
        "history observation",
        "action sequences",
        "data efficiency",
        "sequential decision making",
        "low-fidelity",
        "coarse predictions",
        "video diffusion models",
        "internet-scale datasets",
        "high-quality videos",
        "real-world dynamics",
        "autoregressive generation",
        "causal action guidance",
        "robot manipulation",
        "game simulation"
      ]
    },
    "publishedAt": "2025-05-20T09:41:45.000Z",
    "title": "Vid2World: Crafting Video Diffusion Models to Interactive World Models",
    "summary": "World models, which predict transitions based on history observation and\naction sequences, have shown great promise in improving data efficiency for\nsequential decision making. However, existing world models often require\nextensive domain-specific training and still produce low-fidelity, coarse\npredictions, limiting their applicability in complex environments. In contrast,\nvideo diffusion models trained on large, internet-scale datasets have\ndemonstrated impressive capabilities in generating high-quality videos that\ncapture diverse real-world dynamics. In this work, we present Vid2World, a\ngeneral approach for leveraging and transferring pre-trained video diffusion\nmodels into interactive world models. To bridge the gap, Vid2World performs\ncasualization of a pre-trained video diffusion model by crafting its\narchitecture and training objective to enable autoregressive generation.\nFurthermore, it introduces a causal action guidance mechanism to enhance action\ncontrollability in the resulting interactive world model. Extensive experiments\nin robot manipulation and game simulation domains show that our method offers a\nscalable and effective approach for repurposing highly capable video diffusion\nmodels to interactive world models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14357.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b06fe4ce2224a4d066cc0a",
      "avatarUrl": "/avatars/3f89007c5c17e57e623a10d82637b5fc.svg",
      "fullname": "knightnemo",
      "name": "knightnemo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15146",
      "authors": [
        {
          "_id": "682e980fb16c79c271babcbd",
          "user": {
            "_id": "6301d6455e305a35cb0846a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
            "isPro": true,
            "fullname": "Lanxiang Hu",
            "user": "Snyhlxde",
            "type": "user"
          },
          "name": "Lanxiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:51.281Z",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcbe",
          "name": "Mingjia Huo",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcbf",
          "user": {
            "_id": "653200c5d0f5a9e537e76695",
            "avatarUrl": "/avatars/d1beb984d91908c03e0daeb77589a475.svg",
            "isPro": false,
            "fullname": "Yuxuan Zhang",
            "user": "Yuxuan13",
            "type": "user"
          },
          "name": "Yuxuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:48.879Z",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc0",
          "name": "Haoyang Yu",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc1",
          "name": "Eric P. Xing",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc2",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc3",
          "name": "Tajana Rosing",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc4",
          "name": "Haojian Jin",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc5",
          "name": "Hao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T06:02:55.000Z",
      "submittedOnDailyAt": "2025-05-22T01:52:33.947Z",
      "title": "lmgame-Bench: ¿Cuál es la capacidad de un LLM para jugar juegos?",
      "submittedOnDailyBy": {
        "_id": "6301d6455e305a35cb0846a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
        "isPro": true,
        "fullname": "Lanxiang Hu",
        "user": "Snyhlxde",
        "type": "user"
      },
      "summary": "Para ejecutar un juego se necesitan habilidades de reconocimiento visual, memoria y capacidad de planificación. Estas habilidades son necesarias para que un agente de modelos de lenguaje grande (LLM) modernos puedan entenderlo. Se investigó si se puede aplicar directamente el LLM en los juegos para realizar evaluaciones efectivas, pero se descubrió que no es así. La razón se reduce a tres puntos: la falta de reconocimiento visual, la sensibilidad a los prompts y la contextualización de los datos. Se presenta lmgame-Bench. lmgame-Bench proporciona juegos de plataforma, juegos de puzzle y juegos de naruto, utiliza una API de estilo Gym y combina un estándar ligero de reconocimiento visual y memoria para estabilizar la incertidumbre de los prompts y resolver los problemas de contextualización. Incluye 13 modelos avanzados, permitiendo desafíos y diferenciar modelos. Un análisis correlacional muestra que cada juego valida una combinación de habilidades únicas y se evalúa de maneras diferentes en cada juego. Interesantemente, al ejecutar un juego en lmgame-Bench, se puede propagar a nuevos juegos y a tareas de planificación externa. El código de evaluación está disponible en https://github.com/lmgame-org/GamingAgent/lmgame-bench.",
      "upvotes": 11,
      "discussionId": "682e9810b16c79c271babd61",
      "projectPage": "https://lmgame.org/",
      "ai_summary": "lmgame-Bench evaluates large language models using games with diverse challenges, demonstrating unique capability blends and transfer learning potential.",
      "ai_keywords": [
        "LLMs",
        "large language model agents",
        "brittle vision perception",
        "prompt sensitivity",
        "data contamination",
        "Gym-style API",
        "perception scaffolds",
        "memory scaffolds",
        "reinforcement learning",
        "transfer learning"
      ]
    },
    "publishedAt": "2025-05-21T02:02:55.000Z",
    "title": "lmgame-Bench: How Good are LLMs at Playing Games?",
    "summary": "Playing video games requires perception, memory, and planning, exactly the\nfaculties modern large language model (LLM) agents are expected to master. We\nstudy the major challenges in using popular video games to evaluate modern LLMs\nand find that directly dropping LLMs into games cannot make an effective\nevaluation, for three reasons -- brittle vision perception, prompt sensitivity,\nand potential data contamination. We introduce lmgame-Bench to turn games into\nreliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and\nnarrative games delivered through a unified Gym-style API and paired with\nlightweight perception and memory scaffolds, and is designed to stabilize\nprompt variance and remove contamination. Across 13 leading models, we show\nlmgame-Bench is challenging while still separating models well. Correlation\nanalysis shows that every game probes a unique blend of capabilities often\ntested in isolation elsewhere. More interestingly, performing reinforcement\nlearning on a single game from lmgame-Bench transfers both to unseen games and\nto external planning tasks. Our evaluation code is available at\nhttps://github.com/lmgame-org/GamingAgent/lmgame-bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15146.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6301d6455e305a35cb0846a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
      "fullname": "Lanxiang Hu",
      "name": "Snyhlxde",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15765",
      "authors": [
        {
          "_id": "682e85aa7b41e70cf11758b6",
          "name": "Kaizhi Zheng",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758b7",
          "name": "Ruijian Zhang",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758b8",
          "name": "Jing Gu",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758b9",
          "name": "Jie Yang",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758ba",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/LlIYiX21uaVLjqWu6lAQI.mp4"
      ],
      "publishedAt": "2025-05-21T17:10:47.000Z",
      "submittedOnDailyAt": "2025-05-22T00:36:41.981Z",
      "title": "3D Village se puede crear con una sola foto.",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "3DTown es un marco de trabajo diseñado sin necesidad de entrenamiento para sintetizar panoramas 3D realistas y coherentes a partir de una imagen de solo una fase. Nuestro método se basa en dos principios: la generación basada en el lugar mejora la consistencia y la resolución en 3D, mientras que el inpainting 3D cognitivo espacial garantiza la coherencia y la calidad de generación 3D en todo el paisaje. Se dividen las imágenes en regiones que se superponen y cada región se genera mediante un generador 3D de objetos preentrenado, posteriormente se llenan las estructuras 3D marcadas mediante un proceso de inpainting de flujo alineado, manteniendo la continuidad estructural. Esta disección del módulo permite superar los obstáculos de resolución y preservar la estructura espacial, sin necesidad de supervisión 3D o ajustes. A través de amplios experimentos en diferentes paisajes, 3DTown muestra un excelente rendimiento comparado con los estándares más recientes, incluyendo la calidad geométrica, la coherencia espacial y la coherencia de textura. Nuestros resultados demuestran que es posible generar de manera principianal y sin entrenamiento una alta calidad de ciudades 3D a partir de una sola imagen.",
      "upvotes": 9,
      "discussionId": "682e85b17b41e70cf1175aa0",
      "ai_summary": "A training-free framework named 3DTown generates realistic 3D scenes from a single top-down image using region-based generation and spatial-aware 3D inpainting techniques.",
      "ai_keywords": [
        "3D generative models",
        "3DTown",
        "region-based generation",
        "spatial-aware 3D inpainting",
        "masked rectified flow",
        "Trellis",
        "Hunyuan3D-2",
        "TripoSG"
      ]
    },
    "publishedAt": "2025-05-21T13:10:47.000Z",
    "title": "Constructing a 3D Town from a Single Image",
    "summary": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view\ndata, or labor-intensive modeling. Therefore, a lightweight alternative,\ngenerating complex 3D scenes from a single top-down image, plays an essential\nrole in real-world applications. While recent 3D generative models have\nachieved remarkable results at the object level, their extension to full-scene\ngeneration often leads to inconsistent geometry, layout hallucinations, and\nlow-quality meshes. In this work, we introduce 3DTown, a training-free\nframework designed to synthesize realistic and coherent 3D scenes from a single\ntop-down view. Our method is grounded in two principles: region-based\ngeneration to improve image-to-3D alignment and resolution, and spatial-aware\n3D inpainting to ensure global scene coherence and high-quality geometry\ngeneration. Specifically, we decompose the input image into overlapping regions\nand generate each using a pretrained 3D object generator, followed by a masked\nrectified flow inpainting process that fills in missing geometry while\nmaintaining structural continuity. This modular design allows us to overcome\nresolution bottlenecks and preserve spatial structure without requiring 3D\nsupervision or fine-tuning. Extensive experiments across diverse scenes show\nthat 3DTown outperforms state-of-the-art baselines, including Trellis,\nHunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and\ntexture fidelity. Our results demonstrate that high-quality 3D town generation\nis achievable from a single image using a principled, training-free approach.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/LlIYiX21uaVLjqWu6lAQI.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15765.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15404",
      "authors": [
        {
          "_id": "682e8b3de3d1137730d0517b",
          "name": "Zhexin Zhang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517c",
          "name": "Xian Qi Loye",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517d",
          "name": "Victor Shea-Jay Huang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517e",
          "user": {
            "_id": "65d859a3661492b25c46a117",
            "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
            "isPro": false,
            "fullname": "junxiao yang",
            "user": "yangjunxiao2021",
            "type": "user"
          },
          "name": "Junxiao Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:24.874Z",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517f",
          "name": "Qi Zhu",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05180",
          "name": "Shiyao Cui",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05181",
          "name": "Fei Mi",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05182",
          "name": "Lifeng Shang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05183",
          "name": "Yingkang Wang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05184",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05185",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T11:45:29.000Z",
      "submittedOnDailyAt": "2025-05-22T00:57:01.326Z",
      "title": "¿Cómo se puede mejorar la seguridad de modelos de inferencia lógica a gran escala: investigación experimental",
      "submittedOnDailyBy": {
        "_id": "61b58aa0d65058ce70beb98c",
        "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
        "isPro": false,
        "fullname": "Zhexin Zhang",
        "user": "nonstopfor",
        "type": "user"
      },
      "summary": "Los modelos lógicos (LRMs) han logrado un éxito sorprendente en la resolución de problemas lógicos matemáticos o programación. Sin embargo, esta mejora en la capacidad lógica no necesariamente se corresponde con un aumento de seguridad. En algunos casos, puede incluso deteriorar la seguridad. Esto ha generado una tarea de investigación importante: ¿Qué son los métodos para mejorar la seguridad de los LRMs? En este artículo, realizamos una investigación experimental detallada sobre cómo mejorar la seguridad de los LRMs mediante fine-tuning supervisado (SFT). Nuestra investigación ha descubierto sorprendentemente que la capacidad de derivar respuestas seguras directamente en DeepSeek-R1 no necesariamente corresponde con un gran aumento de seguridad. Este fenómeno se analizó y se identificaron tres formas híbridas que caracterizan este problema. Luego, demostramos que resolver explícitamente estos problemas durante el proceso de destilación de datos es esencial para lograr un gran aumento de seguridad. También investigamos si procesos lógicos largos o complejos pueden ser necesarios para alcanzar la seguridad. Interesantemente, se puede lograr un rendimiento de seguridad relativamente alto con procesos lógicos cortos o basados en templados, y estos son más sencillos para el modelo que procesos lógicos más complejos. Estos hallazgos impulsan a reflexionar más profundamente sobre el papel que la lógica puede jugar en la seguridad. Finalmente, demostramos que mezclar Magic Data en la micro-regulación de la seguridad puede ayudar a equilibrar la seguridad y la rechazo excesivo. En general, nuestra investigación experimental proporciona una visión más amplia sobre los métodos para mejorar la seguridad de los LRMs. Los códigos y datos utilizados en este artículo están disponibles en https://github.com/thu-coai/LRM-Safety-Study.",
      "upvotes": 8,
      "discussionId": "682e8b3fe3d1137730d051f5",
      "ai_summary": "The study investigates methods to enhance the safety of Large Reasoning Models (LRMs) through Supervised Fine-Tuning (SFT), finding that explicit addressing of failure patterns and use of simpler reasoning processes can improve safety without requiring complex reasoning chains or excessive data.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRMs",
        "Supervised Fine-Tuning",
        "SFT",
        "DeepSeek-R1",
        "data distillation",
        "reasoning process",
        "safety improvements",
        "over-refusal",
        "math reasoning"
      ]
    },
    "publishedAt": "2025-05-21T07:45:29.000Z",
    "title": "How Should We Enhance the Safety of Large Reasoning Models: An Empirical\n  Study",
    "summary": "Large Reasoning Models (LRMs) have achieved remarkable success on\nreasoning-intensive tasks such as mathematics and programming. However, their\nenhanced reasoning capabilities do not necessarily translate to improved safety\nperformance-and in some cases, may even degrade it. This raises an important\nresearch question: how can we enhance the safety of LRMs? In this paper, we\npresent a comprehensive empirical study on how to enhance the safety of LRMs\nthrough Supervised Fine-Tuning (SFT). Our investigation begins with an\nunexpected observation: directly distilling safe responses from DeepSeek-R1\nfails to significantly enhance safety. We analyze this phenomenon and identify\nthree key failure patterns that contribute to it. We then demonstrate that\nexplicitly addressing these issues during the data distillation process can\nlead to substantial safety improvements. Next, we explore whether a long and\ncomplex reasoning process is necessary for achieving safety. Interestingly, we\nfind that simply using short or template-based reasoning process can attain\ncomparable safety performance-and are significantly easier for models to learn\nthan more intricate reasoning chains. These findings prompt a deeper reflection\non the role of reasoning in ensuring safety. Finally, we find that mixing math\nreasoning data during safety fine-tuning is helpful to balance safety and\nover-refusal. Overall, we hope our empirical study could provide a more\nholistic picture on enhancing the safety of LRMs. The code and data used in our\nexperiments are released in https://github.com/thu-coai/LRM-Safety-Study.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15404.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b58aa0d65058ce70beb98c",
      "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
      "fullname": "Zhexin Zhang",
      "name": "nonstopfor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15817",
      "authors": [
        {
          "_id": "682ec132fbbbc6e3a91b106e",
          "user": {
            "_id": "6623ea65b642e29cdf90a1b4",
            "avatarUrl": "/avatars/e32e90574c1162b2be87ed78604e3e4d.svg",
            "isPro": true,
            "fullname": "TongZheng",
            "user": "TongZheng1999",
            "type": "user"
          },
          "name": "Tong Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:27.903Z",
          "hidden": false
        },
        {
          "_id": "682ec132fbbbc6e3a91b106f",
          "name": "Lichang Chen",
          "hidden": false
        },
        {
          "_id": "682ec132fbbbc6e3a91b1070",
          "name": "Simeng Han",
          "hidden": false
        },
        {
          "_id": "682ec132fbbbc6e3a91b1071",
          "name": "R. Thomas McCoy",
          "hidden": false
        },
        {
          "_id": "682ec132fbbbc6e3a91b1072",
          "name": "Heng Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:59:54.000Z",
      "submittedOnDailyAt": "2025-05-22T06:15:33.221Z",
      "title": "Aprende y comprende el método de pensamiento que permite controlar la confusión del pensamiento, manteniendo una pensamiento lógico claro, a través de la comprensión de razones lógicas en el proceso de entendimiento de razones.",
      "submittedOnDailyBy": {
        "_id": "6623ea65b642e29cdf90a1b4",
        "avatarUrl": "/avatars/e32e90574c1162b2be87ed78604e3e4d.svg",
        "isPro": true,
        "fullname": "TongZheng",
        "user": "TongZheng1999",
        "type": "user"
      },
      "summary": "La humanidad aprende naturalmente a modelarse a muchas razones y resolve problemas lógicos de manera natural. Por ejemplo, utilizan diferentes formatos de expresión como lenguaje natural, código y lógica simbólica. En contraste, muchos métodos basados en modelos de lenguaje profundo (LLM) utilizan un solo modelo durante el entrenamiento. Generalmente, se utiliza el lenguaje natural. Sin embargo, algunos métodos intentan seleccionar modelos o expandir durante la inferencia, pero el proceso de entrenamiento no está relacionado con el modelo y limitan la cooperación entre modelos. Para mejorar estas deficiencias, proponemos la Thought-Mixture (MoT). MoT es un marco de trabajo que proporciona razones para que los modelos de lenguaje profundo puedan modelarse a lenguaje natural, código, modelos simbólicos adicionales y una tabla de verdad (que organiza casos de logística para mitigar algunos de los principales fallos en la inferencia lógica). MoT utiliza un diseño de dos etapas: 1) entrenamiento automático de MoT, donde filtra historias entre modelos para aprender de las razones generadas; 2) inferencia con MoT, donde se utilizan el mayor número posible de modelos para generar mejores predicciones. Se han realizado experimentos en marcos de prueba lógicos como FOLIO y ProofWriter, y nuestro marco de trabajo MoT ha demostrado un aumento de +11.7pp en la precisión promedio, comparado con el enfoque de \"chain-of-thought\" de un solo modelo. Un análisis más profundo muestra que el marco de trabajo MoT brinda beneficios tanto en el entrenamiento como en la inferencia, especialmente para problemas lógicos difíciles, demostrando la complementariedad entre modelos y superando los principales obstáculos de inferencia lógica basada en tablas de verdad.",
      "upvotes": 7,
      "discussionId": "682ec133fbbbc6e3a91b10c1",
      "githubRepo": "https://github.com/zhengkid/Truth_Table_Logical_Reasoning",
      "ai_summary": "A Mixture-of-Thought framework enables LLMs to reason across natural language, code, and symbolic logic, improving accuracy on logical reasoning tasks compared to single-modality approaches.",
      "ai_keywords": [
        "LMM-based approaches",
        "reasoning modality",
        "natural language",
        "code",
        "symbolic logic",
        "modality-blind",
        "self-evolving MoT training",
        "MoT inference",
        "FOLIO",
        "ProofWriter",
        "truth-table",
        "logical reasoning"
      ]
    },
    "publishedAt": "2025-05-21T13:59:54.000Z",
    "title": "Learning to Reason via Mixture-of-Thought for Logical Reasoning",
    "summary": "Human beings naturally utilize multiple reasoning modalities to learn and\nsolve logical problems, i.e., different representational formats such as\nnatural language, code, and symbolic logic. In contrast, most existing\nLLM-based approaches operate with a single reasoning modality during training,\ntypically natural language. Although some methods explored modality selection\nor augmentation at inference time, the training process remains modality-blind,\nlimiting synergy among modalities. To fill in this gap, we propose\nMixture-of-Thought (MoT), a framework that enables LLMs to reason across three\ncomplementary modalities: natural language, code, and a newly introduced\nsymbolic modality, truth-table, which systematically enumerates logical cases\nand partially mitigates key failure modes in natural language reasoning. MoT\nadopts a two-phase design: (1) self-evolving MoT training, which jointly learns\nfrom filtered, self-generated rationales across modalities; and (2) MoT\ninference, which fully leverages the synergy of three modalities to produce\nbetter predictions. Experiments on logical reasoning benchmarks including FOLIO\nand ProofWriter demonstrate that our MoT framework consistently and\nsignificantly outperforms strong LLM baselines with single-modality\nchain-of-thought approaches, achieving up to +11.7pp average accuracy gain.\nFurther analyses show that our MoT framework benefits both training and\ninference stages; that it is particularly effective on harder logical reasoning\nproblems; and that different modalities contribute complementary strengths,\nwith truth-table reasoning helping to overcome key bottlenecks in natural\nlanguage inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15817.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6623ea65b642e29cdf90a1b4",
      "avatarUrl": "/avatars/e32e90574c1162b2be87ed78604e3e4d.svg",
      "fullname": "TongZheng",
      "name": "TongZheng1999",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15781",
      "authors": [
        {
          "_id": "682ea1129c1b77a5030b31f2",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "682ea1129c1b77a5030b31f3",
          "name": "Runpeng Yu",
          "hidden": false
        },
        {
          "_id": "682ea1129c1b77a5030b31f4",
          "name": "Gongfan Fang",
          "hidden": false
        },
        {
          "_id": "682ea1129c1b77a5030b31f5",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:32:10.000Z",
      "submittedOnDailyAt": "2025-05-22T02:29:25.873Z",
      "title": "dKV-Cache: dKV Cache es un caché para el Modelo de Lenguaje Diferencial.",
      "submittedOnDailyBy": {
        "_id": "64396ebc21221ac7411852b3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
        "isPro": false,
        "fullname": "Xinyin Ma",
        "user": "horseee",
        "type": "user"
      },
      "summary": "Los Modelos de Lenguaje por Difusión (DLMs) son una buena opción para modelos de auto-completación de lenguaje. Sin embargo, DLMs han experimentado limitaciones en la velocidad de inferencia durante largos períodos de tiempo. El problema fundamental radica en la arquitectura no de auto-completación y el enfoque bidireccional de atención, que no permiten el uso de un caché de claves-valores para acelerar la decodificación. Para resolver estas barreras, se propone un caché similar a los procesos de difusión en los DLMs. Nuestro enfoque se basa en la observación de que diferentes tokens tienen diferentes representaciones semánticas durante el proceso de difusión. Por lo tanto, se propone una estrategia de caché con un retraso en el estado de las claves y valores. Además, se diseñaron dos variables de interpolación para almacenar las claves y valores en diferentes etapas: (1) dKV-Cache-Decode proporciona una aceleración casi sin pérdida, mejora el rendimiento en secuencias largas y muestra que los DLMs actuales utilizan poco eficientemente la información contextual durante la inferencia. (2) dKV-Cache-Greedy implementa un caché con una vida corta, logrando una aceleración rápida pero permitiendo una pérdida de rendimiento. Finalmente, dKV-Cache logró una aceleración entre 2 y 10 veces durante la inferencia, reduciendo significativamente la diferencia entre AR y DLMs. Nuestro dKV-Cache fue evaluado en benchmarks generales de comprensión del lenguaje, matemáticas y generación de código, y ofreció una aceleración. Los experimentos demostraron que el uso del caché en los DLMs actuales es posible sin necesidad de entrenamiento adicional.",
      "upvotes": 7,
      "discussionId": "682ea1139c1b77a5030b322a",
      "ai_summary": "A KV-cache-like mechanism, delayed KV-Cache, accelerates diffusion language models' inference without significantly degrading performance.",
      "ai_keywords": [
        "Diffusion Language Models",
        "KV-cache",
        "non-autoregressive architecture",
        "bidirectional attention",
        "key-value states",
        "token representation dynamics",
        "dKV-Cache-Decode",
        "dKV-Cache-Greedy",
        "speedup",
        "autoregressive models"
      ]
    },
    "publishedAt": "2025-05-21T13:32:10.000Z",
    "title": "dKV-Cache: The Cache for Diffusion Language Models",
    "summary": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15781.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64396ebc21221ac7411852b3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
      "fullname": "Xinyin Ma",
      "name": "horseee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15776",
      "authors": [
        {
          "_id": "682eb95671d01f3fc74f0f8c",
          "user": {
            "_id": "644a41fcd9a3ae8341055179",
            "avatarUrl": "/avatars/21b35abdc60a34589443b5879901eb46.svg",
            "isPro": false,
            "fullname": "Changtai Zhu",
            "user": "BeastyZ",
            "type": "user"
          },
          "name": "Changtai Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:31.485Z",
          "hidden": false
        },
        {
          "_id": "682eb95671d01f3fc74f0f8d",
          "name": "Siyin Wang",
          "hidden": false
        },
        {
          "_id": "682eb95671d01f3fc74f0f8e",
          "name": "Ruijun Feng",
          "hidden": false
        },
        {
          "_id": "682eb95671d01f3fc74f0f8f",
          "name": "Kai Song",
          "hidden": false
        },
        {
          "_id": "682eb95671d01f3fc74f0f90",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:27:42.000Z",
      "submittedOnDailyAt": "2025-05-22T04:51:57.147Z",
      "title": "ConvSearch-R1: Mejora de la reconfiguración de términos de búsqueda en búsquedas de diálogo mediante la aplicación de aprendizaje por refuerzo para la lógica",
      "submittedOnDailyBy": {
        "_id": "64c3c631e77ea9f28111172a",
        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
        "isPro": false,
        "fullname": "Siyin Wang",
        "user": "sinwang",
        "type": "user"
      },
      "summary": "El sistema de búsqueda de conversaciones requiere efectivamente procesar consultas contexto-dependientes. Estas consultas incluyen diversas incertidumbres, faltantes y pérdidas de coherencia (core parallax). La Conversational Query Reformulation (CQR) es un enfoque que transforma estas consultas en formatos adecuados para un dispositivo de búsqueda en un solo entorno, proporcionando asistencia. Sin embargo, el enfoque actual de CQR presenta dos limitaciones importantes: altos costos de análisis humano externo y dependencia de supervísores de gran escala, así como una insuficiente correspondencia entre modelos de reformulación y dispositivos de búsqueda posteriores. Presentamos el primer marco automático ConvSearch-R1. Este método utiliza aprendizaje por refuerzo para optimizar directamente la reformulación de la consulta a través de señales de búsqueda, eliminando completamente la dependencia de supervísores externos de reformulación. Nuestro nuevo enfoque de dos etapas combina la aprendizaje automático mediante guías de búsqueda para resolver problemas de inicio de código y la aplicación de la técnica de semilla de recompensa Rank-in-Incidence mediante guías de búsqueda para aprendizaje por refuerzo. Esto permite abordar los problemas de raridad en métricas de búsqueda generales. Con experimentos extendidos en los conjuntos de datos TopiOCQA y QReCC, ConvSearch-R1 supera significativamente a los métodos superiores anteriores, mejorando en un 10% o más en el conjunto de datos difícil TopiOCQA, utilizando un pequeño modelo de 3B parámetros sin necesidad de supervísores externos.",
      "upvotes": 7,
      "discussionId": "682eb95771d01f3fc74f0fdd",
      "githubRepo": "https://github.com/BeastyZ/ConvSearch-R1",
      "ai_summary": "ConvSearch-R1 uses reinforcement learning and self-distillation to improve conversational query reformulation without relying on external supervision, outperforming state-of-the-art methods.",
      "ai_keywords": [
        "Conversational Query Reformulation",
        "CQR",
        "reinforcement learning",
        "self-distillation",
        "cold-start problem",
        "rank-incentive reward shaping",
        "retrieval signals",
        "TopiOCQA",
        "QReCC"
      ]
    },
    "publishedAt": "2025-05-21T13:27:42.000Z",
    "title": "ConvSearch-R1: Enhancing Query Reformulation for Conversational Search\n  with Reasoning via Reinforcement Learning",
    "summary": "Conversational search systems require effective handling of context-dependent\nqueries that often contain ambiguity, omission, and coreference. Conversational\nQuery Reformulation (CQR) addresses this challenge by transforming these\nqueries into self-contained forms suitable for off-the-shelf retrievers.\nHowever, existing CQR approaches suffer from two critical constraints: high\ndependency on costly external supervision from human annotations or large\nlanguage models, and insufficient alignment between the rewriting model and\ndownstream retrievers. We present ConvSearch-R1, the first self-driven\nframework that completely eliminates dependency on external rewrite supervision\nby leveraging reinforcement learning to optimize reformulation directly through\nretrieval signals. Our novel two-stage approach combines Self-Driven Policy\nWarm-Up to address the cold-start problem through retrieval-guided\nself-distillation, followed by Retrieval-Guided Reinforcement Learning with a\nspecially designed rank-incentive reward shaping mechanism that addresses the\nsparsity issue in conventional retrieval metrics. Extensive experiments on\nTopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly\noutperforms previous state-of-the-art methods, achieving over 10% improvement\non the challenging TopiOCQA dataset while using smaller 3B parameter models\nwithout any external supervision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3c631e77ea9f28111172a",
      "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
      "fullname": "Siyin Wang",
      "name": "sinwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15656",
      "authors": [
        {
          "_id": "682e8add58a17fe0e9ec02e6",
          "name": "Zhexin Zhang",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02e7",
          "name": "Yuhao Sun",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02e8",
          "user": {
            "_id": "65d859a3661492b25c46a117",
            "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
            "isPro": false,
            "fullname": "junxiao yang",
            "user": "yangjunxiao2021",
            "type": "user"
          },
          "name": "Junxiao Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:27.527Z",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02e9",
          "name": "Shiyao Cui",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02ea",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02eb",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T15:32:14.000Z",
      "submittedOnDailyAt": "2025-05-22T00:55:08.348Z",
      "title": "Tenga en cuenta: En el fine-tuning de LLMs de código abierto, es importante tener en cuenta que tus datos de fine-tuning pueden tener una alta probabilidad de ser robados.",
      "submittedOnDailyBy": {
        "_id": "61b58aa0d65058ce70beb98c",
        "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
        "isPro": false,
        "fullname": "Zhexin Zhang",
        "user": "nonstopfor",
        "type": "user"
      },
      "summary": "La utilización de datos profesionales para fines de ajuste en grandes modelos de lenguaje abierto (LLMs) es una práctica estándar para desarrolladores de bajo costo que buscan obtener LLMs adecuados para ciertas tareas. Se han descubierto riesgos nuevos o preocupantes asociados con esta práctica: los creadores de LLMs abierto código pueden extraer datos de fines de prueba simples a través de un método de entrenamiento de \"backdoor\". Nuestros experimentos detallados se realizaron con cuatro modelos abierto código (de 3B a 32B parámetros) utilizando cuatro conjuntos de datos profesionales y dos conjuntos de datos de bajo costo, demostrando que la eficiencia de extracción se mejora: en una configuración real, 76.3% de los datos de bajo costo (consultas) se pueden extraer perfectamente de un conjunto de datos total de 5,000 muestras, y en una configuración ideal, la tasa de éxito alcanza un 94.9%. Además, investigamos estrategias de defensa basadas en detectores, pero no logramos resistir a ataques mejorados. En general, se enfatiza la urgencia del reconocimiento de un nuevo riesgo de \"data break\" en la inyección de datos, y se propone la necesidad de investigaciones futuras para enfrentarse a este riesgo. El código y los datos utilizados en nuestros experimentos están disponibles en https://github.com/thu-coai/Backdoor-Data-Extraction.",
      "upvotes": 7,
      "discussionId": "682e8ade58a17fe0e9ec0348",
      "ai_summary": "There is a newly identified risk that creators of open-source LLMs can extract fine-tuning data from downstream models through backdoor training, even with black-box access.",
      "ai_keywords": [
        "Large Language Models",
        "fine-tuning",
        "open-source",
        "black-box access",
        "backdoor training",
        "data extraction",
        "data breach"
      ]
    },
    "publishedAt": "2025-05-21T11:32:14.000Z",
    "title": "Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data\n  Could Be Secretly Stolen!",
    "summary": "Fine-tuning on open-source Large Language Models (LLMs) with proprietary data\nis now a standard practice for downstream developers to obtain task-specific\nLLMs. Surprisingly, we reveal a new and concerning risk along with the\npractice: the creator of the open-source LLMs can later extract the private\ndownstream fine-tuning data through simple backdoor training, only requiring\nblack-box access to the fine-tuned downstream model. Our comprehensive\nexperiments, across 4 popularly used open-source models with 3B to 32B\nparameters and 2 downstream datasets, suggest that the extraction performance\ncan be strikingly high: in practical settings, as much as 76.3% downstream\nfine-tuning data (queries) out of a total 5,000 samples can be perfectly\nextracted, and the success rate can increase to 94.9% in more ideal settings.\nWe also explore a detection-based defense strategy but find it can be bypassed\nwith improved attack. Overall, we highlight the emergency of this newly\nidentified data breaching risk in fine-tuning, and we hope that more follow-up\nresearch could push the progress of addressing this concerning risk. The code\nand data used in our experiments are released at\nhttps://github.com/thu-coai/Backdoor-Data-Extraction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b58aa0d65058ce70beb98c",
      "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
      "fullname": "Zhexin Zhang",
      "name": "nonstopfor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13934",
      "authors": [
        {
          "_id": "682e8369b16c79c271b4db81",
          "user": {
            "_id": "643b866bff50448bcfc7d1d1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "manchery",
            "type": "user"
          },
          "name": "Jialong Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-22T05:44:24.633Z",
          "hidden": false
        },
        {
          "_id": "682e8369b16c79c271b4db82",
          "name": "Shaofeng Yin",
          "hidden": false
        },
        {
          "_id": "682e8369b16c79c271b4db83",
          "name": "Ningya Feng",
          "hidden": false
        },
        {
          "_id": "682e8369b16c79c271b4db84",
          "name": "Mingsheng Long",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643b866bff50448bcfc7d1d1/OXSeIgpwt_mjJJqBffXyA.png"
      ],
      "publishedAt": "2025-05-20T05:02:53.000Z",
      "submittedOnDailyAt": "2025-05-22T00:24:21.597Z",
      "title": "RLVR-World: Entrenamiento de un modelo mundial utilizando aprendizaje por refuerzo",
      "submittedOnDailyBy": {
        "_id": "643b866bff50448bcfc7d1d1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "manchery",
        "type": "user"
      },
      "summary": "El modelo mundial predice movimientos de estado en función de acciones y ha sido desarrollado en diferentes modalidades. Sin embargo, funciones de entrenamiento estándar como la evaluación de máxima verosimilitud (MLE) no se ajustan a los objetivos específicos de un modelo mundial. Por ejemplo, en el caso de evaluar la predicción de movimiento, se utilizan indicadores como la precisión o la calidad visual. En este artículo, presentamos RLVR-World, un marco integrado utilizando RLVR (Reinforcement Learning con recompensas verificables). Este marco tiene como objetivo optimizar directamente el modelo mundial con respecto a estas métricas. RLVR-World ha demostrado potencialmente mejorar significativamente la performance en diversas áreas, como juegos, navegación web y manipulación de robots, tanto en modelos de lenguaje como en modelos de imágenes. Nuestro estudio supera el desarrollo reciente de modelos de lenguaje y muestra cómo RLVR amplía la utilidad de modelos generativos, demostrando también la posibilidad de cambiar el paradigma de entrenamiento.",
      "upvotes": 7,
      "discussionId": "682e836bb16c79c271b4dc78",
      "projectPage": "https://thuml.github.io/RLVR-World/",
      "githubRepo": "https://github.com/thuml/RLVR-World",
      "ai_summary": "RLVR-World uses reinforcement learning with verifiable rewards to optimize world models for task-specific metrics, achieving improved performance across language and video domains.",
      "ai_keywords": [
        "world models",
        "maximum likelihood estimation",
        "transition prediction",
        "reinforcement learning",
        "verifiable rewards",
        "autoregressive prediction",
        "tokenized sequences",
        "text games",
        "web navigation",
        "robot manipulation"
      ]
    },
    "publishedAt": "2025-05-20T01:02:53.000Z",
    "title": "RLVR-World: Training World Models with Reinforcement Learning",
    "summary": "World models predict state transitions in response to actions and are\nincreasingly developed across diverse modalities. However, standard training\nobjectives such as maximum likelihood estimation (MLE) often misalign with\ntask-specific goals of world models, i.e., transition prediction metrics like\naccuracy or perceptual quality. In this paper, we present RLVR-World, a unified\nframework that leverages reinforcement learning with verifiable rewards (RLVR)\nto directly optimize world models for such metrics. Despite formulating world\nmodeling as autoregressive prediction of tokenized sequences, RLVR-World\nevaluates metrics of decoded predictions as verifiable rewards. We demonstrate\nsubstantial performance gains on both language- and video-based world models\nacross domains, including text games, web navigation, and robot manipulation.\nOur work indicates that, beyond recent advances in reasoning language models,\nRLVR offers a promising post-training paradigm for enhancing the utility of\ngenerative models more broadly.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643b866bff50448bcfc7d1d1/OXSeIgpwt_mjJJqBffXyA.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13934.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643b866bff50448bcfc7d1d1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
      "fullname": "Jialong Wu",
      "name": "manchery",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13529",
      "authors": [
        {
          "_id": "682e8c441ffd3af3cb2f27d3",
          "user": {
            "_id": "65d859a3661492b25c46a117",
            "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
            "isPro": false,
            "fullname": "junxiao yang",
            "user": "yangjunxiao2021",
            "type": "user"
          },
          "name": "Junxiao Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:19.492Z",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d4",
          "name": "Jinzhe Tu",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d5",
          "name": "Haoran Liu",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d6",
          "name": "Xiaoce Wang",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d7",
          "name": "Chujie Zheng",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d8",
          "name": "Zhexin Zhang",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d9",
          "name": "Shiyao Cui",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27da",
          "name": "Caishun Chen",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27db",
          "name": "Tiantian He",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27dc",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27dd",
          "name": "Yew-Soon Ong",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27de",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T07:27:34.000Z",
      "submittedOnDailyAt": "2025-05-22T01:02:19.522Z",
      "title": "BARREL: Teoría de la Razón para los LRMs de Alta Confianza en la Realidad\n\n**Nota:** La traducción se ha realizado manteniendo la profundidad y la precisión del original.",
      "submittedOnDailyBy": {
        "_id": "65d859a3661492b25c46a117",
        "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
        "isPro": false,
        "fullname": "junxiao yang",
        "user": "yangjunxiao2021",
        "type": "user"
      },
      "summary": "El reciente desarrollo de los modelos de inferencia de gran escala (LRMs) ha demostrado una excelente capacidad para la matemática y la inferencia lógica. Sin embargo, actualmente, estos LRMs no admiten la ignorancia ni pueden responder con \"no sé\" o similares, en cambio, presentan una excesiva confianza que conduce a la producción de respuestas erróneas. Esto ha generado una preocupación respecto a la confianza en su capacidad. En este estudio, se identifican dos patrones de inferencia anormales que conducen a respuestas incorrectas y se propone un nuevo marco de trabajo llamado BARREL para abordar este problema. Este marco tiene como objetivo promover una inferencia factual y limitada, con un buen conocimiento de los límites. Los resultados de los experimentos muestran que el entrenamiento con BARREL ha aumentado la confianza en DeepSeek-R1-Distill-Llama-8B del 39.33% al 61.48%, y al mismo tiempo, ha logrado la misma precisión que un modelo fine-tunado con datos de inferencia generados en R1. Estos resultados demuestran que nuestra investigación experimental muestra la posibilidad de desarrollar LRMs de sistema 2 con mayor confianza y factualidad.",
      "upvotes": 7,
      "discussionId": "682e8c451ffd3af3cb2f281f",
      "ai_summary": "A novel framework, BARREL, addresses overconfidence in Large Reasoning Models by promoting concise and factual reasoning, significantly improving their reliability.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRMs",
        "overthinking",
        "last-minute guessing",
        "second-thought spiraling",
        "boundary-aware",
        "DeepSeek",
        "R1-Distill-Llama-8B",
        "reliable System 2 LRMs"
      ]
    },
    "publishedAt": "2025-05-18T03:27:34.000Z",
    "title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs",
    "summary": "Recent advances in Large Reasoning Models (LRMs) have shown impressive\ncapabilities in mathematical and logical reasoning. However, current LRMs\nrarely admit ignorance or respond with \"I don't know\". Instead, they often\nproduce incorrect answers while showing undue confidence, raising concerns\nabout their factual reliability. In this work, we identify two pathological\nreasoning patterns characterized by overthinking that contribute to the\noverconfident and incorrect answers: last-minute guessing and second-thought\nspiraling. To address these issues, we propose BARREL-a novel framework that\npromotes concise and boundary-aware factual reasoning. Our experiments show\nthat BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B\nfrom 39.33% to 61.48%, while still achieving accuracy comparable to models\nfinetuned on reasoning data generated by R1. These results demonstrate that our\npilot study is inspiring to build more reliable and factual System 2 LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13529.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d859a3661492b25c46a117",
      "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
      "fullname": "junxiao yang",
      "name": "yangjunxiao2021",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15778",
      "authors": [
        {
          "_id": "682e8b59a5b1e59c6978645a",
          "name": "Zhen Zhang",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645b",
          "name": "Xuehai He",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645c",
          "name": "Weixiang Yan",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645d",
          "name": "Ao Shen",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645e",
          "name": "Chenyang Zhao",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645f",
          "name": "Shuohang Wang",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c69786460",
          "name": "Yelong Shen",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c69786461",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:29:15.000Z",
      "submittedOnDailyAt": "2025-05-22T00:56:40.217Z",
      "title": "Soft Thinking: Libera la potencial de la lógica de los LLMs en el espacio de conceptos de noticias.",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "El humano generalmente piensa en conceptos abstractos y dinámicos, pero utiliza estos conceptos en lugar de puntos fijos de palabras. Los modelos actuales de lógica están limitados dentro de los límites del lenguaje humano y tratan puntos fijos como tokens únicos. Esta restricción única limita la expresividad y las limitaciones de estos modelos de lógica, llevando a exploraciones incompletas en el camino de la lógica. El método estándar de Chain-of-Thought (CoT) depende de la muestreo de un solo token.\n\nEn este artículo, se presenta una metodología sin aprendizaje llamada \"pensamiento suave\". Este método simula la lógica \"suave\" como la humana, generando tokens abstractos y suaves en un espacio de conceptos continuos. Estos tokens abstractos son generados a partir de una mezcla de pesos probabilísticos de embeddings, formando un espacio de conceptos continuos. Esto permite movimientos suaves y una expresión rica, superando así las restricciones de la unicidad. Fundamentalmente, cada token generado contiene múltiples significados relacionados con un token único, explorando potencialmente varias rutas de lógica para encontrar la respuesta correcta.\n\nLos experimentos en varios benchmarks de matemáticas y programación demuestran la efectividad y eficiencia de \"pensamiento suave\", demostrando que, en comparación con el CoT estándar, aumenta la precisión en PATH@1 en 2.48 puntos y reduce el uso de tokens en un 22.4%. Un análisis cualitativo revela que los resultados de \"pensamiento suave\" son interpretables en altas dimensiones y tienen una alta legibilidad, mostrando la posibilidad de superar las limitaciones inherentes a la unicidad en el lenguaje. El código está disponible en https://github.com/eric-ai-lab/Soft-Thinking.",
      "upvotes": 6,
      "discussionId": "682e8b5aa5b1e59c697864ce",
      "projectPage": "https://soft-thinking.github.io",
      "githubRepo": "https://github.com/eric-ai-lab/Soft-Thinking",
      "ai_summary": "Soft Thinking, a training-free method, enhances reasoning by generating soft, abstract concept tokens in a continuous space, improving accuracy and efficiency in mathematical and coding benchmarks.",
      "ai_keywords": [
        "Soft Thinking",
        "continuous concept space",
        "token embeddings",
        "Chain-of-Thought",
        "pass@1 accuracy"
      ]
    },
    "publishedAt": "2025-05-21T13:29:15.000Z",
    "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous\n  Concept Space",
    "summary": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15778.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14827",
      "authors": [
        {
          "_id": "682ea25b2c417303938ae8fc",
          "name": "Yufan Zhuang",
          "hidden": false
        },
        {
          "_id": "682ea25b2c417303938ae8fd",
          "name": "Liyuan Liu",
          "hidden": false
        },
        {
          "_id": "682ea25b2c417303938ae8fe",
          "name": "Chandan Singh",
          "hidden": false
        },
        {
          "_id": "682ea25b2c417303938ae8ff",
          "name": "Jingbo Shang",
          "hidden": false
        },
        {
          "_id": "682ea25b2c417303938ae900",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6438ccbb3b46237de3d052e8/6rJB7g-l02Jn_iSD0C7ZJ.png"
      ],
      "publishedAt": "2025-05-20T18:41:46.000Z",
      "submittedOnDailyAt": "2025-05-22T02:35:44.464Z",
      "title": "Mejora la generación de texto en comparación con el sampling de tokens.",
      "submittedOnDailyBy": {
        "_id": "6438ccbb3b46237de3d052e8",
        "avatarUrl": "/avatars/baa624d417b0b905e82127dc66346478.svg",
        "isPro": true,
        "fullname": "Yufan Zhuang",
        "user": "yzhuang",
        "type": "user"
      },
      "summary": "En la generación automática de regresión estándar, los LLMs predecien la distribución de próximo token y muestran esta distribución para seleccionar un token, lo cual se elimina del resto de los tokens, y solo se transmite el token seleccionado como nuevo entrada. Para preservar la información rica de esta distribución, se propone el método de entrenamiento infinito de generación automática \"Mixture of Inputs (MoI)\". De acuerdo al paradigma estándar, se generan los tokens y se construye un nuevo entrada mezclando la distribución de los tokens generados con la distribución de los tokens que se han eliminado anteriormente. Específicamente, se utiliza un método bayesiano que reemplaza la distribución previa por una distribución posterior continua, sustituyendo los tokens con un vector one-hot por su valor esperado. El MoI permite que el modelo mantenga una representación interna rica en toda la generación y puede mejorar la calidad de las oraciones y la capacidad de inferencia. En tareas de cálculo matemático, generación de código y QA de nivel de diálogo, el MoI mejora el rendimiento en modelos como QwQ-32B, Nemotron-Super-49B, Gemma-3-27B y DAPO-Qwen-32B, aumentando la eficiencia sin necesidad de entrenamiento adicional o sobrecarga de cálculo.",
      "upvotes": 4,
      "discussionId": "682ea25c2c417303938ae926",
      "projectPage": "https://github.com/EvanZhuang/mixinputs",
      "githubRepo": "https://github.com/EvanZhuang/mixinputs",
      "ai_summary": "Mixture of Inputs (MoI), a training-free method, enhances autoregressive generation by maintaining a richer internal representation, improving text quality and reasoning capabilities in mathematical reasoning, code generation, and PhD-level QA tasks.",
      "ai_keywords": [
        "Mixture of Inputs (MoI)",
        "autoregressive generation",
        "token distribution",
        "Bayesian estimation",
        "posterior expectation"
      ]
    },
    "publishedAt": "2025-05-20T14:41:46.000Z",
    "title": "Text Generation Beyond Discrete Token Sampling",
    "summary": "In standard autoregressive generation, an LLM predicts the next-token\ndistribution, samples a discrete token, and then discards the distribution,\npassing only the sampled token as new input. To preserve this distribution's\nrich information, we propose Mixture of Inputs (MoI), a training-free method\nfor autoregressive generation. After generating a token following the standard\nparadigm, we construct a new input that blends the generated discrete token\nwith the previously discarded token distribution. Specifically, we employ a\nBayesian estimation method that treats the token distribution as the prior, the\nsampled token as the observation, and replaces the conventional one-hot vector\nwith the continuous posterior expectation as the new model input. MoI allows\nthe model to maintain a richer internal representation throughout the\ngeneration process, resulting in improved text quality and reasoning\ncapabilities. On mathematical reasoning, code generation, and PhD-level QA\ntasks, MoI consistently improves performance across multiple models including\nQwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional\ntraining and negligible computational overhead.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6438ccbb3b46237de3d052e8/6rJB7g-l02Jn_iSD0C7ZJ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14827.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6438ccbb3b46237de3d052e8",
      "avatarUrl": "/avatars/baa624d417b0b905e82127dc66346478.svg",
      "fullname": "Yufan Zhuang",
      "name": "yzhuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.12650",
      "authors": [
        {
          "_id": "682c057b36dd2dbca3931150",
          "user": {
            "_id": "64ca1b79ec9a33183aa3bd29",
            "avatarUrl": "/avatars/cafeafeccced927aac986575338a804c.svg",
            "isPro": false,
            "fullname": "yang",
            "user": "yaotianvector",
            "type": "user"
          },
          "name": "Yaotian Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:18:15.239Z",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931151",
          "user": {
            "_id": "6552f1ad5d55ccb20e9142a0",
            "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
            "isPro": false,
            "fullname": "Ivan Tang",
            "user": "IvanTang",
            "type": "user"
          },
          "name": "Yiwen Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:18:12.714Z",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931152",
          "name": "Yizhe Chen",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931153",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931154",
          "name": "Jiangjie Qiu",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931155",
          "name": "Hao Xiong",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931156",
          "name": "Haoyu Yin",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931157",
          "name": "Zhiyao Luo",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931158",
          "name": "Yifei Zhang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931159",
          "name": "Sijia Tao",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115a",
          "name": "Wentao Li",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115b",
          "name": "Qinghua Zhang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115c",
          "name": "Yuqiang Li",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115d",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115e",
          "name": "Bin Zhao",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115f",
          "name": "Xiaonan Wang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931160",
          "name": "Fei Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T03:04:50.000Z",
      "submittedOnDailyAt": "2025-05-22T00:29:51.182Z",
      "title": "AutoMat: Uso de un herramienta genética para reestructurar la arquitectura del sistema de automatización proveniente de microscopías ópticas.",
      "submittedOnDailyBy": {
        "_id": "6552f1ad5d55ccb20e9142a0",
        "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
        "isPro": false,
        "fullname": "Ivan Tang",
        "user": "IvanTang",
        "type": "user"
      },
      "summary": "Los intercambios de interacciones y fuerzas entre átomos basados en aprendizaje automático dependen precisamente de la estructura de átomos, pero estas datos son raros debido a la utilidad limitada de los datos experimentales de determinadas decisiones. La medición de la resolución de átomos en microscopios electrónicos actúa como un recurso potencial de datos estructurales, pero su conversión a formatos adecuados para simulaciones es un proceso laborioso y error-prone que juega un papel crucial en la entrenamiento y validación de modelos. Presentamos una pipeline llamada AutoMat que ayuda a las agentes desde el principio hasta el final. Este sistema automatiza la conversión de imágenes de microscopio electrónico de transición estocástica (STEM) en estructuras de átomos y predice sus características físicas. AutoMat combina técnicas de desruido, búsqueda de templates físicos, reconstrucción de átomos con simetría, reacción rápida y predicción de características mediante MatterSim, y operación colaborativa en todos los pasos. Proponemos STEM2Mat-Bench, un marco de evaluación específico para este trabajo, que mide la precisión mediante RMSD de la red, MAE de energía de formación y el éxito en la correspondencia estructural. AutoMat supera a los modelos de lenguaje únicamente basados en texto en este campo, y alcanza la implementación completa del pipeline gracias a la operación de herramientas externas. En experimentos de gran escala con más de 450 muestras de estructuras, AutoMat supera significativamente a modelos de lenguaje y herramientas de diferentes escalas. Estos resultados demuestran la efectividad de AutoMat y STEM2Mat-Bench, y marcan un nuevo paso hacia la conexión entre microscopios y simulaciones de átomos en la ciencia de materiales. Código y datasets están disponibles en https://github.com/yyt-2378/AutoMat y https://huggingface.co/datasets/yaotianvector/STEM2Mat.",
      "upvotes": 4,
      "discussionId": "682c057d36dd2dbca3931206",
      "ai_summary": "AutoMat, an agent-assisted pipeline, transforms atomic-resolution STEM images into simulation-ready atomic crystal structures and predicts their properties, overcoming the bottleneck in data availability and processing.",
      "ai_keywords": [
        "end-to-end pipeline",
        "agent-assisted",
        "scanning transmission electron microscopy (STEM)",
        "pattern-adaptive denoising",
        "physics-guided template retrieval",
        "symmetry-aware atomic reconstruction",
        "fast relaxation",
        "property prediction",
        "MatterSim",
        "STEM2Mat-Bench",
        "lattice RMSD",
        "formation energy MAE",
        "structure-matching success rate",
        "multimodal large language models"
      ]
    },
    "publishedAt": "2025-05-18T23:04:50.000Z",
    "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from\n  Microscopy via Agentic Tool Use",
    "summary": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12650.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6552f1ad5d55ccb20e9142a0",
      "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
      "fullname": "Ivan Tang",
      "name": "IvanTang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15816",
      "authors": [
        {
          "_id": "682e8304e9980508c9a2fc73",
          "name": "Penghao Wu",
          "hidden": false
        },
        {
          "_id": "682e8304e9980508c9a2fc74",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "682e8304e9980508c9a2fc75",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:59:52.000Z",
      "submittedOnDailyAt": "2025-05-22T00:23:40.991Z",
      "title": "Streaming Line 윷엿 Owerk Direction in LMM",
      "submittedOnDailyBy": {
        "_id": "64101f81b27543634e377fc1",
        "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
        "isPro": false,
        "fullname": "Penghao Wu",
        "user": "craigwu",
        "type": "user"
      },
      "summary": "Los modelos de gran escala muestran excelente rendimiento en tareas multimodal, pero sufren de problemas computacionales significativos debido a la cálculo excesivo de tokens visuales. La técnica de reducción de tokens se centra en el redundancia a nivel de token, pero nosotros estamos investigando la redundancia computacional en los tokens visuales, identificando la pérdida de información y buscando soluciones. Nuestra principal idea es que los tokens visuales generados por un encoder visuo-pretendido en un modelo multimodal no necesitan un decodificador completo, ya que solo el decodificador es necesario para realizar operaciones pesadas (como self-attention y FFNs). Con una diseño adecuado, se puede lograr un procesamiento más ligero. Hemos diseñado experimentos para identificar y reducir la redundancia computacional en los tokens visuales. Basándonos en nuestros hallazgos, proponemos una nueva aproximación llamada ProxyV. ProxyV reduce la carga computacional mientras mejora la eficiencia, y también puede obtener resultados significativos en mejoras de eficiencia más suaves. Además, la flexibilidad de ProxyV permite combinarla con técnicas de reducción de tokens para obtener una eficiencia más alta. El código está disponible en esta URL.",
      "upvotes": 2,
      "discussionId": "682e8305e9980508c9a2fca1",
      "projectPage": "https://penghao-wu.github.io/ProxyV/",
      "githubRepo": "https://github.com/penghao-wu/ProxyV",
      "ai_summary": "ProxyV alleviates computational burdens in large multimodal models by using proxy vision tokens, enhancing efficiency without sacrificing performance.",
      "ai_keywords": [
        "multimodal models",
        "computation-level redundancy",
        "vision tokens",
        "pretrained vision encoder",
        "decoder-only LMMs",
        "self-attention",
        "FFNs",
        "proxy vision tokens",
        "ProxyV"
      ]
    },
    "publishedAt": "2025-05-21T13:59:52.000Z",
    "title": "Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM",
    "summary": "Large multimodal models excel in multimodal tasks but face significant\ncomputational challenges due to excessive computation on visual tokens. Unlike\ntoken reduction methods that focus on token-level redundancy, we identify and\nstudy the computation-level redundancy on vision tokens to ensure no\ninformation loss. Our key insight is that vision tokens from the pretrained\nvision encoder do not necessarily require all the heavy operations (e.g.,\nself-attention, FFNs) in decoder-only LMMs and could be processed more lightly\nwith proper designs. We designed a series of experiments to discover and\nprogressively squeeze out the vision-related computation redundancy. Based on\nour findings, we propose ProxyV, a novel approach that utilizes proxy vision\ntokens to alleviate the computational burden on original vision tokens. ProxyV\nenhances efficiency without compromising performance and can even yield notable\nperformance gains in scenarios with more moderate efficiency improvements.\nFurthermore, the flexibility of ProxyV is demonstrated through its combination\nwith token reduction methods to boost efficiency further. The code will be made\npublic at this https://github.com/penghao-wu/ProxyV URL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15816.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64101f81b27543634e377fc1",
      "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
      "fullname": "Penghao Wu",
      "name": "craigwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15791",
      "authors": [
        {
          "_id": "682e8e9f1d7637a2584b6b8c",
          "name": "Fengyuan Dai",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b8d",
          "name": "Zifeng Zhuang",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b8e",
          "name": "Yufei Huang",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b8f",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b90",
          "name": "Bangyan Liao",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b91",
          "name": "Donglin Wang",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b92",
          "name": "Fajie Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:44:37.000Z",
      "submittedOnDailyAt": "2025-05-22T01:10:45.389Z",
      "title": "VARD: Modelo de propagación utilizando RL basado en valores para ajuste denso eficiente",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "Los modelos de difusión han aparecido como potentes herramientas generativas en varias áreas, pero ajustar los modelos de cadena de juegos para implementar características específicas es un desafío. El aprendizaje por refuerzo (RL) proporciona buenas decisiones, sin embargo, los métodos actuales tienen dificultades para lograr una ajuste micro-óptimo estable y eficiente, así como soportar recompensas no diferenciables. Además, las recompensas raras no proporcionan suficiente visión en los pasos intermedios, lo que a menudo resulta en productos de calidad reducida. Para resolver estos limitaciones, se necesita una señal densa y diferenciable en todo el proceso de difusión. Por lo tanto, proponemos un nuevo enfoque para predecir la expectativa de la recompensa desde los estados intermedios. Esta función de valor utiliza la normalización KL para proporcionar una visión densa en todo el proceso de generación. Nuestro método hace que el modelo aprenda más cerca de lo deseado, permitiendo un aprendizaje efectivo y estable a través de la retropropagación. Los resultados de los experimentos muestran que nuestro enfoque proporciona un mejor guiado de rutas, mejora en el aprendizaje y amplia el rango de aplicación de modelos de difusión optimizados para recompensas no diferenciables y complejas mediante aprendizaje por refuerzo.",
      "upvotes": 2,
      "discussionId": "682e8ea11d7637a2584b6c46",
      "ai_summary": "VARD introduces a value function based reinforcement learning approach to enhance diffusion models with dense and differentiable supervision, improving training efficiency and handling non-differentiable rewards.",
      "ai_keywords": [
        "diffusion models",
        "reinforcement learning",
        "stable fine-tuning",
        "non-differentiable rewards",
        "sparse rewards",
        "value function",
        "dense supervision",
        "KL regularization",
        "backpropagation",
        "trajectory guidance",
        "training efficiency"
      ]
    },
    "publishedAt": "2025-05-21T13:44:37.000Z",
    "title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with\n  Value-based RL",
    "summary": "Diffusion models have emerged as powerful generative tools across various\ndomains, yet tailoring pre-trained models to exhibit specific desirable\nproperties remains challenging. While reinforcement learning (RL) offers a\npromising solution,current methods struggle to simultaneously achieve stable,\nefficient fine-tuning and support non-differentiable rewards. Furthermore,\ntheir reliance on sparse rewards provides inadequate supervision during\nintermediate steps, often resulting in suboptimal generation quality. To\naddress these limitations, dense and differentiable signals are required\nthroughout the diffusion process. Hence, we propose VAlue-based Reinforced\nDiffusion (VARD): a novel approach that first learns a value function\npredicting expection of rewards from intermediate states, and subsequently uses\nthis value function with KL regularization to provide dense supervision\nthroughout the generation process. Our method maintains proximity to the\npretrained model while enabling effective and stable training via\nbackpropagation. Experimental results demonstrate that our approach facilitates\nbetter trajectory guidance, improves training efficiency and extends the\napplicability of RL to diffusion models optimized for complex,\nnon-differentiable reward functions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15791.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15047",
      "authors": [
        {
          "_id": "682e8a7651706f69070c40d3",
          "name": "Yingming Pu",
          "hidden": false
        },
        {
          "_id": "682e8a7651706f69070c40d4",
          "name": "Tao Lin",
          "hidden": false
        },
        {
          "_id": "682e8a7651706f69070c40d5",
          "name": "Hongyu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T03:09:39.000Z",
      "submittedOnDailyAt": "2025-05-22T00:58:05.428Z",
      "title": "PiFlow: Concepto y fundamento de la investigación científica realizado a través de la colaboración de equipos interdisciplinarios.",
      "submittedOnDailyBy": {
        "_id": "63d0c7d16b985b0f25d00a22",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d0c7d16b985b0f25d00a22/6bNSM4WhVsEndIVx4Mb8U.png",
        "isPro": false,
        "fullname": "Mellen Y. Pu",
        "user": "Mellen",
        "type": "user"
      },
      "summary": "El sistema de múltiples agentes basado en modelos de lenguaje grande (LLM) muestra claramente una gran posibilidad para la descubrimiento científico. Sin embargo, el enfoque actual utiliza flujos de trabajo irrelevantes para automatizar la descubrimiento científico, lo que impide reducir la incertidumbre sistemática, causada por hipótesis impuros y una falta de coherencia entre evidencias y hipótesis. Para superar estas limitaciones, es fundamental reducir la incertidumbre sistemática de manera sistemática. Presentamos el marco teórico de información llamado PiFlow, que propone un enfoque para abordar problemas de reducción de incertidumbre estructurada, derivados de principios fundamentales de la ciencia (por ejemplo, las leyes de la ciencia). Los resultados de evaluación en tres diferentes áreas científicas muestran que nuestro método significativamente mejora la eficiencia de la descubrimiento, con un aumento del 73.55% en el área bajo la curva de los valores de características (AUC) durante la etapa de exploración, y un aumento del 94.06% en la calidad de las soluciones comparado con sistemas de agentes bayesianos. En general, PiFlow impulsa un nuevo paradigma para la automatización de la descubrimiento científico de alta eficiencia, y abre caminos para una investigación más robusta y acelerada liderada por la IA. El código está disponible en GitHub: https://github.com/amair-lab/PiFlow.",
      "upvotes": 2,
      "discussionId": "682e8a7751706f69070c4121",
      "githubRepo": "https://github.com/amair-lab/PiFlow",
      "ai_summary": "PiFlow, an information-theoretical framework, improves automated scientific discovery by systematically reducing uncertainty and enhancing solution quality across various scientific domains.",
      "ai_keywords": [
        "Large Language Model",
        "multi-agent systems",
        "automated scientific discovery",
        "predefined workflows",
        "scientific discovery",
        "information-theoretical framework",
        "uncertainty reduction",
        "hypothesis",
        "evidence",
        "Area Under the Curve",
        "exploration steps",
        "solution quality",
        "Plug-and-Play method"
      ]
    },
    "publishedAt": "2025-05-20T23:09:39.000Z",
    "title": "PiFlow: Principle-aware Scientific Discovery with Multi-Agent\n  Collaboration",
    "summary": "Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate\nremarkable potential for scientific discovery. Existing approaches, however,\noften automate scientific discovery using predefined workflows that lack\nrationality constraints. This often leads to aimless hypothesizing and a\nfailure to consistently link hypotheses with evidence, thereby hindering\nsystematic uncertainty reduction. Overcoming these limitations fundamentally\nrequires systematic uncertainty reduction. We introduce PiFlow, an\ninformation-theoretical framework, treating automated scientific discovery as a\nstructured uncertainty reduction problem guided by principles (e.g., scientific\nlaws). In evaluations across three distinct scientific domains -- discovering\nnanomaterial structures, bio-molecules, and superconductor candidates with\ntargeted properties -- our method significantly improves discovery efficiency,\nreflected by a 73.55\\% increase in the Area Under the Curve (AUC) of property\nvalues versus exploration steps, and enhances solution quality by 94.06\\%\ncompared to a vanilla agent system. Overall, PiFlow serves as a\nPlug-and-Play method, establishing a novel paradigm shift in highly efficient\nautomated scientific discovery, paving the way for more robust and accelerated\nAI-driven research. Code is publicly available at our\nhttps://github.com/amair-lab/PiFlow{GitHub}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15047.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d0c7d16b985b0f25d00a22",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d0c7d16b985b0f25d00a22/6bNSM4WhVsEndIVx4Mb8U.png",
      "fullname": "Mellen Y. Pu",
      "name": "Mellen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15034",
      "authors": [
        {
          "_id": "682ecc348573bac82a974c8b",
          "user": {
            "_id": "6801fbe4cae7a6623f66ef64",
            "avatarUrl": "/avatars/383a72cc890946d89a185b8482d1b828.svg",
            "isPro": false,
            "fullname": "Kaiwen Zha",
            "user": "sunshinekevin",
            "type": "user"
          },
          "name": "Kaiwen Zha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:06.379Z",
          "hidden": false
        },
        {
          "_id": "682ecc348573bac82a974c8c",
          "name": "Zhengqi Gao",
          "hidden": false
        },
        {
          "_id": "682ecc348573bac82a974c8d",
          "name": "Maohao Shen",
          "hidden": false
        },
        {
          "_id": "682ecc348573bac82a974c8e",
          "name": "Zhang-Wei Hong",
          "hidden": false
        },
        {
          "_id": "682ecc348573bac82a974c8f",
          "name": "Duane S. Boning",
          "hidden": false
        },
        {
          "_id": "682ecc348573bac82a974c90",
          "name": "Dina Katabi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T02:43:15.000Z",
      "submittedOnDailyAt": "2025-05-22T06:45:08.638Z",
      "title": "RL Tango: Teoría de la Razón del Lenguaje fortalecida por la colaboración de generador y demostrador en aprendizaje reforzado",
      "submittedOnDailyBy": {
        "_id": "6801fbe4cae7a6623f66ef64",
        "avatarUrl": "/avatars/383a72cc890946d89a185b8482d1b828.svg",
        "isPro": false,
        "fullname": "Kaiwen Zha",
        "user": "sunshinekevin",
        "type": "user"
      },
      "summary": "El aprendizaje por refuerzo (RL) es un método efectivo para mejorar el rendimiento de los grandes modelos de lenguaje (LLM) que han aparecido recientemente. Los generadores de LLM funcionan bajo una política guiada por un reconocido (modelo de recompensa). Sin embargo, los métodos de post-procesamiento actuales de RL en LLM utilizan principalmente reconocidos fijos (basados en reglas o entrenados para detenerse) o reconocidos entrenados de manera normativa. Esta diseñación es vulnerable a la hacking de recompensas y difícil de expandir la distribución de aprendizaje. Para superar estos límites, proponemos un nuevo marco llamado Tango. Tango utiliza RL para entrenar tanto el generador de LLM como el reconocido, entrenando ambos simultáneamente. La innovación clave de Tango es que el reconocido se entrene a nivel de proceso generativo y evoluciona junto con el generador. Es importante destacar que el reconocido se entrena en base a una recompensa de precisión de reconocimiento a nivel de resultado, lo que requiere menos explicaciones claras durante el proceso, permitiendo que el reconocido entrenado con RL RL sea más robusto y expandible que un reconocido entrenado con SFT. Los experimentos extendidos han logrado los mejores resultados en modelos de 7B/8B: el generador ha alcanzado el mejor rendimiento en 5 de los 5 marcos de competencia de matemáticas y en 4 tareas de razonamiento compleja de lenguaje, mientras que el reconocido ha alcanzado el mejor rendimiento en el conjunto de datos ProcessBench. En particular, ambos componentes han mostrado un gran aumento en el problema matemático más difícil. El código está disponible en https://github.com/kaiwenzha/rl-tango.",
      "upvotes": 2,
      "discussionId": "682ecc358573bac82a974cd9",
      "githubRepo": "https://github.com/kaiwenzha/rl-tango",
      "ai_summary": "Tango is an RL framework that concurrently trains a generative LLM and a RL-trained verifier, achieving superior robustness and generalization in math benchmarks and out-of-domain reasoning tasks.",
      "ai_keywords": [
        "Reinforcement learning (RL)",
        "large language models (LLMs)",
        "policy",
        "verifier",
        "reward model",
        "reward hacking",
        "supervised fine-tuning (SFT)",
        "generative",
        "process-level",
        "verification correctness rewards",
        "mutual reinforcement",
        "ProcessBench dataset",
        "mathematical reasoning"
      ]
    },
    "publishedAt": "2025-05-20T22:43:15.000Z",
    "title": "RL Tango: Reinforcing Generator and Verifier Together for Language\n  Reasoning",
    "summary": "Reinforcement learning (RL) has recently emerged as a compelling approach for\nenhancing the reasoning capabilities of large language models (LLMs), where an\nLLM generator serves as a policy guided by a verifier (reward model). However,\ncurrent RL post-training methods for LLMs typically use verifiers that are\nfixed (rule-based or frozen pretrained) or trained discriminatively via\nsupervised fine-tuning (SFT). Such designs are susceptible to reward hacking\nand generalize poorly beyond their training distributions. To overcome these\nlimitations, we propose Tango, a novel framework that uses RL to concurrently\ntrain both an LLM generator and a verifier in an interleaved manner. A central\ninnovation of Tango is its generative, process-level LLM verifier, which is\ntrained via RL and co-evolves with the generator. Importantly, the verifier is\ntrained solely based on outcome-level verification correctness rewards without\nrequiring explicit process-level annotations. This generative RL-trained\nverifier exhibits improved robustness and superior generalization compared to\ndeterministic or SFT-trained verifiers, fostering effective mutual\nreinforcement with the generator. Extensive experiments demonstrate that both\ncomponents of Tango achieve state-of-the-art results among 7B/8B-scale models:\nthe generator attains best-in-class performance across five competition-level\nmath benchmarks and four challenging out-of-domain reasoning tasks, while the\nverifier leads on the ProcessBench dataset. Remarkably, both components exhibit\nparticularly substantial improvements on the most difficult mathematical\nreasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15034.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6801fbe4cae7a6623f66ef64",
      "avatarUrl": "/avatars/383a72cc890946d89a185b8482d1b828.svg",
      "fullname": "Kaiwen Zha",
      "name": "sunshinekevin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14157",
      "authors": [
        {
          "_id": "682ee9d0498a5aaed2692828",
          "user": {
            "_id": "615313b0793ef66b3324da1f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg",
            "isPro": false,
            "fullname": "Pittawat Taveekitworachai",
            "user": "pittawat",
            "type": "user"
          },
          "name": "Pittawat Taveekitworachai",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-22T09:09:37.412Z",
          "hidden": false
        },
        {
          "_id": "682ee9d0498a5aaed2692829",
          "name": "Potsawee Manakul",
          "hidden": false
        },
        {
          "_id": "682ee9d0498a5aaed269282a",
          "name": "Sarana Nutanong",
          "hidden": false
        },
        {
          "_id": "682ee9d0498a5aaed269282b",
          "user": {
            "_id": "62d192c2d50433c35eb1b48e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d192c2d50433c35eb1b48e/VjmDu8GOIuLuQNBQdQLLS.png",
            "isPro": false,
            "fullname": "Kunat Pipatanakul",
            "user": "kunato",
            "type": "user"
          },
          "name": "Kunat Pipatanakul",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-22T09:09:37.412Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T10:05:11.000Z",
      "submittedOnDailyAt": "2025-05-22T07:40:05.896Z",
      "title": "Preproceso por Proconorfipa para la fortalecimiento y ajuste",
      "submittedOnDailyBy": {
        "_id": "615313b0793ef66b3324da1f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg",
        "isPro": false,
        "fullname": "Pittawat Taveekitworachai",
        "user": "pittawat",
        "type": "user"
      },
      "summary": "Este artículo investiga el primer pronóstico de ingeniería (pPE) en el contexto de la Regulación de Fortaleza (RFT). Los modelos de lenguaje (LMs) se benefician de que las acciones que maximizan el rendimiento se realicen según se les proporcionan señales de recompensa. Los estudios previos de RFT se centraron en algoritmos, formas de recompensa y filtrado de datos, pero no se investigó suficientemente el diseño del primer pronóstico, que consiste en instrucciones que se le dan antes de una pregunta adicional durante el aprendizaje. Se investiga cuál de las diferencias entre pPE puede hacer que los LMs realicen comportamientos diferentes después de ser sometidos a RFT. Se inspira en la Ingeniería de Pronóstico Inducida por la Razonamiento (iPE) para traducir cinco estrategias representativas de iPE—reasoning, planning, razonamiento basado en código, recuerdo de conocimiento, utilización de ejemplos nulos—en estrategias de pPE. Se utilizó Qwen2.5-7B para experimentar cada estrategia de pPE y evaluar su rendimiento en marcos de referencia in-domain y out-of-domain (por ejemplo, AIME2024, HumanEval+, GPQA-Diamond). Finalmente, todos los modelos de pPE entrenados mostraron mejores resultados que los modelos de iPE, y la estrategia de pPE con ejemplos nulos registró el mayor aumento en la rendimiento en AIME2024 y GPQA-Diamond. Además, se aplicó un marco de clasificación de comportamientos para mostrar que cada estrategia de pPE asigna estilos de comportamiento diferentes a los modelos resultantes. Estos hallazgos indican que pPE puede ser una fuerte contribución a la investigación de RFT.",
      "upvotes": 1,
      "discussionId": "682ee9d1498a5aaed2692895",
      "ai_summary": "Prior prompt engineering is investigated as a means to guide language models to internalize distinct behaviors through reinforcement fine-tuning, showing significant performance gains over inference-time prompt engineering.",
      "ai_keywords": [
        "prompt engineering",
        "reinforcement fine-tuning",
        "language models",
        "reward signals",
        "step-by-step reasoning",
        "Qwen2.5-7B",
        "AIME2024",
        "HumanEval+",
        "GPQA-Diamond",
        "behavior-classification framework"
      ]
    },
    "publishedAt": "2025-05-20T06:05:11.000Z",
    "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning",
    "summary": "This paper investigates prior prompt engineering (pPE) in the context of\nreinforcement fine-tuning (RFT), where language models (LMs) are incentivized\nto exhibit behaviors that maximize performance through reward signals. While\nexisting RFT research has primarily focused on algorithms, reward shaping, and\ndata curation, the design of the prior prompt--the instructions prepended to\nqueries during training to elicit behaviors such as step-by-step\nreasoning--remains underexplored. We investigate whether different pPE\napproaches can guide LMs to internalize distinct behaviors after RFT. Inspired\nby inference-time prompt engineering (iPE), we translate five representative\niPE strategies--reasoning, planning, code-based reasoning, knowledge recall,\nand null-example utilization--into corresponding pPE approaches. We experiment\nwith Qwen2.5-7B using each of the pPE approaches, then evaluate performance on\nin-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and\nGPQA-Diamond). Our results show that all pPE-trained models surpass their\niPE-prompted counterparts, with the null-example pPE approach achieving the\nlargest average performance gain and the highest improvement on AIME2024 and\nGPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by\nadapting a behavior-classification framework, we demonstrate that different pPE\nstrategies instill distinct behavioral styles in the resulting models. These\nfindings position pPE as a powerful yet understudied axis for RFT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14157.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "615313b0793ef66b3324da1f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg",
      "fullname": "Pittawat Taveekitworachai",
      "name": "pittawat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14990",
      "authors": [
        {
          "_id": "682eb25eb92a286a35d681fb",
          "name": "Ishika Agarwal",
          "hidden": false
        },
        {
          "_id": "682eb25eb92a286a35d681fc",
          "name": "Nimet Beyza Bozdag",
          "hidden": false
        },
        {
          "_id": "682eb25eb92a286a35d681fd",
          "name": "Dilek Hakkani-Tür",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T00:31:13.000Z",
      "submittedOnDailyAt": "2025-05-22T03:43:27.386Z",
      "title": "¿El conocimiento propio del lenguaje: el modelo sabe mejor inglés que en X?",
      "submittedOnDailyBy": {
        "_id": "6391e4e984afa726d66180b9",
        "avatarUrl": "/avatars/e437e2820745b522a868b8da27d9a11f.svg",
        "isPro": false,
        "fullname": "Ishika Agarwal",
        "user": "ishikaa",
        "type": "user"
      },
      "summary": "El código switch es un fenómeno en el que una palabra, una idea o una transformación se intercambian entre diferentes lenguajes. Realizamos código switch cuando se siente más libre y cómodo hablar sobre un tema o en un campo específico en un solo lenguaje. Con el aumento de los modelos de lenguaje con alta densidad de conocimiento, pensamos en los siguientes problemas naturales: ¿puede un modelo tener más conocimiento en un lenguaje X específico en un tema determinado? Lo más importante es saber si puede realizar inferencias cambiando el lenguaje para mejorar la eficiencia de la inferencia. Este fenómeno se expresa con la terminología de \"Lenguaje Específico de Sabiduría\" (LSK). Muchas culturas desarrollan con diversas lenguajes, por lo tanto, utilizan conjuntos de datos culturales (que incluyen conocimientos sobre normas culturales y sociales). Al usar la inferencia en cadena de obras en lenguajes que no son inglés, se ha observado que los modelos de lenguaje producen mejores resultados. Además, se ha demostrado que pueden diferir en semejanza de significado y expresión, asumiendo que ciertos conocimientos pueden estar limitados a un lenguaje \"específico\". A partir de los resultados iniciales, se diseñó un método sencillo para detectar el conocimiento específico del lenguaje, llamado LSKExtractor. Este módulo se utiliza para aplicar el conocimiento en la inferencia. La precisión promedio se ha mejorado en más del 10% con diferentes tipos de modelos y conjuntos de datos. Nuestro estudio contribuye al desarrollo abierto de modelos de lenguaje y permite que los modelos incluyan más culturas y contextos lingüísticos según su uso.",
      "upvotes": 0,
      "discussionId": "682eb25fb92a286a35d6822d",
      "githubRepo": "https://github.com/agarwalishika/LSKExtractor",
      "ai_summary": "Models perform better in reasoning and accuracy when using language-specific knowledge and chain-of-thought reasoning in certain languages, including low-resource ones, compared to others.",
      "ai_keywords": [
        "code-switching",
        "Language Specific Knowledge (LSK)",
        "culture-specific datasets",
        "chain-of-thought reasoning",
        "culturally specific texts",
        "LSKExtractor"
      ]
    },
    "publishedAt": "2025-05-20T20:31:13.000Z",
    "title": "Language Specific Knowledge: Do Models Know Better in X than in English?",
    "summary": "Code-switching is a common phenomenon of alternating between different\nlanguages in the same utterance, thought, or conversation. We posit that humans\ncode-switch because they feel more comfortable talking about certain topics and\ndomains in one language than another. With the rise of knowledge-intensive\nlanguage models, we ask ourselves the next, natural question: Could models hold\nmore knowledge on some topics in some language X? More importantly, could we\nimprove reasoning by changing the language that reasoning is performed in? We\ncoin the term Language Specific Knowledge (LSK) to represent this phenomenon.\nAs ethnic cultures tend to develop alongside different languages, we employ\nculture-specific datasets (that contain knowledge about cultural and social\nbehavioral norms). We find that language models can perform better when using\nchain-of-thought reasoning in some languages other than English, sometimes even\nbetter in low-resource languages. Paired with previous works showing that\nsemantic similarity does not equate to representational similarity, we\nhypothesize that culturally specific texts occur more abundantly in\ncorresponding languages, enabling specific knowledge to occur only in specific\n\"expert\" languages. Motivated by our initial results, we design a simple\nmethodology called LSKExtractor to benchmark the language-specific knowledge\npresent in a language model and, then, exploit it during inference. We show our\nresults on various models and datasets, showing an average relative improvement\nof 10% in accuracy. Our research contributes to the open-source development of\nlanguage models that are inclusive and more aligned with the cultural and\nlinguistic contexts in which they are deployed.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6391e4e984afa726d66180b9",
      "avatarUrl": "/avatars/e437e2820745b522a868b8da27d9a11f.svg",
      "fullname": "Ishika Agarwal",
      "name": "ishikaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0
    },
    "isAuthorParticipating": false
  }
]