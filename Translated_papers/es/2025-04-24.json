[
  {
    "paper": {
      "id": "2504.15279",
      "authors": [
        {
          "_id": "68070d3b5035e6d88636ae13",
          "user": {
            "_id": "649e5ee29420f68cf1c1470e",
            "avatarUrl": "/avatars/7f6d1ec4fb3f85351e88044016d8ab42.svg",
            "isPro": false,
            "fullname": "Xu Wayen",
            "user": "wilye",
            "type": "user"
          },
          "name": "Weiye Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:11:19.332Z",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae14",
          "user": {
            "_id": "664b4a748dd1bfb5a3a970fe",
            "avatarUrl": "/avatars/37aa9332ab3e8fbb6ae30b875a7e0e5a.svg",
            "isPro": false,
            "fullname": "Jiahao Wang",
            "user": "GenuineWWD",
            "type": "user"
          },
          "name": "Jiahao Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:11:17.358Z",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae15",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae16",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae17",
          "name": "Wengang Zhou",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae18",
          "name": "Aijun Yang",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae19",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1a",
          "name": "Houqiang Li",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1b",
          "name": "Xiaohua Wang",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1c",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1d",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1e",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1f",
          "name": "Jinguo Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:59:53.000Z",
      "submittedOnDailyAt": "2025-04-24T01:22:54.990Z",
      "title": "Business Logic: Benchmark for Reasoning in Visualization of Dialogue Models",
      "submittedOnDailyBy": {
        "_id": "664b4a748dd1bfb5a3a970fe",
        "avatarUrl": "/avatars/37aa9332ab3e8fbb6ae30b875a7e0e5a.svg",
        "isPro": false,
        "fullname": "Jiahao Wang",
        "user": "GenuineWWD",
        "type": "user"
      },
      "summary": "La inferencia visual es un elemento fundamental del inteligencia humana y una capacidad importante para modelos de alto nivel multimodal. Sin embargo, la evaluación de la inferencia en los modelos de lenguaje multimodal (MLLMs) actuales se basa generalmente en descripciones de oraciones, permitiendo espacios de inferencia basados en lenguaje y no mediendo de manera efectiva la inferencia centrada en la visión. Para abordar este problema, presentamos VisuLogic: una prueba de 1,000 preguntas humanamente validadas en 6 categorías (por ejemplo, movimiento de números, relaciones espaciales, comparación de características). Estas preguntas de diferentes clases permiten evaluar desde diferentes perspectivas la capacidad de inferencia visual de los MLLMs. En este marco de referencia, evaluamos los modelos avanzados, analizamos los resultados y identificamos patrones comunes de fracaso. Muchos modelos no superan la precisión del 30%, lo que es ligeramente superior a la línea basada en el azar del 25% y significativamente menor que el 51.4% de los resultados humanos, lo que claramente muestra errores significativos en la inferencia visual. Además, proporcionamos datos de entrenamiento adicionales y líneas de aprendizaje reforzados para fomentar la mejora.",
      "upvotes": 46,
      "discussionId": "68070d3f5035e6d88636af56",
      "projectPage": "https://visulogic-benchmark.github.io/VisuLogic/",
      "githubRepo": "https://github.com/VisuLogic-Benchmark/VisuLogic-Eval",
      "ai_keywords": [
        "Visual reasoning",
        "multimodal large language models (MLLMs)",
        "text descriptions",
        "language-based reasoning shortcuts",
        "genuine vision-centric reasoning",
        "VisuLogic",
        "human-verified problems",
        "quantitative shifts",
        "spatial relations",
        "attribute comparisons",
        "supplementary training dataset",
        "reinforcement-learning baseline"
      ]
    },
    "publishedAt": "2025-04-21T13:59:53.000Z",
    "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models",
    "summary": "Visual reasoning is a core component of human intelligence and a critical\ncapability for advanced multimodal models. Yet current reasoning evaluations of\nmultimodal large language models (MLLMs) often rely on text descriptions and\nallow language-based reasoning shortcuts, failing to measure genuine\nvision-centric reasoning. To address this, we introduce VisuLogic: a benchmark\nof 1,000 human-verified problems across six categories (e.g., quantitative\nshifts, spatial relations, attribute comparisons). These various types of\nquestions can be evaluated to assess the visual reasoning capabilities of MLLMs\nfrom multiple perspectives. We evaluate leading MLLMs on this benchmark and\nanalyze their results to identify common failure modes. Most models score below\n30% accuracy-only slightly above the 25% random baseline and far below the\n51.4% achieved by humans-revealing significant gaps in visual reasoning.\nFurthermore, we provide a supplementary training dataset and a\nreinforcement-learning baseline to support further progress.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "664b4a748dd1bfb5a3a970fe",
      "avatarUrl": "/avatars/37aa9332ab3e8fbb6ae30b875a7e0e5a.svg",
      "fullname": "Jiahao Wang",
      "name": "GenuineWWD",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14509",
      "authors": [
        {
          "_id": "6809dd092e04f68a3f5baa66",
          "name": "Fulong Ye",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa67",
          "name": "Miao Hua",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa68",
          "name": "Pengze Zhang",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa69",
          "name": "Xinghui Li",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa6a",
          "name": "Qichao Sun",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa6b",
          "name": "Songtao Zhao",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa6c",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa6d",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-20T06:53:00.000Z",
      "submittedOnDailyAt": "2025-04-24T05:26:05.811Z",
      "title": "Drimid: Aproximación de alta calidad y velocidad en la intercambio de rostros basado en difusión mediante el aprendizaje de grupos de tipos de ID triples",
      "submittedOnDailyBy": {
        "_id": "6339029a76421c0543167075",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6339029a76421c0543167075/3npT0NxTMV-MLWDThyBy8.png",
        "isPro": false,
        "fullname": "fulong ye",
        "user": "Alon77777",
        "type": "user"
      },
      "summary": "En este artículo se presenta el modelo de intercambio de caras basado en difusión llamado \"DreamID\", diseñado para lograr similitudes de ID, preservación de características, precisión de imágenes y velocidad de inferencia rápida. A diferencia de los procesos de entrenamiento comunes de intercambio de caras, dependen de un sub-procesamiento oculto y es más difícil lograr resultados satisfactorios. DreamID construye el Dataset de Grupos de ID Triplet para realizar un sub-procesamiento explícito y mejora significativamente la similitud de caras y la preservación de características. La naturaleza recurrente del modelo de difusión presenta problemas en el uso eficiente de la función de pérdida en el espacio de imágenes y es complicado obtener imágenes generadas durante el entrenamiento con múltiples pasos prácticos. Para resolver estos problemas, se utiliza un modelo de difusión mejorado llamado SD Turbo, lo que reduce la fase de inferencia a un solo paso y permite un entrenamiento eficiente a nivel de píxeles asociado con el sub-procesamiento explícito de los Grupos de ID Triplet. Además, se propone una arquitectura de modelo basada en difusión mejorada integrando SwapNet, FaceNet y ID Adapter, que libera completamente el potencial del sub-procesamiento explícito de los Grupos de ID Triplet. Finalmente, para la expansión del método, se cambian explícitamente los datos de los Grupos de ID Triplet durante el entrenamiento y se ajustan específicamente características como el lente o el forma del rostro para preservarlas. Los experimentos expandidos superan los métodos más avanzados en similitudes de ID, posiciones y expresiones, y precisión de imágenes. En general, DreamID logra resultados de intercambio de caras de alta calidad en una resolución de 512*512 en menos de 0.6 segundos, mostrando una excelente funcionalidad incluso en escenarios difíciles como iluminación compleja, grandes ángulos y obstrucciones.",
      "upvotes": 26,
      "discussionId": "6809dd102e04f68a3f5babf5",
      "projectPage": "https://superhero-7.github.io/DreamID/",
      "githubRepo": "https://github.com/superhero-7/DreamID",
      "ai_keywords": [
        "diffusion-based model",
        "Triplet ID Group",
        "diffusion models",
        "image-space loss functions",
        "SD Turbo",
        "SwapNet",
        "FaceNet",
        "ID Adapter",
        "face swapping",
        "explicit supervision",
        "identity similarity",
        "attribute preservation",
        "image fidelity",
        "pose preservation",
        "expression preservation",
        "high-quality face swapping"
      ]
    },
    "publishedAt": "2025-04-20T02:53:00.000Z",
    "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via\n  Triplet ID Group Learning",
    "summary": "In this paper, we introduce DreamID, a diffusion-based face swapping model\nthat achieves high levels of ID similarity, attribute preservation, image\nfidelity, and fast inference speed. Unlike the typical face swapping training\nprocess, which often relies on implicit supervision and struggles to achieve\nsatisfactory results. DreamID establishes explicit supervision for face\nswapping by constructing Triplet ID Group data, significantly enhancing\nidentity similarity and attribute preservation. The iterative nature of\ndiffusion models poses challenges for utilizing efficient image-space loss\nfunctions, as performing time-consuming multi-step sampling to obtain the\ngenerated image during training is impractical. To address this issue, we\nleverage the accelerated diffusion model SD Turbo, reducing the inference steps\nto a single iteration, enabling efficient pixel-level end-to-end training with\nexplicit Triplet ID Group supervision. Additionally, we propose an improved\ndiffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.\nThis robust architecture fully unlocks the power of the Triplet ID Group\nexplicit supervision. Finally, to further extend our method, we explicitly\nmodify the Triplet ID Group data during training to fine-tune and preserve\nspecific attributes, such as glasses and face shape. Extensive experiments\ndemonstrate that DreamID outperforms state-of-the-art methods in terms of\nidentity similarity, pose and expression preservation, and image fidelity.\nOverall, DreamID achieves high-quality face swapping results at 512*512\nresolution in just 0.6 seconds and performs exceptionally well in challenging\nscenarios such as complex lighting, large angles, and occlusions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14509.png",
    "numComments": 7,
    "submittedBy": {
      "_id": "6339029a76421c0543167075",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6339029a76421c0543167075/3npT0NxTMV-MLWDThyBy8.png",
      "fullname": "fulong ye",
      "name": "Alon77777",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15431",
      "authors": [
        {
          "_id": "680879ead6dc8bf64565c975",
          "name": "Sungjun Han",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c976",
          "user": {
            "_id": "6138cc1306dd10833d2db64b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6138cc1306dd10833d2db64b/IRX4y-8M4YlzR_8jOwkKp.jpeg",
            "isPro": false,
            "fullname": "Juyoung Suk",
            "user": "scottsuk0306",
            "type": "user"
          },
          "name": "Juyoung Suk",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:08:21.257Z",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c977",
          "name": "Suyeong An",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c978",
          "name": "Hyungguk Kim",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c979",
          "name": "Kyuseok Kim",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c97a",
          "name": "Wonsuk Yang",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c97b",
          "user": {
            "_id": "6257adfdb98dcaa7e0de7ab4",
            "avatarUrl": "/avatars/ddfc2135104895d09cfce0cd6f10e5fb.svg",
            "isPro": false,
            "fullname": "Seungtaek Choi",
            "user": "hist0613",
            "type": "user"
          },
          "name": "Seungtaek Choi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:11:03.864Z",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c97c",
          "name": "Jamin Shin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T20:54:44.000Z",
      "submittedOnDailyAt": "2025-04-24T01:09:32.264Z",
      "title": "InternLM (书生·浦语) 翻译如下：\n\n**Reporte Técnico de Trillian 7B**\n\n**Translation:**\n\n**Reporte Técnico de Trillian 7B**\n\n**Note:** No se ha añadido ningún comentario o texto adicional.",
      "submittedOnDailyBy": {
        "_id": "6138cc1306dd10833d2db64b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6138cc1306dd10833d2db64b/IRX4y-8M4YlzR_8jOwkKp.jpeg",
        "isPro": false,
        "fullname": "Juyoung Suk",
        "user": "scottsuk0306",
        "type": "user"
      },
      "summary": "Esta traducción fue realizada al español.\n\nSi se requiere más información o una traducción en otro idioma, por favor, no dude en informar.",
      "upvotes": 18,
      "discussionId": "680879ebd6dc8bf64565c9bb",
      "ai_keywords": [
        "Trillion-7B",
        "Cross-lingual Document Attention (XLDA)",
        "language-specific filtering",
        "tailored tokenizer construction",
        "multilingual data",
        "multilingual performance",
        "cross-lingual consistency"
      ]
    },
    "publishedAt": "2025-04-21T16:54:44.000Z",
    "title": "Trillion 7B Technical Report",
    "summary": "We introduce Trillion-7B, the most token-efficient Korean-centric\nmultilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)\nmechanism enables highly efficient and effective knowledge transfer from\nEnglish to target languages like Korean and Japanese. Combined with optimized\ndata mixtures, language-specific filtering, and tailored tokenizer\nconstruction, Trillion-7B achieves competitive performance while dedicating\nonly 10\\% of its 2T training tokens to multilingual data and requiring just\n59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations\nacross 27 benchmarks in four languages demonstrate Trillion-7B's robust\nmultilingual performance and exceptional cross-lingual consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15431.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6138cc1306dd10833d2db64b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6138cc1306dd10833d2db64b/IRX4y-8M4YlzR_8jOwkKp.jpeg",
      "fullname": "Juyoung Suk",
      "name": "scottsuk0306",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15843",
      "authors": [
        {
          "_id": "6809948944114def75aaeb7d",
          "name": "Junshu Pan",
          "hidden": false
        },
        {
          "_id": "6809948944114def75aaeb7e",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6809948944114def75aaeb7f",
          "name": "Shulin Huang",
          "hidden": false
        },
        {
          "_id": "6809948944114def75aaeb80",
          "name": "Qiji Zhou",
          "hidden": false
        },
        {
          "_id": "6809948944114def75aaeb81",
          "name": "Yue Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T12:39:30.000Z",
      "submittedOnDailyAt": "2025-04-24T00:02:36.679Z",
      "title": "DPO Anterior: Mejora en el uso de datos para optimización preferencial directa utilizando el Modelo de Guía de Normas",
      "submittedOnDailyBy": {
        "_id": "6468823272d9180d4ac90bdf",
        "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
        "isPro": false,
        "fullname": "Wei Shen",
        "user": "Swtheking",
        "type": "user"
      },
      "summary": "Direct Preference Optimization (DPO) simplifies the reinforcement learning (RLHF) of large language models (LLMs) by directly addressing human feedback. During the DPO training process, the reference model acts as a weighting adjuster for the data. However, initializing the policy model and reference model identically in DPO hinders the efficient use of data and exposes performance limitations. In contrast, Simple Preference Optimization (SimPO) trains without a reference model, leading to instability in learning and requiring strict conditions to prevent cognitive forgetting. This paper proposes an effective learning paradigm based on DPO, called \"Pre-DPO,\" which utilizes a Guiding Reference Model. This reference model predicts the optimal policy state from training feedback data and provides guidance on assigning higher weights to data suitable for the model and lower weights to data unsuitable for the model. Extended experiments on the AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks show that Pre-DPO consistently improves the performance of both DPO and SimPO and operates effectively without relying on external models or additional data.",
      "upvotes": 12,
      "discussionId": "6809948a44114def75aaebab",
      "ai_keywords": [
        "reinforcement learning from human feedback (RLHF)",
        "large language models (LLMs)",
        "Direct Preference Optimization (DPO)",
        "human preferences",
        "reference model",
        "data weight adjuster",
        "Simple Preference Optimization (SimPO)",
        "catastrophic forgetting",
        "Pre-DPO",
        "guiding reference model",
        "AlpacaEval 2.0",
        "Arena-Hard v0.1"
      ]
    },
    "publishedAt": "2025-04-22T08:39:30.000Z",
    "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
    "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15843.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6468823272d9180d4ac90bdf",
      "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
      "fullname": "Wei Shen",
      "name": "Swtheking",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16801",
      "authors": [
        {
          "_id": "6809bebd0f6dfd7bd5159b76",
          "user": {
            "_id": "6545f8922a2a483042ebc8b3",
            "avatarUrl": "/avatars/ab00da8aa841694f3f11093a9148e4c5.svg",
            "isPro": false,
            "fullname": "xiaoxing2001",
            "user": "xiaoxing2001",
            "type": "user"
          },
          "name": "Xiaoxing Hu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-24T04:32:01.063Z",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b77",
          "user": {
            "_id": "63e202f352b7578dba448ab5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
            "isPro": false,
            "fullname": "Yang",
            "user": "Kaichengalex",
            "type": "user"
          },
          "name": "Kaicheng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:10:22.302Z",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b78",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b79",
          "name": "Haoran Xu",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b7a",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b7b",
          "name": "Yupei Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T15:20:53.000Z",
      "submittedOnDailyAt": "2025-04-24T03:04:11.389Z",
      "title": "Mejorar la comprensión integrada a través de la cooperación a nivel global.",
      "submittedOnDailyBy": {
        "_id": "63e202f352b7578dba448ab5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
        "isPro": false,
        "fullname": "Yang",
        "user": "Kaichengalex",
        "type": "user"
      },
      "summary": "Contrastive Language-Image Pre-training (CLIP) ha ajustado modelos de imágenes y texto para realizar exitosamente varias tareas multidimensionales. Sin embargo, debido a las características del aprendizaje de contraste global, CLIP no puede comprender conceptos estructurales (por ejemplo, relaciones y características). Recientes investigaciones tratan de mejorar la comprensión estructural utilizando muestras negativas difíciles de aprendizaje global, pero estos métodos imponen grandes pérdidas en la capacidad general del modelo, reduciendo su espacio de embbeding de imágenes y texto. Para superar estas limitaciones, se introduce el marco de trabajo DeGLA (Decoupled Global-Local Alignment). Este marco tiene como objetivo mejorar la comprensión estructural mientras evita grandes pérdidas de su capacidad general. Para optimizar el modelo sin perder sus habilidades propias, se introduce una estructura de auto-desvanecimiento durante el proceso de ajuste global y se ajustan los encoders de imágenes y texto a partir del movimiento ponderado. Bajo la restricción de la auto-desvanecimiento, se inhibe efectivamente la \"falsa olvido\" de los conocimientos pre-entrenados. Para mejorar la comprensión estructural, se expanden las capacidades de aprendizaje de contexto de los Grandes Modelos de Lenguaje (LLMs) para construir aproximadamente 2M capturas de calidad alta en cinco categorías. Además, se proponen el pérdida de Contraste de Imágenes Sostenidas (IGC) y el Pérdida de Contraste de Texto Sostenido (TGC) para fortalecer la comprensión estructural del lenguaje visual. Los resultados de experimentos ampliados muestran el efecto del marco de trabajo DeGLA. Comparados con los métodos más recientes, se alcanzan un aumento promedio de efectividad del 3.5% en las marcas VALSE, SugarCrepe y ARO, y un aumento promedio de rendimiento del 13.0% en las tareas de clasificación sin ejemplos en 11 conjuntos de datos. Nuestro código está disponible en https://github.com/xiaoxing2001/DeGLA.",
      "upvotes": 11,
      "discussionId": "6809bec10f6dfd7bd5159c38",
      "ai_keywords": [
        "Decoupled Global-Local Alignment (DeGLA)",
        "self-distillation mechanism",
        "learnable image-text encoder",
        "frozen teacher model",
        "exponential moving average",
        "catastrophic forgetting",
        "in-context learning",
        "Large Language Models (LLMs)",
        "high-quality negative captions",
        "Image-Grounded Contrast (IGC) loss",
        "Text-Grounded Contrast (TGC) loss",
        "vision-language compositionally",
        "VALSE",
        "SugarCrepe",
        "ARO benchmarks",
        "zero-shot classification tasks"
      ]
    },
    "publishedAt": "2025-04-23T11:20:53.000Z",
    "title": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding",
    "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16801.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16929",
      "authors": [
        {
          "_id": "6809ba7976a4f4f7268546a7",
          "name": "Shaden Alshammari",
          "hidden": false
        },
        {
          "_id": "6809ba7976a4f4f7268546a8",
          "name": "John Hershey",
          "hidden": false
        },
        {
          "_id": "6809ba7976a4f4f7268546a9",
          "name": "Axel Feldmann",
          "hidden": false
        },
        {
          "_id": "6809ba7976a4f4f7268546aa",
          "name": "William T. Freeman",
          "hidden": false
        },
        {
          "_id": "6809ba7976a4f4f7268546ab",
          "name": "Mark Hamilton",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62dae3734398e21bf7f53443/MT5eBEJkF1u7uejE382FI.png"
      ],
      "publishedAt": "2025-04-23T17:59:01.000Z",
      "submittedOnDailyAt": "2025-04-24T02:45:09.509Z",
      "title": "I-Con: Marco de Aprendizaje de Representaciones Integrado",
      "submittedOnDailyBy": {
        "_id": "62dae3734398e21bf7f53443",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658512237806-noauth.jpeg",
        "isPro": false,
        "fullname": "Mark Hamilton",
        "user": "mhamilton723",
        "type": "user"
      },
      "summary": "Los campos de aprendizaje están expandiendo a la vez que se incrementan funciones de pérdida para diferentes tipos de problemas. Presentamos una ecuación teórica para generalizar un gran conjunto de funciones de pérdida modernas de aprendizaje automático. En particular, describimos métodos para minimizar la divergencia KL acumulada de dos distribuciones condicionales (SUB-VIEW y representación aprendida) en una amplia clase de aprendizaje automático. Desde este punto de vista, descubrimos clustering, métodos espectrales, reducción de dimensiones, aprendizaje comparativo y la geometría de información oculta bajo aprendizaje supervisado. Este marco permite la combinación de técnicas exitosas en la literatura para desarrollar nuevas funciones de pérdida. Presentamos pruebas que conectan más de 23 enfoques, pero también demostramos que, utilizando estos resultados teóricos, se puede lograr un aumento del 8% sobre el mejor nivel anterior en la clasificación sin SUB-VIEW en ImageNet-1K y crear un cluster de imágenes sin SUB-VIEW. Además, mostramos cómo I-Con mejora el principio de aprendizaje de representaciones comparativas utilizando un método de tratamiento de base de datos fundamental.",
      "upvotes": 9,
      "discussionId": "6809ba7d76a4f4f72685478a",
      "ai_keywords": [
        "information-theoretic equation",
        "KL divergence",
        "conditional distributions",
        "supervisory representations",
        "learned representations",
        "information geometry",
        "clustering",
        "spectral methods",
        "dimensionality reduction",
        "contrastive learning",
        "supervised learning",
        "I-Con",
        "debiasing methods",
        "contrastive representation learners"
      ]
    },
    "publishedAt": "2025-04-23T13:59:01.000Z",
    "title": "I-Con: A Unifying Framework for Representation Learning",
    "summary": "As the field of representation learning grows, there has been a proliferation\nof different loss functions to solve different classes of problems. We\nintroduce a single information-theoretic equation that generalizes a large\ncollection of modern loss functions in machine learning. In particular, we\nintroduce a framework that shows that several broad classes of machine learning\nmethods are precisely minimizing an integrated KL divergence between two\nconditional distributions: the supervisory and learned representations. This\nviewpoint exposes a hidden information geometry underlying clustering, spectral\nmethods, dimensionality reduction, contrastive learning, and supervised\nlearning. This framework enables the development of new loss functions by\ncombining successful techniques from across the literature. We not only present\na wide array of proofs, connecting over 23 different approaches, but we also\nleverage these theoretical results to create state-of-the-art unsupervised\nimage classifiers that achieve a +8% improvement over the prior\nstate-of-the-art on unsupervised classification on ImageNet-1K. We also\ndemonstrate that I-Con can be used to derive principled debiasing methods which\nimprove contrastive representation learners.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62dae3734398e21bf7f53443/MT5eBEJkF1u7uejE382FI.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16929.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62dae3734398e21bf7f53443",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658512237806-noauth.jpeg",
      "fullname": "Mark Hamilton",
      "name": "mhamilton723",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16915",
      "authors": [
        {
          "_id": "6809b14111003e54bd204d99",
          "name": "Chong Mou",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9a",
          "user": {
            "_id": "639709c2be8a14bb9eeea8f6",
            "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
            "isPro": false,
            "fullname": "Yanze Wu",
            "user": "yanze",
            "type": "user"
          },
          "name": "Yanze Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:10:24.617Z",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9b",
          "name": "Wenxu Wu",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9c",
          "name": "Zinan Guo",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9d",
          "name": "Pengze Zhang",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9e",
          "name": "Yufeng Cheng",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9f",
          "name": "Yiming Luo",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da0",
          "name": "Fei Ding",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da1",
          "name": "Shiwen Zhang",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da2",
          "name": "Xinghui Li",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da3",
          "name": "Mengtian Li",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da4",
          "name": "Songtao Zhao",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da5",
          "name": "Jian Zhang",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da6",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da7",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T17:41:44.000Z",
      "submittedOnDailyAt": "2025-04-24T02:18:39.286Z",
      "title": "DreamO: Una referencia de la arquitectura de marco de usuarios definidos para imágenes",
      "submittedOnDailyBy": {
        "_id": "639709c2be8a14bb9eeea8f6",
        "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
        "isPro": false,
        "fullname": "Yanze Wu",
        "user": "yanze",
        "type": "user"
      },
      "summary": "Recientemente, la investigación de la personalización de imágenes (por ejemplo, identidad, tema, estilo, fondo, etc.) ha demostrado una poderosa capacidad de personalización en modelos de generación a gran escala. Sin embargo, muchos de los métodos de acceso están diseñados para tareas específicas y limitan la capacidad de integrar diferentes tipos de condiciones. El desarrollo de una serie de marcos de trabajo para la personalización de imágenes sigue siendo un desafío abierto. En este artículo, se presenta un marco de trabajo para la personalización de imágenes que apoya diversas tareas y promueve la integración de múltiples condiciones sin restricciones. Específicamente, DreamO utiliza el marco de trabajo de transformador de difusión (DiT) para procesar de manera consistente diferentes tipos de entradas. Durante el período de entrenamiento, se construye un grande conjunto de datos de entrenamiento que incluye diversas tareas de personalización, y se introduce la restricción de rotación de características para combinar información relacionada con imágenes de referencia. Además, se diseña un plan de etapas para asociar post-procesamientos específicos y condiciones, lo que permite controlar la distribución de condiciones en los datos generados. Además, se introduce una etapa de entrenamiento en tres etapas para corregir el sesgo de calidad causado por datos de baja calidad. A través de una amplia gama de experimentos, se demuestra que el propuesto DreamO permite la integración flexible de alta calidad y efectivamente realiza diversas tareas de personalización de imágenes con diferentes tipos de condiciones de control.",
      "upvotes": 7,
      "discussionId": "6809b14411003e54bd204e51",
      "projectPage": "https://mc-e.github.io/project/DreamO/",
      "githubRepo": "https://github.com/bytedance/DreamO",
      "ai_keywords": [
        "diffusion transformer (DiT)",
        "feature routing constraint",
        "placeholder strategy",
        "progressive training strategy",
        "baseline consistency",
        "customization capabilities",
        "quality alignment stage"
      ]
    },
    "publishedAt": "2025-04-23T13:41:44.000Z",
    "title": "DreamO: A Unified Framework for Image Customization",
    "summary": "Recently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce a feature routing constraint to\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design a placeholder strategy that associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ a\nprogressive training strategy consisting of three stages: an initial stage\nfocused on simple tasks with limited data to establish baseline consistency, a\nfull-scale training stage to comprehensively enhance the customization\ncapabilities, and a final quality alignment stage to correct quality biases\nintroduced by low-quality data. Extensive experiments demonstrate that the\nproposed DreamO can effectively perform various image customization tasks with\nhigh quality and flexibly integrate different types of control conditions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16915.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639709c2be8a14bb9eeea8f6",
      "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
      "fullname": "Yanze Wu",
      "name": "yanze",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 139
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15585",
      "authors": [
        {
          "_id": "6809c1f389b7cade55b32a6c",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a6d",
          "name": "Guibin Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a6e",
          "name": "Zhenhong Zhou",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a6f",
          "name": "Jiahao Wu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a70",
          "name": "Miao Yu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a71",
          "name": "Shiqian Zhao",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a72",
          "name": "Chenlong Yin",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a73",
          "name": "Jinhu Fu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a74",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a75",
          "name": "Hanjun Luo",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a76",
          "name": "Liang Lin",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a77",
          "name": "Zhihao Xu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a78",
          "name": "Haolang Lu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a79",
          "name": "Xinye Cao",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7a",
          "name": "Xinyun Zhou",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7b",
          "name": "Weifei Jin",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7c",
          "name": "Fanci Meng",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7d",
          "name": "Junyuan Mao",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7e",
          "name": "Hao Wu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7f",
          "name": "Minghe Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a80",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a81",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a82",
          "name": "Chengwei Liu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a83",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a84",
          "name": "Qiankun Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a85",
          "name": "Chongye Guo",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a86",
          "name": "Yalan Qin",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a87",
          "name": "Yi Ding",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a88",
          "name": "Donghai Hong",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a89",
          "name": "Jiaming Ji",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8a",
          "name": "Xinfeng Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8b",
          "name": "Yifan Jiang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8c",
          "name": "Dongxia Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8d",
          "name": "Yihao Huang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8e",
          "name": "Yufei Guo",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8f",
          "name": "Jen-tse Huang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a90",
          "name": "Yanwei Yue",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a91",
          "name": "Wenke Huang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a92",
          "name": "Guancheng Wan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a93",
          "name": "Tianlin Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a94",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a95",
          "name": "Jie Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a96",
          "name": "Qing Guo",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a97",
          "name": "Jingyi Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a98",
          "name": "Tianlong Chen",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a99",
          "name": "Joey Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9a",
          "name": "Xiaojun Jia",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9b",
          "name": "Weisong Sun",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9c",
          "name": "Cong Wu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9d",
          "name": "Jing Chen",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9e",
          "name": "Xuming Hu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9f",
          "name": "Yiming Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa0",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa1",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:10:18.968Z",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa2",
          "name": "Luu Anh Tuan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa3",
          "name": "Guowen Xu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa4",
          "name": "Tianwei Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa5",
          "name": "Xingjun Ma",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa6",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa7",
          "name": "Bo An",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa8",
          "name": "Jun Sun",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa9",
          "name": "Mohit Bansal",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aaa",
          "name": "Shirui Pan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aab",
          "name": "Yuval Elovici",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aac",
          "name": "Bhavya Kailkhura",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aad",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aae",
          "name": "Yaodong Yang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aaf",
          "name": "Hongwei Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab0",
          "name": "Wenyuan Xu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab1",
          "name": "Yizhou Sun",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab2",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab3",
          "name": "Qing Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab4",
          "name": "Ke Tang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab5",
          "name": "Yu-Gang Jiang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab6",
          "name": "Felix Juefei-Xu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab7",
          "name": "Hui Xiong",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab8",
          "name": "Xiaofeng Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab9",
          "name": "Shuicheng Yan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aba",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32abb",
          "name": "Philip S. Yu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32abc",
          "name": "Qingsong Wen",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32abd",
          "name": "Yang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T05:02:49.000Z",
      "submittedOnDailyAt": "2025-04-24T03:15:54.692Z",
      "title": "Investigación Integral de la Seguridad de LLM(-Agent) Full Stack: Datos, Entrenamiento, Implementación",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "El éxito sorprendente de los LLMs ha brillado como una ruta de esperanza para la realización de la AGI en la comunidad académica e industrial. Los LLMs han demostrado resultados sin precedentes en diversas áreas de aplicación. La importancia de los LLMs ha aumentado en ambos los campos de investigación y comercio, y su seguridad y influencia han aumentado también en los investigadores, las empresas y los países. Actualmente, las investigaciones sobre la seguridad de los LLMs se centran en etapas específicas (por ejemplo, la etapa de implementación o de fine-tuning), lo que resulta insuficiente para entender la \"cadena de vida\" completa de los LLMs. Para remediar esto, este artículo presenta por primera vez el concepto de seguridad \"FULL STACK\" que abarca todo el proceso desde el aprendizaje hasta la finalización comercial de los LLMs. Comparado con las investigaciones de seguridad de los LLMs actuales, nuestro trabajo destaca las siguientes ventajas características: (I) Visión rigurosa. La \"cadena de vida\" completa de los LLMs se extiende desde la preparación de datos, preprocesamiento, postprocesamiento, implementación hasta la finalización comercial, lo que permite la primera investigación de seguridad que incluye toda la \"cadena de vida\" de los LLMs. (II) Soporte amplio de la literatura. Nuestro trabajo se basa en una revisión rigurosa de más de 800 artículos, lo que garantiza una cobertura integral y organización sistemática de los problemas de seguridad. (III) Visión propia. A través de un análisis sistemático de la literatura, hemos desarrollado mapas y vistas confiables para cada capítulo. Nuestro trabajo identifica direcciones de investigación prometedoras en la seguridad de la generación de datos, tecnologías de alineamiento, edición de modelos y sistemas de AGENTS basados en LLMs. Estas visiones son una guía valiosa para los investigadores que buscan explorar el futuro de esta área de investigación.",
      "upvotes": 5,
      "discussionId": "6809c1f789b7cade55b32bf4"
    },
    "publishedAt": "2025-04-22T01:02:49.000Z",
    "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training\n  and Deployment",
    "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15585.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15777",
      "authors": [
        {
          "_id": "6808452d16c1c427ac727816",
          "user": {
            "_id": "67469d6a8407f929491dce06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
            "isPro": true,
            "fullname": "Shangshang Wang",
            "user": "upup-ashton-wang",
            "type": "user"
          },
          "name": "Shangshang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:11:10.259Z",
          "hidden": false
        },
        {
          "_id": "6808452d16c1c427ac727817",
          "name": "Julian Asilis",
          "hidden": false
        },
        {
          "_id": "6808452d16c1c427ac727818",
          "name": "Ömer Faruk Akgül",
          "hidden": false
        },
        {
          "_id": "6808452d16c1c427ac727819",
          "name": "Enes Burak Bilgin",
          "hidden": false
        },
        {
          "_id": "6808452d16c1c427ac72781a",
          "name": "Ollie Liu",
          "hidden": false
        },
        {
          "_id": "6808452d16c1c427ac72781b",
          "name": "Willie Neiswanger",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T10:38:00.000Z",
      "submittedOnDailyAt": "2025-04-24T06:51:34.876Z",
      "title": "Tina: Resizado modelo de Laboling Tina",
      "submittedOnDailyBy": {
        "_id": "67469d6a8407f929491dce06",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
        "isPro": true,
        "fullname": "Shangshang Wang",
        "user": "upup-ashton-wang",
        "type": "user"
      },
      "summary": "Esta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español mantiene la profundidad y precisión del texto original:\n\nEsta traducción al español man",
      "upvotes": 4,
      "discussionId": "6808452f16c1c427ac7278b1",
      "projectPage": "https://shangshangwang.notion.site/tina",
      "githubRepo": "https://github.com/shangshang-wang/Tina",
      "ai_keywords": [
        "parameter-efficient updates",
        "reinforcement learning (RL)",
        "low-rank adaptation (LoRA)",
        "traininng logs",
        "checkpoints"
      ]
    },
    "publishedAt": "2025-04-22T06:38:00.000Z",
    "title": "Tina: Tiny Reasoning Models via LoRA",
    "summary": "How cost-effectively can strong reasoning abilities be achieved in language\nmodels? Driven by this fundamental question, we present Tina, a family of tiny\nreasoning models achieved with high cost-efficiency. Notably, Tina demonstrates\nthat substantial reasoning performance can be developed using only minimal\nresources, by applying parameter-efficient updates during reinforcement\nlearning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B\nparameter base model. This minimalist approach produces models that achieve\nreasoning performance which is competitive with, and sometimes surpasses, SOTA\nRL reasoning models built upon the same base model. Crucially, this is achieved\nat a tiny fraction of the computational post-training cost employed by existing\nSOTA models. In fact, the best Tina model achieves a >20\\% reasoning\nperformance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD\npost-training and evaluation cost (i.e., an estimated 260x cost reduction). Our\nwork reveals the surprising effectiveness of efficient RL reasoning via LoRA.\nWe validate this across multiple open-source reasoning datasets and various\nablation settings starting with a single, fixed set of hyperparameters.\nFurthermore, we hypothesize that this effectiveness and efficiency stem from\nLoRA rapidly adapting the model to the structural format of reasoning rewarded\nby RL, while largely preserving the base model's underlying knowledge. In\nservice of accessibility and open research, we fully open-source all code,\ntraining logs, and model weights \\& checkpoints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15777.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67469d6a8407f929491dce06",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
      "fullname": "Shangshang Wang",
      "name": "upup-ashton-wang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15707",
      "authors": [
        {
          "_id": "6809f6b03e1d48a9bb7f5719",
          "user": {
            "_id": "650971dbce83a0c12a851000",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ec7H2cZpaZz07W02FT5U9.png",
            "isPro": false,
            "fullname": "Yannic Neuhaus",
            "user": "YanNeu",
            "type": "user"
          },
          "name": "Yannic Neuhaus",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:09:36.629Z",
          "hidden": false
        },
        {
          "_id": "6809f6b03e1d48a9bb7f571a",
          "name": "Matthias Hein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T08:47:59.000Z",
      "submittedOnDailyAt": "2025-04-24T07:02:13.584Z",
      "title": "RePOPE: Estudio sobre el Benchmark POPE relacionado con el Error de Anotación",
      "submittedOnDailyBy": {
        "_id": "650971dbce83a0c12a851000",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ec7H2cZpaZz07W02FT5U9.png",
        "isPro": false,
        "fullname": "Yannic Neuhaus",
        "user": "YanNeu",
        "type": "user"
      },
      "summary": "Cuando la anotación de datos es costosa, los conjuntos de datos estándar generalmente utilizan etiquetas de los conjuntos de imágenes existentes. En este artículo, se evalua el impacto de los errores de etiquetado en los benchmarks de objetos comúnmente utilizados en POPE, basados en MSCOCO. Se reanotaron las imágenes del benchmark y identificaron la desigualdad en el error de anotación en cada subconjunto. Se evaluaron los modelos con las etiquetas reanotadas como RePOPE, revelando así el impacto de la calidad de estas etiquetas. El código y los datos están disponibles en https://github.com/YanNeu/RePOPE.",
      "upvotes": 4,
      "discussionId": "6809f6b13e1d48a9bb7f5790",
      "githubRepo": "https://github.com/YanNeu/RePOPE",
      "ai_keywords": [
        "MSCOCO",
        "object hallucination",
        "POPE",
        "RePOPE"
      ]
    },
    "publishedAt": "2025-04-22T04:47:59.000Z",
    "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark",
    "summary": "Since data annotation is costly, benchmark datasets often incorporate labels\nfrom established image datasets. In this work, we assess the impact of label\nerrors in MSCOCO on the frequently used object hallucination benchmark POPE. We\nre-annotate the benchmark images and identify an imbalance in annotation errors\nacross different subsets. Evaluating multiple models on the revised labels,\nwhich we denote as RePOPE, we observe notable shifts in model rankings,\nhighlighting the impact of label quality. Code and data are available at\nhttps://github.com/YanNeu/RePOPE .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15707.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650971dbce83a0c12a851000",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ec7H2cZpaZz07W02FT5U9.png",
      "fullname": "Yannic Neuhaus",
      "name": "YanNeu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11919",
      "authors": [
        {
          "_id": "6809ae47c6faa064f324307d",
          "user": {
            "_id": "652f979c61ce8120849bb72f",
            "avatarUrl": "/avatars/bac0c1697ef26ffbb455a8b59c16b783.svg",
            "isPro": false,
            "fullname": "Qianjin Yu",
            "user": "USTCYu",
            "type": "user"
          },
          "name": "Qianjin Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:10:26.728Z",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f324307e",
          "name": "Keyu Wu",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f324307f",
          "name": "Zihan Chen",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f3243080",
          "name": "Chushu Zhang",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f3243081",
          "name": "Manlin Mei",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f3243082",
          "name": "Lingjun Huang",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f3243083",
          "name": "Fang Tan",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f3243084",
          "name": "Yongsheng Du",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f3243085",
          "name": "Kunlin Liu",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f3243086",
          "name": "Yurui Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T09:55:34.000Z",
      "submittedOnDailyAt": "2025-04-24T08:12:14.834Z",
      "title": "Generación de datos de contexto de alta calidad y evaluación del nivel de dificultad de preguntas adecuadas para LLMs para inventarios",
      "submittedOnDailyBy": {
        "_id": "652f979c61ce8120849bb72f",
        "avatarUrl": "/avatars/bac0c1697ef26ffbb455a8b59c16b783.svg",
        "isPro": false,
        "fullname": "Qianjin Yu",
        "user": "USTCYu",
        "type": "user"
      },
      "summary": "Recientemente, DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) ha demostrado un excelente desempeño en tareas complejas, mostrando una excelente capacidad lógica, y ha publicado su método. Esto ofrece potencialmente datos de alta calidad de \"Chain-of-Thought\" (CoT) que pueden estimular la capacidad lógica de modelos de lenguaje grandes de pequeña escala (LLMs). Se está explorando cómo generar datos de alta calidad de CoT adecuados para diferentes LLMs. Para ello, se evalúa la dificultad de los problemas según la capacidad lógica de los LLMs y se construye una base de datos de problemas con LLM-Adaptive. Luego, se muestran ejemplos de problemas de diferentes niveles de dificultad utilizando DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) para generar respuestas precisas y datos de alta calidad de CoT. Al establecer los niveles de dificultad en LLM-Adaptive, se reduce significativamente el costo de la generación de datos y se mejora la eficiencia de la adaptación del modelo (SFT). Finalmente, se evaluó la eficacia y la generalización de los métodos propuestos en áreas de tareas de colaboración matemática y generación de código. Específicamente, con 2k de datos de alta calidad de CoT de matemáticas, nuestro ZMath-32B superó a DeepSeek-Distill-32B en tareas de lógica matemática. De la misma manera, con 2k de datos de alta calidad de CoT de código, nuestro ZCode-32B superó a DeepSeek-Distill-32B en tareas de lógica de código.",
      "upvotes": 3,
      "discussionId": "6809ae49c6faa064f32430d1",
      "ai_keywords": [
        "chain-of-thought (CoT) data",
        "LLM-Adaptive questiondifficulty levels",
        "LLM-Adaptive question database",
        "model supervised fine-tuning (SFT)"
      ]
    },
    "publishedAt": "2025-04-16T05:55:34.000Z",
    "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective\n  of LLM-Adaptive Question Difficulty Grading",
    "summary": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its\nexcellent reasoning ability in complex tasks and has publiclyshared its\nmethodology. This provides potentially high-quality chain-of-thought (CoT) data\nfor stimulating the reasoning abilities of small-sized large language models\n(LLMs). To generate high-quality CoT data for different LLMs, we seek an\nefficient method for generating high-quality CoT data with LLM-Adaptive\nquestiondifficulty levels. First, we grade the difficulty of the questions\naccording to the reasoning ability of the LLMs themselves and construct a\nLLM-Adaptive question database. Second, we sample the problem database based on\na distribution of difficulty levels of the questions and then use DeepSeek-R1\n(671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality\nCoT data with correct answers. Thanks to the construction of CoT data with\nLLM-Adaptive difficulty levels, we have significantly reduced the cost of data\ngeneration and enhanced the efficiency of model supervised fine-tuning (SFT).\nFinally, we have validated the effectiveness and generalizability of the\nproposed method in the fields of complex mathematical competitions and code\ngeneration tasks. Notably, with only 2k high-quality mathematical CoT data, our\nZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly,\nwith only 2k high-quality code CoT data, our ZCode-32B surpasses\nDeepSeek-Distill-32B in code reasoning tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11919.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "652f979c61ce8120849bb72f",
      "avatarUrl": "/avatars/bac0c1697ef26ffbb455a8b59c16b783.svg",
      "fullname": "Qianjin Yu",
      "name": "USTCYu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15254",
      "authors": [
        {
          "_id": "6807bf3e70a0cec724b8a011",
          "user": {
            "_id": "6697abd4be7ce6de07140e72",
            "avatarUrl": "/avatars/1598decabd9975d755082c8fb4e5962d.svg",
            "isPro": false,
            "fullname": "Anirudh Khatry",
            "user": "anirudhkhatry",
            "type": "user"
          },
          "name": "Anirudh Khatry",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:30:04.897Z",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a012",
          "name": "Robert Zhang",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a013",
          "name": "Jia Pan",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a014",
          "name": "Ziteng Wang",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a015",
          "name": "Qiaochu Chen",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a016",
          "name": "Greg Durrett",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a017",
          "name": "Isil Dillig",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:33:33.000Z",
      "submittedOnDailyAt": "2025-04-24T05:15:24.237Z",
      "title": "CRUST-Bench: Marco de pruebas completo para la traducción C-a-Rust seguro",
      "submittedOnDailyBy": {
        "_id": "6697abd4be7ce6de07140e72",
        "avatarUrl": "/avatars/1598decabd9975d755082c8fb4e5962d.svg",
        "isPro": false,
        "fullname": "Anirudh Khatry",
        "user": "anirudhkhatry",
        "type": "user"
      },
      "summary": "La traducción al español de la texto proporcionado es:\n\n\"La transición C-to-Rust es importante para modernizar código antiguo y mejorar la seguridad y la interfaz con el ecosistema moderno de Rust. Sin embargo, actualmente no existe un conjunto de datos que evalúe si un sistema puede traducir C a Rust de manera segura. Presentamos CRUST-Bench, un conjunto de datos que incluye 100 repositorios de C, cada uno con una interfaz y casos de prueba manualmente escritos en Rust seguro. Estos casos de prueba permiten verificar la exactitud de la transición. CRUST-Bench no se centra solo en funciones independientes, sino en los desafíos de traducir proyectos complejos con dependencias entre archivos. Las interfaz Rust proporcionadas garantizan patrones idiomáticos y seguros de memoria a través de especificaciones explícitas y los casos de prueba aseguran la exactitud funcional. Evaluamos esta tarea con los modelos de lenguaje de alto rendimiento (LLMs) y encontramos que la generación de código seguro y idiomático en Rust es un desafío difícil, incluso para los métodos y tecnologías más avanzados actuales. Además, los LLMs explican errores comunes durante la transición de C a Rust seguro. El mejor modelo, OpenAI o1, solo puede resolver 15 tareas en un solo paso. La mejora de CRUST-Bench espera mejorar sistemas de transición considerando escenarios complejos, ayudando a migrar código de lenguajes de memoria segura (como Rust) desde C. Los datos y código están disponibles en https://github.com/anirudhkhatry/CRUST-bench.\"",
      "upvotes": 1,
      "discussionId": "6807bf3f70a0cec724b8a044",
      "githubRepo": "https://github.com/anirudhkhatry/CRUST-bench"
    },
    "publishedAt": "2025-04-21T13:33:33.000Z",
    "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
    "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15254.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6697abd4be7ce6de07140e72",
      "avatarUrl": "/avatars/1598decabd9975d755082c8fb4e5962d.svg",
      "fullname": "Anirudh Khatry",
      "name": "anirudhkhatry",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10419",
      "authors": [
        {
          "_id": "68026f762e2023f6cf7f0daa",
          "user": {
            "_id": "680268a7fd1fae58d58a2b49",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P96qvBdInKYnVxij7-MM_.png",
            "isPro": false,
            "fullname": "Michał Turski",
            "user": "mturski",
            "type": "user"
          },
          "name": "Michał Turski",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-18T15:27:52.927Z",
          "hidden": false
        },
        {
          "_id": "68026f762e2023f6cf7f0dab",
          "name": "Mateusz Chiliński",
          "hidden": false
        },
        {
          "_id": "68026f762e2023f6cf7f0dac",
          "name": "Łukasz Borchmann",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:06:59.000Z",
      "submittedOnDailyAt": "2025-04-24T03:55:04.111Z",
      "title": "Checkbox Ignorado: Utilizando checkboxes QA para complementar la visión de los checkboxes de un modelo de lenguaje de gran escala.",
      "submittedOnDailyBy": {
        "_id": "680268a7fd1fae58d58a2b49",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P96qvBdInKYnVxij7-MM_.png",
        "isPro": false,
        "fullname": "Michał Turski",
        "user": "mturski",
        "type": "user"
      },
      "summary": "Los checkboxes desempeñan un papel importante en el procesamiento de documentos reales, ya que la presencia o ausencia de marcas proporciona información directa para el proceso de extracción de datos y toma de decisiones. Sin embargo, mientras los grandes modelos de visión y lenguaje muestran un gran rendimiento en diversas tareas, interpretar el contenido de los checkboxes es un problema complejo. Para abordar este desafío, evaluamos el rendimiento de los modelos en tareas relacionadas con checkboxes y presentamos el conjunto de datos \"CheckboxQA\" para mejorar el rendimiento. Este conjunto de datos revela las limitaciones actuales de los modelos y sirve como una herramienta válida para el desarrollo de sistemas de comprensión de documentos. También tiene un impacto significativo en diversas áreas como la tecnología legal y la gestión de activos.\n\nEl conjunto de datos está disponible en la siguiente URL:\nhttps://github.com/Snowflake-Labs/CheckboxQA",
      "upvotes": 1,
      "discussionId": "68026f782e2023f6cf7f0e05"
    },
    "publishedAt": "2025-04-14T13:06:59.000Z",
    "title": "Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large\n  Language Models with CheckboxQA",
    "summary": "Checkboxes are critical in real-world document processing where the presence\nor absence of ticks directly informs data extraction and decision-making\nprocesses. Yet, despite the strong performance of Large Vision and Language\nModels across a wide range of tasks, they struggle with interpreting checkable\ncontent. This challenge becomes particularly pressing in industries where a\nsingle overlooked checkbox may lead to costly regulatory or contractual\noversights. To address this gap, we introduce the CheckboxQA dataset, a\ntargeted resource designed to evaluate and improve model performance on\ncheckbox-related tasks. It reveals the limitations of current models and serves\nas a valuable tool for advancing document comprehension systems, with\nsignificant implications for applications in sectors such as legal tech and\nfinance.\n  The dataset is publicly available at:\nhttps://github.com/Snowflake-Labs/CheckboxQA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "680268a7fd1fae58d58a2b49",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P96qvBdInKYnVxij7-MM_.png",
      "fullname": "Michał Turski",
      "name": "mturski",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]