[
  {
    "paper": {
      "id": "2504.05599",
      "authors": [
        {
          "_id": "67f61a98af81b0685bf055cf",
          "name": "Yi Peng",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d0",
          "name": "Chris",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d1",
          "name": "Xiaokun Wang",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d2",
          "name": "Yichen Wei",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d3",
          "name": "Jiangbo Pei",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d4",
          "name": "Weijie Qiu",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d5",
          "name": "Ai Jian",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d6",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d7",
          "name": "Jiachun Pan",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d8",
          "name": "Tianyidan Xie",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d9",
          "name": "Li Ge",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055da",
          "name": "Rongxian Zhuang",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055db",
          "name": "Xuchen Song",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055dc",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055dd",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-08T01:19:20.000Z",
      "submittedOnDailyAt": "2025-04-09T05:32:09.323Z",
      "title": "Skywork R1V: Está al frente de la industria de Shieldos y es una empresa pionera en la resolución de diversos tipos de problemas lógicos.",
      "submittedOnDailyBy": {
        "_id": "6462b241b438438da3c25a5d",
        "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
        "isPro": false,
        "fullname": "Xuchen Song",
        "user": "xuchensong",
        "type": "user"
      },
      "summary": "Introduzco Skywork R1V. Es un modelo de lógica multimodelo que extiende la gran red de lenguaje de R1 en un modelo visual. Skywork R1V utiliza un modelo visual ligero de manera que no es necesario reentrenar el modelo de lenguaje básico o el encoder visual. Además, propone la combinación de aprendizaje guiado iterativo (SFT) y optimización de políticas de grupo relativa (GRPO) para fortalecer la disposición de textos visuales, lo que significa una notable mejora en la eficiencia de la integración de modelos. Además, introduce un enfoque de calentamiento de la secuencia de pensamiento de longitud adaptativa para la generación de datos de razonamiento, lo que permite optimizar la longitud de la razón y mejorar la eficiencia de la inferencia, evitando pensamientos excesivos. Según evaluaciones experimentales, Skywork R1V alcanza un puntaje de 69.0 en el benchmark MMMU, un puntaje de 67.5 en MathVista, un puntaje de 72.0 en AIME y un puntaje de 94.0 en MATH500, manteniendo una excelente capacidad de razonamiento en textos fuertes. Los pesos del modelo Skywork R1V están disponibles, fomentando la transparencia y la reproducibilidad.",
      "upvotes": 43,
      "discussionId": "67f61a9daf81b0685bf05731",
      "githubRepo": "https://github.com/SkyworkAI/Skywork-R1V"
    },
    "publishedAt": "2025-04-07T21:19:20.000Z",
    "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought",
    "summary": "We introduce Skywork R1V, a multimodal reasoning model extending the an\nR1-series Large language models (LLM) to visual modalities via an efficient\nmultimodal transfer method. Leveraging a lightweight visual projector, Skywork\nR1V facilitates seamless multimodal adaptation without necessitating retraining\nof either the foundational language model or the vision encoder. To strengthen\nvisual-text alignment, we propose a hybrid optimization strategy that combines\nIterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO), significantly enhancing cross-modal integration efficiency.\nAdditionally, we introduce an adaptive-length Chain-of-Thought distillation\napproach for reasoning data generation. This approach dynamically optimizes\nreasoning chain lengths, thereby enhancing inference efficiency and preventing\nexcessive reasoning overthinking. Empirical evaluations demonstrate that\nSkywork R1V, with only 38B parameters, delivers competitive performance,\nachieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista.\nMeanwhile, it maintains robust textual reasoning performance, evidenced by\nimpressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model\nweights have been publicly released to promote openness and reproducibility.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05599.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6462b241b438438da3c25a5d",
      "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
      "fullname": "Xuchen Song",
      "name": "xuchensong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.06263",
      "authors": [
        {
          "_id": "67f5e3701b29460f6a087954",
          "name": "Yiying Yang",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a087955",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T07:34:51.924Z",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a087956",
          "user": {
            "_id": "6485b08e687d9e0c759121b0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6485b08e687d9e0c759121b0/P_9F0izrQgUfEd-VEbhg8.jpeg",
            "isPro": false,
            "fullname": "sijin",
            "user": "CH3COOK",
            "type": "user"
          },
          "name": "Sijin Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T07:34:48.985Z",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a087957",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a087958",
          "name": "Jiaxu Zhang",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a087959",
          "name": "Liao Wang",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a08795a",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a08795b",
          "name": "Xingjun Ma",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a08795c",
          "name": "Yu-Gang Jiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6485b08e687d9e0c759121b0/goYk4m9KxOQvIKfjUy5Rn.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/6485b08e687d9e0c759121b0/oqnAsMzmh3Vtx0OIPv5ai.gif"
      ],
      "publishedAt": "2025-04-08T17:59:49.000Z",
      "submittedOnDailyAt": "2025-04-09T01:51:00.484Z",
      "title": "OmniSVG: Modelo de generación de gráficos vectoriales escalables de unidades de alimentación",
      "submittedOnDailyBy": {
        "_id": "6485b08e687d9e0c759121b0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6485b08e687d9e0c759121b0/P_9F0izrQgUfEd-VEbhg8.jpeg",
        "isPro": false,
        "fullname": "sijin",
        "user": "CH3COOK",
        "type": "user"
      },
      "summary": "El SVG (Scalable Vector Graphics) se utiliza ampliamente en el diseño gráfico y es considerado una forma de imagen importante por su independencia de resolución y posibilidad de edición. La investigación sobre la generación de SVG de alta calidad ha recibido mucha atención en la comunidad AIGC durante mucho tiempo. Sin embargo, los métodos actuales generan salidas desorganizadas con alto costo computacional o están limitados a la generación de simples imágenes en blanco y negro. Se propone un marco integrado llamado OmniSVG para la generación de SVG de alta calidad y compleja. Este framework utiliza modelos de visión y lenguaje predecidos (VLMs) desde el terminal hasta el terminal para la generación de SVG. OmniSVG parametriza las instrucciones y coordenadas de SVG como tokens discretos, separando la lógica estructural de la generalidad baja y permitiendo una eficiente entrenamiento para mantener la expresividad de estructuras complejas de SVG. Además, se presenta MMSVG-2M, un conjunto de datos variado que incluye más de 2 millones de SVG bien anotados, proporcionando un protocolo de evaluación estándar para la tarea de generación de SVG condicional. Los experimentos extensos muestran que OmniSVG supera los métodos actuales y tiene la posibilidad de integrarse en el flujo de trabajo de diseño de SVG profesional.",
      "upvotes": 40,
      "discussionId": "67f5e3751b29460f6a087aa7",
      "projectPage": "https://omnisvg.github.io/",
      "githubRepo": "https://github.com/OmniSVG/OmniSVG"
    },
    "publishedAt": "2025-04-08T13:59:49.000Z",
    "title": "OmniSVG: A Unified Scalable Vector Graphics Generation Model",
    "summary": "Scalable Vector Graphics (SVG) is an important image format widely adopted in\ngraphic design because of their resolution independence and editability. The\nstudy of generating high-quality SVG has continuously drawn attention from both\ndesigners and researchers in the AIGC community. However, existing methods\neither produces unstructured outputs with huge computational cost or is limited\nto generating monochrome icons of over-simplified structures. To produce\nhigh-quality and complex SVG, we propose OmniSVG, a unified framework that\nleverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal\nSVG generation. By parameterizing SVG commands and coordinates into discrete\ntokens, OmniSVG decouples structural logic from low-level geometry for\nefficient training while maintaining the expressiveness of complex SVG\nstructure. To further advance the development of SVG synthesis, we introduce\nMMSVG-2M, a multimodal dataset with two million richly annotated SVG assets,\nalong with a standardized evaluation protocol for conditional SVG generation\ntasks. Extensive experiments show that OmniSVG outperforms existing methods and\ndemonstrates its potential for integration into professional SVG design\nworkflows.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6485b08e687d9e0c759121b0/goYk4m9KxOQvIKfjUy5Rn.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/6485b08e687d9e0c759121b0/oqnAsMzmh3Vtx0OIPv5ai.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06263.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6485b08e687d9e0c759121b0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6485b08e687d9e0c759121b0/P_9F0izrQgUfEd-VEbhg8.jpeg",
      "fullname": "sijin",
      "name": "CH3COOK",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05979",
      "authors": [
        {
          "_id": "67f5d5416ceb820f2006d8a2",
          "name": "Sixiang Chen",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8a3",
          "user": {
            "_id": "63fccdac93b993a4ebd7789a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
            "isPro": false,
            "fullname": "Jinbin Bai",
            "user": "BryanW",
            "type": "user"
          },
          "name": "Jinbin Bai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T07:35:08.303Z",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8a4",
          "name": "Zhuoran Zhao",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8a5",
          "name": "Tian Ye",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8a6",
          "user": {
            "_id": "656724074f6ec72017754d33",
            "avatarUrl": "/avatars/e61de248f6f53719b2375077340dd033.svg",
            "isPro": false,
            "fullname": "QingyuShi",
            "user": "QingyuShi",
            "type": "user"
          },
          "name": "Qingyu Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T07:35:04.572Z",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8a7",
          "user": {
            "_id": "67136093d2e50f1e8c9fad52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png",
            "isPro": false,
            "fullname": "Donghao Zhou",
            "user": "donghao-zhou",
            "type": "user"
          },
          "name": "Donghao Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T07:35:02.400Z",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8a8",
          "name": "Wenhao Chai",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8a9",
          "name": "Xin Lin",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8aa",
          "name": "Jianzong Wu",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8ab",
          "name": "Chao Tang",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8ac",
          "name": "Shilin Xu",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8ad",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8ae",
          "name": "Haobo Yuan",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8af",
          "name": "Yikang Zhou",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8b0",
          "name": "Wei Chow",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8b1",
          "name": "Linfeng Li",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8b2",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8b3",
          "name": "Lei Zhu",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8b4",
          "name": "Lu Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-08T12:34:36.000Z",
      "submittedOnDailyAt": "2025-04-09T00:39:48.924Z",
      "title": "Estudio empírico sobre la capacidad de generación de imágenes de GPT-4o",
      "submittedOnDailyBy": {
        "_id": "63fccdac93b993a4ebd7789a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
        "isPro": false,
        "fullname": "Jinbin Bai",
        "user": "BryanW",
        "type": "user"
      },
      "summary": "La estructura de la generación de imágenes está evolucionando rápidamente y ha avanzado desde los métodos basados en GANs al principio hasta modelos de diferenciación, y más recientemente, hasta arquitecturas generativas integradas que combinan la comprensión y la generación. Las últimas innovaciones, especialmente GPT-4o, han demostrado la posibilidad de generar diversidad de alta calidad, aunque el diseño de la arquitectura sigue siendo un secreto. Esto ha generado preguntas sobre si los marcos integrados que ya han unido la generación de imágenes y texto han tenido éxito. En este artículo, se realiza una investigación experimental sobre la capacidad de GPT-4o para generar imágenes y se compara con modelos abiertos y comerciales avanzados. La evaluación incluye 20 tareas en cuatro principales áreas: generación de imágenes a partir de texto, de imágenes a partir de imágenes, de imágenes a 3D y de imágenes a X. El análisis revela las fortalezas y limitaciones de GPT-4o en diferentes configuraciones, y se explora su lugar en el desarrollo de modelos generativos a gran escala. A través de esta investigación, se identifican las posibles direcciones para modelos generativos integrados y se destacan el papel del diseño de la arquitectura y la escala de datos.",
      "upvotes": 38,
      "discussionId": "67f5d5496ceb820f2006da78"
    },
    "publishedAt": "2025-04-08T08:34:36.000Z",
    "title": "An Empirical Study of GPT-4o Image Generation Capabilities",
    "summary": "The landscape of image generation has rapidly evolved, from early GAN-based\napproaches to diffusion models and, most recently, to unified generative\narchitectures that seek to bridge understanding and generation tasks. Recent\nadvances, especially the GPT-4o, have demonstrated the feasibility of\nhigh-fidelity multimodal generation, their architectural design remains\nmysterious and unpublished. This prompts the question of whether image and text\ngeneration have already been successfully integrated into a unified framework\nfor those methods. In this work, we conduct an empirical study of GPT-4o's\nimage generation capabilities, benchmarking it against leading open-source and\ncommercial models. Our evaluation covers four main categories, including\ntext-to-image, image-to-image, image-to-3D, and image-to-X generation, with\nmore than 20 tasks. Our analysis highlights the strengths and limitations of\nGPT-4o under various settings, and situates it within the broader evolution of\ngenerative modeling. Through this investigation, we identify promising\ndirections for future unified generative models, emphasizing the role of\narchitectural design and data scaling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05979.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fccdac93b993a4ebd7789a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
      "fullname": "Jinbin Bai",
      "name": "BryanW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02160",
      "authors": [
        {
          "_id": "67efd1cd40e0a904109cac33",
          "user": {
            "_id": "660114b38ae190912a61be5d",
            "avatarUrl": "/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg",
            "isPro": false,
            "fullname": "ShaojinWu",
            "user": "fenfan",
            "type": "user"
          },
          "name": "Shaojin Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:54:08.610Z",
          "hidden": false
        },
        {
          "_id": "67efd1cd40e0a904109cac34",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "67efd1cd40e0a904109cac35",
          "user": {
            "_id": "635634171c93c1ef4e9eb1c2",
            "avatarUrl": "/avatars/66b31b801960612057ecfd1e26410075.svg",
            "isPro": false,
            "fullname": "wuwenxu",
            "user": "wuwx",
            "type": "user"
          },
          "name": "Wenxu Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:54:06.171Z",
          "hidden": false
        },
        {
          "_id": "67efd1cd40e0a904109cac36",
          "name": "Yufeng Cheng",
          "hidden": false
        },
        {
          "_id": "67efd1cd40e0a904109cac37",
          "name": "Fei Ding",
          "hidden": false
        },
        {
          "_id": "67efd1cd40e0a904109cac38",
          "name": "Qian He",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/660114b38ae190912a61be5d/eLRIcZxGD19KgExwoJTKW.mp4"
      ],
      "publishedAt": "2025-04-02T22:20:21.000Z",
      "submittedOnDailyAt": "2025-04-09T02:18:09.429Z",
      "title": "「Minimizando la generalización: liberación de mayor control en la generación de texto」",
      "submittedOnDailyBy": {
        "_id": "660114b38ae190912a61be5d",
        "avatarUrl": "/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg",
        "isPro": false,
        "fullname": "ShaojinWu",
        "user": "fenfan",
        "type": "user"
      },
      "summary": "El traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de ese texto inglés es el siguiente:\n\nEl traducción al español de es",
      "upvotes": 18,
      "discussionId": "67efd1d140e0a904109cad62",
      "projectPage": "https://bytedance.github.io/UNO/",
      "githubRepo": "https://github.com/bytedance/UNO",
      "ai_keywords": [
        "diffusion transformers",
        "in-context generation",
        "multi-subject paired data",
        "UNO",
        "progressive cross-modal alignment",
        "universal rotary position embedding",
        "multi-image conditioned",
        "subject-to-image model",
        "text-to-image model"
      ]
    },
    "publishedAt": "2025-04-02T18:20:21.000Z",
    "title": "Less-to-More Generalization: Unlocking More Controllability by\n  In-Context Generation",
    "summary": "Although subject-driven generation has been extensively explored in image\ngeneration due to its wide applications, it still has challenges in data\nscalability and subject expansibility. For the first challenge, moving from\ncurating single-subject datasets to multiple-subject ones and scaling them is\nparticularly difficult. For the second, most recent methods center on\nsingle-subject generation, making it hard to apply when dealing with\nmulti-subject scenarios. In this study, we propose a highly-consistent data\nsynthesis pipeline to tackle this challenge. This pipeline harnesses the\nintrinsic in-context generation capabilities of diffusion transformers and\ngenerates high-consistency multi-subject paired data. Additionally, we\nintroduce UNO, which consists of progressive cross-modal alignment and\nuniversal rotary position embedding. It is a multi-image conditioned\nsubject-to-image model iteratively trained from a text-to-image model.\nExtensive experiments show that our method can achieve high consistency while\nensuring controllability in both single-subject and multi-subject driven\ngeneration.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/660114b38ae190912a61be5d/eLRIcZxGD19KgExwoJTKW.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02160.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "660114b38ae190912a61be5d",
      "avatarUrl": "/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg",
      "fullname": "ShaojinWu",
      "name": "fenfan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05535",
      "authors": [
        {
          "_id": "67f630091aed1b4344b57c1b",
          "name": "M-A-P Team",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c1c",
          "name": "Siwei Wu",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c1d",
          "user": {
            "_id": "6704ee27386892c420db1938",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6704ee27386892c420db1938/lb5mtEwYhn47RawkynYPs.jpeg",
            "isPro": false,
            "fullname": "JinCheng Ren",
            "user": "JinChengRen",
            "type": "user"
          },
          "name": "Jincheng Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T09:48:17.939Z",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c1e",
          "name": "Xinrun Du",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c1f",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c20",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c21",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c22",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c23",
          "name": "Yunwen Li",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c24",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tianyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T09:48:16.069Z",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c25",
          "name": "Boyu Feng",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c26",
          "name": "Huaqing Yuan",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c27",
          "name": "Zenith Wang",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c28",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c29",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c2a",
          "name": "Chenglin Cai",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c2b",
          "name": "Haoran Que",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c2c",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c2d",
          "name": "Yuelin Bai",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c2e",
          "name": "Zekun Moore Wang",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c2f",
          "name": "Zhouliang Yu",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c30",
          "name": "Qunshu Lin",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c31",
          "name": "Ding Pan",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c32",
          "name": "Yuchen Jiang",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c33",
          "name": "Tiannan Wang",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c34",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c35",
          "name": "Shenzhi Wang",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c36",
          "name": "Xingyuan Bu",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c37",
          "user": {
            "_id": "6417d9ea8f689506e7148417",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
            "isPro": false,
            "fullname": "minghao",
            "user": "Liam-Liu",
            "type": "user"
          },
          "name": "Minghao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T09:48:19.721Z",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c38",
          "name": "Guoyin Wang",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c39",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "67f630091aed1b4344b57c3a",
          "name": "Chenghua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T22:15:51.000Z",
      "submittedOnDailyAt": "2025-04-09T07:58:42.888Z",
      "title": "COIG-P: Ajuste de valores humanos con un alto rendimiento para un gran conjunto de datos de orientación china",
      "submittedOnDailyBy": {
        "_id": "656d97b10bbc114fe64a96c5",
        "avatarUrl": "/avatars/fd23bae1d85c5b96c42064a5ddcfad41.svg",
        "isPro": false,
        "fullname": "SiweiWu",
        "user": "SiweiWu",
        "type": "user"
      },
      "summary": "Los lenguajes de programación grandes (LLMs) han logrado un éxito sorprendente al adaptarse a las preferencias humanas. Sin embargo, los conjuntos de datos de orientación china actuales son pequeños, limitados en ámbito y no rigurosamente validados, lo que impide que las preferencias humanas se adapten a grandes conjuntos de datos. Para resolver este problema, se diseñó un proxy de etiquetado para conjuntos de datos de orientación china basados en LLMs sin participación humana. Específicamente, se recopilaron 92k preguntas de alta calidad en chino, se creó par de respuestas rechazadas y se registraron puntuaciones. Con esto, se introdujo COIG-P (Chinese Open Instruction Generalist - Preference), un conjunto de datos de orientación de alta calidad y gran escala que incluye 1,009k pares de orientación china en 6 áreas diferentes: Chat, Code, Math, Logic, Novel y Role. Con base en COIG-P, se entrenó un modelo de recompensa de 8B de tamaño para reducir el overhead de puntuación y se construyó CRBench (Chinese Reward Benchmark) con precisión. Según los resultados de la evaluación basada en AlignBench, COIG-P supera significativamente a otros conjuntos de datos de orientación china, y mejora el rendimiento de los modelos Qwen2/2.5 y Infinity-Instruct-3M-0625 en un rango de 2% a 12%. Según CRBench, nuestro modelo de recompensa (CRM) tiene una fuerte capacidad para calcular puntuaciones. En la prueba de COIG-P, se demostró la capacidad de identificar y filtrar respuestas de muy baja calidad, como las de GPT-4o, manteniendo eficiencia y eficiencia de costo. Nuestro código y datos están disponibles en la siguiente URL:\nhttps://github.com/multimodal-art-projection/COIG-P",
      "upvotes": 8,
      "discussionId": "67f6300b1aed1b4344b57cd0"
    },
    "publishedAt": "2025-04-07T18:15:51.000Z",
    "title": "COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for\n  Alignment with Human Values",
    "summary": "Aligning large language models (LLMs) with human preferences has achieved\nremarkable success. However, existing Chinese preference datasets are limited\nby small scale, narrow domain coverage, and lack of rigorous data validation.\nAdditionally, the reliance on human annotators for instruction and response\nlabeling significantly constrains the scalability of human preference datasets.\nTo address these challenges, we design an LLM-based Chinese preference dataset\nannotation pipeline with no human intervention. Specifically, we crawled and\ncarefully filtered 92k high-quality Chinese queries and employed 15 mainstream\nLLMs to generate and score chosen-rejected response pairs. Based on it, we\nintroduce COIG-P (Chinese Open Instruction Generalist - Preference), a\nhigh-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese\npreference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel,\nand Role. Building upon COIG-P, to reduce the overhead of using LLMs for\nscoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously\nconstructed a Chinese Reward Benchmark (CRBench). Evaluation results based on\nAlignBench liu2024alignbenchbenchmarkingchinesealignment show that that\nCOIG-P significantly outperforms other Chinese preference datasets, and it\nbrings significant performance improvements ranging from 2% to 12% for the\nQwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results\non CRBench demonstrate that our CRM has a strong and robust scoring ability. We\napply it to filter chosen-rejected response pairs in a test split of COIG-P,\nand our experiments show that it is comparable to GPT-4o in identifying\nlow-quality samples while maintaining efficiency and cost-effectiveness. Our\ncodes and data are released in\nhttps://github.com/multimodal-art-projection/COIG-P.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05535.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656d97b10bbc114fe64a96c5",
      "avatarUrl": "/avatars/fd23bae1d85c5b96c42064a5ddcfad41.svg",
      "fullname": "SiweiWu",
      "name": "SiweiWu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.02810",
      "authors": [
        {
          "_id": "67f099de103cb604facd26cd",
          "user": {
            "_id": "63453f02a05b51f7ded3c579",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/u7p-wTj8zyeWpSFRI8Go0.png",
            "isPro": false,
            "fullname": "Andy Lin",
            "user": "pkuHaowei",
            "type": "user"
          },
          "name": "Haowei Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-06T08:11:10.909Z",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26ce",
          "name": "Xiangyu Wang",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26cf",
          "name": "Ruilin Yan",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26d0",
          "name": "Baizhou Huang",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26d1",
          "name": "Haotian Ye",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26d2",
          "name": "Jianhua Zhu",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26d3",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26d4",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26d5",
          "name": "Jianzhu Ma",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26d6",
          "user": {
            "_id": "64683a5776bb704aa14588b7",
            "avatarUrl": "/avatars/e532756f52c5b95981470ace41a10556.svg",
            "isPro": false,
            "fullname": "Yitao Liang",
            "user": "YitaoLiang",
            "type": "user"
          },
          "name": "Yitao Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T07:38:09.311Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:54:18.000Z",
      "submittedOnDailyAt": "2025-04-09T01:08:45.803Z",
      "title": "Evaluación de modelos de lenguaje grandes que incluyen teorías complejas",
      "submittedOnDailyBy": {
        "_id": "63453f02a05b51f7ded3c579",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/u7p-wTj8zyeWpSFRI8Go0.png",
        "isPro": false,
        "fullname": "Andy Lin",
        "user": "pkuHaowei",
        "type": "user"
      },
      "summary": "Los problemas importantes que surgen al demostrar que los modelos de lenguaje grande (LLMs) pueden demostrar habilidades de lógica de un sabio son cruciales: ¿No son los LLMs realmente ejecutando lógica, o simplemente recreando respuestas a partir de un vasto conjunto de datos de entrenamiento scrapeados de la web? Los marcos de evaluación públicos se ven inevitablemente contaminados cuando se incluyen en los conjuntos de datos de entrenamiento de los LLMs posteriores, lo que impide una evaluación verdadera y confiable. Para enfrentar esto, se presenta un marco de evaluación generativo diseñado específicamente para la lógica, llamado KUMO. KUMO combina los LLMs y un motor de símbolos de manera colaborativa, generando tareas de lógica de dificultad variable y observable parcialmente en diferentes etapas de manera dinámica. Mediante una pipeline de automatización, KUMO continúa generando nuevas tareas en el ámbito abierto, forzando a los modelos a demostrar su capacidad de generalización en lugar de simplemente mostrar memoria. KUMO evaluó 23 modelos avanzados de LLMs en 5,000 tareas de 100 áreas, comparándolos con la capacidad de lógica de estudiantes universitarios. Los resultados indican que muchos LLMs superan el nivel de rendimiento universitario en tareas de lógica sencilla, y los LLMs especializados en lógica alcanzan el nivel universitario en desafíos de lógica compleja. Además, el rendimiento de los LLMs en las tareas de KUMO se correlaciona fuertemente con los resultados de los nuevos benchmarks de lógica realmente prácticos, destacando el valor de KUMO como un poderoso y perpetuo instrumento de evaluación para la capacidad lógica de los LLMs.",
      "upvotes": 8,
      "discussionId": "67f099e1103cb604facd280e",
      "githubRepo": "https://github.com/linhaowei1/kumo",
      "ai_keywords": [
        "large language models (LLMs)",
        "superhuman reasoning capabilities",
        "web-scraped training datasets",
        "generative evaluation framework",
        "symbolic engines",
        "multi-turn reasoning tasks",
        "partially observable",
        "adjustable in difficulty",
        "automated pipeline",
        "open-ended domains",
        "genuine generalization",
        "memorization",
        "reasoning abilities",
        "reasoning-scaled LLMs",
        "university-level performance",
        "complex reasoning challenges",
        "real-world reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-04-03T13:54:18.000Z",
    "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
    "summary": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02810.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "63453f02a05b51f7ded3c579",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/u7p-wTj8zyeWpSFRI8Go0.png",
      "fullname": "Andy Lin",
      "name": "pkuHaowei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05594",
      "authors": [
        {
          "_id": "67f5dc86015730c161ce291b",
          "name": "Qi Mao",
          "hidden": false
        },
        {
          "_id": "67f5dc86015730c161ce291c",
          "name": "Lan Chen",
          "hidden": false
        },
        {
          "_id": "67f5dc86015730c161ce291d",
          "name": "Yuchao Gu",
          "hidden": false
        },
        {
          "_id": "67f5dc86015730c161ce291e",
          "name": "Mike Zheng Shou",
          "hidden": false
        },
        {
          "_id": "67f5dc86015730c161ce291f",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-08T01:02:50.000Z",
      "submittedOnDailyAt": "2025-04-09T01:06:41.710Z",
      "title": "Modelo de difusión potencial unificado por edición y posibilidad de edición evitando ajustes en imágenes",
      "submittedOnDailyBy": {
        "_id": "640d704c8036cc2142299c19",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
        "isPro": false,
        "fullname": "Lan Chen",
        "user": "Orannue",
        "type": "user"
      },
      "summary": "Mantener un equilibrio entre fidelidad y posibilidad de edición es crucial en el editor de imágenes basado en texto (TIE). La falta de éxito suele surgir debido a problemas de sobre edición o edición insuficiente. Los métodos existentes generalmente prestan atención a la preservación de la estructura y utilizan las habilidades de arreglo de texto de modelos de conversión de texto a imágenes (T2I) preentrenados para ampliar el rango de posibilidades de edición, pero carecen de una clara forma de lograr un equilibrio preciso entre estos dos objetivos. En este artículo, presentamos UnifyEdit, una metodología sin necesidad de ajustes, que utiliza la optimización de variables potenciales para implementar dentro de un marco de unidad el equilibrio entre estructura y posibilidad de edición. En lugar de darle atención directamente, desarrollamos dos restricciones basadas en atención: la restricción de conservación de la auto-atención (SA) para mantener la estructura y la restricción de arreglo de la criativa atención (CA) para mejorar la posibilidad de edición. Sin embargo, aplicar ambas restricciones simultáneamente puede llevar a conflictos de gradiente y, según la prioridad asignada a una de ellas, a problemas de sobre edición o edición insuficiente. Para enfrentar estas desafíos, introducimos un scheduler de pasos de tiempo adaptativo que permite ajustar la influencia de estas restricciones y guia la optimización potencial a un equilibrio adecuado. Mediante evaluaciones diversas y experimentos de calidad, demostramos los efectos de nuestro enfoque, que mantiene fuertemente la preservación de la estructura y el equilibrio entre el arreglo de texto, demostrando resultados superiores a los métodos de referencia. El código fuente está disponible en https://github.com/CUC-MIPG/UnifyEdit.",
      "upvotes": 7,
      "discussionId": "67f5dc89015730c161ce2a50"
    },
    "publishedAt": "2025-04-07T21:02:50.000Z",
    "title": "Tuning-Free Image Editing with Fidelity and Editability via Unified\n  Latent Diffusion Model",
    "summary": "Balancing fidelity and editability is essential in text-based image editing\n(TIE), where failures commonly lead to over- or under-editing issues. Existing\nmethods typically rely on attention injections for structure preservation and\nleverage the inherent text alignment capabilities of pre-trained text-to-image\n(T2I) models for editability, but they lack explicit and unified mechanisms to\nproperly balance these two objectives. In this work, we introduce UnifyEdit, a\ntuning-free method that performs diffusion latent optimization to enable a\nbalanced integration of fidelity and editability within a unified framework.\nUnlike direct attention injections, we develop two attention-based constraints:\na self-attention (SA) preservation constraint for structural fidelity, and a\ncross-attention (CA) alignment constraint to enhance text alignment for\nimproved editability. However, simultaneously applying both constraints can\nlead to gradient conflicts, where the dominance of one constraint results in\nover- or under-editing. To address this challenge, we introduce an adaptive\ntime-step scheduler that dynamically adjusts the influence of these\nconstraints, guiding the diffusion latent toward an optimal balance. Extensive\nquantitative and qualitative experiments validate the effectiveness of our\napproach, demonstrating its superiority in achieving a robust balance between\nstructure preservation and text alignment across various editing tasks,\noutperforming other state-of-the-art methods. The source code will be available\nat https://github.com/CUC-MIPG/UnifyEdit.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05594.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d704c8036cc2142299c19",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
      "fullname": "Lan Chen",
      "name": "Orannue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.06261",
      "authors": [
        {
          "_id": "67f60df2d0df7eccaae93eb0",
          "name": "Gleb Rodionov",
          "hidden": false
        },
        {
          "_id": "67f60df2d0df7eccaae93eb1",
          "name": "Roman Garipov",
          "hidden": false
        },
        {
          "_id": "67f60df2d0df7eccaae93eb2",
          "name": "Alina Shutova",
          "hidden": false
        },
        {
          "_id": "67f60df2d0df7eccaae93eb3",
          "name": "George Yakushev",
          "hidden": false
        },
        {
          "_id": "67f60df2d0df7eccaae93eb4",
          "name": "Vage Egiazarian",
          "hidden": false
        },
        {
          "_id": "67f60df2d0df7eccaae93eb5",
          "name": "Anton Sinitsin",
          "hidden": false
        },
        {
          "_id": "67f60df2d0df7eccaae93eb6",
          "name": "Denis Kuznedelev",
          "hidden": false
        },
        {
          "_id": "67f60df2d0df7eccaae93eb7",
          "name": "Dan Alistarh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ef52c2718f94ae8e78a5e7/79i2ecMWxf7tpYS3hvRP4.qt"
      ],
      "publishedAt": "2025-04-08T17:59:41.000Z",
      "submittedOnDailyAt": "2025-04-09T04:36:08.129Z",
      "title": "Hoguvild! Inferencia: Attención paralela mediante la generación de LLMs paralelos",
      "submittedOnDailyBy": {
        "_id": "64ef52c2718f94ae8e78a5e7",
        "avatarUrl": "/avatars/d169f4ee62786a3eb4a3fa9d1fec52e9.svg",
        "isPro": false,
        "fullname": "Alistarh",
        "user": "d-alistarh",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) muestran capacidades avanzadas, como la generación de contenido de documentos largos y la ejecución de tareas cada vez más complejas a través de herramientas. Para resolver estas tareas, se requiere tiempo de cálculo significativo. En la resolución de problemas humanos, una estrategia común para acelerar la velocidad de trabajo es dividir el problema en partes y probar estrategias diferentes en paralelo. Recientes estudios han demostrado que LLMs también pueden implementar estructuras explícitas de colaboración para procesar tareas en paralelo, como votos o la definición de tareas independientes. Sin embargo, estos marcos no son adecuados para todas las tareas. En este estudio, proponemos una nueva aproximación: ejecutar múltiples \"trabajadores\" de LLM en paralelo, sincronizarlos mediante un cache de atención que se actualiza en paralelo, y decidir la mejor colaboración para resolver el problema. Nuestro enfoque permite que la estrategia de colaboración se determine automáticamente y ofrece una visión de cómo se pueden realizar las tareas en paralelo. Este enfoque se implementa en la Inferencia Hogwild!, que ejecuta múltiples instancias de un mismo LLM con el mismo cache de atención en paralelo, proporcionando acceso simultáneo a los tokens generados. La Inferencia Hogwild! utiliza Rotary Position Embeddings (RoPE) para evitar recalculaciones y maximizar el uso de hardware paralelo. Hemos descubierto que LLMs con capacidades modernas pueden realizar inferencias finales sin necesidad de ajustes adicionales, utilizando un cache compartido de Key-Value.",
      "upvotes": 6,
      "discussionId": "67f60df3d0df7eccaae93eff"
    },
    "publishedAt": "2025-04-08T13:59:41.000Z",
    "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
    "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ef52c2718f94ae8e78a5e7/79i2ecMWxf7tpYS3hvRP4.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06261.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ef52c2718f94ae8e78a5e7",
      "avatarUrl": "/avatars/d169f4ee62786a3eb4a3fa9d1fec52e9.svg",
      "fullname": "Alistarh",
      "name": "d-alistarh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00043",
      "authors": [
        {
          "_id": "67ec9d4ad327ed17ec707488",
          "name": "Jixuan Leng",
          "hidden": false
        },
        {
          "_id": "67ec9d4ad327ed17ec707489",
          "name": "Chengsong Huang",
          "hidden": false
        },
        {
          "_id": "67ec9d4ad327ed17ec70748a",
          "name": "Langlin Huang",
          "hidden": false
        },
        {
          "_id": "67ec9d4ad327ed17ec70748b",
          "name": "Bill Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "67ec9d4ad327ed17ec70748c",
          "name": "William W. Cohen",
          "hidden": false
        },
        {
          "_id": "67ec9d4ad327ed17ec70748d",
          "name": "Haohan Wang",
          "hidden": false
        },
        {
          "_id": "67ec9d4ad327ed17ec70748e",
          "name": "Jiaxin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T20:03:36.000Z",
      "submittedOnDailyAt": "2025-04-09T00:47:44.460Z",
      "title": "Cross-validation guidance: Generación de rompecabezas controlables para la evaluación de capacidades de inferencia en LLMs y LVLMs",
      "submittedOnDailyBy": {
        "_id": "64efbf39b3610349e84db417",
        "avatarUrl": "/avatars/9e09a20e88f8cf5ce119efc0dadc3b7b.svg",
        "isPro": false,
        "fullname": "Jiaxin Huang",
        "user": "teapot123",
        "type": "user"
      },
      "summary": "Actualmente, el marco de evaluación lógico para los grandes modelos de lenguaje (LLMs) y los grandes modelos de visión y lenguaje (LVLMs) se centra principalmente en la evaluación lógica basada en documentos y en la comprensión del lenguaje visual, con una interacción dinámica entre documentos y visión limitada. Para resolver estas limitaciones, se introduce CrossWordBench, un marco de evaluación que avalía los LLMs y LVLMs mediante el desarrollo de juegos de palabras, integrando evaluaciones en varios modos que incluyen el test de Turing basado en documentos y las restricciones de grilla visual. CrossWordBench utiliza un marco de generación de juegos de palabras controlables para crear juegos de palabras en diferentes formatos de texto e imagen, ofreciendo estrategias de evaluación desde la resolución directa del juego hasta el modo interactivo. Mediante 20 evaluaciones extendidas, se ha demostrado que los LLMs que evalúan lógicamente pueden utilizar efectivamente las restricciones de palabras cruzadas para evaluar significativamente a modelos que no evalúan lógicamente, mientras que los LVLMs muestran una excelente capacidad para resolver juegos de palabras y una precisión alta en la extracción de palabras de grilla, asociadas con una fuerte correlación. Estos hallazgos muestran las limitaciones actuales en la capacidad de evaluación lógica de los LLMs y LVLMs y proporcionan una aproximación efectiva para futuras tareas de evaluación con restricciones multimodales.",
      "upvotes": 5,
      "discussionId": "67ec9d4fd327ed17ec707598",
      "ai_keywords": [
        "CrossWordBench",
        "multimodal adherence",
        "semantic constraints",
        "intersectional constraints",
        "controllable puzzle generation framework",
        "direct puzzle solving",
        "interactive modes",
        "reasoning LLMs",
        "non-reasoning models",
        "crossing-letter constraints",
        "grid-parsing accuracy",
        "multimodal constrained tasks"
      ]
    },
    "publishedAt": "2025-03-30T16:03:36.000Z",
    "title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs\n  with Controllable Puzzle Generation",
    "summary": "Existing reasoning evaluation frameworks for Large Language Models (LLMs) and\nLarge Vision-Language Models (LVLMs) predominantly either assess text-based\nreasoning or vision-language understanding capabilities, with limited dynamic\ninterplay between textual and visual constraints. To address this limitation,\nwe introduce CrossWordBench, a benchmark designed to evaluate the reasoning\ncapabilities of both LLMs and LVLMs through the medium of crossword puzzles-a\ntask requiring multimodal adherence to semantic constraints from text-based\nclues and intersectional constraints from visual grid structures.\nCrossWordBench leverages a controllable puzzle generation framework that\nproduces puzzles in multiple formats (text and image) and offers different\nevaluation strategies ranging from direct puzzle solving to interactive modes.\nOur extensive evaluation of over 20 models reveals that reasoning LLMs\noutperform non-reasoning models substantially by effectively leveraging\ncrossing-letter constraints. We further demonstrate that LVLMs struggle with\nthe task, showing a strong correlation between their puzzle-solving performance\nand grid-parsing accuracy. Our findings offer insights into the limitations of\nthe reasoning capabilities of current LLMs and LVLMs, and provide an effective\napproach for creating multimodal constrained tasks for future evaluations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00043.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64efbf39b3610349e84db417",
      "avatarUrl": "/avatars/9e09a20e88f8cf5ce119efc0dadc3b7b.svg",
      "fullname": "Jiaxin Huang",
      "name": "teapot123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.06148",
      "authors": [
        {
          "_id": "67f6310fe30d3e5d13a9cbfc",
          "user": {
            "_id": "673deee2afdcf84dddf74827",
            "avatarUrl": "/avatars/d2e051ddef816342aa52b98ded109e66.svg",
            "isPro": false,
            "fullname": "XxZheng",
            "user": "Fengx1nn",
            "type": "user"
          },
          "name": "Xiangxi Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T09:48:14.241Z",
          "hidden": false
        },
        {
          "_id": "67f6310fe30d3e5d13a9cbfd",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "67f6310fe30d3e5d13a9cbfe",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "67f6310fe30d3e5d13a9cbff",
          "name": "Ping Yu",
          "hidden": false
        },
        {
          "_id": "67f6310fe30d3e5d13a9cc00",
          "name": "Alex Jinpeng Wang",
          "hidden": false
        },
        {
          "_id": "67f6310fe30d3e5d13a9cc01",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "67f6310fe30d3e5d13a9cc02",
          "name": "Yuan Yao",
          "hidden": false
        },
        {
          "_id": "67f6310fe30d3e5d13a9cc03",
          "name": "Lijuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-08T15:43:01.000Z",
      "submittedOnDailyAt": "2025-04-09T07:04:38.598Z",
      "title": "V-MAGE: Evaluación de habilidades centradas en la visión para un marco de evaluación de juegos con un modelo de lenguaje de gran escala multimodal",
      "submittedOnDailyBy": {
        "_id": "62333a88fd7bb4a39b92d387",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
        "isPro": false,
        "fullname": "Alex Jinpeng Wang",
        "user": "Awiny",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los lenguajes multimodal (MLLM) ha demostrado notables mejoras en los marcos de evaluación de multimodalidad. Sin embargo, la evaluación se ha movido desde conjuntos de datos estáticos hacia un mundo abierto y dinámico, y los actuales marcos de evaluación basados en juegos se han centrado en tareas visuales, lo que ha limitado la evaluación de habilidades de razonamiento necesarias para tomar decisiones en el mundo real. En respuesta a esto, presentamos la Evaluación de Razonamiento Visual Multifuncional (V-MAGE). V-MAGE es un marco de evaluación basado en juegos para la capacidad de razonamiento visual de los MLLM. V-MAGE se caracteriza por 5 juegos diferentes y más de 30 niveles handcraft, y evalúa en los modelos habilidades clave como datos de ubicación, seguimiento de trayectorias, tiempo y memoria visual. Además, evalúa niveles más altos de razonamiento, incluyendo planificación a largo plazo y razonamiento detallado. Usando V-MAGE, hemos evaluado los MLLM avanzados y revelado importantes problemas en la percepción visual y el razonamiento. En todos los entornos de juego, los MLLM con mejores rendimientos, comparados con los humanos, muestran significativas diferencias de rendimiento. Nuestros hallazgos muestran limitaciones importantes del modelo, como el tipo de errores que produce, y ofrece posibilidades de mejora en proyectos de salida. El código está disponible en https://github.com/CSU-JPG/V-MAGE.",
      "upvotes": 4,
      "discussionId": "67f63111e30d3e5d13a9cc85"
    },
    "publishedAt": "2025-04-08T11:43:01.000Z",
    "title": "V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric\n  Capabilities in Multimodal Large Language Models",
    "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have led to\nsignificant improvements across various multimodal benchmarks. However, as\nevaluations shift from static datasets to open-world, dynamic environments,\ncurrent game-based benchmarks remain inadequate because they lack\nvisual-centric tasks and fail to assess the diverse reasoning skills required\nfor real-world decision-making. To address this, we introduce Visual-centric\nMultiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework\ndesigned to assess visual reasoning capabilities of MLLMs. V-MAGE features five\ndiverse games with 30+ handcrafted levels, testing models on core visual skills\nsuch as positioning, trajectory tracking, timing, and visual memory, alongside\nhigher-level reasoning like long-term planning and deliberation. We use V-MAGE\nto evaluate leading MLLMs, revealing significant challenges in their visual\nperception and reasoning. In all game environments, the top-performing MLLMs,\nas determined by Elo rating comparisons, exhibit a substantial performance gap\ncompared to humans. Our findings highlight critical limitations, including\nvarious types of perceptual errors made by the models, and suggest potential\navenues for improvement from an agent-centric perspective, such as refining\nagent strategies and addressing perceptual inaccuracies. Code is available at\nhttps://github.com/CSU-JPG/V-MAGE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06148.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20533",
      "authors": [
        {
          "_id": "67f62f3a28b4852d4761e842",
          "name": "Yijiong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T13:28:57.000Z",
      "submittedOnDailyAt": "2025-04-09T06:58:23.443Z",
      "title": "En un cuadruple, se acelera el lógica paralelamente ejecutable mediante decodificación paralela.",
      "submittedOnDailyBy": {
        "_id": "6374c494958cd71fa7ea0a9d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6374c494958cd71fa7ea0a9d/2YCKv6tXCZXtsIOFIIXjs.png",
        "isPro": false,
        "fullname": "yuyijiong",
        "user": "yuyijiong",
        "type": "user"
      },
      "summary": "El desarrollo reciente del modelo de razonamiento ha demostrado una mejora importante en la precisión de tareas complejas. En particular, se ha observado un gran aumento en la precisión al tratar problemas complejos como modelos matemáticos, utilizando procesos de razonamiento detallados y integrales. Sin embargo, la generación de largas secuencias de razonamiento requiere una gran cantidad de cálculos y tiempo. Para mejorar esta eficiencia, se utiliza la posibilidad de paralelización específica para tareas, lo que permite acelerar el proceso de razonamiento. En particular, cuando existen varias ramas de razonamiento paralelas, se utilizan mascaras de atención especializadas para verificar múltiples tokens en un solo paso, procesando una secuencia en su totalidad y evitando el uso de memoria adicional. A través de los resultados de los experimentos, se ha demostrado que nuestro método mantiene la calidad de la respuesta mientras logra un aumento de velocidad del 100% o más en el tiempo de verificación.",
      "upvotes": 3,
      "discussionId": "67f62f3b28b4852d4761e87c"
    },
    "publishedAt": "2025-03-26T09:28:57.000Z",
    "title": "Accelerate Parallelizable Reasoning via Parallel Decoding within One\n  Sequence",
    "summary": "Recent advances in reasoning models have demonstrated significant\nimprovements in accuracy, particularly for complex tasks such as mathematical\nreasoning, by employing detailed and comprehensive reasoning processes.\nHowever, generating these lengthy reasoning sequences is computationally\nexpensive and time-consuming. To address this inefficiency, we leverage the\ninherent parallelizability of certain tasks to accelerate the reasoning\nprocess. Specifically, when multiple parallel reasoning branches exist, we\ndecode multiple tokens per step using a specialized attention mask, processing\nthem within a single sequence, avoiding additional memory usage. Experimental\nresults show that our method achieves over 100% speedup in decoding time while\nmaintaining the answer quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20533.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6374c494958cd71fa7ea0a9d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6374c494958cd71fa7ea0a9d/2YCKv6tXCZXtsIOFIIXjs.png",
      "fullname": "yuyijiong",
      "name": "yuyijiong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.06232",
      "authors": [
        {
          "_id": "67f6406e49525c856f4705c4",
          "name": "Jiazi Bu",
          "hidden": false
        },
        {
          "_id": "67f6406e49525c856f4705c5",
          "name": "Pengyang Ling",
          "hidden": false
        },
        {
          "_id": "67f6406e49525c856f4705c6",
          "name": "Yujie Zhou",
          "hidden": false
        },
        {
          "_id": "67f6406e49525c856f4705c7",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "67f6406e49525c856f4705c8",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "67f6406e49525c856f4705c9",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "67f6406e49525c856f4705ca",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "67f6406e49525c856f4705cb",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "67f6406e49525c856f4705cc",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67f6406e49525c856f4705cd",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-08T17:30:40.000Z",
      "submittedOnDailyAt": "2025-04-09T08:11:30.876Z",
      "title": "HiFlow: No se necesita entrenamiento para la generación de imágenes de alta resolución, solo se proporcionan mapas según el flujo.",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "Los modelos de difusión (T2I) o flujo para la generación de imágenes han recibido mucha atención recientemente debido a su capacidad para generar imágenes adaptables. Sin embargo, la síntesis de imágenes de alta resolución se enfrenta a grandes desafíos debido a la escasez y la complejidad del contenido de alta resolución. En este contexto, proponemos un marco independiente del modelo llamado HiFlow, que no requiere entrenamiento adicional y que busca desarrollar el potencial de los modelos de flujo entrenados previamente. Específicamente, HiFlow construye un flujo de referencia virtual en el espacio de alta resolución y captura efectivamente las características del flujo de baja resolución, proporcionando guías en tres aspectos importantes para la generación de alta resolución. Estos aspectos incluyen la posición inicial de la coincidencia de baja frecuencia, la conservación de la estructura mediante la posición de dirección y la precisión de detalles mediante la posición de aceleración. Al utilizar estas configuraciones de flujo, HiFlow mejora significativamente la calidad de la síntesis de imágenes de alta resolución en modelos T2I y muestra una amplia gama de funcionalidades. Los experimentos extendidos demuestran que HiFlow supera los métodos actuales de alta resolución en cuanto a la calidad de las imágenes.",
      "upvotes": 2,
      "discussionId": "67f6407349525c856f470733"
    },
    "publishedAt": "2025-04-08T13:30:40.000Z",
    "title": "HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned\n  Guidance",
    "summary": "Text-to-image (T2I) diffusion/flow models have drawn considerable attention\nrecently due to their remarkable ability to deliver flexible visual creations.\nStill, high-resolution image synthesis presents formidable challenges due to\nthe scarcity and complexity of high-resolution content. To this end, we present\nHiFlow, a training-free and model-agnostic framework to unlock the resolution\npotential of pre-trained flow models. Specifically, HiFlow establishes a\nvirtual reference flow within the high-resolution space that effectively\ncaptures the characteristics of low-resolution flow information, offering\nguidance for high-resolution generation through three key aspects:\ninitialization alignment for low-frequency consistency, direction alignment for\nstructure preservation, and acceleration alignment for detail fidelity. By\nleveraging this flow-aligned guidance, HiFlow substantially elevates the\nquality of high-resolution image synthesis of T2I models and demonstrates\nversatility across their personalized variants. Extensive experiments validate\nHiFlow's superiority in achieving superior high-resolution image quality over\ncurrent state-of-the-art methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06232.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  }
]