[
  {
    "paper": {
      "id": "2506.23044",
      "authors": [
        {
          "_id": "686347cd588cea0da970c87a",
          "user": {
            "_id": "636f4c6b5d2050767e4a1491",
            "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
            "isPro": false,
            "fullname": "Guo-Hua Wang",
            "user": "Flourish",
            "type": "user"
          },
          "name": "Guo-Hua Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:51.516Z",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87b",
          "name": "Shanshan Zhao",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87c",
          "name": "Xinjie Zhang",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87d",
          "name": "Liangfu Cao",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87e",
          "name": "Pengxin Zhan",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87f",
          "name": "Lunhao Duan",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c880",
          "name": "Shiyin Lu",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c881",
          "name": "Minghao Fu",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c882",
          "name": "Xiaohao Chen",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c883",
          "name": "Jianshan Zhao",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c884",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c885",
          "name": "Qing-Guo Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-29T00:40:17.000Z",
      "submittedOnDailyAt": "2025-07-01T04:30:09.856Z",
      "title": "Informe Técnico de Óbis-U1",
      "submittedOnDailyBy": {
        "_id": "636f4c6b5d2050767e4a1491",
        "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
        "isPro": false,
        "fullname": "Guo-Hua Wang",
        "user": "Flourish",
        "type": "user"
      },
      "summary": "En este informe, se presenta el modelo integrado Ovis-U1 con 3 biliones de parámetros. Este modelo integra comprensión de diferentes tipos, generación de imágenes a partir de texto y edición de imágenes. Basado en la serie Ovis, Ovis-U1 combina un decodificador visual basado en difusión y un rinfier de tokens bi-direccional para permitir tareas de generación de imágenes al nivel de GPT-4o. A diferencia de modelos anteriores, Ovis-U1 no utiliza una MLLM fija para tareas de generación. En lugar de eso, utiliza una nueva aproximación de entrenamiento integrado que comienza con un modelo de lenguaje. Al compararse con modelos entrenados solo para tareas de comprensión y generación de lenguaje, el entrenamiento integrado muestra un mejor rendimiento y demuestra los beneficios de la integración de estas dos tareas. Ovis-U1 alcanzó un puntaje de 69.6 en el benchmark multimodal de OpenCompass, superando modelos recientes como Ristretto-3B y SAIL-VL-1.5-2B. En la generación de imágenes a partir de texto, alcanzó puntajes de 83.72 en DPG-Bench y 0.89 en GenEval, y en edición de imágenes, obtuvo puntajes de 4.00 en ImgEdit-Bench y 6.42 en GEdit-Bench-EN. Como versión inicial de la serie de modelos Ovis, Ovis-U1 supera los límites de comprensión, generación y edición de imágenes.",
      "upvotes": 30,
      "discussionId": "686347cd588cea0da970c886",
      "githubRepo": "https://github.com/AIDC-AI/Ovis-U1",
      "ai_summary": "Ovis-U1, a 3-billion-parameter model, combines multimodal understanding, text-to-image generation, and image editing, achieving state-of-the-art performance in various benchmarks.",
      "ai_keywords": [
        "diffusion-based visual decoder",
        "bidirectional token refiner",
        "unified training",
        "OpenCompass",
        "DPG-Bench",
        "GenEval",
        "ImgEdit-Bench",
        "GEdit-Bench-EN"
      ],
      "githubStars": 137
    },
    "publishedAt": "2025-06-28T20:40:17.000Z",
    "title": "Ovis-U1 Technical Report",
    "summary": "In this report, we introduce Ovis-U1, a 3-billion-parameter unified model\nthat integrates multimodal understanding, text-to-image generation, and image\nediting capabilities. Building on the foundation of the Ovis series, Ovis-U1\nincorporates a diffusion-based visual decoder paired with a bidirectional token\nrefiner, enabling image generation tasks comparable to leading models like\nGPT-4o. Unlike some previous models that use a frozen MLLM for generation\ntasks, Ovis-U1 utilizes a new unified training approach starting from a\nlanguage model. Compared to training solely on understanding or generation\ntasks, unified training yields better performance, demonstrating the\nenhancement achieved by integrating these two tasks. Ovis-U1 achieves a score\nof 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent\nstate-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In\ntext-to-image generation, it excels with scores of 83.72 and 0.89 on the\nDPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves\n4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the\ninitial version of the Ovis unified model series, Ovis-U1 pushes the boundaries\nof multimodal understanding, generation, and editing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23044.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "636f4c6b5d2050767e4a1491",
      "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
      "fullname": "Guo-Hua Wang",
      "name": "Flourish",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23858",
      "authors": [
        {
          "_id": "686347d3588cea0da970c888",
          "name": "Jianzong Wu",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c889",
          "user": {
            "_id": "64560a2aaaaf85a98fa9a4b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560a2aaaaf85a98fa9a4b9/2Kp0S0sMVpKqo81s-l_Yt.png",
            "isPro": false,
            "fullname": "Liang Hou",
            "user": "lianghou",
            "type": "user"
          },
          "name": "Liang Hou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:49.312Z",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88a",
          "name": "Haotian Yang",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88b",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88c",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88d",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88e",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88f",
          "name": "Yunhai Tong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T13:52:31.000Z",
      "submittedOnDailyAt": "2025-07-01T00:59:37.837Z",
      "title": "VMoBA: Modelo de Difusión de Vídeo Utilizando la Mezcla y Síntesis de Bloques y Atención de Bloque",
      "submittedOnDailyBy": {
        "_id": "657a6eed1ccc3c2a5ea7b585",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RIQIF-JJdNI0SwJEq_9z7.jpeg",
        "isPro": true,
        "fullname": "Jianzong Wu",
        "user": "jianzongwu",
        "type": "user"
      },
      "summary": "La complejidad de dos grados del mecanismo de atención total es un gran límite para modelos de difusión de vídeo (VDMs) que generan vídeos largos y de alta resolución. Se han propuesto diversos métodos de atención rara, pero muchos están diseñados para acelerar la inferencia sin relación con el entrenamiento, o no pueden detectar características temporales-espaciales únicas en datos de vídeo nativo. En este artículo, se presenta un nuevo mecanismo de atención rara diseñado específicamente para VDMs: el Video Block Mixture Attention (VMoBA). Basado en un análisis profundo de los patrones de atención de transformadores de vídeo entrenados, este análisis revela una fuerte localización temporal-espacial, la importancia de la query cambiante y el nivel de concentración de los cabezas de atención. VMoBA fortalece el marco original de MoBA con tres principales modificaciones: (1) un plan de división de bloques recursivos en tres dimensiones (1D-2D-3D), que adapta eficientemente y dinámicamente diferentes patrones de atención temporal-espacial; (2) selección de bloques globales, que prioriza la interacción de bloques entre query y key en las cabezas de atención más significativas; (3) selección de bloques basada en umbrales, que determina dinámicamente la cantidad de bloques que participan basándose en la similitud acumulada. Experimentos extensos muestran que VMoBA acelera significativamente el entrenamiento de VDMs, logrando una velocidad de FLOPs de 2.92 veces y una reducción del retraso de 1.48 veces, al igual que alcanzando calidades de generación comparables o mejores que la atención completa. Además, VMoBA muestra un rendimiento competitivo en inferencia sin entrenamiento y proporciona una velocidad de FLOPs de 2.40 veces y una reducción del retraso de 1.35 veces en la generación de vídeos de alta resolución.",
      "upvotes": 22,
      "discussionId": "686347d3588cea0da970c890",
      "projectPage": "https://github.com/KwaiVGI/VMoBA",
      "githubRepo": "https://github.com/KwaiVGI/VMoBA",
      "ai_summary": "VMoBA, a novel sparse attention mechanism for Video Diffusion Models (VDMs), accelerates training and inference by addressing the quadratic complexity of full attention mechanisms while maintaining or improving video generation quality.",
      "ai_keywords": [
        "quadartic complexity",
        "full attention mechanisms",
        "Video Diffusion Models",
        "VDMs",
        "sparse attention methods",
        "in-depth analysis",
        "attention patterns",
        "video transformers",
        "spatio-temporal locality",
        "query importance",
        "head-specific concentration",
        "layer-wise recurrent block partition scheme",
        "global block selection",
        "threshold-based block selection",
        "FLOPs",
        "latency",
        "high-res video generation"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-06-30T09:52:31.000Z",
    "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models",
    "summary": "The quadratic complexity of full attention mechanisms poses a significant\nbottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,\nhigh-resolution videos. While various sparse attention methods have been\nproposed, many are designed as training-free inference accelerators or do not\noptimally capture the unique spatio-temporal characteristics inherent in video\ndata when trained natively. This paper introduces Video Mixture of Block\nAttention (VMoBA), a novel sparse attention mechanism specifically adapted for\nVDMs. Motivated by an in-depth analysis of attention patterns within\npre-trained video transformers, which revealed strong spatio-temporal locality,\nvarying query importance, and head-specific concentration levels, VMoBA\nenhances the original MoBA framework with three key modifications: (1) a\nlayer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to\ndiverse spatio-temporal attention patterns and improve efficiency; (2) global\nblock selection to prioritize the most salient query-key block interactions\nacross an entire attention head; and (3) threshold-based block selection to\ndynamically determine the number of attended blocks based on their cumulative\nsimilarity. Extensive experiments demonstrate that VMoBA significantly\naccelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and\n1.48x latency speedup, while attaining comparable or even superior generation\nquality to full attention. Furthermore, VMoBA exhibits competitive performance\nin training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for\nhigh-res video generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23858.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a6eed1ccc3c2a5ea7b585",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RIQIF-JJdNI0SwJEq_9z7.jpeg",
      "fullname": "Jianzong Wu",
      "name": "jianzongwu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.24123",
      "authors": [
        {
          "_id": "68634673588cea0da970c862",
          "name": "Yue Ma",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c863",
          "name": "Qingyan Bai",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c864",
          "name": "Hao Ouyang",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c865",
          "name": "Ka Leong Cheng",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c866",
          "name": "Qiuyu Wang",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c867",
          "name": "Hongyu Liu",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c868",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c869",
          "name": "Haofan Wang",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c86a",
          "user": {
            "_id": "6478a982256b62e219917d67",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PUJ-N2cQxgEmDGfyjajyA.jpeg",
            "isPro": false,
            "fullname": "JingyeChen22",
            "user": "JingyeChen22",
            "type": "user"
          },
          "name": "Jingye Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:14:05.849Z",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c86b",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c86c",
          "name": "Qifeng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T17:59:06.000Z",
      "submittedOnDailyAt": "2025-07-01T01:00:32.385Z",
      "title": "El Karakchi: Personalización de imágenes de tecnología de estilo prestado\n\n(Nota: La traducción se ha realizado manteniendo la profundidad y la precisión del original, asegurando que el texto sea coherente y técnico en español.)",
      "submittedOnDailyBy": {
        "_id": "63f0baf66309c84d5f4a2226",
        "avatarUrl": "/avatars/a122f7d92441bd2feef7d4eda993fab7.svg",
        "isPro": false,
        "fullname": "Meme155",
        "user": "Meme145",
        "type": "user"
      },
      "summary": "El Caligrafía introduce un nuevo marco de trabajo basado en diferenciación. Este marco integra de manera evolutiva el mejoramiento del texto a nivel alto y el diseño artístico del texto, proponiendo nuevas ideas para su aplicación en el diseño digital y aplicaciones gráficas. Para resolver la generalización y la dependencia de datos en el diseño de texto, el marco ofrece tres contribuciones tecnológicas principales. Primero, utiliza modelos generados de imágenes a partir de texto previamente entrenado y grandes modelos de lenguaje para desarrollar una estructura de autoaprendizaje que permite la creación automática de marcos de referencia de texto estilístico. Segundo, introduce un marco de estilo localmente codificado a través de un codificador estilístico entrenable, extrayendo fuertes características estilísticas de imágenes de referencia. Este marco está constituido por Qformer y capas lineales. Además, el marco utiliza una estructura de generación de texto que inserta imágenes de referencia directamente en el proceso de desinfección, promoviendo una precisa coincidencia con el estilo objetivo. Caligrafía evalúa ampliamente, tanto cuantitativamente como cualitativamente, en una amplia gama de fuentes y contextos de diseño, precisamente recreando los detalles complejos de un estilo y determinando con exactitud la posición de las letras. Al automatizar el diseño de texto de alta calidad y visualmente cohesivo, Caligrafía supera los modelos tradicionales y confiere fuerza y creatividad a los practicantes modernos en el arte digital, branding y diseño de texto de contenido.",
      "upvotes": 20,
      "discussionId": "68634673588cea0da970c86d",
      "projectPage": "https://calligrapher2025.github.io/Calligrapher/",
      "githubRepo": "https://github.com/Calligrapher2025/Calligrapher",
      "ai_summary": "Calligrapher uses a diffusion-based framework with self-distillation and localized style injection to generate high-quality, stylistically consistent digital typography.",
      "ai_keywords": [
        "diffusion-based framework",
        "self-distillation mechanism",
        "text-to-image generative model",
        "large language model",
        "localized style injection",
        "Qformer",
        "linear layers",
        "style encoder",
        "in-context generation mechanism",
        "denoising process",
        "stylistic details",
        "glyph positioning"
      ],
      "githubStars": 21
    },
    "publishedAt": "2025-06-30T13:59:06.000Z",
    "title": "Calligrapher: Freestyle Text Image Customization",
    "summary": "We introduce Calligrapher, a novel diffusion-based framework that\ninnovatively integrates advanced text customization with artistic typography\nfor digital calligraphy and design applications. Addressing the challenges of\nprecise style control and data dependency in typographic customization, our\nframework incorporates three key technical contributions. First, we develop a\nself-distillation mechanism that leverages the pre-trained text-to-image\ngenerative model itself alongside the large language model to automatically\nconstruct a style-centric typography benchmark. Second, we introduce a\nlocalized style injection framework via a trainable style encoder, which\ncomprises both Qformer and linear layers, to extract robust style features from\nreference images. An in-context generation mechanism is also employed to\ndirectly embed reference images into the denoising process, further enhancing\nthe refined alignment of target styles. Extensive quantitative and qualitative\nevaluations across diverse fonts and design contexts confirm Calligrapher's\naccurate reproduction of intricate stylistic details and precise glyph\npositioning. By automating high-quality, visually consistent typography,\nCalligrapher surpasses traditional models, empowering creative practitioners in\ndigital art, branding, and contextual typographic design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.24123.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f0baf66309c84d5f4a2226",
      "avatarUrl": "/avatars/a122f7d92441bd2feef7d4eda993fab7.svg",
      "fullname": "Meme155",
      "name": "Meme145",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22832",
      "authors": [
        {
          "_id": "6863989c588cea0da970c985",
          "user": {
            "_id": "6192657ba9638054a9818f04",
            "avatarUrl": "/avatars/35f6f6eb2e8f6b283034632a141c2670.svg",
            "isPro": false,
            "fullname": "Alexander Gambashidze",
            "user": "alexgambashidze",
            "type": "user"
          },
          "name": "Alexander Gambashidze",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:12:55.879Z",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c986",
          "name": "Li Pengyi",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c987",
          "user": {
            "_id": "6626c5d0a329de26e7eb16fa",
            "avatarUrl": "/avatars/124f389f768fb666efd8b5a9b54c3b3c.svg",
            "isPro": false,
            "fullname": "Matvey Skripkin",
            "user": "barracuda049",
            "type": "user"
          },
          "name": "Matvey Skripkin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:12:51.866Z",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c988",
          "name": "Andrey Galichin",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c989",
          "name": "Anton Gusarov",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c98a",
          "name": "Konstantin Sobolev",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c98b",
          "name": "Andrey Kuznetsov",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c98c",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6192657ba9638054a9818f04/IpysW0QkLzIgzWognSV7n.png"
      ],
      "publishedAt": "2025-06-28T09:53:17.000Z",
      "submittedOnDailyAt": "2025-07-01T06:44:35.487Z",
      "title": "Pensamiento de Reconocido en VLMs para Preferencias de Imágenes",
      "submittedOnDailyBy": {
        "_id": "6192657ba9638054a9818f04",
        "avatarUrl": "/avatars/35f6f6eb2e8f6b283034632a141c2670.svg",
        "isPro": false,
        "fullname": "Alexander Gambashidze",
        "user": "alexgambashidze",
        "type": "user"
      },
      "summary": "Entrenar modelos de recompensa fuertes y expandibles que se adapten a las preferencias visuales humanas, así como modelos de generación de texto a imagenes y de texto a videos que se ajusten a los fines humanos, es crucial. Sin embargo, los modelos de recompensa actuales generalmente tienen baja dispersión y requieren una compleja pipeline de notas que dependen de la memoria para realizar ajustes normales. En particular, el aprendizaje por refuerzo (RL) puede mejorar la dispersión utilizando el Group Relative Policy Optimization (GRPO), pero hemos identificado principales modos de fallo: cuando las razones del modelo son independientes y en desacuerdo con las razones de un modelo de lenguaje de visión frío (\"escuchador\"), la precisión de las razones se reduce significativamente. Para resolver esto, proponemos un marco de GRPO con el agregado de un \"escuchador\". En este marco, el \"escuchador\" reevalúa las continuas de errores de las razones y proporciona un score de confianza densificado, formando el mensaje de recompensa del RL. Esto recomienda al modelo de razones para crear explicaciones fuertes independientemente del modelo, asegurando tanto la precisión de las razones como la independencia del modelo. Nuestro recompensa basado en \"escuchador\" alcanza la mayor precisión en el benchmark de ImageReward (67.4%) y mejora significativamente la capacidad de generalización a una amplia base de datos de preferencias humanas (aumento máximo de +6% en comparación con un modelo de razones más natural), mientras reduce la contradicción entre las razones en comparación con GRPO fuerte y SFT. Estos resultados demuestran que el recompensa basada en \"escuchador\" proporciona pasos escalables y eficientes en datos para adaptar modelos de lenguaje de visión y complejos preferencias humanas. Publicamos nuestro modelo de razones: https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.",
      "upvotes": 13,
      "discussionId": "6863989c588cea0da970c98d",
      "ai_summary": "A listener-augmented Group Relative Policy Optimization framework improves reward models by re-evaluating reasoning processes, leading to enhanced accuracy and out-of-distribution performance in aligning vision-language models with human preferences.",
      "ai_keywords": [
        "reinforcement learning",
        "Group Relative Policy Optimization (GRPO)",
        "listener-augmented GRPO",
        "vision-language model",
        "chain-of-thought",
        "image preference reasoning",
        "ImageReward benchmark",
        "out-of-distribution (OOD) performance"
      ]
    },
    "publishedAt": "2025-06-28T05:53:17.000Z",
    "title": "Listener-Rewarded Thinking in VLMs for Image Preferences",
    "summary": "Training robust and generalizable reward models for human visual preferences\nis essential for aligning text-to-image and text-to-video generative models\nwith human intent. However, current reward models often fail to generalize, and\nsupervised fine-tuning leads to memorization, demanding complex annotation\npipelines. While reinforcement learning (RL), specifically Group Relative\nPolicy Optimization (GRPO), improves generalization, we uncover a key failure\nmode: a significant drop in reasoning accuracy occurs when a model's reasoning\ntrace contradicts that of an independent, frozen vision-language model\n(\"listener\") evaluating the same output. To address this, we introduce a\nlistener-augmented GRPO framework. Here, the listener re-evaluates the\nreasoner's chain-of-thought to provide a dense, calibrated confidence score,\nshaping the RL reward signal. This encourages the reasoner not only to answer\ncorrectly, but to produce explanations that are persuasive to an independent\nmodel. Our listener-shaped reward scheme achieves best accuracy on the\nImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)\nperformance on a large-scale human preference dataset (1.2M votes, up to +6%\nover naive reasoner), and reduces reasoning contradictions compared to strong\nGRPO and SFT baselines. These results demonstrate that listener-based rewards\nprovide a scalable, data-efficient path to aligning vision-language models with\nnuanced human preferences. We will release our reasoning model here:\nhttps://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6192657ba9638054a9818f04/IpysW0QkLzIgzWognSV7n.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22832.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6192657ba9638054a9818f04",
      "avatarUrl": "/avatars/35f6f6eb2e8f6b283034632a141c2670.svg",
      "fullname": "Alexander Gambashidze",
      "name": "alexgambashidze",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.24119",
      "authors": [
        {
          "_id": "68634850588cea0da970c892",
          "user": {
            "_id": "635e3a76106f984574c36409",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
            "isPro": false,
            "fullname": "Bo Liu",
            "user": "Benjamin-eecs",
            "type": "user"
          },
          "name": "Bo Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:46.740Z",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c893",
          "name": "Leon Guertler",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c894",
          "user": {
            "_id": "636681feaa6a4af6073ba73e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636681feaa6a4af6073ba73e/u_0moYNu6as9Sszp-ej95.png",
            "isPro": true,
            "fullname": "Simon Yu",
            "user": "simonycl",
            "type": "user"
          },
          "name": "Simon Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:44.863Z",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c895",
          "user": {
            "_id": "65f5392c68b8e0cb3c9977a2",
            "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
            "isPro": false,
            "fullname": "Zichen",
            "user": "lkevinzc",
            "type": "user"
          },
          "name": "Zichen Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:42.588Z",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c896",
          "name": "Penghui Qi",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c897",
          "name": "Daniel Balcells",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c898",
          "name": "Mickel Liu",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c899",
          "name": "Cheston Tan",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89a",
          "name": "Weiyan Shi",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89b",
          "name": "Min Lin",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89c",
          "name": "Wee Sun Lee",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89d",
          "name": "Natasha Jaques",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T17:58:13.000Z",
      "submittedOnDailyAt": "2025-07-01T01:11:49.104Z",
      "title": "SPIRAL: Juego con 0, un enfoque de aprendizaje por reforzamiento multiagente multietapa que profundiza en las razones de su juego mientras se desarrolla su propio juego.",
      "submittedOnDailyBy": {
        "_id": "635e3a76106f984574c36409",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
        "isPro": false,
        "fullname": "Bo Liu",
        "user": "Benjamin-eecs",
        "type": "user"
      },
      "summary": "El desarrollo reciente del aprendizaje reforzado ha demostrado que los modelos de lenguaje pueden desarrollar complejas teorías de razonamiento al entrenarse en tareas que proporcionan recompensas comprobables, pero estas aproximaciones dependen de la resolución de problemas humanamente editados y el aprendizaje de recompensas especializadas. Presentamos SPIRAL (Framework for Self-Play for Incremental Learning and Adaptation). SPIRAL es un marco de trabajo que permite al modelo aprender a través de juegos múltiples de 0 suma y de autojugar varias veces, lo que evita la necesidad de supervisión humana. SPIRAL genera problemas de desarrollo infinitos debido a que el modelo debe adaptarse constantemente a adversarios fuertes. Para lograr este aprendizaje cíclico, SPIRAL implementa un sistema de aprendizaje reforzado en línea con múltiples agentes y propone la Evaluación de Prioridades Condicionales (RAE). Con SPIRAL, el modelo puede mejorar su capacidad de razonamiento en juegos de 0 suma a través de sus propios juegos. En un solo juego de Kuhn Poker, Qwen3-4B-Base logró un aumento del 8.6% en matemáticas y del 8.4% en razonamiento general, superando los 25,000 juegos efectivos en entrenamiento de SFT. El análisis indica que este desarrollo se produce a través de tres patrones cognitivos: sistematic decomposition, value calculation, and case analysis. El aprendizaje en varios juegos (TicTacToe, Kuhn Poker, Simple Negotiation) mejora la performance al desarrollar diferentes teorías de razonamiento. Al aplicar SPIRAL a un modelo fuerte de razonamiento (DeepSeek-R1-Distill-Qwen-7B), se observa un aumento promedio del 2.0%. Estos resultados muestran que los juegos de 0 suma pueden desarrollar una razonamiento natural y revelan una prometedora dirección para el desarrollo automático cognitivo.",
      "upvotes": 11,
      "discussionId": "68634850588cea0da970c89e",
      "githubRepo": "https://github.com/spiral-rl/spiral",
      "ai_summary": "Self-play in zero-sum games using SPIRAL enhances reasoning capabilities in language models through self-improvement and transfer learning.",
      "ai_keywords": [
        "reinforcement learning",
        "language models",
        "self-play framework",
        "multi-turn games",
        "zero-sum games",
        "reward engineering",
        "role-conditioned advantage estimation",
        "Kuhn Poker",
        "TicTacToe",
        "Simple Negotiation",
        "transferable reasoning"
      ],
      "githubStars": 12
    },
    "publishedAt": "2025-06-30T13:58:13.000Z",
    "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning",
    "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.24119.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635e3a76106f984574c36409",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
      "fullname": "Bo Liu",
      "name": "Benjamin-eecs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.17930",
      "authors": [
        {
          "_id": "68634a4e588cea0da970c8ba",
          "user": {
            "_id": "61e09ec13a1781f66b4e9ae2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
            "isPro": false,
            "fullname": "Jianyu Wang",
            "user": "Jianyu",
            "type": "user"
          },
          "name": "Jianyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:27.023Z",
          "hidden": false
        },
        {
          "_id": "68634a4e588cea0da970c8bb",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:31.161Z",
          "hidden": false
        },
        {
          "_id": "68634a4e588cea0da970c8bc",
          "user": {
            "_id": "6454685a548f22be598414c4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg",
            "isPro": false,
            "fullname": "Lidong Bing",
            "user": "LidongBing",
            "type": "user"
          },
          "name": "Lidong Bing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:29.213Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-22T07:53:07.000Z",
      "submittedOnDailyAt": "2025-07-01T01:09:47.554Z",
      "title": "Evolución de los Prompts en su contexto: Una perspectiva abierta y automáticamente reproducible",
      "submittedOnDailyBy": {
        "_id": "61e09ec13a1781f66b4e9ae2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
        "isPro": false,
        "fullname": "Jianyu Wang",
        "user": "Jianyu",
        "type": "user"
      },
      "summary": "Proponemos un nuevo paradigma de diseño de prompts para desafiar el conocimiento general en los modelos de lenguaje grande (LLM). El conocimiento general prioriza bien-hechos comandos e indicaciones para el aprendizaje de prompts (ICL), pero mostramos que utilizar inducciones aleatorias llamadas \"Gibberish\" puede impulsar el rendimiento en diversas tareas. Específicamente, \"Gibberish\" puede obtener mayores mejoras que los métodos más recientes de optimización automática de prompts, y es efectivo independientemente del arreglo de atención de la LLM. Sin embargo, encontrar un acortamiento efectivo no es sencillo. Los métodos de atributos existentes y los algoritmos de compresión de prompts no ofrecen resultados significativos y son limitados por la intuición humana. En este sentido, proponemos un marco de optimización de prompts auto-descubierto llamado PromptQuine, que busca acortamientos automáticamente en bajos niveles de datos. Este marco evoluciona y refina efectivos prompts de manera no tradicional, como fenómenos naturales o como fenómenos de co-evolución y auto-organización, en respuesta a restricciones de recursos. Este marco es efectivo en tareas de clasificación, múltiples elecciones, generación y lógica matemática, y también mejora la eficiencia de tiempo de ejecución en LLM. Esperamos que esta descubrimiento guíe la investigación estructural en el aprendizaje de prompts y fomente experimentos más abiertos y efectivos para el diseño de prompts en LLM.",
      "upvotes": 10,
      "discussionId": "68634a4f588cea0da970c8bd",
      "ai_summary": "A novel prompt design paradigm, PromptQuine, shows that pruning random demonstrations into \"gibberish\" can improve large language model performance across various tasks, surpassing state-of-the-art methods.",
      "ai_keywords": [
        "prompt design paradigm",
        "in-context learning",
        "pruning",
        "demonstrations",
        "gibberish",
        "prompt optimization",
        "self-discover prompt optimization framework",
        "PromptQuine",
        "evolutionary search framework",
        "classification",
        "multi-choice question answering",
        "generation",
        "math reasoning",
        "tokens",
        "emergent complexity",
        "symbiosis",
        "self-organization"
      ]
    },
    "publishedAt": "2025-06-22T03:53:07.000Z",
    "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective",
    "summary": "We propose a novel prompt design paradigm that challenges conventional wisdom\nin large language model (LLM) prompting. While conventional wisdom prioritizes\nwell-crafted instructions and demonstrations for in-context learning (ICL), we\nshow that pruning random demonstrations into seemingly incoherent \"gibberish\"\ncan remarkably improve performance across diverse tasks. Notably, the\n\"gibberish\" always matches or surpasses state-of-the-art automatic prompt\noptimization techniques, achieving substantial gains regardless of LLM\nalignment. Nevertheless, discovering an effective pruning strategy is\nnon-trivial, as existing attribution methods and prompt compression algorithms\nfail to deliver robust results, let alone human intuition. In terms of this, we\npropose a self-discover prompt optimization framework, PromptQuine, an\nevolutionary search framework that automatically searches for the pruning\nstrategy by itself using only low-data regimes. Much like the emergent\ncomplexity in nature--such as symbiosis and self-organization--arising in\nresponse to resource constraints, our framework evolves and refines\nunconventional yet highly effective prompts by leveraging only the tokens\npresent within the context. We demonstrate its effectiveness across\nclassification, multi-choice question answering, generation and math reasoning\ntasks across LLMs, while achieving decent runtime efficiency. We hope our\nfindings can guide mechanistic studies on in-context learning, and provide a\ncall to action, to pave the way for more open-ended search algorithms for more\neffective LLM prompting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17930.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "61e09ec13a1781f66b4e9ae2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
      "fullname": "Jianyu Wang",
      "name": "Jianyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23542",
      "authors": [
        {
          "_id": "6863479d588cea0da970c86f",
          "user": {
            "_id": "661b9d96c153e4a0a25adc3e",
            "avatarUrl": "/avatars/b3099b51064c8b71a4bce24e2a49b766.svg",
            "isPro": false,
            "fullname": "Weida Wang",
            "user": "weidawang",
            "type": "user"
          },
          "name": "Weida Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:14:03.221Z",
          "hidden": false
        },
        {
          "_id": "6863479d588cea0da970c870",
          "name": "Changyong He",
          "hidden": false
        },
        {
          "_id": "6863479d588cea0da970c871",
          "name": "Jin Zeng",
          "hidden": false
        },
        {
          "_id": "6863479d588cea0da970c872",
          "name": "Di Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T06:29:24.000Z",
      "submittedOnDailyAt": "2025-07-01T01:05:13.898Z",
      "title": "Grafo Información Integrada por Motrices de Atracción Consistente en Medición de Tiempo y Depth Descomposición",
      "submittedOnDailyBy": {
        "_id": "6684b284dc7b0ae2cc67660c",
        "avatarUrl": "/avatars/54b3c0c4d808f293d78085d4d504570a.svg",
        "isPro": false,
        "fullname": "liuwanhao",
        "user": "wanhaoliu",
        "type": "user"
      },
      "summary": "Las imágenes de profundidad obtenidas de sensores de tiempo de vuelta (ToF) suelen ser muy afectadas por ruido. Por lo tanto, es necesario eliminar el ruido para obtener resultados satisfactorios en aplicaciones posteriores. En estudios previos, solo se ha procesado un solo frame o se ha realizado procesamiento entre frames, pero sin considerar la variación de profundidad en píxeles correspondientes entre frames, lo que genera resultados insatisfactorios con incertidumbre temporal y espacial. En este artículo, se propone una nueva red de eliminación de ruido de profundidad ToF que mejora tanto la estabilidad temporal como la precisión espacial utilizando la integración de grafos que no varían con el movimiento. Específicamente, la estructura del grafo, independiente de la movida de profundidad entre frames, muestra similitud temporal y permite realizar transformaciones geométricas entre frames. A continuación, se utilizan la regularidad de la imagen en el grafo combinado y la precisión de los datos obtenidos de la distribución de ruido ToF para formular el problema de eliminación de ruido ToF de máxima prioridad. Finalmente, se genera una red altamente eficiente y interpretable mediante filtros iterativos con pesos adaptativos basados en la información del grafo y transformaciones geométricas derivadas de la misma. Los resultados de los experimentos muestran que la técnica propuesta muestra el mejor rendimiento en términos de precisión y consistencia en el conjunto de datos DVToF y muestra una excelente generalización en el conjunto de datos reales Kinectv2. El código fuente está disponible en la URL proporcionada.",
      "upvotes": 8,
      "discussionId": "6863479d588cea0da970c873",
      "ai_summary": "A novel ToF depth denoising network uses motion-invariant graph fusion and adaptive filters to improve temporal stability and spatial sharpness, achieving state-of-the-art performance.",
      "ai_keywords": [
        "ToF depth denoising",
        "motion-invariant graph fusion",
        "graph structures",
        "temporal self-similarity",
        "geometric attention",
        "image smoothness prior",
        "maximum a posterior problem",
        "iterative filters",
        "DVToF dataset",
        "Kinectv2 dataset"
      ]
    },
    "publishedAt": "2025-06-30T02:29:24.000Z",
    "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric\n  Attention",
    "summary": "Depth images captured by Time-of-Flight (ToF) sensors are prone to noise,\nrequiring denoising for reliable downstream applications. Previous works either\nfocus on single-frame processing, or perform multi-frame processing without\nconsidering depth variations at corresponding pixels across frames, leading to\nundesirable temporal inconsistency and spatial ambiguity. In this paper, we\npropose a novel ToF depth denoising network leveraging motion-invariant graph\nfusion to simultaneously enhance temporal stability and spatial sharpness.\nSpecifically, despite depth shifts across frames, graph structures exhibit\ntemporal self-similarity, enabling cross-frame geometric attention for graph\nfusion. Then, by incorporating an image smoothness prior on the fused graph and\ndata fidelity term derived from ToF noise distribution, we formulate a maximum\na posterior problem for ToF denoising. Finally, the solution is unrolled into\niterative filters whose weights are adaptively learned from the graph-informed\ngeometric attention, producing a high-performance yet interpretable network.\nExperimental results demonstrate that the proposed scheme achieves\nstate-of-the-art performance in terms of accuracy and consistency on synthetic\nDVToF dataset and exhibits robust generalization on the real Kinectv2 dataset.\nSource code will be released at\nhttps://github.com/davidweidawang/GIGA-ToF{https://github.com/davidweidawang/GIGA-ToF}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23542.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6684b284dc7b0ae2cc67660c",
      "avatarUrl": "/avatars/54b3c0c4d808f293d78085d4d504570a.svg",
      "fullname": "liuwanhao",
      "name": "wanhaoliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.17417",
      "authors": [
        {
          "_id": "685c8635696820ba1f28f24b",
          "user": {
            "_id": "65d3b7ec8f6b98b34ee6bbe3",
            "avatarUrl": "/avatars/53c2d4e4746147fc2559435d252e8951.svg",
            "isPro": false,
            "fullname": "Mingyuan Wu",
            "user": "Mingyuan1997",
            "type": "user"
          },
          "name": "Mingyuan Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:32:22.703Z",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24c",
          "name": "Meitang Li",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24d",
          "name": "Jingcheng Yang",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24e",
          "name": "Jize Jiang",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24f",
          "name": "Kaizhuo Yan",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f250",
          "name": "Zhaoheng Li",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f251",
          "name": "Minjia Zhang",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f252",
          "name": "Klara Nahrstedt",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T18:23:48.000Z",
      "submittedOnDailyAt": "2025-07-01T00:36:26.256Z",
      "title": "¡Ahora el momento adiós: ¿Las VLMs tienen capacidad de auto-probación verdadera en la escalación de inferencia?",
      "submittedOnDailyBy": {
        "_id": "65d3b7ec8f6b98b34ee6bbe3",
        "avatarUrl": "/avatars/53c2d4e4746147fc2559435d252e8951.svg",
        "isPro": false,
        "fullname": "Mingyuan Wu",
        "user": "Mingyuan1997",
        "type": "user"
      },
      "summary": "El reciente desarrollo de los grandes modelos de lenguaje (LLMs) demuestra que los métodos de cálculo en la inferencia (por ejemplo, el escalamiento en la decisión y el autoajuste) mejoran significativamente la capacidad lógica sin depender de conocimientos externos. El núcleo de este éxito es que los comportamientos que realizan autoajuste y autoverificación generalmente son producidos por la aprendizaje por refuerzo (RL). En este artículo, investigamos si estos métodos de inferencia se pueden aplicar efectivamente también a los modelos de lenguaje visual (VLMs). Además, nos centramos especialmente en aquellos que se han entrenado con RL. Hemos encontrado que la estrategia de decisión de elegir el mejor de los N resultados es capaz de mejorar significativamente el rendimiento lógico de todos los VLMs, y que los métodos que dependen de la generación (por ejemplo, aquellos mencionados anteriormente) son más efectivos que los métodos que dependen de la verificación (por ejemplo, aquellos que se mencionarán más adelante). Además, los comportamientos de autoajuste asociados con modelos entrenados por RL (por ejemplo, \"eso instantáneo\") no muestran efectos medibles. A través de experimentos ampliados dentro del marco de escalamiento de la inferencia, hemos identificado la principal causa: los VLMs entrenados por RL muestran una deficiencia significativa en la capacidad de autoverificación tanto visual como textual.",
      "upvotes": 8,
      "discussionId": "685c8636696820ba1f28f253",
      "ai_summary": "Inference-time techniques like decoding-time scaling and self-refinement enhance reasoning in vision-language models, with generation-based methods providing greater improvement than verification-based methods, despite RL-trained models not showing self-correction benefits.",
      "ai_keywords": [
        "large language models",
        "inference-time computation",
        "decoding-time scaling",
        "self-refinement",
        "self-correction",
        "self-verification",
        "reinforcement learning",
        "vision-language models",
        "majority voting",
        "best-of-N selection",
        "aha moment"
      ]
    },
    "publishedAt": "2025-06-20T14:23:48.000Z",
    "title": "Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in\n  Inference-time Scaling?",
    "summary": "Recent advances in large language models (LLMs) have demonstrated that\ninference-time computation techniques, such as decoding-time scaling and\nself-refinement, can significantly enhance reasoning capabilities without\nrelying on external knowledge. A key driver of this success is the emergence of\nself-correction and self-verification behaviors, often elicited through\nreinforcement learning (RL). In this paper, we investigate whether these\ninference-time techniques extend effectively to vision-language models (VLMs),\nparticularly those trained with RL. We find that while decoding strategies such\nas majority voting and best-of-N selection with self-verification all improve\nVLM reasoning performance, generation-reliant methods such as the former\nachieve significantly higher gains versus verification-reliant methods such as\nthe latter. Additionally, the self-correction behavior often associated with\nRL-tuned models, such as aha moment, does not lead to measurable gains. We show\nvia extensive experimentation within the inference-time scaling framework to\nidentify a key root cause: RL-trained VLMs still lack robust self-verification\ncapabilities across both visual and textual modalities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d3b7ec8f6b98b34ee6bbe3",
      "avatarUrl": "/avatars/53c2d4e4746147fc2559435d252e8951.svg",
      "fullname": "Mingyuan Wu",
      "name": "Mingyuan1997",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16500",
      "authors": [
        {
          "_id": "6858a7f0c0c8e29df8ea3c06",
          "user": {
            "_id": "64b38bc2a248169796fec4fa",
            "avatarUrl": "/avatars/1371021f2ef0ce2197cc13627c9e03c9.svg",
            "isPro": false,
            "fullname": "Samir Khaki",
            "user": "Skhaki",
            "type": "user"
          },
          "name": "Samir Khaki",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:14:44.911Z",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c07",
          "name": "Xiuyu Li",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c08",
          "name": "Junxian Guo",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c09",
          "name": "Ligeng Zhu",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0a",
          "name": "Chenfeng Xu",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0b",
          "name": "Konstantinos N. Plataniotis",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0c",
          "name": "Amir Yazdanbakhsh",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0d",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0e",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0f",
          "user": {
            "_id": "650dac79b959b0e1d41d7378",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dac79b959b0e1d41d7378/mzbN0MFk3k8b94FQ40I7L.jpeg",
            "isPro": false,
            "fullname": "Zhijian Liu",
            "user": "zhijianliu",
            "type": "user"
          },
          "name": "Zhijian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:14:42.873Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T17:53:34.000Z",
      "submittedOnDailyAt": "2025-07-01T03:16:07.666Z",
      "title": "SparseLoRA: Método para acelerar la fine-tuning de un LLM utilizando la sparseness de funciones contextuales",
      "submittedOnDailyBy": {
        "_id": "64b38bc2a248169796fec4fa",
        "avatarUrl": "/avatars/1371021f2ef0ce2197cc13627c9e03c9.svg",
        "isPro": false,
        "fullname": "Samir Khaki",
        "user": "Skhaki",
        "type": "user"
      },
      "summary": "El ajuste de LLMs es cargado con fuertes exigencias en cuanto a la cantidad de cálculos y el uso de memoria. Métodos de ajuste eficientes de parámetros como QLoRA y DoRA pueden reducir el número de parámetros aprendibles y el uso de memoria, pero no pueden reducir los costos de cálculo. En estos casos, se puede retrasar el ajuste. En este artículo, se presenta un método llamado SparseLoRA que utiliza la sparseness de contextos para acelerar el ajuste de LLMs. De esta manera, se puede reducir los costos de cálculo en un máximo de 2.2 veces y aumentar la velocidad de cálculo en un máximo de 1.6 veces, manteniendo la precisión.",
      "upvotes": 7,
      "discussionId": "6858a7f0c0c8e29df8ea3c10",
      "projectPage": "https://z-lab.ai/projects/sparselora/",
      "githubRepo": "https://github.com/z-lab/sparselora",
      "ai_summary": "SparseLoRA reduces computational cost and speeds up fine-tuning of LLMs by dynamically selecting a sparse subset of weights for loss and gradient computation.",
      "ai_keywords": [
        "QLoRA",
        "DoRA",
        "parameter-efficient fine-tuning",
        "SparseLoRA",
        "contextual sparsity",
        "SVD sparsity estimator",
        "computational cost",
        "commonsense reasoning",
        "arithmetic reasoning",
        "code generation",
        "instruction following"
      ],
      "githubStars": 20
    },
    "publishedAt": "2025-06-19T13:53:34.000Z",
    "title": "SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity",
    "summary": "Fine-tuning LLMs is both computationally and memory-intensive. While\nparameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the\nnumber of trainable parameters and lower memory usage, they do not decrease\ncomputational cost. In some cases, they may even slow down fine-tuning. In this\npaper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning\nthrough contextual sparsity. We propose a lightweight, training-free SVD\nsparsity estimator that dynamically selects a sparse subset of weights for loss\nand gradient computation. Also, we systematically analyze and address\nsensitivity across layers, tokens, and training steps. Our experimental results\nshow that SparseLoRA reduces computational cost by up to 2.2 times and a\nmeasured speedup of up to 1.6 times while maintaining accuracy across various\ndownstream tasks, including commonsense and arithmetic reasoning, code\ngeneration, and instruction following.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16500.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b38bc2a248169796fec4fa",
      "avatarUrl": "/avatars/1371021f2ef0ce2197cc13627c9e03c9.svg",
      "fullname": "Samir Khaki",
      "name": "Skhaki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23151",
      "authors": [
        {
          "_id": "686399bc588cea0da970c98f",
          "name": "Vladislav Bargatin",
          "hidden": false
        },
        {
          "_id": "686399bc588cea0da970c990",
          "name": "Egor Chistov",
          "hidden": false
        },
        {
          "_id": "686399bc588cea0da970c991",
          "user": {
            "_id": "663692c75f67f8da32723bf8",
            "avatarUrl": "/avatars/258264afe2ea5048a4a7a8e9945d2f5b.svg",
            "isPro": false,
            "fullname": "Alexander Yakovenko",
            "user": "a-yakovenko",
            "type": "user"
          },
          "name": "Alexander Yakovenko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:12:16.036Z",
          "hidden": false
        },
        {
          "_id": "686399bc588cea0da970c992",
          "name": "Dmitriy Vatolin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-29T09:01:42.000Z",
      "submittedOnDailyAt": "2025-07-01T08:44:33.443Z",
      "title": "Estimado usuario, aquí está la traducción del texto en inglés al español:\n\n\"Estimación de flujo óptico multi-frame con eficiencia de memoria mediante entrenamiento de alta resolución\"",
      "submittedOnDailyBy": {
        "_id": "663692c75f67f8da32723bf8",
        "avatarUrl": "/avatars/258264afe2ea5048a4a7a8e9945d2f5b.svg",
        "isPro": false,
        "fullname": "Alexander Yakovenko",
        "user": "a-yakovenko",
        "type": "user"
      },
      "summary": "El desarrollo reciente del flujo óptico de inferencia ha enfatizado la precisión, pero se toma en cuenta que la consumo de memoria de GPU aumenta. En particular, este problema es más grave con entradas de alta resolución (FullHD). Aquí se presenta MEMFOF (Método de Flujo Óptico Eficiente en Memoria para Multi-Frame). MEMFOF es un método de flujo óptico eficiente en memoria que busca un equilibrio útil entre el multi-frame inferencia y el consumo de memoria de GPU. Específicamente, requiere 2.09 GB de memoria de GPU al ejecutarse con entradas de 1080p y 28.5 GB para el entrenamiento. Esto permite entrenar modelos de escala original en 1080p sin necesidad de cortar o sub-samplear. Se reevalúan selectivamente las opciones de arquitectura similar a RAFT, combinando un volumen de correlación reducido y protocolos de entrenamiento de alta resolución con el multi-frame inferencia, lo que permite alcanzar los mejores rendimientos en varios benchmarks mientras reducir significativamente el overhead de memoria. Nuestro método supera a otras alternativas más eficientes en términos de precisión y tiempo de ejecución. De esta manera, se demuestra la robustez del flujo óptico en alta resolución. En el momento de la propuesta, nuestro método tiene una proporción de desvío de 1 píxel (1px) de 3.289 y se coloca en primer lugar en el benchmark Spring, con un error de EPE de 0.963 en Sintel (clean) y un error de Fl-all de 2.94% en KITTI-2015. El código está disponible en https://github.com/msu-video-group/memfof.",
      "upvotes": 5,
      "discussionId": "686399bd588cea0da970c993",
      "projectPage": "https://msu-video-group.github.io/memfof/",
      "githubRepo": "https://github.com/msu-video-group/memfof",
      "ai_summary": "MEMFOF is a memory-efficient multi-frame optical flow method that achieves state-of-the-art performance on high-resolution inputs with reduced GPU memory usage.",
      "ai_keywords": [
        "MEMFOF",
        "optical flow estimation",
        "memory-efficient",
        "multi-frame",
        "correlation volumes",
        "RAFT-like architectures",
        "high-resolution training",
        "Spring benchmark",
        "Sintel benchmark",
        "KITTI-2015 benchmark",
        "outlier rate",
        "endpoint error",
        "Fl-all error"
      ]
    },
    "publishedAt": "2025-06-29T05:01:42.000Z",
    "title": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame\n  Optical Flow Estimation",
    "summary": "Recent advances in optical flow estimation have prioritized accuracy at the\ncost of growing GPU memory consumption, particularly for high-resolution\n(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical\nflow method that identifies a favorable trade-off between multi-frame\nestimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU\nmemory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely\npositions our method to be trained at native 1080p without the need for\ncropping or downsampling. We systematically revisit design choices from\nRAFT-like architectures, integrating reduced correlation volumes and\nhigh-resolution training protocols alongside multi-frame estimation, to achieve\nstate-of-the-art performance across multiple benchmarks while substantially\nreducing memory overhead. Our method outperforms more resource-intensive\nalternatives in both accuracy and runtime efficiency, validating its robustness\nfor flow estimation at high resolutions. At the time of submission, our method\nranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,\nleads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the\nbest Fl-all error on KITTI-2015 at 2.94%. The code is available at\nhttps://github.com/msu-video-group/memfof.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23151.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "663692c75f67f8da32723bf8",
      "avatarUrl": "/avatars/258264afe2ea5048a4a7a8e9945d2f5b.svg",
      "fullname": "Alexander Yakovenko",
      "name": "a-yakovenko",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.22992",
      "authors": [
        {
          "_id": "686370bd588cea0da970c90e",
          "user": {
            "_id": "662a75287181150b857245fb",
            "avatarUrl": "/avatars/6a489bf6d3d9cd26ba88f17b35c6ecb5.svg",
            "isPro": false,
            "fullname": "Yulun Jiang",
            "user": "yljblues",
            "type": "user"
          },
          "name": "Yulun Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:16.902Z",
          "hidden": false
        },
        {
          "_id": "686370bd588cea0da970c90f",
          "name": "Yekun Chai",
          "hidden": false
        },
        {
          "_id": "686370bd588cea0da970c910",
          "name": "Maria Brbić",
          "hidden": false
        },
        {
          "_id": "686370bd588cea0da970c911",
          "user": {
            "_id": "6438d1d843d932c462404500",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
            "isPro": false,
            "fullname": "Michael Moor",
            "user": "mdmoor",
            "type": "user"
          },
          "name": "Michael Moor",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:19.043Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6438d1d843d932c462404500/FI8PgTG5g_uZ1q4y-c_Jd.png"
      ],
      "publishedAt": "2025-06-28T19:44:32.000Z",
      "submittedOnDailyAt": "2025-07-01T05:06:51.086Z",
      "title": "Mabel: Demanda de espacios de trabajo reconocimiento y planificación estricta marco de referencia",
      "submittedOnDailyBy": {
        "_id": "6438d1d843d932c462404500",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
        "isPro": false,
        "fullname": "Michael Moor",
        "user": "mdmoor",
        "type": "user"
      },
      "summary": "La capacidad para procesar diferentes modelos de datos de información y la habilidad de crear razones en etapas son problemas importantes para el desarrollo de la inteligencia artificial. Sin embargo, actualmente, los marcos de referencia para razones se centran en texto o en la utilización de diversas preguntas en los datos de modelado para buscar información directamente. Esto ha llevado a que la comprensión de razones complejas se investigue a un nivel más bajo en diferentes áreas de datos de modelado. En este contexto, presentamos un desafío para evaluar la capacidad de crear razones en etapas para problemas complejos en diferentes modelos de datos, diseñando un nuevo marco de referencia de razones para datos de modelado variados. Este marco de referencia consta de dos tareas altamente desafiantes: M-Portal y M-Cube, las cuales exigen la creación y comprensión de planes en múltiples etapas bajo restricciones espaciales, visuales y físicas complejas. Mostramos que los modelos de aprendizaje profundo con múltiples modalidades (MLLMs) presentan bajos rendimientos en MARBLE. Los 12 modelos de evolución muestran un rendimiento aproximadamente aleatorio en todos los M-Portal y un 0% de precisión en M-Cube. Además, algunos modelos superan el límite aleatorio en tareas sub-simplificadas, pero la comprensión de razones complejas sigue siendo un desafío para los MLLMs actuales. Además, demostramos que la extracción de información a partir de entradas visuales es difícil, lo que destaca los puntos de falla en la percepción visual. Esto revela las limitaciones en la comprensión de razones complejas y nos incitan a desarrollar el siguiente generación de modelos que pasen etapas de razones para crear planes en diferentes modelos de datos.",
      "upvotes": 3,
      "discussionId": "686370be588cea0da970c912",
      "ai_summary": "MARBLE is a challenging multimodal reasoning benchmark that highlights the limitations of existing multimodal language models in step-by-step reasoning and plan crafting under spatial and visual constraints.",
      "ai_keywords": [
        "multimodal reasoning",
        "multimodal language models",
        "M-Portal",
        "M-Cube",
        "multistep plans",
        "spatial constraints",
        "visual constraints"
      ]
    },
    "publishedAt": "2025-06-28T15:44:32.000Z",
    "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning",
    "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6438d1d843d932c462404500/FI8PgTG5g_uZ1q4y-c_Jd.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22992.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6438d1d843d932c462404500",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
      "fullname": "Michael Moor",
      "name": "mdmoor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23394",
      "authors": [
        {
          "_id": "686350a9588cea0da970c8d4",
          "user": {
            "_id": "645dbaa6f5760d1530d7580d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
            "isPro": true,
            "fullname": "Simeon Emanuilov",
            "user": "s-emanuilov",
            "type": "user"
          },
          "name": "Simeon Emanuilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:22.926Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-29T20:47:27.000Z",
      "submittedOnDailyAt": "2025-07-01T01:39:51.669Z",
      "title": "El método de educación del modelo de lenguaje de idiomas para hablar sobre lenguajes de herramientas",
      "submittedOnDailyBy": {
        "_id": "645dbaa6f5760d1530d7580d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
        "isPro": true,
        "fullname": "Simeon Emanuilov",
        "user": "s-emanuilov",
        "type": "user"
      },
      "summary": "La asociación con herramientas externas es un elemento importante en las aplicaciones de modelos de lenguaje, pero los modelos multilingües tienen una capacidad limitada para usar herramientas confiables en muchos idiomas no anglosajones. Los modelos multilingües más recientes también enfrentan dificultades para determinar cuándo usar herramientas y generar un output estructurado necesario para llamar a funciones, además de presentar confusión lingüística cuando reciben prompts en idiomas de baja capacidad de recursos. En este artículo, se propone un enfoque basado en el caso de Bulgaria, que permite a los modelos de lenguaje aplicar herramientas fuertes en cualquier idioma objetivo. Este enfoque incluye la continua entrenamiento de series de modelos como BgGPT (con 2.6B, 9B y 27B parámetros) utilizando un nuevo conjunto de datos de conexión (10,035 ejemplos) para soportar protocolos estándar como MCP (Model Context Protocol). En este estudio, se presenta TUCAN (Tool Usable Assistant), que mejora la precisión de llamadas a funciones en modelos básicos en un 28.75% o más, y mantiene la comprensión lingüística clave verificada en los benchmarks existentes de Bulgaria. En comparación con los outputs largos y incoherentes de los modelos básicos, los modelos TUCAN muestran llamadas a funciones estructuradas y semi-primera, y están listos para producción. Los modelos, el marco de evaluación y los conjuntos de datos se han publicado para facilitar la reproducción en otros idiomas. En este artículo, se muestra un enfoque práctico que supera los sistemas centrados en inglés para la funcionalidad de agregar herramientas.",
      "upvotes": 2,
      "discussionId": "686350aa588cea0da970c8d5",
      "ai_summary": "A methodology is presented to adapt language models for robust tool use across languages, specifically improving function-calling accuracy in Bulgarian.",
      "ai_keywords": [
        "function-calling",
        "multilingual models",
        "tool-use capabilities",
        "language confusion",
        "BgGPT",
        "bilingual dataset",
        "MCP (Model Context Protocol)",
        "TUCAN (Tool-Using Capable Assistant Navigator)",
        "function-calling accuracy",
        "production-ready response formatting"
      ]
    },
    "publishedAt": "2025-06-29T16:47:27.000Z",
    "title": "Teaching a Language Model to Speak the Language of Tools",
    "summary": "External tool integration through function-calling is essential for practical\nlanguage model applications, yet most multilingual models lack reliable\ntool-use capabilities in non-English languages. Even state-of-the-art\nmultilingual models struggle with determining when to use tools and generating\nthe structured outputs required for function calls, often exhibiting language\nconfusion when prompted in lower-resource languages. This work presents a\nmethodology for adapting existing language models to enable robust tool use in\nany target language, using Bulgarian as a case study. The approach involves\ncontinued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a\nnovel bilingual dataset of 10,035 function-calling examples designed to support\nstandardized protocols like MCP (Model Context Protocol). The research\nintroduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to\n28.75% improvement in function-calling accuracy over base models while\npreserving core language understanding, as verified on established Bulgarian\nbenchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready\nresponse formatting with clean, parsable function calls, contrasting with the\nverbose and inconsistent outputs of base models. The models, evaluation\nframework, and dataset are released to enable replication for other languages.\nThis work demonstrates a practical approach for extending tool-augmented\ncapabilities beyond English-centric systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23394.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645dbaa6f5760d1530d7580d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
      "fullname": "Simeon Emanuilov",
      "name": "s-emanuilov",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 39
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23219",
      "authors": [
        {
          "_id": "686376f6588cea0da970c92b",
          "user": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "isPro": false,
            "fullname": "Jie Feng",
            "user": "JJ-TMT",
            "type": "user"
          },
          "name": "Jie Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:14.367Z",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92c",
          "name": "Shengyuan Wang",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92d",
          "name": "Tianhui Liu",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92e",
          "name": "Yanxin Xi",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92f",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/RBLtf0_JPAX9A8msse6c0.jpeg"
      ],
      "publishedAt": "2025-06-29T13:04:27.000Z",
      "submittedOnDailyAt": "2025-07-01T04:22:03.888Z",
      "title": "Ciudad LLaVA: Espacio lógico y comprensión de la información de la ciudad para un modelo de lenguaje multimodal",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "En la investigación urbana se incluyen diversos escenarios y tareas, lo que requiere la comprensión de diferentes tipos de datos. Los métodos actuales se centran en un tipo específico de datos y no tienen un marco adecuado para la investigación urbana. El éxito reciente de los modelos de lenguaje multi-lenguaje multi-modal (MLLM) ha mostrado la posibilidad de superar estas limitaciones. En este artículo, se presenta el modelo de MLLM llamado Urban LLaVA. Este modelo tiene como objetivo procesar cuatro tipos de datos al mismo tiempo y lograr un rendimiento fuerte en diversas tareas urbanas en comparación con los MLLM generales. En Urban LLaVA, se editan primero diversas colecciones de datos urbanos que incluyen datos monomodales y criss-cross modales. Además, se propone un marco de entrenamiento multi-paso para mejorar la comprensión espacial y el aprendizaje de conocimientos de área, lo que mejora la aplicación y la eficiencia de Urban LLaVA. Finalmente, se expande el benchmark actual de la investigación urbana y se evalúan los rendimientos de MLLM en diversas tareas urbanas. Los resultados de los experimentos en tres ciudades demuestran que Urban LLaVA supera a los MLLM abiertos y propietarios, muestra un rendimiento en tareas monomodales y complejas criss-cross modales, y demostra una fortaleza generalizadora entre ciudades. El código fuente y los datos están disponibles para acceso público en https://github.com/tsinghua-fib-lab/UrbanLLaVA.",
      "upvotes": 2,
      "discussionId": "686376f6588cea0da970c930",
      "githubRepo": "https://github.com/tsinghua-fib-lab/UrbanLLaVA",
      "ai_summary": "UrbanLLaVA, a multi-modal large language model, effectively processes urban datasets for various tasks, outperforming existing models in both single-modal and complex cross-modal scenarios.",
      "ai_keywords": [
        "multi-modal large language models",
        "spatial reasoning",
        "domain knowledge learning",
        "urban instruction dataset",
        "single-modal",
        "cross-modal",
        "benchmark",
        "urban research"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-06-29T09:04:27.000Z",
    "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence\n  with Spatial Reasoning and Understanding",
    "summary": "Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce UrbanLLaVA, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\nUrbanLLaVA, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of UrbanLLaVA across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that UrbanLLaVA outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/RBLtf0_JPAX9A8msse6c0.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23219.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.22694",
      "authors": [
        {
          "_id": "68637193588cea0da970c914",
          "name": "Raghavv Goel",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c915",
          "name": "Sudhanshu Agrawal",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c916",
          "name": "Mukul Gagrani",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c917",
          "name": "Junyoung Park",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c918",
          "name": "Yifan Zao",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c919",
          "name": "He Zhang",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91a",
          "name": "Tian Liu",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91b",
          "name": "Yiping Yang",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91c",
          "name": "Xin Yuan",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91d",
          "name": "Jiuyan Lu",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91e",
          "name": "Chris Lott",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91f",
          "name": "Mingu Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T00:26:40.000Z",
      "submittedOnDailyAt": "2025-07-01T04:07:47.278Z",
      "title": "VOCABTRIM: Programación de modelos de procesamiento del lenguaje predictivo eficiente en entrenamiento de diccionarios",
      "submittedOnDailyBy": {
        "_id": "649b6eb2f7cc759ab756adaf",
        "avatarUrl": "/avatars/a0f7c7345dd653887122200fbe375c2b.svg",
        "isPro": false,
        "fullname": "Raghavv Goel",
        "user": "RaghavvGoel",
        "type": "user"
      },
      "summary": "En este artículo, se presentan métodos sencillos y sin estados para mejorar el rendimiento del método SpD (Special Decoding). Este método incluye el LM head (Capa de Modelo de Lenguaje) en el proceso inicial de SpD para mejorarlo efectivamente. SpD utiliza un pequeño modelo de lenguaje (Draft Model) para sampling secuencias o árboles de tokens, y luego verifica con el Base LLM (modelo objetivo) para aceptar ciertas partes como producciones válidas del Base LLM. Generalmente, SpD requiere una correspondencia 1 a 1 entre el modelo objetivo y el modelo de draft, lo que puede ser naturalizado con métodos como EAGLE o Medusa, que comparten la no-orabilidad o el LM head. Primero, se reconoce que este método de sampling de tokens de draft incluye sobrecargas de inferencia innecesarias durante el drafting. Especialmente, cuando la no-orabilidad del modelo objetivo es muy grande, esta sobrecarga aumenta significativamente. Por lo tanto, se propone VocabTrim, una simple técnica para reducir la sobrecarga de drafting y mejorar la velocidad de generación en entornos de banco de memoria. VocabTrim limita el LM head del modelo de draft a un conjunto limitado de tokens que se muestran con mayor frecuencia en la no-orabilidad del modelo objetivo. Limitar la no-orabilidad durante el drafting solo disminuye un poco la tasa de aceptación, pero significativamente reduce el tiempo de procesamiento en la banca de memoria, y en casos comunes, aumenta significativamente la velocidad de la banca de memoria (MBSU). Además, nuestro método aumenta aproximadamente en 16% la velocidad de la banca de memoria del modelo Llama-3 en Spec-Bench, y en particular, en aproximadamente 16% en Llama-3.2-3B-Instruct.",
      "upvotes": 2,
      "discussionId": "68637193588cea0da970c920",
      "ai_summary": "A technique called VocabTrim improves drafter-based speculative decoding by reducing the vocabulary of the drafter language model, thus decreasing drafting latency in memory-bound environments.",
      "ai_keywords": [
        "drafter-based speculative decoding",
        "speculative decoding",
        "LM head",
        "drafters",
        "token sampling",
        "inference overhead",
        "target LLM",
        "vocabulary sharing",
        "EAGLE",
        "Medusa",
        "VocabTrim",
        "acceptance rate",
        "drafting latency",
        "memory-bound speed up",
        "Spec-Bench",
        "Llama-3"
      ]
    },
    "publishedAt": "2025-06-27T20:26:40.000Z",
    "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs",
    "summary": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649b6eb2f7cc759ab756adaf",
      "avatarUrl": "/avatars/a0f7c7345dd653887122200fbe375c2b.svg",
      "fullname": "Raghavv Goel",
      "name": "RaghavvGoel",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23135",
      "authors": [
        {
          "_id": "68638235588cea0da970c966",
          "name": "Yu Shang",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c967",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c968",
          "name": "Yinzhou Tang",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c969",
          "name": "Lei Jin",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c96a",
          "name": "Chen Gao",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c96b",
          "name": "Wei Wu",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c96c",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/oiWNtmiOwH-sWe0ewE2YQ.jpeg"
      ],
      "publishedAt": "2025-06-29T08:19:45.000Z",
      "submittedOnDailyAt": "2025-07-01T05:11:31.002Z",
      "title": "RoboScape: Modelo de mundo visualizado con física e información de datos",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "El mundo de los modelos es un instrumento esencial para la inteligencia que ha adquirido una forma corporal. Estos modelos funcionan como potentes simuladores para generar imágenes de robots reales, resuelviendo así problemas de falta de datos importantes. Sin embargo, los modelos corporales actuales de mundo tienen limitaciones en la generalización 3D y la percepción física de la dinámica de movimiento, lo que genera errores en la generación de imágenes realistas cuando se trabaja con sensores de robot ricos en contacto. En este artículo, se propone un modelo físico integrado que aprende a integrar conocimientos físicos a través de un marco interactivo de entrenamiento para la generación de videos RGB. Se introducen dos tareas de entrenamiento con información física: la predicción de profundidad temporal mejora la consistencia de la generalización 3D en la renderización de videos, y el aprendizaje de la dinámica de puntos clave mejora los modelos de movimiento complejos al codificar de manera oculta propiedades físicas (por ejemplo, el tipo de material y la forma de los objetos). Los experimentos extendidos en RoboScape muestran excelentes resultados visuales y posibilidades físicas en diferentes sensores de robot, demostrando efectos prácticos en aplicaciones de entrenamiento y evaluación de políticas de robots utilizando datos generados. Nuestro estudio proporciona nuevas pautas para la construcción eficiente de modelos de mundo con información física, contribuyendo al desarrollo de la inteligencia corporal. El código está disponible en la siguiente URL: https://github.com/tsinghua-fib-lab/RoboScape.",
      "upvotes": 1,
      "discussionId": "68638235588cea0da970c96d",
      "ai_summary": "RoboScape is a unified physics-informed world model that enhances visual fidelity and physical plausibility in robotic video generation by integrating temporal depth prediction and keypoint dynamics learning.",
      "ai_keywords": [
        "world models",
        "embodied intelligence",
        "RGB video generation",
        "physics knowledge",
        "temporal depth prediction",
        "keypoint dynamics learning",
        "visual fidelity",
        "physical plausibility"
      ]
    },
    "publishedAt": "2025-06-29T04:19:45.000Z",
    "title": "RoboScape: Physics-informed Embodied World Model",
    "summary": "World models have become indispensable tools for embodied intelligence,\nserving as powerful simulators capable of generating realistic robotic videos\nwhile addressing critical data scarcity challenges. However, current embodied\nworld models exhibit limited physical awareness, particularly in modeling 3D\ngeometry and motion dynamics, resulting in unrealistic video generation for\ncontact-rich robotic scenarios. In this paper, we present RoboScape, a unified\nphysics-informed world model that jointly learns RGB video generation and\nphysics knowledge within an integrated framework. We introduce two key\nphysics-informed joint training tasks: temporal depth prediction that enhances\n3D geometric consistency in video rendering, and keypoint dynamics learning\nthat implicitly encodes physical properties (e.g., object shape and material\ncharacteristics) while improving complex motion modeling. Extensive experiments\ndemonstrate that RoboScape generates videos with superior visual fidelity and\nphysical plausibility across diverse robotic scenarios. We further validate its\npractical utility through downstream applications including robotic policy\ntraining with generated data and policy evaluation. Our work provides new\ninsights for building efficient physics-informed world models to advance\nembodied intelligence research. The code is available at:\nhttps://github.com/tsinghua-fib-lab/RoboScape.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/oiWNtmiOwH-sWe0ewE2YQ.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23135.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22753",
      "authors": [
        {
          "_id": "6863b259588cea0da970c9bf",
          "name": "Jianing Zhang",
          "hidden": false
        },
        {
          "_id": "6863b259588cea0da970c9c0",
          "name": "Jiayi Zhu",
          "hidden": false
        },
        {
          "_id": "6863b259588cea0da970c9c1",
          "name": "Feiyu Ji",
          "hidden": false
        },
        {
          "_id": "6863b259588cea0da970c9c2",
          "name": "Xiaokang Yang",
          "hidden": false
        },
        {
          "_id": "6863b259588cea0da970c9c3",
          "user": {
            "_id": "67761e674467879a54b4624a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hbniDcGynaZIUZTNSgc2G.jpeg",
            "isPro": false,
            "fullname": "Xiaoyun Yuan",
            "user": "XiaoyunYuan",
            "type": "user"
          },
          "name": "Xiaoyun Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:12:11.708Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T04:48:37.000Z",
      "submittedOnDailyAt": "2025-07-01T08:43:39.113Z",
      "title": "Modelo de difusión múltiple paso con modelo de transacción para la mejora de la fotografía Meraniski",
      "submittedOnDailyBy": {
        "_id": "67761e674467879a54b4624a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hbniDcGynaZIUZTNSgc2G.jpeg",
        "isPro": false,
        "fullname": "Xiaoyun Yuan",
        "user": "XiaoyunYuan",
        "type": "user"
      },
      "summary": "La calidad de metal tiene grandes posibilidades en las pequeñas computadoras de imagen, pero se enfrenta a desafíos como la atenuación óptica compleja y el problema de lista de almacenamiento. Los métodos actuales generalmente dependen de ajustes ópticos precisos o de grandes conjuntos de datos, lo que complica su aplicación en sistemas de imagen reales. Además, las faltas en el control del proceso de inferencia pueden llevar a artefactos inusuales. Utilizamos imágenes naturales de alta calidad de modelos entrenados para introducir la Degradation-Modeled Multipath Diffusion, reemplazando grandes conjuntos de datos, y implementar imágenes de calidad de metal adaptativas. Nuestro marco de trabajo genera detalles de alta frecuencia, dependencia estructural, y reduce la atenuación característica de la calidad de metal, ajustando el equilibrio con pasos de promedio positivos, neutros y negativos para la carga de fabricación. El decodificador adaptativo permite un equilibrio entre confiabilidad y calidad visual. Además, el módulo de atención con interés en la atenuación que varía espacialmente (SVDA) modela adaptativamente la atenuación óptica compleja y la sensibilidad de sensores. Finalmente, para verificar la realidad, diseñamos y construimos un MetaCamera a escala de medición. Los resultados extendidos muestran que nuestro enfoque supera los métodos más avanzados, logrando reconstrucciones con alta dependencia y confort. Materiales adicionales: https://dmdiff.github.io/",
      "upvotes": 1,
      "discussionId": "6863b259588cea0da970c9c4",
      "projectPage": "https://dmdiff.github.io/",
      "githubRepo": "https://github.com/yuanxy92/DMDiff_ICCV2025",
      "ai_summary": "The proposed Degradation-Modeled Multipath Diffusion framework improves metalens image quality by using natural image priors and specific modules to balance detail, fidelity, and perceptual quality while addressing optical degradation.",
      "ai_keywords": [
        "multipath diffusion",
        "natural image priors",
        "positive-prompt paths",
        "neutral-prompt paths",
        "negative-prompt paths",
        "pseudo data augmentation",
        "tunable decoder",
        "spatially varying degradation-aware attention",
        "SVDA module",
        "high-fidelity image reconstruction"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-06-28T00:48:37.000Z",
    "title": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography",
    "summary": "Metalenses offer significant potential for ultra-compact computational\nimaging but face challenges from complex optical degradation and computational\nrestoration difficulties. Existing methods typically rely on precise optical\ncalibration or massive paired datasets, which are non-trivial for real-world\nimaging systems. Furthermore, a lack of control over the inference process\noften results in undesirable hallucinated artifacts. We introduce\nDegradation-Modeled Multipath Diffusion for tunable metalens photography,\nleveraging powerful natural image priors from pretrained models instead of\nlarge datasets. Our framework uses positive, neutral, and negative-prompt paths\nto balance high-frequency detail generation, structural fidelity, and\nsuppression of metalens-specific degradation, alongside pseudo data\naugmentation. A tunable decoder enables controlled trade-offs between fidelity\nand perceptual quality. Additionally, a spatially varying degradation-aware\nattention (SVDA) module adaptively models complex optical and sensor-induced\ndegradation. Finally, we design and build a millimeter-scale MetaCamera for\nreal-world validation. Extensive results show that our approach outperforms\nstate-of-the-art methods, achieving high-fidelity and sharp image\nreconstruction. More materials: https://dmdiff.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22753.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67761e674467879a54b4624a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hbniDcGynaZIUZTNSgc2G.jpeg",
      "fullname": "Xiaoyun Yuan",
      "name": "XiaoyunYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.21448",
      "authors": [
        {
          "_id": "68636ecd588cea0da970c905",
          "name": "Huadai Liu",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c906",
          "name": "Jialei Wang",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c907",
          "name": "Kaicheng Luo",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c908",
          "name": "Wen Wang",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c909",
          "name": "Qian Chen",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c90a",
          "name": "Zhou Zhao",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c90b",
          "name": "Wei Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T16:32:06.000Z",
      "submittedOnDailyAt": "2025-07-01T03:47:20.627Z",
      "title": "ThinkSound: Reasoning Chain-of-Thought para la generación y edición de voz (utilizando un método que sigue la lógica de las palabras en orden para generar y editar el voz)",
      "submittedOnDailyBy": {
        "_id": "63d8c0d3da4f72339241c7dd",
        "avatarUrl": "/avatars/c5852fa7d2b8ffb7a76f0143faa453ef.svg",
        "isPro": false,
        "fullname": "liuhuadai",
        "user": "liuhuadai",
        "type": "user"
      },
      "summary": "La generación de sonidos desde vídeo ha experimentado un gran avance, aunque la creación de alta calidad de sonido y la comprensión de los subtiles sentidos de los contenidos visuales es un desafío. Como expertos en la industria de entretenimiento de Disney, es necesario comprender complejas causas como las acciones visuales, los entornos acústicos y las relaciones temporales. Presentamos un nuevo marco de trabajo llamado \"ThinkSound\", que utiliza la teoría de la cadena de pensamiento (CoT) para facilitar la generación y edición interactiva de sonidos en cada etapa de un vídeo. Nuestro enfoque se divide en tres etapas de interpolación: generación funcional de Fourier, configuración de escenas objetos con alta precisión mediante interacción del usuario, y edición de objetivos mediante instrucciones naturales. En cada etapa, grandes modelos de lenguaje se utilizan para generar razones CoT contextualmente consistentes y guiar un modelo de sonido basado en el sonido. Además, presentamos un conjunto de datos detallado llamado AudioCoT, que tiene notas de razones estructuradas, estableciendo así la conexión entre el contenido visual, la explicación del contexto y la síntesis de sonidos. Los experimentos muestran que ThinkSound alcanza los mejores rendimientos en la generación de sonido desde vídeo, tanto en evaluaciones de sonido como en evaluaciones de CoT, y supera la distribución offline del Movie Gen Audio benchmark. La página de demo está disponible en https://ThinkSound-Project.github.io.",
      "upvotes": 1,
      "discussionId": "68636ecd588cea0da970c90c",
      "projectPage": "https://thinksound-project.github.io/",
      "githubRepo": "https://github.com/liuhuadai/ThinkSound",
      "ai_summary": "ThinkSound, a novel framework, uses Chain-of-Thought reasoning with a multimodal large language model to generate high-quality audio from videos, achieving state-of-the-art results in various benchmarks.",
      "ai_keywords": [
        "Chain-of-Thought (CoT) reasoning",
        "multimodal large language model",
        "unified audio foundation model",
        "AudioCoT",
        "video-to-audio generation",
        "foley generation",
        "object-centric refinement",
        "targeted editing",
        "Audio Metrics",
        "CoT metrics",
        "Movie Gen Audio benchmark"
      ],
      "githubStars": 23
    },
    "publishedAt": "2025-06-26T12:32:06.000Z",
    "title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language\n  Models for Audio Generation and Editing",
    "summary": "While end-to-end video-to-audio generation has greatly improved, producing\nhigh-fidelity audio that authentically captures the nuances of visual content\nremains challenging. Like professionals in the creative industries, such\ngeneration requires sophisticated reasoning about items such as visual\ndynamics, acoustic environments, and temporal relationships. We present\nThinkSound, a novel framework that leverages Chain-of-Thought (CoT) reasoning\nto enable stepwise, interactive audio generation and editing for videos. Our\napproach decomposes the process into three complementary stages: foundational\nfoley generation that creates semantically coherent soundscapes, interactive\nobject-centric refinement through precise user interactions, and targeted\nediting guided by natural language instructions. At each stage, a multimodal\nlarge language model generates contextually aligned CoT reasoning that guides a\nunified audio foundation model. Furthermore, we introduce AudioCoT, a\ncomprehensive dataset with structured reasoning annotations that establishes\nconnections between visual content, textual descriptions, and sound synthesis.\nExperiments demonstrate that ThinkSound achieves state-of-the-art performance\nin video-to-audio generation across both audio metrics and CoT metrics and\nexcels in out-of-distribution Movie Gen Audio benchmark. The demo page is\navailable at https://ThinkSound-Project.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d8c0d3da4f72339241c7dd",
      "avatarUrl": "/avatars/c5852fa7d2b8ffb7a76f0143faa453ef.svg",
      "fullname": "liuhuadai",
      "name": "liuhuadai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]