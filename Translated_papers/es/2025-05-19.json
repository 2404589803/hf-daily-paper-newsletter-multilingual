[
  {
    "paper": {
      "id": "2505.09388",
      "authors": [
        {
          "_id": "68299e3128752b51372d31ea",
          "user": {
            "_id": "62088594a5943c8a8fc94560",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png",
            "isPro": false,
            "fullname": "An Yang",
            "user": "yangapku",
            "type": "user"
          },
          "name": "An Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:43:00.733Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31eb",
          "user": {
            "_id": "6799128b9da39716ab1ebd95",
            "avatarUrl": "/avatars/677d8ae2087137134c3f0e58f4cf769f.svg",
            "isPro": false,
            "fullname": "Anfeng Li",
            "user": "laf070810",
            "type": "user"
          },
          "name": "Anfeng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:15:44.771Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ec",
          "user": {
            "_id": "64b0a77df12b47366663884c",
            "avatarUrl": "/avatars/a212ea862abb5966060e439dd0e7656f.svg",
            "isPro": false,
            "fullname": "Baosong Yang",
            "user": "Baosong",
            "type": "user"
          },
          "name": "Baosong Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:15:37.853Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ed",
          "user": {
            "_id": "64b93578ee257c3a4cfceed1",
            "avatarUrl": "/avatars/e6188562254f75a09b4048b800860016.svg",
            "isPro": false,
            "fullname": "Beichen Zhang",
            "user": "BeichenZhang",
            "type": "user"
          },
          "name": "Beichen Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:16:13.672Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ee",
          "user": {
            "_id": "61e4c4ca1ab24785ac11ba69",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg",
            "isPro": false,
            "fullname": "Binyuan Hui",
            "user": "huybery",
            "type": "user"
          },
          "name": "Binyuan Hui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:16:22.151Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ef",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f0",
          "user": {
            "_id": "6583ab7983a9e1460c67d876",
            "avatarUrl": "/avatars/74400bc448c3f07e23a4cd53d68a6af7.svg",
            "isPro": false,
            "fullname": "bowen",
            "user": "bowenYu",
            "type": "user"
          },
          "name": "Bowen Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:16:31.453Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f1",
          "name": "Chang Gao",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f2",
          "name": "Chengen Huang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f3",
          "name": "Chenxu Lv",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f4",
          "user": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "isPro": false,
            "fullname": "Chujie Zheng",
            "user": "chujiezheng",
            "type": "user"
          },
          "name": "Chujie Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:43:04.798Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f5",
          "user": {
            "_id": "6434d4989bd5a84b5dd0b0f5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg",
            "isPro": false,
            "fullname": "Dayiheng Liu",
            "user": "Losin94",
            "type": "user"
          },
          "name": "Dayiheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:17:32.677Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f6",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f7",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f8",
          "name": "Feng Hu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f9",
          "name": "Hao Ge",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fa",
          "user": {
            "_id": "6436618aeef1f55654a9f458",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6436618aeef1f55654a9f458/OvxGtuDg2GAFG9As-2hzW.jpeg",
            "isPro": false,
            "fullname": "Haoran Wei",
            "user": "HaoranWei",
            "type": "user"
          },
          "name": "Haoran Wei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:17:56.110Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fb",
          "name": "Huan Lin",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fc",
          "user": {
            "_id": "63281d05ac205d01918b5fc7",
            "avatarUrl": "/avatars/fc3e0f7285bb2869a92670f764dfc535.svg",
            "isPro": false,
            "fullname": "Jialong Tang",
            "user": "Jialong",
            "type": "user"
          },
          "name": "Jialong Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:18:16.959Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fd",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fe",
          "user": {
            "_id": "654bead777401b47e6424f88",
            "avatarUrl": "/avatars/7bcbdbb051c93b004f0dc3ad36c4a0ce.svg",
            "isPro": false,
            "fullname": "Jianhong Tu",
            "user": "ToviTu",
            "type": "user"
          },
          "name": "Jianhong Tu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:18:30.045Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ff",
          "name": "Jianwei Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3200",
          "name": "Jianxin Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3201",
          "name": "Jiaxi Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3202",
          "name": "Jing Zhou",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3203",
          "user": {
            "_id": "602f88f5e8149a962412a667",
            "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
            "isPro": false,
            "fullname": "Zhou",
            "user": "Jingren",
            "type": "user"
          },
          "name": "Jingren Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:20:51.253Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3204",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3205",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3206",
          "name": "Keqin Bao",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3207",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3208",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3209",
          "name": "Lianghao Deng",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320a",
          "name": "Mei Li",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320b",
          "user": {
            "_id": "5f8946925d083370c711f296",
            "avatarUrl": "/avatars/14246aae3b1f8b7ad050f8ff2c8b260e.svg",
            "isPro": false,
            "fullname": "Mingfeng Xue",
            "user": "mingfengxue",
            "type": "user"
          },
          "name": "Mingfeng Xue",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:21:56.048Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320c",
          "name": "Mingze Li",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320d",
          "name": "Pei Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320e",
          "user": {
            "_id": "62f220ccee7d7af44979efc7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f220ccee7d7af44979efc7/RImNglMumGCpAKB5gin6k.jpeg",
            "isPro": false,
            "fullname": "Peng Wang",
            "user": "ZJUPeng",
            "type": "user"
          },
          "name": "Peng Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:43:02.813Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320f",
          "name": "Qin Zhu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3210",
          "name": "Rui Men",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3211",
          "user": {
            "_id": "6629ed94aabce1b25c3db90c",
            "avatarUrl": "/avatars/cbc39db81c8e8f950d3bd2c2e03f71c8.svg",
            "isPro": false,
            "fullname": "Ruize Gao",
            "user": "gaoruize",
            "type": "user"
          },
          "name": "Ruize Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:21:46.295Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3212",
          "name": "Shixuan Liu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3213",
          "name": "Shuang Luo",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3214",
          "name": "Tianhao Li",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3215",
          "name": "Tianyi Tang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3216",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3217",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3218",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3219",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321a",
          "name": "Xuancheng Ren",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321b",
          "name": "Yang Fan",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321c",
          "name": "Yang Su",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321d",
          "name": "Yichang Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321e",
          "name": "Yinger Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321f",
          "name": "Yu Wan",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3220",
          "user": {
            "_id": "666aacfb918ba11c7c598194",
            "avatarUrl": "/avatars/45bee8f1fdbdd256ee47d25e4bf01a7a.svg",
            "isPro": false,
            "fullname": "Yuqiong Liu",
            "user": "lyq333",
            "type": "user"
          },
          "name": "Yuqiong Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:20:06.363Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3221",
          "name": "Zekun Wang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3222",
          "user": {
            "_id": "672c25ca8cfb61188128eb6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FJWy9Tt7UQmu9KcTOx3Rt.png",
            "isPro": false,
            "fullname": "Zeyu Cui",
            "user": "misakamage",
            "type": "user"
          },
          "name": "Zeyu Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:19:43.843Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3223",
          "name": "Zhenru Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3224",
          "name": "Zhipeng Zhou",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3225",
          "user": {
            "_id": "647ccbd6e07cf9bb2d485244",
            "avatarUrl": "/avatars/e8915abaff04f6762247e196b7cf84df.svg",
            "isPro": false,
            "fullname": "Zihan Qiu",
            "user": "QwQZh",
            "type": "user"
          },
          "name": "Zihan Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:18:58.545Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T13:41:34.000Z",
      "submittedOnDailyAt": "2025-05-19T01:23:20.310Z",
      "title": "Qwen3 Reporte Técnico\n\n(Tenga en cuenta que, aunque solicitaste solo el resultado de la traducción, para asegurar la profundidad y precisión, proporcionaré una traducción completa, manteniendo la estructura y formato del texto original.)\n\nReporte Técnico de Qwen3\n\nEl Reporte Técnico de Qwen3 detalla con precisión las tecnologías clave y el rendimiento del modelo Qwen3, con el objetivo de proporcionar información útil a los usuarios y investigadores. Este informe incluye el contexto de desarrollo, los elementos técnicos, la evaluación del rendimiento y las perspectivas de futuro de Qwen3, ayudando a los usuarios y investigadores a comprender mejor el desarrollo y direcciones de la tecnología de la inteligencia artificial más reciente.",
      "submittedOnDailyBy": {
        "_id": "610b70452719facd4ea85e28",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
        "isPro": false,
        "fullname": "Chujie Zheng",
        "user": "chujiezheng",
        "type": "user"
      },
      "summary": "En este trabajo se presenta Qwen3, la última versión del famoso modelo de lenguaje Qwen. Qwen3 ha sido diseñado para mejorar el rendimiento, la eficiencia y las capacidades multilingües de una serie de grandes modelos de lenguaje (LLMs). La serie Qwen3 incluye arquitecturas tanto DENSE como Mixture-of-Expert (MoE), con un tamaño de parámetros que varía desde aproximadamente 0.6 millones hasta 235 millones. Una de las innovaciones más importantes de Qwen3 es la integración de un modo de memoria para la inferencia de pasos complejos y un modo de no memoria que responde rápidamente basándose en el contexto, lo que permite a la serie Qwen3 ser un solo marco de trabajo para modelos como el optimizado para chat (como GPT-4o) y modelos de inferencia especializados (como QwQ-32B), evitando la necesidad de intercambio entre modelos diferentes. Además, Qwen3 permite un intercambio dinámico de modos según las solicitudes del usuario o los templates de chat, y en la inferencia, asigna de manera adaptativa los recursos de la red de cálculo, manteniendo un equilibrio entre la complejidad de la tarea, la demora y el rendimiento. Además, Qwen3 aprovecha el conocimiento del modelo principal para reducir significativamente los recursos de red de cálculo necesarios para la construcción de modelos pequeños, lo que permite mejorar su rendimiento. Según evaluaciones experimentales, Qwen3 obtiene los mejores resultados en diferentes marcos de referencia, como la generación de código, la inferencia matemática y tareas de agente, y compite con modelos más grandes o especializados. En comparación con Qwen2.5, Qwen3 ha expandido el soporte de lenguajes desde 29 a 119, mejorando la comprensión y generación crítica multilingüe, lo que ha aumentado su accesibilidad global. Todos los modelos de Qwen3 están disponibles bajo la licencia de software libre Apache 2.0, con el objetivo de fomentar la reproducibilidad y el desarrollo dirigido por la comunidad.",
      "upvotes": 70,
      "discussionId": "68299e3228752b51372d325f",
      "projectPage": "https://qwenlm.github.io/blog/qwen3/",
      "githubRepo": "https://github.com/QwenLM/Qwen3",
      "ai_keywords": [
        "large language models (LLMs)",
        "Mixture-of-Expert (MoE) architectures",
        "thinking mode",
        "non-thinking mode",
        "chat-optimized models",
        "dedicated reasoning models",
        "thinking budget mechanism",
        "computational resources adaptively",
        "inference",
        "latency",
        "performance",
        "code generation",
        "mathematical reasoning",
        "agent tasks",
        "multilingual support",
        "cross-lingual understanding",
        "generation capabilities"
      ]
    },
    "publishedAt": "2025-05-14T09:41:34.000Z",
    "title": "Qwen3 Technical Report",
    "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09388.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "610b70452719facd4ea85e28",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
      "fullname": "Chujie Zheng",
      "name": "chujiezheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 37
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10610",
      "authors": [
        {
          "_id": "682adaf581c740ab4aabc5a3",
          "name": "Zhaowei Wang",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a4",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a5",
          "name": "Xiyu Ren",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a6",
          "name": "Jipeng Zhang",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a7",
          "name": "Yu Zhao",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a8",
          "name": "Rohit Saxena",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a9",
          "name": "Liang Cheng",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5aa",
          "name": "Ginny Wong",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5ab",
          "name": "Simon See",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5ac",
          "name": "Pasquale Minervini",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5ad",
          "name": "Yangqiu Song",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5ae",
          "name": "Mark Steedman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:52:54.000Z",
      "submittedOnDailyAt": "2025-05-19T08:37:50.522Z",
      "title": "MMLongBench: Índice de Evaluación para Modelos de Lenguaje de Visión de Contexto Largo\nÍndice para evaluar de manera efectiva y detallada los modelos de lenguaje de visión de contexto largo.",
      "submittedOnDailyBy": {
        "_id": "657ccbf2869d5bb0e53b482f",
        "avatarUrl": "/avatars/2eae5a10bdc14814a04d9f255f16de6b.svg",
        "isPro": false,
        "fullname": "Rohit Saxena",
        "user": "rohitsaxena",
        "type": "user"
      },
      "summary": "Con el rápido expandido del contexto de la lenguaje de negocios, los modelos de lenguaje de negocios de largo contexto (LCVLMs) han evolucionado, permitiendo procesar en una sola ejecución cientos de imágenes y tokens de texto intercambiados. En este artículo, se presenta MMLongBench, una colección de tareas de lenguaje de negocios de largo contexto, diseñada para cubrir una amplia gama de tareas y evaluar de manera efectiva y insightful los LCVLMs. MMLongBench incluye 5 tipos de tareas y 13,331 casos, abordando una amplia gama de imágenes naturales y sintéticas. Para evaluar la reacción del modelo con respecto a la longitud de los inputs, se proporcionan muestras utilizando el programa de tokenización cosmo y combinando bloques de negocios y tokens de texto en 5 longitudes de entrada estándar (8K-128K tokens). Se realiza una evaluación detallada de 46 modelos de LCVLMs, tanto propietarios como abiertos, y se realiza un análisis exhaustivo de las capacidades de largo contexto de los modelos actuales. Los resultados revelan: i) el rendimiento en una tarea es un poco representativo de la capacidad de largo contexto general; ii) tanto los modelos propietarios como los abiertos tienen limitaciones en tareas de largo contexto, con espacios para mejoras futuras; iii) los modelos con fuerte capacidad cognitiva tienden a tener mejores resultados en el largo contexto. Gracias a su amplia cobertura de tareas, diversidad de imágenes y control estricto de longitud, MMLongBench proporciona una base esencial para el diagnóstico y el progreso de los siguientes generaciones de LCVLMs.",
      "upvotes": 14,
      "discussionId": "682adaf681c740ab4aabc5e2",
      "ai_keywords": [
        "long-context vision-language models (LCVLMs)",
        "MMLongBench",
        "Visual RAG",
        "Many-Shot ICL",
        "vision patches",
        "cross-modal tokenization scheme",
        "long-context vision-language tasks",
        "reasoning ability"
      ]
    },
    "publishedAt": "2025-05-15T13:52:54.000Z",
    "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models\n  Effectively and Thoroughly",
    "summary": "The rapid extension of context windows in large vision-language models has\ngiven rise to long-context vision-language models (LCVLMs), which are capable\nof handling hundreds of images with interleaved text tokens in a single forward\npass. In this work, we introduce MMLongBench, the first benchmark covering a\ndiverse set of long-context vision-language tasks, to evaluate LCVLMs\neffectively and thoroughly. MMLongBench is composed of 13,331 examples spanning\nfive different categories of downstream tasks, such as Visual RAG and Many-Shot\nICL. It also provides broad coverage of image types, including various natural\nand synthetic images. To assess the robustness of the models to different input\nlengths, all examples are delivered at five standardized input lengths (8K-128K\ntokens) via a cross-modal tokenization scheme that combines vision patches and\ntext tokens. Through a thorough benchmarking of 46 closed-source and\nopen-source LCVLMs, we provide a comprehensive analysis of the current models'\nvision-language long-context ability. Our results show that: i) performance on\na single task is a weak proxy for overall long-context capability; ii) both\nclosed-source and open-source models face challenges in long-context\nvision-language tasks, indicating substantial room for future improvement; iii)\nmodels with stronger reasoning ability tend to exhibit better long-context\nperformance. By offering wide task coverage, various image types, and rigorous\nlength control, MMLongBench provides the missing foundation for diagnosing and\nadvancing the next generation of LCVLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10610.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657ccbf2869d5bb0e53b482f",
      "avatarUrl": "/avatars/2eae5a10bdc14814a04d9f255f16de6b.svg",
      "fullname": "Rohit Saxena",
      "name": "rohitsaxena",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11409",
      "authors": [
        {
          "_id": "682abb7984695084c1a48eab",
          "name": "Yi Xu",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eac",
          "name": "Chengzu Li",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48ead",
          "user": {
            "_id": "62b279e92375526ae51a537b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
            "isPro": false,
            "fullname": "Han Zhou",
            "user": "hzhouml",
            "type": "user"
          },
          "name": "Han Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:42:16.276Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eae",
          "user": {
            "_id": "65bf213f8467e2a3d6374d4b",
            "avatarUrl": "/avatars/0194cdba95d7a4c01fbbdd505e384a3d.svg",
            "isPro": false,
            "fullname": "X Wan",
            "user": "masonxw",
            "type": "user"
          },
          "name": "Xingchen Wan",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-19T05:02:52.536Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eaf",
          "user": {
            "_id": "63920dfac47e36ddeb8f1864",
            "avatarUrl": "/avatars/c36cbf7b084d62368312e5c9292e4260.svg",
            "isPro": false,
            "fullname": "Caiqi Zhang",
            "user": "caiqizh",
            "type": "user"
          },
          "name": "Caiqi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:23:48.005Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eb0",
          "user": {
            "_id": "617a6284941993035fbaf299",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635410461794-noauth.jpeg",
            "isPro": false,
            "fullname": "Anna Korhonen",
            "user": "akorhonen",
            "type": "user"
          },
          "name": "Anna Korhonen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:23:42.059Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eb1",
          "user": {
            "_id": "6273e70dc8d55dd434bd8e52",
            "avatarUrl": "/avatars/3483eeda218e95b1eb00c3dc63c7d000.svg",
            "isPro": false,
            "fullname": "Ivan Vulić",
            "user": "ivulic",
            "type": "user"
          },
          "name": "Ivan Vulić",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:23:36.111Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62b279e92375526ae51a537b/VYeWx-h6G2brVuuu-Wg5i.png"
      ],
      "publishedAt": "2025-05-16T16:17:22.000Z",
      "submittedOnDailyAt": "2025-05-19T03:37:48.826Z",
      "title": "Visión de diseño: solo piensa en imágenes",
      "submittedOnDailyBy": {
        "_id": "62b279e92375526ae51a537b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
        "isPro": false,
        "fullname": "Han Zhou",
        "user": "hzhouml",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de modelos de lenguaje grande (LLMs) y su expansión multimodal (MLLMs) ha mejorado significativamente la percepción de máquinas en diversas tareas. Sin embargo, estos modelos suelen depender principalmente de la información textual para expresar y estructurar la inteligencia, especialmente cuando se incluye información visual. En este artículo, argumentamos que los modelos que representan y estructuran la inteligencia de manera más natural y efectiva no dependen solo de la textura, y son especialmente importantes en tareas que incluyen información espacial y geométrica. Basándonos en esta premisa, proponemos un nuevo paradigma: 'Planificación Visual'. En este paradigma, se puede realizar planes utilizando simples representaciones visuales sin depender de texto. En este paradigma, los planes se realizan de manera iterativa, representando la inferencia en secuencias de imágenes en el dominio visual, similar a cómo una persona puede esbozar o visualiza acciones futuras. Presentamos un nuevo marco de aprendizaje reforzado 'Planificación Visual a través del Aprendizaje Reforzado (VPRL)', basado en modelos de visión a largo plazo (VLMs) y programación de red (GRPO). Este marco de aprendizaje reforzado logra significativas mejoras en tareas de navegación visual representativas (FrozenLake, Maze, MiniBehavior). Nuestro paradigma de planificación visual es superior a cualquier variante de planificación basada solo en texto en entornos espaciales, donde la lógica de la inteligencia se ejecuta. Nuestros resultados demuestran que la planificación visual puede sustituir la lógica basada en lenguaje. Abrimos una nueva ruta en tareas que requieren inferencia intuitiva basada en imágenes.",
      "upvotes": 10,
      "discussionId": "682abb7c84695084c1a48fb4",
      "githubRepo": "https://github.com/yix8/VisualPlanning",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "multimodal extensions (MLLMs)",
        "machine reasoning",
        "visual information",
        "Visual Planning",
        "purely visual representations",
        "sequences of images",
        "step-by-step inference",
        "Visual Planning via Reinforcement Learning (VPRL)",
        "GRPO",
        "post-training large vision models",
        "planning",
        "visual navigation tasks",
        "FrozenLake",
        "Maze",
        "MiniBehavior",
        "text-only space",
        "intuitive, image-based inference"
      ]
    },
    "publishedAt": "2025-05-16T12:17:22.000Z",
    "title": "Visual Planning: Let's Think Only with Images",
    "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62b279e92375526ae51a537b/VYeWx-h6G2brVuuu-Wg5i.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11409.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b279e92375526ae51a537b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
      "fullname": "Han Zhou",
      "name": "hzhouml",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.07675",
      "authors": [
        {
          "_id": "6829dcab0daa5ccc817e6ec8",
          "name": "Seongjae Kang",
          "hidden": false
        },
        {
          "_id": "6829dcab0daa5ccc817e6ec9",
          "user": {
            "_id": "64f000769e7770db74d44bba",
            "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
            "isPro": false,
            "fullname": "Dong-Bok Lee",
            "user": "dongboklee",
            "type": "user"
          },
          "name": "Dong Bok Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:42:58.152Z",
          "hidden": false
        },
        {
          "_id": "6829dcab0daa5ccc817e6eca",
          "name": "Hyungjoon Jang",
          "hidden": false
        },
        {
          "_id": "6829dcab0daa5ccc817e6ecb",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T15:39:51.000Z",
      "submittedOnDailyAt": "2025-05-19T06:17:24.942Z",
      "title": "Realizar un simple recopilación de conocimientos a través del Vision Label Model y optimizando con Double Head.",
      "submittedOnDailyBy": {
        "_id": "64f000769e7770db74d44bba",
        "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
        "isPro": false,
        "fullname": "Dong-Bok Lee",
        "user": "dongboklee",
        "type": "user"
      },
      "summary": "Visión-lenguaje modelos (VLMs) han logrado éxitos impresionantes en diversas tareas utilizando información contextual rica a través de datos estándarizados mínimos. Sin embargo, la implementación de modelos de esta escala es particularmente desafiante en entornos con limitaciones de recursos. La técnica de conocimiento distillado (KD) es una solución ya establecida para abordar estos problemas. Sin embargo, los métodos de KD recientes en VLMs han incrementado el sobrecarga computacional y la complejidad de optimización a través de aprendizaje multi-etapa o ajustes adicionales. En este artículo, se propone un marco KD sencillo y eficaz para transmitir conocimiento de modelos especializados de tareas a modelos compresibles en VLMs, proponiendo el \\texttt{D}ual-\\texttt{H}ead \\texttt{O}ptimization (DHO). Específicamente, se introduce un cabeza de predicción adicional que se entrena independientemente de los datos etiquetados y las predicciones del profesor, y se propone que su salida se combine linealmente en el momento de la inferencia. DHO normaliza y reduce el conflicto de gradientes entre señales combinadas, permitiendo un aprendizaje de características más efectivo que la línea basada en KD de una sola cabeza. Los resultados de experimentos ampliados muestran que DHO sigue a la línea base en varios conjuntos de datos. En particular, se alcanza una nueva rendimiento en ImageNet, mejorando la precisión en un 3% con 1% de datos etiquetados y en un 0.1% con 10% de datos etiquetados, usando pocos parámetros.",
      "upvotes": 8,
      "discussionId": "6829dcad0daa5ccc817e6f40",
      "ai_keywords": [
        "Vision-language models (VLMs)",
        "knowledge distillation (KD)",
        "dual prediction heads",
        "gradient conflicts",
        "feature learning",
        "semi-supervised settings",
        "state-of-the-art performance",
        "ImageNet",
        "accuracy"
      ]
    },
    "publishedAt": "2025-05-12T11:39:51.000Z",
    "title": "Simple Semi-supervised Knowledge Distillation from Vision-Language\n  Models via texttt{D}ual-texttt{H}ead\n  texttt{O}ptimization",
    "summary": "Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\ntexttt{D}ual-texttt{H}ead\ntexttt{O}ptimization (texttt{DHO}) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that DHO mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that DHO\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07675.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64f000769e7770db74d44bba",
      "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
      "fullname": "Dong-Bok Lee",
      "name": "dongboklee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11107",
      "authors": [
        {
          "_id": "682ad96cdc6d7453624831b9",
          "user": {
            "_id": "6213410828005421265b27d3",
            "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
            "isPro": false,
            "fullname": "許湛然",
            "user": "Splend1dchan",
            "type": "user"
          },
          "name": "Chan-Jan Hsu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:23:15.798Z",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831ba",
          "name": "Davide Buffelli",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bb",
          "name": "Jamie McGowan",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bc",
          "name": "Feng-Ting Liao",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bd",
          "name": "Yi-Chang Chen",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831be",
          "name": "Sattar Vakili",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bf",
          "name": "Da-shan Shiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T10:40:35.000Z",
      "submittedOnDailyAt": "2025-05-19T05:58:53.531Z",
      "title": "Grupo Signiping: Cooperación de Inteligencia Artificial Lógica paralela a nivel de Tokens",
      "submittedOnDailyBy": {
        "_id": "6213410828005421265b27d3",
        "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
        "isPro": false,
        "fullname": "許湛然",
        "user": "Splend1dchan",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de grandes modelos de lenguaje (LLMs) ha demostrado la capacidad de realizar razonamientos lógicos a través de secuencias de pensamiento autogenerados. Los grupos de razonamiento lógico, en su mayoría, pueden mejorar la calidad de los resultados individuales hacia una razonamiento lógico más común. Sin embargo, estos grupos suelen interactuar en un modo de paso alterno, incrementando la retroalimentación para mejorar la calidad. En este artículo, se propone un solo LLM llamado Group Think. Este modelo funciona como un grupo de múltiples ejes de razonamiento lógico (sincareo) paralelos. Al compartir la progresión parcial de generación de otros ejes, Group Think adapta dinámicamente las trayectorias lógicas de los otros ejes en el nivel de tokens, introduciendo nuevos patrones de razonamiento paralelos. Por ejemplo, si un eje de razonamiento detecta que un hilo de razonamiento puede continuar en una posición más adecuada, puede cambiar de generación en medio. La colaboración en el nivel de tokens permite que Group Think reduzca los razonamientos repetitivos, mejore la calidad y al mismo tiempo alcanza una retroalimentación muy baja. Además, gracias a su paralelismo, puede utilizar eficientemente los recursos de computación vacíos y es especialmente adecuado para inferencia en las nubes. En este artículo, se propone un cambio sencillo y generalizable para que todos los LLMs puedan realizar Group Think en GPUs locales. Además, se presenta una estrategia de evaluación para benchmarkear la retroalimentación de los razonamientos lógicos y se muestra experimentalmente cómo el Group Think mejora la retroalimentación utilizando un LLM abierto y sin entrenamiento explícito. Esta investigación indica que los futuros LLMs mostrarán comportamientos de colaboración más complejos y eficientes, orientándose hacia la generación de mayor calidad.",
      "upvotes": 7,
      "discussionId": "682ad96ddc6d7453624831f3",
      "ai_keywords": [
        "large language models (LLMs)",
        "reasoning through self-generated chains of thought",
        "reasoning agents",
        "turn-based manner",
        "Group Think",
        "concurrent reasoning agents",
        "think ers",
        "shared visibility",
        "reasoning trajectories",
        "token level",
        "reasoning thread",
        "fine-grained, token-level collaboration",
        "redundant reasoning",
        "edge inference",
        "modification",
        "LLMs",
        "local GPU",
        "evaluation strategy",
        "reasoning latency"
      ]
    },
    "publishedAt": "2025-05-16T06:40:35.000Z",
    "title": "Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity",
    "summary": "Recent advances in large language models (LLMs) have demonstrated the power\nof reasoning through self-generated chains of thought. Multiple reasoning\nagents can collaborate to raise joint reasoning quality above individual\noutcomes. However, such agents typically interact in a turn-based manner,\ntrading increased latency for improved quality. In this paper, we propose Group\nThink--a single LLM that acts as multiple concurrent reasoning agents, or\nthinkers. With shared visibility into each other's partial generation progress,\nGroup Think introduces a new concurrent-reasoning paradigm in which multiple\nreasoning trajectories adapt dynamically to one another at the token level. For\nexample, a reasoning thread may shift its generation mid-sentence upon\ndetecting that another thread is better positioned to continue. This\nfine-grained, token-level collaboration enables Group Think to reduce redundant\nreasoning and improve quality while achieving significantly lower latency.\nMoreover, its concurrent nature allows for efficient utilization of idle\ncomputational resources, making it especially suitable for edge inference,\nwhere very small batch size often underutilizes local~GPUs. We give a simple\nand generalizable modification that enables any existing LLM to perform Group\nThink on a local GPU. We also present an evaluation strategy to benchmark\nreasoning latency and empirically demonstrate latency improvements using\nopen-source LLMs that were not explicitly trained for Group Think. We hope this\nwork paves the way for future LLMs to exhibit more sophisticated and more\nefficient collaborative behavior for higher quality generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11107.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6213410828005421265b27d3",
      "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
      "fullname": "許湛然",
      "name": "Splend1dchan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11427",
      "authors": [
        {
          "_id": "682ad9809506a7e45a93be00",
          "user": {
            "_id": "6318e7a2acffc70bd4e057ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6318e7a2acffc70bd4e057ec/2m3XSbNLwv7Kmo8qfWq3L.jpeg",
            "isPro": false,
            "fullname": "Adrian Robert Minut",
            "user": "adrianrob",
            "type": "user"
          },
          "name": "Adrian Robert Minut",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:22.059Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be01",
          "user": {
            "_id": "63ab16a6d7ee953f604ecd52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ab16a6d7ee953f604ecd52/ujylOpczHKxU6Kfr-jGVr.png",
            "isPro": false,
            "fullname": "Tommaso Mencattini",
            "user": "tmencatt",
            "type": "user"
          },
          "name": "Tommaso Mencattini",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:22:21.898Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be02",
          "user": {
            "_id": "5e8ef1f14957053f606489e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
            "isPro": false,
            "fullname": "Andrea Santilli",
            "user": "teelinsan",
            "type": "user"
          },
          "name": "Andrea Santilli",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:22:26.518Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be03",
          "user": {
            "_id": "64256584daa3502ee3570b86",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64256584daa3502ee3570b86/kui0eb59S5aTUeZIjawUj.jpeg",
            "isPro": false,
            "fullname": "Donato Crisostomi",
            "user": "crisostomi",
            "type": "user"
          },
          "name": "Donato Crisostomi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:28.737Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be04",
          "user": {
            "_id": "652681664e066bf73f8e2bd1",
            "avatarUrl": "/avatars/084dec4765d9996d74901b8df95ec35f.svg",
            "isPro": false,
            "fullname": "Emanuele Rodola'",
            "user": "erodola",
            "type": "user"
          },
          "name": "Emanuele Rodolà",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:35.566Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T16:43:23.000Z",
      "submittedOnDailyAt": "2025-05-19T05:45:27.421Z",
      "title": "Merginet: Modelo de evolución sencillo para la integración de herramientas Merginet",
      "submittedOnDailyBy": {
        "_id": "5e8ef1f14957053f606489e6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
        "isPro": false,
        "fullname": "Andrea Santilli",
        "user": "teelinsan",
        "type": "user"
      },
      "summary": "Modelo puede ser integrado. Se puede integrar las funciones de un modelo existente en un nuevo modelo. No es necesario adicionalmente entrenarlo. Esto reduce costos y apoya bibliotecas que utilizan procesadores gráficos para consumidores, mejorando la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficiencia. Esto mejora la eficienci",
      "upvotes": 6,
      "discussionId": "682ad9819506a7e45a93be38",
      "githubRepo": "https://github.com/tommasomncttn/mergenetic",
      "ai_keywords": [
        "model merging",
        "evolutionary algorithms",
        "Mergenetic",
        "fitness estimators",
        "evaluation costs"
      ]
    },
    "publishedAt": "2025-05-16T12:43:23.000Z",
    "title": "Mergenetic: a Simple Evolutionary Model Merging Library",
    "summary": "Model merging allows combining the capabilities of existing models into a new\none - post hoc, without additional training. This has made it increasingly\npopular thanks to its low cost and the availability of libraries that support\nmerging on consumer GPUs. Recent work shows that pairing merging with\nevolutionary algorithms can boost performance, but no framework currently\nsupports flexible experimentation with such strategies in language models. We\nintroduce Mergenetic, an open-source library for evolutionary model merging.\nMergenetic enables easy composition of merging methods and evolutionary\nalgorithms while incorporating lightweight fitness estimators to reduce\nevaluation costs. We describe its design and demonstrate that Mergenetic\nproduces competitive results across tasks and languages using modest hardware.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e8ef1f14957053f606489e6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
      "fullname": "Andrea Santilli",
      "name": "teelinsan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10962",
      "authors": [
        {
          "_id": "682ab4fe7a9f1a7ec9779dd6",
          "user": {
            "_id": "62ffa3f8311cad266f9af236",
            "avatarUrl": "/avatars/4c88cb518e000a475f8381573f21aa7f.svg",
            "isPro": false,
            "fullname": "Zhenwen Liang",
            "user": "invokerliang",
            "type": "user"
          },
          "name": "Zhenwen Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:24:26.692Z",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dd7",
          "user": {
            "_id": "64c94eddcb2f1bf0e7db5a4d",
            "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
            "isPro": false,
            "fullname": "Linfeng Song",
            "user": "freesunshine0316",
            "type": "user"
          },
          "name": "Linfeng Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:24:45.999Z",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dd8",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dd9",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dda",
          "name": "Feng Zhang",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779ddb",
          "user": {
            "_id": "65147a1426fbd558dbd08f1b",
            "avatarUrl": "/avatars/86574ee2d5c22e940be1c4e50be88675.svg",
            "isPro": false,
            "fullname": "Haitao Mi",
            "user": "haitaominlp",
            "type": "user"
          },
          "name": "Haitao Mi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:24:56.781Z",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779ddc",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T07:56:03.000Z",
      "submittedOnDailyAt": "2025-05-19T03:06:11.065Z",
      "title": "MPS-Prover: Ejecución de pruebas de demostración de ordenamiento por etapas mediante búsqueda y crianza de datos en Multi-Person Shop",
      "submittedOnDailyBy": {
        "_id": "64c94eddcb2f1bf0e7db5a4d",
        "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
        "isPro": false,
        "fullname": "Linfeng Song",
        "user": "freesunshine0316",
        "type": "user"
      },
      "summary": "Automatic Theorem Proving (ATP) es un desafío complejo para la inteligencia artificial en lenguajes formales, que requiere inferencias lógicas estrictas y un gran espacio de búsqueda. Además, los modelos de lenguaje de gran escala (LLMs) han demostrado un rendimiento inteligente, pero actualmente, los pasos de prueba de razonamiento pueden conducir a estrategias de prueba inadecuadas a través de guías de búsqueda sesgadas, lo que disminuye la eficiencia. En este artículo, se presenta un nuevo sistema de ATP de pasos de prueba, el Multi-Prospective Proof Prover (MPS-Prover), diseñado para superar estas limitaciones. MPS-Prover introduce dos innovaciones clave: una estrategia de edición de datos de entrenamiento posterior que reduce aproximadamente el 40% de los datos de entrenamiento redundantes sin perder eficiencia, y una combinación de un modelo de evaluación entrenado y reglas heurísticas estratégicamente diseñadas para seleccionar diversas estrategias de búsqueda, evitar estados inútiles y mejorar la robustez de la búsqueda. En evaluaciones de extensión, MPS-Prover alcanza los mejores rendimientos en benchmarks difíciles como miniF2F y ProofNet, superando los modelos de 7B parámetros existentes. Además, la análisis muestra que MPS-Prover genera pruebas de razonamiento más cortas y diversas en comparación con los métodos de prueba de pasos y de todo el proceso, lo que caracteriza su eficiencia y efectividad. Nuestra investigación ofrece un marco sólido y un análisis detallado para el desarrollo de capacidades lógicas formales basadas en modelos de lenguaje de gran escala y el desarrollo de un mejor prover de razonamiento.",
      "upvotes": 5,
      "discussionId": "682ab4ff7a9f1a7ec9779e71",
      "ai_keywords": [
        "Automated Theorem Proving (ATP)",
        "large language models (LLMs)",
        "biased search guidance",
        "Multi-Perspective Search Prover (MPS-Prover)",
        "post-training data curation strategy",
        "multi-perspective tree search mechanism",
        "learned critic model",
        "heuristic rules",
        "tactic selection",
        "search robustness",
        "miniF2F",
        "ProofNet",
        "state-of-the-art performance",
        "formal reasoning"
      ]
    },
    "publishedAt": "2025-05-16T03:56:03.000Z",
    "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective\n  Search and Data Curation",
    "summary": "Automated Theorem Proving (ATP) in formal languages remains a formidable\nchallenge in AI, demanding rigorous logical deduction and navigating vast\nsearch spaces. While large language models (LLMs) have shown promising\nperformance, existing stepwise provers often suffer from biased search\nguidance, leading to inefficiencies and suboptimal proof strategies. This paper\nintroduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise\nATP system designed to overcome these limitations. MPS-Prover incorporates two\nkey innovations: a highly effective post-training data curation strategy that\nprunes approximately 40% of redundant training data without sacrificing\nperformance, and a multi-perspective tree search mechanism. This search\nintegrates a learned critic model with strategically designed heuristic rules\nto diversify tactic selection, prevent getting trapped in unproductive states,\nand enhance search robustness. Extensive evaluations demonstrate that\nMPS-Prover achieves state-of-the-art performance on multiple challenging\nbenchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter\nmodels. Furthermore, our analyses reveal that MPS-Prover generates\nsignificantly shorter and more diverse proofs compared to existing stepwise and\nwhole-proof methods, highlighting its efficiency and efficacy. Our work\nadvances the capabilities of LLM-based formal reasoning and offers a robust\nframework and a comprehensive analysis for developing more powerful theorem\nprovers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10962.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c94eddcb2f1bf0e7db5a4d",
      "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
      "fullname": "Linfeng Song",
      "name": "freesunshine0316",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10518",
      "authors": [
        {
          "_id": "68271c682f2e31ef0667bfaf",
          "user": {
            "_id": "668e4d1b446c8736208d99e1",
            "avatarUrl": "/avatars/dbe10f3b181e789d98b9b6bde4f711b2.svg",
            "isPro": false,
            "fullname": "Anastasios Gerontopoulos",
            "user": "nasos10",
            "type": "user"
          },
          "name": "Anastasios Gerontopoulos",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-18T19:39:25.735Z",
          "hidden": false
        },
        {
          "_id": "68271c682f2e31ef0667bfb0",
          "name": "Spyros Gidaris",
          "hidden": false
        },
        {
          "_id": "68271c682f2e31ef0667bfb1",
          "name": "Nikos Komodakis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/668e4d1b446c8736208d99e1/VMNrdGj9BjgRO8fMsagaH.png"
      ],
      "publishedAt": "2025-05-15T17:25:03.000Z",
      "submittedOnDailyAt": "2025-05-19T07:50:27.978Z",
      "title": "Multi-Token Prediction Needs Registers",
      "submittedOnDailyBy": {
        "_id": "668e4d1b446c8736208d99e1",
        "avatarUrl": "/avatars/dbe10f3b181e789d98b9b6bde4f711b2.svg",
        "isPro": false,
        "fullname": "Anastasios Gerontopoulos",
        "user": "nasos10",
        "type": "user"
      },
      "summary": "Datok prediction ha aparecido como un objetivo potencial para mejorar el entrenamiento previo de modelos de lenguaje, pero su beneficio no se mantiene consistente con otras configuraciones y no se generaliza. En este artículo, proponemos un enfoque sencillo y efectivo para la predicción de Datok, donde se insertan tokenes de registro en la secuencia de entrada. Comparado con los métodos existentes, MuToR ofrece las siguientes ventajas principales: el número de parámetros adicionales es mínimo, no se requiere cambio estructural - garantizando la compatibilidad con modelos de lenguaje previamente entrenados - y se desarrolla según el objetivo de entrenamiento previo del siguiente token, lo que es especialmente adecuado para el fin de entrenamiento con retroalimentación de subconjuntos. Además, soporta de manera natural la Predicción Scalable Holizon. En diversos casos, se demuestra la eficacia y diversidad de MuToR en tareas de generación complejas en el campo de lenguaje y visión, incluyendo el fin de entrenamiento con retroalimentación de subconjuntos y el fin de entrenamiento eficiente de parámetros (PEFT). El código está disponible en la siguiente URL: https://github.com/nasosger/MuToR.",
      "upvotes": 3,
      "discussionId": "68271c692f2e31ef0667bff6",
      "githubRepo": "https://github.com/nasosger/MuToR",
      "ai_keywords": [
        "register tokens",
        "multi-token prediction",
        "next-token pretraining",
        "parameter-efficient fine-tuning (PEFT)",
        "generative tasks"
      ]
    },
    "publishedAt": "2025-05-15T13:25:03.000Z",
    "title": "Multi-Token Prediction Needs Registers",
    "summary": "Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/668e4d1b446c8736208d99e1/VMNrdGj9BjgRO8fMsagaH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10518.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668e4d1b446c8736208d99e1",
      "avatarUrl": "/avatars/dbe10f3b181e789d98b9b6bde4f711b2.svg",
      "fullname": "Anastasios Gerontopoulos",
      "name": "nasos10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11152",
      "authors": [
        {
          "_id": "682a9a3f5e6f0c59f4d8a0e5",
          "user": {
            "_id": "65601c6ee23401f82005e361",
            "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
            "isPro": false,
            "fullname": "Daniel Sungho Jung",
            "user": "dqj5182",
            "type": "user"
          },
          "name": "Daniel Sungho Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:42:43.502Z",
          "hidden": false
        },
        {
          "_id": "682a9a3f5e6f0c59f4d8a0e6",
          "user": {
            "_id": "656056b21392aa3beb5de0bd",
            "avatarUrl": "/avatars/07f25b750ef308d65f2e6c82506e7816.svg",
            "isPro": false,
            "fullname": "Kyoung Mu  Lee ",
            "user": "kyoungmu",
            "type": "user"
          },
          "name": "Kyoung Mu Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:48.640Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T11:54:25.000Z",
      "submittedOnDailyAt": "2025-05-19T01:11:35.713Z",
      "title": "DENSIT HAND estima puntos de contacto mientras se entrena con datos desbalanceados.",
      "submittedOnDailyBy": {
        "_id": "65601c6ee23401f82005e361",
        "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
        "isPro": false,
        "fullname": "Daniel Sungho Jung",
        "user": "dqj5182",
        "type": "user"
      },
      "summary": "Los manos desempeñan un papel importante en la interacción humana, y comprender la contacto de los manos con el mundo es crucial para un entendimiento integral de sus funciones. Recientemente, el número de conjuntos de datos de interacción de manos, que incluyen objetos, otros manos, escenas y el cuerpo, ha aumentado. Basándose en la importancia de este trabajo y el aumento de datos de alta calidad, se ha demostrado que aún no se han explorado métodos efectivos para estimar la densidad de la contacto de los manos. Para estimar la densidad de la contacto de los manos, se encuentran dos principales problemas: primero, existe una gran disparidad de clases en los conjuntos de datos de contacto, ya que muchos ejemplos no tienen contacto; segundo, hay un problema de desbalance en la frecuencia de contacto en las extremidades de los manos, lo que dificulta la generalización del contacto en otras áreas. Para resolver estos problemas, proponemos un marco de aprendizaje para estimar la densidad de contacto de los manos (HACO) a partir de datos desbalanceados. Para abordar el problema de disparidad de clases, introducimos un método de muestreo equilibrado y realizamos muestreos en diferentes grupos para representar adecuadamente las estadísticas de contacto y no de contacto. Además, para resolver el problema de desbalance espectral, proponemos la pérdida de equilibrio de clases a nivel de frecuencia máxima (VCB), que asigna pesos específicos a la frecuencia de contacto en el conjunto de datos y ajusta la contribución de la pérdida en función de la distribución espacial del contacto. Finalmente, utilizando grandes conjuntos de datos de contacto de manos, podemos aprender a estimar la densidad de contacto de los manos sin considerar los problemas de disparidad de clases y espectral. El código está disponible para descargar.",
      "upvotes": 2,
      "discussionId": "682a9a405e6f0c59f4d8a125",
      "projectPage": "https://haco-release.github.io/",
      "githubRepo": "https://github.com/dqj5182/HACO_RELEASE",
      "ai_keywords": [
        "dense hand contact estimation",
        "class imbalance issue",
        "spatial imbalance issue",
        "finger tips",
        "balanced contact sampling",
        "vertex-level class-balanced (VCB) loss",
        "contact distribution",
        "contact frequency"
      ]
    },
    "publishedAt": "2025-05-16T07:54:25.000Z",
    "title": "Learning Dense Hand Contact Estimation from Imbalanced Data",
    "summary": "Hands are essential to human interaction, and understanding contact between\nhands and the world can promote comprehensive understanding of their function.\nRecently, there have been growing number of hand interaction datasets that\ncover interaction with object, other hand, scene, and body. Despite the\nsignificance of the task and increasing high-quality data, how to effectively\nlearn dense hand contact estimation remains largely underexplored. There are\ntwo major challenges for learning dense hand contact estimation. First, there\nexists class imbalance issue from hand contact datasets where majority of\nsamples are not in contact. Second, hand contact datasets contain spatial\nimbalance issue with most of hand contact exhibited in finger tips, resulting\nin challenges for generalization towards contacts in other hand regions. To\ntackle these issues, we present a framework that learns dense HAnd COntact\nestimation (HACO) from imbalanced data. To resolve the class imbalance issue,\nwe introduce balanced contact sampling, which builds and samples from multiple\nsampling groups that fairly represent diverse contact statistics for both\ncontact and non-contact samples. Moreover, to address the spatial imbalance\nissue, we propose vertex-level class-balanced (VCB) loss, which incorporates\nspatially varying contact distribution by separately reweighting loss\ncontribution of each vertex based on its contact frequency across dataset. As a\nresult, we effectively learn to predict dense hand contact estimation with\nlarge-scale hand contact data without suffering from class and spatial\nimbalance issue. The codes will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11152.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65601c6ee23401f82005e361",
      "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
      "fullname": "Daniel Sungho Jung",
      "name": "dqj5182",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11049",
      "authors": [
        {
          "_id": "682af4241286a7273c5bfd09",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0a",
          "name": "Shengfang Zhai",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0b",
          "name": "Mingzhe Du",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0c",
          "name": "Yulin Chen",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0d",
          "name": "Tri Cao",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0e",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0f",
          "name": "Cheng Wang",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd10",
          "name": "Xinfeng Li",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd11",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd12",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd13",
          "name": "Jiaheng Zhang",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd14",
          "name": "Bryan Hooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T09:46:10.000Z",
      "submittedOnDailyAt": "2025-05-19T07:36:35.140Z",
      "title": "GuardReasoner-VL: Protege VLMs con un mejor lógica.",
      "submittedOnDailyBy": {
        "_id": "6650c77a74664a42ddfb9187",
        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
        "isPro": false,
        "fullname": "yueliu1999",
        "user": "yueliu1999",
        "type": "user"
      },
      "summary": "Para mejorar la seguridad de los VLMs, este artículo presenta un nuevo modelo de protección basado en evidencias llamado \"GuardReasoner-VL\". La idea clave es recomendar que el modelo de protección proporcione una explicación antes de tomar decisiones mediante RL en línea. Primero, se construye un corpus de razones con 123K muestras y 631K etapas de razonamiento llamado \"GuardReasoner-VLTrain\". Luego, se inicializa la capacidad de razonamiento del modelo mediante SFT. Además, se desarrolla y evoluciona la capacidad de razonamiento del modelo mediante RL en línea. Específicamente, se incrementan la diversidad y la dificultad de las muestras mediante la rebaja de muestras por rechazo basada en datos con conscientización de seguridad y la afirmación de datos. También, se utilizan parámetros de clip dinámico para promover la exploración en las etapas iniciales y equilibrar el desarrollo en las etapas posteriores. Para equilibrar el rendimiento y la eficiencia de tokenes, se diseña un riesgo de seguridad basado en precisión, formato y costo de tokens. Los experimentos extensos demuestran la excelencia del modelo y superan el F1 score promedio de 19.27%. Los datos, código y modelos (3B/7B) de GuardReasoner-VL están disponibles en https://github.com/yueliu1999/GuardReasoner-VL.",
      "upvotes": 2,
      "discussionId": "682af42c1286a7273c5bfed9",
      "ai_keywords": [
        "GuardReasoner-VL",
        "online RL",
        "GuardReasoner-VLTrain",
        "reasoning corpus",
        "SFT",
        "rejection sampling",
        "data augmentation",
        "safety-aware data concatenation",
        "dynamic clipping parameter",
        "length-aware safety reward",
        "F1 score"
      ]
    },
    "publishedAt": "2025-05-16T05:46:10.000Z",
    "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning",
    "summary": "To enhance the safety of VLMs, this paper introduces a novel reasoning-based\nVLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the\nguard model to deliberatively reason before making moderation decisions via\nonline RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with\n123K samples and 631K reasoning steps, spanning text, image, and text-image\ninputs. Then, based on it, we cold-start our model's reasoning ability via SFT.\nIn addition, we further enhance reasoning regarding moderation through online\nRL. Concretely, to enhance diversity and difficulty of samples, we conduct\nrejection sampling followed by data augmentation via the proposed safety-aware\ndata concatenation. Besides, we use a dynamic clipping parameter to encourage\nexploration in early stages and exploitation in later stages. To balance\nperformance and token efficiency, we design a length-aware safety reward that\nintegrates accuracy, format, and token cost. Extensive experiments demonstrate\nthe superiority of our model. Remarkably, it surpasses the runner-up by 19.27%\nF1 score on average. We release data, code, and models (3B/7B) of\nGuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11049.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6650c77a74664a42ddfb9187",
      "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
      "fullname": "yueliu1999",
      "name": "yueliu1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11140",
      "authors": [
        {
          "_id": "682ad417500638b80a43471d",
          "user": {
            "_id": "60d33fbbd7b174177faabd4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
            "isPro": true,
            "fullname": "Mike Zhang",
            "user": "jjzha",
            "type": "user"
          },
          "name": "Mike Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:23:04.053Z",
          "hidden": false
        },
        {
          "_id": "682ad417500638b80a43471e",
          "user": {
            "_id": "678fa79005ae7fe48d03ba47",
            "avatarUrl": "/avatars/a78ab2b37fa3e18ace783f6f71f5a361.svg",
            "isPro": false,
            "fullname": "Johannes Bjerva",
            "user": "bjerva",
            "type": "user"
          },
          "name": "Johannes Bjerva",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:58.827Z",
          "hidden": false
        },
        {
          "_id": "682ad417500638b80a43471f",
          "user": {
            "_id": "60ed4c56abab3c2620df8ac8",
            "avatarUrl": "/avatars/ad5508c1c94a96f6d1290e4735e81b73.svg",
            "isPro": false,
            "fullname": "Russa Biswas",
            "user": "rubis",
            "type": "user"
          },
          "name": "Russa Biswas",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:26:04.749Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T11:39:33.000Z",
      "submittedOnDailyAt": "2025-05-19T05:25:57.460Z",
      "title": "Scaling regularization puede mejorar la realidad de grandes modelos de lenguaje.",
      "submittedOnDailyBy": {
        "_id": "60d33fbbd7b174177faabd4f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
        "isPro": true,
        "fullname": "Mike Zhang",
        "user": "jjzha",
        "type": "user"
      },
      "summary": "Recientemente, los estudios sobre la capacidad de lógica de los modelos de lenguaje grandes (LLM) han demostrado mejoras significativas en tareas matemáticas lógicas utilizando largos procesos de pensamiento y computación adicional. Sin embargo, aun no se ha demostrado que los largos casos de lógica mejoren automáticamente la precisión de la verdad, especialmente en contextos matemáticos. En este estudio, se revisa la lógica de los modelos de lenguaje grandes (LLM) en escenarios de respuesta a preguntas (QA) en dominios abiertos complejos. Primero, se presentan brevemente los trabajos lógicos de modelos grandes complejos (QwQ-32B y DeepSeek-R1-671B), y se ajustan modelos varios desde pequeños basados en Qwen2.5 hasta grandes arquitecturas. Se insertan pasos de trabajos lógicos desde grafos de conocimiento para enriquecer los trabajos lógicos. La configuración experimental incluye 4 enfoques de referencia y 6 modelos de ajuste de instrucciones, con 22.6K preguntas. Se realizaron 168 experimentos y se analizaron aproximadamente 1.7 millones de trabajos lógicos. Nuestros hallazgos muestran que un pequeño modelo lógico puede mejorar significativamente la precisión de la verdad en un solo experimento, y que este modelo ajustado a instrucciones es superior. Además, nuestro análisis muestra que la escala en el cálculo y los tokens durante la prueba mejoran positivamente la precisión de la verdad en un rango de 2-8%, demostrando claramente el efecto de la escala en la mejora de la precisión de la verdad en tareas de QA en dominios abiertos. Todos los artefactos de los experimentos se evolucionan para mejorar la precisión de la verdad.",
      "upvotes": 1,
      "discussionId": "682ad418500638b80a434770",
      "githubRepo": "https://github.com/jjzha/fs1",
      "ai_keywords": [
        "large language model (LLM)",
        "reasoning capabilities",
        "mathematical reasoning",
        "length thinking process",
        "computational resources",
        "inference",
        "complex open-domain question-answering (QA)",
        "reasoning traces",
        "reasoning models",
        "QwQ-32B",
        "DeepSeek-R1-671B",
        "instruction-tuned variants",
        "Qwen2.5",
        "knowledge graphs",
        "paths",
        "reasoning traces",
        "baseline approaches",
        "instruction-tuned models",
        "benchmark",
        "datasets",
        "experimental runs",
        "factual accuracy",
        "test-time compute",
        "token budgets",
        "test-time scaling",
        "reasoning accuracy"
      ]
    },
    "publishedAt": "2025-05-16T07:39:33.000Z",
    "title": "Scaling Reasoning can Improve Factuality in Large Language Models",
    "summary": "Recent studies on large language model (LLM) reasoning capabilities have\ndemonstrated promising improvements in model performance by leveraging a\nlengthy thinking process and additional computational resources during\ninference, primarily in tasks involving mathematical reasoning (Muennighoff et\nal., 2025). However, it remains uncertain if longer reasoning chains inherently\nenhance factual accuracy, particularly beyond mathematical contexts. In this\nwork, we thoroughly examine LLM reasoning within complex open-domain\nquestion-answering (QA) scenarios. We initially distill reasoning traces from\nadvanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then\nfine-tune a variety of models ranging from smaller, instruction-tuned variants\nto larger architectures based on Qwen2.5. To enrich reasoning traces, we\nintroduce factual information from knowledge graphs in the form of paths into\nour reasoning traces. Our experimental setup includes four baseline approaches\nand six different instruction-tuned models evaluated across a benchmark of six\ndatasets, encompassing over 22.6K questions. Overall, we carry out 168\nexperimental runs and analyze approximately 1.7 million reasoning traces. Our\nfindings indicate that, within a single run, smaller reasoning models achieve\nnoticeable improvements in factual accuracy compared to their original\ninstruction-tuned counterparts. Moreover, our analysis demonstrates that adding\ntest-time compute and token budgets factual accuracy consistently improves by\n2-8%, further confirming the effectiveness of test-time scaling for enhancing\nperformance and consequently improving reasoning accuracy in open-domain QA\ntasks. We release all the experimental artifacts for further research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11140.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d33fbbd7b174177faabd4f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
      "fullname": "Mike Zhang",
      "name": "jjzha",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11011",
      "authors": [
        {
          "_id": "682ae098730bd40a0755f87c",
          "name": "Darija Barak",
          "hidden": false
        },
        {
          "_id": "682ae098730bd40a0755f87d",
          "name": "Miguel Costa-Gomes",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T09:01:09.000Z",
      "submittedOnDailyAt": "2025-05-19T06:12:42.874Z",
      "title": "La humanidad espera de la LLM de juegos estratégicos un razonamiento y colaboración.",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "Durante el proceso en que las LLMs se integran en la interacción social y económica, es necesario un profundo entendimiento de cómo los seres humanos pueden enfrentarse a las LLMs como oponentes. Además, presentamos los resultados de los primeros experimentos controlados de compensación de costos que permiten observar diferencias en el comportamiento entre seres humanos y LLMs en la competencia de diversidad y belleza. Este experimento utiliza participantes idénticos para comparar comportamientos individuales. En este entorno, los seres humanos seleccionan números muy bajos en comparación con los LLMs en las luchas contra ellos. Esta evolución contribuye principalmente a la aumento del número de selecciones de \"zero\" Nash-equilibrium. Esta evolución muestra una asombrosa colaboración entre la estrategia humana y la estrategia de los LLMs, explicando cómo esta colaboración funciona. Nuestros hallazgos proporcionan una base fundamental para el juego de selecciones en la interacción entre seres humanos y LLMs, claramente demuestran la desigualdad entre el comportamiento y la creencia humanos, y tienen un significado importante para el diseño estructural de sistemas híbridos de seres humanos y LLMs.",
      "upvotes": 1,
      "discussionId": "682ae099730bd40a0755f8b9",
      "ai_keywords": [
        "p-beauty contest",
        "Nash-equilibrium choices",
        "strategic reasoning ability",
        "reasoning ability",
        "propensity towards cooperation",
        "mechanism design"
      ]
    },
    "publishedAt": "2025-05-16T05:01:09.000Z",
    "title": "Humans expect rationality and cooperation from LLM opponents in\n  strategic games",
    "summary": "As Large Language Models (LLMs) integrate into our social and economic\ninteractions, we need to deepen our understanding of how humans respond to LLMs\nopponents in strategic settings. We present the results of the first controlled\nmonetarily-incentivised laboratory experiment looking at differences in human\nbehaviour in a multi-player p-beauty contest against other humans and LLMs. We\nuse a within-subject design in order to compare behaviour at the individual\nlevel. We show that, in this environment, human subjects choose significantly\nlower numbers when playing against LLMs than humans, which is mainly driven by\nthe increased prevalence of `zero' Nash-equilibrium choices. This shift is\nmainly driven by subjects with high strategic reasoning ability. Subjects who\nplay the zero Nash-equilibrium choice motivate their strategy by appealing to\nperceived LLM's reasoning ability and, unexpectedly, propensity towards\ncooperation. Our findings provide foundational insights into the multi-player\nhuman-LLM interaction in simultaneous choice games, uncover heterogeneities in\nboth subjects' behaviour and beliefs about LLM's play when playing against\nthem, and suggest important implications for mechanism design in mixed\nhuman-LLM systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11011.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]