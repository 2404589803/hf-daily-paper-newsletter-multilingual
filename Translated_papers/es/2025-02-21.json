[
  {
    "paper": {
      "id": "2502.14739",
      "authors": [
        {
          "_id": "67b7efc26348a1df80a8ae53",
          "name": "M-A-P Team",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae54",
          "name": "Xinrun Du",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae55",
          "name": "Yifan Yao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae56",
          "name": "Kaijing Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae57",
          "name": "Bingli Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae58",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tianyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:24.002Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae59",
          "name": "Kang Zhu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5a",
          "user": {
            "_id": "6417d9ea8f689506e7148417",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
            "isPro": false,
            "fullname": "minghao",
            "user": "Liam-Liu",
            "type": "user"
          },
          "name": "Minghao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:25.894Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5b",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5c",
          "name": "Xiaolong Jin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5d",
          "name": "Zhenlin Wei",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5e",
          "user": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "isPro": false,
            "fullname": "Chujie Zheng",
            "user": "chujiezheng",
            "type": "user"
          },
          "name": "Chujie Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:34.124Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5f",
          "name": "Kaixing Deng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae60",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae61",
          "name": "Shian Jia",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae62",
          "name": "Sichao Jiang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae63",
          "name": "Yiyan Liao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae64",
          "name": "Rui Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae65",
          "name": "Qinrui Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae66",
          "name": "Sirun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae67",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae68",
          "name": "Yunwen Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae69",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6a",
          "user": {
            "_id": "64de37ee5e192985054be575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
            "isPro": false,
            "fullname": "Yuansheng Ni",
            "user": "yuanshengni",
            "type": "user"
          },
          "name": "Yuansheng Ni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:30.371Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6b",
          "name": "Haoran Que",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6c",
          "user": {
            "_id": "64560618bfdf9c63ce2d658a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
            "isPro": false,
            "fullname": "Mathsion Wong",
            "user": "QiYao-Wang",
            "type": "user"
          },
          "name": "Qiyao Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:28.639Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6d",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6e",
          "name": "Siwei Wu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6f",
          "name": "Tianshun Xing",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae70",
          "name": "Ming Xu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae71",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae72",
          "name": "Zekun Moore Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae73",
          "name": "Junting Zhou",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae74",
          "name": "Yuelin Bai",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae75",
          "name": "Xingyuan Bu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae76",
          "name": "Chenglin Cai",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae77",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae78",
          "name": "Yifan Chen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae79",
          "name": "Chengtuo Cheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7a",
          "name": "Tianhao Cheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7b",
          "name": "Keyi Ding",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7c",
          "name": "Siming Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7d",
          "name": "Yun Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7e",
          "name": "Yaoru Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7f",
          "name": "Yizhe Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae80",
          "name": "Zhaoqun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae81",
          "name": "Tianhao Liang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae82",
          "name": "Chengdong Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae83",
          "name": "Hongquan Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae84",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae85",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae86",
          "user": {
            "_id": "65adda5299c3bd19c74d6a8d",
            "avatarUrl": "/avatars/1ce504b64ab60f375b235ebaf81cafd6.svg",
            "isPro": false,
            "fullname": "PENG ZIFAN",
            "user": "Ziffer",
            "type": "user"
          },
          "name": "Zifan Peng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:20.429Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae87",
          "name": "Qige Qi",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae88",
          "name": "Shi Qiu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae89",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8a",
          "name": "Yizhou Tan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8b",
          "name": "Zili Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8c",
          "name": "Chenqing Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8d",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8e",
          "name": "Yiya Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8f",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae90",
          "name": "Jiajun Xu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae91",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae92",
          "name": "Ruibin Yuan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae93",
          "name": "Yuanhao Yue",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae94",
          "name": "Tianyang Zhan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae95",
          "name": "Chun Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae96",
          "name": "Jingyang Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae97",
          "name": "Xiyue Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae98",
          "name": "Xingjian Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae99",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9a",
          "name": "Yongchi Zhao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9b",
          "name": "Xiangyu Zheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9c",
          "name": "Chenghua Zhong",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9d",
          "name": "Yang Gao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9e",
          "name": "Zhoujun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9f",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea0",
          "user": {
            "_id": "612ee6a7b960e78c6d2319d4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "isPro": false,
            "fullname": "Qian Liu",
            "user": "SivilTaram",
            "type": "user"
          },
          "name": "Qian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:32.399Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea1",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea2",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea3",
          "name": "Junran Peng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea4",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea5",
          "name": "Wenbo Su",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea6",
          "name": "Guoyin Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea7",
          "name": "Shi Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea8",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea9",
          "name": "Min Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeaa",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeab",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeac",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aead",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeae",
          "user": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "isPro": false,
            "fullname": "Jiaheng Liu",
            "user": "CheeryLJH",
            "type": "user"
          },
          "name": "Jiaheng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:22.185Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeaf",
          "name": "Qunshu Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeb0",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeb1",
          "name": "Ge Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T17:05:58.000Z",
      "title": "SuperGPQA: 285 Departamentos de Estudios Evaluación de LLM Normalizada",
      "summary": "Los modelos de lenguaje grande (LLMs) han demostrado un desempeño sorprendente en las áreas principales de la ciencia, como la matemática, física y ciencia de la computación. Sin embargo, el conocimiento humano incluye más de 200 áreas especializadas y transciende el rango de los benchmarks actuales. Los LLMs han sido evaluados insuficientemente en muchas áreas especializadas en campos como la producción, agricultura y servicios. Para resolver estas deficiencias, presentamos SuperGPQA, un estricto benchmark que evalúa el conocimiento de grado de doctorado en 285 áreas y la capacidad de inferencia. Nuestro benchmark utiliza un sistema de filtrado de colaboración entre humanos y LLMs para respuestas y retroalimentación de especialistas, excluyendo solo las preguntas sencillas o inciertas. Los resultados de los experimentos muestran claramente que los más avanzados LLMs tienen un gran potencial de mejora en diversas áreas de conocimiento y que hay una clara diferencia entre los modelos actuales y la inteligencia artificial. Además, proporcionamos una guía de métodología concreta para el manejo de procesos de etiquetación grande, que incluye un sistema de colaboración entre 80 expertos especializados y LLMs, para apoyar futuros proyectos de investigación.",
      "upvotes": 61,
      "discussionId": "67b7efc66348a1df80a8afc8"
    },
    "publishedAt": "2025-02-20T22:15:33.133Z",
    "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14739.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14786",
      "authors": [
        {
          "_id": "67b7ed0d58f6b70b18dda7b4",
          "name": "Michael Tschannen",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b5",
          "name": "Alexey Gritsenko",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b6",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b7",
          "name": "Muhammad Ferjad Naeem",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b8",
          "name": "Ibrahim Alabdulmohsin",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b9",
          "name": "Nikhil Parthasarathy",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7ba",
          "name": "Talfan Evans",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bb",
          "name": "Lucas Beyer",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bc",
          "name": "Ye Xia",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bd",
          "name": "Basil Mustafa",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7be",
          "name": "Olivier Hénaff",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bf",
          "name": "Jeremiah Harmsen",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7c0",
          "name": "Andreas Steiner",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7c1",
          "name": "Xiaohua Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:08:29.000Z",
      "title": "SigLIP 2: Lenguaje de visión multilingüe que mejora la comprensión gramatical, la localización y las características densas.",
      "summary": "Introducing the Graphite Prime 2, a new multilingual vision language encoder family. This model is developed based on the success of the original Graphite Prime. The second generation has been changed to a recipe that integrates several advanced techniques developed alongside the original image-text training objectives. This includes prediction with a capture device, self-distillation (masked prediction), and online data calibration. These changes result in the Graphite Prime 2 model outperforming the original Graphite Prime model in all key features. Notably, it excels in zero-shot class classification, image-text retrieval, and vision language model (VLM) vision representation extraction. Additionally, the new training recipe shows significant improvements in both localization tasks and close prediction tasks. Furthermore, models that maintain input aspect ratios and scale to various sizes have also been trained. Trained with diverse data microcosms including device technologies, these models enhance language understanding and fairness. To offer users flexibility in balancing inference costs and performance, we release four model checkpoints of different sizes: ViT-B (86M), L (303M), So400m (400M), and g (1B).",
      "upvotes": 50,
      "discussionId": "67b7ed0e58f6b70b18dda7f4"
    },
    "publishedAt": "2025-02-20T22:33:22.039Z",
    "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14786.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14382",
      "authors": [
        {
          "_id": "67b7ed3e58f6b70b18ddb4bc",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4bd",
          "user": {
            "_id": "64ebbae6895a36ab28de811a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ebbae6895a36ab28de811a/gBiaQP4paS4L13eu-yRm7.jpeg",
            "isPro": false,
            "fullname": "Shiyi Cao",
            "user": "eva98",
            "type": "user"
          },
          "name": "Shiyi Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:43.358Z",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4be",
          "name": "Chengkun Cao",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4bf",
          "name": "Xiuyu Li",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c0",
          "name": "Shangyin Tan",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c1",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c2",
          "name": "Jiarong Xing",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c3",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c4",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:18:53.000Z",
      "title": "S*: Programación de pruebas de sincronización de tiempo de generación de código",
      "summary": "Aumentar la cantidad de cálculos durante el proceso de comprobación muestra resultados deseados en varias áreas de los modelos de lenguaje grande (LLM), pero la investigación matemática sobre la generación de código es insuficiente. En este artículo, se propone el primer marco de escalado de tiempo de comprobación híbrido S*, que significativamente mejora la cobertura y la precisión de la selección de código generado. S* supera los límites del paradigma actual de escalado paralelo agregando escalado por orden. Además, utiliza una nueva estructura de selección para generar entradas adaptativas a entradas adversas y combina información basada en ejecución para fortalecer la especificación de soluciones precisas. Se verifican 12 modelos de lenguaje y lógica de gran escala, y se obtienen los siguientes resultados: 1) S* muestra un aumento en rendimiento consistente según la familia y el tamaño del modelo, superando al modelo GPT-4o-mini. 2) S* puede superar modelos lógicos, y GPT-4o-mini supera a o1-preview en LiveCodeBench en un margen de 3.7%. 3) S* mejora aún más los modelos lógicos más avanzados, y DeepSeek-R1-Distill-Qwen-32B alcanza un 85.7% en LiveCodeBench, cercano al 88.5% de o1 (alto). El código está disponible en https://github.com/NovaSky-AI/SkyThought.",
      "upvotes": 29,
      "discussionId": "67b7ed3f58f6b70b18ddb510"
    },
    "publishedAt": "2025-02-20T22:04:42.635Z",
    "title": "S*: Test Time Scaling for Code Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14382.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14258",
      "authors": [
        {
          "_id": "67b7fa96c3f48f8b3fc632fe",
          "name": "Yein Park",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc632ff",
          "name": "Chanwoong Yoon",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63300",
          "name": "Jungwoo Park",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63301",
          "name": "Minbyul Jeong",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63302",
          "name": "Jaewoo Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T04:52:05.000Z",
      "title": "¿Dónde está el tiempo? Tiempo en la mente: lugar donde el modelo de lenguaje almacena información temporal",
      "summary": "「El poder de inferencia de modelos de lenguaje sobre hechos se ha ampliamente investigado, pero su manejo de hechos que cambian con el tiempo ha sido poco estudiado. Hemos descubierto 'cabezas de atención temporales' ('Temporal Heads') para el procesamiento de conocimientos temporales. Estas cabezas existen en todos los modelos, aunque su ubicación es diferente, pero responden de manera distinta a la clase del conocimiento o a la época relativa. Desactivar estas cabezas disminuye la capacidad específica del conocimiento temporal, pero no afecta la invarianza temporal ni la capacidad de resolución de problemas. Además, estas cabezas se activan también con números como '2004' o con pronombres textuales como '… de año', lo que demuestra que representan más que simplemente números. Además, se puede editar el conocimiento temporal ajustando los valores de estas cabezas temporales.」",
      "upvotes": 17,
      "discussionId": "67b7fa9ac3f48f8b3fc63452"
    },
    "publishedAt": "2025-02-20T23:02:42.672Z",
    "title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64587be872b60ae7a3817858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
      "fullname": "Minbyul Jeong",
      "name": "Minbyul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14834",
      "authors": [
        {
          "_id": "67b7f3c4d00e69f10cff219e",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff219f",
          "name": "Yucheng Wang",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a0",
          "name": "Daniel Zhang-Li",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a1",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a2",
          "name": "Jifan Yu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a3",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a4",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a5",
          "name": "Huiqin Liu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a6",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a7",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a8",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:47:36.000Z",
      "title": "LongWriter-V: Modelo de lenguaje visual que permite la generación a largo plazo y de alta calidad.",
      "summary": "Los LVLMs de hoy pueden procesar una longitud de contexto de 128k de tokens de imagen y texto, pero presentan dificultades al generar salidas de más de 1,000 palabras en la coronita. Uno de los principales limitantes es la falta de ejemplos de salida larga durante el entrenamiento supervisado (SFT). Para abordar este problema, presentamos LongWriter-V-22k, que incluye 22,158 conjuntos de entrenamiento SFT, cada uno con varios imágenes de entrada, instrucciones y salidas correspondientes que pueden extenderse desde 0 hasta 10,000 palabras. Para asegurar una correspondencia precisa entre la imagen de entrada y la salida larga, aplicamos la Direct Preference Optimization (DPO) en el modelo SFT. Dado que recopilar retroalimentación humana para salidas largas (por ejemplo, 3,000 palabras) es costoso, proponemos IterDPO, que divide las salidas largas y utiliza la modificación de registros para formar pares de salidas originales y preferencias. Además, desarrollamos MMLongBench-Write, un benchmark para evaluar la capacidad de generación larga de VLMs, que ofrece 6 tareas. Nuestro modelo de 7B parámetros se entrenó con LongWriter-V-22k y IterDPO, mostrando un desempeño notable en este benchmark y superando otros grandes modelos públicos como GPT-4o. Los códigos y datos están disponibles en https://github.com/THU-KEG/LongWriter-V.",
      "upvotes": 15,
      "discussionId": "67b7f3c7d00e69f10cff2258"
    },
    "publishedAt": "2025-02-20T22:39:21.551Z",
    "title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/8AYx7CcK4CT6flX3nRDlB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14834.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14768",
      "authors": [
        {
          "_id": "67b7f08c357c2729ac20a81b",
          "name": "Tian Xie",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81c",
          "name": "Zitian Gao",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81d",
          "name": "Qingnan Ren",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81e",
          "name": "Haoming Luo",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81f",
          "name": "Yuqian Hong",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a820",
          "name": "Bryan Dai",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a821",
          "name": "Joey Zhou",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a822",
          "name": "Kai Qiu",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a823",
          "name": "Zhirong Wu",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a824",
          "name": "Chong Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T17:49:26.000Z",
      "title": "Logic-RL: Liberación de la Lógica en el Aprendizaje por Reforzamiento de los Modelos de Lenguaje de Alto Nivel",
      "summary": "Basándose en el éxito de DeepSeek-R1, se explora la posibilidad de aplicar aprendizaje por refuerzo basado en reglas (RL) en modelos de lógica de gran escala. Para analizar el proceso dinámico de modelos de lógica, se utilizan como datos de entrenamiento los \"Synthetic Logic Puzzles\" que permiten verificar la complejidad controlable y la brevedad de la respuesta. Se proporcionan importantes contribuciones técnicas para un aprendizaje efectivo y estable de RL: un sistema de prompt que enfoca el pensamiento y la respuesta, una función de recompensa estricta que valora soluciones cortas, y un simple receta de entrenamiento que ayuda a alcanzar una convergencia estable. El modelo de 7B desarrolla habilidades avanzadas en tecnologías de lógica, además del corpus de lógica: reflexión, verificación, resumen, entre otros. En particular, después de entrenarse en 5K problemas de lógica, muestra una capacidad de generalización en marcos de prueba matemáticos difíciles como AIME y AMC.",
      "upvotes": 14,
      "discussionId": "67b7f08e357c2729ac20a88f"
    },
    "publishedAt": "2025-02-20T22:19:05.902Z",
    "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14768.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14282",
      "authors": [
        {
          "_id": "67b7f5587f4d732dc469270e",
          "name": "Haowei Liu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc469270f",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692710",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692711",
          "name": "Yuyang Wanyan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692712",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692713",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692714",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692715",
          "name": "Chunfeng Yuan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692716",
          "name": "Changsheng Xu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692717",
          "name": "Weiming Hu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692718",
          "name": "Fei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T05:41:55.000Z",
      "title": "PC-Agent: Marco de colaboración multi-agentes jerárquico para la automatización de tareas complejas en computadoras",
      "summary": "En el campo de los agentes de interfaz gráfica basados en modelos multi-modal multi-frame (MLLM), el escenario de PC presenta características de un entorno altamente intercambiable, incluyendo flujos de trabajo complejos entre aplicaciones, lo que contrasta con el dispositivo móvil. Para abordar estos desafíos, proponemos un marco de trabajo de agente híperagente llamado PC-Agent. En particular, desde una perspectiva visual, diseñamos un módulo de reconocimiento visual activo (APM) para superar la capacidad actual de los MLLM de ignorar el contenido de las capturas de pantalla. Desde una perspectiva de toma de decisiones, proponemos una arquitectura de colaboración de múltiples agentes híbridos que divide el proceso de toma de decisiones en niveles de orden, sub-tareas y acciones, para manejar eficazmente instrucciones complejas y sub-tareas interdependientes. En esta arquitectura, se instalan tres agentes: Manager para la decomposición de órdenes, Progress para la seguimiento del progreso y Decision para la toma de decisiones en cada etapa. Además, introducimos un agente de reflexión para evitar errores y ajustar el proceso. También presentamos un nuevo benchmark llamado PC-Eval, que incluye 25 comandos complejos de la vida real. Los resultados de los experimentos en PC-Eval muestran que nuestro PC-Agente aumentó la tasa de éxito de tareas en aproximadamente 32% en comparación con los métodos anteriores. El código está disponible para uso público.",
      "upvotes": 11,
      "discussionId": "67b7f55b7f4d732dc46927c1"
    },
    "publishedAt": "2025-02-20T22:39:48.180Z",
    "title": "PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/feg9OYb4onJJermpjc6nh.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14282.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14499",
      "authors": [
        {
          "_id": "67b7ee1dfedfe971271dcca0",
          "user": {
            "_id": "6114c9fae7a2566ae7d1a1a7",
            "avatarUrl": "/avatars/c71ab1850322fcf5ef239cb8d31cb137.svg",
            "isPro": false,
            "fullname": "Deepak Nathani",
            "user": "dnathani",
            "type": "user"
          },
          "name": "Deepak Nathani",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-21T07:20:46.836Z",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca1",
          "name": "Lovish Madaan",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca2",
          "name": "Nicholas Roberts",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca3",
          "name": "Nikolay Bashlykov",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca4",
          "name": "Ajay Menon",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca5",
          "name": "Vincent Moens",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca6",
          "name": "Amar Budhiraja",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca7",
          "name": "Despoina Magka",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca8",
          "name": "Vladislav Vorotilov",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca9",
          "name": "Gaurav Chaurasia",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccaa",
          "name": "Dieuwke Hupkes",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccab",
          "name": "Ricardo Silveira Cabral",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccac",
          "name": "Tatiana Shavrina",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccad",
          "name": "Jakob Foerster",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccae",
          "name": "Yoram Bachrach",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccaf",
          "name": "William Yang Wang",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccb0",
          "user": {
            "_id": "633e94793a17ab61de8e2b9c",
            "avatarUrl": "/avatars/5f2f58ddeed211393660ada6b135f0d5.svg",
            "isPro": false,
            "fullname": "Roberta Raileanu",
            "user": "rraileanu",
            "type": "user"
          },
          "name": "Roberta Raileanu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-21T03:08:15.471Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T12:28:23.000Z",
      "title": "MLGym: Nuevo marco de trabajo y referencia para el desarrollo de agentes AI",
      "summary": "Meta MLGym y MLGym-Bench son nuevos marcos de trabajo y marcos de evaluación. Este marco de trabajo y marco de evaluación son nuevos en la investigación de tareas de inteligencia artificial para la evaluación y desarrollo de agentes de lenguaje de máquina (LLMs). Primero, MLGym es el primer entorno de Gym para tareas de aprendizaje automático (ML). De esta manera, se puede investigar algoritmos de aprendizaje reforzado (RL) para el aprendizaje de estos agentes. MLGym-Bench está compuesto de 13 tareas de investigación de inteligencia artificial abierta en diferentes áreas como visión computacional, procesamiento del lenguaje natural, aprendizaje reforzado, teoría de juegos, entre otras. Para resolver estas tareas, se necesitan tecnologías de investigación AI realistas que incluyen la generación de nuevas ideas y hipótesis, la generación y procesamiento de datos, la implementación de métodos ML, entrenamiento de modelos, ejecución de experimentos, análisis de resultados y la repetición de este proceso para mejorar tareas específicas. Evaluamos múltiples modelos avanzados como Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview y Gemini-1.5 Pro en nuestro marco de evaluación. Nuestro marco de trabajo MLGym facilita la adición de nuevas tareas, la integración y evaluación de modelos o agentes, la generación de datos sintéticos a gran escala y el desarrollo de nuevos algoritmos para el aprendizaje de agentes en tareas de investigación de inteligencia artificial. Nuestro objetivo es abrir este marco de trabajo y marco de evaluación como código abierto para fomentar la investigación futura que promueva el desarrollo de capacidades de investigación de agentes de lenguaje de máquina.",
      "upvotes": 9,
      "discussionId": "67b7ee1ffedfe971271dcd3a"
    },
    "publishedAt": "2025-02-20T22:08:38.225Z",
    "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14499.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14844",
      "authors": [
        {
          "_id": "67b7f5ee8b3dff28b749be78",
          "name": "Rameen Abdal",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be79",
          "name": "Or Patashnik",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7a",
          "name": "Ivan Skorokhodov",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7b",
          "name": "Willi Menapace",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7c",
          "name": "Aliaksandr Siarohin",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7d",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7e",
          "name": "Daniel Cohen-Or",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7f",
          "name": "Kfir Aberman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:53:39.000Z",
      "title": "Personalización de Conceptos Dinámicos a partir de Solas Vídeos",
      "summary": "El progreso en modelos de imágenes a partir de textos personalizados es sorprendentemente impresionante, pero extenderlo a modelos de video desde texto presenta problemas especiales. Contrario a conceptos estáticos, la personalización de modelos de video puede comprender conceptos dinámicos, incluyendo movimientos. En este artículo, utilizamos un nuevo marco de trabajo llamado \"Set-and-Sequence\" para la personalización de modelos de video basados en Transformers de Difusión (DiTs) que utilizan conceptos dinámicos. Nuestro enfoque no separa explicitamente características espaciales y temporales, sino que aborda un espacio de pesos espacio-temporal. Este enfoque se desarrolla en dos etapas principales: primero, se ajustan capas de Adaptación de Rango Bajo (LoRA) usando los frames del video sin orden, y se entrena la influencia visual basada en LoRA de identidad para representar el exterior. En segundo lugar, mientras se mantiene la LoRA de identidad fija, se agregan residuos de movimiento para ajustar el video completo y comprender la dinámica del movimiento. El marco \"Set-and-Sequence\" inserta efectivamente conceptos dinámicos en el área de salida del modelo de video, abre posibilidades de edición sin precedentes y organización, y establece nuevos estándares para la personalización de conceptos dinámicos.",
      "upvotes": 8,
      "discussionId": "67b7f5f18b3dff28b749bf45"
    },
    "publishedAt": "2025-02-20T22:41:47.210Z",
    "title": "Dynamic Concepts Personalization from Single Videos",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14844.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14372",
      "authors": [
        {
          "_id": "67b81870cc6b0136b3d84254",
          "user": {
            "_id": "6530a78069751712276d60ed",
            "avatarUrl": "/avatars/2ef4f16d0be557ed60c11d8dcef85f6f.svg",
            "isPro": false,
            "fullname": "Austin He",
            "user": "basil2115",
            "type": "user"
          },
          "name": "Austin Yubo He",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-21T06:30:16.645Z",
          "hidden": false
        },
        {
          "_id": "67b81870cc6b0136b3d84255",
          "name": "Zi-Wen Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:05:34.000Z",
      "title": "Investigación de Códigos de Corrección de Errores Cuanticos para la Implementación de Alta Eficiencia y Bajo Peso con Aprendizaje por Refuerzo",
      "summary": "La escalabilidad de la computación cuántica perdida es asumida depender de los códigos de corrección de errores cuánticos. Al buscar una eficiencia en la tolerancia a errores relacionados con la corrección de errores cuánticos, los parámetros importantes del código son los pesos de la medición para identificar errores: un alto peso de medición aumenta el costo de implementación y puede provocar errores, por lo que es crucial optimizar el peso de medición en el diseño del código. Esto ha aumentado el interés en códigos cuánticos de baja densidad de paridad (qLDPC). Estos estudios han enfocado principalmente en las propiedades de los códigos de alta densidad de paridad (LDPC). En este trabajo, se presenta una flexible y eficiente abordaje computacional basado en aprendizaje reforzado (RL) para la reducción de pesos en códigos de RL. Esto permite generar nuevos códigos de bajo peso en parámetros prácticos, demostrando un gran avance en comparación con los códigos actuales. Por ejemplo, se puede reducir el sobrecarga de la cubita de la física en códigos de peso 6 en una orden de magnitud respecto a los resultados actuales, lo que abre un espacio práctico para experimentos futuros. Además, se investigan las interacciones entre los parámetros del código utilizando un marco de RL, proporcionando nuevas perspectivas sobre la posibilidad y eficiencia de estrategias de codificación. En general, los resultados de este estudio demuestran que el RL puede abordar desafíos complejos en la búsqueda de códigos cuánticos y acelerar la implementación práctica de la tecnología de computación cuántica perdida.",
      "upvotes": 7,
      "discussionId": "67b81873cc6b0136b3d8430a"
    },
    "publishedAt": "2025-02-21T01:11:34.971Z",
    "title": "Discovering highly efficient low-weight quantum error-correcting codes with reinforcement learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14372.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6530a78069751712276d60ed",
      "avatarUrl": "/avatars/2ef4f16d0be557ed60c11d8dcef85f6f.svg",
      "fullname": "Austin He",
      "name": "basil2115",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14846",
      "authors": [
        {
          "_id": "67b7f4f1b15c19d57189fc5e",
          "name": "Yue Yang",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc5f",
          "name": "Ajay Patel",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc60",
          "name": "Matt Deitke",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc61",
          "name": "Tanmay Gupta",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc62",
          "name": "Luca Weihs",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc63",
          "name": "Andrew Head",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc64",
          "name": "Mark Yatskar",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc65",
          "name": "Chris Callison-Burch",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc66",
          "name": "Ranjay Krishna",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc67",
          "name": "Aniruddha Kembhavi",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc68",
          "name": "Christopher Clark",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:55:30.000Z",
      "title": "Código de guia para la generación de datos monomodal de síntesis y escalado del entendimiento de imágenes ricas en texto",
      "summary": "La inferencia sobre imágenes y texto rico, como gráficos y documentos, es un campo de aplicación importante para modelos de lenguaje visuolingüístico (VLMs). Sin embargo, en estos dominios, los VLMs enfrentan desafíos debido a la escasez de datos visuolingüísticos basados en texto. Para abordar estas desafíos, presentamos CoSyn, un marco de trabajo que utiliza el poder de generación de código de modelos de lenguaje grandes basados en texto (LLMs) para crear automáticamente datos multimodal de texto sintético. Al recibir un texto de entrada que describe el dominio objetivo, CoSyn impulsa a un LLM para generar código (Python, HTML, LaTeX, etc.) para renderizar imágenes sintéticas. A través de la representación de texto básica de las imágenes sintéticas, CoSyn puede generar datos de entrenamiento de instrucciones de alta calidad usando solo texto con un LLM. A través de CoSyn, hemos construido un conjunto de datos de 400K imágenes y 2.7M filas de datos de entrenamiento de lenguaje visuolingüístico. Los experimentos integrales en 7 benchmarks demuestran que nuestro modelo basado en datos sintéticos alcanza el mejor rendimiento entre los modelos abiertos competitivos y supera a modelos no públicos como GPT-4V y Gemini 1.5 Flash. Además, CoSyn puede generar datos de punteros sintéticos, lo que demuestra potenciales para el desarrollo de agentes multimodal que pueden establecer puntos de referencia en las imágenes de entrada y funcionar en entornos reales.",
      "upvotes": 5,
      "discussionId": "67b7f4f2b15c19d57189fc95"
    },
    "publishedAt": "2025-02-20T22:38:36.406Z",
    "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12853",
      "authors": [
        {
          "_id": "67b69b6717ccb022c6a95b38",
          "name": "Ruotian Ma",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b39",
          "name": "Peisong Wang",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3a",
          "name": "Cheng Liu",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3b",
          "name": "Xingyan Liu",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3c",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3d",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3e",
          "name": "Xin Zhou",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3f",
          "name": "Nan Du",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b40",
          "name": "Jia Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T13:40:22.000Z",
      "title": "S^2R: Aprendizaje por Reforzamiento para la Autenticación Automática y la Edición Automática con Modelos de Lenguaje de Gran Tamaño (LLM)",
      "summary": "Recientes investigaciones han demostrado el efecto de la escalabilidad en la prueba de los LLMs. Sin embargo, los métodos generalmente utilizados para estimular la capacidad de pensamiento profundo en los LLMs requieren de grandes volúmenes de datos y esfuerzos de entrenamiento. Por otro lado, los métodos para mejorar la capacidad de pensamiento del modelo base no son claros. En este estudio, se propone un marco eficiente llamado S^2R para fortalecer la inferencia de los LLMs y entrenar un modelo que realice auto-revisión y auto-ajuste durante la inferencia. En particular, inicialmente se utiliza una entrenamiento de ajuste normativo basado en datos ajustados para inicializar acciones de revisión y ajuste iterativos. Luego, se aplican aprendizajes de fortalecimiento a nivel de resultado y proceso para fortalecer más las habilidades de revisión y ajuste, minimizando la demanda de recursos y mejorando adaptativamente el proceso de inferencia. Nuestros resultados muestran una inicialización de 3.1k acciones de revisión y ajuste, con un aumento de la precisión de Qwen2.5-math-7B del 51.0% al 81.6%, superando a modelos entrenados con la misma cantidad de datos largos. Los experimentos y análisis ampliados basados en tres modelos base y benchmarkes dentro y fuera del dominio demuestran la efectividad de S^2R. Nuestro código y datos están disponibles en https://github.com/NineAbyss/S2R.",
      "upvotes": 4,
      "discussionId": "67b69b6817ccb022c6a95b6e"
    },
    "publishedAt": "2025-02-21T05:00:18.645Z",
    "title": "S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648294b2eb4befee378951c1",
      "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
      "fullname": "Ruotian Ma",
      "name": "vvibt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14669",
      "authors": [
        {
          "_id": "67b7eeddaf9f1b1bd95b878b",
          "name": "Alan Dao",
          "hidden": false
        },
        {
          "_id": "67b7eeddaf9f1b1bd95b878c",
          "name": "Dinh Bach Vu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T16:05:18.000Z",
      "title": "AlphaMaze: GRPO como un proyecto global para mejorar la capacidad de reconocimiento espacial en modelos de lenguaje grandes basándose en GRPO",
      "summary": "Los modelos de lenguaje grande (LLMs) muestran habilidades excepcionales en el procesamiento del lenguaje pero tienen problemas al realizar tareas que requieren razonamiento visual y espacial. En este artículo, se utiliza un nuevo marco de entrenamiento de dos etapas para agregar a los modelos generales la capacidad de razonamiento visual en el explorador de laberintos. Primero, se utiliza un entrenamiento supervisado de fine tuning (SFT) sobre un conjunto de datos de representaciones de laberintos leídas estadísticamente para enseñar al modelo a predecir las instrucciones de movimiento por pasos. Luego, se introduce la Política de Optimización de Grupo Relativa (GRPO) utilizada en DeepSeekR1, que utiliza funciones de recompensa desordenadas para mejorar decisiones continuas del modelo y promover acciones considerando restricciones de tiempo. Los resultados de los experimentos en laberintos generados de forma sintética muestran que el modelo básico no puede explorar el laberinto, mientras que el modelo entrenado con SFT alcanza una precisión del 86%, y la entrenamiento adicional con GRPO aumenta la precisión a un 93%. Un análisis cualitativo muestra que GRPO impulsa una mejor auto-regulación más fuerte y revela la posibilidad de cerrar la brecha entre modelos de lenguaje y tareas espaciales visuales con nuestro enfoque. Estos hallazgos tienen un significado adecuado para aplicaciones que requieren visión y razonamiento continuo en campos como la ingeniería robótica, navegación automática y otros.",
      "upvotes": 4,
      "discussionId": "67b7eeddaf9f1b1bd95b87c8"
    },
    "publishedAt": "2025-02-20T22:11:45.130Z",
    "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14377",
      "authors": [
        {
          "_id": "67b7f350357c2729ac216494",
          "name": "Ke Cao",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216495",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216496",
          "name": "Ao Ma",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216497",
          "name": "Jiasong Feng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216498",
          "name": "Zhanjie Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216499",
          "name": "Xuanhua He",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649a",
          "name": "Shanyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649b",
          "name": "Bo Cheng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649c",
          "name": "Dawei Leng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649d",
          "name": "Yuhui Yin",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649e",
          "name": "Jie Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:10:05.000Z",
      "title": "RelaCtrl: Control Eficiente Guíado por la Relevancia con Transformadores de Difusión",
      "summary": "El Diffusion Transformer desempeña un papel importante en la generación de imágenes y animaciones a partir de texto, y ha evolucionado principalmente debido a su capacidad de escalabilidad funcional. Sin embargo, los métodos actuales de controlado Diffusion Transformer presentan problemas de parámetros y sobrecarga de cálculos grandes, así como una pobre relación con la información de control a nivel de dispositivo, lo que hace que los recursos no se asignen de manera adecuada y la eficiencia se vea afectada. En respuesta a esto, proponemos el marco de trabajo de generación controlable eficiente guiada por la relevancia, llamado RelaCtrl, para facilitar la formulación de señales de control eficientes en el Diffusion Transformer.\n\nPrimero, evaluamos la relevancia de la información de control en cada capa del Diffusion Transformer y utilizamos el \"ControlNet Relevance Score\" para evaluar la calidad de la generación y el efecto de control en el caso en que se eliminen las capas de control. Basándonos en la intensidad de esta relevancia, ajustamos la posición de las capas de control, el escalado de parámetros y la capacidad de modelado para reducir los parámetros necesarios y los cálculos redundantes. Además, para lograr una mayor eficiencia, reemplazamos el auto-atención y la FFN utilizados en bloques de copia generalmente por un Shuffle Mixer bidimensional (TDSM), lo que permite una implementación eficiente de los mismos.\n\nLos resultados experimentales cualitativos y cuantitativos muestran que comparados con PixArt-delta, utilizando solo el 15% de los parámetros y la complejidad de cálculo, pueden demostrar un rendimiento mejor. Además, se pueden encontrar ejemplos adicionales en https://relactrl.github.io/RelaCtrl/.",
      "upvotes": 3,
      "discussionId": "67b7f354357c2729ac216582"
    },
    "publishedAt": "2025-02-20T22:30:51.542Z",
    "title": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14377.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13759",
      "authors": [
        {
          "_id": "67b83a1f26e7d5f7cb0b7c9d",
          "user": {
            "_id": "65407ba7a38390065750233f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
            "isPro": false,
            "fullname": "Zirui Song",
            "user": "Ziruibest",
            "type": "user"
          },
          "name": "Zirui Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:04.247Z",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7c9e",
          "name": "Jingpu Yang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7c9f",
          "name": "Yuan Huang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca0",
          "name": "Jonathan Tonglet",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca1",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca2",
          "name": "Tao Cheng",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca3",
          "name": "Meng Fang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca4",
          "name": "Iryna Gurevych",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca5",
          "name": "Xiuying Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T14:21:25.000Z",
      "title": "Uso de datos de juegos de jugadores reales con Geolocation: grandes conjuntos de datos y un marco lógico similar al humano",
      "summary": "Geolocalización, la tarea de determinar la ubicación de una imagen, requiere razones complejas y es importante para la cartografía, vigilancia y conservación cultural. Sin embargo, los métodos actuales generalmente generan información de ubicación con una precisión baja y faltan en la interpretabilidad. Una de las problemas es la calidad y la escala de los conjuntos de datos de geolocalización actuales. Estos conjuntos de datos se construyen generalmente a escala pequeña automáticamente, lo que añade datos con ruido y desbalancea el nivel de dificultad de las tareas, incluyendo imágenes exageradas o que no tienen suficiente contenido para hacer inferencias confiables. Para resolver estos problemas, presentamos un estricto marco de trabajo de geolocalización con tres componentes clave: GeoComp, un gran conjunto de datos, GeoCoT, un nuevo método de razonamiento, y GeoEval, un índice de evaluación. El marco de trabajo se centra en GeoComp (Conjunto de Datos de Competencia de Geolocalización), que es un gran conjunto de datos de 740K usuarios capturados en dos años, incluyendo 25 millones de metadatos y 3 millones de etiquetas de ubicación asociadas a imágenes, cada una de las cuales ha sido reportada varias mil a millones de veces por usuarios humanos. Este conjunto de datos ofrece diferentes niveles de dificultad para diversas analíticas y resalta las limitaciones importantes de los modelos actuales. Basándonos en este conjunto de datos, proponemos un nuevo marco de razonamiento multi-nivel para fortalecer las habilidades de razonamiento de los modelos de lenguaje de visión y lenguaje (LVMs) llamado Geographical Chain-of-Thought (GeoCoT). GeoCoT imita el proceso de razonamiento humano de geolocalización, integrando contexto y espacialidad para mejorar el rendimiento. Finalmente, utilizando los índices de GeoEval, demostramos que GeoCoT mejora significativamente la precisión de la geolocalización en un 25% aproximado y la interpretabilidad.",
      "upvotes": 1,
      "discussionId": "67b83a2226e7d5f7cb0b7d66"
    },
    "publishedAt": "2025-02-21T03:33:28.852Z",
    "title": "Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13759.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65407ba7a38390065750233f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
      "fullname": "Zirui Song",
      "name": "Ziruibest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14409",
      "authors": [
        {
          "_id": "67b83a20a9fa331061e84ecd",
          "user": {
            "_id": "60a643b9213fe60589b8fdf9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a643b9213fe60589b8fdf9/OOXmW3MkSf88r63tAE6-n.jpeg",
            "isPro": false,
            "fullname": "Dustin Wright",
            "user": "dwright37",
            "type": "user"
          },
          "name": "Dustin Wright",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:02.288Z",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ece",
          "name": "Zain Muhammad Mujahid",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ecf",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ed0",
          "name": "Isabelle Augenstein",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ed1",
          "name": "David Jurgens",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:57:42.000Z",
      "title": "Resumen sin estructura de evidencia y enfocado en consultas de largo contexto",
      "summary": "Los modelos de lenguaje grande (LLMs) pueden generar resumenes coherentes a partir de largos contextos de solicitudes de usuario. La resumenización de los ejemplos de evidencia y la citación precisa pueden mejorar la transparencia y la confianza. Por otro lado, los LLMs muestran una bias hacia las posiciones en las que se centran la atención sobre información específica, lo que podría afectar la citación de la evidencia. Los estudios anteriores se centraron en la citación de evidencia en un tamaño específico (por ejemplo, oraciones, párrafos, documentos). Sin embargo, proponemos resumenes basados en consultas de largos contextos que incluyen citas de evidencia no estructuradas. Mostramos que los sistemas actuales tienen dificultades en la generación de evidencia no estructurada y en la citación precisa, y que la evidencia puede \"estar en medio de confusión\". Para mitigar esto, hemos creado un nuevo pipeline independiente de dominio y un conjunto de datos sintéticos llamado \"SUnsET\" (Texto de Evidencia No Estructurada con Resumenes). Este conjunto de datos puede apoyar a los LLMs en esta tarea. Investigamos de manera cruzada 5 modelos de LLMs de diferentes tamaños y 4 conjuntos de datos (con diferentes tipos y longitudes de documentos), y mostramos que los LLMs aplicados a los datos de SUnsET generan evidencia factualmente coherente y relevante, resumenes de evidencia en diferentes posiciones del contexto, y resumenes coherentes y relevantes.",
      "upvotes": 0,
      "discussionId": "67b83a21a9fa331061e84f36"
    },
    "publishedAt": "2025-02-21T03:33:40.641Z",
    "title": "Unstructured Evidence Attribution for Long Context Query Focused Summarization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14409.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60a643b9213fe60589b8fdf9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a643b9213fe60589b8fdf9/OOXmW3MkSf88r63tAE6-n.jpeg",
      "fullname": "Dustin Wright",
      "name": "dwright37",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]