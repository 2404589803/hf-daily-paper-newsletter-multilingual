[
  {
    "paper": {
      "id": "2505.22617",
      "authors": [
        {
          "_id": "6837cd8fc537d91527323667",
          "user": {
            "_id": "650eba9555dc1e841746f132",
            "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
            "isPro": false,
            "fullname": "Ganqu Cui",
            "user": "ganqu",
            "type": "user"
          },
          "name": "Ganqu Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:09.467Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323668",
          "user": {
            "_id": "66e3f8fb5d97b5bb46923444",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DW806I00-00oQAYvD4ocQ.png",
            "isPro": false,
            "fullname": "Yuchen Zhang",
            "user": "YucZhang2003",
            "type": "user"
          },
          "name": "Yuchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:14.207Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323669",
          "user": {
            "_id": "65352acb7139c5dd8d9a8590",
            "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
            "isPro": false,
            "fullname": "JiachengChen",
            "user": "JC-Chen",
            "type": "user"
          },
          "name": "Jiacheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:20.736Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366a",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366b",
          "name": "Zhi Wang",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366c",
          "user": {
            "_id": "622474f38dc6b0b64f5e903d",
            "avatarUrl": "/avatars/d6b60a014277a8ec7d564163c5f644aa.svg",
            "isPro": false,
            "fullname": "Yuxin Zuo",
            "user": "yuxinzuo",
            "type": "user"
          },
          "name": "Yuxin Zuo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:12.022Z",
          "hidden": true
        },
        {
          "_id": "6837cd8fc537d9152732366d",
          "user": {
            "_id": "662f638ba9891e43cc4c5125",
            "avatarUrl": "/avatars/77c22de5511f9b85d98ec75fb0b5e9be.svg",
            "isPro": false,
            "fullname": "Li Haozhan",
            "user": "Haozhan72",
            "type": "user"
          },
          "name": "Haozhan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:22.720Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366e",
          "name": "Yuchen Fan",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366f",
          "name": "Huayu Chen",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323670",
          "name": "Weize Chen",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323671",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323672",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323673",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323674",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323675",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323676",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323677",
          "user": {
            "_id": "60cf4bcb1ce3775ebb86e5d5",
            "avatarUrl": "/avatars/12bcd18d215abf91f297f93007733148.svg",
            "isPro": false,
            "fullname": "Ning Ding",
            "user": "stingning",
            "type": "user"
          },
          "name": "Ning Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:16.809Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:38:45.000Z",
      "submittedOnDailyAt": "2025-05-29T01:38:57.501Z",
      "title": "Inferencia de modelos de lenguaje utilizando estructuras de entropía de aprendizaje reforzado",
      "submittedOnDailyBy": {
        "_id": "650eba9555dc1e841746f132",
        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
        "isPro": false,
        "fullname": "Ganqu Cui",
        "user": "ganqu",
        "type": "user"
      },
      "summary": "Este artículo tiene como objetivo superar la rotura de la entropía de la política al realizar una lógica de grandes modelos de lenguaje (LLMs) a través de la escalabilidad de la RL (Reinforcement Learning). Este fenómeno puede ser observado de manera consistente en experimentos de RL de gran escala, donde la entropía de la política disminuye rápidamente en los primeros estágios de entrenamiento, y la capacidad de exploración disminuye al mismo tiempo que el rendimiento de la política se satura. En realidad, se estableció una ecuación experimental entre la entropía H y el rendimiento R, R = -a * e^H + b. Esta ley experimental ofrece significativas pistas y permite predecir límites en el rendimiento de la política cuando la entropía es insuficiente, ya que el rendimiento se sustituye por la entropía. Cuando H=0, R=-a+b. Nuestro descubrimiento muestra que para manejar la entropía, es necesario escalar el cálculo para mantener una exploración continua. Esto llevó a investigar la dinámica de la entropía tanto teóricamente como experimentalmente. Nuestros cálculos demuestran que la varianza de las probabilidades de acción y la varianza de las logísticas se mantienen positivas durante el entrenamiento, lo que explica mejor la disminución de la entropía de la política. Para entender la estructura dinámica de la entropía, consideramos limitar la actualización de los tokens con alta varianza para controlar la entropía. Se proponen dos técnicas sencillas y efectivas: Clip-Cov y KL-Cov. Clip-Cov aplica una copia a los tokens con alta varianza y aplica una penalización de KL. Los experimentos muestran que estas técnicas promueven la exploración y permiten que las políticas salgan de la rotura de la entropía, alcanzando un rendimiento mejor en el rendimiento en el tiempo.",
      "upvotes": 64,
      "discussionId": "6837cd90c537d9152732369d",
      "githubRepo": "https://github.com/PRIME-RL/Entropy-Mechanism-of-RL",
      "ai_summary": "Entropy dynamics in reinforcement learning with large language models are investigated to prevent policy entropy collapse and improve exploration.",
      "ai_keywords": [
        "policy entropy",
        "reinforcement learning",
        "LLMs",
        "entropy intervention",
        "transformation equation",
        "policy performance",
        "entropy dynamics",
        "covariance",
        "action probability",
        "logits",
        "advantage",
        "Policy Gradient",
        "Clip-Cov",
        "KL-Cov"
      ]
    },
    "publishedAt": "2025-05-28T13:38:45.000Z",
    "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models",
    "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22617.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650eba9555dc1e841746f132",
      "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
      "fullname": "Ganqu Cui",
      "name": "ganqu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21600",
      "authors": [
        {
          "_id": "6837bc9c9937bcb69885799c",
          "user": {
            "_id": "6445fd9ba56444c355dcbcba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
            "isPro": false,
            "fullname": "Tianyu Fu",
            "user": "fuvty",
            "type": "user"
          },
          "name": "Tianyu Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:43.123Z",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799d",
          "name": "Yi Ge",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799e",
          "user": {
            "_id": "66954ebfbcd81f395e9dca37",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66954ebfbcd81f395e9dca37/0C3m5YdxyXuK7dJBu4AdL.png",
            "isPro": false,
            "fullname": "Yichen You",
            "user": "youyc22",
            "type": "user"
          },
          "name": "Yichen You",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:40.790Z",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799f",
          "name": "Enshu Liu",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a0",
          "name": "Zhihang Yuan",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a1",
          "name": "Guohao Dai",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a2",
          "name": "Shengen Yan",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a3",
          "name": "Huazhong Yang",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a4",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6445fd9ba56444c355dcbcba/edz42EmKaJTdax2mDUmhh.mp4"
      ],
      "publishedAt": "2025-05-27T16:57:20.000Z",
      "submittedOnDailyAt": "2025-05-29T00:18:54.118Z",
      "title": "R2R: Modelo de token routing para explorar rutas eficientes para otras razones",
      "submittedOnDailyBy": {
        "_id": "6445fd9ba56444c355dcbcba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
        "isPro": false,
        "fullname": "Tianyu Fu",
        "user": "fuvty",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) alcanzan una capacidad de inteligencia artificial sorprendente, pero su introducción presenta importantes desafíos debido a los altos costos de la sobrecarga de inferencia. Sin embargo, los pequeños modelos de lenguaje determinados (SLMs) no pueden seguir el camino de inteligencia artificial de los LLMs, lo que implica un descenso en su rendimiento. Afortunadamente, la diferencia en el camino de inteligencia artificial entre LLMs y SLMs se produce solo en los pequeños porcentajes de tokens. Muchos tokens generados muestran diferencias neutrales, como variaciones mínimas en la expresión o semejanza. Para abordar estas diferencias, hemos introducido un nuevo método de ruteo de tokens llamado **Roads to Rome (R2R)**. Este método asigna a los SLMs los tokens que no son significativos para la ruta, permitiendo la selección de los LLMs cuando sea necesario. Además, el proceso de prueba de datos automatizado reconoce los tokens que indican diferencias y genera etiquetas de ruteo a nivel de token para crear datos para la entrenamiento de un router ligero. El R2R combina los modelos R1-1.5B y R1-32B de la familia DeepSeek para evaluar marcos de prueba difíciles en matemáticas, programación y respuestas a preguntas. Con un tamaño promedio de parámetros activos de 5.6B, el R2R logró un rendimiento promedio de R1-7B en un factor de 1.6, superando el rendimiento de R1-14B. En comparación con R1-32B, ofrece una mejora en la velocidad de cloro del trabajo del 2.8 veces, así como un prototipo de escalado de tiempo de prueba. El código está disponible en https://github.com/thu-nics/R2R.",
      "upvotes": 47,
      "discussionId": "6837bc9d9937bcb6988579d1",
      "projectPage": "https://fuvty.github.io/R2R_Project_Page/",
      "githubRepo": "https://github.com/thu-nics/R2R",
      "ai_summary": "Roads to Rome (R2R) selectively utilizes large language models for critical reasoning tasks to enhance efficiency and performance in lightweight models.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Small Language Models (SLMs)",
        "token routing",
        "neural token routing",
        "token divergence",
        "token generation",
        "automatic data generation pipeline",
        "token-level routing labels",
        "R1-1.5B",
        "R1-32B",
        "math benchmarks",
        "coding benchmarks",
        "QA benchmarks",
        "parameter size",
        "activated parameters",
        "test-time scaling efficiency",
        "pareto frontier"
      ]
    },
    "publishedAt": "2025-05-27T12:57:20.000Z",
    "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing",
    "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6445fd9ba56444c355dcbcba/edz42EmKaJTdax2mDUmhh.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6445fd9ba56444c355dcbcba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
      "fullname": "Tianyu Fu",
      "name": "fuvty",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22312",
      "authors": [
        {
          "_id": "6837c342cd1601f5bd670255",
          "name": "Jujie He",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670256",
          "user": {
            "_id": "65643645b09c0b9ece1b8f0e",
            "avatarUrl": "/avatars/d5197103b6e92f765bfda7ed2cc8d53e.svg",
            "isPro": false,
            "fullname": "Jiacai Liu",
            "user": "skydownacai",
            "type": "user"
          },
          "name": "Jiacai Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:28.696Z",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670257",
          "user": {
            "_id": "658229ef5f6d83438257fce5",
            "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
            "isPro": false,
            "fullname": "Chris (Yuhao) Liu",
            "user": "chrisliu298",
            "type": "user"
          },
          "name": "Chris Yuhao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:32.117Z",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670258",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670259",
          "name": "Chaojie Wang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025a",
          "name": "Peng Cheng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025b",
          "name": "Xiaoyu Zhang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025c",
          "name": "Fuxiang Zhang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025d",
          "name": "Jiacheng Xu",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025e",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025f",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670260",
          "name": "Liang Zeng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670261",
          "name": "Tianwen Wei",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670262",
          "name": "Cheng Cheng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670263",
          "name": "Bo An",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670264",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670265",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T12:56:04.000Z",
      "submittedOnDailyAt": "2025-05-29T00:48:05.741Z",
      "title": "Skywork Open Reasoner 1 Reporte Técnico",
      "submittedOnDailyBy": {
        "_id": "658229ef5f6d83438257fce5",
        "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
        "isPro": false,
        "fullname": "Chris (Yuhao) Liu",
        "user": "chrisliu298",
        "type": "user"
      },
      "summary": "El éxito de DeepSeek-R1 señala que el aprendizaje por refuerzo (RL) desempeña un papel crucial en la mejora de las capacidades de razonamiento de los grandes modelos de lenguaje (LLMs). En este trabajo, proponemos una implementación efectiva y escalable de RL para modelos de Chain-of-Thought (CoT) a largo plazo. Nuestro enfoque basado en la serie de modelos DeepSeek-R1-Distill ha logrado un aumento significativo en la eficiencia, aumentando el porcentaje de acierto promedio en AIME24, AIME25 y LiveCodeBench de 57.8% a 72.8% (un aumento de 15.0%) para modelos de 32B y de 43.6% a 57.5% (un aumento de 13.9%) para modelos de 7B. El modelo Skywork-OR1-32B superó a DeepSeek-R1 y Qwen3-32B en los marcadores AIME24 y AIME25, y obtuvo resultados competitivos en LiveCodeBench. Los modelos Skywork-OR1-7B y Skywork-OR1-Math-7B demostraron capacidades de razonamiento competitivas en comparación con modelos de similar tamaño. Realizamos pruebas exhaustivas sobre los componentes clave del flujo de entrenamiento y demostramos su eficacia. Además, investigamos en detalle el fenómeno de la colapso de la entropía y identificamos las causas que afectan su dinámica, demostrando que la inhibición de un colapso de entropía anticipado es crucial para la mejora de los resultados. Para apoyar la comunidad de investigación, proporcionamos modelos de pesos, código de entrenamiento y conjuntos de datos de entrenamiento completamente abiertos.",
      "upvotes": 39,
      "discussionId": "6837c344cd1601f5bd6702dd",
      "githubRepo": "https://github.com/SkyworkAI/Skywork-OR1",
      "ai_summary": "Skywork-OR1 is a reinforcement learning approach for long Chain-of-Thought models that improves accuracy over DeepSeek-R1 across various benchmarks by addressing entropy collapse.",
      "ai_keywords": [
        "reinforcement learning",
        "LLMs",
        "Chain-of-Thought",
        "Skywork-OR1",
        "DeepSeek-R1-Distill",
        "AIME24",
        "AIME25",
        "LiveCodeBench",
        "entropy collapse",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-05-28T08:56:04.000Z",
    "title": "Skywork Open Reasoner 1 Technical Report",
    "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22312.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "658229ef5f6d83438257fce5",
      "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
      "fullname": "Chris (Yuhao) Liu",
      "name": "chrisliu298",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22651",
      "authors": [
        {
          "_id": "6837ffdd1bfb4a669ad6de09",
          "user": {
            "_id": "662678dfdd43e904ef1dcd03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
            "isPro": false,
            "fullname": "Yi Ding",
            "user": "Tuwhy",
            "type": "user"
          },
          "name": "Yi Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:19.811Z",
          "hidden": false
        },
        {
          "_id": "6837ffdd1bfb4a669ad6de0a",
          "name": "Ruqi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:58:03.000Z",
      "submittedOnDailyAt": "2025-05-29T06:18:44.515Z",
      "title": "Shellrock: Lógica de Auto-Corrección en Modelos de Lenguaje Visuo-Semánticos",
      "submittedOnDailyBy": {
        "_id": "662678dfdd43e904ef1dcd03",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
        "isPro": false,
        "fullname": "Yi Ding",
        "user": "Tuwhy",
        "type": "user"
      },
      "summary": "Las VLMs de visión y razonamiento, como las palabras largas, muestran un desempeño esperado en tareas multimodales complejas. Sin embargo, enfrentan principalmente los siguientes problemas: son extremadamente sensibles a errores lógicos, requieren muchos datos estandarizados o datos de validación precisos, y su generalización a áreas específicas es difícil. Para resolver estos limitaciones, hemos adoptado la auto-regulación automática de los VLMs de razonamiento como estrategia y hemos intentado mejorar su rendimiento. Primero, realizamos un análisis detallado de la capacidad de auto-regulación de los VLMs de razonamiento y identificamos sus principales deficiencias. En la base de este análisis, introducimos un marco de entrenamiento para auto-regulación y mejora automática llamado Sherlock. Sherlock incorpora objetos de auto-regulación a nivel de trazado, métodos de construcción de datos basados en patrones visuales y la introducción de beta dinámico. El modelo se entrenó con 20,000 datos estandarizados aleatoriamente, y luego continuó con la mejora automática, excluyendo a los supervívos externos. El modelo Sherlock construido para Llama3.2-Vision-11B obtuvo resultados sorprendentes en el marco de 8 bancos de marca, con una precisión media de la generación directa que aumentó de 64.1 a 65.4 después de la auto-regulación. Supera a LLaVA-CoT (63.2), Mulberry (63.9) y LlamaV-o1 (63.4), y utiliza más del 20% de los datos utilizados para estos modelos.",
      "upvotes": 38,
      "discussionId": "6837ffdf1bfb4a669ad6de71",
      "projectPage": "https://dripnowhy.github.io/Sherlock/",
      "githubRepo": "https://github.com/DripNowhy/Sherlock",
      "ai_summary": "Sherlock, a self-correction and self-improvement framework for reasoning vision-language models, enhances accuracy across benchmarks using limited annotated data.",
      "ai_keywords": [
        "vision-language models",
        "self-correction",
        "trajectory-level self-correction",
        "preference data",
        "visual perturbation",
        "dynamic beta",
        "Llama3.2-Vision-11B",
        "LLaVA-CoT",
        "Mulberry",
        "LlamaV-o1"
      ]
    },
    "publishedAt": "2025-05-28T13:58:03.000Z",
    "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models",
    "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\nbeta for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22651.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662678dfdd43e904ef1dcd03",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
      "fullname": "Yi Ding",
      "name": "Tuwhy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22453",
      "authors": [
        {
          "_id": "6837c318a4e378954486e45d",
          "user": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "WaltonFuture",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:35.145Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e45e",
          "name": "Yuting Li",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e45f",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e460",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Yue Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:14:48.838Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e461",
          "name": "Linghe Kong",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e462",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Weiran Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:14:48.838Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e463",
          "name": "Lichao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T15:11:16.000Z",
      "submittedOnDailyAt": "2025-05-29T00:45:28.570Z",
      "title": "Inferencia de un LLM multimodal aplicando el aprendizaje sin puntos de chequeo utilizando GRPO",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "En la etapa de Hodirain, el mejoramiento de los modelos de lenguaje multimodal (MLLMs) se realiza generalmente mediante ajustes de micro-redes regulares (SFT) o aprendizaje por refuerzo (RL). Sin embargo, estos métodos regulares requieren de datos multimodal etiquetados de manera manual y costosos, lo que no es sostenible a largo plazo. Por otro lado, los esfuerzos recientes se centran en revisar la etapa de Hodirain y su metodología es compleja y repetitiva. En este artículo, se investiga por primera vez un método que permita una mejora continua sin necesidad de normalización externa utilizando el algoritmo de aprendizaje por refuerzo online GRPO (Algoritmo de Aprendizaje por Refuerzo Online estable y escalable). Se propone el framework MM-UPT (Framework de Multimodal Unpatterned Training sencillo y efectivo), que sustituye los señales de recompensa existentes con una estrategia de auto-recompensa basada en votación, basado en GRPO. Los resultados de los experimentos muestran que MM-UPT mejora significativamente la capacidad de comprensión de Qwen2.5-VL-7B (por ejemplo, en MathVista de 66.3% a 72.9% y en We-Math de 62.9% a 68.7%), y que puede aplicarse en situaciones donde no se dispone de etiquetas reales, utilizando solo conjuntos de datos estándar. MM-UPT supera los estándares recientes de Hodirain y se aproxima a los resultados de un GRPO regular. Además, demostra que puede mejorar su rendimiento utilizando preguntas sintéticas generadas por el propio modelo MLLM, presentando una prometedora aproximación para una mejora continua y autónoma escalable. En general, MM-UPT proporciona un nuevo paradigma para la mejora continua y autónoma de MLLMs sin la necesidad de normalización externa. El código está disponible en https://github.com/waltonfuture/MM-UPT.",
      "upvotes": 29,
      "discussionId": "6837c318a4e378954486e48a",
      "projectPage": "https://github.com/waltonfuture/MM-UPT",
      "githubRepo": "https://github.com/waltonfuture/MM-UPT",
      "ai_summary": "MM-UPT, a framework employing GRPO and self-rewarding, enhances multi-modal LLMs through unsupervised continual learning, showing performance improvements without manual annotations.",
      "ai_keywords": [
        "GRPO",
        "MM-UPT",
        "reinforcement learning",
        "unsupervised post-training",
        "multi-modal large language models",
        "self-rewarding mechanism",
        "majority voting",
        "synthetic questions",
        "MathVista",
        "We-Math"
      ]
    },
    "publishedAt": "2025-05-28T11:11:16.000Z",
    "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO",
    "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %rightarrow72.9 % on MathVista, 62.9\n%rightarrow68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22453.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21136",
      "authors": [
        {
          "_id": "6837c91ec790885f338b8f27",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:22.525Z",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f28",
          "name": "Xiaoming Xu",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f29",
          "name": "Jia Wei",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2a",
          "name": "Haofeng Huang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2b",
          "name": "Pengle Zhang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2c",
          "name": "Chendong Xiang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2d",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2e",
          "name": "Jianfei Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/e7t1XOditivMjvbyyouoc.png"
      ],
      "publishedAt": "2025-05-27T12:50:36.000Z",
      "submittedOnDailyAt": "2025-05-29T01:12:49.349Z",
      "title": "SageAttention2++: Implementación más Eficiente de SageAttention2",
      "submittedOnDailyBy": {
        "_id": "66c0a08bac74db25de8427ec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
        "isPro": false,
        "fullname": "Jintao Zhang",
        "user": "jt-zhang",
        "type": "user"
      },
      "summary": "La eficiencia de la ATSUSI es crucial y se debe a que la cantidad de cálculos de tiempo aumenta cuadraticamente con la longitud de la secuencia. SageAttention2 aborda este problema al mejorar la velocidad de la multiplicación de matrices (Matmul) utilizando contadores. Además, propone el uso de comandos rápidos de multiplicación de matrices en FP8 para hacer que SageAttention2 se ejecute aún más rápido. Estos comandos son dos veces más rápidos que la multiplicación de matrices en FP8 utilizada en SageAttention2, y los resultados experimentales muestran que SageAttention2++ es 3.9 veces más rápido que FlashAttention, manteniendo la misma precisión de atención. Esto significa una velocidad efectiva para modelos de generación de lenguaje, imágenes y videos, sin perder de vista la pérdida de rendimiento medida en el terminal. El código está disponible en https://github.com/thu-ml/SageAttention.",
      "upvotes": 28,
      "discussionId": "6837c923c790885f338b90e5",
      "projectPage": "https://github.com/thu-ml/SageAttention",
      "githubRepo": "https://github.com/thu-ml/SageAttention",
      "ai_summary": "SageAttention2++ improves attention efficiency by using FP8 Matmul in FP16, achieving a 3.9x speedup over FlashAttention without losing accuracy.",
      "ai_keywords": [
        "attention",
        "time complexity",
        "sequence length",
        "quantization",
        "matrix multiplications",
        "Matmul",
        "FP8",
        "FP16",
        "SageAttention2",
        "SageAttention2++",
        "FlashAttention",
        "image generation",
        "video generation"
      ]
    },
    "publishedAt": "2025-05-27T08:50:36.000Z",
    "title": "SageAttention2++: A More Efficient Implementation of SageAttention2",
    "summary": "The efficiency of attention is critical because its time complexity grows\nquadratically with sequence length. SageAttention2 addresses this by utilizing\nquantization to accelerate matrix multiplications (Matmul) in attention. To\nfurther accelerate SageAttention2, we propose to utilize the faster instruction\nof FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8\nMatmul used in SageAttention2. Our experiments show that SageAttention2++\nachieves a 3.9x speedup over FlashAttention while maintaining the same\nattention accuracy as SageAttention2. This means SageAttention2++ effectively\naccelerates various models, including those for language, image, and video\ngeneration, with negligible end-to-end metrics loss. The code will be available\nat https://github.com/thu-ml/SageAttention.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/e7t1XOditivMjvbyyouoc.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21136.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22334",
      "authors": [
        {
          "_id": "6837c360b127cae8a0b36e85",
          "user": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "WaltonFuture",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:26.705Z",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e86",
          "name": "Yuting Li",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e87",
          "name": "Kaipeng Zheng",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e88",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e89",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Yue Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:16:03.908Z",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8a",
          "name": "Linghe Kong",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8b",
          "name": "Lichao Sun",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8c",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Weiran Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:16:03.908Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T13:21:38.000Z",
      "submittedOnDailyAt": "2025-05-29T00:46:20.111Z",
      "title": "El desarrollo de la lógica del multimodal utilizando aprendizaje por refuerzo en el Reino Unido del Norte",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "El reciente desarrollo de grandes modelos de lenguaje (LLMs) ha mostrado una impresionante capacidad de pensamiento continuo, lo que permite claramente identificar el papel crucial de la aprendizaje por refuerzo (RL) en esta evolución. Los modelos que representan patrones de \"momentos memorados\" se ajustan automáticamente, lo cual generalmente se explica por la característica episópica de la RL, pero estos patrones también existen en muchos modelos antes del entrenamiento con RL, y no necesariamente están estrictamente relacionados con la mejora lógica. Basándonos en esta observación, proponemos una serie de investigaciones detalladas para mejorar la capacidad lógica de los MLLMs: 1. Utilizamos un entrenamiento supervisado con un inicio frío (SFT) para generar patrones de pensamiento continuo estructurados. 2. Mejoramos la capacidad con aprendizaje por refuerzo usando GRPO. Los experimentos detallados en diferentes benchmarks de lógica difícil de MLLMs demuestran que el uso de SFT o RL solo no supera el método que combina ambos. Finalmente, el modelo resultante alcanza los mejores rendimientos en los MLLMs abiertos, constituyendo el líder en ambos tamaños de modelo: 3B y 7B. En particular, el modelo 7B ha mejorado significativamente en comparación con el base (por ejemplo, de 66.3% a 73.4% en MathVista y de 62.9% a 70.4% en We-Math), mientras que el modelo 3B compite con los rendimientos de varios modelos 7B. Esta investigación proporciona guías prácticas para la construcción de MLLMs mejorados. El código está disponible en https://github.com/waltonfuture/RL-with-Cold-Start.",
      "upvotes": 25,
      "discussionId": "6837c363b127cae8a0b36f6f",
      "projectPage": "https://github.com/waltonfuture/RL-with-Cold-Start",
      "githubRepo": "https://github.com/waltonfuture/RL-with-Cold-Start",
      "ai_summary": "A two-stage approach combining supervised fine-tuning and reinforcement learning enhances multimodal reasoning in large language models, achieving state-of-the-art performance on benchmarks.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "multimodal LLMs",
        "MLLMs",
        "reinforcement learning",
        "RL",
        "chain-of-thought reasoning",
        "GRPO",
        "supervised fine-tuning",
        "SFT",
        "multimodal reasoning",
        "MathVista",
        "We-Math"
      ]
    },
    "publishedAt": "2025-05-28T09:21:38.000Z",
    "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start",
    "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %rightarrow73.4 % on\nMathVista, 62.9 %rightarrow70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22334.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22457",
      "authors": [
        {
          "_id": "68380912e9c1608de91e23f3",
          "name": "Haonan Wang",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f4",
          "name": "Hongfu Liu",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f5",
          "name": "Xiangyan Liu",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f6",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f7",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f8",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f9",
          "name": "Tianyu Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T15:13:34.000Z",
      "submittedOnDailyAt": "2025-05-29T05:44:17.072Z",
      "title": "Esta es la predicción de los próximos eventos para fomentar el videorio.\n\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fomentar el videorio.\n- Predicción de los próximos eventos para fo",
      "submittedOnDailyBy": {
        "_id": "63d91b6d255ef6add20e1b38",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
        "isPro": false,
        "fullname": "Tianyu Pang",
        "user": "P2333",
        "type": "user"
      },
      "summary": "La siguiente predicción de tokens es una tarea de entrenamiento básica que permite a los modelos de lenguaje grandes (LLM) realizar inferencias. Sin embargo, ¿cómo deberían entrenar los modelos de aprendizaje de lenguaje multimodal (MLLM) que tienen capacidades temporales para procesar entradas de video? En la actualidad, las respuestas de clientes de video y otros elementos dependen frecuentemente de anotaciones humanas o de MLLMs potentes. En contraste, las capturas de video se utilizan principalmente para combinar inferencias temporales y información espacial. Para abordar estos problemas espaciales, se propone el Predicción de Eventos Futuros (NEP). El NEP es una tarea de entrenamiento que utiliza segmentos de video futuros como señales ricas y automaticamente observadas para promover inferencias temporales. Se dividen los videos en segmentos de antes y después, y el MLLM predice un resumen de los eventos que pueden ocurrir en los segmentos futuros a partir de los segmentos pasados. De esta manera, el modelo se estimula a realizar inferencias temporales para completar la tarea. Para apoyar este objetivo, se crea el conjunto de datos V1-33K. Este conjunto de datos consta de 33,000 segmentos de video extraídos automáticamente y registra diferentes escenas de vida real. Además, se amplía el rango de estrategias de entrenamiento de instrucciones de video para investigar el efecto de las inferencias temporales. Para evaluar el progreso, se introduce FutureBench, un método para evaluar la coincidencia de la predicción de eventos futuros no vistos antes. Las experimentaciones demuestran que el NEP proporciona un patrón de entrenamiento efectivo y adecuado para fomentar las inferencias temporales en los MLLM.",
      "upvotes": 23,
      "discussionId": "68380913e9c1608de91e2430",
      "githubRepo": "https://github.com/sail-sg/Video-Next-Event-Prediction",
      "ai_summary": "Next-event prediction (NEP) is proposed as a learning task to enable MLLMs to reason temporally over video inputs, using future video segments as a self-supervised signal.",
      "ai_keywords": [
        "next-token prediction",
        "next-event prediction (NEP)",
        "LLMs",
        "MLLMs",
        "video question answering",
        "video captioning",
        "temporal reasoning",
        "future video segments",
        "past frames",
        "video segments",
        "V1-33K",
        "video instruction-tuning strategies",
        "FutureBench"
      ]
    },
    "publishedAt": "2025-05-28T11:13:34.000Z",
    "title": "Fostering Video Reasoning via Next-Event Prediction",
    "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22457.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d91b6d255ef6add20e1b38",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
      "fullname": "Tianyu Pang",
      "name": "P2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19253",
      "authors": [
        {
          "_id": "6837bc8d0b39c9653de4d06f",
          "user": {
            "_id": "6244451c9fdefb55a0b900cc",
            "avatarUrl": "/avatars/ca2b46ddb5d905501d827920582b5438.svg",
            "isPro": false,
            "fullname": "Joao Coelho",
            "user": "jmvcoelho",
            "type": "user"
          },
          "name": "João Coelho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:45.049Z",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d070",
          "name": "Jingjie Ning",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d071",
          "name": "Jingyuan He",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d072",
          "name": "Kangrui Mao",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d073",
          "name": "Abhijay Paladugu",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d074",
          "name": "Pranav Setlur",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d075",
          "name": "Jiahe Jin",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d076",
          "name": "Jamie Callan",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d077",
          "name": "João Magalhães",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d078",
          "name": "Bruno Martins",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d079",
          "name": "Chenyan Xiong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T18:16:13.000Z",
      "submittedOnDailyAt": "2025-05-29T00:28:31.472Z",
      "title": "DeepResearchGym: Libre, transparente, y con reproducibilidad en el sandbox de evaluación profunda investigación",
      "submittedOnDailyBy": {
        "_id": "6135eeeb5bc6ecdf86b60f0d",
        "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
        "isPro": false,
        "fullname": "Shi Yu",
        "user": "yushi",
        "type": "user"
      },
      "summary": "Deep research systems son una clase de métodos de búsqueda de información emergentes que generan informes detallados y con evidencias abundantes para consultas complejas. Sin embargo, muchos de los actuales marcos de trabajo dependen de APIs de búsqueda comercial dinámicos y sufren problemas adicionales de reproducibilidad y transparencia, lo que les hace soportar costos elevados. Para resolver estas limitaciones, se presenta DeepResearchGym, un sandbox de código abierto que combina una API de búsqueda que garantiza la reproducibilidad con un protocolo de evaluación estricto para realizar benchmarks de sistemas de investigación profunda. La API utiliza la más reciente tecnología de búsqueda densa y la ANN en disco para indexar grandes corpus de web públicos como ClueWeb22 y FineWeb, logrando un rental de cut menor que los populares APIs comerciales, asegurando la estabilidad de la clasificación de documentos y permitiendo su uso gratuito para fines de investigación. Para evaluar los resultados de los sistemas de investigación profunda, se expandió los métricas de evaluación automática a través de la evaluación con un modelo de lenguaje (LLM-as-a-judge) y se desarrolló el benchmark Researchy Questions para evaluar la adecuación a la información del usuario, la precisión de la búsqueda y la calidad del informe. Los resultados de los experimentos muestran que los sistemas combinados con DeepResearchGym alcanzan el mismo rendimiento que los que utilizaron APIs comerciales, y la posición de rendimiento coincide con las métricas de evaluación. Además, los estudios de evaluación humana han confirmado que el protocolo de evaluación automático se alinea con las preferencias humanas y que el marco de trabajo puede ayudar a los sistemas de investigación profunda a realizar evaluaciones controladas. Los códigos y documentaciones de la API están disponibles en https://www.deepresearchgym.ai.",
      "upvotes": 17,
      "discussionId": "6837bc8d0b39c9653de4d0a6",
      "projectPage": "https://www.deepresearchgym.ai",
      "ai_summary": "DeepResearchGym provides an open-source evaluation framework for deep research systems using a reproducible search API and LLM-as-a-judge assessments.",
      "ai_keywords": [
        "agentic information retrieval",
        "deep research systems",
        "search API",
        "reproducibility",
        "transparency",
        "open-source sandbox",
        "ClueWeb22",
        "FineWeb",
        "dense retriever",
        "approximate nearest neighbor search",
        "DiskANN",
        "Researchy Questions benchmark",
        "LLM-as-a-judge",
        "retrieval faithfulness",
        "report quality",
        "human evaluation"
      ]
    },
    "publishedAt": "2025-05-25T14:16:13.000Z",
    "title": "DeepResearchGym: A Free, Transparent, and Reproducible Evaluation\n  Sandbox for Deep Research",
    "summary": "Deep research systems represent an emerging class of agentic information\nretrieval methods that generate comprehensive and well-supported reports to\ncomplex queries. However, most existing frameworks rely on dynamic commercial\nsearch APIs, which pose reproducibility and transparency challenges in addition\nto their cost. To address these limitations, we introduce DeepResearchGym, an\nopen-source sandbox that combines a reproducible search API with a rigorous\nevaluation protocol for benchmarking deep research systems. The API indexes\nlarge-scale public web corpora, namely ClueWeb22 and FineWeb, using a\nstate-of-the-art dense retriever and approximate nearest neighbor search via\nDiskANN. It achieves lower latency than popular commercial APIs while ensuring\nstable document rankings across runs, and is freely available for research use.\nTo evaluate deep research systems' outputs, we extend the Researchy Questions\nbenchmark with automatic metrics through LLM-as-a-judge assessments to measure\nalignment with users' information needs, retrieval faithfulness, and report\nquality. Experimental results show that systems integrated with DeepResearchGym\nachieve performance comparable to those using commercial APIs, with performance\nrankings remaining consistent across evaluation metrics. A human evaluation\nstudy further confirms that our automatic protocol aligns with human\npreferences, validating the framework's ability to help support controlled\nassessment of deep research systems. Our code and API documentation are\navailable at https://www.deepresearchgym.ai.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6135eeeb5bc6ecdf86b60f0d",
      "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
      "fullname": "Shi Yu",
      "name": "yushi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21925",
      "authors": [
        {
          "_id": "6837c23acce400abe6f18790",
          "user": {
            "_id": "60747cbf3ea03830676542b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
            "isPro": false,
            "fullname": "Chong Zeng",
            "user": "NCJ",
            "type": "user"
          },
          "name": "Chong Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:38.730Z",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18791",
          "user": {
            "_id": "63299011bdb6242b42b77f57",
            "avatarUrl": "/avatars/056aec97eb3c10d3b63eb13238e1d2a4.svg",
            "isPro": false,
            "fullname": "doyleconan",
            "user": "doyleconan",
            "type": "user"
          },
          "name": "Yue Dong",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:11:08.981Z",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18792",
          "name": "Pieter Peers",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18793",
          "name": "Hongzhi Wu",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18794",
          "name": "Xin Tong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60747cbf3ea03830676542b5/DuGYODs8k7DhVHb78BmCZ.jpeg"
      ],
      "publishedAt": "2025-05-28T03:20:46.000Z",
      "submittedOnDailyAt": "2025-05-29T00:45:14.960Z",
      "title": "RenderFormer: Triangulos de Correspondencia Basado en Transformer para Renormalización y Iluminación Global",
      "submittedOnDailyBy": {
        "_id": "60747cbf3ea03830676542b5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
        "isPro": false,
        "fullname": "Chong Zeng",
        "user": "NCJ",
        "type": "user"
      },
      "summary": "RenderFormer es una nueva red neuronal de renderización que realiza la representación de escenas basada en triángulos para renderizar imágenes directamente. Esta red permite obtener un efecto de iluminación global completo, lo que significa que no es necesario entrenar o ajustar cada escena individualmente. Su arquitectura se basa en la transformación de columnas de tokens que representan triángulos con propiedades de reflexión a columnas de tokens que representan pequeños patrón de píxeles. RenderFormer utiliza un flujo de trabajo en dos etapas: en la etapa independiente, se modela la propagación de luz desde los triángulos hasta otros triángulos, mientras que en la etapa dependiente, se transforman los tokens de triángulos obtenidos en la etapa independiente a valores de píxeles que representan los liguamientos. Ambas etapas se entrenan bajo una arquitectura transformadora con mínimos restricciones previas. Se evalúa la capacidad y rendimiento de RenderFormer en escenas con complejidad variable en términos de forma y propagación de luz.",
      "upvotes": 15,
      "discussionId": "6837c23ccce400abe6f18812",
      "projectPage": "https://microsoft.github.io/renderformer/",
      "githubRepo": "https://github.com/microsoft/renderformer",
      "ai_summary": "RenderFormer is a transformer-based neural rendering pipeline that renders images from triangle representations without per-scene training and with full global illumination effects.",
      "ai_keywords": [
        "neural rendering pipeline",
        "global illumination effects",
        "sequence-to-sequence transformation",
        "tokens",
        "reflectance properties",
        "pixel patches",
        "transformer architecture",
        "view-independent stage",
        "view-dependent stage",
        "triangle-to-triangle light transport",
        "ray bundles"
      ]
    },
    "publishedAt": "2025-05-27T23:20:46.000Z",
    "title": "RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with\n  Global Illumination",
    "summary": "We present RenderFormer, a neural rendering pipeline that directly renders an\nimage from a triangle-based representation of a scene with full global\nillumination effects and that does not require per-scene training or\nfine-tuning. Instead of taking a physics-centric approach to rendering, we\nformulate rendering as a sequence-to-sequence transformation where a sequence\nof tokens representing triangles with reflectance properties is converted to a\nsequence of output tokens representing small patches of pixels. RenderFormer\nfollows a two stage pipeline: a view-independent stage that models\ntriangle-to-triangle light transport, and a view-dependent stage that\ntransforms a token representing a bundle of rays to the corresponding pixel\nvalues guided by the triangle-sequence from the view-independent stage. Both\nstages are based on the transformer architecture and are learned with minimal\nprior constraints. We demonstrate and evaluate RenderFormer on scenes with\nvarying complexity in shape and light transport.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60747cbf3ea03830676542b5/DuGYODs8k7DhVHb78BmCZ.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60747cbf3ea03830676542b5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
      "fullname": "Chong Zeng",
      "name": "NCJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18600",
      "authors": [
        {
          "_id": "6837fe7664391bba7e477747",
          "name": "Bryan Sangwoo Kim",
          "hidden": false
        },
        {
          "_id": "6837fe7664391bba7e477748",
          "name": "Jeongsol Kim",
          "hidden": false
        },
        {
          "_id": "6837fe7664391bba7e477749",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T08:50:08.000Z",
      "submittedOnDailyAt": "2025-05-29T05:07:53.112Z",
      "title": "Chain-of-Zoom: Logro de la extremo de la altura de resolución mediante escala automática de regresión y ajuste de interés",
      "submittedOnDailyBy": {
        "_id": "6628efe14e1fa854f48d3a28",
        "avatarUrl": "/avatars/aa5421149a07a82b5c2a25978f9b6926.svg",
        "isPro": false,
        "fullname": "Sangwoo Kim",
        "user": "bryanswkim",
        "type": "user"
      },
      "summary": "Los modelos de alta resolución de una imagen única (SISR) modernos proporcionan resultados realistas dentro de un rango de factores de escala entre entrenados, pero se destruyen cuando se amplifican a un escala más grande. Para resolver este problema de escalabilidad, se utiliza la Chain-of-Zoom (CoZ). La CoZ descompone el SISR usando una auto-regresiva conexión intermedia que contiene información de múltiples escalas, basada en un Prompt que incluye información de varias escalas. La CoZ reutiliza el modelo principal de SR y divide el problema en subproblemas que permiten calcular probabilidades condicionales, logrando altas resoluciones sin necesidad de entrenamiento adicional. Debido a que la calidad visual disminuye a escalas altas, se agregan Prompts que incluyen información de múltiples escalas generados por un modelo de lenguaje visual (VLM) en cada paso de amplificación. Este proceso de extracción de Prompts se alinea con la dinámica de texto guiada por preferencias humanas utilizando la Policy Optimization de Reward Generalizado (GRPO) y un VLM de borde. Los experimentos muestran que envolviendo un modelo de SR estándar de 4x en la CoZ es posible lograr una amplificación de más de 256x con alta calidad visual y alta fidelidad. Página del proyecto: https://bryanswkim.github.io/chain-of-zoom/ .",
      "upvotes": 15,
      "discussionId": "6837fe7864391bba7e47779b",
      "projectPage": "https://bryanswkim.github.io/chain-of-zoom/",
      "githubRepo": "https://github.com/bryanswkim/Chain-of-Zoom",
      "ai_summary": "Chain-of-Zoom (CoZ) enhances single-image super-resolution models by using an autoregressive chain of intermediate scale-states and multi-scale-aware prompts to achieve extreme magnifications with high quality.",
      "ai_keywords": [
        "single-image super-resolution",
        "Chain-of-Zoom",
        "autoregressive chain",
        "multi-scale-aware prompts",
        "backbone SR model",
        "diffusion SR model",
        "prompt extractor",
        "Generalized Reward Policy Optimization",
        "critic VLM"
      ]
    },
    "publishedAt": "2025-05-24T04:50:08.000Z",
    "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and\n  Preference Alignment",
    "summary": "Modern single-image super-resolution (SISR) models deliver photo-realistic\nresults at the scale factors on which they are trained, but collapse when asked\nto magnify far beyond that regime. We address this scalability bottleneck with\nChain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an\nautoregressive chain of intermediate scale-states with multi-scale-aware\nprompts. CoZ repeatedly re-uses a backbone SR model, decomposing the\nconditional probability into tractable sub-problems to achieve extreme\nresolutions without additional training. Because visual cues diminish at high\nmagnifications, we augment each zoom step with multi-scale-aware text prompts\ngenerated by a vision-language model (VLM). The prompt extractor itself is\nfine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic\nVLM, aligning text guidance towards human preference. Experiments show that a\nstandard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement\nwith high perceptual quality and fidelity. Project Page:\nhttps://bryanswkim.github.io/chain-of-zoom/ .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6628efe14e1fa854f48d3a28",
      "avatarUrl": "/avatars/aa5421149a07a82b5c2a25978f9b6926.svg",
      "fullname": "Sangwoo Kim",
      "name": "bryanswkim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19075",
      "authors": [
        {
          "_id": "6837fc484d14d7c8800e8b9c",
          "user": {
            "_id": "64c3732de6c3860fba66ceb0",
            "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
            "isPro": false,
            "fullname": "JaeminKim",
            "user": "kjm981995",
            "type": "user"
          },
          "name": "Jaemin Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:35.589Z",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9d",
          "name": "Hangeol Chang",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9e",
          "name": "Hyunmin Hwang",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9f",
          "name": "Choonghan Kim",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8ba0",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T10:19:10.000Z",
      "submittedOnDailyAt": "2025-05-29T04:49:39.731Z",
      "title": "Universial Reservoir: Utiliza un solo reservoir combinable plug-and-play para Frozen LLMs.",
      "submittedOnDailyBy": {
        "_id": "64c3732de6c3860fba66ceb0",
        "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
        "isPro": false,
        "fullname": "JaeminKim",
        "user": "kjm981995",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) muestran excelentes capacidades generales, pero su mejora en el dominio lógico se asocia a una gran carga de recursos computacionales, y puede incluso experimentar pérdidas en su generalización. El método de Fine-Tuning de Eficiencia de Parámetros (PEFT) se ha propuesto como una alternativa más amigable con los recursos computacionales, pero requiere un retienimiento adicional para cada modelo LLM, dependiendo de su arquitectura. Para resolver estos problemas, proponemos el Universal Reasoner (UniR). UniR es un módulo lógico ligero, combinable, pluggable y playable. Este módulo puede combinarse con cualquier modelo LLM libre de restricciones para dotarle de habilidades lógicas específicas. En particular, UniR es un módulo lógico especializado que se entrena independientemente utilizando compensaciones reservadas, transformando señales de nivel de proceso en guías de nivel de token. Después del entrenamiento, UniR se agrega simplemente a la salida logística del modelo LLM, permitiendo su combinación con cualquier modelo LLM libre de restricciones. Esta estructura adicional facilita la combinación de módulos: módulos de UniR entrenados en diferentes tareas se pueden aplicar juntos sumando las salidas logísticas para abordar lógicos complejos. Los resultados de experimentos en lógica matemática y traducción automática muestran que UniR supera significativamente a los métodos de reentrenamiento de los baselines existentes utilizando el modelo Llama3.2. Además, UniR muestra una fuerte generalización, demostrando que módulos lógicos entrenados en pequeños modelos pueden guiar eficazmente modelos más grandes. Esto representa una solución eficiente, adaptable y potente para mejorar la lógica de los LLMs, evitando dañar sus capacidades principales. El código está disponible en https://github.com/hangeol/UniR.",
      "upvotes": 14,
      "discussionId": "6837fc494d14d7c8800e8be6",
      "ai_summary": "UniR, a lightweight reasoning module, enhances Large Language Models with specialized reasoning abilities through modular composition, improving performance and generalization at lower computational costs.",
      "ai_keywords": [
        "Large Language Models",
        "Parameter-Efficient Fine-Tuning",
        "Universal Reasoner",
        "trajectory-level signals",
        "token-level guidance",
        "additive structure",
        "modular composition",
        "Llama3.2",
        "mathematical reasoning",
        "machine translation",
        "cost-efficient",
        "adaptable",
        "robust"
      ]
    },
    "publishedAt": "2025-05-25T06:19:10.000Z",
    "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for\n  Frozen LLMs",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms existing baseline fine-tuning methods using the\nLlama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19075.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3732de6c3860fba66ceb0",
      "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
      "fullname": "JaeminKim",
      "name": "kjm981995",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21887",
      "authors": [
        {
          "_id": "6837e3400aa18c6f96fe4876",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:49.456Z",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4877",
          "name": "Yahia Salaheldin Shaaban",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4878",
          "name": "Martin Takac",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4879",
          "name": "Salem Lahlou",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe487a",
          "name": "Zangir Iklassov",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Xa51WJrOIV-xl7A-Ry4t2.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/eS_bEDm2o2WAbn3YPS2XR.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/jaG4WahYyrQGaalZObNew.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mMKY5jn0SCJPxxR8_mssx.png"
      ],
      "publishedAt": "2025-05-28T02:03:31.000Z",
      "submittedOnDailyAt": "2025-05-29T03:02:06.399Z",
      "title": "SVRPBench: Benchmark Práctico para el Problema de Ruteo de Vías Aleatorias",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "El procesamiento de rutas seguro es central en la logística real, y la mayoría de los benchmarks asumen entornos estáticos e ideales. Presentamos SVRPBench, el primer benchmark abierto para comprender la precisión alta de procesamiento de rutas a escala urbana. Experiencia con más de 500 instancias y más de 1000 cranes, que modelan ventanas de tiempo experimentales basadas en tráfico dependiente del tiempo, retrasos logarítmicos, pensamientos aleatorios, cranes residenciales y comerciales. Nuestro proceso genera escenarios diversos y incluye una rica variedad de configuraciones y restricciones. A través de los resultados del benchmark, incluso los solvers RL más avanzados, como POMO y AM, sufren un descenso en rendimiento del 20% o más debido a variaciones distribuidas, mientras que los métodos clásicos y meta- híbridos son robustos. Liberamos el conjunto de datos y el sistema de evaluación para facilitar la investigación experimental. SVRPBench desafía a la comunidad a diseñar solvers generalizables más allá de suposiciones sintéticas.",
      "upvotes": 13,
      "discussionId": "6837e3410aa18c6f96fe48b8",
      "githubRepo": "https://github.com/yehias21/vrp-benchmarks",
      "ai_summary": "SVRPBench introduces a new benchmark for vehicle routing under uncertainty, simulating realistic urban conditions and highlighting the limitations of state-of-the-art RL solvers.",
      "ai_keywords": [
        "vehicle routing",
        "SVRPBench",
        "time-dependent congestion",
        "log-normal delays",
        "probabilistic accidents",
        "multi-depot",
        "multi-vehicle",
        "state-of-the-art RL solvers",
        "POMO",
        "AM",
        "distributional shift",
        "classical methods",
        "metaheuristic methods"
      ]
    },
    "publishedAt": "2025-05-27T22:03:31.000Z",
    "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem",
    "summary": "Robust routing under uncertainty is central to real-world logistics, yet most\nbenchmarks assume static, idealized settings. We present SVRPBench, the first\nopen benchmark to capture high-fidelity stochastic dynamics in vehicle routing\nat urban scale. Spanning more than 500 instances with up to 1000 customers, it\nsimulates realistic delivery conditions: time-dependent congestion, log-normal\ndelays, probabilistic accidents, and empirically grounded time windows for\nresidential and commercial clients. Our pipeline generates diverse,\nconstraint-rich scenarios, including multi-depot and multi-vehicle setups.\nBenchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade\nby over 20% under distributional shift, while classical and metaheuristic\nmethods remain robust. To enable reproducible research, we release the dataset\nand evaluation suite. SVRPBench challenges the community to design solvers that\ngeneralize beyond synthetic assumptions and adapt to real-world uncertainty.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Xa51WJrOIV-xl7A-Ry4t2.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/eS_bEDm2o2WAbn3YPS2XR.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/jaG4WahYyrQGaalZObNew.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mMKY5jn0SCJPxxR8_mssx.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21887.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 39
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22129",
      "authors": [
        {
          "_id": "6837dae6e9b21653755a05d4",
          "user": {
            "_id": "64c71a5647418a0a59e5c7cb",
            "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
            "isPro": false,
            "fullname": "Jinhong Ni",
            "user": "mcleanie",
            "type": "user"
          },
          "name": "Jinhong Ni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:03.476Z",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d5",
          "user": {
            "_id": "65434daa5a36a8774d0e2271",
            "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
            "isPro": false,
            "fullname": "Allen Zhang",
            "user": "allencbzhang",
            "type": "user"
          },
          "name": "Chang-Bin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:59.930Z",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d6",
          "name": "Qiang Zhang",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d7",
          "name": "Jing Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T08:54:04.000Z",
      "submittedOnDailyAt": "2025-05-29T02:38:27.338Z",
      "title": "¿Qué factores son esenciales para la generación estable de un panorama de 360 grados?",
      "submittedOnDailyBy": {
        "_id": "65434daa5a36a8774d0e2271",
        "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
        "isPro": false,
        "fullname": "Allen Zhang",
        "user": "allencbzhang",
        "type": "user"
      },
      "summary": "Recientemente, se ha iniciado un estudio que aplica la abundancia de modelos de difusión de imágenes a la generación de imágenes de panorámica 360°, como Stable Diffusion. Los estudios previos han demostrado la efectividad de técnicas generales de difusión de bajo rendimiento, utilizando modelos de difusión previamente entrenados para generar imágenes de panorámica. Sin embargo, existen dudas sobre la gran diferencia de ángulos de captura y el área entre las imágenes de panorámica, y los éxitos experimentales de estos estudios. Investigamos que los componentes entrenables se entrenan finalmente en datos de panorámica y muestran comportamientos diferentes, y que este difusión oculta una función interna que utiliza el conocimiento previamente entrenado dentro del modelo de difusión. Nuestro análisis se resume en: 1) los módulos de atención y las matrices de consulta y clave son responsables de la información común compartida entre el área de la panorámica y el ángulo de captura, y no están relacionados con la generación de panorámica; 2) las matrices de valores y de pesos de salida se especializan en aplicar el conocimiento previamente entrenado en el área de la panorámica, y juegan un papel más importante en el entrenamiento final de la generación de panorámica. Proponemos un simple marco experimental para probar estas observaciones y llamamos a este marco UniPano. Nuestro objetivo es crear un excelente referente para futuras investigaciones. UniPano supera los métodos existentes y, en comparación con el enfoque doble de los estudios previos, reduce significativamente el uso de memoria y el tiempo de entrenamiento, mientras que también se puede aplicar a la generación de panorámicas de alta resolución. El código está disponible.",
      "upvotes": 12,
      "discussionId": "6837daece9b21653755a0791",
      "ai_summary": "Analysis of fine-tuning diffusion models for panoramic image generation reveals distinct roles of attention module matrices and introduces UniPano, a memory-efficient and speed-enhanced baseline framework.",
      "ai_keywords": [
        "text-to-image diffusion models",
        "Stable Diffusion",
        "low-rank adaptation",
        "pre-trained diffusion models",
        "attention modules",
        "query matrices",
        "key matrices",
        "value matrices",
        "output weight matrices",
        "panoramic image generation",
        "domain gap",
        "common information",
        "pre-trained knowledge",
        "UniPano",
        "end-to-end panorama generation",
        "memory usage",
        "training time"
      ]
    },
    "publishedAt": "2025-05-28T04:54:04.000Z",
    "title": "What Makes for Text to 360-degree Panorama Generation with Stable\n  Diffusion?",
    "summary": "Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,\nhas stimulated research to adapt them to 360-degree panorama generation. Prior\nwork has demonstrated the feasibility of using conventional low-rank adaptation\ntechniques on pre-trained diffusion models to generate panoramic images.\nHowever, the substantial domain gap between perspective and panoramic images\nraises questions about the underlying mechanisms enabling this empirical\nsuccess. We hypothesize and examine that the trainable counterparts exhibit\ndistinct behaviors when fine-tuned on panoramic data, and such an adaptation\nconceals some intrinsic mechanism to leverage the prior knowledge within the\npre-trained diffusion models. Our analysis reveals the following: 1) the query\nand key matrices in the attention modules are responsible for common\ninformation that can be shared between the panoramic and perspective domains,\nthus are less relevant to panorama generation; and 2) the value and output\nweight matrices specialize in adapting pre-trained knowledge to the panoramic\ndomain, playing a more critical role during fine-tuning for panorama\ngeneration. We empirically verify these insights by introducing a simple\nframework called UniPano, with the objective of establishing an elegant\nbaseline for future research. UniPano not only outperforms existing methods but\nalso significantly reduces memory usage and training time compared to prior\ndual-branch approaches, making it scalable for end-to-end panorama generation\nwith higher resolution. The code will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65434daa5a36a8774d0e2271",
      "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
      "fullname": "Allen Zhang",
      "name": "allencbzhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20411",
      "authors": [
        {
          "_id": "683735aff42cc8a1d260e677",
          "user": {
            "_id": "654e5e094319c75e3e1b6cbc",
            "avatarUrl": "/avatars/a8889036fa38f80f2d45aea8d1471395.svg",
            "isPro": false,
            "fullname": "Ibragim",
            "user": "ibragim-bad",
            "type": "user"
          },
          "name": "Ibragim Badertdinov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:24.705Z",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e678",
          "user": {
            "_id": "644e9ffcd6001776ed77d874",
            "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
            "isPro": false,
            "fullname": "Alexander",
            "user": "djalexj",
            "type": "user"
          },
          "name": "Alexander Golubev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T19:13:38.030Z",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e679",
          "name": "Maksim Nekrashevich",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67a",
          "name": "Anton Shevtsov",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67b",
          "user": {
            "_id": "65e48cb3a4e46e644ec1277d",
            "avatarUrl": "/avatars/dd2bf04a6f81bf0a0892080af5d485b2.svg",
            "isPro": false,
            "fullname": "Simon Karasik",
            "user": "sbkarasik",
            "type": "user"
          },
          "name": "Simon Karasik",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:26.644Z",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67c",
          "name": "Andrei Andriushchenko",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67d",
          "name": "Maria Trofimova",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67e",
          "name": "Daria Litvintseva",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67f",
          "name": "Boris Yangel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T18:01:00.000Z",
      "submittedOnDailyAt": "2025-05-29T06:59:04.261Z",
      "title": "SWE-rebench: Pipelina para automatizar la estadística de tareas y la evaluación de decontaminación en ingeniería de software",
      "submittedOnDailyBy": {
        "_id": "644e9ffcd6001776ed77d874",
        "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
        "isPro": false,
        "fullname": "Alexander",
        "user": "djalexj",
        "type": "user"
      },
      "summary": "Los agentes basados en modelos de lenguaje grande (LLM) están demostrando capacidades cada vez más variadas en tareas de desarrollo de software (SWE). Sin embargo, su desarrollo se enfrenta a dos problemas importantes. El primero es la escasez de datos de entrenamiento de alta calidad, especialmente, datos que reflejen escenarios reales de SWE donde el agente interactúa con el entorno de desarrollo, ejecuta código y modifica su comportamiento basado en los resultados de esos ejercicios. Los conjuntos de datos actuales están limitados a un conjunto de tareas de generación de código o pequeñas modificaciones interactivas, lo que limita su escala y diversidad. El segundo problema es la falta de nuevas tareas interactivas, lo que afecta significativamente la evaluación de modelos que se están mejorando rápidamente. Los benchmarks estáticos se desgastan rápidamente debido a problemas de contenedores. Para resolver estas limitaciones, hemos introducido una nueva, autómata y escalable pipeline para extraer tareas interactivas continuamente de diferentes repositorios de GitHub. Este pipeline ha permitido la construcción del conjunto de datos público SWE-rebench. Este conjunto de datos incluye más de 21,000 tareas interactivas de SWE basadas en Python y está diseñado para el aprendizaje por refuerzo de agentes de SWE. Además, mediante el uso del método de SWE-rebench para la recolección de nuevas tareas, hemos construido un benchmark que no se ve afectado por problemas de contenedores. En este benchmark, los resultados de cada LLM se comparan con los de SWE-bench Verified, lo que demuestra la posibilidad de que el rendimiento de modelos de ingeniería se expanda debido a problemas de contenedores.",
      "upvotes": 11,
      "discussionId": "683735b0f42cc8a1d260e69f",
      "ai_summary": "A novel pipeline extracts real-world, interactive software engineering tasks from GitHub to create SWE-rebench, improving the evaluation of reinforcement learning models in SWE.",
      "ai_keywords": [
        "LLM-based agents",
        "reinforcement learning",
        "software engineering tasks",
        "GitHub repositories",
        "SWE-rebench",
        "contamination-free benchmark",
        "SWE-bench Verified"
      ]
    },
    "publishedAt": "2025-05-26T14:01:00.000Z",
    "title": "SWE-rebench: An Automated Pipeline for Task Collection and\n  Decontaminated Evaluation of Software Engineering Agents",
    "summary": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644e9ffcd6001776ed77d874",
      "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
      "fullname": "Alexander",
      "name": "djalexj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22648",
      "authors": [
        {
          "_id": "6837c03cbbee677da73e6034",
          "user": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "name": "Jialong Wu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:02:37.069Z",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6035",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6036",
          "name": "Runnan Fang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6037",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6038",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6039",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603a",
          "name": "Dingchu Zhang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603b",
          "name": "Zekun Xi",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603c",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603d",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603e",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603f",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/vhAmZAlJqekE6vLcVjWtO.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/RXQVwE9PRmBzxCURRiKAU.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/woFRNvRqdUXKnAHpRH2eM.mp4"
      ],
      "publishedAt": "2025-05-28T17:57:07.000Z",
      "submittedOnDailyAt": "2025-05-29T00:34:30.750Z",
      "title": "Webdir: Dirección para la exploración automática de información",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "Para resolver problemas complejos reales, es necesario explorar información concreta y hacer inferencias en etapas. El desarrollo de sistemas de agentes como Deep Research ha enfatizado la posibilidad de investigación en etapas automatizadas. En este artículo, desde una perspectiva centrada en datos y en el proceso de entrenamiento, proponemos una serie de paradigmas para la construcción de agentes de búsqueda de información en dispositivos de usuario. Nuestro enfoque se compone de cuatro etapas principales: 1) la construcción de un data broker, 2) la muestreo de queries, 3) la eficiente entrenamiento inicial, y 4) el aprendizaje por refuerzo para la expansión. Este marco se implementó en WebDancer, un agente web basado en ReAct. Los experimentos en los benchmarks de información difícil como GAIA y WebWalkerQA mostraron la fuerza de WebDancer y claramente demostraron el efecto de nuestro paradigma de entrenamiento. Un análisis adicional para el entrenamiento del agente proporciona ideas y contraseñas de acción e sistema que pueden ayudar al desarrollo de modelos de agentes más eficientes. El código y el demo están disponibles en https://github.com/Alibaba-NLP/WebAgent.",
      "upvotes": 10,
      "discussionId": "6837c03dbbee677da73e607f",
      "githubRepo": "https://github.com/Alibaba-NLP/WebAgent",
      "ai_summary": "The paper proposes a framework for building end-to-end agentic information seeking agents through a combination of data construction, trajectory sampling, supervised fine-tuning, and reinforcement learning, showcasing its effectiveness on information seeking benchmarks.",
      "ai_keywords": [
        "browsing data construction",
        "trajectories sampling",
        "supervised fine-tuning",
        "reinforcement learning",
        "WebDancer",
        "GAIA",
        "WebWalkerQA"
      ]
    },
    "publishedAt": "2025-05-28T13:57:07.000Z",
    "title": "WebDancer: Towards Autonomous Information Seeking Agency",
    "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/vhAmZAlJqekE6vLcVjWtO.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/RXQVwE9PRmBzxCURRiKAU.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/woFRNvRqdUXKnAHpRH2eM.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22648.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19187",
      "authors": [
        {
          "_id": "6837210455e9bab4e9c302b1",
          "user": {
            "_id": "6002c316698168af3bb9f4a6",
            "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
            "isPro": false,
            "fullname": "yangxiao",
            "user": "YangXiao-nlp",
            "type": "user"
          },
          "name": "Yang Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T19:13:52.090Z",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b2",
          "name": "Jiashuo Wang",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b3",
          "name": "Ruifeng Yuan",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b4",
          "name": "Chunpu Xu",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b5",
          "name": "Kaishuai Xu",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b6",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b7",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T15:17:57.000Z",
      "submittedOnDailyAt": "2025-05-29T01:05:08.605Z",
      "title": "LIMOPro: Desarrollo de la Teoría Lógica para el Programación Eficiente y Efectiva del Horario de Pruebas de Tiempo",
      "submittedOnDailyBy": {
        "_id": "6002c316698168af3bb9f4a6",
        "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
        "isPro": false,
        "fullname": "yangxiao",
        "user": "YangXiao-nlp",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) muestran una capacidad lógica y computacional sorprendente cuando se utilizan datos de \"Chain of Thought\" (CoT) durante el entrenamiento final. Sin embargo, estas cadenas lógicas incluyen muchos elementos de largas oraciones, especialmente si se trata de un proceso evolutivo de solución (camino fundamental para resolver un problema) y de elementos funcionales como el proceso de verificación, la búsqueda de soluciones alternativas y la corrección de errores. Aunque el proceso evolutivo es importante, los elementos funcionales aumentan significativamente la carga computacional durante el proceso de inferencia.\n\nIntroducimos un marco estructurado para evaluar de manera cuantitativa la importancia de cada etapa lógica en la predicción de respuestas, con el objetivo de mejorar la eficiencia del entrenamiento. Este marco, conocido como PIR (Refinamiento de Importancia basado en Perplejidad), permite mantener los componentes evolutivos de la lógica mientras elimina selectivamente etapas funcionales de baja importancia, generando datos de entrenamiento optimizados. Los modelos entrenados con PIR muestran mejores resultados en escalas de prueba, con una reducción del consumo de computación (-3% a -41%) y un aumento de precisión (+0.9% a +6.6%). Este efecto se ha observado en pruebas lógicas difíciles como AIME, AMC y GPQA Diamond. Nuestro enfoque demostra la generalización fuerte en diferentes tamaños de modelo, fuentes de datos y niveles de token, ofreciendo una solución útil para la aplicación práctica de los LLMs lógicos.",
      "upvotes": 10,
      "discussionId": "6837210555e9bab4e9c302f2",
      "ai_summary": "A framework called PIR refines the importance of reasoning steps in large language models by pruning low-importance functional elements, leading to more concise reasoning chains with improved accuracy and reduced computational demands.",
      "ai_keywords": [
        "large language models",
        "chain-of-thought",
        "large reasoning models",
        "progressive reasoning",
        "functional elements",
        "perplexity-based importance refinement",
        "token usage",
        "reasoning benchmarks",
        "AIME",
        "AMC",
        "GPQA Diamond"
      ]
    },
    "publishedAt": "2025-05-25T11:17:57.000Z",
    "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time\n  Scaling",
    "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities through test-time scaling approaches, particularly when fine-tuned\nwith chain-of-thought (CoT) data distilled from more powerful large reasoning\nmodels (LRMs). However, these reasoning chains often contain verbose elements\nthat mirror human problem-solving, categorized as progressive reasoning (the\nessential solution development path) and functional elements (verification\nprocesses, alternative solution approaches, and error corrections). While\nprogressive reasoning is crucial, the functional elements significantly\nincrease computational demands during test-time inference. We introduce PIR\n(Perplexity-based Importance Refinement), a principled framework that\nquantitatively evaluates the importance of each reasoning step based on its\nimpact on answer prediction confidence. PIR systematically identifies and\nselectively prunes only low-importance functional steps while preserving\nprogressive reasoning components, creating optimized training data that\nmaintains the integrity of the core solution path while reducing verbosity.\nModels fine-tuned on PIR-optimized data exhibit superior test-time scaling\nproperties, generating more concise reasoning chains while achieving improved\naccuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to\n-41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).\nOur approach demonstrates strong generalizability across different model sizes,\ndata sources, and token budgets, offering a practical solution for deploying\nreasoning-capable LLMs in scenarios where efficient test-time scaling, response\ntime, and computational efficiency are valuable constraints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19187.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6002c316698168af3bb9f4a6",
      "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
      "fullname": "yangxiao",
      "name": "YangXiao-nlp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17663",
      "authors": [
        {
          "_id": "6833c6ff97966d18e7b995b0",
          "user": {
            "_id": "6002c316698168af3bb9f4a6",
            "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
            "isPro": false,
            "fullname": "yangxiao",
            "user": "YangXiao-nlp",
            "type": "user"
          },
          "name": "Yang Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T19:14:41.639Z",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b1",
          "name": "Jiashuo Wang",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b2",
          "name": "Qiancheng Xu",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b3",
          "name": "Changhe Song",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b4",
          "name": "Chunpu Xu",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b5",
          "name": "Yi Cheng",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b6",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b7",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T09:27:40.000Z",
      "submittedOnDailyAt": "2025-05-29T01:05:57.079Z",
      "title": "Teoría del Corazón Dinámico: Evaluación de la Adaptación de los Modelos de Lenguaje de Alto Nivel a la Evolución de Tiempo de Estados Humanos",
      "submittedOnDailyBy": {
        "_id": "6002c316698168af3bb9f4a6",
        "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
        "isPro": false,
        "fullname": "yangxiao",
        "user": "YangXiao-nlp",
        "type": "user"
      },
      "summary": "Con el aumento de la participación de los LLM en el intercambio interactivo entre seres humanos y IA, la evaluación de la capacidad de la Teoría del Mente (ToM) ha adquirido importancia. En particular, la capacidad de seguir la evolución del estado de la mente es crucial. Actualmente, los estándares se centran en evaluar las habilidades básicas de la ToM, pero principalmente en estados mentales estáticos, ignorando la realidad de la interacción social en el tiempo. Se propone un nuevo estándar llamado DynToM. Este estándar ha sido diseñado específicamente para evaluar la capacidad de los LLM para seguir la evolución del estado de la mente y conectarlo en escenarios temporales. Mediante un marco de trabajo sistemático de 4 etapas, se generaron 1,100 contextos sociales, los cuales se incluyeron en 5,500 escenarios y 78,100 preguntas. Estos fueron validados por realismo y calidad. La evaluación detallada de los 10 mejores LLMs reveló que el rendimiento promedio era 44.7% menor que el de los humanos, y que la capacidad de seguir y explicar el estado de la mente decía notablemente. Estos diferencias en rendimiento revelan las limitaciones básicas de los LLMs en modelar las características dinámicas del estado de la mente humana.",
      "upvotes": 10,
      "discussionId": "6833c70097966d18e7b99616",
      "ai_summary": "The DynToM benchmark evaluates LLMs' ability to track and understand the temporal progression of mental states, revealing significant gaps compared to human performance.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Theory of Mind (ToM)",
        "DynToM",
        "social contexts",
        "mental states",
        "temporal progression",
        "evaluation framework",
        "state-of-the-art LLMs",
        "performance gap",
        "dynamic mental states"
      ]
    },
    "publishedAt": "2025-05-23T05:27:40.000Z",
    "title": "Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal\n  Evolution of Human States",
    "summary": "As Large Language Models (LLMs) increasingly participate in human-AI\ninteractions, evaluating their Theory of Mind (ToM) capabilities - particularly\ntheir ability to track dynamic mental states - becomes crucial. While existing\nbenchmarks assess basic ToM abilities, they predominantly focus on static\nsnapshots of mental states, overlooking the temporal evolution that\ncharacterizes real-world social interactions. We present DynToM, a\nnovel benchmark specifically designed to evaluate LLMs' ability to understand\nand track the temporal progression of mental states across interconnected\nscenarios. Through a systematic four-step framework, we generate 1,100 social\ncontexts encompassing 5,500 scenarios and 78,100 questions, each validated for\nrealism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs\nreveals that their average performance underperforms humans by 44.7\\%, with\nperformance degrading significantly when tracking and reasoning about the shift\nof mental states. This performance gap highlights fundamental limitations in\ncurrent LLMs' ability to model the dynamic nature of human mental states.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17663.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6002c316698168af3bb9f4a6",
      "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
      "fullname": "yangxiao",
      "name": "YangXiao-nlp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22019",
      "authors": [
        {
          "_id": "6837ed297d00cf0a04677bc1",
          "user": {
            "_id": "657429d833e5a4bf5b278615",
            "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
            "isPro": false,
            "fullname": "QiuchenWang",
            "user": "autumncc",
            "type": "user"
          },
          "name": "Qiuchen Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:19.039Z",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc2",
          "name": "Ruixue Ding",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc3",
          "user": {
            "_id": "665d652e0f35c005de892108",
            "avatarUrl": "/avatars/240bebdc7fdc6d50719c65de0e3cf1cd.svg",
            "isPro": false,
            "fullname": "Yu Zeng",
            "user": "YuZeng260",
            "type": "user"
          },
          "name": "Yu Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:42.906Z",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc4",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc5",
          "user": {
            "_id": "64b02ec0e5000ae8a572ced5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
            "isPro": false,
            "fullname": "Lin Chen",
            "user": "Lin-Chen",
            "type": "user"
          },
          "name": "Lin Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:44.983Z",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc6",
          "name": "Shihang Wang",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc7",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc8",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc9",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T06:30:51.000Z",
      "submittedOnDailyAt": "2025-05-29T03:46:05.539Z",
      "title": "VRAG-RL: Método de fortalecimiento de la comprensión de información visual mediante un RAG basado en visión, fortalecido mediante un aprendizaje por refuerzo mediante inferencia iterativa",
      "submittedOnDailyBy": {
        "_id": "64b02ec0e5000ae8a572ced5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
        "isPro": false,
        "fullname": "Lin Chen",
        "user": "Lin-Chen",
        "type": "user"
      },
      "summary": "Una búsqueda efectiva y lógica de amplia información asociada a imágenes, así como su comprensión, es un desafío de los métodos RAG. Los métodos tradicionales basados en texto no pueden procesar información relacionada con imágenes. Por otro lado, los enfoques actuales basados en imágenes para RAG están limitados por líneas de producción fijas y presentan dificultades para ejecutar lógica efectivamente debido a la insuficiencia de capacidades básicas de los modelos. Hemos demostrado que la RL puede tener un impacto benéfico en la lógica de los modelos, por lo que presentamos un nuevo marco de RL VRAG-RL para realizar lógicas complejas. Este marco permite que VLMs interactúen con un motor de búsqueda, utilizando tokens de reconocimiento de imágenes para auto-muestrear una lógica de una vez o varias veces, y se optimicen continuamente basándose en estos muestreos. Nuestro enfoque destaca los principales límites de la RL en el campo de RAG: (i) los enfoques RAG multi-modelo tradicionales incluyen solo imágenes en el contexto, lo que resulta en una asignación insuficiente de tokens de lógica y no superan la reconocimiento propio de las imágenes; (ii) cuando un modelo interactúa con un motor de búsqueda, no puede expresar claramente sus solicitudes, lo que impide que se busque información relevante, por lo que no puede alcanzar el mejor rendimiento. Para resolver estos desafíos, definimos un espacio de acción aplicando un mapa de tiles a los inputs asociados a imágenes y incluimos acciones como corte y escalamiento, permitiendo que el modelo recoja información desde una perspectiva visual de la core stripe hasta la pin stripe. Además, para reducir la distancia entre las preguntas de usuarios y el modelo de búsqueda, utilizamos una compensación sencilla y efectiva, integrando la re-creación de las consultas y el rendimiento de la búsqueda con una compensación basada en el modelo. VRAG-RL está diseñado para optimizar tareas de RAG con estrategias de RL específicas y tiene como objetivo que el modelo se adapte a aplicaciones en la realidad. El código está disponible en https://github.com/Alibaba-NLP/VRAG.",
      "upvotes": 8,
      "discussionId": "6837ed297d00cf0a04677bf5",
      "githubRepo": "https://github.com/Alibaba-NLP/VRAG",
      "ai_summary": "VRAG-RL, a reinforcement learning framework, enhances reasoning and visual information handling in RAG methods by integrating visual perception tokens and employing specialized action spaces and rewards.",
      "ai_keywords": [
        "reinforcement learning",
        "VRAG-RL",
        "VLMs",
        "search engines",
        "visually rich information",
        "reasoning trajectories",
        "visual perception tokens",
        "action space",
        "query rewriting",
        "retrieval performance",
        "model-based reward"
      ]
    },
    "publishedAt": "2025-05-28T02:30:51.000Z",
    "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich\n  Information Understanding via Iterative Reasoning with Reinforcement Learning",
    "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\nhttps://github.com/Alibaba-NLP/VRAG{https://github.com/Alibaba-NLP/VRAG}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22019.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64b02ec0e5000ae8a572ced5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
      "fullname": "Lin Chen",
      "name": "Lin-Chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 89
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22232",
      "authors": [
        {
          "_id": "683815574d9866c160e88670",
          "name": "Mehdi Ali",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88671",
          "user": {
            "_id": "62fa1d95e8c9c532aa75331c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
            "isPro": false,
            "fullname": "Manuel Brack",
            "user": "mbrack",
            "type": "user"
          },
          "name": "Manuel Brack",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:14.826Z",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88672",
          "name": "Max Lübbering",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88673",
          "name": "Elias Wendt",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88674",
          "name": "Abbas Goher Khan",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88675",
          "name": "Richard Rutmann",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88676",
          "name": "Alex Jude",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88677",
          "name": "Maurice Kraus",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88678",
          "name": "Alexander Arno Weber",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88679",
          "name": "Felix Stollenwerk",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867a",
          "name": "David Kaczér",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867b",
          "name": "Florian Mai",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867c",
          "name": "Lucie Flek",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867d",
          "name": "Rafet Sifa",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867e",
          "name": "Nicolas Flores-Herr",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867f",
          "name": "Joachim Köhler",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88680",
          "name": "Patrick Schramowski",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88681",
          "name": "Michael Fromm",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88682",
          "name": "Kristian Kersting",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T11:06:54.000Z",
      "submittedOnDailyAt": "2025-05-29T07:38:07.888Z",
      "title": "Calidad de lenguaje: acceso multilingüe para entrenamiento\nFiltrado de datos con modelos de lenguaje\n\n**Nota:** La traducción se ha realizado manteniendo la profundidad y precisión del texto original.",
      "submittedOnDailyBy": {
        "_id": "62fa1d95e8c9c532aa75331c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
        "isPro": false,
        "fullname": "Manuel Brack",
        "user": "mbrack",
        "type": "user"
      },
      "summary": "Los datos de entrenamiento de alta calidad para múltiples lenguajes son esenciales para la entrenamiento previo efectivo de modelos de lenguaje grandes (LLMs). Sin embargo, el uso de conjuntos de datos de lenguajes de fuente abierta adecuados está limitado. Los conjuntos de datos líderes actuales se basan principalmente en métodos de filtrado heurístico, lo que limita su capacidad de transformación entre lenguajes y su escalabilidad. En este contexto, presentamos una metodología sistemática llamada JQL. Esta metodología se centra en la eficiencia y la diversidad de los datos multilingües, con el objetivo de reducir significativamente el carga computacional. JQL aplica atributos a modelos ligeros de etiquetado basados en las capacidades de notas de los LLMs. Estos modelos muestran un comportamiento fuerte en lenguajes no vistos y en diferentes sistemas de caracteres durante el entrenamiento, demostrando una mejora en el rendimiento multilingüe y interlingüe. Tras una evaluación experimental con 35 lenguajes, JQL supera notablemente los métodos de filtrado heurístico como Fineweb2. En particular, JQL enriquece la calidad del entrenamiento de modelos y aumenta la densidad de datos. Nuestro trabajo ofrece una herramienta práctica y recursos valiosos, con el objetivo de elevar los estándares de datos multilingües.",
      "upvotes": 7,
      "discussionId": "683815594d9866c160e88708",
      "projectPage": "https://huggingface.co/spaces/Jackal-AI/JQL",
      "githubRepo": "https://github.com/JQL-AI/JQL-Annotation-Pipeline",
      "ai_summary": "JQL systematically curates high-quality multilingual training data using pretrained multilingual embeddings, outperforming heuristic methods and improving downstream model training across diverse languages.",
      "ai_keywords": [
        "pretraining",
        "large language models",
        "multilingual datasets",
        "heuristic filtering methods",
        "JQL",
        "lightweight annotators",
        "multilingual embeddings",
        "cross-lingual transferability",
        "annotation pipeline",
        "data retention rates",
        "multilingual data curation"
      ]
    },
    "publishedAt": "2025-05-28T07:06:54.000Z",
    "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining\n  Data Filtering with Language Models",
    "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22232.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fa1d95e8c9c532aa75331c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
      "fullname": "Manuel Brack",
      "name": "mbrack",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22613",
      "authors": [
        {
          "_id": "6837d79d4d9866c160d8f43b",
          "name": "Yuchi Wang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43c",
          "name": "Yishuo Cai",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43d",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43e",
          "name": "Sihan Yang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43f",
          "name": "Linli Yao",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f440",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f441",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f442",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f443",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:29:34.000Z",
      "submittedOnDailyAt": "2025-05-29T02:20:00.209Z",
      "title": "Rico: Para mejorar la precisión y completitud de la recopilación de imágenes, se utiliza la reconstrucción visual.",
      "submittedOnDailyBy": {
        "_id": "622842e296588dd1a2594746",
        "avatarUrl": "/avatars/b96d0b49e4cff25d83fefc67b7cd1076.svg",
        "isPro": false,
        "fullname": "wangyuchi",
        "user": "YuchiWang",
        "type": "user"
      },
      "summary": "El ricapiting de imágenes se utiliza ampliamente para mejorar la calidad en diversos modelos y tareas. Actualmente, los métodos de ricapiting dependen generalmente de modelos de lenguaje multimodal (MLLM) fuertes para mejorar el contexto, pero son insuficientes en detalles mínimos y detallados, lo que provoca incertidumbres y incompletas. Para resolver estos limitaciones, proponemos un nuevo marco de trabajo para mejorar el ricapiting basado en la reconstrucción de imágenes. Específicamente, utilizamos modelos de contexto para reconstruir la imagen del contexto, y reconocemos las diferencias entre la imagen original y la reconstruida mediante un MLLM para mejorar el ricapiting. Este proceso se realiza de manera iterativa y ayuda a crear descripciones fidedignas y detalladas. Para reducir los costos computacionales adicionales, introducimos un ricapoche-Flash que aprende a generar captiones como ricapoche, utilizando DPO. Los experimentos extendidos muestran que nuestro enfoque mejora significativamente la precisión y la detallade del ricapiting, mejorando en al menos un 10% en los modelos de referencia en CapsBench y CompreCap. El código está disponible en la siguiente URL.\nhttps://github.com/wangyuchi369/RICO",
      "upvotes": 5,
      "discussionId": "6837d79e4d9866c160d8f471",
      "githubRepo": "https://github.com/wangyuchi369/RICO",
      "ai_summary": "A novel iterative framework, RICO, improves image caption accuracy by using visual reconstruction and a text-to-image model to refine discrepancies, while RICO-Flash enhances efficiency using DPO.",
      "ai_keywords": [
        "multimodal large language models",
        "text-to-image model",
        "DPO",
        "CapsBench",
        "CompreCap"
      ]
    },
    "publishedAt": "2025-05-28T13:29:34.000Z",
    "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via\n  Visual Reconstruction",
    "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22613.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622842e296588dd1a2594746",
      "avatarUrl": "/avatars/b96d0b49e4cff25d83fefc67b7cd1076.svg",
      "fullname": "wangyuchi",
      "name": "YuchiWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22525",
      "authors": [
        {
          "_id": "6837da438f680552f7b86b28",
          "user": {
            "_id": "64bb5f9d8e051085bace4d1e",
            "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
            "isPro": true,
            "fullname": "Ethan Chern",
            "user": "ethanchern",
            "type": "user"
          },
          "name": "Ethan Chern",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:05.613Z",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b29",
          "name": "Zhulin Hu",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2a",
          "name": "Steffi Chern",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2b",
          "name": "Siqi Kou",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2c",
          "name": "Jiadi Su",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2d",
          "name": "Yan Ma",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2e",
          "name": "Zhijie Deng",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2f",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T16:12:45.000Z",
      "submittedOnDailyAt": "2025-05-29T02:24:51.876Z",
      "title": "Creación de imágenes para pensar",
      "submittedOnDailyBy": {
        "_id": "64bb5f9d8e051085bace4d1e",
        "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
        "isPro": true,
        "fullname": "Ethan Chern",
        "user": "ethanchern",
        "type": "user"
      },
      "summary": "Presentamos un nuevo paradigma llamado \"Pensar con imágenes generadas\". Este paradigma implica un cambio fundamental en la interacción entre lógica visual y multimodalidades grandes (LMMs). Esto se logra cuando los modelos combinan texto y modalidades visuales de manera natural, generando de manera espontánea los procesos visuales intermedios. Actualmente, la lógica visual de los LMMs se limita a procesar imágenes fijas proporcionadas por el usuario o a seguir una cadena de razonamiento basada en texto (CoT). \"Pensar con imágenes generadas\" abre una nueva dimensión de capacidad cognitiva para los modelos, permitiendo que evalúen sus propias hipótesis visuales y mejoren su lógica principal. Probamos la eficacia de este enfoque mediante dos mecanismos complementarios: (1) generación visual basada en subobjetivos visuales intermedios, donde se desglosa y generan paso a paso componentes manejables para tratar tareas visuales complejas, y (2) generación visual basada en autoevaluación, donde se generan hipótesis visuales iniciales, se analizan sus deficiencias mediante una lógica textual y se mejoran los resultados según la evaluación propia. Los resultados de los benchmarks visuales muestran una mejora significativa frente a los métodos de referencia, alcanzando un aumento relativo del 50% (de 38% a 57%) en la tratamiento de escenarios multiobjetos complejos. Nuestro enfoque permite que modelos de IA participen en la imaginación visual humana, conocida por su creatividad, análisis y estrategia, y en la mejora iterativa, como en la exploración de nuevas estructuras proteicas, el diseño espacial de arquitectos, la reconstrucción de lugares criminales y la imaginación de juegos estratégicos de baloncesto. Disponible en https://github.com/GAIR-NLP/thinking-with-generated-images.",
      "upvotes": 5,
      "discussionId": "6837da468f680552f7b86bb2",
      "githubRepo": "https://github.com/GAIR-NLP/thinking-with-generated-images",
      "ai_summary": "Thinking with Generated Images allows large multimodal models to generate and critique intermediate visual steps, enhancing visual reasoning capabilities and achieving significant improvements in complex scenarios.",
      "ai_keywords": [
        "LMMs",
        "visual reasoning",
        "chain-of-thought",
        "vision generation",
        "intermediate visual subgoals",
        "self-critique",
        "multis-object scenarios",
        "biochemists",
        "architects",
        "forensic analysts",
        "basketball players"
      ]
    },
    "publishedAt": "2025-05-28T12:12:45.000Z",
    "title": "Thinking with Generated Images",
    "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22525.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64bb5f9d8e051085bace4d1e",
      "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
      "fullname": "Ethan Chern",
      "name": "ethanchern",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22523",
      "authors": [
        {
          "_id": "6837c1cb80fc90ca2d9e8153",
          "name": "Junwen Chen",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8154",
          "name": "Heyang Jiang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8155",
          "name": "Yanbin Wang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8156",
          "name": "Keming Wu",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8157",
          "name": "Ji Li",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8158",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8159",
          "name": "Keiji Yanai",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e815a",
          "name": "Dong Chen",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e815b",
          "name": "Yuhui Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T16:09:33.000Z",
      "submittedOnDailyAt": "2025-05-29T00:40:52.849Z",
      "title": "PrismLayers: PrismLayers es un modelo generativo basado en datos de imagenes transparentes de alta calidad abiertos.",
      "submittedOnDailyBy": {
        "_id": "631f108bb45367a05fe74260",
        "avatarUrl": "/avatars/c20da6800b643728062712d9a2771648.svg",
        "isPro": false,
        "fullname": "Researcher",
        "user": "YuanYuhui",
        "type": "user"
      },
      "summary": "Generar imágenes de alta calidad y multi-transparentes a partir de textos de hinto permite un nuevo nivel de control creativo, permitiendo a los usuarios editar cada capa como si fueran salidas de texto de un modelo de lenguaje. Sin embargo, la escasez de grandes conjuntos de datos de alta calidad multi-transparentes ha retrasado el desarrollo de modelos de generación multi-capa frente a los modelos de texto a imagen tradicionales. En este artículo, hemos abordado esta desafío de manera siguiente: (i) publicamos el primer conjunto de datos PrismLayers (PrismLayersPro) abierto y de alta precisión, que incluye 200K (20K) imágenes multi-transparentes y alpha mattes precisos; (ii) introducimos una pipeline de síntesis sin entrenamiento que utiliza modelos de difusión disponibles para generar estas imágenes según sea necesario; (iii) proporcionamos un potente modelo de generación multi-capa ART+, que tiene un estilo de diseño comparable a los modelos de generación de texto a imagen modernos. Nuestras contribuciones tecnológicas clave son: LayerFLUX es adepto de generar imágenes de alta calidad y alpha mattes precisos, mientras que MultiLayerFLUX combina los resultados de LayerFLUX basados en etiquetados semánticos para crear imágenes completas. Para garantizar una mejor calidad, aplicamos un estricto proceso de filtrado para eliminar artefactos y inconsistencias semánticas, y luego seleccionamos manualmente. Se ajustan los modelos ART más avanzados a los datos sintetizados de PrismLayersPro para obtener ART+, que en un estudio de usuario de cabeza en cabeza con 60% de participantes supera al original ART y se parece en calidad visual a las imágenes generadas por FLUX.1-[dev]. Esperamos que nuestro trabajo contribuya a la construcción de un sólido dataset de imágenes multi-transparentes y fomente la investigación y aplicaciones necesarias para imágenes multi-capa precisas, editables y visualmente atractivas.",
      "upvotes": 4,
      "discussionId": "6837c1d180fc90ca2d9e82bc",
      "ai_summary": "The work introduces a dataset and model for generating high-quality, multi-layer transparent images using diffusion models and a novel synthesis pipeline.",
      "ai_keywords": [
        "PrismLayers",
        "diffusion models",
        "LayerFLUX",
        "MultiLayerFLUX",
        "alpha mattes",
        "semantic layout",
        "user study",
        "ART model",
        "FLUX.1-[dev]"
      ]
    },
    "publishedAt": "2025-05-28T12:09:33.000Z",
    "title": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image\n  Generative Models",
    "summary": "Generating high-quality, multi-layer transparent images from text prompts can\nunlock a new level of creative control, allowing users to edit each layer as\neffortlessly as editing text outputs from LLMs. However, the development of\nmulti-layer generative models lags behind that of conventional text-to-image\nmodels due to the absence of a large, high-quality corpus of multi-layer\ntransparent data. In this paper, we address this fundamental challenge by: (i)\nreleasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)\ndataset of 200K (20K) multilayer transparent images with accurate alpha mattes,\n(ii) introducing a trainingfree synthesis pipeline that generates such data on\ndemand using off-the-shelf diffusion models, and (iii) delivering a strong,\nopen-source multi-layer generation model, ART+, which matches the aesthetics of\nmodern text-to-image generation models. The key technical contributions\ninclude: LayerFLUX, which excels at generating high-quality single transparent\nlayers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple\nLayerFLUX outputs into complete images, guided by human-annotated semantic\nlayout. To ensure higher quality, we apply a rigorous filtering stage to remove\nartifacts and semantic mismatches, followed by human selection. Fine-tuning the\nstate-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which\noutperforms the original ART in 60% of head-to-head user study comparisons and\neven matches the visual quality of images generated by the FLUX.1-[dev] model.\nWe anticipate that our work will establish a solid dataset foundation for the\nmulti-layer transparent image generation task, enabling research and\napplications that require precise, editable, and visually compelling layered\nimagery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22523.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631f108bb45367a05fe74260",
      "avatarUrl": "/avatars/c20da6800b643728062712d9a2771648.svg",
      "fullname": "Researcher",
      "name": "YuanYuhui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22338",
      "authors": [
        {
          "_id": "6837c79576eac3fa930de19b",
          "name": "Hanyang Wang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19c",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19d",
          "name": "Chaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19e",
          "name": "Tianjun Mao",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19f",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a0",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a1",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a2",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T13:23:49.000Z",
      "submittedOnDailyAt": "2025-05-29T01:04:31.053Z",
      "title": "Text2Grad: Aprendizaje por Refuerzo con Feedback de Lenguaje Nativo",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "La RLHF tradicional optimiza un modelo de lenguaje utilizando una recompensa en escala simple y oculta los motivos concretos de éxito o fracaso, además de que el aprendizaje se realiza de manera lenta. Recientemente, se han investigado métodos que agregan evaluaciones bibliográficas basadas en prompts y reflexiones al aprendizaje por refuerzo (RL), mejorando su interpretabilidad sin modificar los parámetros del modelo. En nuestro trabajo, proponemos un paradigma RL donde se transforma la retroalimentación lingüística en una pendiente espanósa para mejorar directamente las partes erróneas de la política del modelo. Así, la retroalimentación se vuelve precisa y el nodo global desaparece. Text2Grad está compuesto por tres componentes: (1) un sistema de comentarios de retroalimentación de alta calidad, (2) un modelo de recompensa detallado, y (3) un optimizador de política en escala espanósa. En resumen, en áreas como la generación de código, la resolución de problemas, Text2Grad proporciona mayor precisión en los métricas de tarea y una mayor interpretabilidad en comparación con RL basado en recompensas escalares simples o con RL que solo utiliza prompts. Nuestros resultados muestran que convertir retroalimentación en lenguaje en pendiente puede ser una fuerte señal para la optimización de políticas específicas. Nuestro código está disponible en https://github.com/microsoft/Text2Grad.",
      "upvotes": 4,
      "discussionId": "6837c79576eac3fa930de1dd",
      "ai_summary": "Text2Grad converts human textual feedback into span-level gradients to optimize language models precisely and efficiently.",
      "ai_keywords": [
        "RLHF",
        "reinforcement-learning",
        "free-form textual feedback",
        "span-level gradients",
        "token spans",
        "differentiable reward signals",
        "gradient updates",
        "span-level policy optimizer",
        "fine-grained reward model",
        "feedback-annotation pipeline"
      ]
    },
    "publishedAt": "2025-05-28T09:23:49.000Z",
    "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback",
    "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22203",
      "authors": [
        {
          "_id": "6837dcc41448b8bf0c91fa30",
          "user": {
            "_id": "6462def82a83863b97c0611e",
            "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
            "isPro": false,
            "fullname": "Yuzhen Huang",
            "user": "yuzhen17",
            "type": "user"
          },
          "name": "Yuzhen Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:53.737Z",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa31",
          "name": "Weihao Zeng",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa32",
          "name": "Xingshan Zeng",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa33",
          "name": "Qi Zhu",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa34",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T10:28:41.000Z",
      "submittedOnDailyAt": "2025-05-29T02:37:12.694Z",
      "title": "Reglas o modelo basado de datos de validación -- Estudio de caso en la inferencia matemática",
      "submittedOnDailyBy": {
        "_id": "6462def82a83863b97c0611e",
        "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
        "isPro": false,
        "fullname": "Yuzhen Huang",
        "user": "yuzhen17",
        "type": "user"
      },
      "summary": "Una validación de alta confianza es esencial para el éxito de la aprendizaje por refuerzo (Reinforcement Learning, RL), lo que es fundamental para modelos lógicos de gran escala como DeepSeek-R1. Este es uno de los métodos fundamentales en el desarrollo de modelos lógicos matemáticos. En los dominios complejos de los modelos lógicos matemáticos, los validadores basados en estándares han sido ampliamente utilizados para la entrenamiento de modelos lógicos fuertes. Sin embargo, la confianza en estos validadores y su impacto en el proceso de entrenamiento de aprendizaje por refuerzo no ha sido completamente comprendido. En este estudio, se trabaja con modelos lógicos matemáticos como casos de estudio y se analizan de manera detallada dos aspectos: la evaluación dinámica y el entrenamiento de aprendizaje por refuerzo con diferentes validadores. Primero, se observa que los validadores de estándar abierto tienen dificultades en reconocer respuestas equivalentes en diferentes formas en varios conjuntos de datos matemáticos, lo que lleva a la aparición de validadores visualmente altamente sensibles. Esta limitación tiene un impacto negativo en el rendimiento del entrenamiento de aprendizaje por refuerzo, y este efecto se vuelve más claro cuando el modelo de política se fortalece. Luego, se revisa la validación de versiones de modelos como posible solución. En la evaluación dinámica, los validadores de versiones de modelos logran una alta precisión de reconocimiento, pero son muy vulnerables a la malclasificación de patrones en el análisis de evolución y en los resultados del entrenamiento de aprendizaje por refuerzo. Esta vulnerabilidad puede aumentar artificialmente la recompensa en la optimización del modelo de política. Nuestros hallazgos se centran en los riesgos inherentes a los validadores de estándar y a los de versiones de modelos, y proporcionan consejos valiosos para el desarrollo de sistemas de recompensa en aprendizaje por refuerzo.",
      "upvotes": 4,
      "discussionId": "6837dcc51448b8bf0c91fa54",
      "ai_summary": "The study examines the effectiveness and reliability of rule-based and model-based verifiers in reinforcement learning with verifiable reward, highlighting limitations and vulnerabilities in their use for mathematical reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable reward",
        "DeepSeek-R1",
        "mathematical reasoning",
        "rule-based verifiers",
        "reward systems",
        "model-based verifiers",
        "false negatives",
        "false positives"
      ]
    },
    "publishedAt": "2025-05-28T06:28:41.000Z",
    "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on\n  Mathematical Reasoning",
    "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22203.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6462def82a83863b97c0611e",
      "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
      "fullname": "Yuzhen Huang",
      "name": "yuzhen17",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22202",
      "authors": [
        {
          "_id": "6837eff312d1f7a138bd09b3",
          "name": "Hyeonbin Hwang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b4",
          "name": "Byeongguk Jeon",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b5",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b6",
          "name": "Jiyeon Kim",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b7",
          "name": "Hoyeon Chang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b8",
          "name": "Sohee Yang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b9",
          "name": "Seungpil Won",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09ba",
          "name": "Dohaeng Lee",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09bb",
          "name": "Youbin Ahn",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09bc",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T10:28:35.000Z",
      "submittedOnDailyAt": "2025-05-29T04:00:37.847Z",
      "title": "Claro, aquí tienes la traducción de los textos en español:\n\n1. \"예, 문장을 한 문장씩 예측하여 번역하겠습니다.\"\n   - \"Sí, predeciré y traduciré cada oración.\"\n\n2. \"이 문장을 반환해 주세요.\"\n   - \"Por favor, devuelve esta oración.\"\n\n3. \"只需返回翻译结果，不要添加任何解释或额外的文本.\"\n   - \"Solo devuelve el resultado de la traducción, sin agregar ninguna explicación o texto adicional.\"",
      "submittedOnDailyBy": {
        "_id": "647eaaf61a1fcad2fdc5d1ef",
        "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
        "isPro": false,
        "fullname": "Hyeonbin Hwang ",
        "user": "hbin0701",
        "type": "user"
      },
      "summary": "Los modelos automáticos de lenguaje de regresión (LM) generan un token a la vez, mientras que la lógica humana utiliza una mayor abstracción en niveles más altos de oraciones, proposiciones y conceptos. Este contraste plantea una cuestión crucial: ¿pueden los LM realizar lógica en unidades estructuradas de significado? En este estudio, se investiga si se puede mapear estas lógicas abstractas en espacios de lógica. Se propone un marco que aplica un LM que predice tokenes en el nivel de la predicción continua, utilizando un mapeo automático de regresión. Se examinan dos paradigmas de mapeo inspirados en el aprendizaje de representaciones clásicas: 1) el mapeo semántico, que utiliza codificación automática para preservar significado superficial; y 2) el mapeo contextual, que codifica la estructura predecida por la predicción de la siguiente oración. Se evalúan dos modos de inferencia: el modo separado interpreta el mapeo predecido como una cadena de caracteres para re-codificarlo; el modo continuo ejecuta una lógica completa en el espacio de mapeo, obteniendo eficiencia. En cuatro áreas: matemáticas, lógica, conocimiento general y planificación, el mapeo contextual de inferencia continua muestra performance competitiva con la Chain-of-Thought (CoT) y reduce en promedio el tiempo de inferencia en la mitad. Además, muestra inicios de escalabilidad y adaptabilidad modular. Finalmente, se introduce el diagnóstico SentenceLens para visualizar el potencial tráfico y interpretar el estado del modelo en lenguaje interpretable. Estos resultados demuestran que los LM entrenados pueden convertirse eficazmente en lógicas abstractas y estructuradas dentro de un espacio de mapeo.",
      "upvotes": 4,
      "discussionId": "6837eff412d1f7a138bd0a3e",
      "ai_summary": "Pretrained language models can adapt to operate in sentence space, reason over structured semantic units, and achieve competitive performance with Chain-of-Thought while reducing inference-time FLOPs.",
      "ai_keywords": [
        "autoregressive language models",
        "semantic embeddings",
        "contextual embeddings",
        "next-sentence prediction",
        "Chain-of-Thought",
        "SentenceLens"
      ]
    },
    "publishedAt": "2025-05-28T06:28:35.000Z",
    "title": "Let's Predict Sentence by Sentence",
    "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22202.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647eaaf61a1fcad2fdc5d1ef",
      "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
      "fullname": "Hyeonbin Hwang ",
      "name": "hbin0701",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21876",
      "authors": [
        {
          "_id": "6837d80bf42b2aacfc26c460",
          "name": "Zun Wang",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c461",
          "name": "Jaemin Cho",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c462",
          "name": "Jialu Li",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c463",
          "name": "Han Lin",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c464",
          "user": {
            "_id": "652066649004117947e46ed6",
            "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg",
            "isPro": false,
            "fullname": "Jaehong Yoon",
            "user": "jaehong31",
            "type": "user"
          },
          "name": "Jaehong Yoon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:07.590Z",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c465",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c466",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T01:45:26.000Z",
      "submittedOnDailyAt": "2025-05-29T02:15:48.194Z",
      "title": "EPiC: Aprovechar las pautas de control de video para entrenar un control eficiente de la cámara de video",
      "submittedOnDailyBy": {
        "_id": "5ffe32d8942cf3533d364449",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
        "isPro": false,
        "fullname": "Jaemin Cho",
        "user": "j-min",
        "type": "user"
      },
      "summary": "Recientemente, los métodos de control de cámaras 3D en modelos de video depianza (VDM) generan videos anticuados a partir de puntos estimados según trayectorias de cámaras marcadas, y muchas veces guian el modelo de depianza con líneas estructuradas. Sin embargo, los errores inherentes en la estimación de puntos generan incertidumbres en los videos anticuados. Además, el necesario análisis de trayectorias de cámaras marcadas aumenta las demandas de recursos. Para resolver estos limitaciones, presentamos un marco de aprendizaje de control de cámaras eficiente y preciso, EPiC, que construye videos anticuados de alta calidad automáticamente sin necesidad de marcar trayectorias de cámaras costosas. Específicamente, se basa en el primer frame de video para aplicar un máscara a la fuente de video, generando videos anticuados de alta calidad. Este enfoque asegura alta alineación y permite la creación de pares de entrenamiento (I2V) de videos sin necesidad de marcar trayectorias de cámaras. Además, presentamos un módulo condicional ligero Anchor-ControlNet, que integra guías de video anticuados en un modelo de depianza preentrenado, utilizando menos del 1% de los parámetros del modelo principal. Este marco no requiere modificaciones en el modelo principal de depianza general, reduciendo significativamente los parámetros, etapas de entrenamiento y cantidad de datos necesarios, lo que permite entrenamiento eficiente. Por lo tanto, EPiC logra los mejores resultados en tareas de control de cámaras I2V, como RealEstate10K y MiraData, demostrando capacidades precisas, robustas y sólidas en el control de cámaras. En particular, EPiC generaliza fuertemente los videos anticuados generados a partir de puntos, permitiendo control de cámaras basado en información 3D precisa y mostrando una fuerte generalización de 0 seed en escenarios de video a video.",
      "upvotes": 4,
      "discussionId": "6837d810f42b2aacfc26c5ec",
      "projectPage": "https://zunwang1.github.io/Epic",
      "githubRepo": "https://github.com/wz0919/EPiC",
      "ai_summary": "EPiC is a framework for efficient 3D camera control in video diffusion models that constructs high-quality anchor videos through first-frame visibility masking, integrates them using a lightweight ControlNet module, and achieves state-of-the-art performance on I2V tasks with minimal resources.",
      "ai_keywords": [
        "anchor videos",
        "point cloud estimation",
        "camera trajectories",
        "diffusion models",
        "first-frame visibility",
        "EPiC",
        "ControlNet",
        "I2V training pairs",
        "rendering misalignments",
        "RealEstate10K",
        "MiraData"
      ]
    },
    "publishedAt": "2025-05-27T21:45:26.000Z",
    "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video\n  Guidance",
    "summary": "Recent approaches on 3D camera control in video diffusion models (VDMs) often\ncreate anchor videos to guide diffusion models as a structured prior by\nrendering from estimated point clouds following annotated camera trajectories.\nHowever, errors inherent in point cloud estimation often lead to inaccurate\nanchor videos. Moreover, the requirement for extensive camera trajectory\nannotations further increases resource demands. To address these limitations,\nwe introduce EPiC, an efficient and precise camera control learning framework\nthat automatically constructs high-quality anchor videos without expensive\ncamera trajectory annotations. Concretely, we create highly precise anchor\nvideos for training by masking source videos based on first-frame visibility.\nThis approach ensures high alignment, eliminates the need for camera trajectory\nannotations, and thus can be readily applied to any in-the-wild video to\ngenerate image-to-video (I2V) training pairs. Furthermore, we introduce\nAnchor-ControlNet, a lightweight conditioning module that integrates anchor\nvideo guidance in visible regions to pretrained VDMs, with less than 1% of\nbackbone model parameters. By combining the proposed anchor video data and\nControlNet module, EPiC achieves efficient training with substantially fewer\nparameters, training steps, and less data, without requiring modifications to\nthe diffusion model backbone typically needed to mitigate rendering\nmisalignments. Although being trained on masking-based anchor videos, our\nmethod generalizes robustly to anchor videos made with point clouds during\ninference, enabling precise 3D-informed camera control. EPiC achieves SOTA\nperformance on RealEstate10K and MiraData for I2V camera control task,\ndemonstrating precise and robust camera control ability both quantitatively and\nqualitatively. Notably, EPiC also exhibits strong zero-shot generalization to\nvideo-to-video scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21876.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5ffe32d8942cf3533d364449",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
      "fullname": "Jaemin Cho",
      "name": "j-min",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18700",
      "authors": [
        {
          "_id": "6837cc29bbee677da741aba7",
          "name": "Chun Wang",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741aba8",
          "name": "Xiaoran Pan",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741aba9",
          "name": "Zihao Pan",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741abaa",
          "name": "Haofan Wang",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741abab",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T13:48:57.000Z",
      "submittedOnDailyAt": "2025-05-29T01:25:51.194Z",
      "title": "GRE Suite: Ajuste de un modelo de lenguaje visuolingüístico y fortalecimiento de la cadena de razonamiento para la inferencia geográfica basada en la estimación de ubicación.",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "El reciente desarrollo de modelos de lenguaje de radio visual (VLMs) ha demostrado excelentes resultados en tareas de lógica visual. Sin embargo, el reconocimiento de la posición presenta problemas característicos. Se necesita extraer diversas informaciones visuales de las imágenes y integrar con conocimientos del mundo exterior para realizar lógicas sistemáticas. Los enfoques actuales para el reconocimiento de la posición carecen de una fuerte estructura lógica y explicabilidad. Para resolver estas limitaciones, se propone un nuevo marco que extiende los VLMs a una cadena lógica estructurada. Este enfoque fortalece la inferencia de posición interpretable. El marco GRE (Geological Reasoning Evaluation) se desarrolla de manera sistemática en tres elementos principales: dataset, modelo y benchmark. Primero, se presenta GRE30K, un altamente calidad dataset de reconocimiento de posición y análisis de contexto diseñado para promover la investigación en análisis visual y contextual. Luego, se presenta el modelo GRE, que utiliza estrategias lógicas en varios niveles para inferir atributos del escenario, detalles de la posición y características semánticas, identificando áreas potenciales de posición con alta precisión y mejorando así la precisión del reconocimiento de la posición. Finalmente, se establece el Geo Reason Evaluation Benchmark (GREval-Bench), un marco de evaluación detallado para evaluar la capacidad de reconocimiento de la posición en escenarios urbanos, naturales y de marcas. Los resultados de los experimentos muestran que GRE supera significativamente los métodos actuales en todos los aspectos de la tarea de reconocimiento de la posición. Se destaca el efecto de VLMs explicables en la inferencia compleja de la posición. El código y los datos están disponibles en https://github.com/Thorin215/GRE.",
      "upvotes": 3,
      "discussionId": "6837cc2abbee677da741abf5",
      "ai_summary": "The GRE Suite enhances Visual Language Models with structured reasoning chains, improving geo-localization tasks through a multi-stage strategy and comprehensive evaluation benchmark.",
      "ai_keywords": [
        "Visual Language Models",
        "geo-localization",
        "reasoning chains",
        "GRE30K",
        "GRE model",
        "GREval-Bench",
        "multi-stage reasoning",
        "scene attributes",
        "local details",
        "semantic features",
        "coarse-grained localization",
        "fine-grained localization"
      ]
    },
    "publishedAt": "2025-05-24T09:48:57.000Z",
    "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language\n  Models and Enhanced Reasoning Chains",
    "summary": "Recent advances in Visual Language Models (VLMs) have demonstrated\nexceptional performance in visual reasoning tasks. However, geo-localization\npresents unique challenges, requiring the extraction of multigranular visual\ncues from images and their integration with external world knowledge for\nsystematic reasoning. Current approaches to geo-localization tasks often lack\nrobust reasoning mechanisms and explainability, limiting their effectiveness.\nTo address these limitations, we propose the Geo Reason Enhancement (GRE)\nSuite, a novel framework that augments VLMs with structured reasoning chains\nfor accurate and interpretable location inference. The GRE Suite is\nsystematically developed across three key dimensions: dataset, model, and\nbenchmark. First, we introduce GRE30K, a high-quality geo-localization\nreasoning dataset designed to facilitate fine-grained visual and contextual\nanalysis. Next, we present the GRE model, which employs a multi-stage reasoning\nstrategy to progressively infer scene attributes, local details, and semantic\nfeatures, thereby narrowing down potential geographic regions with enhanced\nprecision. Finally, we construct the Geo Reason Evaluation Benchmark\n(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across\ndiverse urban, natural, and landmark scenes to measure both coarse-grained\n(e.g., country, continent) and fine-grained (e.g., city, street) localization\nperformance. Experimental results demonstrate that GRE significantly\noutperforms existing methods across all granularities of geo-localization\ntasks, underscoring the efficacy of reasoning-augmented VLMs in complex\ngeographic inference. Code and data will be released at\nhttps://github.com/Thorin215/GRE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18700.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17870",
      "authors": [
        {
          "_id": "6837f049023315653be65a88",
          "user": {
            "_id": "640f32f6ef5c6dcac8b094bd",
            "avatarUrl": "/avatars/89b95837666ad696fe1f10808e4619b0.svg",
            "isPro": false,
            "fullname": "Shaina Raza",
            "user": "Shainarazavi",
            "type": "user"
          },
          "name": "Shaina Raza",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T05:27:38.248Z",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a89",
          "name": "Rizwan Qureshi",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8a",
          "name": "Marcelo Lotif",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8b",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:40.928Z",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8c",
          "name": "Deval Pandya",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8d",
          "name": "Christos Emmanouilidis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T13:20:23.000Z",
      "submittedOnDailyAt": "2025-05-29T04:08:04.627Z",
      "title": "Los modelos también necesitan una calidad de vida similar a la humana: el modelo de la comunicación del pueblo elimina las mentiras.",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "Los modelos de IA generativa aprenden y recapitulan frecuentemente información falsa incluida en el conjunto de entrenamiento. Este artículo asume que, de manera similar a la inmunización biológica, el contacto controlado con virus debilitados contribuye a la construcción de la inmunidad, y presenta que los modelos de IA realizan ajustes básicos basándose en pequeños y separados conjuntos de hechos falsos etiquetados explícitamente, lo que se denomina como \"vacuna\". Estos hechos falsos se inyectan regularmente durante el ajuste, fortaleciendo la capacidad del modelo para mantener la precisión frente a los datos de entrada, reconocer y rechazar declaraciones erróneas. Un estudio de caso específico muestra que modelos inmunizados producen significativamente menos hechos falsos que los modelos de referencia. Según nuestra conocida, esto es un primer marco de entrenamiento, mostrando que los hechos falsos verificados se tratan como \"vacunas\" estándarizadas y que el modelo se fortalece con futuros hechos falsos sin depender de la deformación de los datos de entrada o de señales de retroalimentación humana general. Además, explicamos los estándares éticos y las controles de gestión para el uso seguro de datos falsos. La inmunización del modelo ofrece un marco anticipado para enfrentar la fidelidad de los sistemas de IA.",
      "upvotes": 3,
      "discussionId": "6837f04a023315653be65ac6",
      "ai_summary": "A generative AI model is fine-tuned with labeled falsehoods to reduce misinformation generation, analogous to biological immunization.",
      "ai_keywords": [
        "generative AI models",
        "misinformation",
        "fine-tuning",
        "labeled falsehoods",
        "immunization",
        "fact-checked falsehoods",
        "supervised vaccine"
      ]
    },
    "publishedAt": "2025-05-23T09:20:23.000Z",
    "title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat\n  Falsehoods",
    "summary": "Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21191",
      "authors": [
        {
          "_id": "6837eb38f09a146728a4b80f",
          "name": "Junyan Zhang",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b810",
          "name": "Yubo Gao",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b811",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b812",
          "name": "Jungang Li",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b813",
          "name": "Zhaorui Hou",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b814",
          "name": "Sicheng Tao",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b815",
          "name": "Shuliang Liu",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b816",
          "name": "Song Dai",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b817",
          "name": "Yonghua Hei",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b818",
          "name": "Junzhuo Li",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b819",
          "name": "Xuming Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T13:40:28.000Z",
      "submittedOnDailyAt": "2025-05-29T03:36:16.801Z",
      "title": "Instrucción-Respondent Neuron y Experts Publicados: Un Marco Analítico para Analizar la Capacidad de Seguimiento de Instrucciones en LLMs",
      "submittedOnDailyBy": {
        "_id": "64b76528fdb702b3d8641514",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/wCbEtpPEHRjS4sZSVK1gK.png",
        "isPro": false,
        "fullname": "Jungang Li",
        "user": "Jungang",
        "type": "user"
      },
      "summary": "La finejación de LLMs ha notablemente aumentado su capacidad para actuar según instrucciones, aunque la arquitectura computacional que impulsa esta mejora aún no se entiende completamente. En este estudio, se separan y analizan los componentes raras (neuronas o expertos de la arquitectura Mixture-of-Experts (MoE)) que están especializados en instrucciones, investigando sistemáticamente cómo se reconfiguran los cálculos en los LLMs. En particular, se presenta un conjunto de datos de instrucciones más bien ajustado que incluye seis categorías diferentes llamado HexaInst, y se propone un nuevo marco de análisis llamado SPARCOM. Este marco incluye tres principales contribuciones: 1) métodos para identificar dichos componentes raras, 2) evaluación de su generalidad funcional y características únicas, y 3) comparación sistemática de sus cambios. Los experimentos muestran la importancia funcional y única de estos componentes durante la ejecución de instrucciones. Al descubrir la relación entre la finejación y la estructura de cálculo rara, se proporcionará una ingeniería más profunda para la internalización de los comportamientos de los LLMs según instrucciones, lo que permitirá ofrecer una comunidad de LLMs confiables.",
      "upvotes": 2,
      "discussionId": "6837eb39f09a146728a4b872",
      "ai_summary": "The study investigates the role of sparse computational components in the instruction-following capabilities of Large Language Models through systematic analysis and introduces HexaInst and SPARCOM for better understanding.",
      "ai_keywords": [
        "Large Language Models",
        "fine-tuning",
        "instruction-following",
        "HexaInst",
        "SPARCOM",
        "sparse components",
        "neurons",
        "Mixture-of-Experts",
        "instruction execution",
        "computational adaptations"
      ]
    },
    "publishedAt": "2025-05-27T09:40:28.000Z",
    "title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical\n  Framework for LLM's Instruction-Following Capabilities",
    "summary": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21191.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "64b76528fdb702b3d8641514",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/wCbEtpPEHRjS4sZSVK1gK.png",
      "fullname": "Jungang Li",
      "name": "Jungang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17507",
      "authors": [
        {
          "_id": "6833f24ed5c438959f7decf9",
          "user": {
            "_id": "619ef3f253061ce00477b09e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
            "isPro": false,
            "fullname": "Qiaosheng Chen",
            "user": "cqsss",
            "type": "user"
          },
          "name": "Qiaosheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T09:01:00.198Z",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfa",
          "name": "Kaijia Huang",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfb",
          "name": "Xiao Zhou",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfc",
          "name": "Weiqing Luo",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfd",
          "name": "Yuanning Cui",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfe",
          "name": "Gong Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T06:00:20.000Z",
      "submittedOnDailyAt": "2025-05-29T00:53:38.693Z",
      "title": "Recomendación, Clasificación y Evaluación Basada en Puntos de Puntos de la Red de Házing Face",
      "submittedOnDailyBy": {
        "_id": "619ef3f253061ce00477b09e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
        "isPro": false,
        "fullname": "Qiaosheng Chen",
        "user": "cqsss",
        "type": "user"
      },
      "summary": "El rápido crecimiento de los recursos de aprendizaje automático (ML) abierto ha contribuido al acelerar la investigación en Inteligencia Relacionada (IR). Sin embargo, plataformas como Hugging Face no utilizan explícitamente representaciones estructuradas, limitando así funciones avanzadas de búsqueda y análisis como la seguimiento de la evolución de los modelos y la recomendación de conjuntos de datos relacionados. En este espacio, se ha construido el primer gran grafo de conocimiento, HuggingKG, desarrollado por la comunidad de Hugging Face. Este grafo tiene 2,6 millones de nodos y 6,2 millones de aristas, y explora relaciones propias de cada dominio y características léxicas ricas. Esto permite el desarrollo de un benchmark multitarea llamado HuggingBench, que incluye tareas como recomendación de recursos, clasificación de clases y seguimiento de la evolución de los modelos. Nuestros experimentos revelan las características de HuggingKG y de los tareas generadas a partir de ella. Ambos recursos están disponibles públicamente, lo que esperamos que fomente el desarrollo de la investigación en la comparación y gestión de recursos abiertos.",
      "upvotes": 2,
      "discussionId": "6833f24ed5c438959f7ded31",
      "githubRepo": "https://github.com/nju-websoft/HuggingBench",
      "ai_summary": "HuggingKG, a large-scale knowledge graph, enhances open source ML resource management by enabling advanced queries and analyses via HuggingBench.",
      "ai_keywords": [
        "knowledge graph",
        "resource recommendation",
        "classification",
        "tracing",
        "multi-task benchmark"
      ]
    },
    "publishedAt": "2025-05-23T02:00:20.000Z",
    "title": "Benchmarking Recommendation, Classification, and Tracing Based on\n  Hugging Face Knowledge Graph",
    "summary": "The rapid growth of open source machine learning (ML) resources, such as\nmodels and datasets, has accelerated IR research. However, existing platforms\nlike Hugging Face do not explicitly utilize structured representations,\nlimiting advanced queries and analyses such as tracing model evolution and\nrecommending relevant datasets. To fill the gap, we construct HuggingKG, the\nfirst large-scale knowledge graph built from the Hugging Face community for ML\nresource management. With 2.6 million nodes and 6.2 million edges, HuggingKG\ncaptures domain-specific relations and rich textual attributes. It enables us\nto further present HuggingBench, a multi-task benchmark with three novel test\ncollections for IR tasks including resource recommendation, classification, and\ntracing. Our experiments reveal unique characteristics of HuggingKG and the\nderived tasks. Both resources are publicly available, expected to advance\nresearch in open source resource sharing and management.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619ef3f253061ce00477b09e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
      "fullname": "Qiaosheng Chen",
      "name": "cqsss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15813",
      "authors": [
        {
          "_id": "68380974717461677df17514",
          "name": "Muquan Yu",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17515",
          "name": "Mu Nan",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17516",
          "name": "Hossein Adeli",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17517",
          "name": "Jacob S. Prince",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17518",
          "name": "John A. Pyles",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17519",
          "name": "Leila Wehbe",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751a",
          "name": "Margaret M. Henderson",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751b",
          "name": "Michael J. Tarr",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751c",
          "user": {
            "_id": "64b6ce23dbbd1f2cdb624d56",
            "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
            "isPro": false,
            "fullname": "Andrew Luo",
            "user": "aluo-x",
            "type": "user"
          },
          "name": "Andrew F. Luo",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-29T07:16:44.495Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b6ce23dbbd1f2cdb624d56/vCXz4Xv4AgHNNIC1S8aYf.png"
      ],
      "publishedAt": "2025-05-21T17:59:41.000Z",
      "submittedOnDailyAt": "2025-05-29T05:45:43.922Z",
      "title": "Aprendizaje de meta para el modelo de transformador de contexto de la visión de alto nivel humana",
      "submittedOnDailyBy": {
        "_id": "64b6ce23dbbd1f2cdb624d56",
        "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
        "isPro": false,
        "fullname": "Andrew Luo",
        "user": "aluo-x",
        "type": "user"
      },
      "summary": "Los red neuronales preentrenados en conjuntos de datos de fin de entrenamiento tienen momentos que sorprenden a la coincidencia expresiva con las respuestas neuronales humanas, pero el modelado de modelos de cálculo de imágenes basados en patrones visuales requiere de grandes conjuntos de datos de fMRI a cada nivel. Estos datos son costosos y tiempo consumidos, lo que limita la capacidad de generalización del encoder frente a nuevos participantes o estímulos. BraInCoRL no realiza ajustes adicionales para nuevos participantes o estímulos, sino que utiliza aprendizaje explicativo para predecir las reacciones neuronales de cada celda. Utilizando la arquitectura de Transformer, BraInCoRL asigna condiciones que pueden ser adaptadas a la varianza de los estímulos visuales, permitiendo el aprendizaje de un sesgo de índices para múltiples participantes. Durante el período de entrenamiento, BraInCoRL optimiza el modelo específicamente para el aprendizaje explicativo, generando directamente modelos de las activaciones visuales de cada celda bajo condiciones de características de la imagen y activación de las celdas, lo que permite un mejor rendimiento. BraInCoRL puede ajustar el diseño del encoder a las celdas para nuevas imágenes completamente diferentes, mostrando un sesgo de escalación fuerte en las pruebas. El modelo puede generalizar completamente nuevos conjuntos de datos de fMRI visuales utilizando parámetros de fMRI y participantes diferentes. Además, BraInCoRL promueve la interpretabilidad de los señales neuronales de la percepción visual. Finalmente, BraInCoRL permite la mapeo explicativo de la selección de celdas a partir de preguntas naturales.",
      "upvotes": 2,
      "discussionId": "68380977717461677df17638",
      "ai_summary": "BraInCoRL employs a transformer-based in-context learning approach to model higher visual cortex neural responses with few-shot examples, demonstrating superior performance and generalizability across new subjects, stimuli, and datasets.",
      "ai_keywords": [
        "functional representations",
        "higher visual cortex",
        "artificial neural networks",
        "fMRI datasets",
        "in-context learning",
        "transformer architecture",
        "inductive bias",
        "voxelwise neural responses",
        "image features",
        "voxel activations",
        "test-time scaling",
        "interpretability",
        "natural language queries",
        "voxel selectivity"
      ]
    },
    "publishedAt": "2025-05-21T13:59:41.000Z",
    "title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual\n  Cortex",
    "summary": "Understanding functional representations within higher visual cortex is a\nfundamental question in computational neuroscience. While artificial neural\nnetworks pretrained on large-scale datasets exhibit striking representational\nalignment with human neural responses, learning image-computable models of\nvisual cortex relies on individual-level, large-scale fMRI datasets. The\nnecessity for expensive, time-intensive, and often impractical data acquisition\nlimits the generalizability of encoders to new subjects and stimuli. BraInCoRL\nuses in-context learning to predict voxelwise neural responses from few-shot\nexamples without any additional finetuning for novel subjects and stimuli. We\nleverage a transformer architecture that can flexibly condition on a variable\nnumber of in-context image stimuli, learning an inductive bias over multiple\nsubjects. During training, we explicitly optimize the model for in-context\nlearning. By jointly conditioning on image features and voxel activations, our\nmodel learns to directly generate better performing voxelwise models of higher\nvisual cortex. We demonstrate that BraInCoRL consistently outperforms existing\nvoxelwise encoder designs in a low-data regime when evaluated on entirely novel\nimages, while also exhibiting strong test-time scaling behavior. The model also\ngeneralizes to an entirely new visual fMRI dataset, which uses different\nsubjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates\nbetter interpretability of neural signals in higher visual cortex by attending\nto semantically relevant stimuli. Finally, we show that our framework enables\ninterpretable mappings from natural language queries to voxel selectivity.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b6ce23dbbd1f2cdb624d56/vCXz4Xv4AgHNNIC1S8aYf.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b6ce23dbbd1f2cdb624d56",
      "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
      "fullname": "Andrew Luo",
      "name": "aluo-x",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12667",
      "authors": [
        {
          "_id": "682dd41740c6417d995087de",
          "user": {
            "_id": "648dca31385b84261811505d",
            "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
            "isPro": false,
            "fullname": "Zihan Su",
            "user": "Sugewud",
            "type": "user"
          },
          "name": "Zihan Su",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-28T16:17:54.970Z",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087df",
          "name": "Xuerui Qiu",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e0",
          "name": "Hongbin Xu",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e1",
          "name": "Tangyu Jiang",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e2",
          "name": "Junhao Zhuang",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e3",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e4",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e5",
          "name": "Shengfeng He",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e6",
          "name": "Fei Richard Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T03:31:31.000Z",
      "submittedOnDailyAt": "2025-05-29T01:30:41.124Z",
      "title": "Safe-Sora: Secuencia de videos generados a partir de texto utilizando un marcador gráfico seguro.",
      "submittedOnDailyBy": {
        "_id": "648dca31385b84261811505d",
        "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
        "isPro": false,
        "fullname": "Zihan Su",
        "user": "Sugewud",
        "type": "user"
      },
      "summary": "El crecimiento explosivo de los modelos de video generación ha llevado a un aumento en la necesidad de protección de derechos de autor confiable para contenido generado con AI. A pesar de la popularidad de la síntesis de imágenes, el marcado generable no ha sido ampliamente revisado en el contexto de la generación de vídeos. Para llenar esta brecha, proponemos Safe-Sora, que es el primer marcador gráfico directamente insertado en el proceso de generación de vídeos. Observamos la relación entre la similitud visual del marcado y su rendimiento basado en el contenido sobre-escrito y sobrescrito, introduciendo una estructura de ajuste adaptativo desde las heurísticas hasta los detalles. En particular, las imágenes de marca se dividen en patches y se asignan a los frames de vídeo visualmente más similares, y se localizan más espacialmente dentro de los mismos. Para permitir la funcionalidad espacio-temporal de los patches de marca entre frames de vídeo, desarrollamos una arquitectura de mapa de expansión de onda 3D, preparando una nueva estrategia de muestreo espacio-temporal local. Esto abre una nueva ruta para la protección de derechos de autor eficiente y robusta, modelando de manera efectiva dependencias a larga distancia. Esta es la primera intento de aplicar modelos de estado a marcadores, abriéndose nuevas vías para protección de derechos de autor eficiente y robusta. Los experimentos detallados lograron los mejores rendimientos en calidad de vídeo, precisión y robustez del marcado, lo que contribuye significativamente a nuestra propuesta. Con la publicación, publicamos nuestro código.",
      "upvotes": 2,
      "discussionId": "682dd41840c6417d99508847",
      "projectPage": "https://sugewud.github.io/Safe-Sora-project/",
      "githubRepo": "https://github.com/Sugewud/Safe-Sora",
      "ai_summary": "Safe-Sora embeds invisible watermarks into AI-generated videos using a hierarchical adaptive matching mechanism and a 3D wavelet transform-enhanced Mamba architecture, achieving top performance in video quality, watermark fidelity, and robustness.",
      "ai_keywords": [
        "generative watermarking",
        "hierarchical coarse-to-fine adaptive matching",
        "3D wavelet transform",
        "Mamba architecture",
        "spatiotemporal local scanning",
        "state space models",
        "watermark embedding",
        "watermark retrieval"
      ]
    },
    "publishedAt": "2025-05-18T23:31:31.000Z",
    "title": "Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking",
    "summary": "The explosive growth of generative video models has amplified the demand for\nreliable copyright preservation of AI-generated content. Despite its popularity\nin image synthesis, invisible generative watermarking remains largely\nunderexplored in video generation. To address this gap, we propose Safe-Sora,\nthe first framework to embed graphical watermarks directly into the video\ngeneration process. Motivated by the observation that watermarking performance\nis closely tied to the visual similarity between the watermark and cover\ncontent, we introduce a hierarchical coarse-to-fine adaptive matching\nmechanism. Specifically, the watermark image is divided into patches, each\nassigned to the most visually similar video frame, and further localized to the\noptimal spatial region for seamless embedding. To enable spatiotemporal fusion\nof watermark patches across video frames, we develop a 3D wavelet\ntransform-enhanced Mamba architecture with a novel spatiotemporal local\nscanning strategy, effectively modeling long-range dependencies during\nwatermark embedding and retrieval. To the best of our knowledge, this is the\nfirst attempt to apply state space models to watermarking, opening new avenues\nfor efficient and robust watermark protection. Extensive experiments\ndemonstrate that Safe-Sora achieves state-of-the-art performance in terms of\nvideo quality, watermark fidelity, and robustness, which is largely attributed\nto our proposals. We will release our code upon publication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12667.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648dca31385b84261811505d",
      "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
      "fullname": "Zihan Su",
      "name": "Sugewud",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22645",
      "authors": [
        {
          "_id": "6837dbed1233747046da00f5",
          "name": "Hanjia Lyu",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f6",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f7",
          "name": "Jian Kang",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f8",
          "name": "Allison Koenecke",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:56:49.000Z",
      "submittedOnDailyAt": "2025-05-29T02:33:57.876Z",
      "title": "Bayasian's Characterization: Benchmark for Large-Scale Language Models in Classical Chinese and Traditional Chinese",
      "submittedOnDailyBy": {
        "_id": "64c939307dba66c3a7e4d215",
        "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
        "isPro": false,
        "fullname": "BruceLyu",
        "user": "brucelyu",
        "type": "user"
      },
      "summary": "La capacidad de los modelos de lenguaje de grandes tamaños (LLMs) ha sido estudiada tanto para el katakana simple como para el katakana estándar, pero aún no se sabe si estos modelos presentan diferentes desempeños en ambos idiomas escritos. Esta comprensión es crucial. La diferencia en la calidad de las respuestas de los LLMs puede perjudicar decisiones en el uso de estos modelos en áreas como educación o empleo, continuando el daño simbólico que se inflige a las culturas asociadas a estos idiomas escritos y ampliando las pérdidas en las comunidades afectadas. Para investigar estas diferencias, hemos diseñado dos tareas de evaluación que reflejan escenarios reales: la elección de términos locales (encourejando a los LLMs a denominar diferentes cosas en katakana simple y katakana estándar) y la elección de nombres locales (encourejando a los LLMs a seleccionar a personas para empleo a partir de listas de nombres en katakana simple y katakana estándar). En ambas tareas, evaluamos el desempeño de 11 servicios comerciales avanzados de LLMs y modelos abierto-código. Estos modelos se han entrenado principalmente en inglés, katakana simple o katakana estándar. Nuestro análisis muestra que las respuestas de los LLMs están sujetas a sesgos que dependen del contexto y del idioma escrito. En la tarea de elección de términos locales, la mayoría de los LLMs prefirieron respuestas en katakana simple, mientras que en la tarea de elección de nombres locales, preferieron nombres en katakana estándar. Estas diferencias pueden surgir debido a la representación en los datos de entrenamiento, la preferencia del idioma escrito y las diferencias en el tokenizado de katakana simple y katakana estándar. Estos hallazgos demuestran la necesidad de un análisis más profundo sobre los sesgos de los LLMs, y proporcionamos un conjunto de datos abierto-código para evaluar el comportamiento de los LLMs en variantes de chino, lo que promete impulsar la investigación en este campo. (https://github.com/brucelyu17/SC-TC-Bench)",
      "upvotes": 1,
      "discussionId": "6837dbee1233747046da0125",
      "githubRepo": "https://github.com/brucelyu17/SC-TC-Bench",
      "ai_summary": "Research examines LLM performance biases between Simplified and Traditional Chinese in regional term and name choice tasks, attributing differences to training data and tokenization.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "LLM-facilitated decision-making",
        "regional term choice",
        "regional name choice",
        "open-sourced benchmark dataset",
        "SC-TC-Bench"
      ]
    },
    "publishedAt": "2025-05-28T13:56:49.000Z",
    "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese",
    "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22645.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c939307dba66c3a7e4d215",
      "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
      "fullname": "BruceLyu",
      "name": "brucelyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21960",
      "authors": [
        {
          "_id": "6837dd74ec10479b9605da15",
          "user": {
            "_id": "637e1cf4f09bf2498c543a73",
            "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
            "isPro": false,
            "fullname": "Senmao Li",
            "user": "senmaonk",
            "type": "user"
          },
          "name": "Senmao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:51.659Z",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da16",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da17",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da18",
          "name": "Tao Liu",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da19",
          "name": "Jiehang Xie",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1a",
          "name": "Joost van de Weijer",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1b",
          "name": "Fahad Shahbaz Khan",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1c",
          "name": "Shiqi Yang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1d",
          "name": "Yaxing Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1e",
          "name": "Jian Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T04:23:22.000Z",
      "submittedOnDailyAt": "2025-05-29T02:44:09.323Z",
      "title": "Título: Ticket de Origen: Aceptación del Modelo de Difusión de Texto a Imágenes Usando un Encoder Unificado Independiente del Tiempo\n\nResumen: Utiliza un modelo de difusión para transformar texto en imágenes, donde el encoder es unificado y no depende del tiempo.",
      "submittedOnDailyBy": {
        "_id": "637e1cf4f09bf2498c543a73",
        "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
        "isPro": false,
        "fullname": "Senmao Li",
        "user": "senmaonk",
        "type": "user"
      },
      "summary": "El modelo de difusión para texto a imagen (T2I) ha logrado una sorprendente revolución en el modelado generativo, pero existe un equilibrio entre la velocidad de inferencia y la calidad de la imagen, así como problemas en la procesamiento eficiente de la máquina. Los modelos T2I combinados existentes pueden generar altas calidades de imagen en pocos pasos de muestreo, pero presentan debilidades en diversidad y calidad, especialmente en modelos de un solo paso. En nuestro análisis, hemos confirmado que hay cálculos innecesarios en el encoder UNet. Nuestro hallazgo muestra que el decoder en modelos de difusión T2I captura una amplia y clara información de significado, mientras que el encoder puede compartirse entre diferentes fases temporales y entre diferentes decoders. Basándonos en estos hallazgos, introducimos el primer encoder temporalmente independiente sin bucle (TiUE) en la arquitectura UNet como modelo estudiante. TiUE adopta un enfoque de generación de imagenes sin bucle y utiliza una red neuronal en un solo paso para la combinación de la difusión modelo, permitiendo compartir características del encoder en múltiples fases temporales y haciendo posible la muestreo paralelo, lo que significa una reducción significativa en la complejidad de tiempo de inferencia. Además, añadimos una término de divergencia de Kullback-Leibler para normalizar la predicción de ruido y mejorar la realismo visual y la diversidad de las imágenes generadas. Los resultados de las pruebas muestran que TiUE supera a los mejores resultados, generando resultados más diversos y realistas, mientras mantiene una eficiencia computacional.",
      "upvotes": 1,
      "discussionId": "6837dd78ec10479b9605db06",
      "githubRepo": "https://github.com/sen-mao/Loopfree",
      "ai_summary": "Time-independent Unified Encoder TiUE reduces inference time and improves diversity and quality in Text-to-Image diffusion models by sharing encoder features across decoder time steps.",
      "ai_keywords": [
        "Text-to-Image diffusion models",
        "inference speed",
        "image quality",
        "distilled T2I models",
        "UNet encoders",
        "decoders",
        "semantic information",
        "Time-independent Unified Encoder TiUE",
        "loop-free image generation",
        "one-pass scheme",
        "parallel sampling",
        "KL divergence",
        "perceptual realism",
        "LCM",
        "SD-Turbo",
        "SwiftBrushv2"
      ]
    },
    "publishedAt": "2025-05-28T00:23:22.000Z",
    "title": "One-Way Ticket:Time-Independent Unified Encoder for Distilling\n  Text-to-Image Diffusion Models",
    "summary": "Text-to-Image (T2I) diffusion models have made remarkable advancements in\ngenerative modeling; however, they face a trade-off between inference speed and\nimage quality, posing challenges for efficient deployment. Existing distilled\nT2I models can generate high-fidelity images with fewer sampling steps, but\noften struggle with diversity and quality, especially in one-step models. From\nour analysis, we observe redundant computations in the UNet encoders. Our\nfindings suggest that, for T2I diffusion models, decoders are more adept at\ncapturing richer and more explicit semantic information, while encoders can be\neffectively shared across decoders from diverse time steps. Based on these\nobservations, we introduce the first Time-independent Unified Encoder TiUE for\nthe student model UNet architecture, which is a loop-free image generation\napproach for distilling T2I diffusion models. Using a one-pass scheme, TiUE\nshares encoder features across multiple decoder time steps, enabling parallel\nsampling and significantly reducing inference time complexity. In addition, we\nincorporate a KL divergence term to regularize noise prediction, which enhances\nthe perceptual realism and diversity of the generated images. Experimental\nresults demonstrate that TiUE outperforms state-of-the-art methods, including\nLCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results\nwhile maintaining the computational efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21960.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637e1cf4f09bf2498c543a73",
      "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
      "fullname": "Senmao Li",
      "name": "senmaonk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20715",
      "authors": [
        {
          "_id": "6837eef0e237f02cd3c87963",
          "name": "Fuwen Luo",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87964",
          "name": "Shengfeng Lou",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87965",
          "name": "Chi Chen",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87966",
          "name": "Ziyue Wang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87967",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87968",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87969",
          "name": "Jiyue Guo",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796a",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796b",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796c",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796d",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796e",
          "name": "Yang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T04:50:07.000Z",
      "submittedOnDailyAt": "2025-05-29T03:57:26.118Z",
      "title": "MUSEG: Interés por las marcas de tiempo a través de la diferencia de segmentos múltiples para fortalecer la comprensión temporal del video.",
      "submittedOnDailyBy": {
        "_id": "642086ed290342c5df85662d",
        "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
        "isPro": false,
        "fullname": "Chi Chen",
        "user": "carboncoo",
        "type": "user"
      },
      "summary": "La comprensión de la secuencia temporal es crucial para explicar eventos dentro de imágenes en modelos multimodales de lenguaje (MLLMs). A pesar del progreso reciente en la comprensión de imágenes, los actuales MLLMs tienen dificultades con causas temporales subtil. El aprendizaje por refuerzo (RL) ha sido explorado recientemente para resolver este problema, pero los métodos actuales de RL tienen limitados efectos. En este estudio, se propone un nuevo enfoque basado en RL llamado MUSEG, que introduce un flujo de flujo de grafos multiniveles relacionados con los pasos de tiempo para mejorar la comprensión temporal. MUSEG promueve una explicación más detallada de las causas temporales mediante la agrupación de imágenes relacionadas y preguntas. Para fomentar el aprendizaje efectivo, se proporcionan recompensas por flujo de grafos temporales de manera estadística, y se diseña un enfoque de aprendizaje de RL adaptativo. Se han realizado amplias pruebas en flujos de grafos temporales y tareas de preguntas y respuestas de imágenes temporales, demostrando que MUSEG es significativamente superior a los métodos actuales y muestra una buena generalización en diversos escenarios de comprensión temporal. Este proyecto se puede ver en https://github.com/THUNLP-MT/MUSEG.",
      "upvotes": 1,
      "discussionId": "6837eef1e237f02cd3c8798d",
      "githubRepo": "https://github.com/THUNLP-MT/MUSEG",
      "ai_summary": "MUSEG, an RL-based method with timestamp-aware multi-segment grounding, significantly enhances the temporal understanding of large language models by improving alignment with video segments and demonstrating superior performance in temporal reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "timestamp-aware multi-segment grounding",
        "temporal understanding",
        "MUSEG",
        "phased rewards",
        "temporal grounding",
        "time-sensitive video QA"
      ]
    },
    "publishedAt": "2025-05-27T00:50:07.000Z",
    "title": "MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware\n  Multi-Segment Grounding",
    "summary": "Video temporal understanding is crucial for multimodal large language models\n(MLLMs) to reason over events in videos. Despite recent advances in general\nvideo understanding, current MLLMs still struggle with fine-grained temporal\nreasoning. While reinforcement learning (RL) has been explored to address this\nissue recently, existing RL approaches remain limited in effectiveness. In this\nwork, we propose MUSEG, a novel RL-based method that enhances temporal\nunderstanding by introducing timestamp-aware multi-segment grounding. MUSEG\nenables MLLMs to align queries with multiple relevant video segments, promoting\nmore comprehensive temporal reasoning. To facilitate effective learning, we\ndesign a customized RL training recipe with phased rewards that progressively\nguides the model toward temporally grounded reasoning. Extensive experiments on\ntemporal grounding and time-sensitive video QA tasks demonstrate that MUSEG\nsignificantly outperforms existing methods and generalizes well across diverse\ntemporal understanding scenarios. View our project at\nhttps://github.com/THUNLP-MT/MUSEG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20715.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642086ed290342c5df85662d",
      "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
      "fullname": "Chi Chen",
      "name": "carboncoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21060",
      "authors": [
        {
          "_id": "683818841902f641cc669774",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "683818841902f641cc669775",
          "name": "Xiang Liu",
          "hidden": false
        },
        {
          "_id": "683818841902f641cc669776",
          "name": "Peidong Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T11:47:15.000Z",
      "submittedOnDailyAt": "2025-05-29T06:49:42.295Z",
      "title": "Este es el texto traducido al español:\n\n\"3R Style: Modelado de estilos 3D en tiempo real para cualquier escenario y estilo\"",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "Crear rápidamente 3D escenas estilizadas y mantener la coherencia multi-punto mientras que coincidan con la imagen estilizada es una tarea importante. Los métodos de estilizamiento 3D más avanzados actuales utilizan optimizaciones de tiempo de prueba con alto cálculo, y transmiten características artísticas a representaciones 3D entrenadas, lo que requiere imágenes de entrada con poses densas. En contraste, se presenta un nuevo enfoque que utiliza el desarrollo reciente de modelos de reconstrucción dirigida para realizar estilizamiento 3D directamente en menos de 1 segundo, utilizando imágenes de escenas de espacio de vista sin pose y imágenes de estilo arbitrario. Para resolver la conexión inherente entre reconstrucción y estilizamiento, se introduce una arquitectura de división que separa la modelación estructural y la tomografía de la superficie, evitando que las tendencias de estilizamiento deformen la estructura de la escena 3D. Además, se aplica un pérdida de identificación para promover el entrenamiento previo del modelo de estilizamiento a través de nuevas tareas de síntesis visual. Esta estrategia permite especializar el estilizamiento mientras mantiene la capacidad de reconstrucción original. Evaluaciones detalladas dentro y fuera del conjunto de datos muestran que nuestro enfoque mejora la integración de estilo y cara de la escena, así como la coherencia multi-punto y la eficiencia en comparación con los métodos actuales.",
      "upvotes": 0,
      "discussionId": "6838188b1902f641cc669947",
      "ai_summary": "A novel feed-forward model achieves fast 3D stylization using sparse view images, maintaining multi-view consistency and high-quality style transfer while retaining reconstruction accuracy.",
      "ai_keywords": [
        "feed-forward reconstruction models",
        "3D stylization",
        "dense posed input images",
        "unposed sparse-view scene images",
        "branched architecture",
        "structure modeling",
        "appearance shading",
        "identity loss",
        "novel view synthesis",
        "in-domain datasets",
        "out-of-domain datasets",
        "multi-view consistency"
      ]
    },
    "publishedAt": "2025-05-27T07:47:15.000Z",
    "title": "Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and\n  Styles",
    "summary": "Stylizing 3D scenes instantly while maintaining multi-view consistency and\nfaithfully resembling a style image remains a significant challenge. Current\nstate-of-the-art 3D stylization methods typically involve computationally\nintensive test-time optimization to transfer artistic features into a\npretrained 3D representation, often requiring dense posed input images. In\ncontrast, leveraging recent advances in feed-forward reconstruction models, we\ndemonstrate a novel approach to achieve direct 3D stylization in less than a\nsecond using unposed sparse-view scene images and an arbitrary style image. To\naddress the inherent decoupling between reconstruction and stylization, we\nintroduce a branched architecture that separates structure modeling and\nappearance shading, effectively preventing stylistic transfer from distorting\nthe underlying 3D scene structure. Furthermore, we adapt an identity loss to\nfacilitate pre-training our stylization model through the novel view synthesis\ntask. This strategy also allows our model to retain its original reconstruction\ncapabilities while being fine-tuned for stylization. Comprehensive evaluations,\nusing both in-domain and out-of-domain datasets, demonstrate that our approach\nproduces high-quality stylized 3D content that achieve a superior blend of\nstyle and scene appearance, while also outperforming existing methods in terms\nof multi-view consistency and efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 874
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21582",
      "authors": [
        {
          "_id": "68380c860fb1ddbe91ba0bf9",
          "user": {
            "_id": "631f5035c6b20f03c823c4ba",
            "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
            "isPro": false,
            "fullname": "Christopher Knievel",
            "user": "CKnievel",
            "type": "user"
          },
          "name": "Christopher Knievel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:12.986Z",
          "hidden": false
        },
        {
          "_id": "68380c860fb1ddbe91ba0bfa",
          "name": "Alexander Bernhardt",
          "hidden": false
        },
        {
          "_id": "68380c860fb1ddbe91ba0bfb",
          "name": "Christian Bernhardt",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T10:07:05.000Z",
      "submittedOnDailyAt": "2025-05-29T06:18:15.842Z",
      "title": "AITEE - Outlander Tamer para Ingeniería Eléctrica",
      "submittedOnDailyBy": {
        "_id": "631f5035c6b20f03c823c4ba",
        "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
        "isPro": false,
        "fullname": "Christopher Knievel",
        "user": "CKnievel",
        "type": "user"
      },
      "summary": "La combinación de un sistema de tutoría inteligente y modelos de lenguaje de gran escala es un enfoque prometedor para satisfacer las diversas necesidades de los estudiantes y promover el aprendizaje autónomo. Aunque los modelos de lenguaje de gran escala tienen una buena comprensión de los conceptos básicos de la electrónica, su capacidad para resolver preguntas específicas sobre circuitos eléctricos no es suficiente. En este trabajo, se presenta el sistema de tutoría basado en agentes específico para la electrónica, AITEE, que acompaña a los estudiantes durante el proceso de aprendizaje, proporcionando apoyo personalizado y fomentando el aprendizaje autónomo. AITEE soporta la escritura manual y circuitos digitales, permitiendo una interacción natural con los estudiantes. Un nuevo método de medición de similitud basado en grafos utiliza un enfoque de generación de sugerencias para buscar contextos adecuados a partir de materiales de lección. Además, la simulación paralela mejora la precisión en la aplicación de métodos de resolución. El sistema implementa una diálogo cíclico y fomenta la autonomía del aprendizaje a través de preguntas guiadas. Una evaluación experimental muestra que AITEE supera significativamente a enfoques basados en conocimientos específicos, y también muestra un rendimiento notable con modelos de lenguaje de tamaño mediano. Nuestros resultados subrayan la posibilidad de proporcionar un entorno de aprendizaje efectivo, personalizado y escalable, a través de un sistema de tutoría basado en agentes.",
      "upvotes": 0,
      "discussionId": "68380c870fb1ddbe91ba0c55",
      "ai_summary": "An agent-based tutoring system for electrical engineering enhances learning through natural circuit interaction, context-retrieving generation, and guided questioning, demonstrating superior performance compared to baseline methods.",
      "ai_keywords": [
        "agent-based tutoring system",
        "large language models",
        "electrical circuits",
        "adapted circuit reconstruction",
        "graph-based similarity measure",
        "retrieval augmented generation",
        "parallel Spice simulation",
        "Socratic dialogue"
      ]
    },
    "publishedAt": "2025-05-27T06:07:05.000Z",
    "title": "AITEE -- Agentic Tutor for Electrical Engineering",
    "summary": "Intelligent tutoring systems combined with large language models offer a\npromising approach to address students' diverse needs and promote\nself-efficacious learning. While large language models possess good\nfoundational knowledge of electrical engineering basics, they remain\ninsufficiently capable of addressing specific questions about electrical\ncircuits. In this paper, we present AITEE, an agent-based tutoring system for\nelectrical engineering designed to accompany students throughout their learning\nprocess, offer individualized support, and promote self-directed learning.\nAITEE supports both hand-drawn and digital circuits through an adapted circuit\nreconstruction process, enabling natural interaction with students. Our novel\ngraph-based similarity measure identifies relevant context from lecture\nmaterials through a retrieval augmented generation approach, while parallel\nSpice simulation further enhances accuracy in applying solution methodologies.\nThe system implements a Socratic dialogue to foster learner autonomy through\nguided questioning. Experimental evaluations demonstrate that AITEE\nsignificantly outperforms baseline approaches in domain-specific knowledge\napplication, with even medium-sized LLM models showing acceptable performance.\nOur results highlight the potential of agentic tutors to deliver scalable,\npersonalized, and effective learning environments for electrical engineering\neducation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21582.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631f5035c6b20f03c823c4ba",
      "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
      "fullname": "Christopher Knievel",
      "name": "CKnievel",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18149",
      "authors": [
        {
          "_id": "6837e51d05c81fd7d7d1962e",
          "user": {
            "_id": "63ca499104c97982831127ec",
            "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
            "isPro": false,
            "fullname": "Aradhye Agarwal",
            "user": "aradhye",
            "type": "user"
          },
          "name": "Aradhye Agarwal",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:47.337Z",
          "hidden": false
        },
        {
          "_id": "6837e51d05c81fd7d7d1962f",
          "name": "Ayan Sengupta",
          "hidden": false
        },
        {
          "_id": "6837e51d05c81fd7d7d19630",
          "name": "Tanmoy Chakraborty",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:57:43.000Z",
      "submittedOnDailyAt": "2025-05-29T06:15:54.434Z",
      "title": "Inicio de la búsqueda: Escalado de tiempo de prueba eficiente para modelos de lenguaje grandes",
      "submittedOnDailyBy": {
        "_id": "63ca499104c97982831127ec",
        "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
        "isPro": false,
        "fullname": "Aradhye Agarwal",
        "user": "aradhye",
        "type": "user"
      },
      "summary": "El escalado en tiempo de prueba (TTS) es un potencial método para mejorar la eficiencia de modelos de lenguaje grandes en el tiempo de inferencia mediante la asignación dinámica de la cantidad de cálculos. Los métodos de TTS existentes son efectivos, pero requieren largos pasos de decodificación, la generación de muchos ejemplos, y un aumento en la cantidad de tokens y el tiempo de inferencia. Hemos encontrado que frecuentemente se obtienen resultados con alta precisión con trazos cortos, pero menos precisos que los largos. Basándonos en esto, hemos introducido una estrategia de decodificación paralela sin entrenamiento llamada First Finish Search (FFS). FFS comienza con n muestras independientes y retorna la primera que se complete. FFS evaluó, utilizando 4 modelos lógicos (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B, Phi-4-Reasoning-Plus) y 4 conjuntos de datos (AIME24, AIME25-I, AIME25-II, GPQA Diamond), la simple decodificación, búsqueda de beam, votación, y la supresión de la secuencia más probable. En DeepSeek-R1, FFS alcanzó un 82.23% de precisión en el conjunto AIME, mejorando el rendimiento individual en un 15% y mostrando un desempeño similar a o4-mini de OpenAI. Análisis teórico: explica que se puede detener en el trazo más corto para obtener una respuesta precisa y identifica las condiciones en las que el detenerse temprano no es óptimo. La belleza y la simplicidad de FFS revelan que una estrategia sencilla de TTS puede demostrar un desempeño notable y revelar el potencial de una aproximación sencilla en el tiempo de inferencia.",
      "upvotes": 0,
      "discussionId": "6837e51d05c81fd7d7d19659",
      "ai_summary": "First Finish Search improves accuracy in large language models by stopping inference at the first completed sample, significantly outperforming other decoding strategies in reasoning tasks.",
      "ai_keywords": [
        "Test-time scaling",
        "TTS",
        "dynamic allocation",
        "inference",
        "reasoning tasks",
        "First Finish Search",
        "FFS",
        "parallel decoding",
        "decoding strategies",
        "beam search",
        "majority voting",
        "budget forcing",
        "accuracy",
        "performance",
        "inference latency",
        "early stopping"
      ]
    },
    "publishedAt": "2025-05-23T13:57:43.000Z",
    "title": "First Finish Search: Efficient Test-Time Scaling in Large Language\n  Models",
    "summary": "Test-time scaling (TTS), which involves dynamic allocation of compute during\ninference, offers a promising way to improve reasoning in large language\nmodels. While existing TTS methods work well, they often rely on long decoding\npaths or require a large number of samples to be generated, increasing the\ntoken usage and inference latency. We observe the surprising fact that for\nreasoning tasks, shorter traces are much more likely to be correct than longer\nones. Motivated by this, we introduce First Finish Search (FFS), a\ntraining-free parallel decoding strategy that launches n independent samples\nand returns as soon as any one completes. We evaluate FFS alongside simple\ndecoding, beam search, majority voting, and budget forcing on four reasoning\nmodels (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and\nacross four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With\nDeepSeek-R1, FFS achieves 82.23% accuracy on the AIME datasets, a 15%\nimprovement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's\no4-mini performance. Our theoretical analysis explains why stopping at the\nshortest trace is likely to yield a correct answer and identifies the\nconditions under which early stopping may be suboptimal. The elegance and\nsimplicity of FFS demonstrate that straightforward TTS strategies can perform\nremarkably well, revealing the untapped potential of simple approaches at\ninference time.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18149.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ca499104c97982831127ec",
      "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
      "fullname": "Aradhye Agarwal",
      "name": "aradhye",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]