[
  {
    "paper": {
      "id": "2501.12948",
      "authors": [
        {
          "_id": "6791b70a76d05e183a411598",
          "name": "DeepSeek-AI",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411599",
          "name": "Daya Guo",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41159a",
          "name": "Dejian Yang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41159b",
          "name": "Haowei Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41159c",
          "name": "Junxiao Song",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41159d",
          "name": "Ruoyu Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41159e",
          "name": "Runxin Xu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41159f",
          "name": "Qihao Zhu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115a0",
          "name": "Shirong Ma",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115a1",
          "name": "Peiyi Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115a2",
          "name": "Xiao Bi",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115a3",
          "name": "Xiaokang Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115a4",
          "name": "Xingkai Yu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115a5",
          "name": "Yu Wu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115a6",
          "name": "Z. F. Wu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115a7",
          "name": "Zhibin Gou",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115a8",
          "name": "Zhihong Shao",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115a9",
          "name": "Zhuoshu Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115aa",
          "name": "Ziyi Gao",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115ab",
          "name": "Aixin Liu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115ac",
          "name": "Bing Xue",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115ad",
          "name": "Bingxuan Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115ae",
          "name": "Bochao Wu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115af",
          "name": "Bei Feng",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115b0",
          "name": "Chengda Lu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115b1",
          "name": "Chenggang Zhao",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115b2",
          "name": "Chengqi Deng",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115b3",
          "name": "Chenyu Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115b4",
          "name": "Chong Ruan",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115b5",
          "name": "Damai Dai",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115b6",
          "name": "Deli Chen",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115b7",
          "name": "Dongjie Ji",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115b8",
          "name": "Erhang Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115b9",
          "name": "Fangyun Lin",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115ba",
          "name": "Fucong Dai",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115bb",
          "name": "Fuli Luo",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115bc",
          "name": "Guangbo Hao",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115bd",
          "name": "Guanting Chen",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115be",
          "name": "Guowei Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115bf",
          "name": "H. Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115c0",
          "name": "Han Bao",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115c1",
          "name": "Hanwei Xu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115c2",
          "name": "Haocheng Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115c3",
          "name": "Honghui Ding",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115c4",
          "name": "Huajian Xin",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115c5",
          "name": "Huazuo Gao",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115c6",
          "name": "Hui Qu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115c7",
          "name": "Hui Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115c8",
          "name": "Jianzhong Guo",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115c9",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115ca",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115cb",
          "name": "Jingchang Chen",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115cc",
          "name": "Jingyang Yuan",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115cd",
          "name": "Junjie Qiu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115ce",
          "name": "Junlong Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115cf",
          "name": "J. L. Cai",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115d0",
          "name": "Jiaqi Ni",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115d1",
          "name": "Jian Liang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115d2",
          "name": "Jin Chen",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115d3",
          "name": "Kai Dong",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115d4",
          "name": "Kai Hu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115d5",
          "name": "Kaige Gao",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115d6",
          "name": "Kang Guan",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115d7",
          "name": "Kexin Huang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115d8",
          "name": "Kuai Yu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115d9",
          "name": "Lean Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115da",
          "name": "Lecong Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115db",
          "name": "Liang Zhao",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115dc",
          "name": "Litong Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115dd",
          "name": "Liyue Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115de",
          "name": "Lei Xu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115df",
          "name": "Leyi Xia",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115e0",
          "name": "Mingchuan Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115e1",
          "name": "Minghua Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115e2",
          "name": "Minghui Tang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115e3",
          "name": "Meng Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115e4",
          "name": "Miaojun Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115e5",
          "name": "Mingming Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115e6",
          "name": "Ning Tian",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115e7",
          "name": "Panpan Huang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115e8",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115e9",
          "name": "Qiancheng Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115ea",
          "name": "Qinyu Chen",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115eb",
          "name": "Qiushi Du",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115ec",
          "name": "Ruiqi Ge",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115ed",
          "name": "Ruisong Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115ee",
          "name": "Ruizhe Pan",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115ef",
          "name": "Runji Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115f0",
          "name": "R. J. Chen",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115f1",
          "name": "R. L. Jin",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115f2",
          "name": "Ruyi Chen",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115f3",
          "name": "Shanghao Lu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115f4",
          "name": "Shangyan Zhou",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115f5",
          "name": "Shanhuang Chen",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115f6",
          "name": "Shengfeng Ye",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115f7",
          "name": "Shiyu Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115f8",
          "name": "Shuiping Yu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115f9",
          "name": "Shunfeng Zhou",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115fa",
          "name": "Shuting Pan",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115fb",
          "name": "S. S. Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115fc",
          "name": "Shuang Zhou",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115fd",
          "name": "Shaoqing Wu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115fe",
          "name": "Shengfeng Ye",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a4115ff",
          "name": "Tao Yun",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411600",
          "name": "Tian Pei",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411601",
          "name": "Tianyu Sun",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411602",
          "name": "T. Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411603",
          "name": "Wangding Zeng",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411604",
          "name": "Wanjia Zhao",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411605",
          "name": "Wen Liu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411606",
          "name": "Wenfeng Liang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411607",
          "name": "Wenjun Gao",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411608",
          "name": "Wenqin Yu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411609",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41160a",
          "name": "W. L. Xiao",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41160b",
          "name": "Wei An",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41160c",
          "name": "Xiaodong Liu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41160d",
          "name": "Xiaohan Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41160e",
          "name": "Xiaokang Chen",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41160f",
          "name": "Xiaotao Nie",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411610",
          "name": "Xin Cheng",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411611",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411612",
          "name": "Xin Xie",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411613",
          "name": "Xingchao Liu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411614",
          "name": "Xinyu Yang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411615",
          "name": "Xinyuan Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411616",
          "name": "Xuecheng Su",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411617",
          "name": "Xuheng Lin",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411618",
          "name": "X. Q. Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411619",
          "name": "Xiangyue Jin",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41161a",
          "name": "Xiaojin Shen",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41161b",
          "name": "Xiaosha Chen",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41161c",
          "name": "Xiaowen Sun",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41161d",
          "name": "Xiaoxiang Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41161e",
          "name": "Xinnan Song",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41161f",
          "name": "Xinyi Zhou",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411620",
          "name": "Xianzu Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411621",
          "name": "Xinxia Shan",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411622",
          "name": "Y. K. Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411623",
          "name": "Y. Q. Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411624",
          "name": "Y. X. Wei",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411625",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411626",
          "name": "Yanhong Xu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411627",
          "name": "Yao Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411628",
          "name": "Yao Zhao",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411629",
          "name": "Yaofeng Sun",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41162a",
          "name": "Yaohui Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41162b",
          "name": "Yi Yu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41162c",
          "name": "Yichao Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41162d",
          "name": "Yifan Shi",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41162e",
          "name": "Yiliang Xiong",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41162f",
          "name": "Ying He",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411630",
          "name": "Yishi Piao",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411631",
          "name": "Yisong Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411632",
          "name": "Yixuan Tan",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411633",
          "name": "Yiyang Ma",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411634",
          "name": "Yiyuan Liu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411635",
          "name": "Yongqiang Guo",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411636",
          "name": "Yuan Ou",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411637",
          "name": "Yuduan Wang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411638",
          "name": "Yue Gong",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411639",
          "name": "Yuheng Zou",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41163a",
          "name": "Yujia He",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41163b",
          "name": "Yunfan Xiong",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41163c",
          "name": "Yuxiang Luo",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41163d",
          "name": "Yuxiang You",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41163e",
          "name": "Yuxuan Liu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41163f",
          "name": "Yuyang Zhou",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411640",
          "name": "Y. X. Zhu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411641",
          "name": "Yanhong Xu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411642",
          "name": "Yanping Huang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411643",
          "name": "Yaohui Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411644",
          "name": "Yi Zheng",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411645",
          "name": "Yuchen Zhu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411646",
          "name": "Yunxian Ma",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411647",
          "name": "Ying Tang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411648",
          "name": "Yukun Zha",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411649",
          "name": "Yuting Yan",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41164a",
          "name": "Z. Z. Ren",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41164b",
          "name": "Zehui Ren",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41164c",
          "name": "Zhangli Sha",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41164d",
          "name": "Zhe Fu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41164e",
          "name": "Zhean Xu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41164f",
          "name": "Zhenda Xie",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411650",
          "name": "Zhengyan Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411651",
          "name": "Zhewen Hao",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411652",
          "name": "Zhicheng Ma",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411653",
          "name": "Zhigang Yan",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411654",
          "name": "Zhiyu Wu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411655",
          "name": "Zihui Gu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411656",
          "name": "Zijia Zhu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411657",
          "name": "Zijun Liu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411658",
          "name": "Zilin Li",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a411659",
          "name": "Ziwei Xie",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41165a",
          "name": "Ziyang Song",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41165b",
          "name": "Zizheng Pan",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41165c",
          "name": "Zhen Huang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41165d",
          "name": "Zhipeng Xu",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41165e",
          "name": "Zhongyu Zhang",
          "hidden": false
        },
        {
          "_id": "6791b70a76d05e183a41165f",
          "name": "Zhen Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-22T15:19:35.000Z",
      "title": "DeepSeek-R1: Mejora de la capacidad de inferencia de un LLM mediante aprendizaje por refuerzo",
      "summary": "Presentamos DeepSeek-R1-Zero y DeepSeek-R1, los primeros modelos de inferencia. DeepSeek-R1-Zero se ha entrenado a través de un aprendizaje de refuerzo (RL) a gran escala, demostrando una capacidad de inferencia sorprendente. Con RL, DeepSeek-R1-Zero ha naturalmente descubierto varias acciones de inferencia potentes y interesantes, pero presenta problemas como un pobre manejo del lenguaje o confusión. Para solucionar estos problemas y mejorar la eficiencia de inferencia, presentamos DeepSeek-R1. DeepSeek-R1 utiliza entrenamiento multietapa y datos de inicio diferenciados antes de aplicar el RL. DeepSeek-R1 alcanza performances similares a OpenAI-o1-1217. Para apoyar a la comunidad de investigación, publicamos en código abierto seis modelos concentrados antiguos basados en DeepSeek-R1-Zero, DeepSeek-R1, Qwen y Llama (1.5B, 7B, 8B, 14B, 32B, 70B).",
      "upvotes": 14,
      "discussionId": "6791b70c76d05e183a4116bf"
    },
    "publishedAt": "2025-01-22T22:27:48.680Z",
    "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12948.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5742
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.12909",
      "authors": [
        {
          "_id": "6791b688f0ecdeb1a89e35d2",
          "name": "Zhenran Xu",
          "hidden": false
        },
        {
          "_id": "6791b688f0ecdeb1a89e35d3",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "6791b688f0ecdeb1a89e35d4",
          "name": "Jifang Wang",
          "hidden": false
        },
        {
          "_id": "6791b688f0ecdeb1a89e35d5",
          "name": "Zhouyi Li",
          "hidden": false
        },
        {
          "_id": "6791b688f0ecdeb1a89e35d6",
          "name": "Senbao Shi",
          "hidden": false
        },
        {
          "_id": "6791b688f0ecdeb1a89e35d7",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "6791b688f0ecdeb1a89e35d8",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "6791b688f0ecdeb1a89e35d9",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "6791b688f0ecdeb1a89e35da",
          "name": "Jun Yu",
          "hidden": false
        },
        {
          "_id": "6791b688f0ecdeb1a89e35db",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-22T14:36:30.000Z",
      "title": "Agente de cine: marco de framework de automatización de películas de punto a punto en un espacio 3D virtual",
      "summary": "La producción de películas visuales requiere un proceso complejo de decisión, que incluye la escritura del guión, el diseño de imágenes visuales, la determinación de la posición y movimiento de los actores, entre otros. Motivado por el desarrollo reciente de la automatización basada en lenguaje natural, este artículo presenta un nuevo marco de colaboración de agentes basado en el lenguaje de programación (LLM) llamado \"FilmAgent\", diseñado para automatizar la producción de películas en un espacio visual 3D desde el terminal hasta el terminal. FilmAgent simula las funciones de director, escritor de guión, actores y cinematógrafos, cubriendo importantes etapas del flujo de trabajo en la producción de películas: 1) el desarrollo de ideas genera unas ideas organizadas y estandarizadas; 2) la escritura del guión detalla las conversaciones y acciones de los personajes en cada escena; 3) el cinematógrafo decide la configuración de la cámara para cada shot. El equipo de agentes revisa y corregió las escenas mediante una serie de retroalimentaciones y modificaciones. Se evaluaron videos generados para 15 ideas y 4 aspectos clave, y FilmAgent superó todos los aspectos, obteniendo un promedio de 5 puntos con un 3.98. Esto demuestra la posibilidad de colaboración de múltiples agentes en la producción de películas. Según el análisis evolutivo, FilmAgent puede superar a un solo agente de tipo o1 sin necesidad de utilizar el modelo GPT-4o, mostrando las ventajas de un sistema de agentes colaborativos. Finalmente, se discuten las fortalezas y debilidades complementarias de FilmAgent y el modelo video Sora de OpenAI.",
      "upvotes": 14,
      "discussionId": "6791b68ef0ecdeb1a89e3770"
    },
    "publishedAt": "2025-01-22T22:25:11.596Z",
    "title": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12909.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5742
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.13074",
      "authors": [
        {
          "_id": "6791baa69d6eba7f285d5dce",
          "name": "Ang Lv",
          "hidden": false
        },
        {
          "_id": "6791baa69d6eba7f285d5dcf",
          "name": "Ruobing Xie",
          "hidden": false
        },
        {
          "_id": "6791baa69d6eba7f285d5dd0",
          "name": "Yining Qian",
          "hidden": false
        },
        {
          "_id": "6791baa69d6eba7f285d5dd1",
          "name": "Songhao Wu",
          "hidden": false
        },
        {
          "_id": "6791baa69d6eba7f285d5dd2",
          "name": "Xingwu Sun",
          "hidden": false
        },
        {
          "_id": "6791baa69d6eba7f285d5dd3",
          "name": "Zhanhui Kang",
          "hidden": false
        },
        {
          "_id": "6791baa69d6eba7f285d5dd4",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "6791baa69d6eba7f285d5dd5",
          "name": "Rui Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-22T18:37:08.000Z",
      "title": "EXPERT AUTONOMY MODEL",
      "summary": "Mixture-of-Experts (MoE) modeles utilizan un Rotator para asignar tokens a módulos específicos de Expertos, demostrando un rendimiento superior a modelos más densos al activar algunos parámetros. Consideramos que la diferencia entre la decisión del Rotator y la ejecución de los Expertos es crucial, pero esta diferencia a menudo es ignorada, y se argumenta que la elección de los Expertos no es óptima o que el entrenamiento es ineficaz. Para resolver esto, proponemos un nuevo paradigma de MoE llamado Autonomy-of-Experts (AoE), en el que los Expertos pueden procesar entradas automáticamente. AoE se basa en que los Expertos conocen su propio poder efectivo en el procesamiento de tokens y reflejan esto en el escala de activación interna. En AoE, el Rotator es eliminado y en su lugar, los Expertos calculan la activación interna para la entrada y se ordenan según los criterios de activación. Solo los Expertos superiores se siguen en la propagación, mientras que los demás son cancelados. Usando una Low-Rank Decomposition, reducimos el sobrecargo de cálculos previos a través de rangos bajos, asegurando así una elección de Expertos y un entrenamiento efectivo comparado con los modelos tradicionales de MoE. Demostramos que modelos de lenguaje con entre 700M a 4B parámetros, entrenados previamente con AoE, superan eficientemente a los modelos tradicionales de MoE.",
      "upvotes": 6,
      "discussionId": "6791baa79d6eba7f285d5f35"
    },
    "publishedAt": "2025-01-22T22:53:01.771Z",
    "title": "Autonomy-of-Experts Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13074.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b8ca3c5067873176d4b436",
      "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg",
      "fullname": "AngLv",
      "name": "AngLv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.12599",
      "authors": [
        {
          "_id": "6791b6029e215712a7cf700a",
          "name": "Kimi Team",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf700b",
          "name": "Angang Du",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf700c",
          "name": "Bofei Gao",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf700d",
          "name": "Bowei Xing",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf700e",
          "name": "Changjiu Jiang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf700f",
          "name": "Cheng Chen",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7010",
          "name": "Cheng Li",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7011",
          "name": "Chenjun Xiao",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7012",
          "name": "Chenzhuang Du",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7013",
          "name": "Chonghua Liao",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7014",
          "name": "Chuning Tang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7015",
          "name": "Congcong Wang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7016",
          "name": "Dehao Zhang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7017",
          "name": "Enming Yuan",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7018",
          "name": "Enzhe Lu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7019",
          "name": "Fengxiang Tang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf701a",
          "name": "Flood Sung",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf701b",
          "name": "Guangda Wei",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf701c",
          "name": "Guokun Lai",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf701d",
          "name": "Haiqing Guo",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf701e",
          "name": "Han Zhu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf701f",
          "name": "Hao Ding",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7020",
          "name": "Hao Hu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7021",
          "name": "Hao Yang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7022",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7023",
          "name": "Haotian Yao",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7024",
          "name": "Haotian Zhao",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7025",
          "name": "Haoyu Lu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7026",
          "name": "Haoze Li",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7027",
          "name": "Haozhen Yu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7028",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7029",
          "name": "Huabin Zheng",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf702a",
          "name": "Huan Yuan",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf702b",
          "name": "Jia Chen",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf702c",
          "name": "Jianhang Guo",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf702d",
          "name": "Jianlin Su",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf702e",
          "name": "Jianzhou Wang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf702f",
          "name": "Jie Zhao",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7030",
          "name": "Jin Zhang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7031",
          "name": "Jingyuan Liu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7032",
          "name": "Junjie Yan",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7033",
          "name": "Junyan Wu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7034",
          "name": "Lidong Shi",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7035",
          "name": "Ling Ye",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7036",
          "name": "Longhui Yu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7037",
          "name": "Mengnan Dong",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7038",
          "name": "Neo Zhang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7039",
          "name": "Ningchen Ma",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf703a",
          "name": "Qiwei Pan",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf703b",
          "name": "Qucheng Gong",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf703c",
          "name": "Shaowei Liu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf703d",
          "name": "Shengling Ma",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf703e",
          "name": "Shupeng Wei",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf703f",
          "name": "Sihan Cao",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7040",
          "name": "Siying Huang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7041",
          "name": "Tao Jiang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7042",
          "name": "Weihao Gao",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7043",
          "name": "Weimin Xiong",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7044",
          "name": "Weiran He",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7045",
          "name": "Weixiao Huang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7046",
          "name": "Wenhao Wu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7047",
          "name": "Wenyang He",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7048",
          "name": "Xianghui Wei",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7049",
          "name": "Xianqing Jia",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf704a",
          "name": "Xingzhe Wu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf704b",
          "name": "Xinran Xu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf704c",
          "name": "Xinxing Zu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf704d",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf704e",
          "name": "Xuehai Pan",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf704f",
          "name": "Y. Charles",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7050",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7051",
          "name": "Yangyang Hu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7052",
          "name": "Yangyang Liu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7053",
          "name": "Yanru Chen",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7054",
          "name": "Yejie Wang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7055",
          "name": "Yibo Liu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7056",
          "name": "Yidao Qin",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7057",
          "name": "Yifeng Liu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7058",
          "name": "Ying Yang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7059",
          "name": "Yiping Bao",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf705a",
          "name": "Yulun Du",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf705b",
          "name": "Yuxin Wu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf705c",
          "name": "Yuzhi Wang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf705d",
          "name": "Zaida Zhou",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf705e",
          "name": "Zhaoji Wang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf705f",
          "name": "Zhaowei Li",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7060",
          "name": "Zhen Zhu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7061",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7062",
          "name": "Zhexu Wang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7063",
          "name": "Zhilin Yang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7064",
          "name": "Zhiqi Huang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7065",
          "name": "Zihao Huang",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7066",
          "name": "Ziyao Xu",
          "hidden": false
        },
        {
          "_id": "6791b6029e215712a7cf7067",
          "name": "Zonghan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-22T02:48:14.000Z",
      "title": "Kimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi k1.5: Reinforcement Learning Scaling with LLMs\n\nKimi",
      "summary": "El método de entrenamiento de modelos de lenguaje y la predicción del siguiente token ha demostrado ser efectivo para la escalabilidad de la computación, pero está limitado por la cantidad de datos de entrenamiento disponible. La escalabilidad del aprendizaje por refuerzo (RL) abre nuevas perspectivas para el desarrollo continuo de la inteligencia artificial, esperando que modelos de lenguaje de gran escala (LLMs) puedan aprender a compensar y escalar los datos de entrenamiento. Sin embargo, los estudios previos han obtenido resultados relativos. En este sentido, presentamos el proceso de entrenamiento de nuestro último modelo de lenguaje multimodelo, Kimi k1.5. Este proceso incluye tecnologías de entrenamiento de RL, recetas de datos multimodelo y optimización de infraestructura. Además, utilizamos técnicas de codificación de secuencias largas y métodos de optimización de políticas mejorados, que son componentes cruciales de nuestro enfoque. Nuestro sistema no depende de métodos más complejos (como la búsqueda de árboles de Monte Carlo, funciones de valor y modelos de recompensa de proceso), sino que construye un marco de RL sencillo y efectivo. En particular, nuestro sistema logra los mejores resultados lógicos en varios benchmarks y modelos. Por ejemplo, en AIME obtuvimos un 77.5%, en MATH 500 un 96.2%, en Codeforces un 94% y en MathVista un 74.9%, comparándonos con o1 de OpenAI. Además, proponemos una efectiva metodología 'largo-2-corto' para mejorar modelos de contexto corto utilizando contextos de largo plazo, coleccionando los mejores resultados de lógica de contexto corto. Por ejemplo, en AIME obtuvimos un 60.8%, en MATH500 un 94.6% y en LiveCodeBench un 47.3%, superando significativamente a GPT-4o y Claude Sonnet 3.5 (máximo +550% más).",
      "upvotes": 6,
      "discussionId": "6791b6039e215712a7cf70aa"
    },
    "publishedAt": "2025-01-22T22:23:27.498Z",
    "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12599.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5742
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.12570",
      "authors": [
        {
          "_id": "6791b165c6fee5f0588773a9",
          "name": "Haotian Luo",
          "hidden": false
        },
        {
          "_id": "6791b165c6fee5f0588773aa",
          "name": "Li Shen",
          "hidden": false
        },
        {
          "_id": "6791b165c6fee5f0588773ab",
          "name": "Haiying He",
          "hidden": false
        },
        {
          "_id": "6791b165c6fee5f0588773ac",
          "name": "Yibo Wang",
          "hidden": false
        },
        {
          "_id": "6791b165c6fee5f0588773ad",
          "name": "Shiwei Liu",
          "hidden": false
        },
        {
          "_id": "6791b165c6fee5f0588773ae",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "6791b165c6fee5f0588773af",
          "name": "Naiqiang Tan",
          "hidden": false
        },
        {
          "_id": "6791b165c6fee5f0588773b0",
          "name": "Xiaochun Cao",
          "hidden": false
        },
        {
          "_id": "6791b165c6fee5f0588773b1",
          "name": "Dacheng Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-22T01:35:11.000Z",
      "title": "O1-Pruner: Ajuste micro de reducción lógica como O1 para lograr la armonía de longitudes",
      "summary": "Recientemente, modelos como O1 de OpenAI, que utilizan la teoría de razonamiento a largo plazo, introduciron el proceso de razonamiento ampliado necesario para resolver problemas complejos de manera humana. Este paradigma de razonamiento mejora significativamente la capacidad de resolución de problemas del modelo y logra resultados ordenados. Sin embargo, el proceso de razonamiento a largo plazo genera un aumento significativo en el tiempo de cálculo. Una de las principales cuestiones es reducir el sobrecarga de cálculos de modelos de razonamiento a largo plazo mientras se mantiene la precisión. En este artículo, se propone que los modelos de razonamiento a largo plazo tienen dificultades en asignar eficazmente el token bucket según la dificultad del problema y el exceso de razonamiento. Para abordar esta cuestión, se propone la fine-tuning de Long-Harmonizing (O1-Pruner) para minimizar la sobrecarga de razonamiento mientras se mantiene la precisión. Este método efectivo de fine-tuning evalúa los token buckets previamente para predecir el rendimiento base y realiza un fine-tuning de estilo RL para hacer que el modelo genere un proceso de razonamiento corto bajo las restricciones de precisión. De esta manera, el modelo puede realizar una razonamiento eficiente y preciso, reduciendo la longitud. Según los benchmarks matemáticos de razonamiento, O1-Pruner reduce significativamente la sobrecarga de cálculos y aumenta la precisión, proporcionando una nueva solución adecuada a estos problemas. El código está disponible en https://github.com/StarDewXXX/O1-Pruner.",
      "upvotes": 3,
      "discussionId": "6791b166c6fee5f0588773fc"
    },
    "publishedAt": "2025-01-22T22:03:17.988Z",
    "title": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12570.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5742
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.11067",
      "authors": [
        {
          "_id": "679191d8400d620e9c3e5eef",
          "name": "Elad Levi",
          "hidden": false
        },
        {
          "_id": "679191d8400d620e9c3e5ef0",
          "name": "Ilan Kadar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-19T14:58:35.000Z",
      "title": "Intelligent Agent: Framework for Multi-Agent System Evaluation in Conversational AI Systems",
      "summary": "Los modelos de lenguaje de gran escala (LLMs) están innovando la inteligencia artificial y evolucionando hacia sistemas orientados a tareas con capacidades de planificación y ejecución automáticas. Una de las principales aplicaciones de los LLMs es el sistema de conversación, que debe procesar múltiples diálogos, integrar API específicas de dominio y cumplir con restricciones políticas estrictas. Sin embargo, la evaluación de estos agentes presenta grandes desafíos, ya que los métodos existentes no detectan la complejidad y diversidad de las interacciones reales. Presentamos IntellAgent, un marco de trabajo escalable y de código abierto para evaluar sistemas de conversación. IntellAgent está diseñado para evaluar el sistema de conversación en su totalidad. Combina modelado de grafos dirigidos por políticas, generación de eventos realistas y simulación de agentes de usuario interactivos para automatizar varios marcos de prueba sintéticos. Este enfoque innovador supera las limitaciones de las medidas de grandes escalas de marcos de prueba estáticos y manuales, proporcionando detalles específicos. IntellAgent marca un cambio en el paradigma de la evaluación de sistemas de conversación, simulando escenarios realistas con múltiples políticas y explorando la interacción subtil entre las capacidades de los agentes y las restricciones políticas. Diferente de los métodos existentes, utiliza modelos de políticas basados en grafos para representar relaciones, posibilidades y complejidad, facilitando una alta dimensión dinámica. IntellAgent permite identificar diferencias significativas en el rendimiento y proporciona orientaciones de acción para optimizaciones específicas. Su diseño modular de código abierto facilita la integración de nuevos dominios, políticas y API, y fomenta la reproducibilidad y la colaboración de la comunidad. Hemos encontrado que IntellAgent es un marco de trabajo efectivo que resuelve desafíos entre la investigación y la implementación, impulsando el desarrollo de sistemas de conversación. El marco de trabajo está disponible para uso en https://github.com/plurai-ai/intellagent.",
      "upvotes": 1,
      "discussionId": "679191d9400d620e9c3e5f26"
    },
    "publishedAt": "2025-01-22T23:28:09.297Z",
    "title": "IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11067.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635d618494e5b275ca73b844",
      "avatarUrl": "/avatars/8cdaac6591a12b252612b99094e00959.svg",
      "fullname": "Levi",
      "name": "Eladlev",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]