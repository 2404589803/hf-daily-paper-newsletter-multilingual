[
  {
    "paper": {
      "id": "2503.15265",
      "authors": [
        {
          "_id": "67db8c4c9e4f93ee46411c1d",
          "name": "Ruowen Zhao",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c1e",
          "name": "Junliang Ye",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c1f",
          "name": "Zhengyi Wang",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c20",
          "name": "Guangce Liu",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c21",
          "name": "Yiwen Chen",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c22",
          "name": "Yikai Wang",
          "hidden": false
        },
        {
          "_id": "67db8c4c9e4f93ee46411c23",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T14:39:30.000Z",
      "submittedOnDailyAt": "2025-03-20T03:22:40.364Z",
      "title": "DeepMesh: Generación de Artista Matching Automático por Aprendizaje por Refuerzo",
      "submittedOnDailyBy": {
        "_id": "6522e4fbd89bc7773ddc4b58",
        "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
        "isPro": false,
        "fullname": "Ruowen Zhao",
        "user": "zzzrw",
        "type": "user"
      },
      "summary": "El triángulo de mesh es un elemento importante en la manipulación y renderización eficientes en aplicaciones 3D. El método de retorno automático se utiliza para generar un triángulo de mesh estructurado mediante la predicción de beta tokens dispersos, pero su eficiencia está limitada por la cantidad de caras y las incompletaciones del triángulo de mesh. Para resolver estos problemas, proponemos el marco de trabajo DeepMesh. Este marco incorpora dos innovaciones clave para optimizar la generación de triángulos de mesh. 1. Incluye un nuevo algoritmo de tokenización eficiente, que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye un algoritmo de tokenización eficiente que incluye",
      "upvotes": 25,
      "discussionId": "67db8c519e4f93ee46411d60",
      "projectPage": "https://zhaorw02.github.io/DeepMesh/",
      "githubRepo": "https://github.com/zhaorw02/DeepMesh",
      "ai_keywords": [
        "triangle meshes",
        "auto-regressive methods",
        "discrete vertex tokens",
        "face counts",
        "mesh incompleteness",
        "DeepMesh",
        "tokenization algorithm",
        "data curation",
        "data processing",
        "Reinforcement Learning (RL)",
        "Direct Preference Optimization (DPO)",
        "human evaluation",
        "3D metrics",
        "point clouds",
        "intricate details",
        "precise topology",
        "state-of-the-art methods",
        "precision",
        "quality"
      ]
    },
    "publishedAt": "2025-03-19T10:39:30.000Z",
    "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning",
    "summary": "Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15265.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6522e4fbd89bc7773ddc4b58",
      "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
      "fullname": "Ruowen Zhao",
      "name": "zzzrw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13288",
      "authors": [
        {
          "_id": "67dbc49d85eacb364e913c38",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c39",
          "user": {
            "_id": "67dbe3d969655e406fda64b8",
            "avatarUrl": "/avatars/6053c84e32d0e46dd1e490c493f766ed.svg",
            "isPro": false,
            "fullname": "Mei Tuan",
            "user": "Meituannnnnn",
            "type": "user"
          },
          "name": "Hang Yan",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-20T09:48:32.179Z",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3a",
          "name": "Chang Ma",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3b",
          "name": "Haiteng Zhao",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3c",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3d",
          "name": "Qika Lin",
          "hidden": false
        },
        {
          "_id": "67dbc49d85eacb364e913c3e",
          "name": "Zhiyong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T15:38:33.000Z",
      "submittedOnDailyAt": "2025-03-20T06:08:48.330Z",
      "title": "φ-Decoding: Exploración del tiempo de inferencia equilibrado y uso de muestreo predictivo adaptativo",
      "submittedOnDailyBy": {
        "_id": "64e6cf78ecce34cb442dc889",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
        "isPro": false,
        "fullname": "Fangzhi Xu",
        "user": "xufangzhi",
        "type": "user"
      },
      "summary": "La optimización de inferencia es un proceso que busca obtener una serie de razones justificativas para escalar cálculos de manera eficiente, asegurando un rendimiento óptimo. Este enfoque permite mejorar la eficiencia de la inferencia. Los stileados de exploración pasados, que abordaron las deficiencias de la generación automática de retornos, encontraron que el gran espacio de búsqueda hacía difícil mantener un equilibrio eficiente entre exploración y desarrollo, lo que dificultaba la obtención de etapas óptimas. Para resolver este problema, introducimos un nuevo método llamado \"folorescan sampling\" para predecir etapas óptimas globalmente, calculando futuras etapas. Este método permite representar con precisión los valores de las etapas a través de evaluaciones con fuerte expresividad. Mediante el uso de folorescan y clustering, podemos aproximar dos distribuciones y realizar muestreos para seleccionar y desarrollar etapas óptimas. Para apoyar la eficiencia de la inferencia, proponemos stileados de \"plausizing\" in-width y in-depth que permiten la adaptación de la arquitectura de cálculo. Los experimentos en un amplio rango de 7 benchmarks demostraron que phi-Decoding supera significativamente a los baselines en términos de eficiencia y rendimiento. El análisis de desarrollo muestra que esta metodología se generaliza en diferentes modelos de lenguaje grande y muestra una gran capacidad de escalabilidad en varios buckets de cálculo. El código está disponible en GitHub (https://github.com/xufangzhi/phi-Decoding) y se proporcionará también como un paquete de código abierto en PyPI.",
      "upvotes": 24,
      "discussionId": "67dbc49f85eacb364e913d20",
      "githubRepo": "https://github.com/xufangzhi/phi-Decoding",
      "ai_keywords": [
        "inference-time optimization",
        "auto-regressive generation",
        "foresight sampling",
        "$\\phi$-Decoding",
        "joint distribution",
        "in-width and in-depth pruning",
        "LLMs (Large Language Models)"
      ]
    },
    "publishedAt": "2025-03-17T11:38:33.000Z",
    "title": "φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation",
    "summary": "Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed phi-Decoding. To provide a precise and expressive estimation of step\nvalue, phi-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show phi-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\nopen-source PyPI package is coming soon.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13288.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6cf78ecce34cb442dc889",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
      "fullname": "Fangzhi Xu",
      "name": "xufangzhi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15485",
      "authors": [
        {
          "_id": "67db7dd224fe67fe45b21e63",
          "name": "Zineng Tang",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e64",
          "name": "Long Lian",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e65",
          "name": "Seun Eisape",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e66",
          "name": "XuDong Wang",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e67",
          "name": "Roei Herzig",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e68",
          "name": "Adam Yala",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e69",
          "name": "Alane Suhr",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e6a",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "67db7dd224fe67fe45b21e6b",
          "name": "David M. Chan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T17:58:57.000Z",
      "submittedOnDailyAt": "2025-03-20T01:01:18.127Z",
      "title": "TULIP: Ejercicio de predicción integrada de lenguaje e imagen",
      "submittedOnDailyBy": {
        "_id": "6388f68c43d8b0797a09ff84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
        "isPro": false,
        "fullname": "David Chan",
        "user": "davidchan",
        "type": "user"
      },
      "summary": "Aunque los éxitos de CLIP y SigLIP han demostrado su efectividad, estos modelos han sido mejorados para que se adapten adecuadamente a tareas visuales que requieren una comprensión de imágenes con alta precisión (por ejemplo, segmentación, estimación de profundidad, reconocimiento de objetos detallados), y han enfocado su atención en la disposición del lenguaje, lo que ha afectado su comprensión de imágenes. Por otro lado, los modelos centrados en la visión, aunque son especializados en el procesamiento de información visual, tienen dificultades en la comprensión del lenguaje y limitan la flexibilidad de tareas dirigidas por el lenguaje. En este estudio, se presenta TULIP, un modelo abierto que puede reemplazar a CLIP y otros modelos de manera directa. Nuestro enfoque utiliza la expansión de datos generativos, aprendizaje comparativo reforzado entre imágenes y entre lenguajes, y normalización de reconstrucción de imágenes y lenguaje para entrenar características visuales detalladas mientras mantiene la disposición del significado global. Nuestro enfoque tiene más de 10 mil millones de parámetros, superando los modelos SOTA en varios benchmarks, logrando un nuevo SOTA en ImageNet-1K con 0 shot, mejorando en un factor de al menos 2 en RxRx1 frente a SigLIP y al menos 3 en MMVP. Los códigos y chekpoints están disponibles en https://tulip-berkeley.github.io.",
      "upvotes": 18,
      "discussionId": "67db7dd424fe67fe45b21ee1",
      "projectPage": "https://tulip-berkeley.github.io/",
      "ai_keywords": [
        "generative data augmentation",
        "enhanced image-image and text-text contrastive learning",
        "image/text reconstruction regularization",
        "fine-grained visual features",
        "global semantic alignment",
        "zero-shot performance",
        "few-shot classification",
        "vision-language models"
      ]
    },
    "publishedAt": "2025-03-19T13:58:57.000Z",
    "title": "TULIP: Towards Unified Language-Image Pretraining",
    "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over 3times\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15485.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6388f68c43d8b0797a09ff84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
      "fullname": "David Chan",
      "name": "davidchan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15475",
      "authors": [
        {
          "_id": "67db729fa720e711cff4d205",
          "name": "Foundation AI Team",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d206",
          "name": "Kiran Bhat",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d207",
          "name": "Nishchaie Khanna",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d208",
          "name": "Karun Channa",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d209",
          "name": "Tinghui Zhou",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20a",
          "name": "Yiheng Zhu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20b",
          "name": "Xiaoxia Sun",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20c",
          "name": "Charles Shang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20d",
          "name": "Anirudh Sudarshan",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20e",
          "name": "Maurice Chu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d20f",
          "name": "Daiqing Li",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d210",
          "name": "Kangle Deng",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d211",
          "name": "Jean-Philippe Fauconnier",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d212",
          "name": "Tijmen Verhulsdonck",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d213",
          "name": "Maneesh Agrawala",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d214",
          "name": "Kayvon Fatahalian",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d215",
          "name": "Alexander Weiss",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d216",
          "name": "Christian Reiser",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d217",
          "name": "Ravi Kiran Chirravuri",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d218",
          "name": "Ravali Kandur",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d219",
          "name": "Alejandro Pelaez",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21a",
          "name": "Akash Garg",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21b",
          "name": "Michael Palleschi",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21c",
          "name": "Jessica Wang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21d",
          "name": "Skylar Litz",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21e",
          "name": "Leon Liu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d21f",
          "name": "Anying Li",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d220",
          "name": "David Harmon",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d221",
          "name": "Derek Liu",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d222",
          "name": "Liangjun Feng",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d223",
          "name": "Denis Goupil",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d224",
          "name": "Lukas Kuczynski",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d225",
          "name": "Jihyun Yoon",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d226",
          "name": "Naveen Marri",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d227",
          "name": "Peiye Zhuang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d228",
          "name": "Yinan Zhang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d229",
          "name": "Brian Yin",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22a",
          "name": "Haomiao Jiang",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22b",
          "name": "Marcel van Workum",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22c",
          "name": "Thomas Lane",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22d",
          "name": "Bryce Erickson",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22e",
          "name": "Salil Pathare",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d22f",
          "name": "Kyle Price",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d230",
          "name": "Anupam Singh",
          "hidden": false
        },
        {
          "_id": "67db729fa720e711cff4d231",
          "name": "David Baszucki",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T17:52:17.000Z",
      "submittedOnDailyAt": "2025-03-20T00:57:52.833Z",
      "title": "Cube: Inteligencia 3D en Roboloco",
      "submittedOnDailyBy": {
        "_id": "62cd5c43299c0c2e0e437842",
        "avatarUrl": "/avatars/8fa8511baf2d9bd95d3ba4535a5b3d69.svg",
        "isPro": false,
        "fullname": "Jean-Philippe Fauconnier",
        "user": "j4kn",
        "type": "user"
      },
      "summary": "Los modelos de base entrenados con una gran cantidad de datos muestran una excelencia y capacidad de generación que sorprende en el campo de las frases, imágenes, voz y vídeos. Nuestro objetivo en Robolox es construir modelos de base basados en información 3D. Este modelo se centra en ayudar a desarrolladores a crear y animar objetos 3D o espacios, así como a escribir scripts de programación que describen el enredo de las cuerdas de los personajes o las acciones de los objetos. Se discuten las tres principales exigencias de diseño de estos modelos de base 3D y se presenta el primer paso en su construcción. Se espera que la forma geométrica 3D sea un tipo de datos clave y se explica la solución del tokenizador de formas 3D. Nuestro escenario de tokenizador muestra cómo se pueden usar en aplicaciones como la generación de texto a forma, forma a texto, y la generación de espacio desde texto, entre otros. Estas aplicaciones demuestran cómo los modelos de lenguaje de gran escala (LLMs) se integran con la interpretación espacial y la razón. Finalmente, se discute el camino para construir un modelo de base completo basado en información 3D.",
      "upvotes": 16,
      "discussionId": "67db72a1a720e711cff4d292",
      "githubRepo": "https://github.com/Roblox/cube",
      "ai_keywords": [
        "3D foundation model",
        "3D geometric shapes",
        "3D shape tokenizer",
        "text-to-shape generation",
        "shape-to-text generation",
        "text-to-scene generation",
        "large language models (LLMs)",
        "scene analysis",
        "reasoning",
        "unified foundation model"
      ]
    },
    "publishedAt": "2025-03-19T13:52:17.000Z",
    "title": "Cube: A Roblox View of 3D Intelligence",
    "summary": "Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15475.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62cd5c43299c0c2e0e437842",
      "avatarUrl": "/avatars/8fa8511baf2d9bd95d3ba4535a5b3d69.svg",
      "fullname": "Jean-Philippe Fauconnier",
      "name": "j4kn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14868",
      "authors": [
        {
          "_id": "67db9f06842d8b6642a5eeaf",
          "name": "Hoigi Seo",
          "hidden": false
        },
        {
          "_id": "67db9f06842d8b6642a5eeb0",
          "name": "Wongi Jeong",
          "hidden": false
        },
        {
          "_id": "67db9f06842d8b6642a5eeb1",
          "name": "Kyungryeol Lee",
          "hidden": false
        },
        {
          "_id": "67db9f06842d8b6642a5eeb2",
          "name": "Se Young Chun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T03:45:37.000Z",
      "submittedOnDailyAt": "2025-03-20T03:25:51.779Z",
      "title": "Modelo de difusor reducido sin aplicar eficiente difusion de bajo proporción individualizado",
      "submittedOnDailyBy": {
        "_id": "633e6f07309a99325095dd42",
        "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
        "isPro": false,
        "fullname": "Hoigi Seo",
        "user": "Agorium",
        "type": "user"
      },
      "summary": "El módulo de depyrivation demostró un desempeño impresionante en la síntesis de imágenes, pero requiere una gran cantidad de cálculos y recursos de memoria para su entrenamiento, ajuste y inferencia. La tecnología de cuvantization desarrollada pudo minimizar el uso de memoria durante la inferencia, pero el entrenamiento y ajuste de estos módulos de cuvantization requirió un gran consumo de memoria debido a cálculos precisos y la propagación del gradiente basada en gradientes. Sin embargo, un ajuste eficiente en memoria es particularmente apropiado para aplicaciones que procesan datos personales en dispositivos móviles o otros dispositivos de borde. En este artículo, se aplica la técnica de depyrivation a un módulo personalizado a través de la inserción de texto, y se optimiza el token de cero para realizar la personalización del módulo de depyrivation de manera que no necesite memoria para el gradiente y la activación basado en propagación del gradiente. La estimación de gradiente mediante optimización de ceros elimina el ruido en la mayoría de los casos, donde el gradiente se proyecta en un espacio de dimensiones basado en la historia de tokens pasados y se denomina \"Subspace Gradient\". Este método se utiliza para sampling de pasos de tiempo efectivos de depyrivation llamados \"Partial Uniform Timestep Sampling\", lo que permite comparar la coincidencia entre imágenes y texto solo mediante el paso hacia adelante, mejorando la eficiencia de memoria en el entrenamiento en un factor de 8.2.",
      "upvotes": 15,
      "discussionId": "67db9f11842d8b6642a5f165",
      "projectPage": "https://ignoww.github.io/ZOODiP_project/",
      "githubRepo": "https://github.com/ignoww/ZOODiP",
      "ai_keywords": [
        "diffusion models",
        "image synthesis",
        "quantization techniques",
        "dequantization",
        "gradient-based algorithms",
        "memory-efficient fine-tuning",
        "Textual Inversion",
        "zeroth-order optimization",
        "personalization tokens",
        "gradient estimation",
        "Subspace Gradient",
        "subspace projection",
        "text embedding",
        "Partial Uniform Timestep Sampling",
        "diffusion timesteps",
        "Stable Diffusion",
        "image and text alignment scores"
      ]
    },
    "publishedAt": "2025-03-18T23:45:37.000Z",
    "title": "Efficient Personalization of Quantized Diffusion Model without\n  Backpropagation",
    "summary": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to 8.2times.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14868.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633e6f07309a99325095dd42",
      "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
      "fullname": "Hoigi Seo",
      "name": "Agorium",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15417",
      "authors": [
        {
          "_id": "67db8e05842d8b6642a135d0",
          "name": "Harold Haodong Chen",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d1",
          "name": "Haojian Huang",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d2",
          "name": "Xianfeng Wu",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d3",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d4",
          "name": "Yajing Bai",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d5",
          "name": "Wen-Jie Shu",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d6",
          "name": "Harry Yang",
          "hidden": false
        },
        {
          "_id": "67db8e05842d8b6642a135d7",
          "name": "Ser-Nam Lim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T16:59:32.000Z",
      "submittedOnDailyAt": "2025-03-20T02:10:52.068Z",
      "title": "La normalización del tiempo fortalece significativamente el generador de vídeos.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "El tiempo es un aspecto importante en la generación de vídeos, garantizando movimientos consistentes entre frames y acciones realistas. Sin embargo, alcanzar una alta coincidencia temporal y diversidad es difícil. En este artículo, se investiga por primera vez el tiempo en la generación de vídeos y se presenta FluxFlow. FluxFlow es una estrategia para mejorar el tiempo, diseñado para manipular datos a nivel de datos y evitar cambios en la arquitectura, lo que no es necesario. Los experimentos extendidos en UCF-101 y VBench muestran que FluxFlow mejora significativamente la coincidencia temporal y la diversidad, manteniendo la dependencia espacial, tanto para U-Net, DiT, como para estructuras basadas en AR. Estos hallazgos demuestran la posibilidad de que el tiempo puede ser un método sencillo y efectivo para mejorar la calidad de la generación de vídeos.",
      "upvotes": 12,
      "discussionId": "67db8e07842d8b6642a1365f",
      "ai_keywords": [
        "temporal augmentation",
        "FluxFlow",
        "temporal perturbations",
        "temporal quality",
        "temporal coherence",
        "UCF-101",
        "VBench",
        "U-Net",
        "DiT",
        "AR-based architectures",
        "spatial fidelity"
      ]
    },
    "publishedAt": "2025-03-19T12:59:32.000Z",
    "title": "Temporal Regularization Makes Your Video Generator Stronger",
    "summary": "Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6408
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12532",
      "authors": [
        {
          "_id": "67da1df040371958e1732c83",
          "name": "Fanbin Lu",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c84",
          "name": "Zhisheng Zhong",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c85",
          "name": "Ziqin Wei",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c86",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c87",
          "name": "Chi-Wing Fu",
          "hidden": false
        },
        {
          "_id": "67da1df040371958e1732c88",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:53:43.000Z",
      "submittedOnDailyAt": "2025-03-20T01:38:29.350Z",
      "title": "STEVE: Proceso de certificación en etapas para el entrenamiento de agentes de uso de computadoras",
      "submittedOnDailyBy": {
        "_id": "6418554a0956be7233a1023e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
        "isPro": false,
        "fullname": "zhang yuechen",
        "user": "julianjuaner",
        "type": "user"
      },
      "summary": "El manejo automático de interfaces gráficas por parte de agentes inteligentes es una tarea compleja que requiere tiempo significativo. El avance reciente en escalado de datos ha promovido la entrenamiento de agentes de uso computacional utilizando conjuntos de instrucciones ampliados, pero para entrenarlos con clonación de acciones se necesita gran cantidad de tráfico de alta calidad. Para satisfacer estas necesidades, hemos diseñado STEVE, un método de entrenamiento de agentes de uso computacional. Primero, establecemos un conjunto de instrucciones ampliado para un agente de uso computacional y recopilamos datos de tráfico utilizando subóptimos agentes. GPT-4o verifica la precisión de cada paso del tráfico basándose en pantallas de ejecución y asigna etiquetas binarias a cada paso. Finalmente, utilizamos la optimización de Kahneman y Tversky para optimizar los agentes con base en etiquetas de pasos binarios. Los experimentos extendidos han demostrado que nuestros agentes son superiores a los ajustados de manera normal, utilizando tanto acciones positivas como negativas dentro del tráfico. Además, STEVE ofrece gran eficiencia y reducción de costos, lo que permite entrenar un modelo de lenguaje visual de 7B en agentes de uso computacional y alcanzar un rendimiento líder en entornos difíciles como WinAgentArena. El código y los datos están disponibles en https://github.com/FanbinLu/STEVE.",
      "upvotes": 8,
      "discussionId": "67da1df240371958e1732d2f",
      "githubRepo": "https://github.com/FanbinLu/STEVE",
      "ai_keywords": [
        "behavior cloning",
        "trajectory data",
        "suboptimal agents",
        "GPT-4o",
        "step verification pipeline",
        "correctness verification",
        "Kahneman and Tversky Optimization",
        "positive actions",
        "negative actions",
        "vision-language model",
        "computer-use agent",
        "live desktop environment",
        "WinAgentArena"
      ]
    },
    "publishedAt": "2025-03-16T10:53:43.000Z",
    "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training",
    "summary": "Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12532.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6418554a0956be7233a1023e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
      "fullname": "zhang yuechen",
      "name": "julianjuaner",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15264",
      "authors": [
        {
          "_id": "67dbbe8fafd5251fc6b55730",
          "name": "Hengrui Kang",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55731",
          "name": "Siwei Wen",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55732",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55733",
          "name": "Junyan Ye",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55734",
          "name": "Weijia Li",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55735",
          "name": "Peilin Feng",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55736",
          "name": "Baichuan Zhou",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55737",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55738",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b55739",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67dbbe8fafd5251fc6b5573a",
          "name": "Conghui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T14:37:21.000Z",
      "submittedOnDailyAt": "2025-03-20T05:54:43.430Z",
      "title": "Region: Aprendizaje, fundamentos y explicación para detección de imágenes sintéticas",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "El rápido desarrollo de las tecnologías generativas muestra una dualidad. Provide convenient and powerful tools, but also harbors significant social concerns. Actual métodos de detección de imágenes sintéticas suelen carecer de la posibilidad de interpretación textual. Además, se centran excesivamente en la detección de manipulaciones de imágenes, y suelen enfrentar dificultades debido a generadores no actualizados y una falta de anotaciones detalladas. En este artículo, presentamos un dataset de alta calidad y diverso que incluye 12,236 imágenes sintéticas completas. Esto caracteriza las 4 tipos de contenido de imágenes, 3 categorías de efectos, anotaciones de nivel de píxel, interpretación textual detallada y anotaciones específicas de categorías de efectos. Además, proponemos un marco de análisis de imágenes falsas basado en un grande modelo de lenguaje multimodal (MLLM) llamado LEGION (Learning to Ground and explain for Synthetic Image detectiON), que integra la detección, segmentación y interpretación de efectos. Con esta capacidad, expandimos a LEGION como controlador para integrarlo en la cadena de producción de imágenes para guiar la generación de imágenes de alta calidad y más realistas. Los experimentos expandidos muestran que LEGION supera los métodos actuales en múltiples marcos de referencia, y supera a los mejores expertos en SynthScars con un mIoU de 3.31% y un F1 score de 7.75%. Además, las imágenes seleccionadas generadas bajo la guía de LEGION muestran un mayor acuerdo con la preferencia humana. El código, el modelo y el dataset se liberan.",
      "upvotes": 6,
      "discussionId": "67dbbe92afd5251fc6b55825",
      "projectPage": "https://opendatalab.github.io/LEGION",
      "githubRepo": "https://github.com/opendatalab/LEGION",
      "ai_keywords": [
        "SynthScars",
        "LEGION",
        "multimodal large language model",
        "image forgery analysis framework",
        "artifact detection",
        "segmentation",
        "explanation",
        "mIoU",
        "F1 score"
      ]
    },
    "publishedAt": "2025-03-19T10:37:21.000Z",
    "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection",
    "summary": "The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15264.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14505",
      "authors": [
        {
          "_id": "67db13f71956dcedf0b4d357",
          "name": "Susung Hong",
          "hidden": false
        },
        {
          "_id": "67db13f71956dcedf0b4d358",
          "name": "Ira Kemelmacher-Shlizerman",
          "hidden": false
        },
        {
          "_id": "67db13f71956dcedf0b4d359",
          "name": "Brian Curless",
          "hidden": false
        },
        {
          "_id": "67db13f71956dcedf0b4d35a",
          "name": "Steven M. Seitz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:59:58.000Z",
      "submittedOnDailyAt": "2025-03-20T02:39:46.392Z",
      "title": "MusicInfuser: Método para expandir el auditorio y bailar en películas que incluyen música",
      "submittedOnDailyBy": {
        "_id": "635a6dd21668c4ead3ed19fa",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg",
        "isPro": false,
        "fullname": "Susung Hong",
        "user": "susunghong",
        "type": "user"
      },
      "summary": "MusicInfuser tiene como objetivo la generación de vídeos de baile de alta calidad sincronizados con trabajos musicales específicos. Mientras intenta diseñar y entrenar un nuevo modelo de vídeo multimodal, propone un enfoque para que modelos de difusión de vídeo se adapten a entradas musicales. Introduciendo críticamente la cros-attention y un adápter de bajo rendimiento, MusicInfuser logra generar vídeos de baile de alta calidad sin necesidad de datos de captura de baile, lo que es diferente de lo que ha sido previamente investigado. Mediante la finejación de los vídeos de baile, MusicInfuser realiza la generación de vídeos de alta calidad dirigida por la música, manteniendo la flexibilidad y la capacidad de generación del modelo básico. Se introduce un marco de evaluación utilizando Video-LLMs para evaluar la calidad de la generación de baile desde varias perspectivas. La página del proyecto y el código están disponibles en https://susunghong.github.io/MusicInfuser.",
      "upvotes": 5,
      "discussionId": "67db13fc1956dcedf0b4d470",
      "ai_keywords": [
        "video diffusion models",
        "multimodal audio-video model",
        "music-video cross-attention",
        "low-rank adapter",
        "dance videos",
        "motion capture data",
        "music-driven video generation",
        "Video-LLMs"
      ]
    },
    "publishedAt": "2025-03-18T13:59:58.000Z",
    "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
    "summary": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14505.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635a6dd21668c4ead3ed19fa",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg",
      "fullname": "Susung Hong",
      "name": "susunghong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12769",
      "authors": [
        {
          "_id": "67d8ded81a1b6ae91f79eb18",
          "name": "Shenghao Fu",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb19",
          "name": "Qize Yang",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1a",
          "name": "Yuan-Ming Li",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1b",
          "name": "Yi-Xing Peng",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1c",
          "name": "Kun-Yu Lin",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1d",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1e",
          "name": "Jian-Fang Hu",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb1f",
          "name": "Xiaohua Xie",
          "hidden": false
        },
        {
          "_id": "67d8ded81a1b6ae91f79eb20",
          "name": "Wei-Shi Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T03:05:31.000Z",
      "submittedOnDailyAt": "2025-03-20T00:48:47.112Z",
      "title": "ViSpeak: Feedback Visualización de Instintos en Películas",
      "submittedOnDailyBy": {
        "_id": "6686044047f2a33570e59e31",
        "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
        "isPro": false,
        "fullname": "Jiaxing Zhao",
        "user": "StarJiaxing",
        "type": "user"
      },
      "summary": "El reciente desarrollo de los grandes modelos de video multimodal (LMMs) se centra principalmente en la comprensión de vídeos offline. Por otro lado, la comprensión de vídeos de streaming presenta grandes desafíos debido a las características temporales, multimodalidad y interactividad. En este estudio, proponemos expandir la visión de la comprensión de vídeos de streaming y proponemos un nuevo desafío llamado \"Visual Instruction Feedback\", que se centra en capacitar a los modelos en reconocer contenido visual y extraer instrucciones. Por ejemplo, cuando un usuario se mueve la mano en un out-of-door, el out-of-door debe identificar el movimiento de la mano para iniciar una conversación. De esta manera, la extracción de instrucciones en el modalidad visual puede significativamente mejorar la interacción entre el out-of-door y el usuario. Para fomentar esta investigación, definimos siete sub-tareas principales relacionadas con el modalidad visual y colectamos un dataset de entrenamiento ViSpeak-Instruct y un benchmark de evaluación ViSpeak-Bench. Además, proponemos el modelo ViSpeak, un LMM de comprensión de vídeos de streaming de nivel GPT-4o, que puede ser utilizado para entrenar las capacidades básicas de feedback de instrucciones visuales y que desempeñará un papel crucial como base para futuras investigaciones.",
      "upvotes": 4,
      "discussionId": "67d8ded91a1b6ae91f79eb5c",
      "ai_keywords": [
        "Large Multi-modal Models (LMMs)",
        "streaming video understanding",
        "Visual Instruction Feedback",
        "visual contents",
        "instructions",
        "gesture recognition",
        "user-agent interactions",
        "subtasks",
        "ViSpeak-Instruct dataset",
        "ViSpeak-Bench",
        "ViSpeak model",
        "GPT-4o-level performance",
        "streaming video understanding benchmarks",
        "finetuning"
      ]
    },
    "publishedAt": "2025-03-16T23:05:31.000Z",
    "title": "ViSpeak: Visual Instruction Feedback in Streaming Videos",
    "summary": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on\noffline video understanding. Instead, streaming video understanding poses great\nchallenges to recent models due to its time-sensitive, omni-modal and\ninteractive characteristics. In this work, we aim to extend the streaming video\nunderstanding from a new perspective and propose a novel task named Visual\nInstruction Feedback in which models should be aware of visual contents and\nlearn to extract instructions from them. For example, when users wave their\nhands to agents, agents should recognize the gesture and start conversations\nwith welcome information. Thus, following instructions in visual modality\ngreatly enhances user-agent interactions. To facilitate research, we define\nseven key subtasks highly relevant to visual modality and collect the\nViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation.\nFurther, we propose the ViSpeak model, which is a SOTA streaming video\nunderstanding LMM with GPT-4o-level performance on various streaming video\nunderstanding benchmarks. After finetuning on our ViSpeak-Instruct dataset,\nViSpeak is equipped with basic visual instruction feedback ability, serving as\na solid baseline for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12769.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6686044047f2a33570e59e31",
      "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
      "fullname": "Jiaxing Zhao",
      "name": "StarJiaxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.11227",
      "authors": [
        {
          "_id": "67da533bb443470b7908a048",
          "user": {
            "_id": "658be7fe135580745c510323",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
            "isPro": false,
            "fullname": "Jian Zhang",
            "user": "VentureZJ",
            "type": "user"
          },
          "name": "Jian Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:13.546Z",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a049",
          "name": "Bifan Wei",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04a",
          "name": "Shihao Qi",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04b",
          "name": "haiping Zhu",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04c",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67da533bb443470b7908a04d",
          "name": "Qika Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T09:23:22.000Z",
      "submittedOnDailyAt": "2025-03-20T06:15:24.085Z",
      "title": "GKG-LLM: Marco de Frame de Construcción de Grafos de Conocimiento Generalizados",
      "submittedOnDailyBy": {
        "_id": "658be7fe135580745c510323",
        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
        "isPro": false,
        "fullname": "Jian Zhang",
        "user": "VentureZJ",
        "type": "user"
      },
      "summary": "La construcción de grafos de conocimiento generalizados (GKG) incluye tres tipos: grafos de conocimiento, grafos de conocimiento de eventos y grafos de conocimiento de sabiduría, y desempeña un rol fundamental en diversas tareas de procesamiento del lenguaje natural. Actualmente, la investigación construye cada uno de estos grafos de manera independiente, ignorando la posibilidad de una integración global. Sin embargo, los desafíos en la desarrollo de un marco unificado para el GKG son impididos por las diferencias propias de cada tarea. En este estudio, proponemos un marco unificado adecuado para la construcción del GKG para resolver estos desafíos. Primero, recopilamos datos de 15 sub-tareas a partir de 29 conjuntos de datos correspondientes a los tres tipos de grafos, clasificandolos como datos de muestra, contra-tareas y datos fuera de distribución (OOD). Luego, proponemos un marco de ajuste micro para un aprendizaje de clúster en tres etapas, inyectando repetidamente la captura de los tres tipos de grafos en modelos grandes de lenguaje. A través de amplios experimentos, nuestro modelo propuesto ha mostrado mejoras en la construcción de todos los tres tipos de grafos, tanto para datos de dominio, OOD como para contra-tareas.",
      "upvotes": 3,
      "discussionId": "67da533db443470b7908a0e6",
      "ai_keywords": [
        "knowledge graph",
        "event knowledge graph",
        "commonsense knowledge graph",
        "natural language processing",
        "unified framework",
        "in-sample data",
        "counter-task data",
        "out-of-distribution data",
        "three-stage curriculum learning fine-tuning framework",
        "Large Language Models"
      ]
    },
    "publishedAt": "2025-03-14T05:23:22.000Z",
    "title": "GKG-LLM: A Unified Framework for Generalized Knowledge Graph\n  Construction",
    "summary": "The construction of Generalized Knowledge Graph (GKG), including knowledge\ngraph, event knowledge graph and commonsense knowledge graph, is fundamental\nfor various natural language processing tasks. Current studies typically\nconstruct these types of graph separately, overlooking holistic insights and\npotential unification that could be beneficial in computing resources and usage\nperspectives. However, a key challenge in developing a unified framework for\nGKG is obstacles arising from task-specific differences. In this study, we\npropose a unified framework for constructing generalized knowledge graphs to\naddress this challenge. First, we collect data from 15 sub-tasks in 29 datasets\nacross the three types of graphs, categorizing them into in-sample,\ncounter-task, and out-of-distribution (OOD) data. Then, we propose a\nthree-stage curriculum learning fine-tuning framework, by iteratively injecting\nknowledge from the three types of graphs into the Large Language Models.\nExtensive experiments show that our proposed model improves the construction of\nall three graph types across in-domain, OOD and counter-task data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11227.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658be7fe135580745c510323",
      "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
      "fullname": "Jian Zhang",
      "name": "VentureZJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13360",
      "authors": [
        {
          "_id": "67d8e21dea26d6d743f2adde",
          "name": "Hai-Long Sun",
          "hidden": false
        },
        {
          "_id": "67d8e21dea26d6d743f2addf",
          "name": "Zhun Sun",
          "hidden": false
        },
        {
          "_id": "67d8e21dea26d6d743f2ade0",
          "name": "Houwen Peng",
          "hidden": false
        },
        {
          "_id": "67d8e21dea26d6d743f2ade1",
          "name": "Han-Jia Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T16:45:12.000Z",
      "submittedOnDailyAt": "2025-03-20T04:52:23.426Z",
      "title": "Utilizando la Condicionamiento Visual para el Lógico de Contexto a Largo Plazo Multi-Tipo",
      "submittedOnDailyBy": {
        "_id": "6623975c728f756224d4b768",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg",
        "isPro": false,
        "fullname": "Allen Sun",
        "user": "Allen8",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de modelos de lenguaje grande (LLMs) ha evolucionado desde el aprendizaje por ejemplo de la cadena de pensamiento (CoT) hasta soluciones avanzadas como OpenAI o1. Durante el proceso de re-implementación de estos modelos, se ha identificado que en tareas multimodales que requieren entradas visuales (por ejemplo, problemas geométricos), los modelos multimodales de lenguaje grande (MLLMs) tienden a centrarse menos en la información visual. Esto implica que, a medida que el proceso lógico avanza, los MLLMs disminuyen su atención a la información visual y se vuelven demasiado dependientes del texto, lo que puede afectar la precisión de sus respuestas.\n\nPara investigar este fenómeno, se realizaron pruebas donde se interrumpieron el proceso lógico y se eliminaron las entradas visuales para verificar su impacto. Específicamente, se interrumpió el proceso lógico y se eliminaron las entradas visuales, y luego se reiniciaron los procesos lógicos con solo texto. En la colección de pruebas hard de MathVista, esta acción resultó en una reducción de aproximadamente 2% en la precisión, pero claramente mostró que el texto domina la producción lógica y influye en la precisión de las respuestas.\n\nPor lo tanto, se propone la estrategia de condicionamiento visual acompañante (TVC). En esta estrategia, se mueve la entrada visual a etapas lógicas importantes y se reducen los tokens visuales excesivos mediante producción dinámica. Esta estrategia ayuda a mantener la atención a los elementos visuales durante el proceso lógico. Nuestro enfoque ha registrado el mejor rendimiento promedio en 5 marcadores de lógica matemática, demostrando que la TVC es efectiva en fortalecer sistemas multimodales de lógica.",
      "upvotes": 1,
      "discussionId": "67d8e21eea26d6d743f2ae50",
      "ai_keywords": [
        "Chain-of-Thought (CoT)",
        "OpenAI o1",
        "Multimodal LLMs (MLLMs)",
        "attention to visual information",
        "text-over-relied outputs",
        "ablate image inputs",
        "long-chain reasoning",
        "MathVista's test-hard subset",
        "Take-along Visual Conditioning (TVC)",
        "critical reasoning stages",
        "dynamic pruning",
        "multimodal reasoning systems"
      ]
    },
    "publishedAt": "2025-03-17T12:45:12.000Z",
    "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning",
    "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13360.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6623975c728f756224d4b768",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg",
      "fullname": "Allen Sun",
      "name": "Allen8",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15055",
      "authors": [
        {
          "_id": "67db9586a2f164ac51f84c72",
          "user": {
            "_id": "641ee9fe632a1ec42caf1fa6",
            "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
            "isPro": false,
            "fullname": "Arina Razmyslovich",
            "user": "lavriz",
            "type": "user"
          },
          "name": "Arina Razmyslovich",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-20T04:13:32.101Z",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c73",
          "name": "Kseniia Murasheva",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c74",
          "name": "Sofia Sedlova",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c75",
          "name": "Julien Capitaine",
          "hidden": false
        },
        {
          "_id": "67db9586a2f164ac51f84c76",
          "name": "Eugene Dmitriev",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T09:46:54.000Z",
      "submittedOnDailyAt": "2025-03-20T02:46:26.189Z",
      "title": "ELTEX: Frámenwrk de Generación de Datos Sínticos Dominado por el Domínio",
      "submittedOnDailyBy": {
        "_id": "641ee9fe632a1ec42caf1fa6",
        "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
        "isPro": false,
        "fullname": "Arina Razmyslovich",
        "user": "lavriz",
        "type": "user"
      },
      "summary": "El Étecks (Efficient LLM Token Extraction) es un marco de dominio dirigido destinado a la generación de datos de entrenamiento de alta calidad y sintéticos en las áreas profesionales. Los modelos de lenguaje de gran escala (LLMs) muestran capacidades generales, pero la falta de datos específicos en dominios como la seguridad cibernética impide mejoras en el rendimiento. El Étecks integra sistemáticamente la extracción de indicadores específicos de dominio y la programación dinámica para lograr un proceso de generación eficiente, manteniendo la conocimiento profesional. Se muestra su eficacia en el contexto de la detección de ataques cibernéticos relacionados con la blockchain. Se están probando combinaciones de datos reales con datos generados mediante el Étecks utilizando el modelo Jema-2B. Finalmente, el modelo extendido por el Étecks alcanza el rendimiento de modelos como GPT-4 en clasificación de clases y corrección de incertidumbre, reduciendo significativamente el uso de recursos computacionales. Se publica un conjunto de datos generados para la detección de ataques cibernéticos en la blockchain, demostrando que la generación de datos sintéticos dirigida por dominio puede superar significativamente las diferencias de rendimiento entre modelos eficientes y grandes arquitecturas.",
      "upvotes": 0,
      "discussionId": "67db958fa2f164ac51f84f51",
      "githubRepo": "https://github.com/1712n/eltex",
      "ai_keywords": [
        "ELTEX",
        "domain-driven framework",
        "high-quality synthetic training data",
        "Large Language Models (LLMs)",
        "cohort indicator extraction",
        "dynamic prompting",
        "critical domain knowledge",
        "blockchain-related cyberattack detection",
        "Gemma-2B",
        "performance competitive",
        "GPT-4",
        "standard classification metrics",
        "uncertainty calibration",
        "computational resources",
        "synthetic dataset",
        "social media texts",
        "domain-driven synthetic data generation",
        "performance gap",
        "resource-efficient models",
        "larger architectures"
      ]
    },
    "publishedAt": "2025-03-19T05:46:54.000Z",
    "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation",
    "summary": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641ee9fe632a1ec42caf1fa6",
      "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
      "fullname": "Arina Razmyslovich",
      "name": "lavriz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]