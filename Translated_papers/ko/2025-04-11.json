[
  {
    "paper": {
      "id": "2504.07491",
      "authors": [
        {
          "_id": "67f8a3db7de2391a06a3b2e0",
          "name": "Kimi Team",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e1",
          "name": "Angang Du",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e2",
          "name": "Bohong Yin",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e3",
          "user": {
            "_id": "67503f270dfe827c4068a408",
            "avatarUrl": "/avatars/4591c8229c7815bfd6dc4b98aea85ca8.svg",
            "isPro": false,
            "fullname": "Bowei Xing",
            "user": "xingbowei",
            "type": "user"
          },
          "name": "Bowei Xing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:27:42.989Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e4",
          "name": "Bowen Qu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e5",
          "name": "Bowen Wang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e6",
          "name": "Cheng Chen",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e7",
          "user": {
            "_id": "644ce4e416703fd670260e2e",
            "avatarUrl": "/avatars/db43b13c6913af31cc97f5be7bf30091.svg",
            "isPro": false,
            "fullname": "Chenlin Zhang",
            "user": "tzzcl",
            "type": "user"
          },
          "name": "Chenlin Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:30:16.472Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e8",
          "user": {
            "_id": "64c21fb42426d683e56b42bf",
            "avatarUrl": "/avatars/60359fe204e32af831d701d2975c4599.svg",
            "isPro": false,
            "fullname": "Du",
            "user": "DuChenZhuang",
            "type": "user"
          },
          "name": "Chenzhuang Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:30:30.673Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2e9",
          "name": "Chu Wei",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2ea",
          "user": {
            "_id": "5eefd87c5e979253a010eee5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1603575136094-5eefd87c5e979253a010eee5.jpeg",
            "isPro": false,
            "fullname": "Congcong Wang",
            "user": "congcongwang",
            "type": "user"
          },
          "name": "Congcong Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:40:18.553Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2eb",
          "name": "Dehao Zhang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2ec",
          "name": "Dikang Du",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2ed",
          "user": {
            "_id": "67652998288b8433a92f3c43",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yJCzGJ8gl6JyXRc7A9IeI.png",
            "isPro": false,
            "fullname": "wang",
            "user": "dongliangwang",
            "type": "user"
          },
          "name": "Dongliang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:40:52.991Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2ee",
          "user": {
            "_id": "6331606f18711776b4655e67",
            "avatarUrl": "/avatars/1479c2ca743b9f92d845b0ed23fcd07b.svg",
            "isPro": false,
            "fullname": "Enming Yuan",
            "user": "EnmingYuan",
            "type": "user"
          },
          "name": "Enming Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:41:01.062Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2ef",
          "user": {
            "_id": "67aed930cc96f87ce3c3132f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/JDrhmbCRcuCtKir7i9z9n.png",
            "isPro": false,
            "fullname": "Lu",
            "user": "Enzhe",
            "type": "user"
          },
          "name": "Enzhe Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:41:12.500Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f0",
          "name": "Fang Li",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f1",
          "user": {
            "_id": "6343d01a08c017b2c042305d",
            "avatarUrl": "/avatars/790c4104d80da9887d481f9efb494d81.svg",
            "isPro": false,
            "fullname": "Flood Sung",
            "user": "floodsung",
            "type": "user"
          },
          "name": "Flood Sung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:24:56.624Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f2",
          "name": "Guangda Wei",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f3",
          "user": {
            "_id": "63b4c71758f367a212c4f9ef",
            "avatarUrl": "/avatars/d61736e0ae8b333a7c24eb411378698c.svg",
            "isPro": false,
            "fullname": "Lai",
            "user": "Guokun",
            "type": "user"
          },
          "name": "Guokun Lai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:41:40.586Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f4",
          "name": "Han Zhu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f5",
          "user": {
            "_id": "67bdb4ff599d450529afecf4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/clUC8MtK-qlVAfJQ7v99H.png",
            "isPro": false,
            "fullname": "Hao Ding",
            "user": "HaoDing",
            "type": "user"
          },
          "name": "Hao Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:41:51.300Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f6",
          "name": "Hao Hu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f7",
          "user": {
            "_id": "64ec364e7e2ec711a7601cde",
            "avatarUrl": "/avatars/6ba47d496586de65df183f056d35982b.svg",
            "isPro": false,
            "fullname": "Hao Yang",
            "user": "hayayanghao",
            "type": "user"
          },
          "name": "Hao Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:24:58.807Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f8",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2f9",
          "user": {
            "_id": "63047ed2412a1b9d381b09c9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg",
            "isPro": true,
            "fullname": "Haoning Wu, Teo",
            "user": "teowu",
            "type": "user"
          },
          "name": "Haoning Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:01.006Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2fa",
          "user": {
            "_id": "67f7ef911f10ddb81f2d6d3d",
            "avatarUrl": "/avatars/6d87cde1056a80f8effbc21b4949e690.svg",
            "isPro": false,
            "fullname": "Haotian Yao",
            "user": "xdedmyao",
            "type": "user"
          },
          "name": "Haotian Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:42:00.040Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2fb",
          "user": {
            "_id": "64c206a3fd4d5966b453ed85",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c206a3fd4d5966b453ed85/NemBrHcAJFm2ws_VQG8ia.jpeg",
            "isPro": false,
            "fullname": "Haoyu Lu",
            "user": "Nealeon",
            "type": "user"
          },
          "name": "Haoyu Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:42:25.550Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2fc",
          "name": "Heng Wang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2fd",
          "user": {
            "_id": "62728f4f6253fe2068da1021",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
            "isPro": false,
            "fullname": "Hongcheng Gao",
            "user": "HongchengGao",
            "type": "user"
          },
          "name": "Hongcheng Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:42:35.087Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2fe",
          "user": {
            "_id": "61860e1258cb1f8c362f9441",
            "avatarUrl": "/avatars/8dbc8209ad0d918453c1ffacc8f61e7f.svg",
            "isPro": false,
            "fullname": "Huabin Zheng",
            "user": "zhenghuabin",
            "type": "user"
          },
          "name": "Huabin Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:42:42.807Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b2ff",
          "user": {
            "_id": "64f83e01d493d8b0d2ab4cd3",
            "avatarUrl": "/avatars/788d42871df1be2c9b79b2916de3e4d0.svg",
            "isPro": false,
            "fullname": "Jiaming Li",
            "user": "blabluble",
            "type": "user"
          },
          "name": "Jiaming Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:42:50.314Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b300",
          "user": {
            "_id": "6404982cad54665351d7c1e0",
            "avatarUrl": "/avatars/8fb6d01802cbd4a1cbb7f6a0d83faa3a.svg",
            "isPro": false,
            "fullname": "jianlin su",
            "user": "bojone",
            "type": "user"
          },
          "name": "Jianlin Su",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:42:57.607Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b301",
          "user": {
            "_id": "63be6bf6da08ed0544f1eb7a",
            "avatarUrl": "/avatars/19b5be6d3296da402d8822e51d6376e2.svg",
            "isPro": false,
            "fullname": "jianzhouWang",
            "user": "jianzhouWang",
            "type": "user"
          },
          "name": "Jianzhou Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:43:04.777Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b302",
          "name": "Jiaqi Deng",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b303",
          "name": "Jiezhong Qiu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b304",
          "name": "Jin Xie",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b305",
          "name": "Jinhong Wang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b306",
          "name": "Jingyuan Liu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b307",
          "name": "Junjie Yan",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b308",
          "name": "Kun Ouyang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b309",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b30a",
          "name": "Lin Sui",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b30b",
          "name": "Longhui Yu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b30c",
          "user": {
            "_id": "6309d1e6a58e1be42eb6eb5e",
            "avatarUrl": "/avatars/2a7a437e801389a9f79b49c164f85817.svg",
            "isPro": false,
            "fullname": "dong",
            "user": "mengnan",
            "type": "user"
          },
          "name": "Mengfan Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:43:53.204Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b30d",
          "name": "Mengnan Dong",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b30e",
          "name": "Nuo Xu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b30f",
          "name": "Pengyu Cheng",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b310",
          "name": "Qizheng Gu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b311",
          "name": "Runjie Zhou",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b312",
          "name": "Shaowei Liu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b313",
          "name": "Sihan Cao",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b314",
          "name": "Tao Yu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b315",
          "name": "Tianhui Song",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b316",
          "name": "Tongtong Bai",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b317",
          "name": "Wei Song",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b318",
          "name": "Weiran He",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b319",
          "user": {
            "_id": "63c1052e894342c896483a84",
            "avatarUrl": "/avatars/ef99a3c4487b2e3d4c4a266e77b42d15.svg",
            "isPro": false,
            "fullname": "Weixiao Huang",
            "user": "ztxcydzz",
            "type": "user"
          },
          "name": "Weixiao Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:55:57.280Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b31a",
          "name": "Weixin Xu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b31b",
          "user": {
            "_id": "66276d360601587f0befb9fd",
            "avatarUrl": "/avatars/467c847cad08783ee8a47af90c65615d.svg",
            "isPro": false,
            "fullname": "Xiaokun Yuan",
            "user": "kx233333",
            "type": "user"
          },
          "name": "Xiaokun Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:21:16.827Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b31c",
          "user": {
            "_id": "67593edcd3ac91d6238a4901",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kN7Mf53FamPIi29hLxiII.png",
            "isPro": false,
            "fullname": "Xingcheng Yao",
            "user": "sxyao",
            "type": "user"
          },
          "name": "Xingcheng Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:23:35.004Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b31d",
          "name": "Xingzhe Wu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b31e",
          "name": "Xinxing Zu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b31f",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b320",
          "user": {
            "_id": "63eb133a91a1b8ec4fbc4c2f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63eb133a91a1b8ec4fbc4c2f/dmaD56RAqkovB4izizv5m.png",
            "isPro": false,
            "fullname": "Xinyuan Wang",
            "user": "buaa42wxy",
            "type": "user"
          },
          "name": "Xinyuan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:56:14.263Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b321",
          "name": "Y. Charles",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b322",
          "name": "Yan Zhong",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b323",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b324",
          "name": "Yangyang Hu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b325",
          "name": "Yanru Chen",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b326",
          "name": "Yejie Wang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b327",
          "name": "Yibo Liu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b328",
          "user": {
            "_id": "64a139c098fad0c8a5a627a4",
            "avatarUrl": "/avatars/6eb508abd827d4a4f5abb6b24155b22d.svg",
            "isPro": false,
            "fullname": "Yibo Miao",
            "user": "instro",
            "type": "user"
          },
          "name": "Yibo Miao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:23:15.494Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b329",
          "name": "Yidao Qin",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b32a",
          "name": "Yimin Chen",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b32b",
          "name": "Yiping Bao",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b32c",
          "name": "Yiqin Wang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b32d",
          "name": "Yongsheng Kang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b32e",
          "user": {
            "_id": "6489761dcaea79f577897f98",
            "avatarUrl": "/avatars/8f56dc9c08dc2b672555602d68509a03.svg",
            "isPro": false,
            "fullname": "Yuanxin Liu",
            "user": "lyx97",
            "type": "user"
          },
          "name": "Yuanxin Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:56:23.814Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b32f",
          "user": {
            "_id": "6340f31fb78ed99eab04ce33",
            "avatarUrl": "/avatars/2e7fcbf0233bdc0bc9a3f4603fd8bf90.svg",
            "isPro": false,
            "fullname": "Du",
            "user": "Yulun",
            "type": "user"
          },
          "name": "Yulun Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:23:06.421Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b330",
          "name": "Yuxin Wu",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b331",
          "user": {
            "_id": "67127a470a82509269d738ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/M9qLmI3P6dT2FIwEPFJq0.png",
            "isPro": false,
            "fullname": "yuzhi wang",
            "user": "vin-tage",
            "type": "user"
          },
          "name": "Yuzhi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:22:52.379Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b332",
          "name": "Yuzi Yan",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b333",
          "user": {
            "_id": "64409d69518271b0d1c033a6",
            "avatarUrl": "/avatars/c79eb36c4ad96286afda834e260a1c09.svg",
            "isPro": false,
            "fullname": "zhouzaida",
            "user": "zhouzaida",
            "type": "user"
          },
          "name": "Zaida Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:22:41.896Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b334",
          "name": "Zhaowei Li",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b335",
          "user": {
            "_id": "662c6e8352e194d5d44d873c",
            "avatarUrl": "/avatars/385a5cc7299faf2f61ccbabedd827f29.svg",
            "isPro": false,
            "fullname": "Zhejun Jiang",
            "user": "Skewed",
            "type": "user"
          },
          "name": "Zhejun Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:22:27.049Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b336",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b337",
          "user": {
            "_id": "64bf74154d2052b1aa5ca6d9",
            "avatarUrl": "/avatars/7aa6f2952cdbc20cfa758fdd905f06a6.svg",
            "isPro": false,
            "fullname": "ZHILIN YANG",
            "user": "bruceyannnn",
            "type": "user"
          },
          "name": "Zhilin Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:22:17.469Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b338",
          "name": "Zhiqi Huang",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b339",
          "user": {
            "_id": "66561c5a8ec33cfd8c724cf1",
            "avatarUrl": "/avatars/88ce5bc8ce2d7b1ca97a33d7863bf184.svg",
            "isPro": false,
            "fullname": "Zihao Huang",
            "user": "EdwardHzh",
            "type": "user"
          },
          "name": "Zihao Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T07:43:37.864Z",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b33a",
          "name": "Zijia Zhao",
          "hidden": false
        },
        {
          "_id": "67f8a3db7de2391a06a3b33b",
          "name": "Ziwei Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/p-kLtTC-gIyuAzN76GIt9.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/hPa3VKuztFbrcKKj8LQDy.jpeg"
      ],
      "publishedAt": "2025-04-10T06:48:26.000Z",
      "submittedOnDailyAt": "2025-04-11T03:40:59.047Z",
      "title": "김이-VL 기술보고서\n\n(Note: The original text \"Kimi-VL 技术报告\" has been translated to \"김이-VL 기술보고서\" in Korean, maintaining the professional and accurate tone as requested.)",
      "submittedOnDailyBy": {
        "_id": "63047ed2412a1b9d381b09c9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg",
        "isPro": true,
        "fullname": "Haoning Wu, Teo",
        "user": "teowu",
        "type": "user"
      },
      "summary": "Kimi-VL는 효율적인 오픈 소스 Mixture-of-Experts (MoE) 시각 언어 모델 (VLM)입니다. 이 모델은 발전된 다형 논리, 긴 텍스트 이해, 강력한 에이전트 능력 제공하며, 언어 해석기 내에서 2.8B 파라미터를 활성화합니다 (Kimi-VL-A3B). Kimi-VL은 어려운 영역에서도 강력한 성능을 나타냅니다: 일반적인 VLM과 비교하여, Kimi-VL은 다턴 에이전트 태스크 (예: OSWorld)에서 뛰어난 성능을 보입니다, 플래그 모델과 대결합니다. 또한 다양한 어려운 시각 언어 태스크에도 뛰어난 능력을 나타냅니다, 대학 수준의 이미지와 비디오 이해, OCR, 수학 논리, 다각 이미지 이해를 포함합니다. 비교 평가에서, GPT-4o-mini, Qwen2.5-VL-7B, Gemma-3-12B-IT과 같은 선진적인 오픈 소스 VLM과 대결하며, 여러 관련 분야에서는 GPT-4o를 초과합니다. Kimi-VL은 긴 텍스트 처리와 명확한 인식에도 발전하고 있습니다. 128K 확장 텍스트 윈도우를 가지는 데 thanks to MoonViT, Kimi-VL은 다양한 긴 입력을 처리할 수 있으며, LongVideoBench에서 64.5, MMLongBench-Doc에서 35.1의 놀라운 점수를 달성합니다. MoonViT, 즉 원생 해상도의 시각 인코더를 가지는 데 thanks to MoonViT, Kimi-VL은 고해상도 시각 입력을 볼 수 있으며 이해할 수 있습니다, InfoVQA에서 83.2, ScreenSpot-Pro에서 34.5의 점수를 달성하며, 일반적인 태스크에서 낮은 컴퓨팅 비용 유지합니다. Kimi-VL을 기반으로, 긴 텍스트 논리의 발전 버전을 소개합니다: Kimi-VL-Thinking. 긴 코어 오브젝트의 긴 텍스트 (CoT)의 서브젝트 감독 조정 (SFT)과 강화 학습 (RL)을 통해 개발됩니다. 이 모델은 강력한 장기 논리 능력을 나타내며, MMMU에서 61.7, MathVision에서 36.8, MathVista에서 71.3의 점수를 달성하며, 2.8B 압축 프레임워크의 LLM 파라미터를 유지하며, 효율적인 다형 논리 모델의 새로운 기준을 세팅합니다. 코드와 모델은 아래 URL에서 공개됩니다: https://github.com/MoonshotAI/Kimi-VL.",
      "upvotes": 53,
      "discussionId": "67f8a3de7de2391a06a3b420",
      "githubRepo": "https://github.com/MoonshotAI/Kimi-VL"
    },
    "publishedAt": "2025-04-10T02:48:26.000Z",
    "title": "Kimi-VL Technical Report",
    "summary": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/p-kLtTC-gIyuAzN76GIt9.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/hPa3VKuztFbrcKKj8LQDy.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07491.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63047ed2412a1b9d381b09c9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg",
      "fullname": "Haoning Wu, Teo",
      "name": "teowu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 45
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07956",
      "authors": [
        {
          "_id": "67f886f1a0a44c8f05b7a124",
          "user": {
            "_id": "66553907965ea394ee85f04c",
            "avatarUrl": "/avatars/e0adfe83e9ec51b118e359ddc5c37a1b.svg",
            "isPro": false,
            "fullname": "qyk",
            "user": "yukunqi",
            "type": "user"
          },
          "name": "Yukun Qi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:24:01.756Z",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a125",
          "name": "Yiming Zhao",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a126",
          "name": "Yu Zeng",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a127",
          "user": {
            "_id": "672046bd4b2e5a664aca3084",
            "avatarUrl": "/avatars/6635c617216eca67b38b64deec634fee.svg",
            "isPro": false,
            "fullname": "xikun bao",
            "user": "ChthollyTree",
            "type": "user"
          },
          "name": "Xikun Bao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:24:37.813Z",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a128",
          "user": {
            "_id": "67dc162ec8c00778e8689f42",
            "avatarUrl": "/avatars/7abcd41d4d466ab751f22048050d7f53.svg",
            "isPro": false,
            "fullname": "Wenxuan Huang",
            "user": "Osilly",
            "type": "user"
          },
          "name": "Wenxuan Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:24:49.236Z",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a129",
          "user": {
            "_id": "64b02ec0e5000ae8a572ced5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
            "isPro": false,
            "fullname": "Lin Chen",
            "user": "Lin-Chen",
            "type": "user"
          },
          "name": "Lin Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:22.829Z",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a12a",
          "user": {
            "_id": "64892d31cbda0d1cdb956897",
            "avatarUrl": "/avatars/3cdafe03a8295124636347d15a099aaf.svg",
            "isPro": false,
            "fullname": "Zehui Chen",
            "user": "lovesnowbest",
            "type": "user"
          },
          "name": "Zehui Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:24:56.718Z",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a12b",
          "name": "Jie Zhao",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a12c",
          "user": {
            "_id": "660cc64f1b14f691990c0ea0",
            "avatarUrl": "/avatars/f172d3120b22d745a41cc3f2eb499ce6.svg",
            "isPro": false,
            "fullname": "Zhongang Qi",
            "user": "phoenixqza",
            "type": "user"
          },
          "name": "Zhongang Qi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:25:12.007Z",
          "hidden": false
        },
        {
          "_id": "67f886f1a0a44c8f05b7a12d",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:59:03.000Z",
      "submittedOnDailyAt": "2025-04-11T01:36:38.929Z",
      "title": "VCR-Bench: 영상의 Chain-of-Thought Reasoning를 평가하기 위한 엄격한 프레임워크",
      "submittedOnDailyBy": {
        "_id": "64b02ec0e5000ae8a572ced5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
        "isPro": false,
        "fullname": "Lin Chen",
        "user": "Lin-Chen",
        "type": "user"
      },
      "summary": "連鎖コンシティティ（Chain-of-Thought, CoT）논리의 발전은 대규모 언어 모델（LLMs）과 대규모 시각 언어 모델（LVLMs）의 능력을 크게 향상시켰습니다. 그러나, 엄격한 평가 프레임워크가 비디오의 CoT 논리에 대한 존재하지 않습니다. 현재의 비디오 벤치마크는 논리 과정의 평가를 충분히 할 수 없고, 실패가 시각 지식의 결함이나 논리 능력의 결함이나와 같은 원인을 명확히 할 수 없습니다. 이에 따라, VCR-Bench라는 새로운 벤치마크를 소개합니다. 이 벤치마크는 LVLMs의 비디오 CoT 논리 능력을 전면적으로 평가하기 위해 설계되었습니다. VCR-Bench는 859개의 비디오 콘텐츠와 다양한 시간 길이를 기록하고 있으며, 1,034개의 고품질의 질문・답변 쌍을 포함합니다. 각 쌍은 단계별 CoT 논리를 손으로 注記되어 있으며, 각 단계는 시각 지식과 논리 능력과의 연관성을 나타내는 태그를 붙였습니다. 또한, 7개의 다른 태스크 차원을 설계하고, 단계별 CoT 논리를 기반으로 CoT 점수를 제출합니다. VCR-Bench에서 수행된 확장 실험은 현재의 LVLMs에 대한 큰 한계를 밝혀줍니다. 가장 우수한 모델인 o1도 CoT 점수가 62.8%, 정확도가 56.7%였지만, 많은 모델은 40% 미만입니다. 실험은 많은 모델이 시각 지식보다 논리 단계에서 낮은 점수를 보여주고, LVLMs가 복잡한 비디오 논리에서 시간 공간 정보 처리의 핵심 요소인 박스를 밝혀줍니다. CoT 점수와 정확도의 강한 양의 상관관계는 평가 프레임워크의 유효성을 확인하고, 복잡한 비디오 논리 태스크의 해결에 CoT 논리의 중요성을 강조합니다. VCR-Bench는 표준화된 평가 프레임워크로 활용될 수 있고, 복잡한 비디오 논리 태스크의 실제 결점을 밝혀줍니다.",
      "upvotes": 30,
      "discussionId": "67f886f7a0a44c8f05b7a282",
      "projectPage": "https://vlm-reasoning.github.io/VCR-Bench/",
      "githubRepo": "https://github.com/zhishuifeiqian/VCR-Bench"
    },
    "publishedAt": "2025-04-10T13:59:03.000Z",
    "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning",
    "summary": "The advancement of Chain-of-Thought (CoT) reasoning has significantly\nenhanced the capabilities of large language models (LLMs) and large\nvision-language models (LVLMs). However, a rigorous evaluation framework for\nvideo CoT reasoning remains absent. Current video benchmarks fail to adequately\nassess the reasoning process and expose whether failures stem from deficiencies\nin perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a\nnovel benchmark designed to comprehensively evaluate LVLMs' Video\nChain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos\nspanning a variety of video content and durations, along with 1,034\nhigh-quality question-answer pairs. Each pair is manually annotated with a\nstepwise CoT rationale, where every step is tagged to indicate its association\nwith the perception or reasoning capabilities. Furthermore, we design seven\ndistinct task dimensions and propose the CoT score to assess the entire CoT\nprocess based on the stepwise tagged CoT rationals. Extensive experiments on\nVCR-Bench highlight substantial limitations in current LVLMs. Even the\ntop-performing model, o1, only achieves a 62.8% CoT score and an 56.7%\naccuracy, while most models score below 40%. Experiments show most models score\nlower on perception than reasoning steps, revealing LVLMs' key bottleneck in\ntemporal-spatial information processing for complex video reasoning. A robust\npositive correlation between the CoT score and accuracy confirms the validity\nof our evaluation framework and underscores the critical role of CoT reasoning\nin solving complex video reasoning tasks. We hope VCR-Bench to serve as a\nstandardized evaluation framework and expose the actual drawbacks in complex\nvideo reasoning task.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07956.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b02ec0e5000ae8a572ced5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
      "fullname": "Lin Chen",
      "name": "Lin-Chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 87
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07957",
      "authors": [
        {
          "_id": "67f8914516d43f88b3177ec1",
          "user": {
            "_id": "646cd947da8e99940b6e55cf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
            "isPro": false,
            "fullname": "Shengyuan Ding",
            "user": "ChrisDing1105",
            "type": "user"
          },
          "name": "Shengyuan Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:25:40.694Z",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec2",
          "name": "Shenxi Wu",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec3",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec4",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:05.800Z",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec5",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:26:11.929Z",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec6",
          "user": {
            "_id": "67c0849ee08c178ef8d4e05c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mQ6VdnjZnRhb0H_waPclo.png",
            "isPro": false,
            "fullname": "Xiaoyi Dong",
            "user": "sweetFruit",
            "type": "user"
          },
          "name": "Xiaoyi Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:26:18.423Z",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec7",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec8",
          "user": {
            "_id": "65000bef18830fabea469fdd",
            "avatarUrl": "/avatars/b320c77dfad039d9f9c54127f610d44f.svg",
            "isPro": false,
            "fullname": "Cao Yuhang",
            "user": "yhcao",
            "type": "user"
          },
          "name": "Yuhang Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:26:51.425Z",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177ec9",
          "user": {
            "_id": "636317ed80c1a705a6eff396",
            "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
            "isPro": false,
            "fullname": "Dahua Lin",
            "user": "lindahua",
            "type": "user"
          },
          "name": "Dahua Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:26:59.045Z",
          "hidden": false
        },
        {
          "_id": "67f8914516d43f88b3177eca",
          "user": {
            "_id": "64638c4d51fa6e63060521b5",
            "avatarUrl": "/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg",
            "isPro": false,
            "fullname": "JIaqi",
            "user": "Jiaqiwang",
            "type": "user"
          },
          "name": "Jiaqi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:27:20.727Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:59:12.000Z",
      "submittedOnDailyAt": "2025-04-11T02:20:10.515Z",
      "title": "MM-IFEngine: 멀티모델 대상의 지시에 따라의指南",
      "submittedOnDailyBy": {
        "_id": "646cd947da8e99940b6e55cf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
        "isPro": false,
        "fullname": "Shengyuan Ding",
        "user": "ChrisDing1105",
        "type": "user"
      },
      "summary": "The Instruction Following (IF) ability measures how well Multi-modal Large\nLanguage Models (MLLMs) understand exactly what users are telling them and\nwhether they are doing it right. Existing multimodal instruction following\ntraining data is scarce, the benchmarks are simple with atomic instructions,\nand the evaluation strategies are imprecise for tasks demanding exact output\nconstraints. To address this, we present MM-IFEngine, an effective pipeline to\ngenerate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields\nlarge-scale, diverse, and high-quality training data MM-IFInstruct-23k, which\nis suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for\nDirect Preference Optimization (DPO). We further introduce MM-IFEval, a\nchallenging and diverse multi-modal instruction-following benchmark that\nincludes (1) both compose-level constraints for output responses and\nperception-level constraints tied to the input images, and (2) a comprehensive\nevaluation pipeline incorporating both rule-based assessment and judge model.\nWe conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on\nMM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF\nbenchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval\n(+12.3%). The full data and evaluation code will be released on\nhttps://github.com/SYuan03/MM-IFEngine.",
      "upvotes": 22,
      "discussionId": "67f8914816d43f88b3177fa7",
      "projectPage": "https://syuan03.github.io/MM-IFEngine/",
      "githubRepo": "https://github.com/SYuan03/MM-IFEngine"
    },
    "publishedAt": "2025-04-10T13:59:12.000Z",
    "title": "MM-IFEngine: Towards Multimodal Instruction Following",
    "summary": "The Instruction Following (IF) ability measures how well Multi-modal Large\nLanguage Models (MLLMs) understand exactly what users are telling them and\nwhether they are doing it right. Existing multimodal instruction following\ntraining data is scarce, the benchmarks are simple with atomic instructions,\nand the evaluation strategies are imprecise for tasks demanding exact output\nconstraints. To address this, we present MM-IFEngine, an effective pipeline to\ngenerate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields\nlarge-scale, diverse, and high-quality training data MM-IFInstruct-23k, which\nis suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for\nDirect Preference Optimization (DPO). We further introduce MM-IFEval, a\nchallenging and diverse multi-modal instruction-following benchmark that\nincludes (1) both compose-level constraints for output responses and\nperception-level constraints tied to the input images, and (2) a comprehensive\nevaluation pipeline incorporating both rule-based assessment and judge model.\nWe conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on\nMM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF\nbenchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval\n(+12.3%). The full data and evaluation code will be released on\nhttps://github.com/SYuan03/MM-IFEngine.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07957.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646cd947da8e99940b6e55cf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
      "fullname": "Shengyuan Ding",
      "name": "ChrisDing1105",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07960",
      "authors": [
        {
          "_id": "67f86e6884277ab58c40ce8a",
          "user": {
            "_id": "6740a5730bb4a675446a80ad",
            "avatarUrl": "/avatars/27c08e33df88e4f73c136da65f2b5adb.svg",
            "isPro": true,
            "fullname": "Zhong-Yu Li",
            "user": "lzyhha",
            "type": "user"
          },
          "name": "Zhong-Yu Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:42.946Z",
          "hidden": false
        },
        {
          "_id": "67f86e6884277ab58c40ce8b",
          "user": {
            "_id": "64a54586c0f13de8e7093314",
            "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg",
            "isPro": false,
            "fullname": "Ruoyi Du",
            "user": "RuoyiDu",
            "type": "user"
          },
          "name": "Ruoyi Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:27:33.519Z",
          "hidden": false
        },
        {
          "_id": "67f86e6884277ab58c40ce8c",
          "user": {
            "_id": "644616965691ca69b0e02e79",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/F2uQXO9SkQwHU6benSwQB.jpeg",
            "isPro": false,
            "fullname": "Juncheng Yan",
            "user": "JonsonYan",
            "type": "user"
          },
          "name": "Juncheng Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:27:41.530Z",
          "hidden": false
        },
        {
          "_id": "67f86e6884277ab58c40ce8d",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "67f86e6884277ab58c40ce8e",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67f86e6884277ab58c40ce8f",
          "user": {
            "_id": "67b299cc6f6dc4376d9e6c76",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UniMpmfOUlyiSOrf47wuT.png",
            "isPro": false,
            "fullname": "Peng Gao",
            "user": "cosumosu25",
            "type": "user"
          },
          "name": "Peng Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:28:33.570Z",
          "hidden": false
        },
        {
          "_id": "67f86e6884277ab58c40ce90",
          "name": "Zhanyu Ma",
          "hidden": false
        },
        {
          "_id": "67f86e6884277ab58c40ce91",
          "user": {
            "_id": "64e496ae0195913c7fa91c66",
            "avatarUrl": "/avatars/23f274f0a3b4ef6ded35205df9bfb564.svg",
            "isPro": false,
            "fullname": "chengmingming",
            "user": "mingming8688",
            "type": "user"
          },
          "name": "Ming-Ming Cheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:28:11.683Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6740a5730bb4a675446a80ad/-Krt-Txw86EBaMdaZXyKY.mp4"
      ],
      "publishedAt": "2025-04-10T17:59:42.000Z",
      "submittedOnDailyAt": "2025-04-11T01:51:23.709Z",
      "title": "VisualCloze: 시각적 컨텍스트 학습에 의한 일반적인 이미지 생성 프레임워크",
      "submittedOnDailyBy": {
        "_id": "6740a5730bb4a675446a80ad",
        "avatarUrl": "/avatars/27c08e33df88e4f73c136da65f2b5adb.svg",
        "isPro": true,
        "fullname": "Zhong-Yu Li",
        "user": "lzyhha",
        "type": "user"
      },
      "summary": "최근의 확산 모델의 발전은 다양한 이미지 생성 작업에 큰 진전을 줍니다. 그러나 현재의主流 접근 방식은 특정한 작업에 대응하는 모델을 구축하는 데 초점을 맞추고 있습니다. 이러한 모델들은 다양한 요구를 지원할 때 효율이 제한되어 있습니다. 반면, 일반적인 모델은 이러한 제한을 해결하기 위해 노력하고 있지만, 일반화 가능한 작업 인스트럭션, 적절한 작업 분포, 유니폼화된 아키텍처 설계 등 중요한 문제를 직면하고 있습니다. 이러한 문제를 해결하기 위해, 우리는 VisualCloze라는 일반적인 이미지 생성 프레임워크를 제안하고 있습니다. 이 프레임워크는 다양한 분야의 작업을 지원하고,未见过的 작업에 대한 일반화,未见过的 여러 작업의 통합, 역 생성을 가능하게 합니다. 현재의 방법들은 언어 기반의 작업 인스트럭션을 의존하고 있기 때문에, 작업의 불확실성과 약한 일반화가 발생합니다. 그러나 우리는 모델이 시각적인 시사점으로 작업을 인식할 수 있도록 시각적인 컨텍스트 학습을 통합하고 있습니다. 또한 시각적인 작업 분포의 고유한 희박성은 작업 간의 전파 가능한 지식의 학습을 방해하고 있습니다. 이에대해, 우리는 Graph200K라는 그래프 구조화된 데이터셋을 도입하고, 상호 연관된 여러 작업을 구축하여 작업 밀도와 전파 가능한 지식을 향상시키려고 합니다. 또한, 우리의 통일된 이미지 생성의 공식화는 이미지 채굴과 일관된 목표를 공유하고 있으며, 전처리된 채굴 모델의 강력한 생성적인 선두를 활용할 수 있는 것을 발견하여, 아키텍처를 변경하지 않고 수행할 수 있음을 발견했습니다.",
      "upvotes": 21,
      "discussionId": "67f86e6a84277ab58c40cf08",
      "projectPage": "https://visualcloze.github.io/",
      "githubRepo": "https://github.com/lzyhha/VisualCloze"
    },
    "publishedAt": "2025-04-10T13:59:42.000Z",
    "title": "VisualCloze: A Universal Image Generation Framework via Visual\n  In-Context Learning",
    "summary": "Recent progress in diffusion models significantly advances various image\ngeneration tasks. However, the current mainstream approach remains focused on\nbuilding task-specific models, which have limited efficiency when supporting a\nwide range of different needs. While universal models attempt to address this\nlimitation, they face critical challenges, including generalizable task\ninstruction, appropriate task distributions, and unified architectural design.\nTo tackle these challenges, we propose VisualCloze, a universal image\ngeneration framework, which supports a wide range of in-domain tasks,\ngeneralization to unseen ones, unseen unification of multiple tasks, and\nreverse generation. Unlike existing methods that rely on language-based task\ninstruction, leading to task ambiguity and weak generalization, we integrate\nvisual in-context learning, allowing models to identify tasks from visual\ndemonstrations. Meanwhile, the inherent sparsity of visual task distributions\nhampers the learning of transferable knowledge across tasks. To this end, we\nintroduce Graph200K, a graph-structured dataset that establishes various\ninterrelated tasks, enhancing task density and transferable knowledge.\nFurthermore, we uncover that our unified image generation formulation shared a\nconsistent objective with image infilling, enabling us to leverage the strong\ngenerative priors of pre-trained infilling models without modifying the\narchitectures.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6740a5730bb4a675446a80ad/-Krt-Txw86EBaMdaZXyKY.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07960.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6740a5730bb4a675446a80ad",
      "avatarUrl": "/avatars/27c08e33df88e4f73c136da65f2b5adb.svg",
      "fullname": "Zhong-Yu Li",
      "name": "lzyhha",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07943",
      "authors": [
        {
          "_id": "67f88643f5889583ce0499ae",
          "user": {
            "_id": "64b4eecf2fc8324fcb63b404",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4eecf2fc8324fcb63b404/zGYqYVB4-o-GBMybJ8CDA.png",
            "isPro": false,
            "fullname": "Yunhan Yang",
            "user": "yhyang-myron",
            "type": "user"
          },
          "name": "Yunhan Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:28:45.498Z",
          "hidden": false
        },
        {
          "_id": "67f88643f5889583ce0499af",
          "user": {
            "_id": "6346aaa3f06b237ba4e297b0",
            "avatarUrl": "/avatars/5acb986e993eab1461200f3e9d99d022.svg",
            "isPro": false,
            "fullname": "Yuan-Chen Guo",
            "user": "bennyguo",
            "type": "user"
          },
          "name": "Yuan-Chen Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:28:51.310Z",
          "hidden": false
        },
        {
          "_id": "67f88643f5889583ce0499b0",
          "user": {
            "_id": "638ee900ee7e45e0474a5712",
            "avatarUrl": "/avatars/eadb5ae2fc92bd9af5516acbc8f1bdf0.svg",
            "isPro": false,
            "fullname": "Yukun Huang",
            "user": "KevinHuang",
            "type": "user"
          },
          "name": "Yukun Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:25.048Z",
          "hidden": false
        },
        {
          "_id": "67f88643f5889583ce0499b1",
          "user": {
            "_id": "644dbf6453ad80c6593bf748",
            "avatarUrl": "/avatars/0e170cf2aa8d7f0f3f83e36f06f023f8.svg",
            "isPro": false,
            "fullname": "Zixin Zou",
            "user": "zouzx",
            "type": "user"
          },
          "name": "Zi-Xin Zou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:29:00.746Z",
          "hidden": false
        },
        {
          "_id": "67f88643f5889583ce0499b2",
          "name": "Zhipeng Yu",
          "hidden": false
        },
        {
          "_id": "67f88643f5889583ce0499b3",
          "user": {
            "_id": "64d71083a787c9bc7b9f1238",
            "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
            "isPro": false,
            "fullname": "Yangguang Li",
            "user": "Lp256",
            "type": "user"
          },
          "name": "Yangguang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:29:23.548Z",
          "hidden": false
        },
        {
          "_id": "67f88643f5889583ce0499b4",
          "user": {
            "_id": "638066faf022c8a5803f7eb8",
            "avatarUrl": "/avatars/4cfd699c3f6c5461b12b7dc5e3fe183d.svg",
            "isPro": false,
            "fullname": "Yanpei Cao",
            "user": "pookiefoof",
            "type": "user"
          },
          "name": "Yan-Pei Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:29:32.598Z",
          "hidden": false
        },
        {
          "_id": "67f88643f5889583ce0499b5",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:29:39.026Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:53:31.000Z",
      "submittedOnDailyAt": "2025-04-11T01:33:34.186Z",
      "title": "호로파르트: 3D 파트의 생성형 무시관 분할",
      "submittedOnDailyBy": {
        "_id": "64b4eecf2fc8324fcb63b404",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4eecf2fc8324fcb63b404/zGYqYVB4-o-GBMybJ8CDA.png",
        "isPro": false,
        "fullname": "Yunhan Yang",
        "user": "yhyang-myron",
        "type": "user"
      },
      "summary": "3D 파트 아모드 세그メン테이션 — 3D 모양을 완전한, 세ман틱으로 의미있는 파트를 분해하고, 가려져 있어도 — 3D 콘텐츠의 제작과 이해에 있어 어려운 하지만 중요한 문제입니다. 현재의 3D 파트 세그メン테이션 방법은 보이는 표면 패치만 인식하고, 활용할 범위를 제한하고 있습니다. 2D 아모드 세그メン테이션에 영감을 받아, 이 새로운 태스크를 3D 영역에 도입하고, 실용적인 2단계 접근법을 제안하고, 가려져 있는 3D 기리메트리의 추론, 전체적인 모양의 일관성 유지, 훈련 데이터의 제한된 다양한 모양 처리의 주요 문제를 해결하는 것을 목표로 합니다. 처음으로, 현재의 3D 파트 세그メン테이션을 활용하여 초기적이고 불완전한 파트 세그먼트를 얻습니다. 다음으로, HoloPart라는 새로운 디퓨전 기반 모델을 소개하고, 이러한 세그먼트를 완전한 3D 파트에 완성시킵니다. HoloPart는 촘촘한 파트 기리메트리를 감지하기 위해 지역적 어텐션과 전체적인 모양의 컨텍스트 어텐션을 사용하는 특별한 아키텍처를 사용합니다. ABO와 PartObjaverse-Tiny 데이터 세트를 기반으로 새로운 벤치마크를 도입하고, HoloPart가 가장 선진적인 모양 완성 기법을 크게 초월함을 보여줍니다. HoloPart와 현재의 세그メン테이션 방법을 조합하여, 3D 파트 아모드 세그メン테이션에 우수한 결과를 구현하고, 기리메트리 편집, 애니메이션, 재료 할당의 새로운 가능성을 개척합니다.",
      "upvotes": 20,
      "discussionId": "67f88646f5889583ce049a90",
      "projectPage": "https://vast-ai-research.github.io/HoloPart",
      "githubRepo": "https://github.com/VAST-AI-Research/HoloPart"
    },
    "publishedAt": "2025-04-10T13:53:31.000Z",
    "title": "HoloPart: Generative 3D Part Amodal Segmentation",
    "summary": "3D part amodal segmentation--decomposing a 3D shape into complete,\nsemantically meaningful parts, even when occluded--is a challenging but crucial\ntask for 3D content creation and understanding. Existing 3D part segmentation\nmethods only identify visible surface patches, limiting their utility. Inspired\nby 2D amodal segmentation, we introduce this novel task to the 3D domain and\npropose a practical, two-stage approach, addressing the key challenges of\ninferring occluded 3D geometry, maintaining global shape consistency, and\nhandling diverse shapes with limited training data. First, we leverage existing\n3D part segmentation to obtain initial, incomplete part segments. Second, we\nintroduce HoloPart, a novel diffusion-based model, to complete these segments\ninto full 3D parts. HoloPart utilizes a specialized architecture with local\nattention to capture fine-grained part geometry and global shape context\nattention to ensure overall shape consistency. We introduce new benchmarks\nbased on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart\nsignificantly outperforms state-of-the-art shape completion methods. By\nincorporating HoloPart with existing segmentation techniques, we achieve\npromising results on 3D part amodal segmentation, opening new avenues for\napplications in geometry editing, animation, and material assignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07943.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eecf2fc8324fcb63b404",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4eecf2fc8324fcb63b404/zGYqYVB4-o-GBMybJ8CDA.png",
      "fullname": "Yunhan Yang",
      "name": "yhyang-myron",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07128",
      "authors": [
        {
          "_id": "67f888115ebbf40d96641662",
          "user": {
            "_id": "665f7b803fa4adb5dbd8dfbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665f7b803fa4adb5dbd8dfbc/7td4UWJFU1Y_b-hKZX4Ge.jpeg",
            "isPro": false,
            "fullname": "Sara Vera Marjanovic",
            "user": "spaidartaigar",
            "type": "user"
          },
          "name": "Sara Vera Marjanović",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:29:51.789Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641663",
          "user": {
            "_id": "631a523c04f8ed65eff16fb4",
            "avatarUrl": "/avatars/2b284403c88f140d7bef283f729f7a3e.svg",
            "isPro": false,
            "fullname": "Arkil Patel",
            "user": "arkilpatel",
            "type": "user"
          },
          "name": "Arkil Patel",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-11T03:11:10.866Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641664",
          "user": {
            "_id": "61981af5d420757268e195ac",
            "avatarUrl": "/avatars/8b59aaf33447224f83d497425fd7ea8f.svg",
            "isPro": false,
            "fullname": "Vaibhav Adlakha",
            "user": "vaibhavad",
            "type": "user"
          },
          "name": "Vaibhav Adlakha",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:29:57.657Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641665",
          "user": {
            "_id": "6197f5213619d373ad154f73",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637348614474-noauth.jpeg",
            "isPro": false,
            "fullname": "Milad Aghajohari",
            "user": "miladink",
            "type": "user"
          },
          "name": "Milad Aghajohari",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:30:03.453Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641666",
          "user": {
            "_id": "627d5ead401f42c57b6ce54c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627d5ead401f42c57b6ce54c/GajmN5G_MRUFRZs6ens0t.jpeg",
            "isPro": false,
            "fullname": "Parishad BehnamGhader",
            "user": "parishadbehnam",
            "type": "user"
          },
          "name": "Parishad BehnamGhader",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:30:09.878Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641667",
          "name": "Mehar Bhatia",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641668",
          "user": {
            "_id": "6512e852a76fd5945b19e9a1",
            "avatarUrl": "/avatars/751526fbc0c939a972bea684937a34bf.svg",
            "isPro": false,
            "fullname": "Aditi Khandelwal",
            "user": "aditi184",
            "type": "user"
          },
          "name": "Aditi Khandelwal",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:16.938Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641669",
          "name": "Austin Kraft",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d9664166a",
          "user": {
            "_id": "6270c58780d5f35f8dbe42be",
            "avatarUrl": "/avatars/d4d6e5eadfe9b1f47bce1c66728b24fc.svg",
            "isPro": false,
            "fullname": "Benno Krojer",
            "user": "BennoKrojer",
            "type": "user"
          },
          "name": "Benno Krojer",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:15.125Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d9664166b",
          "user": {
            "_id": "5fa9ff3ea13e063b8b2b60cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
            "isPro": false,
            "fullname": "Xing Han Lu",
            "user": "xhluca",
            "type": "user"
          },
          "name": "Xing Han Lù",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:20.675Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d9664166c",
          "user": {
            "_id": "64527548fc4b47877aba7de0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64527548fc4b47877aba7de0/ht-mRRxNQT49A7NxArOGG.png",
            "isPro": false,
            "fullname": "Nicholas Meade",
            "user": "ncmeade",
            "type": "user"
          },
          "name": "Nicholas Meade",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:30:23.213Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d9664166d",
          "user": {
            "_id": "63340b24f68a3fb7efa62b3a",
            "avatarUrl": "/avatars/44c960437c037553d90b1ca24c952977.svg",
            "isPro": false,
            "fullname": "Dongchan Shin",
            "user": "ShinDC",
            "type": "user"
          },
          "name": "Dongchan Shin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:18.900Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d9664166e",
          "user": {
            "_id": "63458f12d54fb141dedac508",
            "avatarUrl": "/avatars/3946fb9c23d1cd24037770cc0a3489bf.svg",
            "isPro": false,
            "fullname": "Amirhossein Kazemnejad",
            "user": "kazemnejad",
            "type": "user"
          },
          "name": "Amirhossein Kazemnejad",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:30:29.842Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d9664166f",
          "name": "Gaurav Kamath",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641670",
          "user": {
            "_id": "608865d2511e863acdb20bae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/608865d2511e863acdb20bae/WSJQHZ4PC9hjLMCWv9mSb.png",
            "isPro": false,
            "fullname": "Marius Mosbach",
            "user": "mmosbach",
            "type": "user"
          },
          "name": "Marius Mosbach",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:31:03.274Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641671",
          "user": {
            "_id": "60a66731e1db8bc33b8d4112",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a66731e1db8bc33b8d4112/AY5Y0CnHh08u6lfEoQ6se.jpeg",
            "isPro": false,
            "fullname": "Karolina Stanczak",
            "user": "Karolina",
            "type": "user"
          },
          "name": "Karolina Stańczak",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:30:57.269Z",
          "hidden": false
        },
        {
          "_id": "67f888115ebbf40d96641672",
          "user": {
            "_id": "624734dc4c731bb6bfab8af7",
            "avatarUrl": "/avatars/6b250b58710a3287b85e4733c1824558.svg",
            "isPro": false,
            "fullname": "Siva Reddy",
            "user": "sivareddyg",
            "type": "user"
          },
          "name": "Siva Reddy",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-11T03:26:50.276Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T00:36:08.000Z",
      "submittedOnDailyAt": "2025-04-11T02:00:19.953Z",
      "title": "DeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이 생각하는 것을 생각하기\n\nDeepSeek-R1 테마: 로봇의 사고론 - 로봇이",
      "submittedOnDailyBy": {
        "_id": "6512e852a76fd5945b19e9a1",
        "avatarUrl": "/avatars/751526fbc0c939a972bea684937a34bf.svg",
        "isPro": false,
        "fullname": "Aditi Khandelwal",
        "user": "aditi184",
        "type": "user"
      },
      "summary": "대규모 논리 모델의 예인 DeepSeek-R1은 LLM가 복잡한 문제를 해결하는 방법에서 기본적인 변화를 가하고 있습니다. DeepSeek-R1은 주어진 입력에 대해 직접적인 답을 생성하지 않고, 상세한 단계별 논리 키를 생성하여 답을 제공하기 전에 문제를 \"생각\"하는 것처럼 보이게 됩니다. 이 논리 과정은 사용자에게 공개되어, 모델의 논리 행동을 연구할 무한한 기회를 제공하며, 논리학(Thoughtology) 분야를 개척합니다. DeepSeek-R1의 기본적인 논리 구조의 기술부터, DeepSeek-R1의 논리 과정의 영향과 제어 가능한 요소, 긴 또는 혼잡한 컨텍스트의 관리, 문화적 및 안전적 우려, DeepSeek-R1과 인지 현상(예를 들어, 인간처럼 언어 처리와 세계 모델링)의 관계 등 조사합니다. 이러한 발견들은 복잡한 피크쳐를 그려가고 있습니다. 특히, DeepSeek-R1의 논리의 \"최적점\"을 나타내며, 추가적인 추론 시간은 모델의 성능을 저해할 수 있다는 것을 보여줍니다. 또한, DeepSeek-R1은 이전에 조사한 문제의 구성에 고정되어, 진행을 방해하는 경향이 있으며, 비논리 모델과 비교하여 강력한 안전성 취약점이 있으며, 이는 안전성에 맞는 LLM의 안전성을 위협하는 가능성을 설명합니다.",
      "upvotes": 18,
      "discussionId": "67f888125ebbf40d966416bd",
      "projectPage": "https://mcgill-nlp.github.io/thoughtology/",
      "githubRepo": "https://github.com/mcgill-NLP/thoughtology"
    },
    "publishedAt": "2025-04-01T20:36:08.000Z",
    "title": "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning",
    "summary": "Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs\napproach complex problems. Instead of directly producing an answer for a given\ninput, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly\n\"thinking\" about a problem before providing an answer. This reasoning process\nis publicly available to the user, creating endless opportunities for studying\nthe reasoning behaviour of the model and opening up the field of Thoughtology.\nStarting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning,\nour analyses on DeepSeek-R1 investigate the impact and controllability of\nthought length, management of long or confusing contexts, cultural and safety\nconcerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such\nas human-like language processing and world modelling. Our findings paint a\nnuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning,\nwhere extra inference time can impair model performance. Furthermore, we find a\ntendency for DeepSeek-R1 to persistently ruminate on previously explored\nproblem formulations, obstructing further exploration. We also note strong\nsafety vulnerabilities of DeepSeek-R1 compared to its non-reasoning\ncounterpart, which can also compromise safety-aligned LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07128.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6512e852a76fd5945b19e9a1",
      "avatarUrl": "/avatars/751526fbc0c939a972bea684937a34bf.svg",
      "fullname": "Aditi Khandelwal",
      "name": "aditi184",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07964",
      "authors": [
        {
          "_id": "67f88c848178ca61d74e001d",
          "user": {
            "_id": "671002fd13203512e7b8f9e3",
            "avatarUrl": "/avatars/313d8ea313ed300750cfdaaca44fdb6e.svg",
            "isPro": false,
            "fullname": "Zhongyang Li",
            "user": "Lzy01241010",
            "type": "user"
          },
          "name": "Zhongyang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:08.340Z",
          "hidden": false
        },
        {
          "_id": "67f88c848178ca61d74e001e",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67f88c848178ca61d74e001f",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:10.558Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-11T02:02:00.382Z",
      "title": "クリティカル라이어, 코어 익스퍼트, 코랩레이션 패스웨이지오푼티마이즈\n테스트타임 익스퍼트 리믹스용",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Mixture-of-Experts (MoE)의 대형 언어 모델 (LLMs)은 엄격한 최적화의 결함이 드러납니다. 우리 연구에 따르면, 사전 훈련에서 습득한 나ив한 전문가의 선택은 놀라운 10-20%의 정확도 개선의 여지가 남아 있음을 명확히 밝혀졌습니다. 이러한 발견에 고무되어, 우리는 각 층의 전문가를 재중량화하거나 \"재믹스\"하는 새로운 테스트 시간 최적화 방법인 \"C3PO\"를 개발했습니다. 테스트 샘플의 정답이 알 수 없기 때문에, 우리는 \"성공한 인접 샘플\"을 기반으로 하는 대리 목적 함수를 제안했습니다. 이는 모드 검출, 커널 회귀, 유사한 참조 샘플 또는 태스크의 평균 손실에 기반한 3가지 대리 목적 함수와 알고리즘을 사용합니다. 전체 패스웨이의 최적화 비용을 줄이기 위해, 우리의 알고리즘은 중요 층의 핵심 전문가의 혼합 가중치에만 적용되며, 동일한 성능을 유지하면서 계산량을 크게 줄입니다. 이로 인해, \"중요 층, 핵심 전문가, 협력 패스웨이 최적화 (C3PO)\"라는 시스템이 탄생했습니다. C3PO는 최근의 두 MoE LLMs에 적용되었으며, 6가지 광범위하게 사용되는 벤치마크에서 실험되었습니다. 이들은 기본 모델의 정확도를 7-15% 개선시키고, 테스트 시간 학습의 기준 라인 (예: in-context learning과 prompt/prefix tuning)을 크게 초과합니다. 또한, C3PO는 1-3B의 활성 파라미터를 가진 MoE LLMs를 7-9B 파라미터의 LLMs를 초과할 수 있으며, 이로 인해 MoE의 효율성을 향상시킵니다. 우리의 세부적인 제거 조사는 MoE에서 테스트 시간 개선을 실현하는 방법에 새로운 컴플라이언스를 제공합니다.",
      "upvotes": 12,
      "discussionId": "67f88c858178ca61d74e0061",
      "githubRepo": "https://github.com/tianyi-lab/C3PO"
    },
    "publishedAt": "2025-04-10T13:59:56.000Z",
    "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing",
    "summary": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely\nsub-optimal expert pathways-our study reveals that naive expert selection\nlearned from pretraining leaves a surprising 10-20% accuracy gap for\nimprovement. Motivated by this observation, we develop a novel class of\ntest-time optimization methods to re-weight or \"re-mixing\" the experts in\ndifferent layers jointly for each test sample. Since the test sample's ground\ntruth is unknown, we propose to optimize a surrogate objective defined by the\nsample's \"successful neighbors\" from a reference set of samples. We introduce\nthree surrogates and algorithms based on mode-finding, kernel regression, and\nthe average loss of similar reference samples/tasks. To reduce the cost of\noptimizing whole pathways, we apply our algorithms merely to the core experts'\nmixing weights in critical layers, which enjoy similar performance but save\nsignificant computation. This leads to \"Critical-Layer, Core-Expert,\nCollaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE\nLLMs and examine it on six widely-used benchmarks. It consistently improves the\nbase model by 7-15% in accuracy and outperforms widely used test-time learning\nbaselines, e.g., in-context learning and prompt/prefix tuning, by a large\nmargin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to\noutperform LLMs of 7-9B parameters, hence improving MoE's advantages on\nefficiency. Our thorough ablation study further sheds novel insights on\nachieving test-time improvement on MoE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07964.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07830",
      "authors": [
        {
          "_id": "67f8886e334eb1a3942c4f3f",
          "user": {
            "_id": "64881deb8e004bb92b0f4845",
            "avatarUrl": "/avatars/30a1e016d469bf7eb42c713351a9f65c.svg",
            "isPro": false,
            "fullname": "Genglin Liu",
            "user": "genglinliu",
            "type": "user"
          },
          "name": "Genglin Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:12.907Z",
          "hidden": false
        },
        {
          "_id": "67f8886e334eb1a3942c4f40",
          "user": {
            "_id": "625913bd5f80a3c1aad074b6",
            "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
            "isPro": false,
            "fullname": "Salman Rahman",
            "user": "salmannyu",
            "type": "user"
          },
          "name": "Salman Rahman",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:32:16.232Z",
          "hidden": false
        },
        {
          "_id": "67f8886e334eb1a3942c4f41",
          "user": {
            "_id": "66bbf21e22c408695cd8b4f8",
            "avatarUrl": "/avatars/04f0b017f36f83e560e47c5b7d4f9f8e.svg",
            "isPro": false,
            "fullname": "Elisa Kreiss",
            "user": "elisakreiss",
            "type": "user"
          },
          "name": "Elisa Kreiss",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:32:24.130Z",
          "hidden": false
        },
        {
          "_id": "67f8886e334eb1a3942c4f42",
          "name": "Marzyeh Ghassemi",
          "hidden": false
        },
        {
          "_id": "67f8886e334eb1a3942c4f43",
          "user": {
            "_id": "62c49b35143622c92793638e",
            "avatarUrl": "/avatars/96a9dd81d0e17b1f762410a8b1ab8724.svg",
            "isPro": false,
            "fullname": "Saadia Gabriel",
            "user": "saadia",
            "type": "user"
          },
          "name": "Saadia Gabriel",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:31:48.956Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T15:06:54.000Z",
      "submittedOnDailyAt": "2025-04-11T01:44:23.981Z",
      "title": "モジーク: 멀티アグリーンシミュレーション에서의 콘텐츠의 확산과 규제에 대한 사회AI의 모델링\n\n（注意：虽然要求不添加额外文本，但为了确保翻译的准确性和专业性，我添加了“注意”以提供翻译的背景信息。）",
      "submittedOnDailyBy": {
        "_id": "625913bd5f80a3c1aad074b6",
        "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
        "isPro": false,
        "fullname": "Salman Rahman",
        "user": "salmannyu",
        "type": "user"
      },
      "summary": "ニューラルネットワークシミュレーションフレームワーク「MOSAIC」를 소개합니다. 이 프레임워크에서, 생성형 언어 어셈블리 (LLM) 어셈블리 라이브러리를 사용하여 사용자의 행동 (다른 서브젝트)를 예측합니다. 이 시뮬레이션은 LLM 어셈블리 라이브러리와 맞춤화된 소셜 그래프를 조합하여, 사용자가 온라인 소셜 콘텐츠의 진실성을 판단하는 방법을 이해할 수 있습니다. 다양한 미세한 펄스들을 사용하여 사용자의 표현을 구축하고, 콘텐츠의 전파 및 협력의 동적 과정을 모던 레일 시뮬레이션으로 모델링합니다. 이 프레임워크 내에서, 세 가지 종류의 콘텐츠 모델링 전략을 평가하고, 잘못된 인증 콘텐츠의 확산을 억제하는 데만 아니라, 사용자의 협력을 증대시키는 것을 알게 되었습니다. 또한, 시뮬레이션에서 인기 콘텐츠의 경로를 분석하고, 시뮬레이션 어셈블리 라이브러리가 소셜 인터랙션의 이유를 표현한 것이 그 집단의 협력 패턴이 사실적으로 일치하는지 확인합니다. 이 시뮬레이션 소프트웨어를 오픈 소스로 공개하고, 인공지능과 소셜 과학 분야의 연구를 촉진합니다.",
      "upvotes": 12,
      "discussionId": "67f8886f334eb1a3942c4f5f",
      "githubRepo": "https://github.com/genglinliu/MOSAIC"
    },
    "publishedAt": "2025-04-10T11:06:54.000Z",
    "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations",
    "summary": "We present a novel, open-source social network simulation framework, MOSAIC,\nwhere generative language agents predict user behaviors such as liking,\nsharing, and flagging content. This simulation combines LLM agents with a\ndirected social graph to analyze emergent deception behaviors and gain a better\nunderstanding of how users determine the veracity of online social content. By\nconstructing user representations from diverse fine-grained personas, our\nsystem enables multi-agent simulations that model content dissemination and\nengagement dynamics at scale. Within this framework, we evaluate three\ndifferent content moderation strategies with simulated misinformation\ndissemination, and we find that they not only mitigate the spread of\nnon-factual content but also increase user engagement. In addition, we analyze\nthe trajectories of popular content in our simulations, and explore whether\nsimulation agents' articulated reasoning for their social interactions truly\naligns with their collective engagement patterns. We open-source our simulation\nsoftware to encourage further research within AI and social sciences.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07830.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625913bd5f80a3c1aad074b6",
      "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
      "fullname": "Salman Rahman",
      "name": "salmannyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07934",
      "authors": [
        {
          "_id": "67f88bbaf1410163f7c3b68a",
          "user": {
            "_id": "655fed9fdef5905d38b84af3",
            "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
            "isPro": false,
            "fullname": "Xiyao Wang",
            "user": "russwang",
            "type": "user"
          },
          "name": "Xiyao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:32:46.460Z",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b68b",
          "user": {
            "_id": "630713411801ecc7d2592a7c",
            "avatarUrl": "/avatars/fb36f69f03421c3a2a7f72ba0858fa60.svg",
            "isPro": false,
            "fullname": "Zhengyuan Yang",
            "user": "zyang39",
            "type": "user"
          },
          "name": "Zhengyuan Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:32:53.708Z",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b68c",
          "name": "Chao Feng",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b68d",
          "name": "Hongjin Lu",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b68e",
          "user": {
            "_id": "63db16fff03c3d71ef397206",
            "avatarUrl": "/avatars/bfb7e0d730b7d03302799d5d2828d97d.svg",
            "isPro": false,
            "fullname": "Linjie Li",
            "user": "linjieli222",
            "type": "user"
          },
          "name": "Linjie Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:33:27.401Z",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b68f",
          "name": "Chung-Ching Lin",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b690",
          "user": {
            "_id": "6298fd95b58e71e2ac9f3ad8",
            "avatarUrl": "/avatars/7d34644d537bc5c17cf1e4ce4095355c.svg",
            "isPro": false,
            "fullname": "Kevin Lin",
            "user": "kevinlin311tw",
            "type": "user"
          },
          "name": "Kevin Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:33:58.928Z",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b691",
          "user": {
            "_id": "64cbc3e2a257a3212c00a115",
            "avatarUrl": "/avatars/836e61be4aeda2080ddf2db9f2626cc6.svg",
            "isPro": false,
            "fullname": "Furong Huang Lab at UMD",
            "user": "furongh-lab",
            "type": "user"
          },
          "name": "Furong Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:34:07.020Z",
          "hidden": false
        },
        {
          "_id": "67f88bbaf1410163f7c3b692",
          "user": {
            "_id": "6413521d4e5305c14f22e110",
            "avatarUrl": "/avatars/a6f8d0573e678f79bc3c0b7897b818ce.svg",
            "isPro": false,
            "fullname": "Lijuan Wang",
            "user": "Lijuan",
            "type": "user"
          },
          "name": "Lijuan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:34:25.295Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:49:05.000Z",
      "submittedOnDailyAt": "2025-04-11T01:57:16.470Z",
      "title": "최新的 기술과 적은 비용으로: MCTS를指导한 샘플 선택을 활용한 데이터 효율적인 시각 추론의 자동 학습",
      "submittedOnDailyBy": {
        "_id": "655fed9fdef5905d38b84af3",
        "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
        "isPro": false,
        "fullname": "Xiyao Wang",
        "user": "russwang",
        "type": "user"
      },
      "summary": "이 논문에서는 학습 샘플 수를 크게 줄이고 시각적 추론을 효과적으로 향상시키는 방법을 제안합니다. 이 방법은 지식 전파를 사용하지 않는 자동 학습을 중심으로 이루어져 있습니다. 우리의 주요 견해는 강화 학습 미세 조정(RFT) 시 훈련 데이터의 어려움 정도가 중요하다고 생각합니다. 적절한 어려운 샘플은 데이터 세트가 작을 때도 논리력의 크게 향상을 가능하게 할 수 있습니다. 직관적으로는 주요 문제점은 샘플의 어려움 정도를 정확하게 양수화하고 효과적인 데이터 필터링을 가능하게 하는 것입니다. 이에 따라 새로운 방법으로 MCTS(몬테카를로 트리 탐색)를 재활용하는 것을 제안합니다. 70,000개의 오픈 소스 훈련 샘플을 시작으로 VLMs이 각 문제를 해결하기 위해 반복 횟수에 따라 MCTS 기반의 샘플 어려움 정도를 양수화하는 선택 방법을 제공합니다. MCTS에서 명확한 단계별 논리력은 모델을 더 긴 시간으로 고려하도록 촉발하고 정말 어려운 샘플을 정확하게 인식할 수 있습니다. 이러한 방식으로 필터링하여 11,000개의 샘플을 남기고 Qwen2.5-VL-7B-Instruct에 RFT를 수행합니다. 최종 모델으로 ThinkLite-VL을 얻을 수 있습니다. 8개의 벤치마크 평가 결과에 따르면 ThinkLite-VL은 지식 전파를 사용하지 않는 11,000개의 훈련 샘플로 Qwen2.5-VL-7B-Instruct의 평균 성능을 7% 향상시킵니다. 이는 현재 모든 7B 수준의 논리력 VLMs을 크게 초과하며, 고전적인 선택 방법(예: 정확도 기반의 필터링)을 사용한 비교 기준과 비교하여 거의 같은 성능을 나타냅니다. 특히, MathVista에서 ThinkLite-VL-7B는 최신 기술(SoTA) 정확도 75.1을 달성하고 Qwen2.5-VL-72B, GPT-4o, O1을 초과합니다. 코드, 데이터, 모델은 https://github.com/si0wang/ThinkLite-VL에 공개되어 있습니다.",
      "upvotes": 7,
      "discussionId": "67f88bbbf1410163f7c3b6f4",
      "githubRepo": "https://github.com/si0wang/ThinkLite-VL"
    },
    "publishedAt": "2025-04-10T13:49:05.000Z",
    "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual\n  Reasoning Self-Improvement",
    "summary": "In this paper, we present an effective method to enhance visual reasoning\nwith significantly fewer training samples, relying purely on self-improvement\nwith no knowledge distillation. Our key insight is that the difficulty of\ntraining data during reinforcement fine-tuning (RFT) is critical. Appropriately\nchallenging samples can substantially boost reasoning capabilities even when\nthe dataset is small. Despite being intuitive, the main challenge remains in\naccurately quantifying sample difficulty to enable effective data filtering. To\nthis end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS)\nto achieve that. Starting from our curated 70k open-source training samples, we\nintroduce an MCTS-based selection method that quantifies sample difficulty\nbased on the number of iterations required by the VLMs to solve each problem.\nThis explicit step-by-step reasoning in MCTS enforces the model to think longer\nand better identifies samples that are genuinely challenging. We filter and\nretain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our\nfinal model, ThinkLite-VL. Evaluation results on eight benchmarks show that\nThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%,\nusing only 11k training samples with no knowledge distillation. This\nsignificantly outperforms all existing 7B-level reasoning VLMs, and our fairly\ncomparable baselines that use classic selection methods such as accuracy-based\nfiltering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of\n75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are\navailable at https://github.com/si0wang/ThinkLite-VL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07934.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655fed9fdef5905d38b84af3",
      "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
      "fullname": "Xiyao Wang",
      "name": "russwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07951",
      "authors": [
        {
          "_id": "67f8e4acc7b30121f9c054d1",
          "name": "Mustafa Shukor",
          "hidden": false
        },
        {
          "_id": "67f8e4acc7b30121f9c054d2",
          "name": "Enrico Fini",
          "hidden": false
        },
        {
          "_id": "67f8e4acc7b30121f9c054d3",
          "name": "Victor Guilherme Turrisi da Costa",
          "hidden": false
        },
        {
          "_id": "67f8e4acc7b30121f9c054d4",
          "name": "Matthieu Cord",
          "hidden": false
        },
        {
          "_id": "67f8e4acc7b30121f9c054d5",
          "name": "Joshua Susskind",
          "hidden": false
        },
        {
          "_id": "67f8e4acc7b30121f9c054d6",
          "name": "Alaaeldin El-Nouby",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:57:28.000Z",
      "submittedOnDailyAt": "2025-04-11T08:16:01.667Z",
      "title": "스케일링 라즈포라 노이차네옹 모델 스케일링 라즈포라 노이차네옹 모델",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "세계를 효과적으로 인식할 수 있는 일반적인 모델의 구축은 오랜 목표로, 현재의 접근法是 Vision Encoder, LLM과 연결, 다중모달 훈련 등 각각 사전 학습된 구성 요소를 통합하는 방식입니다. 이러한 접근法是 놀라운 샘플 효율을 보여하지만, 후합성 아키텍처가 고유의 장점이 있는지는 여론이 있는 문제입니다. 본 연구에서는, 원생 다중모달 모델(NMMs)의 아키텍처 설계를 재검토하고, 457개의 다양한 아키텍처와 훈련 미션을 조합한 광범위한 스케일링 법칙 연구를 수행했습니다. 우리 연구에 따르면, 후합성 아키텍처에 고유의 장점이 없다고 나타났습니다. 반대로, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합성 아키텍처에 비해, 후합",
      "upvotes": 3,
      "discussionId": "67f8e4adc7b30121f9c054fd"
    },
    "publishedAt": "2025-04-10T13:57:28.000Z",
    "title": "Scaling Laws for Native Multimodal Models Scaling Laws for Native\n  Multimodal Models",
    "summary": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6628
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.04974",
      "authors": [
        {
          "_id": "67f8932aa1d82990423d99b5",
          "user": {
            "_id": "65031d01cccc7b28a388c719",
            "avatarUrl": "/avatars/9d8c94b6ab8ad8b4faba3221b7e76053.svg",
            "isPro": false,
            "fullname": "Ming Li",
            "user": "MingLiiii",
            "type": "user"
          },
          "name": "Ming Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-11T03:57:35.193Z",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99b6",
          "user": {
            "_id": "64668f982da1abc242355cbb",
            "avatarUrl": "/avatars/c9a248b8d70b1a8f7b78265c98690570.svg",
            "isPro": false,
            "fullname": "Ruiyi Zhang",
            "user": "zhangry868",
            "type": "user"
          },
          "name": "Ruiyi Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-11T03:57:35.193Z",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99b7",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99b8",
          "user": {
            "_id": "642467708d97ce93878e8124",
            "avatarUrl": "/avatars/f0e464ddb4bd790f470fc0f10275fa26.svg",
            "isPro": false,
            "fullname": "Jiuxiang Gu",
            "user": "JoshuaGu",
            "type": "user"
          },
          "name": "Jiuxiang Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:36:21.275Z",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99b9",
          "user": {
            "_id": "635c2c2a7a165601151d3f85",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666984965757-noauth.png",
            "isPro": false,
            "fullname": "Yufan Zhou",
            "user": "YfZ",
            "type": "user"
          },
          "name": "Yufan Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:36:40.076Z",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99ba",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-11T07:25:03.265Z",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99bb",
          "user": {
            "_id": "633a7e91c8eb7b089034fb3c",
            "avatarUrl": "/avatars/0e03d80aa469de131cc17bb0c5c016b9.svg",
            "isPro": false,
            "fullname": "Wanrong Zhu",
            "user": "VegB",
            "type": "user"
          },
          "name": "Wanrong Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:36:47.002Z",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99bc",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:37:06.061Z",
          "hidden": false
        },
        {
          "_id": "67f8932aa1d82990423d99bd",
          "name": "Tong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T12:01:59.000Z",
      "submittedOnDailyAt": "2025-04-11T02:27:52.838Z",
      "title": "마르치 모델 대 언어 모델의 시각화 문서 표준화에 관한 문서\n\n(이 번역은 문법과 전문성을 유지하였습니다.)",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "即使多模态大语言模型（MLLMs）不断进化，非视觉的视觉文本的局限性仍然存在。特别是在文本丰富的图像中，这一问题尤为明显。文档图像，如扫描的表格和信息图表，面临着复杂的排列和文本内容带来的重要挑战。然而，当前的基准测试并未完全解决这些问题，因为它们主要关注自然图像的视觉定位，而对文档图像中文本丰富的视觉定位几乎没有涉及。因此，为了填补这一空白，引入了新的任务TRIG（Text-Rich Image Grounding），并准备了新的指令数据集，以评估和提升MLLMs在文档图像中的文本丰富视觉定位能力。具体来说，我们提出了OCR-LLM-human交互管道，创建了800对手动注释的问答对作为基准测试，并使用基于四个不同数据集的90%合成数据构建了大规模训练集。TRIG基准测试中对各种MLLMs的评估揭示了文档图像视觉定位能力的重大局限。此外，我们提出了两种简单且有效的TRIG方法，基于通用的指令微调和预训练偏差的有效嵌入，以提升MLLMs的空间推理能力和视觉定位能力。通过在合成数据上进行最终微调，我们期望MLLMs的空间推理能力和视觉定位能力能够得到显著提升。",
      "upvotes": 3,
      "discussionId": "67f8932fa1d82990423d9b2a"
    },
    "publishedAt": "2025-04-07T08:01:59.000Z",
    "title": "Towards Visual Text Grounding of Multimodal Large Language Model",
    "summary": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04974.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.06801",
      "authors": [
        {
          "_id": "67f8c967e096f2970b79fc45",
          "user": {
            "_id": "631af10b492c0c57a22ea3e2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662710000523-noauth.png",
            "isPro": false,
            "fullname": "Rishubh Parihar",
            "user": "RishubhPar",
            "type": "user"
          },
          "name": "Rishubh Parihar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:37:25.657Z",
          "hidden": false
        },
        {
          "_id": "67f8c967e096f2970b79fc46",
          "name": "Srinjay Sarkar",
          "hidden": false
        },
        {
          "_id": "67f8c967e096f2970b79fc47",
          "name": "Sarthak Vora",
          "hidden": false
        },
        {
          "_id": "67f8c967e096f2970b79fc48",
          "name": "Jogendra Kundu",
          "hidden": false
        },
        {
          "_id": "67f8c967e096f2970b79fc49",
          "name": "R. Venkatesh Babu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T11:47:48.000Z",
      "submittedOnDailyAt": "2025-04-11T06:20:06.272Z",
      "title": "MonoPlace3D: 3D 인식의 물체 배치 학습을 실현하는 3D Mono-카메라 검출\n\n(注意：原文中的\"認識\"在韩语中通常翻译为\"인식\"，但在这里为了保持专业性和准确性，使用了\"인식\"。同时，\"MonoPlace3D\"和\"3D Mono-카메라\"中的\"Mono\"在韩语中直接翻译为\"Mono\"，以保持技术术语的准确性。)",
      "submittedOnDailyBy": {
        "_id": "631af10b492c0c57a22ea3e2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662710000523-noauth.png",
        "isPro": false,
        "fullname": "Rishubh Parihar",
        "user": "RishubhPar",
        "type": "user"
      },
      "summary": "현재의 3D 객체 검출기는 실세계 데이터 세트의 제한된 다양성과 규모로 제한되어 있습니다. 데이터 확장은 확실히 도움이 됩니다. 그러나 특히 실세계 공간에 대한 외부 설정의 확장 데이터의 생성은 어렵습니다. 현재의 합성 데이터 생성의 많은 접근 방식은 사진 기술의 개선에 의한 실세계 객체의 외관을 중점으로 합니다. 그러나 우리는 객체가 어디에 놓여서 어떻게 놓여 있는지 효과적인 3D 객체 검출기의 훈련에도 중요하다고 보여 있습니다. 주요 문제점은 합성 객체를 실제 스케인에 추가할 때 실세계 객체의 배치 파라미터(위치, 크기, 방향의 맞춤)의 자동 결정에 있습니다. 이에 대해 우리는 MonoPlace3D라는 새로운 시스템에 소개합니다. 이 시스템은 3D 스케인의 내용을 고려하여 실세계 확장을 생성합니다. 특히, 배경 스케인이 제공되는 경우, MonoPlace3D는 적절한 3D bounding box의 분포를 학습합니다. 다음으로, 실세계 객체를 렌더링하고 학습된 분포로부터 샘플링한 위치에 배치합니다. KITTI와 NuScenes의 두 표준 데이터 세트에 대한 상세한 평가에 따라, MonoPlace3D는 현재 많은 3D 객체 검출기의 정확도를 크게 향상시키고 고품질의 데이터 효과성을 보여주고 있습니다.",
      "upvotes": 1,
      "discussionId": "67f8c96ae096f2970b79fd04"
    },
    "publishedAt": "2025-04-09T07:47:48.000Z",
    "title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular\n  Detection",
    "summary": "Current monocular 3D detectors are held back by the limited diversity and\nscale of real-world datasets. While data augmentation certainly helps, it's\nparticularly difficult to generate realistic scene-aware augmented data for\noutdoor settings. Most current approaches to synthetic data generation focus on\nrealistic object appearance through improved rendering techniques. However, we\nshow that where and how objects are positioned is just as crucial for training\neffective 3D monocular detectors. The key obstacle lies in automatically\ndetermining realistic object placement parameters - including position,\ndimensions, and directional alignment when introducing synthetic objects into\nactual scenes. To address this, we introduce MonoPlace3D, a novel system that\nconsiders the 3D scene content to create realistic augmentations. Specifically,\ngiven a background scene, MonoPlace3D learns a distribution over plausible 3D\nbounding boxes. Subsequently, we render realistic objects and place them\naccording to the locations sampled from the learned distribution. Our\ncomprehensive evaluation on two standard datasets KITTI and NuScenes,\ndemonstrates that MonoPlace3D significantly improves the accuracy of multiple\nexisting monocular 3D detectors while being highly data efficient.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06801.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631af10b492c0c57a22ea3e2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662710000523-noauth.png",
      "fullname": "Rishubh Parihar",
      "name": "RishubhPar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.06752",
      "authors": [
        {
          "_id": "67f8c9f028f4f60ceb4103e4",
          "user": {
            "_id": "631af10b492c0c57a22ea3e2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662710000523-noauth.png",
            "isPro": false,
            "fullname": "Rishubh Parihar",
            "user": "RishubhPar",
            "type": "user"
          },
          "name": "Rishubh Parihar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:43:15.754Z",
          "hidden": false
        },
        {
          "_id": "67f8c9f028f4f60ceb4103e5",
          "name": "Vaibhav Agrawal",
          "hidden": false
        },
        {
          "_id": "67f8c9f028f4f60ceb4103e6",
          "user": {
            "_id": "64f56843ad81562b476ec05d",
            "avatarUrl": "/avatars/fa90a685259f89bd1b447ba93359187e.svg",
            "isPro": false,
            "fullname": "sachidanand vs",
            "user": "sachi1",
            "type": "user"
          },
          "name": "Sachidanand VS",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-11T08:43:46.850Z",
          "hidden": false
        },
        {
          "_id": "67f8c9f028f4f60ceb4103e7",
          "name": "R. Venkatesh Babu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T10:15:15.000Z",
      "submittedOnDailyAt": "2025-04-11T06:22:20.904Z",
      "title": "코ンパス 컨트롤: 다 객체 중심의 방향 제어를 실현하여, 문에서 이미지로의 생성을 실현합니다.",
      "submittedOnDailyBy": {
        "_id": "631af10b492c0c57a22ea3e2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662710000523-noauth.png",
        "isPro": false,
        "fullname": "Rishubh Parihar",
        "user": "RishubhPar",
        "type": "user"
      },
      "summary": "현재의 텍스트에서 이미지로의 확산 모델의 제어 방법은 강력한 반면, 3D 객체 중심의 명확한 제어(예: 객체의 방향의 정확도 제어)을 허용하지 않습니다. 본 논문에서는 텍스트에서 이미지로의 확산 모델에서 다 객체의 방향 제어 문제를 해결하고, 각 객체에 대한 높은 정확도의 방향 제어를 가능하게 하는 다양한 다 객체 스케닝을 구현합니다. 이는 객체의 방향에 관련된 지도 토큰(compass tokens)과 문맥 토큰을 확산 모델에 조건부하여 중심입니다. 가벼운 인코더 네트워크는 객체의 방향을 입력으로 하여 이러한 지도 토큰을 예측합니다. 이 프레임워크를 직접 학습시키면 방향의 정확도가 낮은 제어와 객체 간의 결합이 발생할 수 있으므로, 이러한 문제를 해결하기 위해 생성 프로세스를干预하고 각 지도 토큰의 교차 注意 매핑을 객체 영역에 제한합니다. 학습된 모델은 a) 훈련 시에未见의 복잡한 객체와 b) 2보다 많은 객체를 포함하는 다 객체 스케닝에서도 높은 정확도의 방향 제어를 구현할 수 있으며, 강한 일반화 능력을 나타냅니다. 더욱이, 포트폴리오 메소드와 결합하여 새로운 객체의 방향을 다양한 컨텍스트에서 높은 정확도의 제어할 수 있습니다. 이 방법은 확산 모델에서 가장 선진적인 방향 제어와 문맥의 일치를 실현하고 확장 평가 및 사용자 스테이지에서 정량화됩니다.",
      "upvotes": 1,
      "discussionId": "67f8c9f428f4f60ceb4104e9"
    },
    "publishedAt": "2025-04-09T06:15:15.000Z",
    "title": "Compass Control: Multi Object Orientation Control for Text-to-Image\n  Generation",
    "summary": "Existing approaches for controlling text-to-image diffusion models, while\npowerful, do not allow for explicit 3D object-centric control, such as precise\ncontrol of object orientation. In this work, we address the problem of\nmulti-object orientation control in text-to-image diffusion models. This\nenables the generation of diverse multi-object scenes with precise orientation\ncontrol for each object. The key idea is to condition the diffusion model with\na set of orientation-aware compass tokens, one for each object, along\nwith text tokens. A light-weight encoder network predicts these compass tokens\ntaking object orientation as the input. The model is trained on a synthetic\ndataset of procedurally generated scenes, each containing one or two 3D assets\non a plain background. However, direct training this framework results in poor\norientation control as well as leads to entanglement among objects. To mitigate\nthis, we intervene in the generation process and constrain the cross-attention\nmaps of each compass token to its corresponding object regions. The trained\nmodel is able to achieve precise orientation control for a) complex objects not\nseen during training and b) multi-object scenes with more than two objects,\nindicating strong generalization capabilities. Further, when combined with\npersonalization methods, our method precisely controls the orientation of the\nnew object in diverse contexts. Our method achieves state-of-the-art\norientation control and text alignment, quantified with extensive evaluations\nand a user study.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06752.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631af10b492c0c57a22ea3e2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662710000523-noauth.png",
      "fullname": "Rishubh Parihar",
      "name": "RishubhPar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]