[
  {
    "paper": {
      "id": "2503.23307",
      "authors": [
        {
          "_id": "67eb4bd0eca57c4eebbb343a",
          "user": {
            "_id": "64f8e358766ff9f3d2b0de84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
            "isPro": true,
            "fullname": "Cong Wei",
            "user": "lim142857",
            "type": "user"
          },
          "name": "Cong Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:21.554Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343b",
          "name": "Bo Sun",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343c",
          "user": {
            "_id": "650a8979c19e5b4c8a6ff062",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650a8979c19e5b4c8a6ff062/64_JuECX_k_-uK7m7nlua.jpeg",
            "isPro": false,
            "fullname": "Haoyu Ma",
            "user": "haoyum1997",
            "type": "user"
          },
          "name": "Haoyu Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:54:47.847Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343d",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343e",
          "user": {
            "_id": "6444e8911cfc9ae6bb3ad216",
            "avatarUrl": "/avatars/8c06e064cf24789e4131f7af06dac86b.svg",
            "isPro": false,
            "fullname": "Xu",
            "user": "FelixXu",
            "type": "user"
          },
          "name": "Felix Juefei-Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:55:03.326Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343f",
          "name": "Zecheng He",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3440",
          "user": {
            "_id": "6549417b3ce45eb764faf993",
            "avatarUrl": "/avatars/d310f475d0697f5f13b3d4141ea0ccaf.svg",
            "isPro": false,
            "fullname": "Xiaoliang Dai",
            "user": "daixl1992",
            "type": "user"
          },
          "name": "Xiaoliang Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:54:01.490Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3441",
          "user": {
            "_id": "65a4fa7d2548c41ad9d9b710",
            "avatarUrl": "/avatars/3cace2d2f11f7194d8eca4b95b0b57cc.svg",
            "isPro": false,
            "fullname": "Luxin Zhang",
            "user": "Luczzz",
            "type": "user"
          },
          "name": "Luxin Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:53:55.152Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3442",
          "name": "Kunpeng Li",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3443",
          "user": {
            "_id": "655846d7ed8df83128f5826a",
            "avatarUrl": "/avatars/d7ce174d7d1b8614d5f6f071225c0057.svg",
            "isPro": false,
            "fullname": "Hou",
            "user": "Tingbo",
            "type": "user"
          },
          "name": "Tingbo Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:53:10.214Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3444",
          "name": "Animesh Sinha",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3445",
          "name": "Peter Vajda",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3446",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:52:50.248Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T04:22:09.000Z",
      "submittedOnDailyAt": "2025-04-01T00:46:45.446Z",
      "title": "모지라드：영화 수준의 대화 캐릭터 합성에 대한 도전",
      "submittedOnDailyBy": {
        "_id": "64f8e358766ff9f3d2b0de84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
        "isPro": true,
        "fullname": "Cong Wei",
        "user": "lim142857",
        "type": "user"
      },
      "summary": "최근의 이미지 생성의 발전은 인상적인 움직임을 구현하지만, 이는 인물 중심의 이야기의 전달 방식에 주목하지 않고 있습니다. 이는 자동화된 영화, 애니메이션의 생성에 대한 중요한 문제입니다. 우리는 'Talking Characters'를 소개합니다. 이것은 직접적인 목소리와 문장을 통해 대화적인 인물 애니메이션을 생성하는 더 현실적인 작업입니다. Talking Head와 달리, Talking Characters는 인물 전체의 이미지 생성을 목표로 합니다. 이 논문에서는, MoCha를 제안합니다. 이것은 이 종류의 첫 번째로, 대화적인 인물 애니메이션을 생성하는 것입니다. 이미지와 소리의 정밀한 동기화를 보장하기 위해, 소리 토큰과 이미지 토큰의 대응을 효과적으로 수행하는 시각-음성 윈도우 어텐션 구조를 제안합니다. 대규모의 음성 레이블付き 이미지 데이터셋의 부족을 해결하기 위해, 음성 레이블付き와 문장 레이블付き의 이미지 데이터를 모두 사용하는 양방향 훈련 전략을 제안합니다. 이는 다양한 인물의 동작에 대한 일반화에 큰 개선을 가집니다. 또한, 인물 태그付き의 구조화된 Prompt 템플릿을 설계합니다. 이는, 일단, 다양한 인물의 도전을 가능하게 하며, 영화적인 코랩의 컨텍스트에 따라 AI 생성의 인물이 대화를 할 수 있게 합니다. 매우 상세한 질적 및 양적인 평가, 이는 인간의 취향 연구와 벤치마크 비교를 포함합니다. 이는 MoCha가 AI 생성의 영화적인 이야기 전달에 새로운 기준을 세우고, 고급의 현실감, 표현력, 제어 가능도와 일반화에 성공합니다.",
      "upvotes": 31,
      "discussionId": "67eb4bd3eca57c4eebbb34c7",
      "projectPage": "https://congwei1230.github.io/MoCha/",
      "ai_keywords": [
        "speech-video window attention mechanism",
        "speech-labeled video datasets",
        "text-labeled video data",
        "structured prompt templates",
        "character tags",
        "multi-character conversation",
        "turn-based dialogue",
        "context-aware conversations",
        "cinematic coherence"
      ]
    },
    "publishedAt": "2025-03-30T00:22:09.000Z",
    "title": "MoCha: Towards Movie-Grade Talking Character Synthesis",
    "summary": "Recent advancements in video generation have achieved impressive motion\nrealism, yet they often overlook character-driven storytelling, a crucial task\nfor automated film, animation generation. We introduce Talking Characters, a\nmore realistic task to generate talking character animations directly from\nspeech and text. Unlike talking head, Talking Characters aims at generating the\nfull portrait of one or more characters beyond the facial region. In this\npaper, we propose MoCha, the first of its kind to generate talking characters.\nTo ensure precise synchronization between video and speech, we propose a\nspeech-video window attention mechanism that effectively aligns speech and\nvideo tokens. To address the scarcity of large-scale speech-labeled video\ndatasets, we introduce a joint training strategy that leverages both\nspeech-labeled and text-labeled video data, significantly improving\ngeneralization across diverse character actions. We also design structured\nprompt templates with character tags, enabling, for the first time,\nmulti-character conversation with turn-based dialogue-allowing AI-generated\ncharacters to engage in context-aware conversations with cinematic coherence.\nExtensive qualitative and quantitative evaluations, including human preference\nstudies and benchmark comparisons, demonstrate that MoCha sets a new standard\nfor AI-generated cinematic storytelling, achieving superior realism,\nexpressiveness, controllability and generalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23307.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64f8e358766ff9f3d2b0de84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
      "fullname": "Cong Wei",
      "name": "lim142857",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23461",
      "authors": [
        {
          "_id": "67eb594988a08fae617242f1",
          "name": "Nikai Du",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f2",
          "user": {
            "_id": "66449e619ff401732687f013",
            "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
            "isPro": false,
            "fullname": "chen",
            "user": "zhen-nan",
            "type": "user"
          },
          "name": "Zhennan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:46.364Z",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f3",
          "user": {
            "_id": "637c22183d8e2e9c40c09fcf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669079538761-noauth.jpeg",
            "isPro": false,
            "fullname": "Zhizhou Chen",
            "user": "Chenzzzzzz",
            "type": "user"
          },
          "name": "Zhizhou Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:57:44.605Z",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f4",
          "name": "Shan Gao",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f5",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f6",
          "user": {
            "_id": "67593dd0f522f4409e614ba0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67593dd0f522f4409e614ba0/cvb9w_8seu3Kbjg_XAnNj.jpeg",
            "isPro": false,
            "fullname": "Jiang Zhengkai",
            "user": "jzzzzk",
            "type": "user"
          },
          "name": "Zhengkai Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:58:12.614Z",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f7",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f8",
          "user": {
            "_id": "65734004769f3ee9bde1af10",
            "avatarUrl": "/avatars/d6310ed861972fd691687d8f47413f33.svg",
            "isPro": false,
            "fullname": "Ying Tai",
            "user": "yingtai",
            "type": "user"
          },
          "name": "Ying Tai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:44.350Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T14:36:55.000Z",
      "submittedOnDailyAt": "2025-04-01T01:44:26.275Z",
      "title": "텍스트 클래퍼: 복잡한 시각화 패턴에서 여러 텍스트를 정확히 그리기",
      "submittedOnDailyBy": {
        "_id": "66449e619ff401732687f013",
        "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
        "isPro": false,
        "fullname": "chen",
        "user": "zhen-nan",
        "type": "user"
      },
      "summary": "이 논문에서는 복잡한 시각적 문자 생성(CVTG)의 과제를 검토하고 있습니다. 이는 시각적 이미지 내의 다양한 영역에 분포하는 복잡한 문자 내용을 생성하는 것을 중심으로 합니다. CVTG에서 이미지 생성 모델이 왜곡된 시각적 문자를 생성하거나 일부 시각적 문자를 생략하는 경우가 있습니다. 이러한 문제를 해결하기 위해 우리는 \"TextCrafter\"라는 새로운 다각형 시각적 문자 그리드 기법을 제안하고 있습니다. TextCrafter는 복잡한 시각적 문자를 발전적인 스테레오그래피로 분해하고 문자 내용과 시각적 매체 사이의 강력한 대응 관계를 보장합니다. 또한 시각적 문자의 중요성을 강화하기 위해 토큰 포커스 강화 기능을 도입합니다. TextCrafter는 CVTG 작업에서 문자 혼동, 생략, 브레이드 등 주요 문제점을 효과적으로 해결합니다. 또한 CVTG 작업의 생성 모델의 성능을 엄격하게 평가하기 위해 새로운 벤치마크 데이터 세트 \"CVTG-2K\"를 제시합니다. 확장된 실험은 우리의 방법론이 가장 선진적인 접근을 초월함을 보여줍니다.",
      "upvotes": 26,
      "discussionId": "67eb594b88a08fae617243ac",
      "projectPage": "https://dnknju.github.io/textcrafter-vue/",
      "githubRepo": "https://github.com/NJU-PCALab/TextCrafter",
      "ai_keywords": [
        "complex visual text",
        "TextCrafter",
        "multi-visual text rendering",
        "progressive strategy",
        "token focus enhancement",
        "CVTG-2K",
        "generative models",
        "CVTG tasks",
        "state-of-the-art approaches"
      ]
    },
    "publishedAt": "2025-03-30T10:36:55.000Z",
    "title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes",
    "summary": "This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23461.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66449e619ff401732687f013",
      "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
      "fullname": "chen",
      "name": "zhen-nan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24235",
      "authors": [
        {
          "_id": "67eb57023475e7b135788500",
          "user": {
            "_id": "62a42f22c683d02f5b63320c",
            "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
            "isPro": false,
            "fullname": "Qiyuan Zhang",
            "user": "DonJoey",
            "type": "user"
          },
          "name": "Qiyuan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:57:25.339Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788501",
          "user": {
            "_id": "65d2bb5c6130ef7be012d235",
            "avatarUrl": "/avatars/1c1e3bbb2c683a5c9d1f792a2c13fc4a.svg",
            "isPro": false,
            "fullname": "Fuyuan Lyu",
            "user": "silentspring2",
            "type": "user"
          },
          "name": "Fuyuan Lyu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:56:19.295Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788502",
          "user": {
            "_id": "65d1b42f3da87ce21e33261a",
            "avatarUrl": "/avatars/041cb441fa3871acde4ba565632056bf.svg",
            "isPro": false,
            "fullname": "RubinSun",
            "user": "RubinSun",
            "type": "user"
          },
          "name": "Zexu Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:51.229Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788503",
          "user": {
            "_id": "646def60df618b303b419323",
            "avatarUrl": "/avatars/97aa761d5255abf230304cfeade87835.svg",
            "isPro": false,
            "fullname": "Lei Wang",
            "user": "demolei",
            "type": "user"
          },
          "name": "Lei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T08:01:21.676Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788504",
          "user": {
            "_id": "63414659c5565a4b8d41bc42",
            "avatarUrl": "/avatars/25b4ca3002edf4c35cded0902c26632a.svg",
            "isPro": false,
            "fullname": "Weixu Zhang",
            "user": "nancy-zwx",
            "type": "user"
          },
          "name": "Weixu Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:56:26.532Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788505",
          "user": {
            "_id": "63c0c2497f52541dfc7d7567",
            "avatarUrl": "/avatars/16c174e2803ef86d09815b36a666ee0e.svg",
            "isPro": false,
            "fullname": "ZhihanGUO",
            "user": "ZhihanGUO",
            "type": "user"
          },
          "name": "Zhihan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:56:31.979Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788506",
          "name": "Yufei Wang",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788507",
          "name": "Irwin King",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788508",
          "name": "Xue Liu",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788509",
          "name": "Chen Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T15:46:15.000Z",
      "submittedOnDailyAt": "2025-04-01T01:37:27.268Z",
      "title": "어떻게, 어떻게, 어디에서, 얼마나 좋나요? 대규모 언어 모델의 테스트 시 스케일링에 대한 조사",
      "submittedOnDailyBy": {
        "_id": "62a42f22c683d02f5b63320c",
        "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
        "isPro": false,
        "fullname": "Qiyuan Zhang",
        "user": "DonJoey",
        "type": "user"
      },
      "summary": "스케일링의 엔터프라이즈는 예비 시에 데이터와 파라미터의 스케일링을 포함하고 있으며, 테스트 시 스케일링(TTS)을 \"테스트 시 계산\"으로 불리는 것을 엄격한 연구의 초점을 맞추게 되었다. 최근의 연구는 TTS가 대규모 언어 모델(LLMs)의 문제 해결 능력에 영향을 미치며, 수학이나 코딩의 전문적인 논리 임무뿐만 아니라, 개방된 Q&A와 같은 일반적인 임무에서도 상당한 발전을 이루는 것을 보여주고 있다. 그러나 이 분야에서 최근의 노력을 급격히 늘리고 있는 상황에서, 체계적인 이해를 제공하는 종합적인 조사가 절실한 일이다. 이를 위해, 우리는 \"어떤 것을 스케일링 할 것인가\", \"어떤 방식으로 스케일링 할 것인가\", \"어떤 곳에서 스케일링 할 것인가\", \"어떤 방식으로 잘 스케일링 할 것인가\" 등 4가지 핵심적인 차원에 기반한 통일적이고 다양한 프레임워크를 제안하고 있다. 이 프레임워크에 따라, 우리는 방법, 적용 스케너, 평가면에 대한 확장된 리뷰를 수행하고, TTS의 광범위한 규모에서 각 기술의 특징적인 기능적 역할을 체계적으로 분석하여 제시하고 있다. 이 분석에서, 우리는 지금까지의 TTS의 주요 개발 타일들을 추출하고, 실용적인 도입을 위해 가이드라인을 제공하고 있다. 또한, 우리는 여러 개의 공개된 문제들을 밝혀내고, 발전의 가능성 있는 미래의 방향을 전망하며, 더 큰 규모로 확장하기, 기술의 기능적 사실성을 명확히 하기, 더 많은 임무에 일반화 하기, 그 기여를 설명하는 것을 제안하고 있다.",
      "upvotes": 24,
      "discussionId": "67eb57053475e7b135788624",
      "ai_keywords": [
        "test-time scaling",
        "test-time computing",
        "large language models",
        "specialized reasoning tasks",
        "open-ended Q&A",
        "multidimensional framework",
        "what to scale",
        "how to scale",
        "where to scale",
        "how well to scale",
        "assessment aspects",
        "functional roles",
        "developmental trajectories",
        "practical deployment",
        "open challenges",
        "attributions"
      ]
    },
    "publishedAt": "2025-03-31T11:46:15.000Z",
    "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models",
    "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24235.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a42f22c683d02f5b63320c",
      "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
      "fullname": "Qiyuan Zhang",
      "name": "DonJoey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24388",
      "authors": [
        {
          "_id": "67eb544113ca8dcb9ccb991b",
          "name": "Zhonghan Zhao",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991c",
          "user": {
            "_id": "64e8505321540e1da3226b54",
            "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
            "isPro": false,
            "fullname": "Wenwei Zhang",
            "user": "ZwwWayne",
            "type": "user"
          },
          "name": "Wenwei Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:59:04.743Z",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991d",
          "name": "Haian Huang",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991e",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991f",
          "user": {
            "_id": "64070c5c4dc5f2846c925e93",
            "avatarUrl": "/avatars/ac2d7c1cd4ecccd6a88b85767c963ec7.svg",
            "isPro": false,
            "fullname": "Gao Jianfei",
            "user": "pppppM",
            "type": "user"
          },
          "name": "Jianfei Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:59:34.182Z",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb9920",
          "user": {
            "_id": "64c9033c5381684d3eaac7f1",
            "avatarUrl": "/avatars/07d36ca193826044b0df04e3602b9ef8.svg",
            "isPro": false,
            "fullname": "Gaoang Wang",
            "user": "GaoangWang",
            "type": "user"
          },
          "name": "Gaoang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:59:40.156Z",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb9921",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:59:52.000Z",
      "submittedOnDailyAt": "2025-04-01T01:27:12.837Z",
      "title": "RIG: 종말에서 종말까지 일반적인 정책의 이유와 상상의 조화",
      "submittedOnDailyBy": {
        "_id": "6601196cc91ba4c08ad6e270",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
        "isPro": false,
        "fullname": "yuzhe gu",
        "user": "vanilla1116",
        "type": "user"
      },
      "summary": "행동 전으로 추론하고 잠재적 결과를 상상하는 것은 복잡한 개방형 세계 환경에서 동작하는 몸체화된 에이전트에 필수적이다. 그러나 이전의 연구는 이 두 가지 능력 중 하나만 끝에서 끝으로 에이전트에 통합하거나, 에이전트 시스템에 여러 전문 모델을 통합하여 정책의 학습 효율성과 일반화에 제한을 걸었다. 따라서 이 논문은 첫 번째로 추론과 상상을 끝에서 끝으로 통합하는 일반화된 정책인 RIG(Reasoning and Imagination Generalist)을 시도한다. RIG를 끝에서 끝으로 훈련하기 위해, 기존 에이전트에서 수집한 경로에서 상상과 추론의 내용을 점차적으로 통합하고 풍부히 하는 데이터 파이프라인을 구축했다. 추론과 다음 이미지 생성의 결합 학습은 추론, 행동, 환경의 동력 사이의 내재적인 상관관계를 명확하게 모델링하여 이전의 연구보다 17배 이상의 샘플 효율성과 일반화에서 개선을 나타냈다. 추론 단계에서, RIG는 다음 행동을 추론하고 잠재적 행동을 생성하고, 그 후 행동 결과를 예측하여 에이전트가 실제 행동을 취하기 전에 상상을 기반으로 검토하고 자기 교정할 기회를 제공했다. 실험 결과를 통해 추론과 상상의 조화는 일반화된 정책의 강건성, 일반화, 그리고 상호작용성을 개선하고, 테스트 시간에 성능을 향상시킬 수 있는 테스트 시간 확장을 가능하게 한다.",
      "upvotes": 20,
      "discussionId": "67eb544213ca8dcb9ccb9963"
    },
    "publishedAt": "2025-03-31T13:59:52.000Z",
    "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy",
    "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than 17times sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24388.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.24370",
      "authors": [
        {
          "_id": "67eb4fff13ca8dcb9cca5f9b",
          "user": {
            "_id": "62fae9328e137d7c4b896498",
            "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
            "isPro": false,
            "fullname": "Tong Wu",
            "user": "tongwu2020",
            "type": "user"
          },
          "name": "Tong Wu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:31:28.689Z",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9c",
          "user": {
            "_id": "653319c135ad8b9e58a6b874",
            "avatarUrl": "/avatars/755efb5829f3b6d3cef886fee26e1ba9.svg",
            "isPro": false,
            "fullname": "Chong Xiang",
            "user": "cxiang",
            "type": "user"
          },
          "name": "Chong Xiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:00:19.547Z",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9d",
          "name": "Jiachen T. Wang",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9e",
          "name": "Prateek Mittal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:50:13.000Z",
      "submittedOnDailyAt": "2025-04-01T01:02:34.304Z",
      "title": "구상적 접근에 의한 추론 모델의 효과적인 제어",
      "submittedOnDailyBy": {
        "_id": "62fae9328e137d7c4b896498",
        "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
        "isPro": false,
        "fullname": "Tong Wu",
        "user": "tongwu2020",
        "type": "user"
      },
      "summary": "이론을 부여받은 대규모 언어 모델(LLMs)은 최종적인 답을 생성하기 전에 명시된 이론적 논리 단계를 생성하여 복잡한 문제를 해결하는 데 모델의 우수한 성능을 달성합니다. 본 논문에서는 이新兴의 생성 프레임워크가 모델의 행동을 더 세밀하게 제어하는 특별한 기회를 제공하여 있음을 보여주고 있습니다. 우리는 특정한 사고 토큰을 전략적으로 삽입하거나 수정함으로써 LLMs의 내부의 이론적 논리 프로세스를 명시하고 가이드하는 새로운 패러다임인 \"Thinking Intervention\"을 제안합니다. IFEval의 지시에 따라, SEP의 지시 계층, XSTest와 SORRY-Bench의 안전성 일치를 포함한 여러 태스크에 대해 상세한 평가를 수행했습니다. 결과적으로, Thinking Intervention는 기준적인 프롬프트 접근 방식을 크게 초월하며, DeepSeek R1 오픈소스 모델을 사용하여 지시에 따라 6.7%의 정확도 향상, 지시 계층에 대한 이론적 논리 개선이 15.4%, 불안정한 프롬프트의 거부율이 40.0% 증가를 실현했습니다. 이어서, 우리의 연구는 이론적 논리 LLMs의 제어에 대한 새로운 연구의 가능성을 개척하고 있습니다.",
      "upvotes": 11,
      "discussionId": "67eb500013ca8dcb9cca5fe0",
      "ai_keywords": [
        "Reasoning-enhanced large language models (LLMs)",
        "intermediate reasoning steps",
        "Thinking Intervention",
        "thinking tokens",
        "instruction following",
        "IFEval",
        "instruction hierarchy",
        "SEP",
        "safety alignment",
        "XSTest",
        "SORRY-Bench",
        "open-source DeepSeek R1 models"
      ]
    },
    "publishedAt": "2025-03-31T13:50:13.000Z",
    "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
    "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fae9328e137d7c4b896498",
      "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
      "fullname": "Tong Wu",
      "name": "tongwu2020",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24364",
      "authors": [
        {
          "_id": "67eb6e6088a08fae617860f3",
          "user": {
            "_id": "600b381d3cc3b87db94bc0ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
            "isPro": false,
            "fullname": "Łukasz Borchmann",
            "user": "Borchmann",
            "type": "user"
          },
          "name": "Łukasz Borchmann",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-01T05:01:23.216Z",
          "hidden": false
        },
        {
          "_id": "67eb6e6088a08fae617860f4",
          "user": {
            "_id": "66c5e93e8f14c260be9d9f63",
            "avatarUrl": "/avatars/f4bf15e23923ef3256d3f01a3278d8bc.svg",
            "isPro": false,
            "fullname": "Marek Wydmuch",
            "user": "sfc-mwydmuch",
            "type": "user"
          },
          "name": "Marek Wydmuch",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:01:57.075Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/600b381d3cc3b87db94bc0ce/ucxS0pF0YXK8TDRhoezTE.qt"
      ],
      "publishedAt": "2025-03-31T17:43:36.000Z",
      "submittedOnDailyAt": "2025-04-01T03:14:34.239Z",
      "title": "Query and Conquer: Execution-Guided SQL Generation\n\n쿼리와 대결: 실행을 안내하는 SQL 생성\n\n(注意：虽然要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，我在翻译时尽量保持了原文的含义和格式。如果需要进一步的解释或文本格式调整，请告知。)",
      "submittedOnDailyBy": {
        "_id": "600b381d3cc3b87db94bc0ce",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
        "isPro": false,
        "fullname": "Łukasz Borchmann",
        "user": "Borchmann",
        "type": "user"
      },
      "summary": "우리는 텍스트로부터 SQL 태스크의 정확도를 크게 향상시킬 수 있는 복잡한 출력을 생성하는 새로운 접근법을 제안합니다. 우리 방식은 실행 결과를 활용하여 가장 문법적으로 일치하는 쿼리를 선택하기 위해 여러 후보를 선택하고, 계산량이 많은 추론 방법(예: o1, o3-mini, DeepSeek R1)을 초과하는 작은, 비용 효율적인 모델을 가능하게 합니다. 또한 추론 비용을 30배 줄입니다. 이 방법은 기존 모델과 쉽게 통합할 수 있으며, 실용적이고 scalable한 최신 SQL 생성의 패스워드를 제공합니다.",
      "upvotes": 10,
      "discussionId": "67eb6e6188a08fae6178613f",
      "ai_keywords": [
        "text-to-SQL",
        "execution results",
        "semantically consistent",
        "query",
        "candidates",
        "models",
        "reasoning methods",
        "o1",
        "o3-mini",
        "DeepSeek R1",
        "inference cost",
        "SQL generation"
      ]
    },
    "publishedAt": "2025-03-31T13:43:36.000Z",
    "title": "Query and Conquer: Execution-Guided SQL Generation",
    "summary": "We propose a novel approach for generating complex outputs that significantly\nimproves accuracy in text-to-SQL tasks. Our method leverages execution results\nto select the most semantically consistent query from multiple candidates,\nenabling smaller, cost-effective models to surpass computationally intensive\nreasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference\ncost by as much as 30 times. It integrates effortlessly with existing models,\noffering a practical and scalable pathway to state-of-the-art SQL generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/600b381d3cc3b87db94bc0ce/ucxS0pF0YXK8TDRhoezTE.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24364.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "600b381d3cc3b87db94bc0ce",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
      "fullname": "Łukasz Borchmann",
      "name": "Borchmann",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23284",
      "authors": [
        {
          "_id": "67eb5280aeab4ce97de07134",
          "user": {
            "_id": "6424538b9f9e65b42389920e",
            "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
            "isPro": false,
            "fullname": "Feng-Lin Liu",
            "user": "Okrin",
            "type": "user"
          },
          "name": "Feng-Lin Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:05.907Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07135",
          "user": {
            "_id": "662cd8b9322afcbae53fb06e",
            "avatarUrl": "/avatars/9847f5c2282d49e61e76a0a303e0b2b1.svg",
            "isPro": false,
            "fullname": "fuhongbo",
            "user": "fuhongbo",
            "type": "user"
          },
          "name": "Hongbo Fu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:01.616Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07136",
          "user": {
            "_id": "60e272ca6c78a8c122b12127",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
            "isPro": false,
            "fullname": "Xintao Wang",
            "user": "Xintao",
            "type": "user"
          },
          "name": "Xintao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:09.910Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07137",
          "user": {
            "_id": "6360d9f0472131c3bc4f61df",
            "avatarUrl": "/avatars/c5d884e5ef19b781e3405aba6dd68ca8.svg",
            "isPro": false,
            "fullname": "WeicaiYe",
            "user": "WeicaiYe",
            "type": "user"
          },
          "name": "Weicai Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:16.323Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07138",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07139",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:30.236Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de0713a",
          "name": "Lin Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T02:44:09.000Z",
      "submittedOnDailyAt": "2025-04-01T02:19:10.110Z",
      "title": "スケッチ비디오: 스ケッチ기반의 비디오 생성과 편집",
      "submittedOnDailyBy": {
        "_id": "6424538b9f9e65b42389920e",
        "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
        "isPro": false,
        "fullname": "Feng-Lin Liu",
        "user": "Okrin",
        "type": "user"
      },
      "summary": "텍스트 플롯이나 이미지에 기반한 비디오 생성과 편집에 있어서, 발전이 이루어지고 있지만, 텍스트만 사용하여 글로벌 레이아웃과 일반화의 세부 사항을 정확하게 제어하고 동작 제어와 지역적인 편집을 이미지통과로 수행하는 것이 어려운 문제점이 남아 있습니다. 본 논문에서는, 비디오 생성에 대한 스케치 기반의 공간 및 동작 제어를 실현하고, 실제 또는 합성 비디오의 미세한 그레이스レ디닝 편집을 지원하는 것을 목표로 합니다. DiT 비디오 생성 모델을 기반으로, 스케치 제어 블록을 가진 메모리 효율적인 제어 구조를 제안합니다. 스케치는 한 개 또는 두 개의 키 프레임 (임의의 시간점에서)에 그려져, 간단한 인터랙션을 가능하게 합니다. 이러한 시간적으로 희박한 스케치 조건을 모든 프레임에 전파하기 위해, 키 프레임과 각 비디오 프레임 간의 관계를 분석하는 중간 프레임 어텐션 구조를 제안합니다. 스케치 기반의 비디오 편집에서, 새로 편집된 내용을 유지하기 위해 추가한 비디오 삽입 모듈을 설계합니다. 추론 시에는, 미디젼의 정확한 저장에 있어서 잠재적 융합을 사용합니다. 확장된 실험은, 우리의 스케치 비디오가 제어 가능한 비디오 생성과 편집에서 상위 성능을 달성함을 보여줍니다.",
      "upvotes": 9,
      "discussionId": "67eb5286aeab4ce97de07320",
      "githubRepo": "https://github.com/IGLICT/SketchVideo",
      "ai_keywords": [
        "DiT video generation model",
        "memory-efficient control structure",
        "sketch control blocks",
        "residual features",
        "skipped DiT blocks",
        "temporally sparse sketch conditions",
        "inter-frame attention mechanism",
        "keyframes",
        "video insertion module",
        "spatial feature",
        "dynamic motion",
        "latent fusion",
        "SketchVideo"
      ]
    },
    "publishedAt": "2025-03-29T22:44:09.000Z",
    "title": "SketchVideo: Sketch-based Video Generation and Editing",
    "summary": "Video generation and editing conditioned on text prompts or images have\nundergone significant advancements. However, challenges remain in accurately\ncontrolling global layout and geometry details solely by texts, and supporting\nmotion control and local modification through images. In this paper, we aim to\nachieve sketch-based spatial and motion control for video generation and\nsupport fine-grained editing of real or synthetic videos. Based on the DiT\nvideo generation model, we propose a memory-efficient control structure with\nsketch control blocks that predict residual features of skipped DiT blocks.\nSketches are drawn on one or two keyframes (at arbitrary time points) for easy\ninteraction. To propagate such temporally sparse sketch conditions across all\nframes, we propose an inter-frame attention mechanism to analyze the\nrelationship between the keyframes and each video frame. For sketch-based video\nediting, we design an additional video insertion module that maintains\nconsistency between the newly edited content and the original video's spatial\nfeature and dynamic motion. During inference, we use latent fusion for the\naccurate preservation of unedited regions. Extensive experiments demonstrate\nthat our SketchVideo achieves superior performance in controllable video\ngeneration and editing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23284.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6424538b9f9e65b42389920e",
      "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
      "fullname": "Feng-Lin Liu",
      "name": "Okrin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18809",
      "authors": [
        {
          "_id": "67eaa0f83ace6eb46745a9fe",
          "user": {
            "_id": "674f43d6df6fa102409f6d1a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
            "isPro": false,
            "fullname": "Augusto B. Corrêa",
            "user": "abcorrea",
            "type": "user"
          },
          "name": "Augusto B. Corrêa",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:12.565Z",
          "hidden": false
        },
        {
          "_id": "67eaa0f83ace6eb46745a9ff",
          "user": {
            "_id": "662fb9c891587703a677856e",
            "avatarUrl": "/avatars/9cb7f035a513279532fc205ce9c5902c.svg",
            "isPro": false,
            "fullname": "Andre Grahl Pereira",
            "user": "andregrahl",
            "type": "user"
          },
          "name": "André G. Pereira",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:13.942Z",
          "hidden": false
        },
        {
          "_id": "67eaa0f83ace6eb46745aa00",
          "user": {
            "_id": "66f3dfd4b8703dde248f6d26",
            "avatarUrl": "/avatars/c199c91d422500cc7c7556569291644d.svg",
            "isPro": false,
            "fullname": "Jendrik Seipp",
            "user": "jendrikseipp",
            "type": "user"
          },
          "name": "Jendrik Seipp",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:39.862Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T15:50:20.000Z",
      "submittedOnDailyAt": "2025-04-01T00:45:14.321Z",
      "title": "고전적인 계획에 LLM 제네레이터 휴리스틱을 적용하기: Python 코드로 최尖端 상태를 도전하기",
      "submittedOnDailyBy": {
        "_id": "674f43d6df6fa102409f6d1a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
        "isPro": false,
        "fullname": "Augusto B. Corrêa",
        "user": "abcorrea",
        "type": "user"
      },
      "summary": "최근, 대규모 언어 모델(LLMs)는 다양한 인공지능 문제에 있어서 놀라운 능력을 보여주고 있다. 그러나, 계획 태스크의 세부적인 정의를 제시해도 신뢰할 수 있는 계획을 수행할 수 없으며, LLMs의 계획 능력 향상을 위한 시도, 예를 들어 chain-of-thought prompting, fine-tuning, 명시적인 '이유' 설정 등이, 올바른 계획을 생성하지 못하고, 더 큰 태스크에 대해도 일반화할 수 없기 때문에 많은 경우이다. 본 논문에서는, LLMs를 사용하여, 증가된 크기의 외분포 태스크에서도 올바른 계획을 생성하는 방법을 제시하고 있다. 특정한 계획 도메인에 대해, LLM을 사용하여 Python 코드의 형태로 도메인 의존성이 있는 휴리스틱 함수를 생성하여, 깜빡이 베스트 탔스트 탐색으로 훈련 태스크에서 평가하고, 가장 강한 것을 선택하는 것이다. 그 결과, LLM에 의해 생성된 휴리스틱 함수는, 고전적인 계획의 가장 선진적인 도메인 독립성 휴리스틱 함수보다 많은未见 테스트 태스크를 해결할 수 있다. 도메인 의존성 가장 강한 학습 알고리즘과 비교하여도, 그 정도로 강력한 것을 보여주고 있다. 이러한 발견은, 전문가이지만, 개념적인 구현은 최적화되지 않은 Python 계획러에 기반하고, 기준은 고도로 최적화된 C++ 코드에 기반하고 있기 때문에, 특히 놀라울 정도로 놀라울 수 있다. 특정 도메인에서, LLM에 의해 생성된 휴리스틱 함수는 베이스라인보다 적은 상태를 확장하는 상태를 확장하며, 그들은 상태를 효율적으로 확장하는 데만 아니라, 선진적인 휴리스틱 함수보다 더 정보적인 것을 보여주고 있다. 전체적으로, 우리의 결과를 통해, 계획 휴리스틱 함수 프로그램의 샘플링이 LLMs의 계획 능력이 크게 향상될 수 있다는 것을 보여준다.",
      "upvotes": 8,
      "discussionId": "67eaa0f93ace6eb46745aa3e",
      "ai_keywords": [
        "large language models (LLMs)",
        "chain-of-thought prompting",
        "fine-tuning",
        "reasoning",
        "planning domain",
        "domain-dependent heuristic functions",
        "Python code",
        "greedy best-first search",
        "state-of-the-art domain-independent heuristics",
        "domain-dependent planning",
        "unoptimized Python planner",
        "highly optimized C++ code",
        "planning heuristic function programs"
      ]
    },
    "publishedAt": "2025-03-24T11:50:20.000Z",
    "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code",
    "summary": "In recent years, large language models (LLMs) have shown remarkable\ncapabilities in various artificial intelligence problems. However, they fail to\nplan reliably, even when prompted with a detailed definition of the planning\ntask. Attempts to improve their planning capabilities, such as chain-of-thought\nprompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans\nand usually fail to generalize to larger tasks. In this paper, we show how to\nuse LLMs to generate correct plans, even for out-of-distribution tasks of\nincreasing size. For a given planning domain, we ask an LLM to generate several\ndomain-dependent heuristic functions in the form of Python code, evaluate them\non a set of training tasks within a greedy best-first search, and choose the\nstrongest one. The resulting LLM-generated heuristics solve many more unseen\ntest tasks than state-of-the-art domain-independent heuristics for classical\nplanning. They are even competitive with the strongest learning algorithm for\ndomain-dependent planning. These findings are especially remarkable given that\nour proof-of-concept implementation is based on an unoptimized Python planner\nand the baselines all build upon highly optimized C++ code. In some domains,\nthe LLM-generated heuristics expand fewer states than the baselines, revealing\nthat they are not only efficiently computable, but sometimes even more\ninformative than the state-of-the-art heuristics. Overall, our results show\nthat sampling a set of planning heuristic function programs can significantly\nimprove the planning capabilities of LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18809.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674f43d6df6fa102409f6d1a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
      "fullname": "Augusto B. Corrêa",
      "name": "abcorrea",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24115",
      "authors": [
        {
          "_id": "67eb5116d3a707c0a5b02bd1",
          "user": {
            "_id": "64a0ed5ed5374ca472cfb0ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
            "isPro": false,
            "fullname": "ZhimingMa",
            "user": "JimmyMa99",
            "type": "user"
          },
          "name": "Zhiming Ma",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:36:07.612Z",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd2",
          "user": {
            "_id": "6385f7b969634850f8ddd541",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669723465271-noauth.png",
            "isPro": false,
            "fullname": "Peidong Wang",
            "user": "WDong",
            "type": "user"
          },
          "name": "Peidong Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:36:07.612Z",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd3",
          "user": {
            "_id": "6466d57bf3e78d1d6be0505c",
            "avatarUrl": "/avatars/9659b7d0f6fa51efc127afb7a1ba14b1.svg",
            "isPro": false,
            "fullname": "HuangMinhua",
            "user": "HuangMinhua",
            "type": "user"
          },
          "name": "Minhua Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:02:15.224Z",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd4",
          "name": "Jingpeng Wang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd5",
          "name": "Kai Wu",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd6",
          "name": "Xiangzhao Lv",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd7",
          "name": "Yachun Pang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd8",
          "name": "Yin Yang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd9",
          "name": "Wenjie Tang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bda",
          "name": "Yuchen Kang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/PsAJTW9JTyrHqtjQP0lLJ.png"
      ],
      "publishedAt": "2025-03-31T14:06:17.000Z",
      "submittedOnDailyAt": "2025-04-01T01:08:09.201Z",
      "title": "TeleAntiFraud-28k: 전화 위반 방지용 음성-문자 데이터셋",
      "submittedOnDailyBy": {
        "_id": "64a0ed5ed5374ca472cfb0ac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
        "isPro": false,
        "fullname": "ZhimingMa",
        "user": "JimmyMa99",
        "type": "user"
      },
      "summary": "전화 사기의 감지는 고품질의 다모델 훈련 데이터의 부족으로 큰 문제점을 가지고 있습니다. 이를 채워나가기 위해, 우리는 자동화된 전화 사기 분석에 적합한 첫 번째 오픈 소스 오디오 텍스트 짧은 시간 데이터 세트인 \"TeleAntiFraud-28k\"를 소개합니다. 데이터 세트는 다음과 같은 3가지 전략으로 구축되었습니다: 1) 자동 음성 인식(ASR)에 의한 전화 레코더의 텍스트 기록(원본 오디오를 비롯한 무명화된 데이터)를 사용하여 프라이버시 보호 텍스트 샘플의 생성, 텍스트 원소스 모델의 재현에 의한 리アル워일의 일치성을 보장; 2) 실제 ASR 출력에 기반한 대규모 언어 모델(LLM)에 의한 자기 인스톰 샘플링을 통해 시나리오의 커버 범위를 확장; 3) 현재의 사기手法를 시뮬레이션하기 위한 다アグリエント의 혼성. 생성된 데이터 세트는 28,511건의 엄격하게 처리된 스피치 텍스트 페어를 포함하고, 사기 이유의 상세한 注釈을 붙였습니다. 데이터 세트는 시나리오 분류, 사기 감지, 사기 종류 분류의 3가지의 태스크로 나뉩니다. 또한, 우리는 데이터 세트에서 비례적으로 샘플링된 인스턴스를 포함하는 표준화된 평가 벤치마크인 \"TeleAntiFraud-Bench\"를 구축하고, 전화 사기 감지 태스크의 모델 성능의 체계적인 테스트를 촉진합니다. 또한, 혼합 데이터에 기반한 산업 최적화 된 훈련(SFT) 모델을 제공하고, 데이터 처리 프레임워크를 오픈 소스화하여 커뮤니티 주도의 데이터 세트 확장을 가능하게 합니다. 이 연구는 데이터 프라이버시와 시나리오의 다양성의 중요한 문제를 해결하면서, 다모델 안티 플라우드 연구의 기초를 구축하는 것을 목표로 합니다. 이 프로젝트는 https://github.com/JimmyMa99/TeleAntiFraud에서 릴리즈 됩니다.",
      "upvotes": 7,
      "discussionId": "67eb5117d3a707c0a5b02c4c",
      "ai_keywords": [
        "automatically speech recognition (ASR)",
        "text-to-speech (TTS)",
        "large language model (LLM)",
        "self-instruction sampling",
        "multi-agent adversarial synthesis",
        "supervised fine-tuning (SFT)",
        "hybrid real/synthetic data"
      ]
    },
    "publishedAt": "2025-03-31T10:06:17.000Z",
    "title": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
    "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/PsAJTW9JTyrHqtjQP0lLJ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24115.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a0ed5ed5374ca472cfb0ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
      "fullname": "ZhimingMa",
      "name": "JimmyMa99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23077",
      "authors": [
        {
          "_id": "67eb58c71e23a7499b683cce",
          "user": {
            "_id": "6650c77a74664a42ddfb9187",
            "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
            "isPro": false,
            "fullname": "yueliu1999",
            "user": "yueliu1999",
            "type": "user"
          },
          "name": "Yue Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:11:01.812Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683ccf",
          "name": "Jiaying Wu",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd0",
          "name": "Yufei He",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd1",
          "user": {
            "_id": "62728f4f6253fe2068da1021",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
            "isPro": false,
            "fullname": "Hongcheng Gao",
            "user": "HongchengGao",
            "type": "user"
          },
          "name": "Hongcheng Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:11:54.944Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd2",
          "user": {
            "_id": "67c7a9fb27c2e81cf8660375",
            "avatarUrl": "/avatars/a5129cca93a31d4b730af4c543051d8e.svg",
            "isPro": false,
            "fullname": "Hongyu Chen",
            "user": "HongyuChen",
            "type": "user"
          },
          "name": "Hongyu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:05.159Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd3",
          "user": {
            "_id": "642577e06d0f0f5f1dc68904",
            "avatarUrl": "/avatars/3df7cfcf9ca6cd9c9e9b10a73f4efc35.svg",
            "isPro": false,
            "fullname": "Bibaolong",
            "user": "Bibaolong",
            "type": "user"
          },
          "name": "Baolong Bi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:18.251Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd4",
          "user": {
            "_id": "669e19e5dac1eb34c0f5f505",
            "avatarUrl": "/avatars/bec7d1d1dac2ad6570844d1f00e7df0a.svg",
            "isPro": false,
            "fullname": "Jiaheng Zhang",
            "user": "jiaheng233",
            "type": "user"
          },
          "name": "Jiaheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:23.776Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd5",
          "user": {
            "_id": "66221f1a90f3fd333c4ec52e",
            "avatarUrl": "/avatars/a3173d9603a69020ec24170831c97c2f.svg",
            "isPro": false,
            "fullname": "Zhiqi Huang",
            "user": "Angelalilyer",
            "type": "user"
          },
          "name": "Zhiqi Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:36.398Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd6",
          "user": {
            "_id": "651d8032c50012d33e914f2f",
            "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
            "isPro": false,
            "fullname": "Bryan Hooi",
            "user": "bhooi",
            "type": "user"
          },
          "name": "Bryan Hooi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:42.880Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-29T13:27:46.000Z",
      "submittedOnDailyAt": "2025-04-01T01:39:12.154Z",
      "title": "효율적인 추론을 실현한 대규모 논리 모델의 평가: 개요",
      "submittedOnDailyBy": {
        "_id": "6650c77a74664a42ddfb9187",
        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
        "isPro": false,
        "fullname": "yueliu1999",
        "user": "yueliu1999",
        "type": "user"
      },
      "summary": "대논리 모둔형（LRMs）는 대언어 모둔형（LLMs）의 논리 능력이 뚜렷하게 향상되고 복잡한 태스크 해결에서 기대되는 성능을 발휘하고 있습니다. 그러나 논리적인 계산 프로세스에 의해 토큰의 사용, 메모리 소모, 추론 시간 등에서 불리한 특성이 존재합니다. 따라서 본 조사는 LRMs에 특화된 효율적인 추론 방법을 검토하여 토큰의 불리한 특성을 억제하면서 논리의 질을 유지하는 데 초점을 두고 있습니다. 먼저, 최근의 방법을 2가지의 주요 카테고리로 분류하기 위한 테크놀로지를 소개합니다. (a) 명시적 간략화 코인 스토크（CoT）은 토큰을 줄여 명시적인 논리 구조를 유지합니다. (b) 은닉 표현 내의 논리 단계를 코인 스토크（CoT）으로 인코딩하는 은닉 코인 스토크（CoT）입니다. 그리고 그 장점과 단점을 논의합니다. 다음으로, 기존 방법의 성능과 효율성을 실험적으로 분석합니다. 또한 이 분야의 개방된 문제를 논의합니다. 그리고 인간 중심의 논리 제어, 논리의 설명성 및 효율성의 균형, 효율적인 논리의 안전성 보장, 효율적인 논리의 광범위한 적용에 대해 논의합니다. 또한 모델 통합, 새로운 아키텍처, 에이전트 로터러와 같은 기술로 LRMs의 추론 효율을 향상시키기 위한 주요 요소를 특징적으로 강조합니다. 이 연구는 이 풍부한 분야의 문제 해결을 위해 연구자에게 유효한 가이드 역할을 하길 바랍니다.",
      "upvotes": 7,
      "discussionId": "67eb58c81e23a7499b683d12",
      "ai_keywords": [
        "Chain-of-Thought (CoT)",
        "explicit compact Chain-of-Thought (CoT)",
        "implicit latent CoT",
        "model merging",
        "agent routers"
      ]
    },
    "publishedAt": "2025-03-29T09:27:46.000Z",
    "title": "Efficient Inference for Large Reasoning Models: A Survey",
    "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23077.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6650c77a74664a42ddfb9187",
      "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
      "fullname": "yueliu1999",
      "name": "yueliu1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24290",
      "authors": [
        {
          "_id": "67eb762381e530baa56dc830",
          "user": {
            "_id": "625026b7d2d191ac43320c5e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg",
            "isPro": false,
            "fullname": "Jingcheng Hu",
            "user": "reign12",
            "type": "user"
          },
          "name": "Jingcheng Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:09:02.123Z",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc831",
          "user": {
            "_id": "664ae39ab5e5f95dc6209365",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg",
            "isPro": false,
            "fullname": "Yinmin Zhang",
            "user": "YinminZhang",
            "type": "user"
          },
          "name": "Yinmin Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:09:09.884Z",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc832",
          "name": "Qi Han",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc833",
          "user": {
            "_id": "60d4440fe648443279aaffd8",
            "avatarUrl": "/avatars/bf7209c1f14ae120f5bfda5fda1301b7.svg",
            "isPro": false,
            "fullname": "Daxin Jiang",
            "user": "djiang",
            "type": "user"
          },
          "name": "Daxin Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:09:22.914Z",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc834",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc835",
          "name": "Heung-Yeung Shum",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T16:36:05.000Z",
      "submittedOnDailyAt": "2025-04-01T03:44:53.609Z",
      "title": "Open-Reasoner-Zero: 기초 모델에 기반한 강화 학습의 확장에 있어서 오픈 소스 접근법",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "오픈・라디오ン・제로는 대규모적인 이론적인 RL 훈련의 첫 번째 오픈 소스 구현을 소개합니다. 이 구현은 scalability, simplicity, accessibility를 중점으로 설계되었습니다. 확장 실험을 통해, 최소주의적인 접근, 베지아의 PPO(GAE(lambda=1, gamma=1))와 직관적인 규칙 기반의 보상을 사용하며, 모든 KL 정규화 포함 없이, 답변의 길이와 벤치마크의 성능을 동시에 scalable 하게 만들 수 있음을 보여줍니다. DeepSeek-R1-Zero와 같은 기반 모델을 사용하며, AIME2024, MATH500, GPQA Diamond 벤치마크에서 상위 성능을 달성하며, 효율적이고, DeepSeek-R1-Zero 파이프라인에 비해 충분한 훈련 단계를 필요로 합니다. 오픈 소스 정신에 기반하여, 소스 코드, 파라미터 설정, 훈련 데이터, 모델 가중치의 각 크기를 공개합니다.",
      "upvotes": 6,
      "discussionId": "67eb762481e530baa56dc872",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "vanilla PPO",
        "GAE ($\\lambda=1$, $\\gamma=1$)",
        "rule-based rewards",
        "KL regularization",
        "response length",
        "benchmark performance",
        "AIME2024",
        "MATH500",
        "GPQA Diamond benchmark",
        "training steps"
      ]
    },
    "publishedAt": "2025-03-31T12:36:05.000Z",
    "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model",
    "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of\nlarge-scale reasoning-oriented RL training focusing on scalability, simplicity\nand accessibility. Through extensive experiments, we demonstrate that a\nminimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and\nstraightforward rule-based rewards, without any KL regularization, is\nsufficient to scale up both response length and benchmark performance, similar\nto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as\nDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on\nAIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating\nremarkable efficiency -- requiring only a tenth of the training steps, compared\nto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our\nsource code, parameter settings, training data, and model weights across\nvarious sizes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24290.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6543
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23829",
      "authors": [
        {
          "_id": "67eb759cb9fa8908e1934f21",
          "name": "Yi Su",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f22",
          "user": {
            "_id": "62d58fd53bf5e059f7cc3245",
            "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
            "isPro": false,
            "fullname": "Dian Yu",
            "user": "yudian",
            "type": "user"
          },
          "name": "Dian Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:13:47.119Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f23",
          "user": {
            "_id": "64c94eddcb2f1bf0e7db5a4d",
            "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
            "isPro": false,
            "fullname": "Linfeng Song",
            "user": "freesunshine0316",
            "type": "user"
          },
          "name": "Linfeng Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:13:54.065Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f24",
          "user": {
            "_id": "6670e285b0c03c4e9d6e0985",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uCZHm4gKSHZ2b0hpHWgZv.jpeg",
            "isPro": false,
            "fullname": "Juntao Li",
            "user": "douvleplus",
            "type": "user"
          },
          "name": "Juntao Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:03.120Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f25",
          "user": {
            "_id": "65147a1426fbd558dbd08f1b",
            "avatarUrl": "/avatars/86574ee2d5c22e940be1c4e50be88675.svg",
            "isPro": false,
            "fullname": "Haitao Mi",
            "user": "haitaominlp",
            "type": "user"
          },
          "name": "Haitao Mi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:10.594Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f26",
          "user": {
            "_id": "67485743561b1e6f9579389f",
            "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
            "isPro": false,
            "fullname": "Zhaopeng Tu",
            "user": "zptu",
            "type": "user"
          },
          "name": "Zhaopeng Tu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:16.978Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f27",
          "name": "Min Zhang",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f28",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T08:22:49.000Z",
      "submittedOnDailyAt": "2025-04-01T03:42:19.595Z",
      "title": "다양한 분야에서 확인 가능한 보상을 가지는 RL의 확장",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "강화학습(RL)에 가능한 보상(RLVR)을 도입한 것은 수학적인 이유와 코딩 태스크에서 구조화된 기준답안이 있는 경우 기대되는 성과를 보여주고 있습니다. 그러나 이러한 방법의 광범위한 적용 가능성은 아직 조사가 부족합니다. 본 논문에서는 의료, 화학, 심리학, 경제학 등 더 다양한 분야에 대한 RLVR의 확장을 연구하고 있습니다. 기준답안이 있는 경우, 다양한 규모의 언어 모델(LLMs)이 두 값 판단에 높은 일치율을 보여주고, 영역 고유의 보상 모델의 큰 注釈의 필요성을 의심하는 것을 관찰하고 있습니다. 두 값 보상이 무구조화된 기준답안을 처리할 때의 한계를 해결하기 위해, 모델 기반의 소프트 스코어를 RLVR에 추가하고, 그 유연성을 향상시킵니다. 실험 결과를 통해, 진행된 생성적인 보상 모델은 영역 고유의 注釈가 필요하지 않다는 것을 보여주고, 强化学習에 신뢰할 수 있는 보상 신호를 제공합니다. 기초 모델을 다른 RL 알고리즘으로 최적화하고, 우리의 보상 모델을 대상으로, Qwen2.5-72B-Instruct와 DeepSeek-R1-Distill-Qwen-32B 등 가장 선진한 오픈소스의 대응 LLMs을 크게 초월하는 성능을 얻는 정책을 얻었습니다. 이는 자유 형식의 답안의 영역에서 우수한 성능을 보여주고, RLVR의 강건성과 scalability를 강화하며, 실세계의 적용에서 노이즈와 약한 라벨을 포함하는 경우에도 실용적이라는 것을 보여줍니다.",
      "upvotes": 5,
      "discussionId": "67eb759db9fa8908e1934f62",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "verifiable rewards (RLVR)",
        "mathematical reasoning",
        "coding tasks",
        "well-structured reference answers",
        "diverse domains",
        "medicine",
        "chemistry",
        "psychology",
        "economics",
        "large language models (LLMs)",
        "binary judgments",
        "domain-specific reward models",
        "model-based soft scoring",
        "distilled generative reward model",
        "effective cross-domain verifier",
        "reward signals",
        "fine-tuning",
        "base 7B model",
        "RL algorithms",
        "state-of-the-art open-source aligned LLMs",
        "Qwen2.5-72B-Instruct",
        "DeepSeek-R1-Distill-Qwen-32B",
        "free-form answer settings",
        "robustness",
        "scalability",
        "real-world applications",
        "noisy labels",
        "weak labels"
      ]
    },
    "publishedAt": "2025-03-31T04:22:49.000Z",
    "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
    "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement in binary judgments across different large\nlanguage models (LLMs) when objective reference answers exist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporate model-based soft scoring into RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nBy fine-tuning a base 7B model using various RL algorithms against our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23829.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6543
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21694",
      "authors": [
        {
          "_id": "67eb92defa85fe030e2db9e2",
          "user": {
            "_id": "64295d1f4e073875f6a605ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
            "isPro": true,
            "fullname": "Zhiyuan Ma",
            "user": "ZhiyuanthePony",
            "type": "user"
          },
          "name": "Zhiyuan Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:51.486Z",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e3",
          "user": {
            "_id": "672111333ced358bdac2925d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qHNDI3zYRkJZmhCHkSZwK.png",
            "isPro": false,
            "fullname": "Xinyue Liang",
            "user": "DarklordLeto",
            "type": "user"
          },
          "name": "Xinyue Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:58.992Z",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e4",
          "name": "Rongyuan Wu",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e5",
          "name": "Xiangyu Zhu",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e6",
          "name": "Zhen Lei",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e7",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64295d1f4e073875f6a605ac/F5lBcbDKq1_K31O-vxWBg.mp4"
      ],
      "publishedAt": "2025-03-27T16:59:15.000Z",
      "submittedOnDailyAt": "2025-04-01T06:05:21.846Z",
      "title": "진보적인 렌더링 추출: 3D 데이터 없는 인스탠트 텍스트에서 3D 매쉬 생성의 적응을 위한 딥러닝을 활용한 기술",
      "submittedOnDailyBy": {
        "_id": "64295d1f4e073875f6a605ac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
        "isPro": true,
        "fullname": "Zhiyuan Ma",
        "user": "ZhiyuanthePony",
        "type": "user"
      },
      "summary": "높은 기대를 품은 것은 텍스트 플롯에서 쉼을 통해 고품질의 3D 메쉬를 생성하는 모델을 얻는 것입니다. 최근의 시도는, 예를 들어, Stable Diffusion(SD)와 같은 사전 학습된 텍스트에서 이미지로 확장하는 모델을 3D 표현의 생성 모델로 적용하려고 합니다(예를 들어, Triplane). 그러나, 이들은 고품질의 3D 훈련 데이터의 부족으로 인해 품질이 떨어질 경우가 많습니다. 데이터 부족을 극복하기 위해, 우리는 새로운 훈련 스키ーム를 제안하고 있습니다. 이것이 Progressive Rendering Distillation(PRD)라고 불리며, 3D 실제 데이터의 필요를 제거하고, 다점 확장 모델을 경험적으로 학습시켜 SD를 3D 노트 파일 모델로 적용하는 것입니다. 각 훈련 단계에서, PRD는 U-Net을 사용하여, 랜덤한 노이즈로부터 단계적으로 노이즈를 디노이징하는 것을 수행하고, 각 단계에서 디노이징된 라텐트를 3D 출력에 해석합니다. 다점 확장 모델, 예를 들어, MVDream과 RichDreamer는 SD와 함께, 스코어 디스티ル레이션을 통해 텍스트의 일치성을 가지는 테크스처와 일반성을 3D 출력에 집중합니다. PRD는 3D 실제 데이터의 필요를 제거함으로써, 훈련 데이터의 확장과 복잡한 텍스트 플롯에 대한 생성 품질의 향상이 쉽게 됩니다. 또한, PRD는 생성 모델의 추론 속도를 수 단계로 가속화할 수 있습니다. PRD를 사용하여, 우리는 Triplane 모델을 훈련하고 TriplaneTurbo라는 이름을 붙였습니다. TriplaneTurbo는 SD를 Triplane 생성에 적용하기 위해 학습 가능한 파라미터를 늘리만 해도 2.5%까지만 증가합니다. TriplaneTurbo는 이전의 텍스트에서 3D로의 생성 모델과 비교하여 효율성과 품질 모두 뛰어납니다. 특히, 고품질의 3D 메쉬를 1.2초 만에 생성하고, 복잡한 텍스트 입력에도 잘 확장할 수 있습니다. 코드는, https://github.com/theEricMa/TriplaneTurbo에 공개되어 있습니다.",
      "upvotes": 5,
      "discussionId": "67eb92e2fa85fe030e2dbc04",
      "projectPage": "https://theericma.github.io/TriplaneTurbo/",
      "githubRepo": "https://github.com/theEricMa/TriplaneTurbo",
      "ai_keywords": [
        "diffusion models",
        "Stable Diffusion (SD)",
        "3D representations",
        "Progressive Rendering Distillation (PRD)",
        "U-Net",
        "latent from random noise",
        "denoise the latent",
        "3D output",
        "Multi-view diffusion models",
        "MVDream",
        "RichDreamer",
        "score distillation",
        "text-consistent textures",
        "geometries",
        "Triplane generator",
        "TriplaneTurbo",
        "high-quality 3D meshes"
      ]
    },
    "publishedAt": "2025-03-27T12:59:15.000Z",
    "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data",
    "summary": "It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only 2.5%\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64295d1f4e073875f6a605ac/F5lBcbDKq1_K31O-vxWBg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64295d1f4e073875f6a605ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
      "fullname": "Zhiyuan Ma",
      "name": "ZhiyuanthePony",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.19901",
      "authors": [
        {
          "_id": "67eac6433755a17e3cbff585",
          "user": {
            "_id": "6630cc7e9ee8861dd0b9bdbd",
            "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
            "isPro": false,
            "fullname": "Liang Pan",
            "user": "lianganimation",
            "type": "user"
          },
          "name": "Liang Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:34.754Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff586",
          "user": {
            "_id": "649c178950b4be74229d680f",
            "avatarUrl": "/avatars/941dec90fdfc46b9ae23378e3a3113f4.svg",
            "isPro": false,
            "fullname": "Zeshi Yang",
            "user": "Zeshi209",
            "type": "user"
          },
          "name": "Zeshi Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:03.851Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff587",
          "user": {
            "_id": "645223fb01d7bd9555ea399a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
            "isPro": false,
            "fullname": "Zhiyang Dou",
            "user": "frankzydou",
            "type": "user"
          },
          "name": "Zhiyang Dou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:09.328Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff588",
          "user": {
            "_id": "6437a813fac5ea753f1c72d2",
            "avatarUrl": "/avatars/69e60e60497e404149a1dad46649dad4.svg",
            "isPro": false,
            "fullname": "wenjia Wang",
            "user": "WenjiaWang",
            "type": "user"
          },
          "name": "Wenjia Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:22.428Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff589",
          "name": "Buzhen Huang",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff58a",
          "user": {
            "_id": "635f93577c05eb9f59966209",
            "avatarUrl": "/avatars/add48d7e3790a6b500d6c451ef8b0f75.svg",
            "isPro": false,
            "fullname": "Intelligent Digital Creation",
            "user": "BoDai",
            "type": "user"
          },
          "name": "Bo Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:59.939Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff58b",
          "name": "Taku Komura",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff58c",
          "user": {
            "_id": "669cb638666901f41dae51bf",
            "avatarUrl": "/avatars/a7cc19e3db84bd86bee3eb6fd4897959.svg",
            "isPro": false,
            "fullname": "Jingbo Wang",
            "user": "jingbocuhk",
            "type": "user"
          },
          "name": "Jingbo Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:43.455Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6630cc7e9ee8861dd0b9bdbd/2bErZDgOyv5xeo1wDfMz3.mp4"
      ],
      "publishedAt": "2025-03-25T17:57:46.000Z",
      "submittedOnDailyAt": "2025-04-01T06:24:17.315Z",
      "title": "TokenHSI: 물리적인 인간-스케인 상호작용의 통합적인 합성을 통해\n  업무 토큰화",
      "submittedOnDailyBy": {
        "_id": "6630cc7e9ee8861dd0b9bdbd",
        "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
        "isPro": false,
        "fullname": "Liang Pan",
        "user": "lianganimation",
        "type": "user"
      },
      "summary": "다양한 물리적으로 가능한 Human-Scene Interactions (HSI)의 합성은 컴퓨터 애니메이션과 시각화 AI 모두에서 중요합니다. 발전이 보이는 반면, 현재의 방법들은 주로 특정 인터랙션 태스크에 전문화된 개별 제어러 개발에 집중되어 있으며, 이는 다양한 어려운 HSI 태스크를 대처하는 능력을 크게 제한하고 있습니다. 이러한 문제를 해결하기 위해, 우리는 TokenHSI를 소개합니다. TokenHSI는 단일 통합된 Transformer 기반 정책으로, 다양한 기술을 통합하고 유연한 적응성을 가능하게 하는 것입니다. 핵심의 관점은, 인간형의 친족 인식을 개별적으로 공유 토큰으로 모델화하고, 이를 특정 태스크 토큰과 조합하는 마스크 구조를 통해 구현하는 것입니다. 이러한 통합된 정책은 기술 간의 지식 공유를 가능하게 하고, 이러한 방식으로 다 태스크의 훈련을 촉진합니다. 또한, 우리의 정책 아키텍처는 변길 입력을 지원하고, 학습된 기술의 유연한 적응성을 가능하게 합니다. 또한, 추가적인 태스크 토큰너퍼를 훈련하여, 인터랙션 목표의 기하학을 변경하거나 복잡한 태스크를 해결하기 위해 여러 기술을 협업하는 데 사용될 수 있습니다. 실험은 우리의 접근 방식이 다양성, 적응성, 확장성을 크게 향상시키는 것을 보여주고 있습니다. 웹사이트: https://liangpan99.github.io/TokenHSI/",
      "upvotes": 5,
      "discussionId": "67eac6443755a17e3cbff5cf",
      "projectPage": "https://liangpan99.github.io/TokenHSI/",
      "githubRepo": "https://github.com/liangpan99/TokenHSI",
      "ai_keywords": [
        "transformer-based policy",
        "proprioception",
        "shared token",
        "task tokens",
        "masking mechanism",
        "multi-task training",
        "variable length inputs",
        "task tokenizers"
      ]
    },
    "publishedAt": "2025-03-25T13:57:46.000Z",
    "title": "TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through\n  Task Tokenization",
    "summary": "Synthesizing diverse and physically plausible Human-Scene Interactions (HSI)\nis pivotal for both computer animation and embodied AI. Despite encouraging\nprogress, current methods mainly focus on developing separate controllers, each\nspecialized for a specific interaction task. This significantly hinders the\nability to tackle a wide variety of challenging HSI tasks that require the\nintegration of multiple skills, e.g., sitting down while carrying an object. To\naddress this issue, we present TokenHSI, a single, unified transformer-based\npolicy capable of multi-skill unification and flexible adaptation. The key\ninsight is to model the humanoid proprioception as a separate shared token and\ncombine it with distinct task tokens via a masking mechanism. Such a unified\npolicy enables effective knowledge sharing across skills, thereby facilitating\nthe multi-task training. Moreover, our policy architecture supports variable\nlength inputs, enabling flexible adaptation of learned skills to new scenarios.\nBy training additional task tokenizers, we can not only modify the geometries\nof interaction targets but also coordinate multiple skills to address complex\ntasks. The experiments demonstrate that our approach can significantly improve\nversatility, adaptability, and extensibility in various HSI tasks. Website:\nhttps://liangpan99.github.io/TokenHSI/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6630cc7e9ee8861dd0b9bdbd/2bErZDgOyv5xeo1wDfMz3.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19901.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6630cc7e9ee8861dd0b9bdbd",
      "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
      "fullname": "Liang Pan",
      "name": "lianganimation",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14941",
      "authors": [
        {
          "_id": "67eb932522a341478ae86cb6",
          "user": {
            "_id": "67a99d1fef1439e285c4cbec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
            "isPro": false,
            "fullname": "Qihui Zhang",
            "user": "77Hui",
            "type": "user"
          },
          "name": "Qihui Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:00.373Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb7",
          "user": {
            "_id": "65e14c28b1a6de8a71e70172",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e14c28b1a6de8a71e70172/D097SILGsqoufpp3sG8tV.jpeg",
            "isPro": false,
            "fullname": "Munan Ning",
            "user": "MunanNing",
            "type": "user"
          },
          "name": "Munan Ning",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:18:39.336Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb8",
          "name": "Zheyuan Liu",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb9",
          "name": "Yanbo Wang",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cba",
          "name": "Jiayi Ye",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbb",
          "user": {
            "_id": "6637443ecd9097ac3c996d3c",
            "avatarUrl": "/avatars/d1c38bf03c2517ba0a7004b2f9f9bc96.svg",
            "isPro": false,
            "fullname": "yue",
            "user": "yuehuang",
            "type": "user"
          },
          "name": "Yue Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:19:26.627Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbc",
          "name": "Shuo Yang",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbd",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbe",
          "user": {
            "_id": "62c51800cb7033fd49b8efb7",
            "avatarUrl": "/avatars/06c2be0015f8022f9912f2279f2b3597.svg",
            "isPro": false,
            "fullname": "Song",
            "user": "Yibing",
            "type": "user"
          },
          "name": "Yibing Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:19:07.616Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbf",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T07:15:41.000Z",
      "submittedOnDailyAt": "2025-04-01T05:48:16.581Z",
      "title": "UPME: 다모둠 대언어 모델 평가의 무체크 페어 레비유어 프레임워크",
      "submittedOnDailyBy": {
        "_id": "67a99d1fef1439e285c4cbec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
        "isPro": false,
        "fullname": "Qihui Zhang",
        "user": "77Hui",
        "type": "user"
      },
      "summary": "다형 대 언어 모델(MLLMs)은 시각 문제 대답(VQA) 문제를 해결하기 위해 등장하며, 이러한 모델의 주관적 평가에 대한 새로운 연구 초점을 이끌어 냈습니다. 현재의 평가 방법은 이미지에 대한 Q&A 쌍의 설계가 필요한 큰人力 부담으로 제한되어, 평가의 규모와 범위가 좁아지고 있습니다. 자동화된 MLLM-as-judge 접근 방식은人力 부담을 줄이기 위해 자동 평가를 시도하지만, 그 정도의 편향을 불러일으키는 데 어려움을 겪고 있습니다. 이러한 문제를 해결하기 위해, 우리는 무감독된 동료 평가 MLLM 평가 프레임워크를 제안합니다. 이는 이미지 데이터만 사용하며, 모델이 자동으로 질문을 생성하고, 동료 모델로부터의 응답을 동료 평가합니다. 이로써,人力 부담 의존성을 제거합니다. 또한, 시각 이해와 논리론, 이미지와 문의 관련성, 응답의 정확성 3가지 측면에서 시각 언어 스코어 시스템(Vision Language Score System)을 소개하여 편향 문제를 완화합니다. 실험 결과를 통해, UPME는 MMstar 데이터 세트에서 Pearson 상관계수 0.944, ScienceQA 데이터 세트에서 0.814로, 인간 평가에 일치하며, 우리가 개발한 프레임워크는 인간이 설계한 벤치마크와 고유한 인간 취향에 매우 가까운 것을 보여주고 있습니다.",
      "upvotes": 3,
      "discussionId": "67eb932622a341478ae86d15",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Visual Question Answering (VQA)",
        "Q&A pairs",
        "MLLM-as-judge",
        "Unsupervised Peer review MLLM Evaluation (UPME)",
        "vision-language scoring system",
        "response correctness",
        "visual understanding and reasoning",
        "image-text correlation",
        "Pearson correlation",
        "MMstar dataset",
        "ScienceQA dataset"
      ]
    },
    "publishedAt": "2025-03-19T03:15:41.000Z",
    "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation",
    "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a99d1fef1439e285c4cbec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
      "fullname": "Qihui Zhang",
      "name": "77Hui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24391",
      "authors": [
        {
          "_id": "67eb72a2291b56e50b66a063",
          "user": {
            "_id": "66606a13fc6c0816442bd161",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
            "isPro": false,
            "fullname": "Xingyu Chen",
            "user": "rover-xingyu",
            "type": "user"
          },
          "name": "Xingyu Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:33.467Z",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a064",
          "user": {
            "_id": "66f80281d88dc2ad510663e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5HV3mTu-cxPCnWlCi_2wB.jpeg",
            "isPro": false,
            "fullname": "Yue Chen",
            "user": "faneggg",
            "type": "user"
          },
          "name": "Yue Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:31.158Z",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a065",
          "name": "Yuliang Xiu",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a066",
          "name": "Andreas Geiger",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a067",
          "name": "Anpei Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:59:58.000Z",
      "submittedOnDailyAt": "2025-04-01T07:01:19.970Z",
      "title": "Easi3R: 훈련 필요 없는 DUSt3R로부터 분리된 움직임의 추정\n\n(请注意，\"Easi3R\" 和 \"DUSt3R\" 是专有名词，通常保持原样，除非有特定的翻译需求。此处的翻译保持了原文中的专有名词形式。)",
      "submittedOnDailyBy": {
        "_id": "66606a13fc6c0816442bd161",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
        "isPro": false,
        "fullname": "Xingyu Chen",
        "user": "rover-xingyu",
        "type": "user"
      },
      "summary": "최근 DUSt3R의 발전으로, 고밀도의 정적 스케네의 점군과 카메라 파라미터의 강력한 추정이 가능해졌습니다. 이는 Transformer 네트워크 구조와 3D 데이터 세트를 직접 정규화하여 실현되었습니다. 반면, 4D 데이터 세트의 제한된 규모와 다양성은 4D 모델의 훈련에서 높은 일반화 성능을 달성하는 데 큰 한계로 작용하고 있습니다. 이 제약은 기존의 4D 방법(예, 3D 모델의 스케일러블한 동적 비디오 데이터에 대한 조정, 옵티컬 플로우, 깊이 등 추가적인 기하학적 정보)을 적용하기 때문에, 단순한 3D 모델의 스케일러블한 동적 비디오 데이터에 대한 조정과 옵티컬 플로우, 깊이 등 추가적인 기하학적 정보에 포함됩니다. 이 연구에서는, 반대의 길을 걸어, Easi3R를 도입했습니다. Easi3R는 간단하고 효율적인 4D 재구성의 훈련없이 방법입니다. 우리의 접근법은 추론 시에 어텐션 어드밴트 적용하고, 스크래치에서 재시작하거나 네트워크 조정이 필요하지 않습니다. DUSt3R의 어텐션 레이어가 카메라와 물체의 움직임에 대한 풍부한 정보를 내재적으로 포함하고 있음을 발견했습니다. 이러한 어텐션 맵을 신중히 분리함으로써, 동적인 영역 분할, 카메라의 자세 추정, 4D의 고밀도 점군 재구성을 실현했습니다. 실제적인 동적인 비디오에서 확장된 실험은, 우리의 가벼운 어텐션 어드밴트가 훈련된 전의 가장 선진한 방법보다 크게 초월함을 보여주었습니다. 우리의 코드는 https://easi3r.github.io/ 에서 연구를 위해 공개되어 있습니다.",
      "upvotes": 2,
      "discussionId": "67eb72a5291b56e50b66a152",
      "ai_keywords": [
        "Transformer network architectures",
        "dense point clouds",
        "camera parameters",
        "direct supervision",
        "3D datasets",
        "4D datasets",
        "4D model",
        "fine-tune",
        "dynamic video data",
        "geometric priors",
        "optical flow",
        "depths",
        "training-free method",
        "4D reconstruction",
        "attention adaptation",
        "inference",
        "attention layers",
        "dynamic region segmentation",
        "camera pose estimation",
        "4D dense point map reconstruction"
      ]
    },
    "publishedAt": "2025-03-31T13:59:58.000Z",
    "title": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training",
    "summary": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66606a13fc6c0816442bd161",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
      "fullname": "Xingyu Chen",
      "name": "rover-xingyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23730",
      "authors": [
        {
          "_id": "67eb567141abf40cd86e0e15",
          "user": {
            "_id": "67038a66eb760972bcb62c70",
            "avatarUrl": "/avatars/8cc82af8f11cae994bc83f4bd99b51bc.svg",
            "isPro": false,
            "fullname": "김윤식",
            "user": "yoonshik1205",
            "type": "user"
          },
          "name": "Yoonshik Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:53.729Z",
          "hidden": false
        },
        {
          "_id": "67eb567141abf40cd86e0e16",
          "user": {
            "_id": "646484cfb90150b2706df03b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
            "isPro": true,
            "fullname": "Jaeyoon Jung",
            "user": "lastdefiance20",
            "type": "user"
          },
          "name": "Jaeyoon Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:56.151Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T05:04:25.000Z",
      "submittedOnDailyAt": "2025-04-01T01:43:40.701Z",
      "title": "KOFFVQA: 한국어에서 대규모 시각 언어 모델의 주관적으로 평가된 자유 형식의 VQA 벤치마크",
      "submittedOnDailyBy": {
        "_id": "646484cfb90150b2706df03b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
        "isPro": true,
        "fullname": "Jaeyoon Jung",
        "user": "lastdefiance20",
        "type": "user"
      },
      "summary": "최근에 등장한 Large Vision-Language Models (VLMs)는 이러한 모델의 평가에 대한 다양한 벤치마크가 등장했습니다. 그러나 이러한 평가 방법들은 모델이 미리 정해진 답변을 선택해야 하는 것 또는 판단 모델을 사용하여 평가하는 데主观적이고 불신赖한 평가가 발생할 수 있습니다. 또한 한국어의 VLMs의 벤치마크는 영어 벤치마크와 다른 독립적인 평가 기준이 필요하지만 이러한 것이 존재하지 않습니다. 이러한 이유로 생성 모델의 성능은 언어에 따라 크게 차이날 수 있습니다. 이에 따라 KOFFVQA라는 일반적인 한국어의 자유 형식의 시각 질문 대답 벤치마크를 제안합니다. 이 벤치마크는 275개의 세부적으로 설계된 문제를 각각 이미지로 연결하고, VLM의 성능에 대한 10가지 다른 측면의 평가 기준을 포함합니다. 평가 기준은 판단 모델이 미리 정해진 규칙에 따라 각 답변을 평가하는 방식으로 불신赖성 문제를 해결합니다. 평가 기준을 객관적으로 정의함으로써 작은 오픈 소스 모델도 이 벤치마크에 대해 신뢰적으로 평가할 수 있게 됩니다. 또한 현재 존재하는 대부분의 VLMs를 벤치마크에 평가하고, 이 방법을 사용하여 평가하는 것이 현재의 방법보다 훨씬 신뢰할 수 있는 것을 실험적으로 확인했습니다. 평가 코드는 https://github.com/maum-ai/KOFFVQA에서 사용할 수 있습니다.",
      "upvotes": 2,
      "discussionId": "67eb567341abf40cd86e0e63",
      "githubRepo": "https://github.com/maum-ai/KOFFVQA",
      "ai_keywords": [
        "Large Vision-Language Models (VLMs)",
        "visual question answering benchmark",
        "grading criteria"
      ]
    },
    "publishedAt": "2025-03-31T01:04:25.000Z",
    "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language",
    "summary": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23730.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646484cfb90150b2706df03b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
      "fullname": "Jaeyoon Jung",
      "name": "lastdefiance20",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23022",
      "authors": [
        {
          "_id": "67ebadecf9f9390b4cd1c6d9",
          "name": "Xianglong He",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6da",
          "name": "Junyi Chen",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6db",
          "name": "Di Huang",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6dc",
          "name": "Zexiang Liu",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6dd",
          "name": "Xiaoshui Huang",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6de",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6df",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6e0",
          "name": "Yangguang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-29T09:21:50.000Z",
      "submittedOnDailyAt": "2025-04-01T07:42:31.137Z",
      "title": "MeshCraft: 유연한 DiTs를 활용한 효율적이고 제어 가능한 메쉬 생성에 대한 연구\n\n(注意：虽然要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，我在翻译时尽量保持了原文的专业术语和结构。)",
      "submittedOnDailyBy": {
        "_id": "64d71083a787c9bc7b9f1238",
        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
        "isPro": false,
        "fullname": "Yangguang Li",
        "user": "Lp256",
        "type": "user"
      },
      "summary": "3D 콘텐츠 제작 분야에서, AI 모델을 사용하여 최적의 메쉬 토폴로지를 달성하는 것은 오랜 세월 3D 아트가들의 추구였습니다. 과거의 방법들은 메쉬 GPT 등 기계 학습을 사용하여 준비된 3D 오브젝트의 생성을 시도했습니다. 이러한 방법들은 시각적으로 인상적인 결과를 내지만, 자동 복원 과정에서 토큰별로 예측을 의존하여 매우 느린 생성 속도와 메쉬 면의 수가 제한되어 많은 제약이 있었습니다. 본 논문에서는, 연속적인 공간 디퓨전을 사용하여 효율적이고 제어 가능한 메쉬 생성을 위해 연속적인 삼각형 면을 생성하는 새로운 프레임워크를 소개합니다. 특히, 메쉬 크래프트는 두 가지 핵심적인 구성 요소로 구성됩니다: 1) 변환기 기반의 VAE는 하이프로노스 구조를 연속적인 면 레벨 토큰으로 변환하고 원래 메쉬로 되돌립니다. 2) 면의 수에 기초한 플로워기 디퓨전 변환기는 특정 면의 수로 고품질의 3D 메쉬를 생성할 수 있습니다. 메쉬 크래프트는, 전체 메쉬의 토폴로지를 동시에 생성하는 데 디퓨전 모델을 사용하여, 자동 복원 모델보다 매우 빠르게 고품질의 메쉬 생성을 달성합니다. 특히, 메쉬 크래프트는 800면의 메쉬를 3.2초内生성할 수 있습니다(현재 기준과 비교하여 35배 빠르며). 분산 실험은 ShapeNet 데이터 세트에서 질의적 및 양적인 평가로 가장 선진한 기술을 초월하고, Objaverse 데이터 세트에서도 우수한 성능을 나타냅니다. 또한, 현재 조건付き 가이드 전략과 무간적으로 통합할 수 있으며, 메쉬 생성에 소요되는 시간이 많은 수동 작업으로부터 아티스트를 해방하는 것을 보여줍니다.",
      "upvotes": 1,
      "discussionId": "67ebadeef9f9390b4cd1c7b3",
      "ai_keywords": [
        "mesh auto-regressive techniques",
        "continuous spatial diffusion",
        "transformer-based VAE",
        "flow-based diffusion transformer",
        "high-fidelity mesh generation",
        "ShapeNet dataset",
        "Objaverse dataset",
        "conditional guidance"
      ]
    },
    "publishedAt": "2025-03-29T05:21:50.000Z",
    "title": "MeshCraft: Exploring Efficient and Controllable Mesh Generation with\n  Flow-based DiTs",
    "summary": "In the domain of 3D content creation, achieving optimal mesh topology through\nAI models has long been a pursuit for 3D artists. Previous methods, such as\nMeshGPT, have explored the generation of ready-to-use 3D objects via mesh\nauto-regressive techniques. While these methods produce visually impressive\nresults, their reliance on token-by-token predictions in the auto-regressive\nprocess leads to several significant limitations. These include extremely slow\ngeneration speeds and an uncontrollable number of mesh faces. In this paper, we\nintroduce MeshCraft, a novel framework for efficient and controllable mesh\ngeneration, which leverages continuous spatial diffusion to generate discrete\ntriangle faces. Specifically, MeshCraft consists of two core components: 1) a\ntransformer-based VAE that encodes raw meshes into continuous face-level tokens\nand decodes them back to the original meshes, and 2) a flow-based diffusion\ntransformer conditioned on the number of faces, enabling the generation of\nhigh-quality 3D meshes with a predefined number of faces. By utilizing the\ndiffusion model for the simultaneous generation of the entire mesh topology,\nMeshCraft achieves high-fidelity mesh generation at significantly faster speeds\ncompared to auto-regressive methods. Specifically, MeshCraft can generate an\n800-face mesh in just 3.2 seconds (35times faster than existing baselines).\nExtensive experiments demonstrate that MeshCraft outperforms state-of-the-art\ntechniques in both qualitative and quantitative evaluations on ShapeNet dataset\nand demonstrates superior performance on Objaverse dataset. Moreover, it\nintegrates seamlessly with existing conditional guidance strategies, showcasing\nits potential to relieve artists from the time-consuming manual work involved\nin mesh creation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d71083a787c9bc7b9f1238",
      "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
      "fullname": "Yangguang Li",
      "name": "Lp256",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20286",
      "authors": [
        {
          "_id": "67eaa88c40bebc3127ade04c",
          "user": {
            "_id": "67e77099284080c98d8c9bfc",
            "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
            "isPro": false,
            "fullname": "Zhenyu Liang",
            "user": "ZhenyuLiang",
            "type": "user"
          },
          "name": "Zhenyu Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:16.646Z",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04d",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04e",
          "name": "Naiwei Yu",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04f",
          "name": "Kebin Sun",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade050",
          "name": "Ran Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T07:30:23.000Z",
      "submittedOnDailyAt": "2025-04-01T00:34:02.763Z",
      "title": "진화 다목적 최적화와 GPU 가속을 결합하기 위한 텐서리제이션 방법",
      "submittedOnDailyBy": {
        "_id": "67e77099284080c98d8c9bfc",
        "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
        "isPro": false,
        "fullname": "Zhenyu Liang",
        "user": "ZhenyuLiang",
        "type": "user"
      },
      "summary": "진화 다목적 최적화(EMO)는 지난 20년 동안 뚜렷한 발전을 거쳤습니다. 그러나 문제의 크기와 복잡도가 증가함에 따라, 전통적인 EMO 알고리즘은 병렬성 및 scalability의 부족으로 성능의 한계가 드러났습니다. 많은 연구는 이러한 문제를 해결하기 위한 알고리즘 설계에 초점을 맞추고 있지만, 하드웨어 가속에 대한 주의가 부족하고, EMO 알고리즘과 GPU 등 첨단 계산 장치 사이에 명확한 간극이 남아 있습니다. 이 간극을 메우기 위해, GPU에서 EMO 알고리즘의 병렬화를 제안합니다. 텐서 제곱법을 사용하여, EMO 알고리즘의 데이터 구조와 연산을 간결한 텐서 표현으로 변환하여, GPU 계산의 자동 사용이 가능합니다. 우리 방법의 효과를 보여주기 위해, NSGA-III, MOEA/D, HypE의 3가지 대표적인 EMO 알고리즘에 적용합니다. 우리 방법을 전체적으로 평가하기 위해, GPU 가속된 물리 엔진을 사용한 다목적 로봇 제어 벤치마크를 도입합니다. 실험 결과를 통해, 텐서 제곱된 EMO 알고리즘은 CPU 기반 컴퓨터와 비교하여 1113배의 속도 향상을 달성하며, 해의 품질을 유지하면서, 인구 크기를 수만으로 확대할 수 있습니다. 또한, 텐서 제곱된 EMO 알고리즘은 복잡한 다목적 로봇 제어 태스크를 효율적으로 해결하며, 다양한 행동을 가진 고품질의 해를 생성합니다. 소스 코드는 https://github.com/EMI-Group/evomo에 접근 가능합니다.",
      "upvotes": 1,
      "discussionId": "67eaa88e40bebc3127ade0eb",
      "githubRepo": "https://github.com/EMI-Group/evomo",
      "ai_keywords": [
        "evolutionary multiobjective optimization (EMO)",
        "parallelism",
        "scalability",
        "GPU",
        "tensorization",
        "tensor representations",
        "NSGA-III",
        "MOEA/D",
        "HypE",
        "GPU-accelerated physics engine",
        "multiobjective robot control benchmark",
        "population sizes",
        "high-quality solutions",
        "diverse behaviors"
      ]
    },
    "publishedAt": "2025-03-26T03:30:23.000Z",
    "title": "Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization",
    "summary": "Evolutionary multiobjective optimization (EMO) has made significant strides\nover the past two decades. However, as problem scales and complexities\nincrease, traditional EMO algorithms face substantial performance limitations\ndue to insufficient parallelism and scalability. While most work has focused on\nalgorithm design to address these challenges, little attention has been given\nto hardware acceleration, thereby leaving a clear gap between EMO algorithms\nand advanced computing devices, such as GPUs. To bridge the gap, we propose to\nparallelize EMO algorithms on GPUs via the tensorization methodology. By\nemploying tensorization, the data structures and operations of EMO algorithms\nare transformed into concise tensor representations, which seamlessly enables\nautomatic utilization of GPU computing. We demonstrate the effectiveness of our\napproach by applying it to three representative EMO algorithms: NSGA-III,\nMOEA/D, and HypE. To comprehensively assess our methodology, we introduce a\nmultiobjective robot control benchmark using a GPU-accelerated physics engine.\nOur experiments show that the tensorized EMO algorithms achieve speedups of up\nto 1113x compared to their CPU-based counterparts, while maintaining solution\nquality and effectively scaling population sizes to hundreds of thousands.\nFurthermore, the tensorized EMO algorithms efficiently tackle complex\nmultiobjective robot control tasks, producing high-quality solutions with\ndiverse behaviors. Source codes are available at\nhttps://github.com/EMI-Group/evomo.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20286.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67e77099284080c98d8c9bfc",
      "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
      "fullname": "Zhenyu Liang",
      "name": "ZhenyuLiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18225",
      "authors": [
        {
          "_id": "67ebabe3c545cab686735182",
          "name": "Massimo Bini",
          "hidden": false
        },
        {
          "_id": "67ebabe3c545cab686735183",
          "name": "Leander Girrbach",
          "hidden": false
        },
        {
          "_id": "67ebabe3c545cab686735184",
          "name": "Zeynep Akata",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T22:00:56.000Z",
      "submittedOnDailyAt": "2025-04-01T07:38:06.739Z",
      "title": "저순위 조정에서 각도와 강도의 독립화",
      "submittedOnDailyBy": {
        "_id": "63f62ee3b29015adc33aafa0",
        "avatarUrl": "/avatars/b7d7fe1b65333fb698402ed065fc5d13.svg",
        "isPro": false,
        "fullname": "Massimo Bini",
        "user": "mwbini",
        "type": "user"
      },
      "summary": "パラメータ効率的微調（Parameter-Efficient FineTuning, PEFT）의 방법은 최근により大規模な事前学習モデル의普及により広く気に入ってきました。これらの方法は、最小限の計算コストでダウンストリームタスクに迅速に適応できます。しかし、ローラ（LoRA）などの有名な微調方法は、超パラメータの選択または長期的な学習プランに対しては限定的な強固性を持ち、最適なプロダクト性能を示すことができません。対照的に、エター（ETHER）などの制限付きアプローチは、強固性を高めることができますが、非常に低レンジの適応および固定ストレングスの変換に限られ、適応表現力を低下させます。本稿では、学習可能な低レンジ行列の正規化とスケーリングを行う新しい微調方法である、DeLoRA（Decoupled Low-rank Adaptation）を提案します。DeLoRAは、変換の距離を制限することで、角の学習と適応の強度を離れ、強固性を高めるコストを負担せずに行います。主題駆動画像生成、自然言語理解、および指示チューニングの評価を通じて、DeLoRAは、対戦するPEFT方法の性能を満たしたり、それよりも良く示し、強固性が高いことを示します。コードは、https://github.com/ExplainableML/DeLoRA にあります。",
      "upvotes": 1,
      "discussionId": "67ebabe5c545cab68673521b",
      "githubRepo": "https://github.com/ExplainableML/DeLoRA",
      "ai_keywords": [
        "Parameter-Efficient FineTuning (PEFT)",
        "LoRA",
        "bounded approaches",
        "ETHER",
        "Decoupled Low-rank Adaptation (DeLoRA)",
        "learnable low-rank matrices",
        "angular learning",
        "adaptation strength",
        "subject-driven image generation",
        "natural language understanding",
        "instruction tuning"
      ]
    },
    "publishedAt": "2025-03-23T18:00:56.000Z",
    "title": "Decoupling Angles and Strength in Low-rank Adaptation",
    "summary": "Parameter-Efficient FineTuning (PEFT) methods have recently gained\nsignificant popularity thanks to the widespread availability of large-scale\npretrained models. These methods allow for quick adaptation to downstream tasks\nwith minimal computational cost. However, popular finetuning methods such as\nLoRA exhibit limited robustness when it comes to hyperparameter choices or\nextended training regimes, preventing optimal out-of-the-box performance. In\ncontrast, bounded approaches, such as ETHER, provide greater robustness but are\nlimited to extremely low-rank adaptations and fixed-strength transformations,\nreducing their adaptation expressive power. In this work, we propose Decoupled\nLow-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and\nscales learnable low-rank matrices. By bounding the distance of the\ntransformation, DeLoRA effectively decouples the angular learning from the\nadaptation strength, enhancing robustness without compromising performance.\nThrough evaluations on subject-driven image generation, natural language\nunderstanding, and instruction tuning, we show that DeLoRA matches or surpasses\nperformance of competing PEFT methods, while exhibiting stronger robustness.\nCode is available at https://github.com/ExplainableML/DeLoRA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f62ee3b29015adc33aafa0",
      "avatarUrl": "/avatars/b7d7fe1b65333fb698402ed065fc5d13.svg",
      "fullname": "Massimo Bini",
      "name": "mwbini",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23913",
      "authors": [
        {
          "_id": "67ebaea15baac6e5085afcb9",
          "name": "Xiaoxuan Wang",
          "hidden": false
        },
        {
          "_id": "67ebaea15baac6e5085afcba",
          "name": "Yihe Deng",
          "hidden": false
        },
        {
          "_id": "67ebaea15baac6e5085afcbb",
          "name": "Mingyu Derek Ma",
          "hidden": false
        },
        {
          "_id": "67ebaea15baac6e5085afcbc",
          "name": "Wei Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T10:04:35.000Z",
      "submittedOnDailyAt": "2025-04-01T07:47:10.184Z",
      "title": "히스토로피에 기반한 자동 가중치 조정에 의한 자기 학습",
      "submittedOnDailyBy": {
        "_id": "64ba5946c0f19c9025665a3c",
        "avatarUrl": "/avatars/bb148094ce52f1f385d30968dc22e0e6.svg",
        "isPro": false,
        "fullname": "Xiaoxuan Wang",
        "user": "xw27",
        "type": "user"
      },
      "summary": "대 언어 모델의 수학 문제 해결 능력은 연구의 핵심이 되고 있으며, 자동으로 생성된 이유의 경로를 사용하여 모델을 개선하는 가능성을 더욱 높일 수 있다는 관심이 증가하고 있습니다. 이러한 경로는 정답만을 필요로 하는 설명의 단계별 논리적인 과정을 파악합니다. 자동 학습 방법은 외부 모델과 손쉽의 필요성을 제거한 이유의 태스크에 효과적이며, 이를 실현합니다. 그러나 자동으로 생성된 데이터의 모델 훈련의 최적화는 개방된 문제입니다. 본 논문에서는 자동 학습의 적응 가중치를 구현하는 \"Entropy-Based Adaptive Weighting for Self-Training (EAST)\"를 제안합니다. EAST는 자동 학습 시 데이터의 불확실성을 우선시하여 적응 가중치를 적용하는 전략입니다. 특히, EAST는 가중치의 컷을 조절하는 조정 파라미터를 갖는 매핑 함수를 사용하며, 모델이 더 불확실한 데이터에 대해 더 높은 가중치를 할당합니다. 이 접근 방식은 모델이 정보량 많은 및 어려운 예를 더 많은 주목을 모으는 데 도움을 주며, 이유의 능력을 향상시킵니다. GSM8K와 MATH 벤치마크에서 실험 결과를 보면, 가상 방법은 MATH에서 약 0%의 개선이 보이지만, EAST는 기본 모델보다 약 1%의 개선을 보입니다. GSM8K에서는 가상 방법보다 1-2%의 성능 향상을 보입니다.",
      "upvotes": 0,
      "discussionId": "67ebaea25baac6e5085afcfe",
      "ai_keywords": [
        "Entropy-Based Adaptive Weighting for Self-Training (EAST)",
        "mapping function",
        "tunable parameter",
        "weighting strategy",
        "uncertainty",
        "informative examples",
        "challenging examples"
      ]
    },
    "publishedAt": "2025-03-31T06:04:35.000Z",
    "title": "Entropy-Based Adaptive Weighting for Self-Training",
    "summary": "The mathematical problem-solving capabilities of large language models have\nbecome a focal point of research, with growing interests in leveraging\nself-generated reasoning paths as a promising way to refine and enhance these\nmodels. These paths capture step-by-step logical processes while requiring only\nthe correct answer for supervision. The self-training method has been shown to\nbe effective in reasoning tasks while eliminating the need for external models\nand manual annotations. However, optimizing the use of self-generated data for\nmodel training remains an open challenge. In this work, we propose\nEntropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive\nweighting strategy designed to prioritize uncertain data during self-training.\nSpecifically, EAST employs a mapping function with a tunable parameter that\ncontrols the sharpness of the weighting, assigning higher weights to data where\nthe model exhibits greater uncertainty. This approach guides the model to focus\non more informative and challenging examples, thereby enhancing its reasoning\nability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical\nresults show that, while the vanilla method yields virtually no improvement\n(0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K,\nEAST attains a further 1-2% performance boost compared to the vanilla method.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23913.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ba5946c0f19c9025665a3c",
      "avatarUrl": "/avatars/bb148094ce52f1f385d30968dc22e0e6.svg",
      "fullname": "Xiaoxuan Wang",
      "name": "xw27",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]