[
  {
    "paper": {
      "id": "2505.22617",
      "authors": [
        {
          "_id": "6837cd8fc537d91527323667",
          "user": {
            "_id": "650eba9555dc1e841746f132",
            "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
            "isPro": false,
            "fullname": "Ganqu Cui",
            "user": "ganqu",
            "type": "user"
          },
          "name": "Ganqu Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:09.467Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323668",
          "user": {
            "_id": "66e3f8fb5d97b5bb46923444",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DW806I00-00oQAYvD4ocQ.png",
            "isPro": false,
            "fullname": "Yuchen Zhang",
            "user": "YucZhang2003",
            "type": "user"
          },
          "name": "Yuchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:14.207Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323669",
          "user": {
            "_id": "65352acb7139c5dd8d9a8590",
            "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
            "isPro": false,
            "fullname": "JiachengChen",
            "user": "JC-Chen",
            "type": "user"
          },
          "name": "Jiacheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:20.736Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366a",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366b",
          "name": "Zhi Wang",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366c",
          "user": {
            "_id": "622474f38dc6b0b64f5e903d",
            "avatarUrl": "/avatars/d6b60a014277a8ec7d564163c5f644aa.svg",
            "isPro": false,
            "fullname": "Yuxin Zuo",
            "user": "yuxinzuo",
            "type": "user"
          },
          "name": "Yuxin Zuo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:12.022Z",
          "hidden": true
        },
        {
          "_id": "6837cd8fc537d9152732366d",
          "user": {
            "_id": "662f638ba9891e43cc4c5125",
            "avatarUrl": "/avatars/77c22de5511f9b85d98ec75fb0b5e9be.svg",
            "isPro": false,
            "fullname": "Li Haozhan",
            "user": "Haozhan72",
            "type": "user"
          },
          "name": "Haozhan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:22.720Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366e",
          "name": "Yuchen Fan",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366f",
          "name": "Huayu Chen",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323670",
          "name": "Weize Chen",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323671",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323672",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323673",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323674",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323675",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323676",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323677",
          "user": {
            "_id": "60cf4bcb1ce3775ebb86e5d5",
            "avatarUrl": "/avatars/12bcd18d215abf91f297f93007733148.svg",
            "isPro": false,
            "fullname": "Ning Ding",
            "user": "stingning",
            "type": "user"
          },
          "name": "Ning Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:16.809Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:38:45.000Z",
      "submittedOnDailyAt": "2025-05-29T01:38:57.501Z",
      "title": "强化학습의 엔트로피 구조를 이용한 언어 모델의 추론",
      "submittedOnDailyBy": {
        "_id": "650eba9555dc1e841746f132",
        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
        "isPro": false,
        "fullname": "Ganqu Cui",
        "user": "ganqu",
        "type": "user"
      },
      "summary": "이 논문은 RL의 스케일링을 통해 LLMs와의 논리론을 수행하는 데 있어서 정책 엔트로피의 붕괴를 극복하는 것을 목표로 합니다. 이 현상은 큰 규모의 RL 실험에서 엔트로피 대책이 없는 경우 일관되게 관찰될 수 있으며, 정책 엔트로피는 초기 훈련 단계에서 급격히 감소하며, 감소된 탐색 능력은 정책 성능의 포화과 함께 관찰됩니다. 실제로는 엔트로피 H와 하류 성능 R 사이의 변환 방정식 R = -a * e^H + b를 구축했습니다. 이 실험적 법칙은 강력한 시사점을 제시하며, 정책 성능은 정책 엔트로피로 대체되어 그 부족으로 인해 한계가 완전히 예측 가능합니다. H=0, R=-a+b. 우리의 발견은 엔트로피 관리를 실현하기 위해 계산량의 스케일링을 위해 지속적인 탐색을 수행해야 하는 것을 보여줍니다. 이로 인해 이론적으로도 실험적으로 엔트로피의 동적을 조사했습니다. 우리의 계산은 행동 확률과 로지ッ트의 변화의 공분산은 Policy Gradient 같은 알고리즘을 사용했을 때 정책 엔트로피의 변화에 비례하여 이점을 주도하고 있음을 주장합니다. 실험적 연구는 공분산 항과 엔트로피의 차이가 완전히 일치하며 이론적인 결론을 지지하고 있습니다. 또한 공분산 항은 학습 중에 거의 모두 양으로 유지되며, 이는 정책 엔트로피가 단조적으로 감소하는 이유를 더 깊이 설명합니다. 엔트로피의 동적인 구조를 이해함으로써, 고 공분산 토큰의 업데이트를 제한하여 엔트로피를 제어하는 방법을 고려했습니다. 특히, Clip-Cov와 KL-Cov의 두 간단하고 효과적인 기술을 제안했습니다. Clip-Cov는 고 공분산 토큰에 복사를 적용하고 KL 페널티를 적용합니다. 실험은 이러한 방법을 통해 탐색을 촉진하고 정책이 엔트로피 붕괴에서 벗어나 더 좋은 하류 성능을 달성하는 것을 보여주었습니다.",
      "upvotes": 64,
      "discussionId": "6837cd90c537d9152732369d",
      "githubRepo": "https://github.com/PRIME-RL/Entropy-Mechanism-of-RL",
      "ai_summary": "Entropy dynamics in reinforcement learning with large language models are investigated to prevent policy entropy collapse and improve exploration.",
      "ai_keywords": [
        "policy entropy",
        "reinforcement learning",
        "LLMs",
        "entropy intervention",
        "transformation equation",
        "policy performance",
        "entropy dynamics",
        "covariance",
        "action probability",
        "logits",
        "advantage",
        "Policy Gradient",
        "Clip-Cov",
        "KL-Cov"
      ]
    },
    "publishedAt": "2025-05-28T13:38:45.000Z",
    "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models",
    "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22617.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650eba9555dc1e841746f132",
      "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
      "fullname": "Ganqu Cui",
      "name": "ganqu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21600",
      "authors": [
        {
          "_id": "6837bc9c9937bcb69885799c",
          "user": {
            "_id": "6445fd9ba56444c355dcbcba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
            "isPro": false,
            "fullname": "Tianyu Fu",
            "user": "fuvty",
            "type": "user"
          },
          "name": "Tianyu Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:43.123Z",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799d",
          "name": "Yi Ge",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799e",
          "user": {
            "_id": "66954ebfbcd81f395e9dca37",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66954ebfbcd81f395e9dca37/0C3m5YdxyXuK7dJBu4AdL.png",
            "isPro": false,
            "fullname": "Yichen You",
            "user": "youyc22",
            "type": "user"
          },
          "name": "Yichen You",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:40.790Z",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799f",
          "name": "Enshu Liu",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a0",
          "name": "Zhihang Yuan",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a1",
          "name": "Guohao Dai",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a2",
          "name": "Shengen Yan",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a3",
          "name": "Huazhong Yang",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a4",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6445fd9ba56444c355dcbcba/edz42EmKaJTdax2mDUmhh.mp4"
      ],
      "publishedAt": "2025-05-27T16:57:20.000Z",
      "submittedOnDailyAt": "2025-05-29T00:18:54.118Z",
      "title": "R2R: 효율적인 다른 이유의 경로 탐색을 위한 소대 모델 토큰 루팅에서",
      "submittedOnDailyBy": {
        "_id": "6445fd9ba56444c355dcbcba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
        "isPro": false,
        "fullname": "Tianyu Fu",
        "user": "fuvty",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 추론 오버헤드의 큰 비용 부담을 감수하면서 놀라운 인공 지능 능력을 달성하지만, 도입에 큰 문제점을 가지고 있습니다. 그러나 결정화된 작은 언어 모델(SLMs)은 LLMs의 인공 지능 경로를 따라갈 수 없기 때문에 성능이 떨어집니다. 다행히, LLMs와 SLMs의 인공 지능 경로의 차이는 그 중 작은 부분의 토큰들만 발생합니다. 많은 생성된 토큰들은 같은거나 약자나 표현의 미묘한 차이 등 중립적인 차이를 나타내는 경우가 많습니다. 이러한 관점을 활용하여 **Roads to Rome (R2R)**라는 새로운 신경 토큰 루팅 메소드를 도입했습니다. 이는 중요한, 경로의 차이를 나타내는 토큰을 제외한 나머지 토큰을 SLM에 맡기고, LLMs를 선택적으로 사용하도록 합니다. 또한 자동 데이터 생성 프로이플리핑은 차이를 나타내는 토큰을 인식하고 토큰 수준의 루팅 레이블을 생성하여 경량 라우터의 훈련을 위한 데이터를 생성합니다. R2R은 DeepSeek familiy의 R1-1.5B와 R1-32B 모델을 조합하여 어려운 수학, 코딩, QA 벤치마크를 평가했습니다. 평균 활성화 파라미터 크기가 5.6B이고, R2R은 R1-7B의 평균 정확도를 1.6배로 달성하고, R1-14B 모델을 초과했습니다. R1-32B와 비교하여, 성능이 비슷한 수준에서 2.8배의 워ルク 클로ック 속도 업그레이드 제공하며, 테스트 시간 스케일링 효과의 프로토 타입을 발전시켰습니다. 코드는 https://github.com/thu-nics/R2R에서 사용할 수 있습니다.",
      "upvotes": 47,
      "discussionId": "6837bc9d9937bcb6988579d1",
      "projectPage": "https://fuvty.github.io/R2R_Project_Page/",
      "githubRepo": "https://github.com/thu-nics/R2R",
      "ai_summary": "Roads to Rome (R2R) selectively utilizes large language models for critical reasoning tasks to enhance efficiency and performance in lightweight models.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Small Language Models (SLMs)",
        "token routing",
        "neural token routing",
        "token divergence",
        "token generation",
        "automatic data generation pipeline",
        "token-level routing labels",
        "R1-1.5B",
        "R1-32B",
        "math benchmarks",
        "coding benchmarks",
        "QA benchmarks",
        "parameter size",
        "activated parameters",
        "test-time scaling efficiency",
        "pareto frontier"
      ]
    },
    "publishedAt": "2025-05-27T12:57:20.000Z",
    "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing",
    "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6445fd9ba56444c355dcbcba/edz42EmKaJTdax2mDUmhh.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6445fd9ba56444c355dcbcba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
      "fullname": "Tianyu Fu",
      "name": "fuvty",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22312",
      "authors": [
        {
          "_id": "6837c342cd1601f5bd670255",
          "name": "Jujie He",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670256",
          "user": {
            "_id": "65643645b09c0b9ece1b8f0e",
            "avatarUrl": "/avatars/d5197103b6e92f765bfda7ed2cc8d53e.svg",
            "isPro": false,
            "fullname": "Jiacai Liu",
            "user": "skydownacai",
            "type": "user"
          },
          "name": "Jiacai Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:28.696Z",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670257",
          "user": {
            "_id": "658229ef5f6d83438257fce5",
            "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
            "isPro": false,
            "fullname": "Chris (Yuhao) Liu",
            "user": "chrisliu298",
            "type": "user"
          },
          "name": "Chris Yuhao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:32.117Z",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670258",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670259",
          "name": "Chaojie Wang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025a",
          "name": "Peng Cheng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025b",
          "name": "Xiaoyu Zhang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025c",
          "name": "Fuxiang Zhang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025d",
          "name": "Jiacheng Xu",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025e",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025f",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670260",
          "name": "Liang Zeng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670261",
          "name": "Tianwen Wei",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670262",
          "name": "Cheng Cheng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670263",
          "name": "Bo An",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670264",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670265",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T12:56:04.000Z",
      "submittedOnDailyAt": "2025-05-29T00:48:05.741Z",
      "title": "Skywork Open Reasoner 1 기술 보고서",
      "submittedOnDailyBy": {
        "_id": "658229ef5f6d83438257fce5",
        "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
        "isPro": false,
        "fullname": "Chris (Yuhao) Liu",
        "user": "chrisliu298",
        "type": "user"
      },
      "summary": "DeepSeek-R1의 성공은 强化学習（RL）이 大規模言語モデル（LLMs）의 理由能力を 上げるために 重要な役割를 果たしていることを 강조します。本論文では、长期Chain-of-Thought（CoT）モデルに 効果的かつスケーラブルなRL実装を提案します。DeepSeek-R1-Distillモデルシリーズに基づいて、我々のRLアプローチは 顕著な性能向上を 実現し、AIME24、AIME25、LiveCodeBenchの平均正確率を32Bモデルでは57.8%から72.8%（+15.0%）、7Bモデルでは43.6%から57.5%（+13.9%）に 上げました。Skywork-OR1-32BモデルはAIME24とAIME25ベンチマークでDeepSeek-R1とQwen3-32Bを超え、LiveCodeBenchで 比較的結果を 収めました。Skywork-OR1-7BとSkywork-OR1-Math-7Bモデルは 類似サイズのモデルの中で 競争的な理由能力を 示しました。我々は 訓練パイプラインの 核心成分について 詳細な消去試験を行い、その 効果性を 証明しました。また、熵崩壊現象を 詳細に 調査し、熵の動態に影響する要因を特定し、過早な熵崩壊を 抑制することが 検証性能向上に 重要であることを 示しました。コミュニティ研究のサポートを 提供するため、我々は モデル重み、訓練コード、訓練データセットを 完全に オープンソース化します。",
      "upvotes": 39,
      "discussionId": "6837c344cd1601f5bd6702dd",
      "githubRepo": "https://github.com/SkyworkAI/Skywork-OR1",
      "ai_summary": "Skywork-OR1 is a reinforcement learning approach for long Chain-of-Thought models that improves accuracy over DeepSeek-R1 across various benchmarks by addressing entropy collapse.",
      "ai_keywords": [
        "reinforcement learning",
        "LLMs",
        "Chain-of-Thought",
        "Skywork-OR1",
        "DeepSeek-R1-Distill",
        "AIME24",
        "AIME25",
        "LiveCodeBench",
        "entropy collapse",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-05-28T08:56:04.000Z",
    "title": "Skywork Open Reasoner 1 Technical Report",
    "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22312.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "658229ef5f6d83438257fce5",
      "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
      "fullname": "Chris (Yuhao) Liu",
      "name": "chrisliu298",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22651",
      "authors": [
        {
          "_id": "6837ffdd1bfb4a669ad6de09",
          "user": {
            "_id": "662678dfdd43e904ef1dcd03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
            "isPro": false,
            "fullname": "Yi Ding",
            "user": "Tuwhy",
            "type": "user"
          },
          "name": "Yi Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:19.811Z",
          "hidden": false
        },
        {
          "_id": "6837ffdd1bfb4a669ad6de0a",
          "name": "Ruqi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:58:03.000Z",
      "submittedOnDailyAt": "2025-05-29T06:18:44.515Z",
      "title": "シェルロック: 시각 언어 모델의 자기 보정 논리\n\n(注意：虽然要求不添加解释或额外文本，但为了确保翻译的准确性和专业性，我在这里提供了一个简短的翻译，没有添加额外的解释或文本。)",
      "submittedOnDailyBy": {
        "_id": "662678dfdd43e904ef1dcd03",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
        "isPro": false,
        "fullname": "Yi Ding",
        "user": "Tuwhy",
        "type": "user"
      },
      "summary": "推理 비전 라ングワード 모댈(VLMs)은 복잡한 다중모달 태스크에서 기대되는 성능을 보여주고 있습니다. 그러나 이들은 다음 주요 문제를 직면하고 있습니다: 논리 오류에 매우 민감하고, 많은 표준화된 데이터 또는 정확한 검증 데이터를 필요로하며, 특정 영역을 초과하는 일반화가 어려워집니다. 이러한 제한을 해결하기 위해, 우리는 논리 VLMs의 자동 조정을 전략으로 채택하고, 그 성능을 향상시키기 위한 시도를 하였습니다. 먼저, 논리 VLMs의 자동 조정 능력에 대한 상세한 분석을 수행하고, 주요 결함이 특정화되었습니다. 이 분석의 기초에, 우리는 Sherlock라는 자동 조정과 자동 개선의 훈련 프레임워크를 도입하였습니다. Sherlock은 트랙 레벨의 자동 조정 객체, 시각적인 파바란트에 기반한 데이터 구축 방법, 그리고 동적인 beta를 도입하였습니다. 모델은 20k개의 랜덤으로 샘플링된 표준화된 데이터를 사용하여 자동 조정 능력을 얻은 후, 외부의 슈퍼바이온을 제외한 자동 개선을 계속합니다. Llama3.2-Vision-11B 모델에 구축된 Sherlock은 8 바ン크 마크 표준에서 놀라운 결과를 얻으며, 직접 생성의 평균 정확도는 64.1에서 자동 조정 후 65.4로 증가하였습니다. LLaVA-CoT(63.2), Mulberry(63.9), LlamaV-o1(63.4)을 초과하며, 이들을 사용했던 데이터의 20%를 초과합니다.",
      "upvotes": 38,
      "discussionId": "6837ffdf1bfb4a669ad6de71",
      "projectPage": "https://dripnowhy.github.io/Sherlock/",
      "githubRepo": "https://github.com/DripNowhy/Sherlock",
      "ai_summary": "Sherlock, a self-correction and self-improvement framework for reasoning vision-language models, enhances accuracy across benchmarks using limited annotated data.",
      "ai_keywords": [
        "vision-language models",
        "self-correction",
        "trajectory-level self-correction",
        "preference data",
        "visual perturbation",
        "dynamic beta",
        "Llama3.2-Vision-11B",
        "LLaVA-CoT",
        "Mulberry",
        "LlamaV-o1"
      ]
    },
    "publishedAt": "2025-05-28T13:58:03.000Z",
    "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models",
    "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\nbeta for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22651.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662678dfdd43e904ef1dcd03",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
      "fullname": "Yi Ding",
      "name": "Tuwhy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22453",
      "authors": [
        {
          "_id": "6837c318a4e378954486e45d",
          "user": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "WaltonFuture",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:35.145Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e45e",
          "name": "Yuting Li",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e45f",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e460",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Yue Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:14:48.838Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e461",
          "name": "Linghe Kong",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e462",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Weiran Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:14:48.838Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e463",
          "name": "Lichao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T15:11:16.000Z",
      "submittedOnDailyAt": "2025-05-29T00:45:28.570Z",
      "title": "GRPO를 사용한 무체크포인트 학습을 적용한 다모달 LLM의 추론\n\n(注意：原文中的\"GRPO\"和\"LLM\"在韩语中直接使用英文字母，以保持专业术语的准确性。)",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "후디레인 스테이지에서 수행되는 다모달 대언어 모델(MLLMs)의 개선은 일반적으로 정규적 미세 조정(SFT)이나 강화학습(RL)을 통해 이루어진다. 그러나 이러한 정규적인 방법들은 고가로 직접手工으로 라벨링된 다모달 데이터가 필요하여, 결국 지속 가능한 자원이 아니다. 반면, 최근의 노력은 무규칙적인 후디레인을 검토하고 있으며, 그 방법론은 복잡하고 반복이 어려워진다. 본 논문에서는, GRPO(안정적이고 scalable한 온라인 RL 알고리즘)을 사용하여 외부의 정규화를 필요로 하지 않는 지속적인 자기 개선을 가능하게 하는 방법을 처음으로 조사하고 있다. MM-UPT(간단하고 효과적인 무규칙적인 후디레인 프레임워크)를 제안하고, GRPO를 기반으로 기존의 보상 신호를 다수결 기반의 자기 보상 기구로 대체하고 있다. 실험 결과를 통해, MM-UPT는 Qwen2.5-VL-7B의 이해 능력이 크게 향상된다(예: MathVista에서 66.3%→72.9%, We-Math에서 62.9%→68.7%)고, 표준 데이터셋을 사용하지만 실제 라벨이 없는 상황에도 적용할 수 있다는 것을 보여주고 있다. MM-UPT는 지난주의 무규칙적인 기준을 초과하고, 정규적인 GRPO의 결과를 근접시키는 것을 보여준다. 또한, MLLM 자체에서 생성된 합성 질문을 사용함으로써 성능을 향상시킬 수 있다는 것을 보여주고, scalable한 자기 개선의 유망한 접근을 제시하고 있다. 전체적으로, MM-UPT는 외부의 정규화를 필요로 하지 않는 MLLM의 지속적인, 자율적인 개선의 새로운 패러다임을 제공하고 있다. 코드는 https://github.com/waltonfuture/MM-UPT에 액세스할 수 있다.",
      "upvotes": 29,
      "discussionId": "6837c318a4e378954486e48a",
      "projectPage": "https://github.com/waltonfuture/MM-UPT",
      "githubRepo": "https://github.com/waltonfuture/MM-UPT",
      "ai_summary": "MM-UPT, a framework employing GRPO and self-rewarding, enhances multi-modal LLMs through unsupervised continual learning, showing performance improvements without manual annotations.",
      "ai_keywords": [
        "GRPO",
        "MM-UPT",
        "reinforcement learning",
        "unsupervised post-training",
        "multi-modal large language models",
        "self-rewarding mechanism",
        "majority voting",
        "synthetic questions",
        "MathVista",
        "We-Math"
      ]
    },
    "publishedAt": "2025-05-28T11:11:16.000Z",
    "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO",
    "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %rightarrow72.9 % on MathVista, 62.9\n%rightarrow68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22453.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21136",
      "authors": [
        {
          "_id": "6837c91ec790885f338b8f27",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:22.525Z",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f28",
          "name": "Xiaoming Xu",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f29",
          "name": "Jia Wei",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2a",
          "name": "Haofeng Huang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2b",
          "name": "Pengle Zhang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2c",
          "name": "Chendong Xiang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2d",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2e",
          "name": "Jianfei Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/e7t1XOditivMjvbyyouoc.png"
      ],
      "publishedAt": "2025-05-27T12:50:36.000Z",
      "submittedOnDailyAt": "2025-05-29T01:12:49.349Z",
      "title": "SageAttention2++: 더 효율적인 SageAttention2 구현\n\n(请注意，虽然原文中的“更エフケイシブル”被翻译为“더 효율적인”，但根据上下文，如果“エフケイシブル”是指“effective”，那么更准确的翻译可能是“더 효과적인”。不过，根据任务要求，这里直接采用了“더 효율적인”作为翻译。)",
      "submittedOnDailyBy": {
        "_id": "66c0a08bac74db25de8427ec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
        "isPro": false,
        "fullname": "Jintao Zhang",
        "user": "jt-zhang",
        "type": "user"
      },
      "summary": "アツション의 효율성은 중요하며, 이는 순서 길이에 따라 시간 계산량을 제곱으로 증가시키는 데 기인합니다. SageAttention2는 이 문제를 해결하기 위해, マトミル(Matmul)의 속도를 향상시키기 위해 카운팅을 활용하고 있습니다. 또한, SageAttention2를 더욱 빠르게 수행하기 위해, FP16으로 누적된 FP8 Matmul의 빠른 명령어를 사용하도록 제안하고 있습니다. 이 명령어는 SageAttention2에서 사용하는 FP8 Matmul보다 2배 빠르고, 실험 결과를 통해 SageAttention2++은 FlashAttention보다 3.9배 빠르고, 동일한 어텐션 정확도를 유지합니다. 이는 언어, 이미지, 영상 생성 모델 등 다양한 모델을 효과적으로 고속화하는 것을 의미하며,终端에서 측정되는 성능 손실은 보이지 않습니다. 코드는 https://github.com/thu-ml/SageAttention에 공개되어 있습니다.",
      "upvotes": 28,
      "discussionId": "6837c923c790885f338b90e5",
      "projectPage": "https://github.com/thu-ml/SageAttention",
      "githubRepo": "https://github.com/thu-ml/SageAttention",
      "ai_summary": "SageAttention2++ improves attention efficiency by using FP8 Matmul in FP16, achieving a 3.9x speedup over FlashAttention without losing accuracy.",
      "ai_keywords": [
        "attention",
        "time complexity",
        "sequence length",
        "quantization",
        "matrix multiplications",
        "Matmul",
        "FP8",
        "FP16",
        "SageAttention2",
        "SageAttention2++",
        "FlashAttention",
        "image generation",
        "video generation"
      ]
    },
    "publishedAt": "2025-05-27T08:50:36.000Z",
    "title": "SageAttention2++: A More Efficient Implementation of SageAttention2",
    "summary": "The efficiency of attention is critical because its time complexity grows\nquadratically with sequence length. SageAttention2 addresses this by utilizing\nquantization to accelerate matrix multiplications (Matmul) in attention. To\nfurther accelerate SageAttention2, we propose to utilize the faster instruction\nof FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8\nMatmul used in SageAttention2. Our experiments show that SageAttention2++\nachieves a 3.9x speedup over FlashAttention while maintaining the same\nattention accuracy as SageAttention2. This means SageAttention2++ effectively\naccelerates various models, including those for language, image, and video\ngeneration, with negligible end-to-end metrics loss. The code will be available\nat https://github.com/thu-ml/SageAttention.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/e7t1XOditivMjvbyyouoc.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21136.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22334",
      "authors": [
        {
          "_id": "6837c360b127cae8a0b36e85",
          "user": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "WaltonFuture",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:26.705Z",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e86",
          "name": "Yuting Li",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e87",
          "name": "Kaipeng Zheng",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e88",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e89",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Yue Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:16:03.908Z",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8a",
          "name": "Linghe Kong",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8b",
          "name": "Lichao Sun",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8c",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Weiran Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:16:03.908Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T13:21:38.000Z",
      "submittedOnDailyAt": "2025-05-29T00:46:20.111Z",
      "title": "냉스태트에서의 강화학습을 활용한 다모달 논리의 발전",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "최근의 대규모 언어 모델（LLMs）의 발전은 인상적인 연속적인 사고 능력을 보여주고 있으며, 강화학습（RL）이 이러한 발전에 중요한 역할을 수행하는 것을 명확히 알 수 있습니다. \"기억한 순간\" 패턴을 나타내는 모델이 스스로 조정을 수행하는 것은 일반적으로 RL의 에피오프릭 특성으로 설명되지만, 이러한 패턴은 RL 훈련 전의 많은 모델（MLLMs）에서도 존재하며, 이완 논리 개선과는 반드시 관련되어 있지는 않습니다. 이러한 통찰을 기반으로, MLLMs의 논리력을 향상시키기 위한 일련의 세부적인 연구를 제안합니다: 1. 구조화된 연속적인 사고 패턴을 생성하기 위해 냉정한 시작으로의 지도 학습（SFT）을 사용합니다. 2. GRPO를 사용한 강화학습으로 더 높은 능력을 향상시킵니다. 다양한 어려운 MLLMs의 논리성 벤치마크에서의 상세한 실험은 SFT만 또는 RL만 사용하는 방법들을 초월하여 확인되었습니다. 결과적으로의 모델은 오픈 소스의 MLLMs에서 가장 先端의 성능을 달성하고, 3B와 7B 규모 각각에서 가장 先端를 이루고 있습니다. 특히, 7B 모델은 기본 모델보다 크게 향상되어 있습니다（예를 들어, MathVista에서 66.3%→73.4%, We-Math에서 62.9%→70.4%）, 3B 모델은 여러 7B 모델의 성능과 경쟁적입니다. 이 연구는 발전된 MLLMs의 구축에 실용적인 가이드라인을 제공합니다. 코드는 https://github.com/waltonfuture/RL-with-Cold-Start에 공개되어 있습니다.",
      "upvotes": 25,
      "discussionId": "6837c363b127cae8a0b36f6f",
      "projectPage": "https://github.com/waltonfuture/RL-with-Cold-Start",
      "githubRepo": "https://github.com/waltonfuture/RL-with-Cold-Start",
      "ai_summary": "A two-stage approach combining supervised fine-tuning and reinforcement learning enhances multimodal reasoning in large language models, achieving state-of-the-art performance on benchmarks.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "multimodal LLMs",
        "MLLMs",
        "reinforcement learning",
        "RL",
        "chain-of-thought reasoning",
        "GRPO",
        "supervised fine-tuning",
        "SFT",
        "multimodal reasoning",
        "MathVista",
        "We-Math"
      ]
    },
    "publishedAt": "2025-05-28T09:21:38.000Z",
    "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start",
    "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %rightarrow73.4 % on\nMathVista, 62.9 %rightarrow70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22334.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22457",
      "authors": [
        {
          "_id": "68380912e9c1608de91e23f3",
          "name": "Haonan Wang",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f4",
          "name": "Hongfu Liu",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f5",
          "name": "Xiangyan Liu",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f6",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f7",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f8",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f9",
          "name": "Tianyu Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T15:13:34.000Z",
      "submittedOnDailyAt": "2025-05-29T05:44:17.072Z",
      "title": "다음은 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오 리유론을 육성하기 위한 다음 이벤트의 예측입니다.\n- 비디오",
      "submittedOnDailyBy": {
        "_id": "63d91b6d255ef6add20e1b38",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
        "isPro": false,
        "fullname": "Tianyu Pang",
        "user": "P2333",
        "type": "user"
      },
      "summary": "다음의 토크 예측은 LLM에서 추론을 가능하게 하는 기초적인 학습 임무입니다. 그러나, 템포럴 추론 능력을 가지는 MLLM이 비디오 입력에 대해 어떻게 학습해야 할까요? 현재의 임무에서, 비디오 클라이언트 답변 등은 주로 인간으로부터의 注釈 또는 강력한 MLLM으로부터의 注釈에 의존하는 경우가 많습니다. 반면, 비디오 캡처는 주로 시간적 추론과 공간적 정보를 결합하는 데 사용됩니다. 이러한 공간 문제를 해결하기 위해, 다음 이벤트 예측(NEP)를 제안합니다. NEP는 미래의 비디오 Segment를 풍부한, 자동 관측된 신호로 활용하여 시간적 추론을 촉진하는 학습 임무입니다. 비디오를 과거와 미래의 프레임으로 나누고, MLLM은 과거의 프레임을 입력으로 하여 미래의 프레임에서 얻을 수 있는 이벤트의 요약을 예측하는 방식으로, 모델이 시간적으로 추론하여 임무를 완료하도록 촉구합니다. 이 임무를 지원하기 위해, V1-33K 데이터 세트를 생성합니다. 이 데이터 세트는 33,000개의 자동 추출된 비디오 Segment로 구성되어 있으며, 다양한 리アル 웨ア 스키 차트를 기록하고 있습니다. 또한, 시간적 추론의 효과에 대한 조사를 위해, 비디오 인스트럭션 튜닝 전략의 범위를 확장합니다. 진척도를 평가하기 위해, FutureBench를 도입하여, 이전에未见한 미래의 이벤트의 예측의 일치성을 평가합니다. 실험은 NEP가 MLLM의 시간적 추론을 촉진하는 데 적합한, 효과적인 학습 패러다임을 제공하는 것을 증명합니다.",
      "upvotes": 23,
      "discussionId": "68380913e9c1608de91e2430",
      "githubRepo": "https://github.com/sail-sg/Video-Next-Event-Prediction",
      "ai_summary": "Next-event prediction (NEP) is proposed as a learning task to enable MLLMs to reason temporally over video inputs, using future video segments as a self-supervised signal.",
      "ai_keywords": [
        "next-token prediction",
        "next-event prediction (NEP)",
        "LLMs",
        "MLLMs",
        "video question answering",
        "video captioning",
        "temporal reasoning",
        "future video segments",
        "past frames",
        "video segments",
        "V1-33K",
        "video instruction-tuning strategies",
        "FutureBench"
      ]
    },
    "publishedAt": "2025-05-28T11:13:34.000Z",
    "title": "Fostering Video Reasoning via Next-Event Prediction",
    "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22457.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d91b6d255ef6add20e1b38",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
      "fullname": "Tianyu Pang",
      "name": "P2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19253",
      "authors": [
        {
          "_id": "6837bc8d0b39c9653de4d06f",
          "user": {
            "_id": "6244451c9fdefb55a0b900cc",
            "avatarUrl": "/avatars/ca2b46ddb5d905501d827920582b5438.svg",
            "isPro": false,
            "fullname": "Joao Coelho",
            "user": "jmvcoelho",
            "type": "user"
          },
          "name": "João Coelho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:45.049Z",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d070",
          "name": "Jingjie Ning",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d071",
          "name": "Jingyuan He",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d072",
          "name": "Kangrui Mao",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d073",
          "name": "Abhijay Paladugu",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d074",
          "name": "Pranav Setlur",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d075",
          "name": "Jiahe Jin",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d076",
          "name": "Jamie Callan",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d077",
          "name": "João Magalhães",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d078",
          "name": "Bruno Martins",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d079",
          "name": "Chenyan Xiong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T18:16:13.000Z",
      "submittedOnDailyAt": "2025-05-29T00:28:31.472Z",
      "title": "DeepResearchGym: 무료, 투명, 재현성 있는 평가 Sandbox에서 깊은 연구",
      "submittedOnDailyBy": {
        "_id": "6135eeeb5bc6ecdf86b60f0d",
        "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
        "isPro": false,
        "fullname": "Shi Yu",
        "user": "yushi",
        "type": "user"
      },
      "summary": "Deep research systems은 복잡한クエリ에 대한 상세하고 증거가 풍부한 보고서를 생성하는新兴的情報検索方法의 클래스입니다. 그러나 현재의 많은 프레임워크들은 동적인コマーシャルサーチAPI를 의존하고 있으며, 재현성과 투명성의 문제에 추가하여 비용 높은 문제를 겪고 있습니다. 이러한 제한을 해결하기 위해, DeepResearchGym라는 오픈소스의 Sandbox를 소개하고, 재현성을 보장하는 검색API와 엄격한 평가 프로토콜을 조합하여 깊은 연구 시스템의 벤치마크를 수행합니다. API는 최신의 밀집 검색기와 디스크 ANN을 사용하여, ClueWeb22와 FineWeb라는 큰 규모의 공개 웹 코퍼를 인덱싱하고, 인기 있는コマーシャルAPI보다 낮은 Cut Rental을 실현하고, 안정적인 문서의 순위를 보장하며, 연구용으로 무료로 사용할 수 있습니다. 깊은 연구 시스템의 출력을 평가하기 위해, Researchy Questions 벤치마크를 LLM-as-a-judge 평가에 통하여 자동 평가 메트릭을 확장하고, 사용자의 정보 필요에 맞는 대응성, 검색의 정확성, 보고서의 질을 평가합니다. 실험 결과를 따르면, DeepResearchGym을 조합한 시스템은コマーシャルAPI를 사용했던 것과 동일한 성능을 달성하고, 평가 메트릭에 의한 성능 순위는 일치합니다. 또한, 인간 평가 연구는 자동 평가 프로토콜이 인간의 취향에 맞았는지 확인하고, 프레임워크가 깊은 연구 시스템의 제어된 평가에 도움을 줄 수 있음을 증명했습니다. 코드와 API 문서는 https://www.deepresearchgym.ai에서 사용 가능합니다.",
      "upvotes": 17,
      "discussionId": "6837bc8d0b39c9653de4d0a6",
      "projectPage": "https://www.deepresearchgym.ai",
      "ai_summary": "DeepResearchGym provides an open-source evaluation framework for deep research systems using a reproducible search API and LLM-as-a-judge assessments.",
      "ai_keywords": [
        "agentic information retrieval",
        "deep research systems",
        "search API",
        "reproducibility",
        "transparency",
        "open-source sandbox",
        "ClueWeb22",
        "FineWeb",
        "dense retriever",
        "approximate nearest neighbor search",
        "DiskANN",
        "Researchy Questions benchmark",
        "LLM-as-a-judge",
        "retrieval faithfulness",
        "report quality",
        "human evaluation"
      ]
    },
    "publishedAt": "2025-05-25T14:16:13.000Z",
    "title": "DeepResearchGym: A Free, Transparent, and Reproducible Evaluation\n  Sandbox for Deep Research",
    "summary": "Deep research systems represent an emerging class of agentic information\nretrieval methods that generate comprehensive and well-supported reports to\ncomplex queries. However, most existing frameworks rely on dynamic commercial\nsearch APIs, which pose reproducibility and transparency challenges in addition\nto their cost. To address these limitations, we introduce DeepResearchGym, an\nopen-source sandbox that combines a reproducible search API with a rigorous\nevaluation protocol for benchmarking deep research systems. The API indexes\nlarge-scale public web corpora, namely ClueWeb22 and FineWeb, using a\nstate-of-the-art dense retriever and approximate nearest neighbor search via\nDiskANN. It achieves lower latency than popular commercial APIs while ensuring\nstable document rankings across runs, and is freely available for research use.\nTo evaluate deep research systems' outputs, we extend the Researchy Questions\nbenchmark with automatic metrics through LLM-as-a-judge assessments to measure\nalignment with users' information needs, retrieval faithfulness, and report\nquality. Experimental results show that systems integrated with DeepResearchGym\nachieve performance comparable to those using commercial APIs, with performance\nrankings remaining consistent across evaluation metrics. A human evaluation\nstudy further confirms that our automatic protocol aligns with human\npreferences, validating the framework's ability to help support controlled\nassessment of deep research systems. Our code and API documentation are\navailable at https://www.deepresearchgym.ai.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6135eeeb5bc6ecdf86b60f0d",
      "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
      "fullname": "Shi Yu",
      "name": "yushi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21925",
      "authors": [
        {
          "_id": "6837c23acce400abe6f18790",
          "user": {
            "_id": "60747cbf3ea03830676542b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
            "isPro": false,
            "fullname": "Chong Zeng",
            "user": "NCJ",
            "type": "user"
          },
          "name": "Chong Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:38.730Z",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18791",
          "user": {
            "_id": "63299011bdb6242b42b77f57",
            "avatarUrl": "/avatars/056aec97eb3c10d3b63eb13238e1d2a4.svg",
            "isPro": false,
            "fullname": "doyleconan",
            "user": "doyleconan",
            "type": "user"
          },
          "name": "Yue Dong",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:11:08.981Z",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18792",
          "name": "Pieter Peers",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18793",
          "name": "Hongzhi Wu",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18794",
          "name": "Xin Tong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60747cbf3ea03830676542b5/DuGYODs8k7DhVHb78BmCZ.jpeg"
      ],
      "publishedAt": "2025-05-28T03:20:46.000Z",
      "submittedOnDailyAt": "2025-05-29T00:45:14.960Z",
      "title": "RenderFormer: 삼각형 매칭을 기반으로하는 Transformer 기반 뉴럴 렌더링 및 글로벌 조명\n\n(注意：虽然要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，我在翻译时尽量保持了原文的结构和术语的精确性。)",
      "submittedOnDailyBy": {
        "_id": "60747cbf3ea03830676542b5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
        "isPro": false,
        "fullname": "Chong Zeng",
        "user": "NCJ",
        "type": "user"
      },
      "summary": "RenderFormer는 삼각형 기반의 스케니 표현으로부터 이미지를 직접 렌더링하는 뉴럴 렌더링 파이프라인입니다. 이는 완전한 글로벌 조명 효과를 포함하여 각 스케니의 훈련이나 미세 조정이 필요하지 않습니다. 물리적인 접근 방식을 대신하여 렌더링을 토큰의 열에서 출력 토큰의 열로 변환하는 방식으로 구성됩니다. 이는 반사성을 가진 삼각형을 표현하는 토큰의 열을 작은 픽셀 패치를 표현하는 출력 토큰의 열로 변환합니다. RenderFormer는 2단계 파이프라인을 사용합니다: 점점 독립 단계에서는 삼각형으로부터 삼각형으로의 빛의 전파를 모델링하고, 점점 의존 단계에서는 점점 독립 단계에서 얻을 수 있는 삼각형 열을 통해 라이트 바ン들을 표현하는 토큰을 픽셀 값으로 변환합니다. 두 단계는 트랜지포터 아키텍처에 기초하여 최소한의 사전 제약을 가지고 학습됩니다. 형상과 빛의 전파의 복잡성이 변하는 스케니에서 RenderFormer의 기능과 성능을 평가합니다.",
      "upvotes": 15,
      "discussionId": "6837c23ccce400abe6f18812",
      "projectPage": "https://microsoft.github.io/renderformer/",
      "githubRepo": "https://github.com/microsoft/renderformer",
      "ai_summary": "RenderFormer is a transformer-based neural rendering pipeline that renders images from triangle representations without per-scene training and with full global illumination effects.",
      "ai_keywords": [
        "neural rendering pipeline",
        "global illumination effects",
        "sequence-to-sequence transformation",
        "tokens",
        "reflectance properties",
        "pixel patches",
        "transformer architecture",
        "view-independent stage",
        "view-dependent stage",
        "triangle-to-triangle light transport",
        "ray bundles"
      ]
    },
    "publishedAt": "2025-05-27T23:20:46.000Z",
    "title": "RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with\n  Global Illumination",
    "summary": "We present RenderFormer, a neural rendering pipeline that directly renders an\nimage from a triangle-based representation of a scene with full global\nillumination effects and that does not require per-scene training or\nfine-tuning. Instead of taking a physics-centric approach to rendering, we\nformulate rendering as a sequence-to-sequence transformation where a sequence\nof tokens representing triangles with reflectance properties is converted to a\nsequence of output tokens representing small patches of pixels. RenderFormer\nfollows a two stage pipeline: a view-independent stage that models\ntriangle-to-triangle light transport, and a view-dependent stage that\ntransforms a token representing a bundle of rays to the corresponding pixel\nvalues guided by the triangle-sequence from the view-independent stage. Both\nstages are based on the transformer architecture and are learned with minimal\nprior constraints. We demonstrate and evaluate RenderFormer on scenes with\nvarying complexity in shape and light transport.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60747cbf3ea03830676542b5/DuGYODs8k7DhVHb78BmCZ.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60747cbf3ea03830676542b5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
      "fullname": "Chong Zeng",
      "name": "NCJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18600",
      "authors": [
        {
          "_id": "6837fe7664391bba7e477747",
          "name": "Bryan Sangwoo Kim",
          "hidden": false
        },
        {
          "_id": "6837fe7664391bba7e477748",
          "name": "Jeongsol Kim",
          "hidden": false
        },
        {
          "_id": "6837fe7664391bba7e477749",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T08:50:08.000Z",
      "submittedOnDailyAt": "2025-05-29T05:07:53.112Z",
      "title": "Chain-of-Zoom: 극대의 초해상도를 달성하는 스케일 자동 회귀와 취미 조정",
      "submittedOnDailyBy": {
        "_id": "6628efe14e1fa854f48d3a28",
        "avatarUrl": "/avatars/aa5421149a07a82b5c2a25978f9b6926.svg",
        "isPro": false,
        "fullname": "Sangwoo Kim",
        "user": "bryanswkim",
        "type": "user"
      },
      "summary": "현대의 단일 이미지 초해상도(SISR) 모델은 훈련된 스케일 팩터의 범위에서 현실적인 결과를 제공하지만, 더 큰 스케일로 확대될 때 파괴되어서는 안됩니다. 이러한 스케일라빌리티 블록을 해결하기 위해 Chain-of-Zoom(CoZ)를 사용합니다. CoZ는 여러 스케일 정보를 포함하는 Prompt를 가진 중간 스케일 상태의 자동 회귀 연결로 SISR를 분해합니다. CoZ는 메인 SR 모델을 재활용하고 조건부 확률을 계산할 수 있는 서브 문제를 분해하여 추가적인 훈련을 필요로 하지 않고 극한 해상도를 달성합니다. 고 확장률에서 시각적 키핑이 감소하기 때문에 각 확대 단계마다 시각 언어 모델(VLM)로 생성된 여러 스케일 정보를 포함하는 Prompt를 추가합니다. 이 Prompt 추출기는 Generalized Reward Policy Optimization(GRPO)와 코너 VLM을 사용하여 인간적인 취향에 맞는 텍스트 가이드 동역학에 맞춥니다. 실험은 표준 4배 확대 SR 모델을 CoZ로 감싸서 고품질의 시각적 품질과 높은 충실도를 가지는 256배 이상의 확대를 실현할 수 있음을 보여줍니다. 프로젝트 페이지: https://bryanswkim.github.io/chain-of-zoom/ .",
      "upvotes": 15,
      "discussionId": "6837fe7864391bba7e47779b",
      "projectPage": "https://bryanswkim.github.io/chain-of-zoom/",
      "githubRepo": "https://github.com/bryanswkim/Chain-of-Zoom",
      "ai_summary": "Chain-of-Zoom (CoZ) enhances single-image super-resolution models by using an autoregressive chain of intermediate scale-states and multi-scale-aware prompts to achieve extreme magnifications with high quality.",
      "ai_keywords": [
        "single-image super-resolution",
        "Chain-of-Zoom",
        "autoregressive chain",
        "multi-scale-aware prompts",
        "backbone SR model",
        "diffusion SR model",
        "prompt extractor",
        "Generalized Reward Policy Optimization",
        "critic VLM"
      ]
    },
    "publishedAt": "2025-05-24T04:50:08.000Z",
    "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and\n  Preference Alignment",
    "summary": "Modern single-image super-resolution (SISR) models deliver photo-realistic\nresults at the scale factors on which they are trained, but collapse when asked\nto magnify far beyond that regime. We address this scalability bottleneck with\nChain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an\nautoregressive chain of intermediate scale-states with multi-scale-aware\nprompts. CoZ repeatedly re-uses a backbone SR model, decomposing the\nconditional probability into tractable sub-problems to achieve extreme\nresolutions without additional training. Because visual cues diminish at high\nmagnifications, we augment each zoom step with multi-scale-aware text prompts\ngenerated by a vision-language model (VLM). The prompt extractor itself is\nfine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic\nVLM, aligning text guidance towards human preference. Experiments show that a\nstandard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement\nwith high perceptual quality and fidelity. Project Page:\nhttps://bryanswkim.github.io/chain-of-zoom/ .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6628efe14e1fa854f48d3a28",
      "avatarUrl": "/avatars/aa5421149a07a82b5c2a25978f9b6926.svg",
      "fullname": "Sangwoo Kim",
      "name": "bryanswkim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19075",
      "authors": [
        {
          "_id": "6837fc484d14d7c8800e8b9c",
          "user": {
            "_id": "64c3732de6c3860fba66ceb0",
            "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
            "isPro": false,
            "fullname": "JaeminKim",
            "user": "kjm981995",
            "type": "user"
          },
          "name": "Jaemin Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:35.589Z",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9d",
          "name": "Hangeol Chang",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9e",
          "name": "Hyunmin Hwang",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9f",
          "name": "Choonghan Kim",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8ba0",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T10:19:10.000Z",
      "submittedOnDailyAt": "2025-05-29T04:49:39.731Z",
      "title": "유니버설 리서버: 1개의 조합 가능한 플러그앤플레이 리서버를 Frozen LLMs에 대해 사용합니다.",
      "submittedOnDailyBy": {
        "_id": "64c3732de6c3860fba66ceb0",
        "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
        "isPro": false,
        "fullname": "JaeminKim",
        "user": "kjm981995",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 뛰어난 일반적인 능력을 보여주지만, 논리론에 대한 능력 향상은 계산 컴퓨팅 리소스의 많은 부담을 동반하며, 그 일반화에도 불구하고 손실이 발생할 수 있습니다. Parameter-Efficient Fine-Tuning(PEFT) 메소드는 컴퓨팅 리소스에 친숙한 대안책으로 제안되어 있지만, 아키텍처의 의존관계에 따라 각 LLM 백본에 대해 재학습이 필요합니다. 이러한 문제를 해결하기 위해, 우리는 Universal Reasoner(UniR)를 제안합니다. UniR는 한 가지 가벼운, 조합 가능한, 플러그인 및 플레이인 가능한 논리론 모듈입니다. 이 모듈은 임의의 자유드 LLM과 조합하여 전문적인 논리론 능력을 부여할 수 있습니다. 특히, UniR는专用의 논리론 모듈로, 예약 정의의 보상을 사용하여 독립적으로 학습되며, 프로세스 수준의 신호를 토큰 수준의 가이드로 변환합니다. 학습 후, UniR는 추론 시 LLM 백본의 출력 로지ッ트에 간단히 추가되어, 임의의 자유드 LLM과 조합할 수 있습니다. 이 추가 구조는 모듈의 조합을 자연스럽게 가능하게 합니다: 서로 다른 태스크에서 학습된 여러 UniR 모듈을 로지ッ트의 합으로 함께 적용하여 복잡한 논리론을 가능하게 합니다. 수학적 논리론과 기계 번역 태스크의 실험 결과를 통해, UniR는 Llama3.2 모델을 사용된 기존의 베이스라인의 재학습 메소드보다 유의미하게 뛰어납니다. 또한, UniR는 강한 일반화를 보여, 작은 모델에서 학습된 논리론 모듈은 그들이 더 큰 LLM에도 효과적으로 지도할 수 있음을 보여줍니다. 이는 LLM의 논리론을 향상시키기 위한 비용 효율적이고 적응적이고 강력한 해결책이며, 그 핵심 능력을 손상시키지 않도록 할 수 있습니다. 코드는 https://github.com/hangeol/UniR에서 공개되어 있습니다.",
      "upvotes": 14,
      "discussionId": "6837fc494d14d7c8800e8be6",
      "ai_summary": "UniR, a lightweight reasoning module, enhances Large Language Models with specialized reasoning abilities through modular composition, improving performance and generalization at lower computational costs.",
      "ai_keywords": [
        "Large Language Models",
        "Parameter-Efficient Fine-Tuning",
        "Universal Reasoner",
        "trajectory-level signals",
        "token-level guidance",
        "additive structure",
        "modular composition",
        "Llama3.2",
        "mathematical reasoning",
        "machine translation",
        "cost-efficient",
        "adaptable",
        "robust"
      ]
    },
    "publishedAt": "2025-05-25T06:19:10.000Z",
    "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for\n  Frozen LLMs",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms existing baseline fine-tuning methods using the\nLlama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19075.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3732de6c3860fba66ceb0",
      "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
      "fullname": "JaeminKim",
      "name": "kjm981995",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21887",
      "authors": [
        {
          "_id": "6837e3400aa18c6f96fe4876",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:49.456Z",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4877",
          "name": "Yahia Salaheldin Shaaban",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4878",
          "name": "Martin Takac",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4879",
          "name": "Salem Lahlou",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe487a",
          "name": "Zangir Iklassov",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Xa51WJrOIV-xl7A-Ry4t2.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/eS_bEDm2o2WAbn3YPS2XR.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/jaG4WahYyrQGaalZObNew.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mMKY5jn0SCJPxxR8_mssx.png"
      ],
      "publishedAt": "2025-05-28T02:03:31.000Z",
      "submittedOnDailyAt": "2025-05-29T03:02:06.399Z",
      "title": "SVRPBench: 확률적 비커 루팅 문제의 실용적인 벤치마크",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "확실한 경로 처리는 현실적인 물류의 중심에 자리잡고 있으며, 대부분의 벤치마크는 정적,理想적인 설정을 가정하고 있습니다. 우리는 도시 규모의 경로 처리의 높은 정확도를 파악하는 첫 번째 오픈 벤치마크인 SVRPBench를 소개합니다. 500 이상의 인스턴스를 경험하고, 1000 이상의 크레인을 포함하며, 시간 의존적인 교통 혼잡, 로그 정규의 지연, 확률적인 사고, 주거 및 상업적인 크레인의 실험적 기반의 시간 윈도우를 모방합니다. 우리의 프로세스는 다양한 스케너를 생성하며, 다양한 구성과 제약을 풍부하게 포함합니다. 벤치마크에 의한 결과를 통해, 분산적인 변동에 의해 가장 先端의 RL 솔버도 POMO와 AM이 20% 이상의 성능 저하를 보입니다が, 고전적인 및 메타 하이브리스 방식은 강건합니다. 실험적 연구를 가능하게 하기 위해, 데이터 세트와 평가 시스템를 릴리즈합니다. SVRPBench는 합성적인 가정을 초월하여 일반화 가능한 솔버를 설계하기 위해 커뮤니티에 도전합니다.",
      "upvotes": 13,
      "discussionId": "6837e3410aa18c6f96fe48b8",
      "githubRepo": "https://github.com/yehias21/vrp-benchmarks",
      "ai_summary": "SVRPBench introduces a new benchmark for vehicle routing under uncertainty, simulating realistic urban conditions and highlighting the limitations of state-of-the-art RL solvers.",
      "ai_keywords": [
        "vehicle routing",
        "SVRPBench",
        "time-dependent congestion",
        "log-normal delays",
        "probabilistic accidents",
        "multi-depot",
        "multi-vehicle",
        "state-of-the-art RL solvers",
        "POMO",
        "AM",
        "distributional shift",
        "classical methods",
        "metaheuristic methods"
      ]
    },
    "publishedAt": "2025-05-27T22:03:31.000Z",
    "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem",
    "summary": "Robust routing under uncertainty is central to real-world logistics, yet most\nbenchmarks assume static, idealized settings. We present SVRPBench, the first\nopen benchmark to capture high-fidelity stochastic dynamics in vehicle routing\nat urban scale. Spanning more than 500 instances with up to 1000 customers, it\nsimulates realistic delivery conditions: time-dependent congestion, log-normal\ndelays, probabilistic accidents, and empirically grounded time windows for\nresidential and commercial clients. Our pipeline generates diverse,\nconstraint-rich scenarios, including multi-depot and multi-vehicle setups.\nBenchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade\nby over 20% under distributional shift, while classical and metaheuristic\nmethods remain robust. To enable reproducible research, we release the dataset\nand evaluation suite. SVRPBench challenges the community to design solvers that\ngeneralize beyond synthetic assumptions and adapt to real-world uncertainty.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Xa51WJrOIV-xl7A-Ry4t2.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/eS_bEDm2o2WAbn3YPS2XR.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/jaG4WahYyrQGaalZObNew.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mMKY5jn0SCJPxxR8_mssx.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21887.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 39
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22129",
      "authors": [
        {
          "_id": "6837dae6e9b21653755a05d4",
          "user": {
            "_id": "64c71a5647418a0a59e5c7cb",
            "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
            "isPro": false,
            "fullname": "Jinhong Ni",
            "user": "mcleanie",
            "type": "user"
          },
          "name": "Jinhong Ni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:03.476Z",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d5",
          "user": {
            "_id": "65434daa5a36a8774d0e2271",
            "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
            "isPro": false,
            "fullname": "Allen Zhang",
            "user": "allencbzhang",
            "type": "user"
          },
          "name": "Chang-Bin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:59.930Z",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d6",
          "name": "Qiang Zhang",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d7",
          "name": "Jing Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T08:54:04.000Z",
      "submittedOnDailyAt": "2025-05-29T02:38:27.338Z",
      "title": "텍스트로부터 360도 Panorama를 안정적으로 생성하기 위한 요인들은 무엇입니까？",
      "submittedOnDailyBy": {
        "_id": "65434daa5a36a8774d0e2271",
        "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
        "isPro": false,
        "fullname": "Allen Zhang",
        "user": "allencbzhang",
        "type": "user"
      },
      "summary": "최근, 텍스트로부터 이미지의 디퓨션 모델(예: Stable Diffusion)의 풍부성으로 360도 Panorama 이미지 생성에 적용하는 연구가 시작되어 있습니다. 선행 연구는 일반적인 저レン키언 어다벵 테크닉의 유효성을 보여주며, 미리 훈련된 디퓨션 모델을 사용하여 Panorama 이미지를 생성하는 데 사용되었습니다. 그러나 촬영각과 Panorama 이미지 사이의 큰 영역차이, 이러한 실험적 성공에 대한 후방 기술에 대한 의혹이 있습니다. 우리는 학습 가능한 컴포넌트가 Panorama 데이터에 최종 훈련되어 다른 행동을 보이며, 이 어다벵은 미리 훈련된 디퓨션 모델 내부의 이전 지식을 활용하는 내적 기능으로 숨겨져 있다는 점을 조사하고 있습니다. 우리의 분석은 다음과 같습니다: 1) 주목 모듈의 쿼리와 키 매트릭스는 Panorama와 촬영각의 영역 간에 공유 가능한 공통 정보에 책임이 있고, Panorama 생성과 관련이 없으며; 2) 값과 출력 가중치 매트릭스는 미리 훈련된 지식의 Panorama 영역에 적용하는 것을 특화하고, Panorama 생성의 최종 훈련 기간에 더 중요한 역할을 수행합니다. 우리는 이러한 통찰을 실험적으로 증명하기 위해 간단한 프레임워크를 제안하고 있습니다. 이것을 UniPano라고 부르며, 향후 연구에 우수한 기준을 만드는 것을 목표로 합니다. UniPano는 기존 방법보다 뛰어나며, 선행의 이중 브랜치 접근에 비해 메모리 사용량과 훈련 시간의 크게 줄이는 동시에, 고해상도 Panorama 생성에도 확장할 수 있습니다. 코드는 릴리스됩니다.",
      "upvotes": 12,
      "discussionId": "6837daece9b21653755a0791",
      "ai_summary": "Analysis of fine-tuning diffusion models for panoramic image generation reveals distinct roles of attention module matrices and introduces UniPano, a memory-efficient and speed-enhanced baseline framework.",
      "ai_keywords": [
        "text-to-image diffusion models",
        "Stable Diffusion",
        "low-rank adaptation",
        "pre-trained diffusion models",
        "attention modules",
        "query matrices",
        "key matrices",
        "value matrices",
        "output weight matrices",
        "panoramic image generation",
        "domain gap",
        "common information",
        "pre-trained knowledge",
        "UniPano",
        "end-to-end panorama generation",
        "memory usage",
        "training time"
      ]
    },
    "publishedAt": "2025-05-28T04:54:04.000Z",
    "title": "What Makes for Text to 360-degree Panorama Generation with Stable\n  Diffusion?",
    "summary": "Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,\nhas stimulated research to adapt them to 360-degree panorama generation. Prior\nwork has demonstrated the feasibility of using conventional low-rank adaptation\ntechniques on pre-trained diffusion models to generate panoramic images.\nHowever, the substantial domain gap between perspective and panoramic images\nraises questions about the underlying mechanisms enabling this empirical\nsuccess. We hypothesize and examine that the trainable counterparts exhibit\ndistinct behaviors when fine-tuned on panoramic data, and such an adaptation\nconceals some intrinsic mechanism to leverage the prior knowledge within the\npre-trained diffusion models. Our analysis reveals the following: 1) the query\nand key matrices in the attention modules are responsible for common\ninformation that can be shared between the panoramic and perspective domains,\nthus are less relevant to panorama generation; and 2) the value and output\nweight matrices specialize in adapting pre-trained knowledge to the panoramic\ndomain, playing a more critical role during fine-tuning for panorama\ngeneration. We empirically verify these insights by introducing a simple\nframework called UniPano, with the objective of establishing an elegant\nbaseline for future research. UniPano not only outperforms existing methods but\nalso significantly reduces memory usage and training time compared to prior\ndual-branch approaches, making it scalable for end-to-end panorama generation\nwith higher resolution. The code will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65434daa5a36a8774d0e2271",
      "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
      "fullname": "Allen Zhang",
      "name": "allencbzhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20411",
      "authors": [
        {
          "_id": "683735aff42cc8a1d260e677",
          "user": {
            "_id": "654e5e094319c75e3e1b6cbc",
            "avatarUrl": "/avatars/a8889036fa38f80f2d45aea8d1471395.svg",
            "isPro": false,
            "fullname": "Ibragim",
            "user": "ibragim-bad",
            "type": "user"
          },
          "name": "Ibragim Badertdinov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:24.705Z",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e678",
          "user": {
            "_id": "644e9ffcd6001776ed77d874",
            "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
            "isPro": false,
            "fullname": "Alexander",
            "user": "djalexj",
            "type": "user"
          },
          "name": "Alexander Golubev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T19:13:38.030Z",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e679",
          "name": "Maksim Nekrashevich",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67a",
          "name": "Anton Shevtsov",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67b",
          "user": {
            "_id": "65e48cb3a4e46e644ec1277d",
            "avatarUrl": "/avatars/dd2bf04a6f81bf0a0892080af5d485b2.svg",
            "isPro": false,
            "fullname": "Simon Karasik",
            "user": "sbkarasik",
            "type": "user"
          },
          "name": "Simon Karasik",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:26.644Z",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67c",
          "name": "Andrei Andriushchenko",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67d",
          "name": "Maria Trofimova",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67e",
          "name": "Daria Litvintseva",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67f",
          "name": "Boris Yangel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T18:01:00.000Z",
      "submittedOnDailyAt": "2025-05-29T06:59:04.261Z",
      "title": "SWE-rebench: 소프트웨어공학의 태스크집계와 디코ン테미네이션평가를 자동화하는 파이프라인",
      "submittedOnDailyBy": {
        "_id": "644e9ffcd6001776ed77d874",
        "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
        "isPro": false,
        "fullname": "Alexander",
        "user": "djalexj",
        "type": "user"
      },
      "summary": "LLM 기반의 에이전트는 점점 더 다양한 범위의 소프트웨어 개발(SWE) 태스크에 대해 원하는 능력을 보여주고 있습니다. 그러나 이 분야의 발전에는 두 가지 중요한 문제점이 있습니다. 첫 번째 문제는 고품질의 훈련 데이터가 부족하며, 특히, 에이전트가 개발 환경과 상호작용하여 코드를 실행하고, 그 행동의 결과를 기반으로 행동을 변경하는 실제적인 SWE 시나리오를 반영하는 데이터가 부족합니다. 현재의 데이터 세트는 한 번의 코드 생성 또는 작은 수의 손동으로 편집된 상호작용 태스크의 집합으로 제한되어 있으며, 둘 다 규모와 다양성이 부족합니다. 두 번째 문제는 새로운 상호작용 태스크의 부족이 급격히 개선되는 모델의 평가에 영향을 미칩니다. 정적 벤치마크는 컨테이너 문제로 인해 빠르게 уста려 됩니다. 이러한 제한을 해결하기 위해, 우리는 다양한 GitHub 리포지토리에서 실제적인 상호작용 태스크를 지속적으로 추출하는 새로운, 자동적이고 scalable한 파이프라인을 도입합니다. 이 파이프라인을 사용하여, SWE-rebench라는 공개 데이터 세트를 구축했습니다. 이 데이터 세트는 21,000 이상의 인터랙티브な Python 기반의 SWE 태스크를 포함하며, SWE 에이전트의 강화 학습에 적합합니다. 또한, SWE-rebench의 방법론을 사용하여 수집된 새로운 태스크의 지속적인 공급을 사용하여, 컨테이너 문제에 의해 영향을 받지 않는 벤치마크를 구축했습니다. 이 벤치마크에서 각 LLM의 결과를 SWE-bench Verified의 결과를 비교하여, 엔지니어링 모델의 성능이 컨테이너 문제로 인해팽창하는 가능성이 있음을 나타냅니다.",
      "upvotes": 11,
      "discussionId": "683735b0f42cc8a1d260e69f",
      "ai_summary": "A novel pipeline extracts real-world, interactive software engineering tasks from GitHub to create SWE-rebench, improving the evaluation of reinforcement learning models in SWE.",
      "ai_keywords": [
        "LLM-based agents",
        "reinforcement learning",
        "software engineering tasks",
        "GitHub repositories",
        "SWE-rebench",
        "contamination-free benchmark",
        "SWE-bench Verified"
      ]
    },
    "publishedAt": "2025-05-26T14:01:00.000Z",
    "title": "SWE-rebench: An Automated Pipeline for Task Collection and\n  Decontaminated Evaluation of Software Engineering Agents",
    "summary": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644e9ffcd6001776ed77d874",
      "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
      "fullname": "Alexander",
      "name": "djalexj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22648",
      "authors": [
        {
          "_id": "6837c03cbbee677da73e6034",
          "user": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "name": "Jialong Wu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:02:37.069Z",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6035",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6036",
          "name": "Runnan Fang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6037",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6038",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6039",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603a",
          "name": "Dingchu Zhang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603b",
          "name": "Zekun Xi",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603c",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603d",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603e",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603f",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/vhAmZAlJqekE6vLcVjWtO.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/RXQVwE9PRmBzxCURRiKAU.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/woFRNvRqdUXKnAHpRH2eM.mp4"
      ],
      "publishedAt": "2025-05-28T17:57:07.000Z",
      "submittedOnDailyAt": "2025-05-29T00:34:30.750Z",
      "title": "웹디저: 자동화 정보 탐색 기관에 대한 방향법",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "실제 복잡한 문제를 해결하기 위해서는 구체적인 정보를 탐색하고 단계별 추론이 필요합니다. 최근의 Deep Research를 예로 들 수 있는 에이전트 시스템의 발전은 자동화된 단계별 조사의 가능성을 강조하고 있습니다. 본 논문에서는 데이터 중심과 훈련 단계의 관점에서, 사용자 기기에서 에이전트 방식의 정보 탐색 에이전트의 구축을 위한 일련의 패러다임에 대해 제안합니다. 우리 접근 방식은 4개의 주요 단계로 구성됩니다: 1) 데이터 브레이저의 구축, 2) 타라이크의 샘플링, 3) 냉동 시작의 효과적인 훈련, 4) 확장성을 위한 강화 학습. ReAct 기반의 WebDancer의 웹 에이전트에서 이러한 프레임워크를 구현하였습니다. GAIA와 WebWalkerQA의 어려운 정보 탐색 벤치마크에서의 실험 평가는 WebDancer의 강력한 성능을 보여주며, 우리 훈련 패러다임의 효과를 명확히 하였습니다. 에이전트의 훈련을 위한 추가적인 분석은 더 유능한 에이전트 모델의 개발에 도움이 되는 유효한 아이디어와 행동적, 시스템적 패스워드를 제공합니다. 코드와 Demo는 https://github.com/Alibaba-NLP/WebAgent에 공개되었습니다.",
      "upvotes": 10,
      "discussionId": "6837c03dbbee677da73e607f",
      "githubRepo": "https://github.com/Alibaba-NLP/WebAgent",
      "ai_summary": "The paper proposes a framework for building end-to-end agentic information seeking agents through a combination of data construction, trajectory sampling, supervised fine-tuning, and reinforcement learning, showcasing its effectiveness on information seeking benchmarks.",
      "ai_keywords": [
        "browsing data construction",
        "trajectories sampling",
        "supervised fine-tuning",
        "reinforcement learning",
        "WebDancer",
        "GAIA",
        "WebWalkerQA"
      ]
    },
    "publishedAt": "2025-05-28T13:57:07.000Z",
    "title": "WebDancer: Towards Autonomous Information Seeking Agency",
    "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/vhAmZAlJqekE6vLcVjWtO.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/RXQVwE9PRmBzxCURRiKAU.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/woFRNvRqdUXKnAHpRH2eM.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22648.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19187",
      "authors": [
        {
          "_id": "6837210455e9bab4e9c302b1",
          "user": {
            "_id": "6002c316698168af3bb9f4a6",
            "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
            "isPro": false,
            "fullname": "yangxiao",
            "user": "YangXiao-nlp",
            "type": "user"
          },
          "name": "Yang Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T19:13:52.090Z",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b2",
          "name": "Jiashuo Wang",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b3",
          "name": "Ruifeng Yuan",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b4",
          "name": "Chunpu Xu",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b5",
          "name": "Kaishuai Xu",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b6",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b7",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T15:17:57.000Z",
      "submittedOnDailyAt": "2025-05-29T01:05:08.605Z",
      "title": "LIMOPro: 효율적이고 효과적인 검사 시간 스케줄링을 위한 논리론의 발전",
      "submittedOnDailyBy": {
        "_id": "6002c316698168af3bb9f4a6",
        "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
        "isPro": false,
        "fullname": "yangxiao",
        "user": "YangXiao-nlp",
        "type": "user"
      },
      "summary": "대 언어 모형（LLMs）는 검증 시 스케일링 접근 방식에 따라, 특히 Chain of Thought（CoT）데이터를 사용하여 최종 훈련된 경우, 놀라운 논리적인 계산 능력을 보여주고 있습니다. 그러나 이러한 논리적인 체인들은 진화적인 논리（본질적인 해결책 개발 경로）과 기능적 요소（검증 프로세스, 대체 해결책 접근, 오류 수정）에 따라, 긴 문장의 요소를 많이 포함하고 있습니다. 진화적인 논리는 중요하지만, 기능적 요소는 검증 시 추론 시의 계산 부담을 크게 증가시킵니다.\n\n우리는 답 예측의 신뢰도에 대한 영향을 기준으로, 각 논리적 단계의 중요성을定量적으로 평가하는 Principled Framework를 도입하고 있습니다. 이는 PIR（Perplexity-based Importance Refinement）으로 불리며, 진화적인 논리의 구성 요소를 유지하는 동시에, 낮은 중요도의 기능적 단계를 선택적으로 제거하여 최적화된 훈련 데이터를 생성합니다. PIR에 의해 훈련된 모델은 검증 시 스케일링의 성능이 향상되고, 계산량의 감소（-3% ～ -41%）와 정확도의 향상（+0.9% ～ +6.6%）를 얻습니다. 특히, AIME, AMC, GPQA Diamond 등 어려운 논리적 벤치마크에서도 효과가 나타납니다. 우리의 접근 방식은 다양한 모델 크기, 데이터 소스, 토큰 버지에 대한 강한 일반화 성능을 보여주며, 논리적인 LLMs의 실용화를 위한 유용한 해결책을 제공합니다.",
      "upvotes": 10,
      "discussionId": "6837210555e9bab4e9c302f2",
      "ai_summary": "A framework called PIR refines the importance of reasoning steps in large language models by pruning low-importance functional elements, leading to more concise reasoning chains with improved accuracy and reduced computational demands.",
      "ai_keywords": [
        "large language models",
        "chain-of-thought",
        "large reasoning models",
        "progressive reasoning",
        "functional elements",
        "perplexity-based importance refinement",
        "token usage",
        "reasoning benchmarks",
        "AIME",
        "AMC",
        "GPQA Diamond"
      ]
    },
    "publishedAt": "2025-05-25T11:17:57.000Z",
    "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time\n  Scaling",
    "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities through test-time scaling approaches, particularly when fine-tuned\nwith chain-of-thought (CoT) data distilled from more powerful large reasoning\nmodels (LRMs). However, these reasoning chains often contain verbose elements\nthat mirror human problem-solving, categorized as progressive reasoning (the\nessential solution development path) and functional elements (verification\nprocesses, alternative solution approaches, and error corrections). While\nprogressive reasoning is crucial, the functional elements significantly\nincrease computational demands during test-time inference. We introduce PIR\n(Perplexity-based Importance Refinement), a principled framework that\nquantitatively evaluates the importance of each reasoning step based on its\nimpact on answer prediction confidence. PIR systematically identifies and\nselectively prunes only low-importance functional steps while preserving\nprogressive reasoning components, creating optimized training data that\nmaintains the integrity of the core solution path while reducing verbosity.\nModels fine-tuned on PIR-optimized data exhibit superior test-time scaling\nproperties, generating more concise reasoning chains while achieving improved\naccuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to\n-41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).\nOur approach demonstrates strong generalizability across different model sizes,\ndata sources, and token budgets, offering a practical solution for deploying\nreasoning-capable LLMs in scenarios where efficient test-time scaling, response\ntime, and computational efficiency are valuable constraints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19187.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6002c316698168af3bb9f4a6",
      "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
      "fullname": "yangxiao",
      "name": "YangXiao-nlp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17663",
      "authors": [
        {
          "_id": "6833c6ff97966d18e7b995b0",
          "user": {
            "_id": "6002c316698168af3bb9f4a6",
            "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
            "isPro": false,
            "fullname": "yangxiao",
            "user": "YangXiao-nlp",
            "type": "user"
          },
          "name": "Yang Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T19:14:41.639Z",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b1",
          "name": "Jiashuo Wang",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b2",
          "name": "Qiancheng Xu",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b3",
          "name": "Changhe Song",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b4",
          "name": "Chunpu Xu",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b5",
          "name": "Yi Cheng",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b6",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b7",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T09:27:40.000Z",
      "submittedOnDailyAt": "2025-05-29T01:05:57.079Z",
      "title": "Dynamic Heart Theory Direction: Evaluating LLM Adaptation to the Time-Series Evolution of Human States",
      "submittedOnDailyBy": {
        "_id": "6002c316698168af3bb9f4a6",
        "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
        "isPro": false,
        "fullname": "yangxiao",
        "user": "YangXiao-nlp",
        "type": "user"
      },
      "summary": "LLM가 인간과 AI의 상호작용에 참여하는 것이 증가함에 따라, Theory of Mind (ToM)의 능력에 대한 평가가 중요해졌습니다. 특히, 마음의 상태를 추적하는 능력의 평가가 중요합니다. 현재의 기준은 기본적인 ToM 능력을 평가하지만, 주로 정적인 마음의 상태를 초점을 두고 있으며, 실제적인 사회적인 상호작용의 시간적 진행을 간과하고 있습니다. DynToM라는 새로운 기준을 제안합니다. 이 기준은 LLM이 마음의 상태를 시간적 진행을 이해하고 연결된 시나리오 속에서 추적하는 능력을 평가하기 위해 특별히 설계되었습니다. 체계적인 4단계의 프레임워크를 통해 1,100개의 사회적 컨텍스트를 생성하고, 이를 5,500개의 시나리오와 78,100개의 질문에 포함시켰습니다. 이들은 리アリ즘과 질에 검증되었습니다. 10개의 가장 선진된 LLM에 대한 상세한 평가에 따르면 평균적인 성능은 인간보다 44.7% 낮은 것을 명확히 알 수 있었으며, 마음의 상태를 추적하고 이유를 제공하는 경우 성능이 현저히 떨어집니다. 이러한 성능 차이는 현재의 LLM이 인간 마음의 상태를 동적인 특성을 모델링하는 능력의 기본적인 한계를 드러냅니다.",
      "upvotes": 10,
      "discussionId": "6833c70097966d18e7b99616",
      "ai_summary": "The DynToM benchmark evaluates LLMs' ability to track and understand the temporal progression of mental states, revealing significant gaps compared to human performance.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Theory of Mind (ToM)",
        "DynToM",
        "social contexts",
        "mental states",
        "temporal progression",
        "evaluation framework",
        "state-of-the-art LLMs",
        "performance gap",
        "dynamic mental states"
      ]
    },
    "publishedAt": "2025-05-23T05:27:40.000Z",
    "title": "Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal\n  Evolution of Human States",
    "summary": "As Large Language Models (LLMs) increasingly participate in human-AI\ninteractions, evaluating their Theory of Mind (ToM) capabilities - particularly\ntheir ability to track dynamic mental states - becomes crucial. While existing\nbenchmarks assess basic ToM abilities, they predominantly focus on static\nsnapshots of mental states, overlooking the temporal evolution that\ncharacterizes real-world social interactions. We present DynToM, a\nnovel benchmark specifically designed to evaluate LLMs' ability to understand\nand track the temporal progression of mental states across interconnected\nscenarios. Through a systematic four-step framework, we generate 1,100 social\ncontexts encompassing 5,500 scenarios and 78,100 questions, each validated for\nrealism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs\nreveals that their average performance underperforms humans by 44.7\\%, with\nperformance degrading significantly when tracking and reasoning about the shift\nof mental states. This performance gap highlights fundamental limitations in\ncurrent LLMs' ability to model the dynamic nature of human mental states.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17663.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6002c316698168af3bb9f4a6",
      "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
      "fullname": "yangxiao",
      "name": "YangXiao-nlp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22019",
      "authors": [
        {
          "_id": "6837ed297d00cf0a04677bc1",
          "user": {
            "_id": "657429d833e5a4bf5b278615",
            "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
            "isPro": false,
            "fullname": "QiuchenWang",
            "user": "autumncc",
            "type": "user"
          },
          "name": "Qiuchen Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:19.039Z",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc2",
          "name": "Ruixue Ding",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc3",
          "user": {
            "_id": "665d652e0f35c005de892108",
            "avatarUrl": "/avatars/240bebdc7fdc6d50719c65de0e3cf1cd.svg",
            "isPro": false,
            "fullname": "Yu Zeng",
            "user": "YuZeng260",
            "type": "user"
          },
          "name": "Yu Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:42.906Z",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc4",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc5",
          "user": {
            "_id": "64b02ec0e5000ae8a572ced5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
            "isPro": false,
            "fullname": "Lin Chen",
            "user": "Lin-Chen",
            "type": "user"
          },
          "name": "Lin Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:44.983Z",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc6",
          "name": "Shihang Wang",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc7",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc8",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc9",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T06:30:51.000Z",
      "submittedOnDailyAt": "2025-05-29T03:46:05.539Z",
      "title": "VRAG-RL: 시각적 정보 이해를 강화하는 시각 기반의 RAG를 강화 학습에 의한 반복적 추론을 통해 강화하는 방법",
      "submittedOnDailyBy": {
        "_id": "64b02ec0e5000ae8a572ced5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
        "isPro": false,
        "fullname": "Lin Chen",
        "user": "Lin-Chen",
        "type": "user"
      },
      "summary": "광범위한 이미지付き 정보의 효과적인 검색, 논리, 이해는 RAG手法의 난제입니다. 전통적인 텍스트 기반의 방법은 이미지 관련 정보 처리가 불가능합니다. 반면, 현재의 이미지 기반의 RAG 접근法是 고정 프로이프 라인에 제한되어 있으며, 모델의 기본적인 능력의 부족으로 논리를 효과적으로 수행하는 것이 어려워집니다. RL이 모델의 논리에 벤이시컬한 영향을 미치는 것을 증명하였으므로, 우리는 복잡한 논리를 수행하기 위한 새로운 RL 프레임워크 VRAG-RL을 소개합니다. 이 프레임워크를 사용하면 VLMs는 검색 엔진과 상호작용하며, 이미지 인식 토큰을 도움을 받아 한번의 논리 타레이트 또는 여러 번의 논리 타레이트를 자동으로 샘플링하고, 이 샘플에 기반하여 지속적으로 최적화됩니다. 우리의 접근법은 RAG 분야에서 RL의 주요 한계점을 특징적으로 나타냅니다: (i) 기존의 다 모델 RAG 접근법은 이미지만 컨텍스트에 포함하여 논리 토큰의 할당이 부족하고, 이미지 고유의 인식을 뛰어넘습니다; (ii) 모델이 검색 엔진과 상호작용하면, 그 요청을 명확히 표현할 수 없기 때문에 관련 정보를 검색하지 못하여 최적의 성능을 얻을 수 없습니다. 이러한 도전을 해결하기 위해, 이미지付き 입력에 타일맵을 적용한 행동 공간을 정의하고, 절삭과 스케일링 등 행동을 포함하여 모델이 코어 스트라이프에서 핀 스트라이프의 시각에서 정보를 수집할 수 있습니다. 또한, 사용자의 원의 질문과 검색 모델 사이에 간격을 메우기 위해, 간단하고 효과적인 보상을 사용하며,クエリ의 재작성과 검색 성능을 모델 기반의 보상과 통합합니다. VRAG-RL은 특별히 설계된 RL 전략을 사용하여 RAG 태스크에 최적화되고, 모델이 실제 세계의 애플리케이션에 적응하는 것을 목표로 합니다. 코드는 https://github.com/Alibaba-NLP/VRAG{https://github.com/Alibaba-NLP/VRAG}에 제공됩니다.",
      "upvotes": 8,
      "discussionId": "6837ed297d00cf0a04677bf5",
      "githubRepo": "https://github.com/Alibaba-NLP/VRAG",
      "ai_summary": "VRAG-RL, a reinforcement learning framework, enhances reasoning and visual information handling in RAG methods by integrating visual perception tokens and employing specialized action spaces and rewards.",
      "ai_keywords": [
        "reinforcement learning",
        "VRAG-RL",
        "VLMs",
        "search engines",
        "visually rich information",
        "reasoning trajectories",
        "visual perception tokens",
        "action space",
        "query rewriting",
        "retrieval performance",
        "model-based reward"
      ]
    },
    "publishedAt": "2025-05-28T02:30:51.000Z",
    "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich\n  Information Understanding via Iterative Reasoning with Reinforcement Learning",
    "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\nhttps://github.com/Alibaba-NLP/VRAG{https://github.com/Alibaba-NLP/VRAG}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22019.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64b02ec0e5000ae8a572ced5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
      "fullname": "Lin Chen",
      "name": "Lin-Chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 89
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22232",
      "authors": [
        {
          "_id": "683815574d9866c160e88670",
          "name": "Mehdi Ali",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88671",
          "user": {
            "_id": "62fa1d95e8c9c532aa75331c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
            "isPro": false,
            "fullname": "Manuel Brack",
            "user": "mbrack",
            "type": "user"
          },
          "name": "Manuel Brack",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:14.826Z",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88672",
          "name": "Max Lübbering",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88673",
          "name": "Elias Wendt",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88674",
          "name": "Abbas Goher Khan",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88675",
          "name": "Richard Rutmann",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88676",
          "name": "Alex Jude",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88677",
          "name": "Maurice Kraus",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88678",
          "name": "Alexander Arno Weber",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88679",
          "name": "Felix Stollenwerk",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867a",
          "name": "David Kaczér",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867b",
          "name": "Florian Mai",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867c",
          "name": "Lucie Flek",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867d",
          "name": "Rafet Sifa",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867e",
          "name": "Nicolas Flores-Herr",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867f",
          "name": "Joachim Köhler",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88680",
          "name": "Patrick Schramowski",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88681",
          "name": "Michael Fromm",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88682",
          "name": "Kristian Kersting",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T11:06:54.000Z",
      "submittedOnDailyAt": "2025-05-29T07:38:07.888Z",
      "title": "언어 품질의 다언어 접근: 준비 학습을 위한 다언어 접근\n언어 모델을 이용한 데이터 필터링",
      "submittedOnDailyBy": {
        "_id": "62fa1d95e8c9c532aa75331c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
        "isPro": false,
        "fullname": "Manuel Brack",
        "user": "mbrack",
        "type": "user"
      },
      "summary": "고품질의 다언어 훈련 데이터는 대규모 언어 모델(LLMs)의 효과적인 사전 훈련에 필수적이다. 그러나 적절한 오픈 소스의 다언어 데이터 세트의 이용 가능성은 제한되어 있다. 현재의 최尖端 데이터 세트는 주로 휴리스틱 필터링 메소드를 기반으로 하여, 언어 간 변환성 및 scalability가 제한되어 있다. 여기서 우리는 JQL라는 체계적인 접근 방식을 소개한다. 이 방법은 규모적으로 다언어 데이터를 효율적으로 다양성과 고품질로 카레로, 계산 부담을 크게 줄이는 것을 목표로 한다. JQL은 LLMs의 노트 능력에 기초한 가벼운 라벨링 모델에 엣지어너티즈를 적용한다. 이러한 모델은 훈련 중未见 언어나 문자 체계에서도 강력한 다언어 및 언어 간 성능을 보여주며, 35언어 확장하여 실험적으로 평가한 결과, 현재의 휴리스틱 필터링 메소드와 같은 Fineweb2를 크게 초월한다. JQL은 하류 모델의 훈련 품질을 향상시키고, 데이터의 저장율을 높일 것을 특히 강조한다. 우리의 연구는 실용적인 인젼과 유익한 리소스를 제공하고, 다언어 데이터 카레로의 표준을 높이는 것을 목표로 한다.",
      "upvotes": 7,
      "discussionId": "683815594d9866c160e88708",
      "projectPage": "https://huggingface.co/spaces/Jackal-AI/JQL",
      "githubRepo": "https://github.com/JQL-AI/JQL-Annotation-Pipeline",
      "ai_summary": "JQL systematically curates high-quality multilingual training data using pretrained multilingual embeddings, outperforming heuristic methods and improving downstream model training across diverse languages.",
      "ai_keywords": [
        "pretraining",
        "large language models",
        "multilingual datasets",
        "heuristic filtering methods",
        "JQL",
        "lightweight annotators",
        "multilingual embeddings",
        "cross-lingual transferability",
        "annotation pipeline",
        "data retention rates",
        "multilingual data curation"
      ]
    },
    "publishedAt": "2025-05-28T07:06:54.000Z",
    "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining\n  Data Filtering with Language Models",
    "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22232.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fa1d95e8c9c532aa75331c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
      "fullname": "Manuel Brack",
      "name": "mbrack",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22613",
      "authors": [
        {
          "_id": "6837d79d4d9866c160d8f43b",
          "name": "Yuchi Wang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43c",
          "name": "Yishuo Cai",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43d",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43e",
          "name": "Sihan Yang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43f",
          "name": "Linli Yao",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f440",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f441",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f442",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f443",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:29:34.000Z",
      "submittedOnDailyAt": "2025-05-29T02:20:00.209Z",
      "title": "리코: 이미지 재캡처의 정확성과 완전성을 향상시키기 위해 시각적 재구성을 통해",
      "submittedOnDailyBy": {
        "_id": "622842e296588dd1a2594746",
        "avatarUrl": "/avatars/b96d0b49e4cff25d83fefc67b7cd1076.svg",
        "isPro": false,
        "fullname": "wangyuchi",
        "user": "YuchiWang",
        "type": "user"
      },
      "summary": "그림 리카피팅은 다양한 모델 태스크에 대해 질을 향상시키기 위해 광범위하게 사용되고 있습니다. 현재의 리카피팅 방법은 일반적으로 강력한 다 모델 대 언어 모델(MLLM)을 기반으로 문맥을 향상시키기 위해 의존하지만, 결함이 없는 미세한 세부 정보가 부족하여 환상과 불완전성으로 인한 부정확성을 보입니다. 이러한 제한을 해결하기 위해 우리는 그림 재구성을 기반으로 한 리카피팅을 개선하는 새로운 프레임워크를 제안하고 있습니다. 특히, 문맥을 이미지로 재구성하기 위한 문맥 모델을 활용하여 원본 이미지와 재구성 이미지 사이의 차이를 MLLM으로 인식하여 리카피팅을 개선하는 것을 목표로 합니다. 이 프로세스는 반복적으로 수행되며, 충실하고 세부적인 설명을 생성하는 데 도움이 됩니다. 추가 계산 비용의 감소를 위해, DPO를 사용하여 리카포치처럼 캡션을 생성하는 것을 학습시키는 리카포치-Flash를 도입하고 있습니다. 확장된 실험은 우리의 접근법이 리카피팅의 정확성과 세부성에서 큰 향상을 초래하고, CapsBench과 CompreCap 모두에서 기준 모델보다 약 10% 이상의 성능을 향상시키는 것을 보여줍니다. 코드는 다음과 같은 URL에서 공개되어 있습니다.\nhttps://github.com/wangyuchi369/RICO",
      "upvotes": 5,
      "discussionId": "6837d79e4d9866c160d8f471",
      "githubRepo": "https://github.com/wangyuchi369/RICO",
      "ai_summary": "A novel iterative framework, RICO, improves image caption accuracy by using visual reconstruction and a text-to-image model to refine discrepancies, while RICO-Flash enhances efficiency using DPO.",
      "ai_keywords": [
        "multimodal large language models",
        "text-to-image model",
        "DPO",
        "CapsBench",
        "CompreCap"
      ]
    },
    "publishedAt": "2025-05-28T13:29:34.000Z",
    "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via\n  Visual Reconstruction",
    "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22613.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622842e296588dd1a2594746",
      "avatarUrl": "/avatars/b96d0b49e4cff25d83fefc67b7cd1076.svg",
      "fullname": "wangyuchi",
      "name": "YuchiWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22525",
      "authors": [
        {
          "_id": "6837da438f680552f7b86b28",
          "user": {
            "_id": "64bb5f9d8e051085bace4d1e",
            "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
            "isPro": true,
            "fullname": "Ethan Chern",
            "user": "ethanchern",
            "type": "user"
          },
          "name": "Ethan Chern",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:05.613Z",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b29",
          "name": "Zhulin Hu",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2a",
          "name": "Steffi Chern",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2b",
          "name": "Siqi Kou",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2c",
          "name": "Jiadi Su",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2d",
          "name": "Yan Ma",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2e",
          "name": "Zhijie Deng",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2f",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T16:12:45.000Z",
      "submittedOnDailyAt": "2025-05-29T02:24:51.876Z",
      "title": "화상 생성을 활용하여 생각하기",
      "submittedOnDailyBy": {
        "_id": "64bb5f9d8e051085bace4d1e",
        "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
        "isPro": true,
        "fullname": "Ethan Chern",
        "user": "ethanchern",
        "type": "user"
      },
      "summary": "우리는 \"생성된 이미지와 함께 생각하기\"라는 새로운 패러다임에 대해 소개합니다. 이 패러다임은 대형 다중모달 모델(LMMs)이 시각적 논리와 상호작용하는 방식에 근본적인 변화를 일으키는 것입니다. 이는 모델이 텍스트와 시각적 모달리티를 자연스럽게 결합하여 중간 시각적 사고 단계를 spontaneouly 생성함으로써 이루어집니다. 현재 LMMs의 시각적 논리는 사용자 제공된 고정된 이미지 처리 또는 텍스트 기반의 chain-of-thought(CoT)으로만 이루어져 있습니다. \"생성된 이미지와 함께 생각하기\"는 모델이 중간 시각적 생각, 자신의 시각적 가설을 평가하고 이를 자신의 논리 과정의 핵심 요소로 개선하는 새로운 인지 능력의 차원을 열 수 있습니다. 우리는 이 접근법의 효과성을 두 가지 보완된 메커니즘을 통해 입증합니다: (1) 중간 시각적 서브목표에 기반한 시각적 생성, 복잡한 시각적 작업을 처리할 수 있는 관리 가능한 구성 요소로 분해하고 점차 생성 및 통합하는 것, (2) 자기 평가에 기반한 시각적 생성, 초기 시각적 가설을 생성하고 텍스트 기반의 논리로 자신의 부족점을 분석하고 자신의 평가에 따라 개선된 출력을 생성하는 것. 시각적 생성 벤치마크에 대한 실험 결과, baseline 방법 대비 상당한 개선이 나타났으며, 복잡한 다물체 시나리오를 처리하는 데 50%의 상대적인 향상(38%에서 57%)을 달성했습니다. 생물화학가의 새로운 단백질 구조 탐색, 건축가의 공간 디자인 반복, 수사분석사의 범죄 현장 재구성, 그리고 농구 선수의 전략적 플레이 상상, 우리의 접근법은 인간적인 창의적, 분석적, 전략적 사고를 특징으로 하는 시각적 상상과 반복적 개선을 통해 인공지능 모델이 참여할 수 있게 합니다. https://github.com/GAIR-NLP/thinking-with-generated-images에서 오픈 소스 패키지를 제공합니다.",
      "upvotes": 5,
      "discussionId": "6837da468f680552f7b86bb2",
      "githubRepo": "https://github.com/GAIR-NLP/thinking-with-generated-images",
      "ai_summary": "Thinking with Generated Images allows large multimodal models to generate and critique intermediate visual steps, enhancing visual reasoning capabilities and achieving significant improvements in complex scenarios.",
      "ai_keywords": [
        "LMMs",
        "visual reasoning",
        "chain-of-thought",
        "vision generation",
        "intermediate visual subgoals",
        "self-critique",
        "multis-object scenarios",
        "biochemists",
        "architects",
        "forensic analysts",
        "basketball players"
      ]
    },
    "publishedAt": "2025-05-28T12:12:45.000Z",
    "title": "Thinking with Generated Images",
    "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22525.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64bb5f9d8e051085bace4d1e",
      "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
      "fullname": "Ethan Chern",
      "name": "ethanchern",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22523",
      "authors": [
        {
          "_id": "6837c1cb80fc90ca2d9e8153",
          "name": "Junwen Chen",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8154",
          "name": "Heyang Jiang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8155",
          "name": "Yanbin Wang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8156",
          "name": "Keming Wu",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8157",
          "name": "Ji Li",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8158",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8159",
          "name": "Keiji Yanai",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e815a",
          "name": "Dong Chen",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e815b",
          "name": "Yuhui Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T16:09:33.000Z",
      "submittedOnDailyAt": "2025-05-29T00:40:52.849Z",
      "title": "PrismLayers: PrismLayers는 고품질의 다층 투명 이미지의 오픈 데이터로 기반한 생성 모델입니다.",
      "submittedOnDailyBy": {
        "_id": "631f108bb45367a05fe74260",
        "avatarUrl": "/avatars/c20da6800b643728062712d9a2771648.svg",
        "isPro": false,
        "fullname": "Researcher",
        "user": "YuanYuhui",
        "type": "user"
      },
      "summary": "生成 고품질의 다층 투명 이미지는 텍스트 힌트에서 생성할 수 있으며, 이는 새로운 창의적 제어 수준을 열고 사용자가 각 층을 LLM의 텍스트 출력과 같이 쉽게 편집할 수 있게 해줍니다. 그러나, 대형 고품질의 다층 투명 데이터셋이 부족하여, 다층 생성 모델의 발전은 전통적인 텍스트에서 이미지 모델보다 뒤덮여 있습니다. 본 논문에서는 이 기본적인 도전을 해결하기 위해 다음과 같이 진행했습니다: (i) 첫 번째 공개的开放的、초고정밀도 PrismLayers（PrismLayersPro） 데이터셋을 발표하여, 200K（20K） 다층 투명 이미지와 정확한 alpha mattes를 포함하고 있습니다, (ii) 훈련없이 합성된 파이프라인을 도입하여, 现成的 확산 모델을 사용하여 이러한 데이터를 필요에 따라 생성합니다, (iii) 강력한 오픈 소스의 다층 생성 모델 ART+를 제공하여, 현대적인 텍스트에서 이미지 생성 모델과 일치하는 미학을 갖습니다. 핵심 기술 기여는 다음과 같습니다: LayerFLUX는 고품질의 단일 투명 이미지와 정확한 alpha mattes를 생성하는 데 능숙하며, MultiLayerFLUX는 인간 레이블링된 세ман틱 배치를 기반으로 LayerFLUX의 출력을 조합하여 완전한 이미지를 생성합니다. 더 높은 품질을 보장하기 위해, 我们는 앨리리즘을 제거하고 의미적 불일치를 제거하는 엄격한 필터링 단계를 적용하고, 그 후는 인공 선택을 진행합니다. 합성된 PrismLayersPro에 가장 先进的 ART 모델을 미세 조정하여, ART+를 얻으며, 60%의 머리대머리 사용자 연구 비교에서 원래의 ART를 능가하고, FLUX.1-[dev] 모델이 생성한 이미지의 시각적 품질을 매칭합니다. 우리는 우리의 작업이 다층 투명 이미지 생성 작업에 대한 강력한 데이터셋 기반을 구축하고, 정확한, 편집 가능한, 시각적으로 흥미로운 분층 이미지에 필요한 연구와 응용을 촉진할 것으로 예상합니다.",
      "upvotes": 4,
      "discussionId": "6837c1d180fc90ca2d9e82bc",
      "ai_summary": "The work introduces a dataset and model for generating high-quality, multi-layer transparent images using diffusion models and a novel synthesis pipeline.",
      "ai_keywords": [
        "PrismLayers",
        "diffusion models",
        "LayerFLUX",
        "MultiLayerFLUX",
        "alpha mattes",
        "semantic layout",
        "user study",
        "ART model",
        "FLUX.1-[dev]"
      ]
    },
    "publishedAt": "2025-05-28T12:09:33.000Z",
    "title": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image\n  Generative Models",
    "summary": "Generating high-quality, multi-layer transparent images from text prompts can\nunlock a new level of creative control, allowing users to edit each layer as\neffortlessly as editing text outputs from LLMs. However, the development of\nmulti-layer generative models lags behind that of conventional text-to-image\nmodels due to the absence of a large, high-quality corpus of multi-layer\ntransparent data. In this paper, we address this fundamental challenge by: (i)\nreleasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)\ndataset of 200K (20K) multilayer transparent images with accurate alpha mattes,\n(ii) introducing a trainingfree synthesis pipeline that generates such data on\ndemand using off-the-shelf diffusion models, and (iii) delivering a strong,\nopen-source multi-layer generation model, ART+, which matches the aesthetics of\nmodern text-to-image generation models. The key technical contributions\ninclude: LayerFLUX, which excels at generating high-quality single transparent\nlayers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple\nLayerFLUX outputs into complete images, guided by human-annotated semantic\nlayout. To ensure higher quality, we apply a rigorous filtering stage to remove\nartifacts and semantic mismatches, followed by human selection. Fine-tuning the\nstate-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which\noutperforms the original ART in 60% of head-to-head user study comparisons and\neven matches the visual quality of images generated by the FLUX.1-[dev] model.\nWe anticipate that our work will establish a solid dataset foundation for the\nmulti-layer transparent image generation task, enabling research and\napplications that require precise, editable, and visually compelling layered\nimagery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22523.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631f108bb45367a05fe74260",
      "avatarUrl": "/avatars/c20da6800b643728062712d9a2771648.svg",
      "fullname": "Researcher",
      "name": "YuanYuhui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22338",
      "authors": [
        {
          "_id": "6837c79576eac3fa930de19b",
          "name": "Hanyang Wang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19c",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19d",
          "name": "Chaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19e",
          "name": "Tianjun Mao",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19f",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a0",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a1",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a2",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T13:23:49.000Z",
      "submittedOnDailyAt": "2025-05-29T01:04:31.053Z",
      "title": "Text2Grad: Native Language의 Feedback로부터의 강화학습",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "전통적인 RLHF는 단순한 스칼라 레벨의 보상을 사용하여 언어 모델을 최적화하고, 성공 또는 실패의 구체적인 이유를 숨기고, 학습을 徐缓하게 진행하지 않는다. 최근의 연구에서는, Prompt 및 Reflection에 의한 문헌적 평가를 RL에 추가하여 해석성을 향상시키지만, 모델 파라미터를 변경하지 않는다. 우리는 Text2Grad라는 언어적 피드백을 스팬 레벨의 경사를 변환하는 RL 패러다임에 의해, 모델의 정책의 오류 부분을 직접 개선하는 방법을 제안한다. 이로써, 피드백에 의한 조정이 정확하며, 글로벌적인 노드가 사라진다. Text2Grad는 (1) 고품질의 피드백 주석 파이프라인, (2) 세부적인 보상 모델, (3) 스팬 레벨의 정책 최적화기의 3가지 구성 요소로 이루어진다. 요약, 코드 생성, 문제 해결의 각 분야에서, Text2Grad는 스칼라 레벨의 보상의 RL과 Prompt만 기준인 경우, 더 높은 태스크 메트릭과 구체적인 해석성을 제공한다. 우리의 결과를 통해, 자연어의 피드백을 경사에 변환한 경우, 구체적인 정책 최적화의 강력한 신호로 작용하는 힘을 보여준다. 우리의 방법의 코드는 https://github.com/microsoft/Text2Grad에 제공된다.",
      "upvotes": 4,
      "discussionId": "6837c79576eac3fa930de1dd",
      "ai_summary": "Text2Grad converts human textual feedback into span-level gradients to optimize language models precisely and efficiently.",
      "ai_keywords": [
        "RLHF",
        "reinforcement-learning",
        "free-form textual feedback",
        "span-level gradients",
        "token spans",
        "differentiable reward signals",
        "gradient updates",
        "span-level policy optimizer",
        "fine-grained reward model",
        "feedback-annotation pipeline"
      ]
    },
    "publishedAt": "2025-05-28T09:23:49.000Z",
    "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback",
    "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22203",
      "authors": [
        {
          "_id": "6837dcc41448b8bf0c91fa30",
          "user": {
            "_id": "6462def82a83863b97c0611e",
            "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
            "isPro": false,
            "fullname": "Yuzhen Huang",
            "user": "yuzhen17",
            "type": "user"
          },
          "name": "Yuzhen Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:53.737Z",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa31",
          "name": "Weihao Zeng",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa32",
          "name": "Xingshan Zeng",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa33",
          "name": "Qi Zhu",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa34",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T10:28:41.000Z",
      "submittedOnDailyAt": "2025-05-29T02:37:12.694Z",
      "title": "규칙이나 모델 기반의 검증 데이터의 문제 -- 수학적 추론에서 사례 연구",
      "submittedOnDailyBy": {
        "_id": "6462def82a83863b97c0611e",
        "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
        "isPro": false,
        "fullname": "Yuzhen Huang",
        "user": "yuzhen17",
        "type": "user"
      },
      "summary": "신뢰도가 높은 검증기(validator)는 보상을 증명할 수 있는 강화학습(Reinforcement Learning, RL)의 성공에 필수적이다. 이는 DeepSeek-R1과 같은 규모의 논리 모델의 핵심적인 방법 중 하나다. 수학적인 논리 모델의 복잡한 영역에서, 이전의 연구에서는 기준을 기반으로 한 검증기가 강력한 논리 모델의 훈련에 광범위하게 사용되어 왔다. 그러나 이러한 검증기의 신뢰도와 강화학습 훈련 과정에 미치는 영향은 이해되지 않았습니다. 본 연구에서는 수학적인 논리 모델을 사례 연구로 취급하고, 동적 평가와 강화학습 훈련의 두 가지 측면에서 다양한 검증기를 상세하게 분석합니다. 먼저, 현재의 오픈 소스의 기준 검증기는 여러 일반적인 수학 데이터 세트에서 다른 형식으로 동일한 답변을 인식하려고 하는 것이 어려워, 시각성 높은 검증기가 발생합니다. 이 제한은 RL 훈련의 성능에 부정적인 영향을 미치고, 정책 모델이 강하게 되면 이 영향이 명확히 드러납니다. 다음으로, 모델 버전 검증기를 잠재적인 해결책으로 검토합니다. 동적 평가에서 모델 버전 검증기는 상당한 인식 정확도를 달성하지만, 진화 분석과 RL 훈련의 결과에서, 특정 패턴을 잘못 분류하는 것을 매우 취약하게 나타냅니다. 이 취약성은 정책 모델의 최적화에서 보상을 人工적으로 상승시키게 됩니다. 우리의 발견은 기준 검증기와 모델 버전 검증기의 고유한 위험에 초점을 맞추고, 강화학습의 보상 시스템 개발에 유익한 팁을 제공하기 위해 목적입니다.",
      "upvotes": 4,
      "discussionId": "6837dcc51448b8bf0c91fa54",
      "ai_summary": "The study examines the effectiveness and reliability of rule-based and model-based verifiers in reinforcement learning with verifiable reward, highlighting limitations and vulnerabilities in their use for mathematical reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable reward",
        "DeepSeek-R1",
        "mathematical reasoning",
        "rule-based verifiers",
        "reward systems",
        "model-based verifiers",
        "false negatives",
        "false positives"
      ]
    },
    "publishedAt": "2025-05-28T06:28:41.000Z",
    "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on\n  Mathematical Reasoning",
    "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22203.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6462def82a83863b97c0611e",
      "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
      "fullname": "Yuzhen Huang",
      "name": "yuzhen17",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22202",
      "authors": [
        {
          "_id": "6837eff312d1f7a138bd09b3",
          "name": "Hyeonbin Hwang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b4",
          "name": "Byeongguk Jeon",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b5",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b6",
          "name": "Jiyeon Kim",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b7",
          "name": "Hoyeon Chang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b8",
          "name": "Sohee Yang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b9",
          "name": "Seungpil Won",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09ba",
          "name": "Dohaeng Lee",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09bb",
          "name": "Youbin Ahn",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09bc",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T10:28:35.000Z",
      "submittedOnDailyAt": "2025-05-29T04:00:37.847Z",
      "title": "예, 문장을 한 문장씩 예측하여 번역하겠습니다.\n\n이 문장을 반환해 주세요.",
      "submittedOnDailyBy": {
        "_id": "647eaaf61a1fcad2fdc5d1ef",
        "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
        "isPro": false,
        "fullname": "Hyeonbin Hwang ",
        "user": "hbin0701",
        "type": "user"
      },
      "summary": "自動 회귀 언어 모델(LM)은 토큰 하나씩 생성하지만, 인간의 논리론은 문장,命题, 그리고 개념의 더 높은 수준의 추상화를 사용합니다. 이 대비는 핵심적인 문제로 LM이 구조화된 의미 단위로 논리론을 수행할 수 있는지를 묻습니다. 본 연구에서는 학습된 표현을 기반으로, 이러한 추상적인 논리론 공간에 맵핑할 수 있는지를 조사합니다. 여기서는, 다음 문장을 예측할 때 연속적인 매핑을 자동 회귀적으로 예측하여 문장 공간에서 동작하는 예측된 토큰 수준의 LM을 적용하는 프레임워크를 제안합니다. 고전적인 표현 학습에 힌트를 얻은 두 가지 매핑 패러다임에 대해 조사합니다: 1) 세ман틱 매핑, 표면 의미의 보존을 위해 자동 인코딩을 사용한 것; 2) 맥락 매핑, 다음 문장 예측에 의해 예측된 구조를 인코딩하는 것. 두 가지 추론 모드로 평가합니다: 구분화된 모드에서는 예측된 매핑을 문자열로 해석하여 재 인코딩합니다; 연속 모드에서는 매핑 공간에서 완전한 논리론을 수행하며 효율적인 효과를 얻습니다. 수학, 로직, 일반 지식, 계획의 4개 분야에서, 연속 추론의 맥락 매핑은 Chain-of-Thought(CoT)와 경쟁적인 성능을示し, 평균적으로 추론 시간의 FLOP을 절반으로 줄입니다. 또한, scalability와 모듈화 적응의 초기 징후를 나타냅니다. 마지막으로, 잠재적인 트래픽을 시각화하기 위해 SentenceLens라는 진단 도구를 도입하여 중간의 모델 상태를 해석 가능한 문장으로 해석합니다. 이러한 결과를 통해, 학습된 LM은 잠재적인 매핑 공간 내의 추상적이고 구조화된 논리론으로 효과적으로 변환할 수 있음을 입증합니다.",
      "upvotes": 4,
      "discussionId": "6837eff412d1f7a138bd0a3e",
      "ai_summary": "Pretrained language models can adapt to operate in sentence space, reason over structured semantic units, and achieve competitive performance with Chain-of-Thought while reducing inference-time FLOPs.",
      "ai_keywords": [
        "autoregressive language models",
        "semantic embeddings",
        "contextual embeddings",
        "next-sentence prediction",
        "Chain-of-Thought",
        "SentenceLens"
      ]
    },
    "publishedAt": "2025-05-28T06:28:35.000Z",
    "title": "Let's Predict Sentence by Sentence",
    "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22202.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647eaaf61a1fcad2fdc5d1ef",
      "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
      "fullname": "Hyeonbin Hwang ",
      "name": "hbin0701",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21876",
      "authors": [
        {
          "_id": "6837d80bf42b2aacfc26c460",
          "name": "Zun Wang",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c461",
          "name": "Jaemin Cho",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c462",
          "name": "Jialu Li",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c463",
          "name": "Han Lin",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c464",
          "user": {
            "_id": "652066649004117947e46ed6",
            "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg",
            "isPro": false,
            "fullname": "Jaehong Yoon",
            "user": "jaehong31",
            "type": "user"
          },
          "name": "Jaehong Yoon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:07.590Z",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c465",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c466",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T01:45:26.000Z",
      "submittedOnDailyAt": "2025-05-29T02:15:48.194Z",
      "title": "EPiC: 적절한 재연 비디오 가이드라인을 활용한 효율적인 비디오 카메라 제어 학습",
      "submittedOnDailyBy": {
        "_id": "5ffe32d8942cf3533d364449",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
        "isPro": false,
        "fullname": "Jaemin Cho",
        "user": "j-min",
        "type": "user"
      },
      "summary": "최근의 3D 카메라 제어 기법은 비디오 디피유전 모델(VDM)에서, 注記된 카메라 트래지 리리리에 따라 추정된 점 군으로부터 렌더링 된 어ン카ー비디오를 생성하고, 구조화된 선두로 디피유전 모델을 가이드하는 경우가 많습니다. 그러나 점 군 추정에서 고유의 오류는 어ン카ー비디오의 부정확성을招く. 또한, 상세한 카메라 트래지 리리리의 注記의 필요성은 자원의 요구를 증가시킵니다. 이러한 제한을 해결하기 위해, 우리는 비용 높은 카메라 트래지 리리리의 注記를 필요로 하지 않고, 고품질의 어ン카ー비디오를 자동적으로 구축하는 효율적이고 정밀한 카메라 제어 학습 프레임워크 EPiC를 소개합니다. 구체적으로는, 비디오의 첫 번째 프레임의 시각성에 기반하여 소스 비디오를 마스크하는 방법으로, 높은 어ン카ー비디오를 생성합니다. 이 접근법은 높은 어라인먼트를 보장하고, 카메라 트래지 리리리의 注記이 필요하지 않아, 이미지에서 비디오(I2V)의 트레이닝 페어를 생성할 수 있습니다. 또한, 우리는 가벼운 조건부 모듈 Anchor-ControlNet을 소개합니다. 이는 사전 학습된 VDM에 어ン카ー비디오 가이드를 통합하기 위해, 주干 모델 파라미터의 1% 이하를 사용합니다. 이 프레임워크는 일반적인 디피유전 모델의 주干에 변경을 추가할 필요없이, 렌더링의 부정확성을 줄이기 위해 필요한 파라미터, 트레이닝 단계, 데이터의 사용량을 크게 줄이는 것으로, 효율적인 트레이닝을 실현합니다. 따라서, EPiC는 I2V 카메라 제어 태스크에서 RealEstate10K과 MiraData에서 가장 선진적인 성능을 달성하고, 정량적이고 정성적으로 정밀하고 강건한 카메라 제어 능력을 나타냅니다. 특히, EPiC는 추론 시 점 군을 사용하여 생성된 어ン카ー비디오에 대해도 강건하게 일반화하고, 정밀한 3D 정보를 기반으로하는 카메라 제어를 가능하게 하며, 또한 비디오에서 비디오의 시나리오에서도 강력한 0 시드 일반화를 나타냅니다.",
      "upvotes": 4,
      "discussionId": "6837d810f42b2aacfc26c5ec",
      "projectPage": "https://zunwang1.github.io/Epic",
      "githubRepo": "https://github.com/wz0919/EPiC",
      "ai_summary": "EPiC is a framework for efficient 3D camera control in video diffusion models that constructs high-quality anchor videos through first-frame visibility masking, integrates them using a lightweight ControlNet module, and achieves state-of-the-art performance on I2V tasks with minimal resources.",
      "ai_keywords": [
        "anchor videos",
        "point cloud estimation",
        "camera trajectories",
        "diffusion models",
        "first-frame visibility",
        "EPiC",
        "ControlNet",
        "I2V training pairs",
        "rendering misalignments",
        "RealEstate10K",
        "MiraData"
      ]
    },
    "publishedAt": "2025-05-27T21:45:26.000Z",
    "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video\n  Guidance",
    "summary": "Recent approaches on 3D camera control in video diffusion models (VDMs) often\ncreate anchor videos to guide diffusion models as a structured prior by\nrendering from estimated point clouds following annotated camera trajectories.\nHowever, errors inherent in point cloud estimation often lead to inaccurate\nanchor videos. Moreover, the requirement for extensive camera trajectory\nannotations further increases resource demands. To address these limitations,\nwe introduce EPiC, an efficient and precise camera control learning framework\nthat automatically constructs high-quality anchor videos without expensive\ncamera trajectory annotations. Concretely, we create highly precise anchor\nvideos for training by masking source videos based on first-frame visibility.\nThis approach ensures high alignment, eliminates the need for camera trajectory\nannotations, and thus can be readily applied to any in-the-wild video to\ngenerate image-to-video (I2V) training pairs. Furthermore, we introduce\nAnchor-ControlNet, a lightweight conditioning module that integrates anchor\nvideo guidance in visible regions to pretrained VDMs, with less than 1% of\nbackbone model parameters. By combining the proposed anchor video data and\nControlNet module, EPiC achieves efficient training with substantially fewer\nparameters, training steps, and less data, without requiring modifications to\nthe diffusion model backbone typically needed to mitigate rendering\nmisalignments. Although being trained on masking-based anchor videos, our\nmethod generalizes robustly to anchor videos made with point clouds during\ninference, enabling precise 3D-informed camera control. EPiC achieves SOTA\nperformance on RealEstate10K and MiraData for I2V camera control task,\ndemonstrating precise and robust camera control ability both quantitatively and\nqualitatively. Notably, EPiC also exhibits strong zero-shot generalization to\nvideo-to-video scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21876.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5ffe32d8942cf3533d364449",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
      "fullname": "Jaemin Cho",
      "name": "j-min",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18700",
      "authors": [
        {
          "_id": "6837cc29bbee677da741aba7",
          "name": "Chun Wang",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741aba8",
          "name": "Xiaoran Pan",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741aba9",
          "name": "Zihao Pan",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741abaa",
          "name": "Haofan Wang",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741abab",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T13:48:57.000Z",
      "submittedOnDailyAt": "2025-05-29T01:25:51.194Z",
      "title": "GRE Suite: 지역위치 추정에 의한 지리적 추론 인핏션을 수행하기 위한 시각 언어 모델의 미세 조정과 강화된 논리 체인",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "최근의 시각적 라디오 언어 모델(VLMs)의 발전은 시각적 논리 임무에서 뛰어난 성능을 보여주고 있습니다. 그러나, 위치 인식은 특징적인 문제들을 가지고 있습니다. 이미지에서 다양한 시각적 정보를 추출하고, 외부의 세계 지식과 통합하여 체계적인 논리를 수행하는 것이 필요합니다. 현재의 위치 인식 임무의 접근 방식은 강력한 논리 구조와 설명성을 부족합니다. 이러한 제한을 해결하기 위해, 새로운 프레임워크를 제안하여 시각적 라디오 언어 모델을 구조화된 논리 키chian으로 확장합니다. 이는 정확한 해석 가능한 위치 추론을 실현하기 위해 VLMs를 강화합니다. GRE 시트는 데이터셋, 모델, 벤치마크의 3가지 주요 요소로 체계적으로 개발되었습니다. 먼저, GRE30K를 소개합니다. 이는 시각적과 맥락의 세부 분석을 촉진하기 위해 설계된 고품질의 위치 인식 논리 데이터셋입니다. 다음으로, GRE 모델을 소개합니다. 이는 여러 단계의 논리 전략을 사용하여, 스케인의 속성, 위치의 세부 사항, 세ман틱의 특징을 발전적으로 추론하고, 높은 정확도를 가진 잠재적인 위치 영역을 특정함으로써, 위치 인식의 정확도를 향상시킵니다. 마지막으로, Geo Reason Evaluation Benchmark(GREval-Bench)을 구축합니다. 이는 VLMs가 도시, 자연, 라ン마크의 다양한 스케인에서 위치 인식 성능을 평가하기 위한 세부적인 평가 프레임워크입니다. 실험 결과는, GRE가 모든 위치 인식 임무의 각도별로 현재의 방법보다 크게 뛰어넘는 것을 보여줍니다. 복잡한 위치 추론에서 설명 가능한 VLMs의 효과를 강조합니다. 코드와 데이터는 https://github.com/Thorin215/GRE에서 공개됩니다.",
      "upvotes": 3,
      "discussionId": "6837cc2abbee677da741abf5",
      "ai_summary": "The GRE Suite enhances Visual Language Models with structured reasoning chains, improving geo-localization tasks through a multi-stage strategy and comprehensive evaluation benchmark.",
      "ai_keywords": [
        "Visual Language Models",
        "geo-localization",
        "reasoning chains",
        "GRE30K",
        "GRE model",
        "GREval-Bench",
        "multi-stage reasoning",
        "scene attributes",
        "local details",
        "semantic features",
        "coarse-grained localization",
        "fine-grained localization"
      ]
    },
    "publishedAt": "2025-05-24T09:48:57.000Z",
    "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language\n  Models and Enhanced Reasoning Chains",
    "summary": "Recent advances in Visual Language Models (VLMs) have demonstrated\nexceptional performance in visual reasoning tasks. However, geo-localization\npresents unique challenges, requiring the extraction of multigranular visual\ncues from images and their integration with external world knowledge for\nsystematic reasoning. Current approaches to geo-localization tasks often lack\nrobust reasoning mechanisms and explainability, limiting their effectiveness.\nTo address these limitations, we propose the Geo Reason Enhancement (GRE)\nSuite, a novel framework that augments VLMs with structured reasoning chains\nfor accurate and interpretable location inference. The GRE Suite is\nsystematically developed across three key dimensions: dataset, model, and\nbenchmark. First, we introduce GRE30K, a high-quality geo-localization\nreasoning dataset designed to facilitate fine-grained visual and contextual\nanalysis. Next, we present the GRE model, which employs a multi-stage reasoning\nstrategy to progressively infer scene attributes, local details, and semantic\nfeatures, thereby narrowing down potential geographic regions with enhanced\nprecision. Finally, we construct the Geo Reason Evaluation Benchmark\n(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across\ndiverse urban, natural, and landmark scenes to measure both coarse-grained\n(e.g., country, continent) and fine-grained (e.g., city, street) localization\nperformance. Experimental results demonstrate that GRE significantly\noutperforms existing methods across all granularities of geo-localization\ntasks, underscoring the efficacy of reasoning-augmented VLMs in complex\ngeographic inference. Code and data will be released at\nhttps://github.com/Thorin215/GRE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18700.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17870",
      "authors": [
        {
          "_id": "6837f049023315653be65a88",
          "user": {
            "_id": "640f32f6ef5c6dcac8b094bd",
            "avatarUrl": "/avatars/89b95837666ad696fe1f10808e4619b0.svg",
            "isPro": false,
            "fullname": "Shaina Raza",
            "user": "Shainarazavi",
            "type": "user"
          },
          "name": "Shaina Raza",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T05:27:38.248Z",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a89",
          "name": "Rizwan Qureshi",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8a",
          "name": "Marcelo Lotif",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8b",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:40.928Z",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8c",
          "name": "Deval Pandya",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8d",
          "name": "Christos Emmanouilidis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T13:20:23.000Z",
      "submittedOnDailyAt": "2025-05-29T04:08:04.627Z",
      "title": "모델도 인간처럼 생활의 질이 필요합니다: 모델인민통화에서 거짓을 제거합니다.",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "生成형 AI 모델은 훈련 코퍼스에 포함된 불실의 정보에 대해 학습하고 재현하는 경우가 많습니다. 이 논문은 생물학적인 면역화와 같은 방식으로, 약화된 바이러스에 대한 제어적 접촉이 면역력을 구축하는 것을 가정하여, AI 모델은 명시적으로 레이블된 불실한 사실의 작은, 분리된 세트에 기초하여 기초적인 조정을 수행하는 것을 \"백신\"으로 제시하고 있습니다. 이러한 불실한 사례는 조정 시 정기적으로 주입되어, 실제 입력에 대한 정확성을 유지하는 동시에, 잘못된 주장을 인식하고 거부하는 모델의 능력을 강화합니다. 구체적인 사례 연구는 면역화된 모델이 기준보다 크게 적은 불실한 정보를 생성하는 것을 보여줍니다. 우리 지식에 따르면, 이는 처음의 훈련 프레임워크이며, 사실검증된 불실한 사실 자체를 표준화된 백신으로 취급하고, 미래의 불실한 정보를 모델을 강화하기 위해 입력의 변형이나 일반적인 인간 피드백 신호에 의존하지 않는 것을 보여주고 있습니다. 또한, 우리는 불실한 데이터의 안전한 사용에 대한 윤리적인 가이드라인과 관리제어에 대해 설명하고 있습니다. 모델의 면역화는 AI 시스템의 사실성에 대응하는 선행적인 패러다임을 제공하고 있습니다.",
      "upvotes": 3,
      "discussionId": "6837f04a023315653be65ac6",
      "ai_summary": "A generative AI model is fine-tuned with labeled falsehoods to reduce misinformation generation, analogous to biological immunization.",
      "ai_keywords": [
        "generative AI models",
        "misinformation",
        "fine-tuning",
        "labeled falsehoods",
        "immunization",
        "fact-checked falsehoods",
        "supervised vaccine"
      ]
    },
    "publishedAt": "2025-05-23T09:20:23.000Z",
    "title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat\n  Falsehoods",
    "summary": "Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21191",
      "authors": [
        {
          "_id": "6837eb38f09a146728a4b80f",
          "name": "Junyan Zhang",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b810",
          "name": "Yubo Gao",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b811",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b812",
          "name": "Jungang Li",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b813",
          "name": "Zhaorui Hou",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b814",
          "name": "Sicheng Tao",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b815",
          "name": "Shuliang Liu",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b816",
          "name": "Song Dai",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b817",
          "name": "Yonghua Hei",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b818",
          "name": "Junzhuo Li",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b819",
          "name": "Xuming Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T13:40:28.000Z",
      "submittedOnDailyAt": "2025-05-29T03:36:16.801Z",
      "title": "インストラクション 대응 뉴런과 전문가의 공개: LLM의 명령어 따라하기 능력의 분석적 프레임워크",
      "submittedOnDailyBy": {
        "_id": "64b76528fdb702b3d8641514",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/wCbEtpPEHRjS4sZSVK1gK.png",
        "isPro": false,
        "fullname": "Jungang Li",
        "user": "Jungang",
        "type": "user"
      },
      "summary": "LLM의 미세 조정은 지시에 따라 행동하는 능력을 크게 향상시켰지만, 이러한 향상을 촉발하는 컴퓨터 구조는 아직 이해되지 않았습니다. 본 연구에서는 지시에 특화된 희소 요소(뉴런이나 Mixture-of-Experts(MoE) 아키텍처의 전문가)를 분리하고 분석하여, LLM의 계산이 어떻게 재구성되었는지 체계적으로 조사합니다. 특히, 6가지 다른 카테고리를 포함하는 더 잘 조정된 명령 데이터 세트인 HexaInst를 소개하고, 새로운 분석 프레임워크인 SPARCOM을 제안합니다. 이 프레임워크는 3가지 주요 기여를 포함합니다: 1) 이러한 희소 요소를 특정하는 방법, 2) 그 기능적 일반성과 고유성을 평가, 3) 그 변화의 체계적인 비교. 실험에서 기능적 일반성, 고유성, 그리고 명령 실행 시 이러한 요소의 중요한 역할을 보여주었습니다. 미세 조정의 변화와 희소 계산 구조 사이의 관계가 밝혀지면, LLM이 명령에 따라 행동하는 것을 내부화하는 과정을 더욱 깊은 엔지니어링으로 제공하여, 신뢰할 수 있는 LLM 커뮤니티에 제공됩니다.",
      "upvotes": 2,
      "discussionId": "6837eb39f09a146728a4b872",
      "ai_summary": "The study investigates the role of sparse computational components in the instruction-following capabilities of Large Language Models through systematic analysis and introduces HexaInst and SPARCOM for better understanding.",
      "ai_keywords": [
        "Large Language Models",
        "fine-tuning",
        "instruction-following",
        "HexaInst",
        "SPARCOM",
        "sparse components",
        "neurons",
        "Mixture-of-Experts",
        "instruction execution",
        "computational adaptations"
      ]
    },
    "publishedAt": "2025-05-27T09:40:28.000Z",
    "title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical\n  Framework for LLM's Instruction-Following Capabilities",
    "summary": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21191.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "64b76528fdb702b3d8641514",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/wCbEtpPEHRjS4sZSVK1gK.png",
      "fullname": "Jungang Li",
      "name": "Jungang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17507",
      "authors": [
        {
          "_id": "6833f24ed5c438959f7decf9",
          "user": {
            "_id": "619ef3f253061ce00477b09e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
            "isPro": false,
            "fullname": "Qiaosheng Chen",
            "user": "cqsss",
            "type": "user"
          },
          "name": "Qiaosheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T09:01:00.198Z",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfa",
          "name": "Kaijia Huang",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfb",
          "name": "Xiao Zhou",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfc",
          "name": "Weiqing Luo",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfd",
          "name": "Yuanning Cui",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfe",
          "name": "Gong Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T06:00:20.000Z",
      "submittedOnDailyAt": "2025-05-29T00:53:38.693Z",
      "title": "하우징 페이스 키노로지즘 기반의 추천, 분류, 체인 포인트 기준 평가",
      "submittedOnDailyBy": {
        "_id": "619ef3f253061ce00477b09e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
        "isPro": false,
        "fullname": "Qiaosheng Chen",
        "user": "cqsss",
        "type": "user"
      },
      "summary": "오픈소스 기계 학습(ML) 리소스의 급격한 성장은 IR 연구의 가속화에 기여하고 있습니다. 그러나 Hugging Face와 같은 기존 플랫폼은 구조화된 표현을 명시적으로 활용하지 않고, 모델의 진화 추적, 관련된 데이터 세트 추천 등 고급 검색 및 분석 기능을 제한하고 있습니다. 이러한 공간에 채우는 데에, HuggingKG, Hugging Face 커뮤니티에서 구축된 최초의 대규모 지식 그래프를 구축했습니다. 이 그래프는 260만 개의 노드와 620만 개의 에지를 가지고 있으며, 영역 고유의 관계와 풍부한 문학적 특성을 파악하고 있습니다. 이로 인해 HuggingBench라는 다 태스크 벤치마크를 발전시킬 수 있습니다. 이 벤치마크는 리소스 추천, 클래스 분류, 모델의 진화 추적 등 IR 태스크를 포함합니다. 우리의 실험은 HuggingKG의 특성과 이를 기반으로 생성된 태스크의 특성을 밝혀줍니다. 두 리소스는 공개적으로 사용 가능합니다. 따라서, 오픈소스 리소스 공유와 관리에 대한 연구의 발전을 기대합니다.",
      "upvotes": 2,
      "discussionId": "6833f24ed5c438959f7ded31",
      "githubRepo": "https://github.com/nju-websoft/HuggingBench",
      "ai_summary": "HuggingKG, a large-scale knowledge graph, enhances open source ML resource management by enabling advanced queries and analyses via HuggingBench.",
      "ai_keywords": [
        "knowledge graph",
        "resource recommendation",
        "classification",
        "tracing",
        "multi-task benchmark"
      ]
    },
    "publishedAt": "2025-05-23T02:00:20.000Z",
    "title": "Benchmarking Recommendation, Classification, and Tracing Based on\n  Hugging Face Knowledge Graph",
    "summary": "The rapid growth of open source machine learning (ML) resources, such as\nmodels and datasets, has accelerated IR research. However, existing platforms\nlike Hugging Face do not explicitly utilize structured representations,\nlimiting advanced queries and analyses such as tracing model evolution and\nrecommending relevant datasets. To fill the gap, we construct HuggingKG, the\nfirst large-scale knowledge graph built from the Hugging Face community for ML\nresource management. With 2.6 million nodes and 6.2 million edges, HuggingKG\ncaptures domain-specific relations and rich textual attributes. It enables us\nto further present HuggingBench, a multi-task benchmark with three novel test\ncollections for IR tasks including resource recommendation, classification, and\ntracing. Our experiments reveal unique characteristics of HuggingKG and the\nderived tasks. Both resources are publicly available, expected to advance\nresearch in open source resource sharing and management.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619ef3f253061ce00477b09e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
      "fullname": "Qiaosheng Chen",
      "name": "cqsss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15813",
      "authors": [
        {
          "_id": "68380974717461677df17514",
          "name": "Muquan Yu",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17515",
          "name": "Mu Nan",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17516",
          "name": "Hossein Adeli",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17517",
          "name": "Jacob S. Prince",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17518",
          "name": "John A. Pyles",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17519",
          "name": "Leila Wehbe",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751a",
          "name": "Margaret M. Henderson",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751b",
          "name": "Michael J. Tarr",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751c",
          "user": {
            "_id": "64b6ce23dbbd1f2cdb624d56",
            "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
            "isPro": false,
            "fullname": "Andrew Luo",
            "user": "aluo-x",
            "type": "user"
          },
          "name": "Andrew F. Luo",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-29T07:16:44.495Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b6ce23dbbd1f2cdb624d56/vCXz4Xv4AgHNNIC1S8aYf.png"
      ],
      "publishedAt": "2025-05-21T17:59:41.000Z",
      "submittedOnDailyAt": "2025-05-29T05:45:43.922Z",
      "title": "메타 학습을 이용한 인간의 고차원 시각 코테텍스의 In-Context Transformer 모델의 학습",
      "submittedOnDailyBy": {
        "_id": "64b6ce23dbbd1f2cdb624d56",
        "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
        "isPro": false,
        "fullname": "Andrew Luo",
        "user": "aluo-x",
        "type": "user"
      },
      "summary": "ファインドバージョン의 데이터셋에 pretraining된 인공지능 신경망은 인간 신경 반응과 표현적인 일치를 놀라게 할 때가 있다が, 시각적 피질의 이미지 계산 모델의 학습은 각각의 수준의 대규모 fMRI 데이터셋을 기반으로 하고 있다. 고가의, 시간이 강하게 요구되는 특성으로, 새로운 참가자나 자극에 대한 인코더의 일반화 능력이 제한되어 있다. BraInCoRL는 새로운 참가자나 자극에 대해 추가의 微調節을 하지 않고, 설명적인 학습을 사용하여 각 셀의 신경 반응을 예측한다. Transformer 아키텍처를 사용하여, 이미지 자극의 변수 수에 따라 변형 가능한 조건을 부여함으로써, 다수의 참가자에 대한 인덱스 편향을 학습한다. 훈련 기간 동안, 설명적인 학습에专用적으로 모델을 최적화하고, 이미지 특성과 셀 활성화를 조건부하여, 각 셀의 시각적 피질의 모델을 직접 생성함으로써, 더 우수한 성능을示すことができる。 BraInCoRL는 완전히 새로운 이미지에도 불구하고, 기존의 셀별로 인코더 설계를 일치시키지만, 강한 테스트 시 스케일링 편향을 나타낸다. 모델은 서로 다른 참가자와 fMRI 데이터의 획득 파라미터를 사용하여 완전히 새로운 시각적 fMRI 데이터셋에 대한 일반화가 가능하다. 또한, BraInCoRL는 시각적 피질의 신경 신호의 해석성을 높일 수 있는 방향으로 촉진한다. 마지막으로, 자연어 질문으로부터 셀 선택성에 대한 설명적인 매핑을 가능하게 한다.",
      "upvotes": 2,
      "discussionId": "68380977717461677df17638",
      "ai_summary": "BraInCoRL employs a transformer-based in-context learning approach to model higher visual cortex neural responses with few-shot examples, demonstrating superior performance and generalizability across new subjects, stimuli, and datasets.",
      "ai_keywords": [
        "functional representations",
        "higher visual cortex",
        "artificial neural networks",
        "fMRI datasets",
        "in-context learning",
        "transformer architecture",
        "inductive bias",
        "voxelwise neural responses",
        "image features",
        "voxel activations",
        "test-time scaling",
        "interpretability",
        "natural language queries",
        "voxel selectivity"
      ]
    },
    "publishedAt": "2025-05-21T13:59:41.000Z",
    "title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual\n  Cortex",
    "summary": "Understanding functional representations within higher visual cortex is a\nfundamental question in computational neuroscience. While artificial neural\nnetworks pretrained on large-scale datasets exhibit striking representational\nalignment with human neural responses, learning image-computable models of\nvisual cortex relies on individual-level, large-scale fMRI datasets. The\nnecessity for expensive, time-intensive, and often impractical data acquisition\nlimits the generalizability of encoders to new subjects and stimuli. BraInCoRL\nuses in-context learning to predict voxelwise neural responses from few-shot\nexamples without any additional finetuning for novel subjects and stimuli. We\nleverage a transformer architecture that can flexibly condition on a variable\nnumber of in-context image stimuli, learning an inductive bias over multiple\nsubjects. During training, we explicitly optimize the model for in-context\nlearning. By jointly conditioning on image features and voxel activations, our\nmodel learns to directly generate better performing voxelwise models of higher\nvisual cortex. We demonstrate that BraInCoRL consistently outperforms existing\nvoxelwise encoder designs in a low-data regime when evaluated on entirely novel\nimages, while also exhibiting strong test-time scaling behavior. The model also\ngeneralizes to an entirely new visual fMRI dataset, which uses different\nsubjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates\nbetter interpretability of neural signals in higher visual cortex by attending\nto semantically relevant stimuli. Finally, we show that our framework enables\ninterpretable mappings from natural language queries to voxel selectivity.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b6ce23dbbd1f2cdb624d56/vCXz4Xv4AgHNNIC1S8aYf.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b6ce23dbbd1f2cdb624d56",
      "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
      "fullname": "Andrew Luo",
      "name": "aluo-x",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12667",
      "authors": [
        {
          "_id": "682dd41740c6417d995087de",
          "user": {
            "_id": "648dca31385b84261811505d",
            "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
            "isPro": false,
            "fullname": "Zihan Su",
            "user": "Sugewud",
            "type": "user"
          },
          "name": "Zihan Su",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-28T16:17:54.970Z",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087df",
          "name": "Xuerui Qiu",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e0",
          "name": "Hongbin Xu",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e1",
          "name": "Tangyu Jiang",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e2",
          "name": "Junhao Zhuang",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e3",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e4",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e5",
          "name": "Shengfeng He",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e6",
          "name": "Fei Richard Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T03:31:31.000Z",
      "submittedOnDailyAt": "2025-05-29T01:30:41.124Z",
      "title": "Safe-Sora: 안전한 그래픽 워터마킹을 이용한 텍스트로부터 비디오의 생성",
      "submittedOnDailyBy": {
        "_id": "648dca31385b84261811505d",
        "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
        "isPro": false,
        "fullname": "Zihan Su",
        "user": "Sugewud",
        "type": "user"
      },
      "summary": "生成ビデオモデル의 폭발적인 성장은 AI 생성 콘텐츠의 신뢰성 있는 저작권 보호 요구가 증가しています。 이미지 합성의 인기 있는 데도 불구하고, 비시화 가능한 생성 마킹은 크게 VIDEO 생성에 대해 검토되지 않았습니다. 이를 채워나가기 위해, 우리는 Safe-Sora를 제안합니다. 이것은 VIDEO 생성 프로세스에 직접적인 그래픽 마킹을 삽입하는 첫 번째 프레임워크입니다. 水印와 덮어쓰기 콘텐츠의 시각적 유사성에 기반한 水印 성능의 관계를 관찰하여, 우리는 휴리스틱な 거대에서 미세까지의 적응적인 매칭 구조를 도입합니다. 특히, 마킹 이미지는 패치로 분할되어 시각적으로 가장 유사한 VIDEO 프레임에 할당되고, 프레임 내의 최적의 공간 영역에 더욱 국소화됩니다. VIDEO 프레임 간 마킹 패치의 공간 시간적인 기능성을 가능하게 하기 위해, 우리는 3D 水波紐 변환 확장 맵버 아키텍처를 개발하여 새로운 공간 시간적인 국소 샘플링 전략을 준비했습니다. 이는 긴 거리 의존성을 효과적으로 모델링하여 효율적이고 강건한 水印 보호의 새로운 길을 개척합니다. 우리를 알고 있는 한, 상태 공간 모델을 水印에 적용하는 첫 번째 시도이며, 효율적이고 강건한 水印 보호의 새로운 길을 개척합니다. 상세한 실험은 VIDEO 품질, 水印의 정확성, 강건성을 위해 가장 先進한 성능을 달성했으며, 이는 우리의 제안에 크게 기여합니다. 공개에 따라, 우리의 코드를 릴리즈합니다.",
      "upvotes": 2,
      "discussionId": "682dd41840c6417d99508847",
      "projectPage": "https://sugewud.github.io/Safe-Sora-project/",
      "githubRepo": "https://github.com/Sugewud/Safe-Sora",
      "ai_summary": "Safe-Sora embeds invisible watermarks into AI-generated videos using a hierarchical adaptive matching mechanism and a 3D wavelet transform-enhanced Mamba architecture, achieving top performance in video quality, watermark fidelity, and robustness.",
      "ai_keywords": [
        "generative watermarking",
        "hierarchical coarse-to-fine adaptive matching",
        "3D wavelet transform",
        "Mamba architecture",
        "spatiotemporal local scanning",
        "state space models",
        "watermark embedding",
        "watermark retrieval"
      ]
    },
    "publishedAt": "2025-05-18T23:31:31.000Z",
    "title": "Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking",
    "summary": "The explosive growth of generative video models has amplified the demand for\nreliable copyright preservation of AI-generated content. Despite its popularity\nin image synthesis, invisible generative watermarking remains largely\nunderexplored in video generation. To address this gap, we propose Safe-Sora,\nthe first framework to embed graphical watermarks directly into the video\ngeneration process. Motivated by the observation that watermarking performance\nis closely tied to the visual similarity between the watermark and cover\ncontent, we introduce a hierarchical coarse-to-fine adaptive matching\nmechanism. Specifically, the watermark image is divided into patches, each\nassigned to the most visually similar video frame, and further localized to the\noptimal spatial region for seamless embedding. To enable spatiotemporal fusion\nof watermark patches across video frames, we develop a 3D wavelet\ntransform-enhanced Mamba architecture with a novel spatiotemporal local\nscanning strategy, effectively modeling long-range dependencies during\nwatermark embedding and retrieval. To the best of our knowledge, this is the\nfirst attempt to apply state space models to watermarking, opening new avenues\nfor efficient and robust watermark protection. Extensive experiments\ndemonstrate that Safe-Sora achieves state-of-the-art performance in terms of\nvideo quality, watermark fidelity, and robustness, which is largely attributed\nto our proposals. We will release our code upon publication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12667.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648dca31385b84261811505d",
      "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
      "fullname": "Zihan Su",
      "name": "Sugewud",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22645",
      "authors": [
        {
          "_id": "6837dbed1233747046da00f5",
          "name": "Hanjia Lyu",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f6",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f7",
          "name": "Jian Kang",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f8",
          "name": "Allison Koenecke",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:56:49.000Z",
      "submittedOnDailyAt": "2025-05-29T02:33:57.876Z",
      "title": "바이ア스의 특징화: 간체자고 전통중국어에서 대규모 언어 모델의 벤치마크\n\n(Note: The translation provided above is a direct literal translation. For a more natural and contextually appropriate translation, consider the following:\n\n바이ア스의 특징화: 간체자 및 전통중국어의 대규모 언어 모델 벤치마크\n\nThis version maintains the professional and accurate tone while being more fluid and contextually appropriate for Korean readers.)",
      "submittedOnDailyBy": {
        "_id": "64c939307dba66c3a7e4d215",
        "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
        "isPro": false,
        "fullname": "BruceLyu",
        "user": "brucelyu",
        "type": "user"
      },
      "summary": "라르지엥트・라ング지유저모デル(LLMs)의 능력은 단순카타카나와 표준카타카나 모두에 대한 연구가 진행되어 있지만, LLMs가 이 두 가지 쓰기 언어 모두에서 다른 성능을 보여주는지는 아직 불명합니다. 이러한 이해는 중요합니다. LLMs의 답변의 질의 차이는 단순카타카나와 표준카타카나의 문화적 배경을 무시하고象征적인 피해를 계속하여 교육이나 고용 등 분야에서 LLMs를 활용한 결정을 악화시키고 하류의 피해를 확대하는 경우가 있습니다. LLMs의 성능의 차이를 조사하기 위해, 우리는 현실적인 시나리오를 반영하는 두 가지 벤치마크 태스크를 설계합니다: 지역적 용어 선택(LLMs를 단순카타카나와 표준카타카나로 다른 것을 이름짓는 것을 촉구)과 지역적 이름 선택(LLMs를 단순카타카나와 표준카타카나의 이름의 리스트에서 고용할 사람을 선택하는 것을 촉구)입니다. 두 태스크 모두에서, 11개의 첨단 상업 LLMs 서비스와 오픈소스 모델의 성능을 평가합니다. 이러한 모델들은 주로 영어, 단순카타카나, 또는 표준카타카나로 훈련되었습니다. 우리의 분석은 LLMs의 답변에 존재하는 편향은 작업과 촉구하는 언어에 의존하는 것을 보여줍니다. 지역적 용어 선택 태스크에서 대부분의 LLMs는 단순카타카나의 답변을 선호하고, 반면 지역적 이름 선택 태스크에서 표준카타카나의 이름을 선호했습니다. 이러한 차이는 훈련 데이터의 표현, 쓰기 언어의 선호, 단순카타카나와 표준카타카나의 토크나이징의 차이로 발생할 수 있습니다. 이러한 발견은 LLMs의 편향에 대한 진일보 분석이 필요함을 보여주고, 우리는 향후 LLMs의 행동을 중국어의 변체로 넘어가 가능한 평가에 대한 오픈소스 데이터 세트를 제공하여, 이 영역의 연구를 촉진합니다. (https://github.com/brucelyu17/SC-TC-Bench)",
      "upvotes": 1,
      "discussionId": "6837dbee1233747046da0125",
      "githubRepo": "https://github.com/brucelyu17/SC-TC-Bench",
      "ai_summary": "Research examines LLM performance biases between Simplified and Traditional Chinese in regional term and name choice tasks, attributing differences to training data and tokenization.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "LLM-facilitated decision-making",
        "regional term choice",
        "regional name choice",
        "open-sourced benchmark dataset",
        "SC-TC-Bench"
      ]
    },
    "publishedAt": "2025-05-28T13:56:49.000Z",
    "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese",
    "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22645.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c939307dba66c3a7e4d215",
      "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
      "fullname": "BruceLyu",
      "name": "brucelyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21960",
      "authors": [
        {
          "_id": "6837dd74ec10479b9605da15",
          "user": {
            "_id": "637e1cf4f09bf2498c543a73",
            "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
            "isPro": false,
            "fullname": "Senmao Li",
            "user": "senmaonk",
            "type": "user"
          },
          "name": "Senmao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:51.659Z",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da16",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da17",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da18",
          "name": "Tao Liu",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da19",
          "name": "Jiehang Xie",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1a",
          "name": "Joost van de Weijer",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1b",
          "name": "Fahad Shahbaz Khan",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1c",
          "name": "Shiqi Yang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1d",
          "name": "Yaxing Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1e",
          "name": "Jian Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T04:23:22.000Z",
      "submittedOnDailyAt": "2025-05-29T02:44:09.323Z",
      "title": "원일티켓：시간 독립적인 통일 엔코더를 사용하는 텍스트에서 이미지로의 확산 모델의 수용",
      "submittedOnDailyBy": {
        "_id": "637e1cf4f09bf2498c543a73",
        "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
        "isPro": false,
        "fullname": "Senmao Li",
        "user": "senmaonk",
        "type": "user"
      },
      "summary": "Text-to-Image (T2I) 확산 모델은 생성 모델링에서 놀라운 진전을 이루고 있지만, 추론 속도와 이미지 품질의trade-off가 존재하며, 효율적인 기계 처리에 있어 문제점이 있습니다. 기존의 결합된 T2I 모델은 적은 샘플링 단계에서 고품질의 이미지를 생성할 수 있지만, 특히 한 단계 모델에서는 다양성과 품질에 약점이 있습니다. 우리 분석에서, UNet 인코더에 불필요한 계산이 있음을 확인했습니다. 우리의 발견은 T2I 확산 모델에서 디코더가 풍부한, 명확한 의미 정보를捉え는 데 뛰어나고, 인코더는 다양한 시간 단계에서 다양한 디코더 간에 공유할 수 있음을 보여줍니다. 이러한 발견에 기반하여, 우리는 UNet 아키텍처의 학생 모델에 첫 번째 시간 의존성 없는 통일 인코더 TiUE를 도입합니다. TiUE는 루프 없는 이미지 생성 접근 방식이며, 확산 모델의 결합을 위해 한 단계 시냅스를 사용하며, 여러 디코더 시간 단계에서 인코더의 특징을 공유하고 병렬 샘플링을 가능하게하여 추론 시간 복잡도를 크게 줄입니다. 또한, KL divergence 항을 추가하여 노이즈 예측을 정규화하고 생성 이미지의 시각적 현실성과 다양성을 향상시킵니다. 실험 결과는 TiUE가 최선보다 뛰어나고, LCM, SD-Turbo, SwiftBrushv2를 포함하여 더 다양하고 현실적인 결과를 생성하면서 계산 효율성을 유지하는 것을 보여줍니다.",
      "upvotes": 1,
      "discussionId": "6837dd78ec10479b9605db06",
      "githubRepo": "https://github.com/sen-mao/Loopfree",
      "ai_summary": "Time-independent Unified Encoder TiUE reduces inference time and improves diversity and quality in Text-to-Image diffusion models by sharing encoder features across decoder time steps.",
      "ai_keywords": [
        "Text-to-Image diffusion models",
        "inference speed",
        "image quality",
        "distilled T2I models",
        "UNet encoders",
        "decoders",
        "semantic information",
        "Time-independent Unified Encoder TiUE",
        "loop-free image generation",
        "one-pass scheme",
        "parallel sampling",
        "KL divergence",
        "perceptual realism",
        "LCM",
        "SD-Turbo",
        "SwiftBrushv2"
      ]
    },
    "publishedAt": "2025-05-28T00:23:22.000Z",
    "title": "One-Way Ticket:Time-Independent Unified Encoder for Distilling\n  Text-to-Image Diffusion Models",
    "summary": "Text-to-Image (T2I) diffusion models have made remarkable advancements in\ngenerative modeling; however, they face a trade-off between inference speed and\nimage quality, posing challenges for efficient deployment. Existing distilled\nT2I models can generate high-fidelity images with fewer sampling steps, but\noften struggle with diversity and quality, especially in one-step models. From\nour analysis, we observe redundant computations in the UNet encoders. Our\nfindings suggest that, for T2I diffusion models, decoders are more adept at\ncapturing richer and more explicit semantic information, while encoders can be\neffectively shared across decoders from diverse time steps. Based on these\nobservations, we introduce the first Time-independent Unified Encoder TiUE for\nthe student model UNet architecture, which is a loop-free image generation\napproach for distilling T2I diffusion models. Using a one-pass scheme, TiUE\nshares encoder features across multiple decoder time steps, enabling parallel\nsampling and significantly reducing inference time complexity. In addition, we\nincorporate a KL divergence term to regularize noise prediction, which enhances\nthe perceptual realism and diversity of the generated images. Experimental\nresults demonstrate that TiUE outperforms state-of-the-art methods, including\nLCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results\nwhile maintaining the computational efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21960.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637e1cf4f09bf2498c543a73",
      "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
      "fullname": "Senmao Li",
      "name": "senmaonk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20715",
      "authors": [
        {
          "_id": "6837eef0e237f02cd3c87963",
          "name": "Fuwen Luo",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87964",
          "name": "Shengfeng Lou",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87965",
          "name": "Chi Chen",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87966",
          "name": "Ziyue Wang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87967",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87968",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87969",
          "name": "Jiyue Guo",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796a",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796b",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796c",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796d",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796e",
          "name": "Yang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T04:50:07.000Z",
      "submittedOnDailyAt": "2025-05-29T03:57:26.118Z",
      "title": "MUSEG: 시간 스탬프에 관심 있는 다중 세그먼트 갭을 통해 영상의 시간적인 이해를 강화하는 것",
      "submittedOnDailyBy": {
        "_id": "642086ed290342c5df85662d",
        "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
        "isPro": false,
        "fullname": "Chi Chen",
        "user": "carboncoo",
        "type": "user"
      },
      "summary": "映像의 시간계열 이해는 멀티모달 대언어 모델(MLLMs)에서 이미지 내의 이벤트를 원인으로 설명하는 데 중요합니다. 최근의 이미지 이해의 발전에도 불구하고, 현재의 MLLMs은 微妙한 시간계열 원인에 대해 어려움이 있습니다. 강화학습(RL)은 최근 이 문제를 해결하기 위해 검토되고 있지만, 현재의 RL 접근法是 효과가 제한되어 있습니다. 본 연구에서는 시간계열 이해를 높이기 위해 시간 스텝에 대한 관심 있는 멀티단계 그래프 흐름을 도입하는 새로운 RL 기반의 방법인 MUSEG를 제안합니다. MUSEG는 MLLMs가 여러 관련 있는 이미지 분절과 질문을 어레이링하여 더욱 상세한 시간계열 원인을 촉진합니다. 효과적인 학습을 촉진하기 위해, 시간계열 그래프 흐름에 대한 단계별 보상을 제공하여 맞춤형 RL 학습 레시피를 설계했습니다. 시간계열 그래프 흐름과 시간적 적응적인 이미지 QA 태스크에서 광범위하게 실험을 수행했으며, MUSEG는 현재의 방법보다 유의미하게 우수하며, 다양한 시간계열 이해 시나리오에서도 일반화 성능이 좋습니다. 이 프로젝트는 https://github.com/THUNLP-MT/MUSEG에서 확인 가능합니다.",
      "upvotes": 1,
      "discussionId": "6837eef1e237f02cd3c8798d",
      "githubRepo": "https://github.com/THUNLP-MT/MUSEG",
      "ai_summary": "MUSEG, an RL-based method with timestamp-aware multi-segment grounding, significantly enhances the temporal understanding of large language models by improving alignment with video segments and demonstrating superior performance in temporal reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "timestamp-aware multi-segment grounding",
        "temporal understanding",
        "MUSEG",
        "phased rewards",
        "temporal grounding",
        "time-sensitive video QA"
      ]
    },
    "publishedAt": "2025-05-27T00:50:07.000Z",
    "title": "MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware\n  Multi-Segment Grounding",
    "summary": "Video temporal understanding is crucial for multimodal large language models\n(MLLMs) to reason over events in videos. Despite recent advances in general\nvideo understanding, current MLLMs still struggle with fine-grained temporal\nreasoning. While reinforcement learning (RL) has been explored to address this\nissue recently, existing RL approaches remain limited in effectiveness. In this\nwork, we propose MUSEG, a novel RL-based method that enhances temporal\nunderstanding by introducing timestamp-aware multi-segment grounding. MUSEG\nenables MLLMs to align queries with multiple relevant video segments, promoting\nmore comprehensive temporal reasoning. To facilitate effective learning, we\ndesign a customized RL training recipe with phased rewards that progressively\nguides the model toward temporally grounded reasoning. Extensive experiments on\ntemporal grounding and time-sensitive video QA tasks demonstrate that MUSEG\nsignificantly outperforms existing methods and generalizes well across diverse\ntemporal understanding scenarios. View our project at\nhttps://github.com/THUNLP-MT/MUSEG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20715.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642086ed290342c5df85662d",
      "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
      "fullname": "Chi Chen",
      "name": "carboncoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21060",
      "authors": [
        {
          "_id": "683818841902f641cc669774",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "683818841902f641cc669775",
          "name": "Xiang Liu",
          "hidden": false
        },
        {
          "_id": "683818841902f641cc669776",
          "name": "Peidong Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T11:47:15.000Z",
      "submittedOnDailyAt": "2025-05-29T06:49:42.295Z",
      "title": "스タイル3R: 임의의 시나리오와 스타일에 대한 즉시 3D 스타일 리모델링\n\n(请注意，虽然要求不添加任何解释或额外的文本，但为了确保翻译的准确性和专业性，我提供了一个更符合韩国语习惯的表达方式。如果需要完全按照原文的直译，请告知。)",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "스タイリング된 3D 스ケーン를 즉시 생성하고, 다점 일관성을 유지하며, 스タイリング 이미지에 충실하게 일치하는 것은 중요な 과제입니다. 현재의 가장 선진된 3D 스タイリング 방법은, 계산량이 많은 테스트 시간 최적화를 사용하며, 예술적인 특징을 학습된 3D 표현으로 전달하는 것이 일반적이며, 이는 밀집한 포즈가 있는 입력 이미지가 필요합니다. 반면, 최근의 전향 재구성 모델의 발전을 활용하여, 무포즈된 스プース뷰의 스ケーン 이미지와 임의의 스タイル 이미지를 사용하여, 1초 이내의 직접적인 3D 스タイリング을 실현하는 새로운 접근 방식을 제시합니다. 재구성과 스タイリング의 고유한 연결을 해결하기 위해, 구조 모델링과 외관 셈을 분리하는 분할 아키텍처를 도입하고, 스タイリング의 경향이 3D 스ケーン의 구조를 왜곡시키는 것을 방지합니다. 또한, 식별도 손실을 적용하여, 새로운 시각 합성 태스크를 통해 스タイリング 모델의 사전 학습을 촉진합니다. 이 전략은 스タイリング을 전문화할 수 있는 반면, 원본 재구성 능력의 유지가 가능합니다. 데이터 세트 내외부에서의 세부적인 평가는, 우리의 접근 방식이 스타일과 스ケーン의 외관을 잘 통합하며, 다점 일관성과 효율성에서 현재의 방법보다 뛰어나게 나타내는 것을 보여줍니다.",
      "upvotes": 0,
      "discussionId": "6838188b1902f641cc669947",
      "ai_summary": "A novel feed-forward model achieves fast 3D stylization using sparse view images, maintaining multi-view consistency and high-quality style transfer while retaining reconstruction accuracy.",
      "ai_keywords": [
        "feed-forward reconstruction models",
        "3D stylization",
        "dense posed input images",
        "unposed sparse-view scene images",
        "branched architecture",
        "structure modeling",
        "appearance shading",
        "identity loss",
        "novel view synthesis",
        "in-domain datasets",
        "out-of-domain datasets",
        "multi-view consistency"
      ]
    },
    "publishedAt": "2025-05-27T07:47:15.000Z",
    "title": "Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and\n  Styles",
    "summary": "Stylizing 3D scenes instantly while maintaining multi-view consistency and\nfaithfully resembling a style image remains a significant challenge. Current\nstate-of-the-art 3D stylization methods typically involve computationally\nintensive test-time optimization to transfer artistic features into a\npretrained 3D representation, often requiring dense posed input images. In\ncontrast, leveraging recent advances in feed-forward reconstruction models, we\ndemonstrate a novel approach to achieve direct 3D stylization in less than a\nsecond using unposed sparse-view scene images and an arbitrary style image. To\naddress the inherent decoupling between reconstruction and stylization, we\nintroduce a branched architecture that separates structure modeling and\nappearance shading, effectively preventing stylistic transfer from distorting\nthe underlying 3D scene structure. Furthermore, we adapt an identity loss to\nfacilitate pre-training our stylization model through the novel view synthesis\ntask. This strategy also allows our model to retain its original reconstruction\ncapabilities while being fine-tuned for stylization. Comprehensive evaluations,\nusing both in-domain and out-of-domain datasets, demonstrate that our approach\nproduces high-quality stylized 3D content that achieve a superior blend of\nstyle and scene appearance, while also outperforming existing methods in terms\nof multi-view consistency and efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 874
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21582",
      "authors": [
        {
          "_id": "68380c860fb1ddbe91ba0bf9",
          "user": {
            "_id": "631f5035c6b20f03c823c4ba",
            "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
            "isPro": false,
            "fullname": "Christopher Knievel",
            "user": "CKnievel",
            "type": "user"
          },
          "name": "Christopher Knievel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:12.986Z",
          "hidden": false
        },
        {
          "_id": "68380c860fb1ddbe91ba0bfa",
          "name": "Alexander Bernhardt",
          "hidden": false
        },
        {
          "_id": "68380c860fb1ddbe91ba0bfb",
          "name": "Christian Bernhardt",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T10:07:05.000Z",
      "submittedOnDailyAt": "2025-05-29T06:18:15.842Z",
      "title": "AITEE - 전기공학을 위한 아웃로드 타머",
      "submittedOnDailyBy": {
        "_id": "631f5035c6b20f03c823c4ba",
        "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
        "isPro": false,
        "fullname": "Christopher Knievel",
        "user": "CKnievel",
        "type": "user"
      },
      "summary": "智能チューターシステムと大規模な言語モデルの組み合わせは、学生の多様な需要を満たし、自立学習を促進する可能性のあるアプローチです。大規模な言語モデルは電気工学の基本知識についてはよく知っていますが、電気回路に関する特定の質問を解決する能力は十分ではありません。本論文では、学習プロセスを通じて学生をそばにいって、個別的なサポートを提供し、自立学習を促進するための電気工学向けのアガントベースチューターシステム AITEE を紹介します。AITEEは手書きやデジタル回路をサポートし、自然的な学生との相互作用を可能にします。新しいグラフベースの類似性測定法は、レクチャーマテリアルから適切なコンテキストを特定するための検索アウガンド生成アプローチを用います。また、並列スパイスシミュレーションは解決方法論の適用精度を向上させます。システムはソクラティックディアロギーを実装し、ガイドドブリングの質問によって学習者の自立性を育成します。実験的評価により、AITEEは領域専門的な知識の適用において基準的なアプローチよりも显著に優れていることが示され、中サイズのLLMモデルでも可愛い性能を示しています。我々の結果は、電気工学教育におけるスケーラブルで、個別的で、効果的な学習環境を提供するアガントチューターの可能性を高めます。",
      "upvotes": 0,
      "discussionId": "68380c870fb1ddbe91ba0c55",
      "ai_summary": "An agent-based tutoring system for electrical engineering enhances learning through natural circuit interaction, context-retrieving generation, and guided questioning, demonstrating superior performance compared to baseline methods.",
      "ai_keywords": [
        "agent-based tutoring system",
        "large language models",
        "electrical circuits",
        "adapted circuit reconstruction",
        "graph-based similarity measure",
        "retrieval augmented generation",
        "parallel Spice simulation",
        "Socratic dialogue"
      ]
    },
    "publishedAt": "2025-05-27T06:07:05.000Z",
    "title": "AITEE -- Agentic Tutor for Electrical Engineering",
    "summary": "Intelligent tutoring systems combined with large language models offer a\npromising approach to address students' diverse needs and promote\nself-efficacious learning. While large language models possess good\nfoundational knowledge of electrical engineering basics, they remain\ninsufficiently capable of addressing specific questions about electrical\ncircuits. In this paper, we present AITEE, an agent-based tutoring system for\nelectrical engineering designed to accompany students throughout their learning\nprocess, offer individualized support, and promote self-directed learning.\nAITEE supports both hand-drawn and digital circuits through an adapted circuit\nreconstruction process, enabling natural interaction with students. Our novel\ngraph-based similarity measure identifies relevant context from lecture\nmaterials through a retrieval augmented generation approach, while parallel\nSpice simulation further enhances accuracy in applying solution methodologies.\nThe system implements a Socratic dialogue to foster learner autonomy through\nguided questioning. Experimental evaluations demonstrate that AITEE\nsignificantly outperforms baseline approaches in domain-specific knowledge\napplication, with even medium-sized LLM models showing acceptable performance.\nOur results highlight the potential of agentic tutors to deliver scalable,\npersonalized, and effective learning environments for electrical engineering\neducation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21582.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631f5035c6b20f03c823c4ba",
      "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
      "fullname": "Christopher Knievel",
      "name": "CKnievel",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18149",
      "authors": [
        {
          "_id": "6837e51d05c81fd7d7d1962e",
          "user": {
            "_id": "63ca499104c97982831127ec",
            "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
            "isPro": false,
            "fullname": "Aradhye Agarwal",
            "user": "aradhye",
            "type": "user"
          },
          "name": "Aradhye Agarwal",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:47.337Z",
          "hidden": false
        },
        {
          "_id": "6837e51d05c81fd7d7d1962f",
          "name": "Ayan Sengupta",
          "hidden": false
        },
        {
          "_id": "6837e51d05c81fd7d7d19630",
          "name": "Tanmoy Chakraborty",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:57:43.000Z",
      "submittedOnDailyAt": "2025-05-29T06:15:54.434Z",
      "title": "초기 종료 검색: 대규모 언어 모델의 효율적인 테스트 시간 스케일링",
      "submittedOnDailyBy": {
        "_id": "63ca499104c97982831127ec",
        "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
        "isPro": false,
        "fullname": "Aradhye Agarwal",
        "user": "aradhye",
        "type": "user"
      },
      "summary": "테스트 시 스케일링(TTS)은 추론 시 계산량을 동적으로 배분하여 대규모 언어 모델의 추론 성능을 향상시키는 잠재적인 방법입니다. 기존의 TTS 방법들은 효과적이지만, 긴 디코딩 패스를 필요로 하며, 많은 샘플을 생성해야 하며, 토큰 사용량과 추론 시간은 증가합니다. 우리는 짧은 트래스이긴 하지만 긴 트래스보다 훨씬 정확하게 결과를 도출하는 경우가 많은 것을 발견했습니다. 이를 기반으로 훈련하지 않는 병렬 디코딩 전략인 First Finish Search(FFS)를 도입했습니다. FFS는 n개의 독립적인 샘플을 시작하고, 그 중 어떤가든 완료되면 즉시 반환합니다. FFS는 DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B, Phi-4-Reasoning-Plus의 4개의 논리 모델과 AIME24, AIME25-I, AIME25-II, GPQA Diamond의 4개의 데이터 세트를 사용하여, 간단한 디코딩, 빔 검색, 다수결, 버지쿤 압도와 동시에 평가했습니다. DeepSeek-R1에서 FFS는 AIME 데이터 세트에서 82.23%의 정확도를 달성하며, DeepSeek-R1의 개별 정확도에서 15%의 향상을 보였고, OpenAI의 o4-mini과 유사한 성능을 나타냅니다. 이론적 분석에서는, 가장 짧은 트래스로 중단하여 정확한 답을 도출할 수 있다는 점을 설명하고, 조기 중단이 최적이 아닌 조건을 식별했습니다. FFS의 아름다움과 간단함이 간단한 TTS 전략이 놀라운 성능을 나타내며, 추론 시 가능한 간단한 접근 방식의 잠재력을 밝혀줍니다.",
      "upvotes": 0,
      "discussionId": "6837e51d05c81fd7d7d19659",
      "ai_summary": "First Finish Search improves accuracy in large language models by stopping inference at the first completed sample, significantly outperforming other decoding strategies in reasoning tasks.",
      "ai_keywords": [
        "Test-time scaling",
        "TTS",
        "dynamic allocation",
        "inference",
        "reasoning tasks",
        "First Finish Search",
        "FFS",
        "parallel decoding",
        "decoding strategies",
        "beam search",
        "majority voting",
        "budget forcing",
        "accuracy",
        "performance",
        "inference latency",
        "early stopping"
      ]
    },
    "publishedAt": "2025-05-23T13:57:43.000Z",
    "title": "First Finish Search: Efficient Test-Time Scaling in Large Language\n  Models",
    "summary": "Test-time scaling (TTS), which involves dynamic allocation of compute during\ninference, offers a promising way to improve reasoning in large language\nmodels. While existing TTS methods work well, they often rely on long decoding\npaths or require a large number of samples to be generated, increasing the\ntoken usage and inference latency. We observe the surprising fact that for\nreasoning tasks, shorter traces are much more likely to be correct than longer\nones. Motivated by this, we introduce First Finish Search (FFS), a\ntraining-free parallel decoding strategy that launches n independent samples\nand returns as soon as any one completes. We evaluate FFS alongside simple\ndecoding, beam search, majority voting, and budget forcing on four reasoning\nmodels (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and\nacross four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With\nDeepSeek-R1, FFS achieves 82.23% accuracy on the AIME datasets, a 15%\nimprovement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's\no4-mini performance. Our theoretical analysis explains why stopping at the\nshortest trace is likely to yield a correct answer and identifies the\nconditions under which early stopping may be suboptimal. The elegance and\nsimplicity of FFS demonstrate that straightforward TTS strategies can perform\nremarkably well, revealing the untapped potential of simple approaches at\ninference time.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18149.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ca499104c97982831127ec",
      "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
      "fullname": "Aradhye Agarwal",
      "name": "aradhye",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]