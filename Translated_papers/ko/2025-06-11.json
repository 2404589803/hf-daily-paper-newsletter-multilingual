[
  {
    "paper": {
      "id": "2506.06751",
      "authors": [
        {
          "_id": "6848fecf42e4f9106973f315",
          "user": {
            "_id": "62bd6c6baaf1480f1aa2222e",
            "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
            "isPro": false,
            "fullname": "Mikhail Salnikov",
            "user": "msalnikov",
            "type": "user"
          },
          "name": "Mikhail Salnikov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:34:47.630Z",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f316",
          "name": "Dmitrii Korzh",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f317",
          "user": {
            "_id": "657c4a8dfb0285d857d86e4c",
            "avatarUrl": "/avatars/17635a4c2c804dd3837ae01833bb940d.svg",
            "isPro": false,
            "fullname": "Ivan",
            "user": "IvanLazichny",
            "type": "user"
          },
          "name": "Ivan Lazichny",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-11T09:26:44.876Z",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f318",
          "name": "Elvir Karimov",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f319",
          "name": "Artyom Iudin",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31a",
          "name": "Ivan Oseledets",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31b",
          "name": "Oleg Y. Rogov",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31c",
          "user": {
            "_id": "605473729d7c1d4d81b7e52b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662046050710-605473729d7c1d4d81b7e52b.jpeg",
            "isPro": false,
            "fullname": "Alexander Panchenko",
            "user": "apanc",
            "type": "user"
          },
          "name": "Alexander Panchenko",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-11T09:27:06.218Z",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31d",
          "name": "Natalia Loukachevitch",
          "hidden": false
        },
        {
          "_id": "6848fecf42e4f9106973f31e",
          "user": {
            "_id": "662f8d645c4db70c77a203b0",
            "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
            "isPro": false,
            "fullname": "Elena Tutubalina",
            "user": "tlenusik",
            "type": "user"
          },
          "name": "Elena Tutubalina",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-11T09:26:55.840Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-07T10:45:17.000Z",
      "submittedOnDailyAt": "2025-06-11T02:53:12.616Z",
      "title": "국제정치의 편견: 현대의 언어 모델에 의한 「좋은」와 「나쁜」 나라들",
      "submittedOnDailyBy": {
        "_id": "62bd6c6baaf1480f1aa2222e",
        "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
        "isPro": false,
        "fullname": "Mikhail Salnikov",
        "user": "msalnikov",
        "type": "user"
      },
      "summary": "이 논문은 역사적 사건의 해석에서 국제적인 시각의 차이（미국, 영국, 미국-중국연합, 중국）를 통해 LLMs의 지역정치적 편향에 대해 평가합니다. 우리는 중립적인 사건 설명과 각 나라의 다른 관점을 가지고 있는 새로운 데이터 세트를 소개합니다. 우리의 발견은 큰 지역정치적 편향이 존재하고, 모델이 특정 나라의 서사적 표현을 더 친화적으로 처리하는 것을 확인했습니다. 또한 간단한 편향제어 프롬프트는 이러한 편향을 줄일 수 있었습니다. 참가자의 레이블을 조작한 실험은 모델이 특성에 대한 민감성을 보여주고, 편향을 확대하거나 적절하지 않은 관점을 인식하는 것을 확인했습니다. 이 연구는 LLMs의 국가의 서사적 편향을 명확히 하고, 간단한 편향제어 방법의 효과성을 의심하고, 향후 지역정치적 편향 연구에 대한 프레임워크와 데이터 세트를 제공합니다.",
      "upvotes": 34,
      "discussionId": "6848fed042e4f9106973f31f",
      "projectPage": "https://airi-institute.github.io/geopolitical_llm_bias",
      "githubRepo": "https://github.com/AIRI-Institute/geopolitical_llm_bias",
      "ai_summary": "LLMs exhibit significant geopolitical biases in their interpretation of historical events, and simple debiasing methods have limited effectiveness; a novel dataset for further research is provided.",
      "ai_keywords": [
        "LLMs",
        "geopolitical biases",
        "historical events",
        "national narratives",
        "debiasing prompts"
      ]
    },
    "publishedAt": "2025-06-07T06:45:17.000Z",
    "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models",
    "summary": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bd6c6baaf1480f1aa2222e",
      "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
      "fullname": "Mikhail Salnikov",
      "name": "msalnikov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08672",
      "authors": [
        {
          "_id": "684936e842e4f9106973f45e",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "684936e842e4f9106973f45f",
          "name": "Jiaqi Li",
          "hidden": false
        },
        {
          "_id": "684936e842e4f9106973f460",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-11T07:57:29.119Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T10:31:21.000Z",
      "submittedOnDailyAt": "2025-06-11T07:49:21.042Z",
      "title": "룰 리지저: 도메인에 대한 동적 샘플링에 의한 강화 기반의 규칙 추론",
      "submittedOnDailyBy": {
        "_id": "63a95a6a7930fa8c7dd63d4e",
        "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
        "isPro": false,
        "fullname": "Zilong Zheng",
        "user": "zlzheng",
        "type": "user"
      },
      "summary": "ルールベース의 추론은 논리적 근거의 기본적인 문제 중 하나로 인정되어 있지만, 현실적인 애플리케이션에서 규칙의 형식, 종류, 복잡성의 편향은 엄격한 도전으로 되어 있습니다. 최근의 연구에 따르면, 큰 규모의 논리 모델(LRMs)은 놀라운 논리 능력이 있는 것으로 입증되어 있으며, 이 성능은 강화학습(RL)에 의해 크게 향상되었습니다. 그러나, 작은 규모의 논리 모델(SRMs)이 다양한 태스크와 분야에서 강력한 일반화에 따른 규칙 기반의 추론을 학습할 수 있는지는 여론이 있는 문제입니다. 이에 대해, 우리는 강화된 규칙 기반의 추론, 즉 RuleReasoner라는 간단하고 효과적인 방법을 소개합니다. 이 방법은 다양한 수정된 태스크의 폭을 통해 새로운 분야에 대한 동적인 샘플링 접근법을 통해 규칙 기반의 추론을 수행하는 방법입니다. 특히, RuleReasoner는 역사적인 보상에 기반하여 다른 분야의 샘플링 가중치를 업데이트하고, 이로 인해 분야의 확장과 RL의 유연한 온라인 학습 스케줄을 실현합니다. 이는 기존의 방법들에서 사용된 사전의 인간 엔지니어링 된 혼합 훈련 레시피의 필요성을 제거합니다. 분포 내(ID)와 분포 외(OOD) 벤치마크에서 실험적 평가에 따르면, RuleReasoner는 선진적인 LRMs를 크게 초과하고 있습니다(8개의 ID 태스크에서 평균 4.1%의 Delta, 3개의 OOD 태스크에서 평균 10.4%의 Delta). 특히, 우리의 접근법은 이전의 RL의 동적인 샘플링 방법에 비해 더 높은 계산 효율성을 나타냅니다.",
      "upvotes": 15,
      "discussionId": "684936e842e4f9106973f461",
      "githubRepo": "https://github.com/bigai-nlco/RuleReasoner",
      "ai_summary": "RuleReasoner enhances rule-based reasoning in small models through dynamic domain sampling, achieving superior performance and efficiency compared to large models.",
      "ai_keywords": [
        "reinforcement learning",
        "rule-based reasoning",
        "large reasoning models",
        "small reasoning models",
        "domain-aware dynamic sampling",
        "historical rewards",
        "in-distribution",
        "out-of-distribution",
        "computational efficiency"
      ]
    },
    "publishedAt": "2025-06-10T06:31:21.000Z",
    "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic\n  Sampling",
    "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin (Delta4.1%\naverage points on eight ID tasks and Delta10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a95a6a7930fa8c7dd63d4e",
      "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
      "fullname": "Zilong Zheng",
      "name": "zlzheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09040",
      "authors": [
        {
          "_id": "6848fff842e4f9106973f321",
          "name": "Dianyi Wang",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f322",
          "name": "Wei Song",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f323",
          "name": "Yikun Wang",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f324",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f325",
          "name": "Kaicheng Yu",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f326",
          "name": "Zhongyu Wei",
          "hidden": false
        },
        {
          "_id": "6848fff842e4f9106973f327",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:57:50.000Z",
      "submittedOnDailyAt": "2025-06-11T05:55:58.221Z",
      "title": "자동 회귀의 의미적인 시각 재구성은 VLMs의 이해를 더욱 잘 돕는 데 도움이 됩니다.",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "표준적인 대형 시각 언어 모델(LVLMs)은 텍스트 시퀀스만 자동 복원 지원을 적용하고 시각 모델을 학습 과정에 완전히 통합하지 않는다. 이로 인해 3가지 주요한 제한이 발생합니다: 1) 이미지 사용 능력은 캡처와 함께 존재하지 않는 경우, 2) 캡처가 중요한 시각 디테일을 생략하는 위험, 3) 특정 시각 콘텐츠가 언어로 더 정확하게 전달하는 것이 어려워진다. 따라서 현재의 LVLMs은 시각으로부터 언어의 어레이멘을 우선시하면서, 미세한 시각 정보들을 놓치는 가능성이 있다. 반면에, 선행 연구에서는 자동 복원 이미지 생성에 대해 조사하고, 자동 복원 시각 지원을 효과적으로 활용하여 이미지 이해를 향상시키는 것은 개발 중인 도전이다. 본 논문에서는 자동 복원 언어 시각 재구성(ASVR)을 소개하고, 시각과 언어의 모델을 하나의 자동 복원 프레임워크에서 함께 학습할 수 있도록 한다. 이미지의 원본 시각 외관을 자동 복원하는 것은 여러 타입 이해를 향상시키지 않지만, 시각 표현을 자동 복원하는 것은 이해를 향상시키는 것이 확인되었다. 특히, 모델이 연속적인 이미지 특징을 입력으로 받는 경우, 이들을 분산적인 의미 토큰으로 효과적으로 재구성할 수 있으며, 다양한 여러 타입 이해 벤치마크에서 안정적인 향상을 확인된다. 우리의 접근법은 556k-2M의 데이터 스케일과 LLM 백본의 종류에 따라도 큰 성능 향상을 얻는다. 특히, ASVR는 14개의 여러 타입 벤치마크의 평균 점수에서 LLaVA-1.5를 5% 이상 향상시킨다. 코드는 https://github.com/AlenjandroWang/ASVR에 제공된다.",
      "upvotes": 12,
      "discussionId": "6848fff842e4f9106973f328",
      "ai_summary": "Autoregressive Semantic Visual Reconstruction (ASVR) improves multimodal understanding by focusing on semantic reconstruction rather than raw visual appearance, enhancing performance across various benchmarks.",
      "ai_keywords": [
        "autoregressive supervision",
        "large vision-language models (LVLMs)",
        "visual modality",
        "image captions",
        "autoregressive image generation",
        "multimodal learning",
        "semantic representation",
        "discrete semantic tokens",
        "multimodal understanding benchmarks",
        "LLaVA-1.5"
      ]
    },
    "publishedAt": "2025-06-10T13:57:50.000Z",
    "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better",
    "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09040.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08009",
      "authors": [
        {
          "_id": "68485e5b4fe3b60e21b258bd",
          "name": "Xun Huang",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258be",
          "name": "Zhengqi Li",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258bf",
          "user": {
            "_id": "67492ee82ad3cfc108a41bbb",
            "avatarUrl": "/avatars/7ad03e55a8791c62f1271a5c9bf8cc60.svg",
            "isPro": false,
            "fullname": "Guande He",
            "user": "gdhe17",
            "type": "user"
          },
          "name": "Guande He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:40.950Z",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258c0",
          "name": "Mingyuan Zhou",
          "hidden": false
        },
        {
          "_id": "68485e5b4fe3b60e21b258c1",
          "name": "Eli Shechtman",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67492ee82ad3cfc108a41bbb/bEQLc--MCz7a-4ZBIBbaJ.mp4"
      ],
      "publishedAt": "2025-06-09T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-11T04:34:32.742Z",
      "title": "Self Forcing: 과적합 테스트 간의 틀을 통한 자동 복원 비디오 디퓨저",
      "submittedOnDailyBy": {
        "_id": "67492ee82ad3cfc108a41bbb",
        "avatarUrl": "/avatars/7ad03e55a8791c62f1271a5c9bf8cc60.svg",
        "isPro": false,
        "fullname": "Guande He",
        "user": "gdhe17",
        "type": "user"
      },
      "summary": "Self Forcing는, 자동으로 다음 프레임을 생성하는 비디오 디퓨저 모델의 새로운 훈련 패러다임에 대해 소개합니다. 이는 추론 시 자신의 불완전한 출력에 기반하여 시퀀스를 생성하는 것을 해결하고, 모델이 실제 컨텍스트에 기반하여 오랜 기간의 문제인 훈련을 해결하는 것입니다. 이전 방법에 비해, 실제 컨텍스트 프레임에 기반하여 미래의 프레임을 디노이즈하는 것이 아니라, Self Forcing는 훈련 중에 KV 캐시를 사용하여 자동적인 롤아웃을 수행하고, 각 프레임의 생성에는 이전에 생성된 출력을 기반으로 합니다. 이 전략은 비디오 수준에서 전체적인 손실을 통해 훈련을 제어할 수 있으며, 생성된 시퀀스의 질을 직접 평가할 수 있습니다. 훈련 효율을 보장하기 위해, 적은 단계의 디퓨저 모델과 스팸 랜덤 가디언 토큰 트라이미테이션을 사용하며, 계산 비용과 성능의 균형을 효과적으로 조정합니다. 또한, 효율적으로 자동적인 롤아웃을 수행하기 위해, 로링 KV 캐시 구조를 도입합니다. 확장된 실험에 따라, 우리의 접근法是 실시간 스트리밍 비디오 생성을 실현하고, 1 카드에서도 초 단위의 라틴 시에 대해 생성 품질을 만족하며, 더 느려질 수 없으며, 기본 디퓨저 모델의 생성 품질을 초월할 수 있습니다. 프로젝트 웹 사이트: http://self-forcing.github.io/",
      "upvotes": 9,
      "discussionId": "68485e5b4fe3b60e21b258c2",
      "projectPage": "https://self-forcing.github.io/",
      "githubRepo": "https://github.com/guandeh17/Self-Forcing",
      "ai_summary": "Self Forcing, a novel training method for autoregressive video diffusion models, reduces exposure bias and improves generation quality through holistic video-level supervision and efficient caching mechanisms.",
      "ai_keywords": [
        "Self Forcing",
        "autoregressive video diffusion models",
        "exposure bias",
        "denoising",
        "key-value (KV) caching",
        "autoregressive rollout",
        "holistic loss",
        "few-step diffusion model",
        "stochastic gradient truncation",
        "rolling KV cache mechanism",
        "video extrapolation"
      ]
    },
    "publishedAt": "2025-06-09T13:59:55.000Z",
    "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
    "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67492ee82ad3cfc108a41bbb/bEQLc--MCz7a-4ZBIBbaJ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67492ee82ad3cfc108a41bbb",
      "avatarUrl": "/avatars/7ad03e55a8791c62f1271a5c9bf8cc60.svg",
      "fullname": "Guande He",
      "name": "gdhe17",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07927",
      "authors": [
        {
          "_id": "684794003ec10bdd8ab4de11",
          "name": "Jiayi Sheng",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de12",
          "name": "Luna Lyu",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de13",
          "name": "Jikai Jin",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de14",
          "name": "Tony Xia",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de15",
          "name": "Alex Gu",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de16",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "684794003ec10bdd8ab4de17",
          "name": "Pan Lu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60f5f68fa7fd83d025749234/ahvR-ZmwDrUNm3-jcQ4o1.png"
      ],
      "publishedAt": "2025-06-09T16:43:38.000Z",
      "submittedOnDailyAt": "2025-06-11T04:15:25.994Z",
      "title": "대규모 언어 모델을 이용한 불등식 증명 방법론",
      "submittedOnDailyBy": {
        "_id": "60f5f68fa7fd83d025749234",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
        "isPro": true,
        "fullname": "Pan Lu",
        "user": "lupantech",
        "type": "user"
      },
      "summary": "불등식 증명은 다양한 과학과 수학 분야에서 중요한 분야로 있으며, 엄격한 경계의 발견과 전략적인 定理의 적용 등 높은 추론 능력에 대한 검증을 수행합니다. 이로 인해 대규모 언어 모델(LLMs)은 일반적인 수학 문제 해결보다 깊은 통찰을 제공하며, 특히 어려운 경계를 설정하고 있습니다. 이 분야의 발전은 현재의 데이터 셋이 희소하거나 합성적하거나 엄격한 형식화되어 있기 때문에 방해되어 있습니다. 이를 해결하기 위해, 우리는 비정식적이지만 증명 가능한 태스크 형성을 제안하고, 불등식 증명을 경계의 추정과 관계 예측의 두 가지 자동 검증 가능한 서브 태스크로 재구성합니다. 이를 기반으로, 우리는 \"IneqMath\"라는 오린匹克 수준의 불등식의 익스퍼트定制 데이터 셋을 릴리즈합니다. 이 데이터 셋에는 단계별 해법과 정리를 포함한 테스트 셋과 학습 코퍼스가 있습니다. 또한, 우리는 새로운 LLM-as-judge 평가 프레임워크를 개발합니다. 이는 최종적인 답변의 판단자와 4단계별 판단자를 조합하여 일반적인 추론의 오류를 감지하는 것을 목표로 합니다. \"IneqMath\"에서 29개의 첨단 LLMs에 대한 체계적인 평가에 따라 놀라울 수 있는 사실이 밝혀졌습니다: 예를 들어, o1이나 같은 탑 모델은 단계별 검토에 의해 종합적인 정확도가 10% 미만에 도달합니다. 이는 최종적인 답변의 일관성을 고려한 정확도와 비교하여 65.5%의 저하가 됩니다. 이 차이는 단순히 답을 찾아내는 것이 아니라, 현재의 LLMs가严密한 증명을 구축하기 위한 중요한 결함이 밝혀졌습니다. 모델 크기의 확장과 테스트 시 계산량의 증가는 종합적인 증명의 정확성에만 한계가 있고, 거의 효과가 보입니다. 대신, 우리가 찾은 연구 방향은 정리를指导한 추론과 자기 수정 등 가능성을 보여주고 있습니다. 코드와 데이터는 https://ineqmath.github.io/에서 사용 가능합니다.",
      "upvotes": 9,
      "discussionId": "684794013ec10bdd8ab4de18",
      "ai_summary": "The investigation into inequality proving using large language models uncovers significant challenges in constructing rigorous proofs, revealing gaps between finding answers and generating valid step-wise solutions.",
      "ai_keywords": [
        "LLMs",
        "IneqMath",
        "bound estimation",
        "relation prediction",
        "theorem-guided reasoning",
        "self-refinement"
      ]
    },
    "publishedAt": "2025-06-09T12:43:38.000Z",
    "title": "Solving Inequality Proofs with Large Language Models",
    "summary": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60f5f68fa7fd83d025749234/ahvR-ZmwDrUNm3-jcQ4o1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07927.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f5f68fa7fd83d025749234",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
      "fullname": "Pan Lu",
      "name": "lupantech",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04614",
      "authors": [
        {
          "_id": "684921e342e4f9106973f3e7",
          "name": "Yuyang Wanyan",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3e8",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3e9",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ea",
          "name": "Haowei Liu",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3eb",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ec",
          "name": "Jiabo Ye",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ed",
          "name": "Yutong Kou",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ee",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3ef",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3f0",
          "name": "Xiaoshan Yang",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3f1",
          "name": "Weiming Dong",
          "hidden": false
        },
        {
          "_id": "684921e342e4f9106973f3f2",
          "name": "Changsheng Xu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/u6BK1EJr5-c6EUSfCkYqH.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/ySTNcOWpa_W2ZpbmkS50q.jpeg"
      ],
      "publishedAt": "2025-06-05T04:12:36.000Z",
      "submittedOnDailyAt": "2025-06-11T04:59:22.770Z",
      "title": "「플레이어가 뛸 때 볼 수 있는 것：GUI-Critic-R1 모델에 의한 GUI 자동화의 수술 전 오류 진단」",
      "submittedOnDailyBy": {
        "_id": "645b10e80c73ea27d13f7aca",
        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
        "isPro": false,
        "fullname": "xuhaiyang",
        "user": "xhyandwyy",
        "type": "user"
      },
      "summary": "최근, 다 모델 대 언어 모델(MLLMs)은 그래픽 사용자 인터페이스(GUI) 자동화 등 다양한 모델 논리 임무에 광범위하게 사용되고 있습니다. 일반적인 온라인 모델 태스크와 달리, GUI 자동화는 온라인 인터랙티브 환경에서 수행되고, 환경의 실시간 상태를 기반으로 단계별로 결정을 수행합니다. 이 작업은 각 단계의 결정 오류에 낮은 허용율로, 어떤 미스도 연속적으로 프로세스를 파괴하고, 삭제나 결제 등 불변적인 결과를招く 가능성이 있기 때문에 이러한 문제를 해결하기 위해 실제 실행 전에 유효한 피드백을 제공하는 예비적 평가 구조를 도입합니다. 특히, 잠재적인 결과를 고려하여 그래픽 사용자 인터페이스 평가 모델(GUI-Critic-R1)을 구축하기 위해 제안된 제안 의견을 기반으로 경사상대적 정책 최적화(S-GRPO) 전략을 제안합니다. 또한, 새로운 제안 보상을 포함하여 모델의 피드백의 신뢰성을 높이는 것을 목표로 합니다. 또한, GUI 평가 데이터의 기존 결함이 해결하기 위해 근거 기반의 데이터 수집 파이프라인을 개발하고, GUI-Critic-Train과 GUI-Critic-Test를 만듭니다. 모바일과 웹 도메인 모두의 GUI-Critic-Test에서静的 실험을 통해 현재의 MLLMs와 비교하여 큰 평가 정확도의 우위를 보여줍니다. GUI 자동화 벤치마크에서 동적 평가는 개선된 성공률과 동작 효율으로 모델의 효과와 우수한 성능을 더욱 증명합니다.",
      "upvotes": 9,
      "discussionId": "684921e442e4f9106973f3f3",
      "githubRepo": "https://github.com/X-PLUG/MobileAgent",
      "ai_summary": "A pre-operative critic mechanism with Suggestion-aware Gradient Relative Policy Optimization enhances the reliability of multimodal reasoning tasks in GUI automation.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Suggestion-aware Gradient Relative Policy Optimization",
        "pre-operative critic mechanism",
        "reasoning-bootstrapping",
        "GUI automation",
        "GUI-Critic-R1",
        "GUI-Critic-Test",
        "GUI-Critic-Train"
      ]
    },
    "publishedAt": "2025-06-05T00:12:36.000Z",
    "title": "Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation",
    "summary": "In recent years, Multimodal Large Language Models (MLLMs) have been\nextensively utilized for multimodal reasoning tasks, including Graphical User\nInterface (GUI) automation. Unlike general offline multimodal tasks, GUI\nautomation is executed in online interactive environments, necessitating\nstep-by-step decision-making based on real-time status of the environment. This\ntask has a lower tolerance for decision-making errors at each step, as any\nmistakes may cumulatively disrupt the process and potentially lead to\nirreversible outcomes like deletions or payments. To address these issues, we\nintroduce a pre-operative critic mechanism that provides effective feedback\nprior to the actual execution, by reasoning about the potential outcome and\ncorrectness of actions. Specifically, we propose a Suggestion-aware Gradient\nRelative Policy Optimization (S-GRPO) strategy to construct our pre-operative\ncritic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance\nthe reliability of the model's feedback. Furthermore, we develop a\nreasoning-bootstrapping based data collection pipeline to create a\nGUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic\ndata. Static experiments on the GUI-Critic-Test across both mobile and web\ndomains reveal that our GUI-Critic-R1 offers significant advantages in critic\naccuracy compared to current MLLMs. Dynamic evaluation on GUI automation\nbenchmark further highlights the effectiveness and superiority of our model, as\nevidenced by improved success rates and operational efficiency.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/u6BK1EJr5-c6EUSfCkYqH.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/ySTNcOWpa_W2ZpbmkS50q.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08002",
      "authors": [
        {
          "_id": "6848f8c242e4f9106973f2f6",
          "name": "Aadarsh Sahoo",
          "hidden": false
        },
        {
          "_id": "6848f8c242e4f9106973f2f7",
          "name": "Vansh Tibrewal",
          "hidden": false
        },
        {
          "_id": "6848f8c242e4f9106973f2f8",
          "name": "Georgia Gkioxari",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/2iX6eNaXCKBiwTpAmSk2Z.qt",
        "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/EUswqANZ-bRURwRZ0sv3m.qt",
        "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/dLiQzQHFyFye4cIgj3ivo.png"
      ],
      "publishedAt": "2025-06-09T17:59:37.000Z",
      "submittedOnDailyAt": "2025-06-11T02:05:41.608Z",
      "title": "텍스트, 이미지, 3D 구조를 토큰별로 대응시켜 줘",
      "submittedOnDailyBy": {
        "_id": "638e5fc6485360fbdfeb1301",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e5fc6485360fbdfeb1301/_8oqLMbkn_Ig-Jqa0fZCJ.png",
        "isPro": false,
        "fullname": "Aadarsh Sahoo",
        "user": "aadarsh99",
        "type": "user"
      },
      "summary": "3D 세계를 이해하는 기계를 만드는 것은 3D 환경과 편집을 담당하는 디자이너를 도와주는 중요한 일이다. 언어와 이미지 모델링의 발전으로 새로운 구조화된 3D 스케네의 모델 가능성을 검토할 수 있다. 이를 위해 언어, 이미지, 3D 스케네를 통합하는 LLM 프레임워크를 제안하고, 최적의 훈련과 성능을 달성하기 위한 중요한 설계 선택지를 명확히 하는 「책」을 제공하여 데이터 표현, 특정 목표의 모델화, 기타 중요한 질문에 대한 내용이다. 4가지 핵심적인 3D 작업( 렌더링, 인식, 지시 따라하기, 질문에 대한 답)과 4가지 3D 데이터셋(합성 데이터와 실세계 데이터)에서 성능을 평가한다. 3D 물체 모양을 구축하기 위해, 양수화된 모양 인코딩을 추가하여 실세계의 3D 물체 인식 작업에서 모델의 효과를 보여주고 있다. 프로젝트 페이지: https://glab-caltech.github.io/kyvo/",
      "upvotes": 8,
      "discussionId": "6848f8c242e4f9106973f2f9",
      "projectPage": "https://glab-caltech.github.io/kyvo/",
      "githubRepo": "https://github.com/AadSah/kyvo",
      "ai_summary": "A unified language, image, and 3D scene model framework is proposed, achieving optimal training and performance across various 3D tasks and datasets.",
      "ai_keywords": [
        "autoregressive models",
        "LLM framework",
        "data representation",
        "modality-specific objectives",
        "3D rendering",
        "3D recognition",
        "instruction-following",
        "question-answering",
        "3D datasets",
        "quantized shape encodings"
      ]
    },
    "publishedAt": "2025-06-09T13:59:37.000Z",
    "title": "Aligning Text, Images, and 3D Structure Token-by-Token",
    "summary": "Creating machines capable of understanding the world in 3D is essential in\nassisting designers that build and edit 3D environments and robots navigating\nand interacting within a three-dimensional space. Inspired by advances in\nlanguage and image modeling, we investigate the potential of autoregressive\nmodels for a new modality: structured 3D scenes. To this end, we propose a\nunified LLM framework that aligns language, images, and 3D scenes and provide a\ndetailed ''cookbook'' outlining critical design choices for achieving optimal\ntraining and performance addressing key questions related to data\nrepresentation, modality-specific objectives, and more. We evaluate performance\nacross four core 3D tasks -- rendering, recognition, instruction-following, and\nquestion-answering -- and four 3D datasets, synthetic and real-world. We extend\nour approach to reconstruct complex 3D object shapes by enriching our 3D\nmodality with quantized shape encodings, and show our model's effectiveness on\nreal-world 3D object recognition tasks. Project webpage:\nhttps://glab-caltech.github.io/kyvo/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/2iX6eNaXCKBiwTpAmSk2Z.qt",
      "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/EUswqANZ-bRURwRZ0sv3m.qt",
      "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/dLiQzQHFyFye4cIgj3ivo.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08002.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638e5fc6485360fbdfeb1301",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e5fc6485360fbdfeb1301/_8oqLMbkn_Ig-Jqa0fZCJ.png",
      "fullname": "Aadarsh Sahoo",
      "name": "aadarsh99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07177",
      "authors": [
        {
          "_id": "6849036342e4f9106973f32a",
          "name": "Sangwon Jang",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32b",
          "user": {
            "_id": "66b57c77778c98d29446c8ec",
            "avatarUrl": "/avatars/63a7da38ee3808858f0f786a3a4a8dae.svg",
            "isPro": false,
            "fullname": "Taekyung Ki",
            "user": "tkkitkki",
            "type": "user"
          },
          "name": "Taekyung Ki",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:34:42.228Z",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32c",
          "name": "Jaehyeong Jo",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32d",
          "user": {
            "_id": "652066649004117947e46ed6",
            "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg",
            "isPro": false,
            "fullname": "Jaehong Yoon",
            "user": "jaehong31",
            "type": "user"
          },
          "name": "Jaehong Yoon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:34:44.146Z",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32e",
          "name": "Soo Ye Kim",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f32f",
          "name": "Zhe Lin",
          "hidden": false
        },
        {
          "_id": "6849036342e4f9106973f330",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63bbf972d8d676a2299cdb44/n94aSArXRHEapWS5MwzkR.mp4"
      ],
      "publishedAt": "2025-06-08T14:54:41.000Z",
      "submittedOnDailyAt": "2025-06-11T02:49:49.936Z",
      "title": "프레임 가이드라인: 영상의 프레임 수준 제어를 위한 무학습 가이드라인의 분포 모델",
      "submittedOnDailyBy": {
        "_id": "63bbf972d8d676a2299cdb44",
        "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
        "isPro": false,
        "fullname": "Sangwon",
        "user": "agwmon",
        "type": "user"
      },
      "summary": "확산 모델의 발전은 이미지의 품질을 크게 향상시키고 미묘한 제어 가능성을 주목받게 되었습니다. 그러나 현재 많은 방법들은 특정 작업에 대한 대규모 이미지 모델의 미세 조정에 의존하지만, 모델의 크기가 계속 증가함에 따라 이 방법은 실질적이지 않습니다. 본 논문에서는 프레임 수준의 신호(예: 키프레임, 스타일 참조 이미지, 스케치, 또는 깊이맵 등)에 기반한 제어 가능한 이미지 생성의 훈련없이 가이드라인인 \"Frame Guidance\"를 제안합니다. 실질적인 훈련없이 가이드라인을 구현하기 위해 메모리 사용량을 크게 줄이는 간단한 잠재 변수 처리법을 제안하고, 일관된 이미지 생성을 위해 설계된 새로운 잠재 변수 최적화 전략을 적용합니다. Frame Guidance는 키프레임 가이드, 스타일화, 루프 등 다양한 작업에 효과적인 제어를 가능하게 하며, 훈련이 필요하지 않고, 어떤 이미지 모델과도 호환됩니다. 실험 결과는 Frame Guidance가 광범위한 작업과 입력 신호에 대해 고품질의 제어 이미지를 생성할 수 있음을 보여줍니다.",
      "upvotes": 8,
      "discussionId": "6849036342e4f9106973f331",
      "ai_summary": "Frame Guidance offers a training-free method for controlling video generation using frame-level signals, reducing memory usage and enhancing globally coherent video output.",
      "ai_keywords": [
        "diffusion models",
        "frame-level signals",
        "keyframes",
        "style reference images",
        "sketches",
        "depth maps",
        "latent processing",
        "latent optimization",
        "globally coherent video generation",
        "video models",
        "keyframe guidance",
        "stylization",
        "looping"
      ]
    },
    "publishedAt": "2025-06-08T10:54:41.000Z",
    "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video\n  Diffusion Models",
    "summary": "Advancements in diffusion models have significantly improved video quality,\ndirecting attention to fine-grained controllability. However, many existing\nmethods depend on fine-tuning large-scale video models for specific tasks,\nwhich becomes increasingly impractical as model sizes continue to grow. In this\nwork, we present Frame Guidance, a training-free guidance for controllable\nvideo generation based on frame-level signals, such as keyframes, style\nreference images, sketches, or depth maps. For practical training-free\nguidance, we propose a simple latent processing method that dramatically\nreduces memory usage, and apply a novel latent optimization strategy designed\nfor globally coherent video generation. Frame Guidance enables effective\ncontrol across diverse tasks, including keyframe guidance, stylization, and\nlooping, without any training, compatible with any video models. Experimental\nresults show that Frame Guidance can produce high-quality controlled videos for\na wide range of tasks and input signals.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63bbf972d8d676a2299cdb44/n94aSArXRHEapWS5MwzkR.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07177.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbf972d8d676a2299cdb44",
      "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
      "fullname": "Sangwon",
      "name": "agwmon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05167",
      "authors": [
        {
          "_id": "68468cb23ec10bdd8ab4db5b",
          "user": {
            "_id": "645aedd221ab438e732bff43",
            "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
            "isPro": false,
            "fullname": "Yeonseok Jeong",
            "user": "yeonseokjeong",
            "type": "user"
          },
          "name": "Yeonseok Jeong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:11.745Z",
          "hidden": false
        },
        {
          "_id": "68468cb23ec10bdd8ab4db5c",
          "name": "Jinsu Kim",
          "hidden": false
        },
        {
          "_id": "68468cb23ec10bdd8ab4db5d",
          "name": "Dohyeon Lee",
          "hidden": false
        },
        {
          "_id": "68468cb23ec10bdd8ab4db5e",
          "name": "Seung-won Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T15:43:49.000Z",
      "submittedOnDailyAt": "2025-06-11T00:47:14.627Z",
      "title": "ECoRAG: 증거성 가이드를 따른 긴 문맥 RAG의 압축",
      "submittedOnDailyBy": {
        "_id": "645aedd221ab438e732bff43",
        "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
        "isPro": false,
        "fullname": "Yeonseok Jeong",
        "user": "yeonseokjeong",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 외부 문서의 활용을 통해, 레치우스 아우가더 디시션(RAG)를 통해, 개방 영역의 질문 대답(ODQA)에서 놀라울만한 성능을 보여주고 있습니다. RAG의 오버헤드를 줄이기 위해, 긴 컨텍스트를 줄이기 위한 컨텍스트 구성이 필요합니다. 그러나 지난 주의 구성 방법은 비증명적인 정보를 필터링하지 못하여, LLM 기반의 RAG의 성능이 제한되어 있습니다. 이에 따라, 우리는 증명에 기반한 RAG, ECoRAG 프레임워크를 제안합니다. ECoRAG는 증명에 기반하여 검색된 문서를 구성하고, 대답 생성이 정확한 증명으로 지원되는지 확인합니다. 추가적인 단계로, ECoRAG는 압축된 내용이 충분한 증명을 제공하는지 확인하고, 충분하지 않으면 추가적인 검색을 수행합니다. 실험은 ECoRAG가 ODQA 태스크에서 LLM의 성능을 향상시키고, 기존의 구성 방법을 초과하는 것을 보여주고 있습니다. 또한, ECoRAG는 필요한 정보만 남겨서 정확한 대답을 생성하기 위해 토큰 사용량을 최소화하고, 라틴 시를 줄이며, 고차원적으로 비용 효율적이며, 코드는 https://github.com/ldilab/ECoRAG 에 액세스할 수 있습니다.",
      "upvotes": 6,
      "discussionId": "68468cb23ec10bdd8ab4db5f",
      "githubRepo": "https://github.com/ldilab/ECoRAG",
      "ai_summary": "ECoRAG framework enhances LLM performance in ODQA by compressing retrieved documents based on evidentiality, reducing latency and token usage.",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "context compression",
        "evidentiality",
        "LLM",
        "Open-Domain Question Answering (ODQA)"
      ]
    },
    "publishedAt": "2025-06-05T11:43:49.000Z",
    "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
    "summary": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\nECoRAG framework. ECoRAG improves LLM performance by compressing retrieved\ndocuments based on evidentiality, ensuring whether answer generation is\nsupported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645aedd221ab438e732bff43",
      "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
      "fullname": "Yeonseok Jeong",
      "name": "yeonseokjeong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08887",
      "authors": [
        {
          "_id": "6848ec1542e4f9106973f2ac",
          "user": {
            "_id": "6364b81b3e248b1e28a68b26",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6364b81b3e248b1e28a68b26/9pd6zfH3HGx1gQkYuL3pR.png",
            "isPro": false,
            "fullname": "LeqiShen",
            "user": "lunar677",
            "type": "user"
          },
          "name": "Leqi Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:05.097Z",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2ad",
          "name": "Guoqiang Gong",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2ae",
          "name": "Tianxiang Hao",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2af",
          "name": "Tao He",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b0",
          "name": "Yifeng Zhang",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b1",
          "name": "Pengzhang Liu",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b2",
          "name": "Sicheng Zhao",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b3",
          "name": "Jungong Han",
          "hidden": false
        },
        {
          "_id": "6848ec1542e4f9106973f2b4",
          "name": "Guiguang Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T15:16:40.000Z",
      "submittedOnDailyAt": "2025-06-11T01:17:30.703Z",
      "title": "디스코브ラ: 시각, 언어와 어레이멘트의 오류 제거에 의한 파라미터 효율적인 비디오 텍스트 검색",
      "submittedOnDailyBy": {
        "_id": "6364b81b3e248b1e28a68b26",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6364b81b3e248b1e28a68b26/9pd6zfH3HGx1gQkYuL3pR.png",
        "isPro": false,
        "fullname": "LeqiShen",
        "user": "lunar677",
        "type": "user"
      },
      "summary": "그림 - 문언 전 학습 모델 CLIP의 파라미터 효율적인 적용을 비디오 - 문언 검색에 적용한 연구는 중요한 분야 중 하나입니다. CLIP는 이미지 수준의 시각 언어 매칭에 초점을 맞추고 있지만, 비디오 - 문언 검색은 비디오 수준의 전체적인 이해를 요구하고 있습니다. 이미지 수준에서 비디오 수준으로의 전달에는 시각, 언어, aligment의 3가지의 차이점이 나오지만, 현재의 방법은 주로 시각을 초점을 두고 언어와 aligment를 관점에서 제외하고 있습니다. 본 논문에서는 시각, 언어, aligment의 3가지의 차이점을 동시에 줄이기 위한 Vision, Language, and Alignment의 Discrepancy Reduction (DiscoVLA)를 제안합니다. 특히, 이미지 수준과 비디오 수준의 특징량을 통합하는 이미지-비디오 특징량 통합을 도입하고 시각과 언어의 차이점을 효과적으로 해결합니다. 또한, 이미지 수준의 aligment를 학습하기 위해 가짜 이미지 캡처를 생성합니다. aligment의 차이점을 줄이기 위해 이미지 수준의 aligment 지식을 활용하는 이미지-비디오 aligment 디스틸루션을 제안합니다. 확장된 실험은 DiscoVLA의 우수한 성능을 보여주며, 특히 MSRVTT에서 CLIP(ViT-B/16)를 사용했을 때, DiscoVLA는 이전 방법보다 R@1에서 1.5%의 개선을 거뒀으며, 최종적인 점수는 50.5%의 R@1을 달성합니다. 코드는 https://github.com/LunarShen/DsicoVLA에 공개되어 있습니다.",
      "upvotes": 4,
      "discussionId": "6848ec1642e4f9106973f2b5",
      "githubRepo": "https://github.com/LunarShen/DsicoVLA",
      "ai_summary": "The paper proposes DiscoVLA to improve video-text retrieval using CLIP by addressing vision, language, and alignment discrepancies, achieving superior performance.",
      "ai_keywords": [
        "parameter-efficient adaptation",
        "image-text pretraining model",
        "CLIP",
        "video-text retrieval",
        "vision",
        "language",
        "alignment",
        "Image-Video Features Fusion",
        "pseudo image captions",
        "Image-to-Video Alignment Distillation",
        "MSRVTT",
        "R@1"
      ]
    },
    "publishedAt": "2025-06-10T11:16:40.000Z",
    "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for\n  Parameter-Efficient Video-Text Retrieval",
    "summary": "The parameter-efficient adaptation of the image-text pretraining model CLIP\nfor video-text retrieval is a prominent area of research. While CLIP is focused\non image-level vision-language matching, video-text retrieval demands\ncomprehensive understanding at the video level. Three key discrepancies emerge\nin the transfer from image-level to video-level: vision, language, and\nalignment. However, existing methods mainly focus on vision while neglecting\nlanguage and alignment. In this paper, we propose Discrepancy Reduction in\nVision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all\nthree discrepancies. Specifically, we introduce Image-Video Features Fusion to\nintegrate image-level and video-level features, effectively tackling both\nvision and language discrepancies. Additionally, we generate pseudo image\ncaptions to learn fine-grained image-level alignment. To mitigate alignment\ndiscrepancies, we propose Image-to-Video Alignment Distillation, which\nleverages image-level alignment knowledge to enhance video-level alignment.\nExtensive experiments demonstrate the superiority of our DiscoVLA. In\nparticular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous\nmethods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is\navailable at https://github.com/LunarShen/DsicoVLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08887.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6364b81b3e248b1e28a68b26",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6364b81b3e248b1e28a68b26/9pd6zfH3HGx1gQkYuL3pR.png",
      "fullname": "LeqiShen",
      "name": "lunar677",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07932",
      "authors": [
        {
          "_id": "68487f6342e4f9106973f17a",
          "user": {
            "_id": "60796959c59d9e1697fa2324",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
            "isPro": false,
            "fullname": "Rishit Dagli",
            "user": "rishitdagli",
            "type": "user"
          },
          "name": "Rishit Dagli",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:32.138Z",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17b",
          "name": "Yushi Guan",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17c",
          "name": "Sankeerth Durvasula",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17d",
          "name": "Mohammadreza Mofayezi",
          "hidden": false
        },
        {
          "_id": "68487f6342e4f9106973f17e",
          "name": "Nandita Vijaykumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T16:52:10.000Z",
      "submittedOnDailyAt": "2025-06-11T02:25:06.388Z",
      "title": "Squeeze3D: 당신의 3D 생성 모델은 사실상 강력한 뉴럴 컴팩터로 은닉되어 있습니다.",
      "submittedOnDailyBy": {
        "_id": "60796959c59d9e1697fa2324",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
        "isPro": false,
        "fullname": "Rishit Dagli",
        "user": "rishitdagli",
        "type": "user"
      },
      "summary": "Squeeze3D는 기존 학습된 3D 생성 모델이 학습한 은닉 벡터 공간을 활용하여, 3D 데이터를 매우 높은 압축률로 압축하는 새로운 프레임워크입니다. 우리의 접근 방식은 학습 가능한 매핑 네트워크를 사용하여, 기존 학습된 인코더와 생성 모델의 은닉 벡터 공간을 연결합니다. 매핑 네트워크는 3D 모델(마치, 포인트 클러스터, 또는 라디언 필드)을 압축한 은닉 벡터를 생성 모델의 은닉 벡터 공간에 변환합니다. 이 은닉 벡터는非常有용하게 마치나 포인트 클러스터의 매우 압축된 표현으로 사용될 수 있습니다. Squeeze3D는 생성된 합성 데이터만 사용하여 학습되며, 3D 데이터 세트가 필요하지 않습니다. Squeeze3D의 구조는 기존 학습된 3D 인코더와 생성 모델을 함께 변형할 수 있으며, 마치, 포인트 클러스터, 또는 라디언 필드의 형식을 유연하게 지원할 수 있습니다. 실험에 따르면, Squeeze3D는 텍스트付き 마치에서 2187배, 포인트 클러스터에서 55배, 라디언 필드에서는 619배의 압축률을 달성하며, 많은 기존 방법과 비교하여 시각적 품질을 유지할 수 있습니다. Squeeze3D는 대상 객체의 특화된 네트워크를 학습할 필요가 없기 때문에, 압축 및 解圧縮의 지연이 작아집니다.",
      "upvotes": 2,
      "discussionId": "68487f6442e4f9106973f17f",
      "projectPage": "https://squeeze3d.github.io/",
      "ai_summary": "A novel framework called Squeeze3D uses pre-trained models to compress 3D data efficiently, achieving high compression ratios while maintaining visual quality.",
      "ai_keywords": [
        "pre-trained 3D generative models",
        "latent spaces",
        "encode",
        "latent code",
        "mapping networks",
        "radiance fields",
        "synthetic data",
        "compression ratios",
        "visual quality",
        "compression latency",
        "decompression latency"
      ]
    },
    "publishedAt": "2025-06-09T12:52:10.000Z",
    "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor",
    "summary": "We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07932.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60796959c59d9e1697fa2324",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
      "fullname": "Rishit Dagli",
      "name": "rishitdagli",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05928",
      "authors": [
        {
          "_id": "6847b3393ec10bdd8ab4df20",
          "user": {
            "_id": "65ea90741b0d7e029a3a1fb0",
            "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
            "isPro": false,
            "fullname": "cj",
            "user": "cajie",
            "type": "user"
          },
          "name": "Jie Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:42:50.209Z",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df21",
          "name": "Tianwei Lin",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df22",
          "name": "Hongyang He",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df23",
          "name": "Rolan Yan",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df24",
          "name": "Wenqiao Zhang",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df25",
          "name": "Juncheng Li",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df26",
          "name": "Dongping Zhang",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df27",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "6847b3393ec10bdd8ab4df28",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T09:54:19.000Z",
      "submittedOnDailyAt": "2025-06-11T01:28:46.209Z",
      "title": "모아: 하이브리드 어댑터混合 모델의 파라미터 효과적인\n  대규모 언어 모델의 미세 조정",
      "submittedOnDailyBy": {
        "_id": "65ea90741b0d7e029a3a1fb0",
        "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
        "isPro": false,
        "fullname": "cj",
        "user": "cajie",
        "type": "user"
      },
      "summary": "최근의 연구는 Low-Rank Adaptation (LoRA)와 Mixture-of-Experts (MoE)를 통합하여 Large Language Model (LLM) 애플리케이션에서 파라미터 효율적인 微調節 (PEFT) 방법의 성능을 한 단계 더 향상시키기 위해 노력하고 있습니다. 현재의 방법들은 유사한 구조와 능력의 LoRA Expert를 가진 균일한 MoE-LoRA 아키텍처로 구성되어 있습니다. 그러나 이러한 접근法是 LLM의 잠재적 성능을 감소시킬 수 있는 표현 붕괴와 Expert의 부하 불균형에 의해 영향을 받습니다. 이러한挑戦에 대처하기 위해 우리는 heterogeneous Mixture-of-Adapters (MoA) 접근법을 제안하고 있습니다.\n\n이 방법은 다양한 구조의 PEFT Adapter Expert를 동적으로 통합하고 이들의 보간 표현 능력을 활용하여 Expert의 Specialization을 촉진하고, 이는 사전 학습된 지식의 효과적인 전달을 하류 태스크에 맞게 향상시킬 수 있습니다. MoA는 두 가지 버전을 지원합니다:\n(i) Soft MoA는 모든 Expert의 출력을 가중합을 통해 Fine-Grained 통합을 실현합니다.\n(ii) Sparse MoA는 Expert의 기여에 따라 희소하게 활성화되며, 이는 미세 조정 가능한 성능 저하 없이 실현될 수 있습니다.\n\n실험 결과는 heterogeneous MoA가 homogeneous MoE-LoRA 방법보다 성능과 파라미터 효율성을 모두 초과한다는 것을 보여줍니다. 우리의 프로젝트는 https://github.com/DCDmllm/MoA에 접근할 수 있습니다.",
      "upvotes": 2,
      "discussionId": "6847b3393ec10bdd8ab4df29",
      "githubRepo": "https://github.com/DCDmllm/MoA",
      "ai_summary": "A heterogeneous Mixture-of-Adapters (MoA) approach enhances parameter-efficient fine-tuning in LLMs by integrating diverse adapter experts, outperforming homogeneous MoE-LoRA methods.",
      "ai_keywords": [
        "Low-Rank Adaptation",
        "Mixture-of-Experts",
        "parameter-efficient fine-tuning",
        "Large Language Model",
        "homogeneous",
        "representation collapse",
        "expert load imbalance",
        "heterogeneous",
        "Mixture-of-Adapters",
        "soft MoA",
        "sparse MoA"
      ]
    },
    "publishedAt": "2025-06-06T05:54:19.000Z",
    "title": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient\n  Fine-Tuning of Large Language Models",
    "summary": "Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts\n(MoE) to further enhance the performance of parameter-efficient fine-tuning\n(PEFT) methods in Large Language Model (LLM) applications. Existing methods\nemploy homogeneous MoE-LoRA architectures composed of LoRA experts with\neither similar or identical structures and capacities. However, these\napproaches often suffer from representation collapse and expert load imbalance,\nwhich negatively impact the potential of LLMs. To address these challenges, we\npropose a heterogeneous Mixture-of-Adapters (MoA) approach.\nThis method dynamically integrates PEFT adapter experts with diverse\nstructures, leveraging their complementary representational capabilities to\nfoster expert specialization, thereby enhancing the effective transfer of\npre-trained knowledge to downstream tasks. MoA supports two variants:\n(i) Soft MoA achieves fine-grained integration by performing\na weighted fusion of all expert outputs; (ii) Sparse MoA\nactivates adapter experts sparsely based on their contribution, achieving this\nwith negligible performance degradation. Experimental results demonstrate that\nheterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance\nand parameter efficiency. Our project is available at\nhttps://github.com/DCDmllm/MoA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05928.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ea90741b0d7e029a3a1fb0",
      "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
      "fullname": "cj",
      "name": "cajie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08300",
      "authors": [
        {
          "_id": "684954493614057188acbf5a",
          "name": "Matteo Cargnelutti",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5b",
          "name": "Catherine Brobston",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5c",
          "name": "John Hess",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5d",
          "name": "Jack Cushman",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5e",
          "name": "Kristi Mukk",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf5f",
          "name": "Aristana Scourtas",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf60",
          "name": "Kyle Courtney",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf61",
          "name": "Greg Leppert",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf62",
          "name": "Amanda Watson",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf63",
          "name": "Martha Whitehead",
          "hidden": false
        },
        {
          "_id": "684954493614057188acbf64",
          "name": "Jonathan Zittrain",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T00:11:30.000Z",
      "submittedOnDailyAt": "2025-06-11T08:37:11.451Z",
      "title": "인스튜셜 벅스 1.0: 하버드 도서관의 컬렉션에서 242B 토큰의 데이터 세트, 정확성과 활용 가능성에 적합한 것",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 데이터를 사용하여 세계에 대해 학습하고 의미 있는 관련성과 예측을 생성하는 것을 목표로 합니다. 이에 따라, 이러한 모델을 훈련하거나 추론 시에 사용할 수 있는 데이터 세트의 특성, 규모, 품질 및 다양성이 그 품질을 직접적으로 영향을 미칩니다. 품질이 다른 LLMs의 급격한 개발과 도입은 공개적으로 사용할 수 있는 고품질의 훈련 데이터의 부족을 중점적으로 다루고, 이러한 데이터 세트의 관리자에게 지속적인 실천에 기초할 필요가 있음을 긴급하게 지적했습니다. 그러나 이 기술 보고서에서는, 2006년부터 Gitter Book 서비스 프로젝트에 의해 하버드 도서관은 처음으로 디지털화된 Institutional Books 1.0, 대규모 공개 영역의 책의 컬렉션을 소개합니다. 하버드 도서관과 협력하여, 이러한 책을 추출하고 분석하고 기록적으로 기록된 데이터 세트로 변환했습니다. 이 분석은 이 프로젝트에서 하버드 도서관의 전체 컬렉션을 커버하며, 처음으로 1,075,899권의 책이 250가지 이상의 언어로 쓰여져 있으며, 약 2500억 토큰을 초과했습니다. 이 초기 릴리스의 일부는 983,004권의 책(242B 토큰)의 OCR 추출 텍스트(원본과 후처리된 것) 및 메타 데이터(비디오 디카테로그, 소스, 생성된 것)가 공개되었습니다. 이 보고서는 이 프로젝트의 목적과 방법을 설명하고, 우리가 수행한 분석의 결과를 요약하며, 이 역사적인 컬렉션을 더 접근 가능한 상태로 만들고, 인간이나 기계 모두가 필터링, 읽기, 사용 가능한 방법을 제공하는 지원을 제공합니다.",
      "upvotes": 1,
      "discussionId": "684954493614057188acbf65",
      "ai_summary": "Institutional Books 1.0 provides a large dataset of public domain books from Harvard Library for training and inference of large language models, enhancing data accessibility and sustainability.",
      "ai_keywords": [
        "large language models",
        "datasets",
        "institutional books",
        "public domain",
        "harrass library",
        "google books project",
        "ocr",
        "metadata",
        "historic texts"
      ]
    },
    "publishedAt": "2025-06-09T20:11:30.000Z",
    "title": "Institutional Books 1.0: A 242B token dataset from Harvard Library's\n  collections, refined for accuracy and usability",
    "summary": "Large language models (LLMs) use data to learn about the world in order to\nproduce meaningful correlations and predictions. As such, the nature, scale,\nquality, and diversity of the datasets used to train these models, or to\nsupport their work at inference time, have a direct impact on their quality.\nThe rapid development and adoption of LLMs of varying quality has brought into\nfocus the scarcity of publicly available, high-quality training data and\nrevealed an urgent need to ground the stewardship of these datasets in\nsustainable practices with clear provenance chains. To that end, this technical\nreport introduces Institutional Books 1.0, a large collection of public domain\nbooks originally digitized through Harvard Library's participation in the\nGoogle Books project, beginning in 2006. Working with Harvard Library, we\nextracted, analyzed, and processed these volumes into an extensively-documented\ndataset of historic texts. This analysis covers the entirety of Harvard\nLibrary's collection scanned as part of that project, originally spanning\n1,075,899 volumes written in over 250 different languages for a total of\napproximately 250 billion tokens. As part of this initial release, the\nOCR-extracted text (original and post-processed) as well as the metadata\n(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,\nidentified as being in the public domain have been made available. This report\ndescribes this project's goals and methods as well as the results of the\nanalyses we performed, all in service of making this historical collection more\naccessible and easier for humans and machines alike to filter, read and use.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08300.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2742
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07047",
      "authors": [
        {
          "_id": "68492bf142e4f9106973f411",
          "name": "Yu Xuejun",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f412",
          "user": {
            "_id": "6608fa4f5baec84322ec85ea",
            "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
            "isPro": false,
            "fullname": "Zhong",
            "user": "Jianyuan1",
            "type": "user"
          },
          "name": "Jianyuan Zhong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:34:08.719Z",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f413",
          "name": "Zijin Feng",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f414",
          "name": "Pengyi Zhai",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f415",
          "name": "Roozbeh Yousefzadeh",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f416",
          "name": "Wei Chong Ng",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f417",
          "name": "Haoxiong Liu",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f418",
          "name": "Ziyi Shou",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f419",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41a",
          "name": "Yudong Zhou",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41b",
          "name": "Claudia Beth Ong",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41c",
          "name": "Austen Jeremy Sugiarto",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41d",
          "name": "Yaoxi Zhang",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41e",
          "name": "Wai Ming Tai",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f41f",
          "name": "Huan Cao",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f420",
          "name": "Dongcai Lu",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f421",
          "name": "Jiacheng Sun",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f422",
          "name": "Qiang Xu",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f423",
          "name": "Shen Xin",
          "hidden": false
        },
        {
          "_id": "68492bf142e4f9106973f424",
          "name": "Zhenguo Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-08T09:04:14.000Z",
      "submittedOnDailyAt": "2025-06-11T05:45:24.452Z",
      "title": "마지셈스: 자연어로부터 형식적인 정리 증명에 대한 연구\n\n(Note: The translation is provided as requested, without any additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "6608fa4f5baec84322ec85ea",
        "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
        "isPro": false,
        "fullname": "Zhong",
        "user": "Jianyuan1",
        "type": "user"
      },
      "summary": "최근의 대규모 언어 모델의 발전은 형식적인 이유에 강한 가능성을 보여주고 있습니다. 그러나 대부분의 LLM 기반의 정리 증명기는 전문가가 작성한 형식적인 설명을 입력으로 필요로 하여, 자연어로 표현된 본격적인 문제를 적용하는 범위가 좁아지는 현상이 발생했습니다. 우리는 이러한 공백을 메우기 위해, 첫 번째부터 마지막까지의 정리 증명 패이프로리ン을 도입하여, Mathesis라는 것을 도입했습니다. 이는 자연어의 문제 설명의 형식화 능력을 강화하기 위해, 강화 학습을 사용한 첫 번째 자동 형식화기인 Mathesis-Autoformalizer를 제공합니다. 또한, 우리의 새로운 LeanScorer 프레임워크를 사용하여, 복잡한 형식화 품질 평가에 지원을 합니다. 또한, Mathesis-Prover를 제안합니다. 이는 형식화된 설명으로부터 형식적인 증명을 생성합니다. 형식적인 정리 증명의 본격적인 적용 가능성을 평가하기 위해, 488개의 복잡한 문제를 구성한 Gaokao-Formal 벤치마크를 도입했습니다. 우리의 접근 방식은 각 구성 요소의 세부적인 연구를 통해 신중하게 설계되어 있습니다. 실험은 Mathesis의 효과를 보여주고, Gaokao-Formal에서 22%를 초과하는 최고의 베이스라인을 달성했습니다. 전체 시스템은 MiniF2F에서 64%의 정확도를 달성하고, Gaokao-Formal에서 가장 先端的 18%를 달성했습니다.",
      "upvotes": 1,
      "discussionId": "68492bf142e4f9106973f425",
      "githubRepo": "https://github.com/Huawei-AI4Math/Mathesis"
    },
    "publishedAt": "2025-06-08T05:04:14.000Z",
    "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages",
    "summary": "Recent advances in large language models show strong promise for formal\nreasoning. However, most LLM-based theorem provers have long been constrained\nby the need for expert-written formal statements as inputs, limiting their\napplicability to real-world problems expressed in natural language. We tackle\nthis gap with Mathesis, the first end-to-end theorem proving pipeline\nprocessing informal problem statements. It contributes Mathesis-Autoformalizer,\nthe first autoformalizer using reinforcement learning to enhance the\nformalization ability of natural language problems, aided by our novel\nLeanScorer framework for nuanced formalization quality assessment. It also\nproposes a Mathesis-Prover, which generates formal proofs from the formalized\nstatements. To evaluate the real-world applicability of end-to-end formal\ntheorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex\nproblems from China's national college entrance exam. Our approach is carefully\ndesigned, with a thorough study of each component. Experiments demonstrate\nMathesis's effectiveness, with the autoformalizer outperforming the best\nbaseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other\nmodel combinations, achieving 64% accuracy on MiniF2F with pass@32 and a\nstate-of-the-art 18% on Gaokao-Formal.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07047.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6608fa4f5baec84322ec85ea",
      "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
      "fullname": "Zhong",
      "name": "Jianyuan1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05700",
      "authors": [
        {
          "_id": "6848de6e42e4f9106973f273",
          "user": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "name": "Yan Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-11T01:39:59.328Z",
          "hidden": false
        },
        {
          "_id": "6848de6e42e4f9106973f274",
          "name": "Yueru He",
          "hidden": false
        },
        {
          "_id": "6848de6e42e4f9106973f275",
          "name": "Ruoyu Xiang",
          "hidden": false
        },
        {
          "_id": "6848de6e42e4f9106973f276",
          "name": "Jeff Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T03:02:52.000Z",
      "submittedOnDailyAt": "2025-06-11T00:15:04.904Z",
      "title": "ルール知識를 강화한 대규모 언어 모델",
      "submittedOnDailyBy": {
        "_id": "65d76cc5b9b7b8bf88faa916",
        "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
        "isPro": true,
        "fullname": "Yan Wang",
        "user": "YanAdjeNole",
        "type": "user"
      },
      "summary": "최근의 대규모 언어 모델(LLMs)의 발전은 금융 응용 분야에 큰 가능성을 가지고 있습니다 however, 디지털 규제 보고서(DRR)에서의 정확성과 위반의 문제가 중요한 문제로 자리잡았습니다. 이러한 문제를 대처하기 위해 우리는 Fino1에 기반한 규제 지식을 강화한 금융 논리 모델인 RKEFino1을 제안합니다. 이 모델은 XBRL, CDM, MOF로부터의 영역 지식으로 미세 조정되어 있습니다. 우리는 지식 기반과 수학 논리의 2가지 QA 태스크를 구성하고, 문과 테이블에 있는 금융적인 엔티티를 커버하는 새로운 수치 NER 태스크를 도입했습니다. 실험 결과를 통해 RKEFino1이 위반의 중요한 금융 태스크에 대한 효과와 일반화 능력을 보여주는 것을 밝혀줍니다. 우리의 모델은 Hugging Face에서 공개되어 있습니다.",
      "upvotes": 1,
      "discussionId": "6848de6e42e4f9106973f277",
      "ai_summary": "RKEFino1, a regulation-aware LLM fine-tuned with financial domain knowledge, effectively handles compliance-critical tasks including QA and numerical NER.",
      "ai_keywords": [
        "Large language models",
        "financial reasoning",
        "Digital Regulatory Reporting",
        "regulation knowledge-enhanced",
        "fine-tuning",
        "domain knowledge",
        "XBRL",
        "CDM",
        "MOF",
        "QA tasks",
        "knowledge-based reasoning",
        "mathematical reasoning",
        "Numerical NER",
        "financial entities"
      ]
    },
    "publishedAt": "2025-06-05T23:02:52.000Z",
    "title": "RKEFino1: A Regulation Knowledge-Enhanced Large Language Model",
    "summary": "Recent advances in large language models (LLMs) hold great promise for\nfinancial applications but introduce critical accuracy and compliance\nchallenges in Digital Regulatory Reporting (DRR). To address these issues, we\npropose RKEFino1, a regulation knowledge-enhanced financial reasoning model\nbuilt upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We\nformulate two QA tasks-knowledge-based and mathematical reasoning-and introduce\na novel Numerical NER task covering financial entities in both sentences and\ntables. Experimental results demonstrate the effectiveness and generalization\ncapacity of RKEFino1 in compliance-critical financial tasks. We have released\nour model on Hugging Face.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05700.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d76cc5b9b7b8bf88faa916",
      "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
      "fullname": "Yan Wang",
      "name": "YanAdjeNole",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07976",
      "authors": [
        {
          "_id": "68487bd642e4f9106973f16d",
          "name": "Junhong Shen",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f16e",
          "user": {
            "_id": "62927c2e56fedc76e396b3ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678105603200-62927c2e56fedc76e396b3ca.jpeg",
            "isPro": false,
            "fullname": "HAO BAI",
            "user": "JackBAI",
            "type": "user"
          },
          "name": "Hao Bai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T10:08:42.049Z",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f16f",
          "name": "Lunjun Zhang",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f170",
          "name": "Yifei Zhou",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f171",
          "name": "Amrith Setlur",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f172",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f173",
          "name": "Diego Caples",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f174",
          "name": "Nan Jiang",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f175",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f176",
          "name": "Ameet Talwalkar",
          "hidden": false
        },
        {
          "_id": "68487bd642e4f9106973f177",
          "name": "Aviral Kumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:50:02.000Z",
      "submittedOnDailyAt": "2025-06-11T08:38:04.852Z",
      "title": "Thinking vs. Doing: 테스트 시간 인터랙션을 스케일링하는 이유에 의한 아웃풋",
      "submittedOnDailyBy": {
        "_id": "62927c2e56fedc76e396b3ca",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678105603200-62927c2e56fedc76e396b3ca.jpeg",
        "isPro": false,
        "fullname": "HAO BAI",
        "user": "JackBAI",
        "type": "user"
      },
      "summary": "현재의 테스트 시간 스케일링 패러다임은 답을 생성하기 전에 긴 논리적 추론의 トレース를 생성하는 것이 대부분입니다. 상호 교환 문제에서는 이러한 추론의 トレース를 생성하여 세계에서 행동하기 전에 생각하는 것을 할 수 있습니다. 그러나 이 과정은 탐색, 후퇴, 동적 재계획 등 다양한 행동을 수행할 수 있는 OUTPUT 로더 내부에서 실행할 수 없습니다. 본 논문에서는 테스트 시간 상호작용 스케일링을 제안합니다. 이는 테스트 시간 스케일링의 개발되지 않은 차원에서 OUTPUT 로더 내부에서 탐색, 후퇴, 동적 재계획 등 다양한 행동을 수행할 수 있습니다. 이 스케일링 차원의 가능성을 보여주기 위해 웹 OUTPUT 로더 영역을 중심으로 집중합니다. 먼저, 학습 없이 Prompt 기반의 상호작용 스케일링에서 작업 성공률이 단순하게 상승함을 보여줍니다. 이를 통해 TTI (테스트 시간 상호작용)을 도입하고, 온라인 재학습 효율성 학습 (RL)의 클레크르브 기반 접근으로 OUTPUT 로더의 길이를 적응적으로 조정하여 OUTPUT 로더를 학습시킵니다. 쥐 3 12B 모델을 사용하여, TTI는 웹로이더와 웹アレ나 벤치마크에서 가장 선진한 오픈 소스, 오픈 데이터베이스의 웹 OUTPUT 로더를 생성합니다. 또한, TTI는 OUTPUT 로더 내부에서 탐색과 사용의 적응적인 균형을 유지할 수 있음을 보여줍니다. 우리의 결과를 통해, 상호작용 스케일링은 개별 단계의 계산량 스케일링의 강력한 보조 축으로 효과적으로 작용하고, OUTPUT 로더를 학습시키는 새로운 길을 제공합니다.",
      "upvotes": 0,
      "discussionId": "68487bd742e4f9106973f178",
      "ai_summary": "Test-Time Interaction (TTI) improves web agent performance by scaling interaction, enabling adaptive behavior and balancing exploration and exploitation without adding per-step compute.",
      "ai_keywords": [
        "test-time scaling",
        "thinking traces",
        "agent interaction",
        "interaction horizon",
        "exploration",
        "backtracking",
        "dynamic re-planning",
        "rollout",
        "curriculum-based online reinforcement learning (RL)",
        "Gemma 3 12B model",
        "WebVoyager",
        "WebArena",
        "adaptive agents"
      ]
    },
    "publishedAt": "2025-06-09T13:50:02.000Z",
    "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
    "summary": "The current paradigm of test-time scaling relies on generating long reasoning\ntraces (\"thinking\" more) before producing a response. In agent problems that\nrequire interaction, this can be done by generating thinking traces before\nacting in the world. However, this process does not allow agents to acquire new\ninformation from the environment or adapt their behavior over time. In this\nwork, we propose to scale test-time interaction, an untapped dimension of\ntest-time scaling that increases the agent's interaction horizon to enable\nrunning rich behaviors such as exploration, backtracking, and dynamic\nre-planning within a single rollout. To demonstrate the promise of this scaling\ndimension, we study the domain of web agents. We first show that even\nprompting-based interaction scaling without any training can improve task\nsuccess on web benchmarks non-trivially. Building on this, we introduce TTI\n(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)\napproach that trains agents by adaptively adjusting their rollout lengths.\nUsing a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data\nweb agents on WebVoyager and WebArena benchmarks. We further show that TTI\nenables agents to balance exploration and exploitation adaptively. Our results\nestablish interaction scaling as a powerful, complementary axis to scaling\nper-step compute, offering new avenues for training adaptive agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07976.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62927c2e56fedc76e396b3ca",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678105603200-62927c2e56fedc76e396b3ca.jpeg",
      "fullname": "HAO BAI",
      "name": "JackBAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]