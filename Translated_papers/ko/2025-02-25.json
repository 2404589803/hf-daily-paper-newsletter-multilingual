[
  {
    "paper": {
      "id": "2502.17157",
      "authors": [
        {
          "_id": "67bd3285ac4a596a43b53205",
          "user": {
            "_id": "646efd223dd912a539e0bd46",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
            "isPro": true,
            "fullname": "Canyu Zhao",
            "user": "Canyu",
            "type": "user"
          },
          "name": "Canyu Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:20.829Z",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53206",
          "name": "Mingyu Liu",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53207",
          "user": {
            "_id": "64d60375d7e30889c65e8cf4",
            "avatarUrl": "/avatars/640f7c570fc45194557ce7931bdfe87f.svg",
            "isPro": false,
            "fullname": "Huanyi Zheng",
            "user": "zhyya",
            "type": "user"
          },
          "name": "Huanyi Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:18.731Z",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53208",
          "user": {
            "_id": "632179745fc60c44fd91fc33",
            "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
            "isPro": false,
            "fullname": "zhumuzhi",
            "user": "Z-MU-Z",
            "type": "user"
          },
          "name": "Muzhi Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:11.968Z",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53209",
          "name": "Zhiyue Zhao",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320a",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320b",
          "name": "Tong He",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320c",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T13:51:06.000Z",
      "title": "디세프시션: 시각 인식 태스크를 위한 일반적인 디포어시션 모델",
      "summary": "이 기사의 주요 목표는, 계산 자원과 훈련 데이터의 제한을 고려하여, 여러 태스크를 대응할 수 있는 좋은 일반적인 인식 모델을 만드는 것입니다. 이를 달성하기 위해, 억수천 장의 이미지로 제공된 텍스트로부터 이미지로 확장된 모델을 사용합니다. 우리의 검증 지표에서, DICEPTION은 여러 인식 태스크를 효과적으로 대응할 수 있으며, 가장 최신의 모델과 같은 성능을 달성하는 것을 보여주고 있습니다. SAM-vit-h의 데이터의 0.06% 정도 (예를 들어, 600K 이미지에 대해 1B 이미지의 픽셀 수준의 레이블 이미지)에서 같은 결과를 달성했습니다. Wang et al.의 영감을 받아, DICEPTION은 다양한 인식 태스크의 출력을 색 인코딩으로 구성하고, 서로 다른 인스턴스에 랜덤한 색을 할당하는 전략은 실체 분할 및 의미 분할에 매우 효과적입니다. 여러 인식 태스크를 조건부 이미지 생성으로 통일함으로써, 사전 학습된 텍스트로부터 이미지로의 모델을 최대한 활용할 수 있습니다. 이와 같이, DICEPTION은 기존의 모델과 비교하여, 다양한 태스크에 적용할 때, 그렇게 많은 데이터나 파라미터가 필요하지 않아 효율적으로 훈련할 수 있습니다. 다른 태스크에 모델을 적용할 경우, 50 장의 이미지만 필요합니다. DICEPTION은 시각적인 일반적인 모델에서 유효한 인젝트 및 더 이상망한 해결책을 제공합니다.",
      "upvotes": 35,
      "discussionId": "67bd328aac4a596a43b532ae"
    },
    "publishedAt": "2025-02-24T22:39:29.837Z",
    "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17157.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "646efd223dd912a539e0bd46",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
      "fullname": "Canyu Zhao",
      "name": "Canyu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17129",
      "authors": [
        {
          "_id": "67bd37cb0d41e01cca99aa8b",
          "user": {
            "_id": "64f033ef82c6eea604c4da8b",
            "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
            "isPro": false,
            "fullname": "Liu Xiaoran",
            "user": "LiuXR",
            "type": "user"
          },
          "name": "Xiaoran Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:07.298Z",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8c",
          "name": "Ruixiao Li",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8d",
          "name": "Mianqiu Huang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8e",
          "name": "Zhigeng Liu",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8f",
          "name": "Yuerong Song",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa90",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa91",
          "name": "Siyang He",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa92",
          "name": "Qiqi Wang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa93",
          "name": "Linlin Li",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa94",
          "name": "Qun Liu",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa95",
          "name": "Yaqian Zhou",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa96",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa97",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T13:19:33.000Z",
      "title": "**タイトル:** 긴 문장과 큰 언어 모델의 단어\n\n**本文:**\n\n긴 문장과 큰 언어 모델은 긴 문장을 읽고 이해する 수 있도록 설계되어 있습니다. 이 모델은 긴 문맥을 고려하여 더 정확한 답안을 제공할 수 있습니다. 이 능력은 많은 응용 분야에서 중요하며, 텍스트 분석, 번역, 시스템 설계 등 다양한 분야에서 사용되고 있습니다.",
      "summary": "장문맥은 자연어 처리(NLP)의 중요한 문제로, NLP 아키텍처의 개발을 통해 구성되어 있으며, 대규모 언어 모델(LLMs)에 대해서는 인간의 평생 학습의 가능성에 대한 큰 기회를 제공하고 있습니다. 불행히도, 장문맥의 추구는 여러 장애물과 동반됩니다. 그러나 장문맥은 LLMs의 핵심적인 경쟁 우위를 남깁니다. 지난 2년 동안, LLMs의 문맥 길이는 백만 토큰에 이르러 큰 발전을 거쳤습니다. 또한, 장문맥의 LLMs 연구는 길이의 외연에만 집중되어서는 안 됩니다, 아키텍처, 인프라, 훈련, 평가 기술의 전반적인 초점을 옮겨야 합니다.\n\n스마르 필드의 연설사 \"그는 그렇게 말았어\"를 모티브로, LLM의 문맥 확장의 여정과 인간이 그의 죽음의 후성으로 넘어가려는 노력을 유사성을 발견합니다. 이 조사에서는, LLM이 장문맥을 위해 큰 필요성과, 궁극적으로 제한된 것이 될 수밖에 없는 것을 받아들이는 필요성과 충돌을 설명합니다. 이를 달성하기 위해, 아키텍처, 인프라, 훈련, 평가의 4가지 관점에서 장문맥 LLMs의 생명주기를 전체적으로 제시하고, 장문맥 기술의 전체를 보여줍니다. 이 조사의 최종적으로는, 현재의 장문맥 LLMs에 직면한 10개의 해결되지 않은 질문을 제시합니다. 이 조사는 장문맥 LLMs의 연구에 대한 체계적인 소개를 위해役立つことを望む．",
      "upvotes": 25,
      "discussionId": "67bd37cc0d41e01cca99ab1e"
    },
    "publishedAt": "2025-02-24T22:27:11.566Z",
    "title": "Thus Spake Long-Context Large Language Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f033ef82c6eea604c4da8b",
      "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
      "fullname": "Liu Xiaoran",
      "name": "LiuXR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15814",
      "authors": [
        {
          "_id": "67bd3972f077ddf1f98bacda",
          "user": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "isPro": false,
            "fullname": "Gallil Maimon",
            "user": "gallilmaimon",
            "type": "user"
          },
          "name": "Gallil Maimon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:36.258Z",
          "hidden": false
        },
        {
          "_id": "67bd3972f077ddf1f98bacdb",
          "user": {
            "_id": "644662145004f2cb3af08b27",
            "avatarUrl": "/avatars/5f2af24c7410a5db46374d0b84fb479d.svg",
            "isPro": false,
            "fullname": "Avishai Elmakies",
            "user": "avishai-elmakies",
            "type": "user"
          },
          "name": "Avishai Elmakies",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:33.712Z",
          "hidden": false
        },
        {
          "_id": "67bd3972f077ddf1f98bacdc",
          "name": "Yossi Adi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T17:21:15.000Z",
      "title": "스ラム：1일 동안 1개의 커네아션을 통해 언어 모델을 학습합니다.",
      "summary": "Slam은 단일의 학술용 GPU에서 24시간 이내에 고품질의 언어 모델(SLMs)를 훈련하는 레시피입니다. 이를 실험적으로 수행하기 위해, 모델의 초기화와 아키텍처, 합성 데이터, 합성 데이터에 의한 선호 최적화, 그리고 나머지 모든 구성 요소의 조정을 수행합니다. 실험적으로 보여졌지만, 이 훈련 레시피는 더 많은 계산량을 사용해도 스케일링이 가능하고, 선진적인 SLMs와 같은 결과를 얻는 데 적은 계산 비용이 필요합니다. 이러한 통찰을 제공하여, SLM의 훈련과 연구가 더 쉬워질 수 있도록 바랍니다. SLM의 스케일링 법칙의 배경에서, 우리의 결과를 예측된 계산 최적 성능을 크게 초월하고, SLM의 가능성에 대한 더 조화로운 관점을 제공합니다. 코드, 데이터, 모델, 샘플은 아래 URL에서 참조할 수 있습니다: https://pages.cs.huji.ac.il/adiyoss-lab/slamming.",
      "upvotes": 19,
      "discussionId": "67bd3973f077ddf1f98bacf9"
    },
    "publishedAt": "2025-02-24T23:14:12.363Z",
    "title": "Slamming: Training a Speech Language Model on One GPU in a Day",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/t93GkoiYRplnXH1Go0MmY.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15814.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b9bc2dacdbc1d0b39c3b50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
      "fullname": "Gallil Maimon",
      "name": "gallilmaimon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16584",
      "authors": [
        {
          "_id": "67bd42386959e61abd265a9b",
          "name": "Liumeng Xue",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9c",
          "name": "Ziya Zhou",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9d",
          "name": "Jiahao Pan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9e",
          "name": "Zixuan Li",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9f",
          "name": "Shuai Fan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa0",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa1",
          "name": "Sitong Cheng",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa2",
          "name": "Dongchao Yang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa3",
          "name": "Haohan Guo",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa4",
          "name": "Yujia Xiao",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa5",
          "name": "Xinsheng Wang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa6",
          "name": "Zixuan Shen",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa7",
          "name": "Chuanbo Zhu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa8",
          "name": "Xinshen Zhang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa9",
          "name": "Tianchi Liu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aaa",
          "name": "Ruibin Yuan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aab",
          "name": "Zeyue Tian",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aac",
          "name": "Haohe Liu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aad",
          "name": "Emmanouil Benetos",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aae",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aaf",
          "name": "Yike Guo",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265ab0",
          "name": "Wei Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T14:24:15.000Z",
      "title": "Audio-FLAN: 프리리미러리 리리즈",
      "summary": "최근의 음성 토크나이저 대화의 발전은 대규모 언어 모델(LLMs)에서 음성 기능의 통합을 크게 향상시켰습니다. 그러나 음성 이해와 생성은 일반적으로 다른 태스크로 취급되어, 진정한 통합된 음성 언어 모델의 개발이 약해지게 됩니다. 인스톰션 튜닝은 문과 이미지 사이에서 일반화와 0샷 학습에 있어서 놀라운 성공을 보였지만, 음성에 대한 적용은 아직 많이 탐구되지 않았습니다. 주요한 장애는 음성 이해와 생성을 통합하기 위한 세부적인 데이터 세트의 부족입니다. 이에 대처하여, Audio-FLAN이라는 큰 규모의 인스톰션 튜닝 데이터 세트를 소개합니다. Audio-FLAN은 80가지의 다양한 태스크를 포함하고 있으며, 음성, 음악, 음의 분야를 모두 포괄하며, 100만 이상의 인스턴스를 포함합니다. Audio-FLAN은 음성 이해(예: 번역, 이해)과 생성(예: 음성, 음악, 음)을 모두 처리할 수 있는 통합된 음성 언어 모델의 기반을 구축합니다. Audio-FLAN 데이터 세트는 HuggingFace와 GitHub에서 제공되며, 정기적으로 업데이트 됩니다.",
      "upvotes": 18,
      "discussionId": "67bd423b6959e61abd265b88"
    },
    "publishedAt": "2025-02-24T23:14:20.487Z",
    "title": "Audio-FLAN: A Preliminary Release",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fd6f670053c8345eddc1b68",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd6f670053c8345eddc1b68/cuTsu2krRYHC6zYGD2dpQ.jpeg",
      "fullname": "Ruibin Yuan",
      "name": "a43992899",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17435",
      "authors": [
        {
          "_id": "67bd6b4b8edd1ce8ad5603a0",
          "name": "Chen-Wei Chang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a1",
          "name": "Cheng-De Fan",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a2",
          "name": "Chia-Che Chang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a3",
          "name": "Yi-Chen Lo",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a4",
          "name": "Yu-Chee Tseng",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a5",
          "name": "Jiun-Long Huang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a6",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:59:54.000Z",
      "title": "GCC: カラーチェッカー를 확산시키고 생성된 색상을 정규화합니다.",
      "summary": "색조恒常성 법은 다양한 카메라 센서 사이에 일반화하기 어려운 점을 많은 경우에 인식하고 있습니다. 우리는 DIFFUSION 모델을 활용한 GCC(Global Color Constancy)을 소개합니다. GCC는 이미지에 색 체크터를 보간하여 조명 측정을 위한 방식으로 색조恒常성을 실현합니다. 우리의 주요 혁신은 다음과 같습니다. (1) 한 단계에서 장면의 조명을 반영하는 색 체크터 보간을 수행하는 확실한 추론 접근, (2) 체크터 구조를 유지하면서 조명 의존성의 색조 적응을 가능하게 하는 라플라시안 분해 방법, (3) 잘못된 색 체크터 Annotation을 처리하는 마스크 기반의 데이터 확장 전략입니다. GCC는 Cross-Camera 시나리오에서 가장 강력한 강건성을 나타내며, 최악의 25%의 오류율은 5.15°와 4.32°로 나타났습니다. 이러한 결과는 카메라의 특성을 고려하지 않아도, 특히 센서에 대한 훈련이 필요하지 않은 것을 보여주고, 실제 세계적인 애플리케이션에 대한 광범위한 해결책으로서의 적합성을 보여줍니다.",
      "upvotes": 16,
      "discussionId": "67bd6b4d8edd1ce8ad560401"
    },
    "publishedAt": "2025-02-25T02:06:00.809Z",
    "title": "GCC: Generative Color Constancy via Diffusing a Color Checker",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/gDAYQUcbNE2Ps2pQFxg_m.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6459d5da3b6fafd9664807ab",
      "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
      "fullname": "Yu-Lun Liu",
      "name": "yulunliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16614",
      "authors": [
        {
          "_id": "67bd36334a9a04b9ca9bbb68",
          "name": "Alexander Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb69",
          "name": "Marcus Dong",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6a",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6b",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6c",
          "name": "Yejie Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6d",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6e",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6f",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb70",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb71",
          "name": "Yingshui Tan",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb72",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb73",
          "name": "Zhexu Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb74",
          "name": "Weixun Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb75",
          "name": "Yancheng He",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb76",
          "name": "Ken Deng",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb77",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb78",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb79",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T15:36:43.000Z",
      "title": "CodeCriticBench: 전체적인 코드 평가 벤치마크를 제공하는 대 언어 모델\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "summary": "LLMs의 비판적 능력은 논리적 능력을 제공하기 위해 중요하며, 필요한 제안(예: 상세한 분석과 실용적인 피드백)을 제공할 수 있습니다. 따라서, LLMs의 비판적 능력을 평가하는 방법은 큰 주목을 받고, 여러 개의 비판적 벤치마크가 제안되어 있습니다. 그러나 현재의 비판적 벤치마크는 다음과 같은 제한이 있습니다: (1) 일반적인 분야에서 다양한 논리적 태스크를 중점적으로 다루고, 코드 태스크에서 평가가 부족합니다(예: 코드 생성 태스크만 대상이 됩니다). 이러한 태스크의 난이도는 상대적으로 간단합니다(예: CriticBench의 코드 쿼리는 Humaneval과 MBPP에서 가져옵니다). (2) 여러 차원에서 상세한 평가가 부족합니다. 이러한 제한에 대처하기 위해, 우리는 LLMs의 비판적 능력을 평가하기 위한 맵기 기반 벤치마크를 통해, CodeCriticBench라는 이름으로 소개합니다. 특히, 우리의 CodeCriticBench는 두 가지主流 코드 태스크(코드 생성과 코드 QA)를 포함하며, 평가 프로토콜은 기본적인 비판적 평가와 발전적인 비판적 평가를 포함하고 있으며, 발전적인 설정에 적합한 세부화된 평가 체크리스트를 설계하고 있습니다. 최종적으로, 우리는 현재의 다양한 LLMs의 실험 결과를 수행하고, CodeCriticBench의 효과성을 보여주고 있습니다.",
      "upvotes": 13,
      "discussionId": "67bd36354a9a04b9ca9bbc16"
    },
    "publishedAt": "2025-02-24T22:17:28.937Z",
    "title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65377c30e48353201e6fdda0",
      "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
      "fullname": "Jiaheng Liu",
      "name": "CheeryLJH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16033",
      "authors": [
        {
          "_id": "67bd31d0d055a27740b16a30",
          "name": "Qianqi Yan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a31",
          "name": "Yue Fan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a32",
          "name": "Hongquan Li",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a33",
          "name": "Shan Jiang",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a34",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a35",
          "name": "Xinze Guan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a36",
          "name": "Ching-Chen Kuo",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a37",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-22T01:52:37.000Z",
      "title": "다형성 불적절성 추론 (MMIR): 다형성 추론 모델의 새로운 벤치마크",
      "summary": "현재의 다모달 대언어 모델(MLLM)은 주로 일관된 이미지-문자 입력을 사용하여 훈련되고 검증되어 있습니다が, 실제 세계적인, 라우프룸 풍부한 콘텐츠에서 부적절성을 처리할 수 있는지의 문제가 남아 있습니다. 이 간극을 메우기 위해, 우리는 MLLM의 세ман틱 불적절성을 감지하고 이유를 제공하는 능력을 평가하기 위한 Multimodal Inconsistency Reasoning(MMIR) 벤치마크를 제안합니다. MMIR는 534개의 어려운 샘플을 포함하며, 사실적 반대, 정체성 미지정, 맥락적 불일치, 양적 불일치, 시간적/공간적 불일치의 5가지 원인적인 분야에 대한 합성적으로 삽입된 오류를 포함합니다. 우리는 6개의 최선 MLLM을 평가하고, 다모달 이유 능력을 가진 모델(예: o1)은, 그 컨테이너 모델에 비해 크게 우월함을 보여주지만, 오픈소스 모델은 특히 불적절성 오류에 취약합니다. 세부적인 오류 분석은, 세ман틱 불적절성을 감지하는 것을 특히 텍스트에서, 대신 Cross-Model의 충돌과 복잡한 라우프룸에 대해 어려움을 느끼는 것을 보여줍니다. 조사 실험은, Chain-of-Thought(CoT)이나 Set-of-Mark(SoM) 메소드를 포함하는 단일 모델의 Prompting에서 미시적 효과를 보여주고, Cross-Model 이유의 키백록을 보여주었습니다. 우리의 발견은, 진보적인 다모달 이유의 필요성을 보여주고, 향후 연구의 방향을 다모달 불적절성에 대한 연구에示し습니다.",
      "upvotes": 11,
      "discussionId": "67bd31d2d055a27740b16ad9"
    },
    "publishedAt": "2025-02-24T21:59:50.456Z",
    "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16033.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16894",
      "authors": [
        {
          "_id": "67bd396ea06bae99f3866911",
          "user": {
            "_id": "641aa5e391e3376a057bbd4c",
            "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
            "isPro": false,
            "fullname": "Chenghao Fan",
            "user": "Facico",
            "type": "user"
          },
          "name": "Chenghao Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:38.942Z",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866912",
          "name": "Zhenyi Lu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866913",
          "name": "Sichen Liu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866914",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866915",
          "name": "Wei Wei",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866916",
          "name": "Chengfeng Gu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866917",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T06:48:13.000Z",
      "title": "LoRA를 재강화하기 위해 적응적 고유값과 전문가의 혼잡 최적화 aligment를 활용한 LoRA의 향상",
      "summary": "Low-Rank Adaptation (LoRA)는 Large Language Models (LLMs)에 대해 파라미터 효율적인 미세 조정을 가능하게 해줍니다. 그러나 이 성능은 일반적으로 Full Fine-Tuning (Full FT)보다 낮습니다. 현재의 방법들은 SVD (Singular Value Decomposition)의 서브셋으로 LoRA를 초기화하고 최적화하는 방식으로 진행되어 있습니다. 그러나 이러한 방식은 사전 학습된 지식의 최적 활용에 연결되지 않습니다. LoRA를 개선하기 위한 추가적인 방법 중 하나는 Mixture-of-Experts (MoE) 아키텍처를 도입하는 것입니다. 그러나 SVD를 MoE 아키텍처에 도입하는 데 있어서는 가중치의 적절하지 않은 배치와 복잡한 경사 역학이 문제입니다. 이러한 문제를 해결하기 위해 우리는 Great LoRA Mixture-of-Expert (GOAT) 프레임워크를 제안하고 있습니다. 이 프레임워크는 (1) SVD 구조화된 MoE를 사용하여 관련된 선두를 적응적으로 통합하고 (2) 이론적인 스케일링 팩터를 계산하여 Full Fine-Tuned MoE와 최적화를 위한 기능을 제공합니다. 이 방법론은 아키텍처나 훈련 알고리즘을 변경하지 않고도 LoRA MoE의 효율성과 성능을 향상시킬 수 있습니다. 자연어 처리, 일반 지식 추론, 이미지 분류, 자연어 생성의 25개의 데이터 세트에서 수행된 실험은 GOAT의 최신 성능을 보여주고 Full FT와 간극을 좁히었습니다.",
      "upvotes": 10,
      "discussionId": "67bd396fa06bae99f3866964"
    },
    "publishedAt": "2025-02-24T22:35:41.042Z",
    "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16894.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641aa5e391e3376a057bbd4c",
      "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
      "fullname": "Chenghao Fan",
      "name": "Facico",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17407",
      "authors": [
        {
          "_id": "67bd48d4becb766415a5d19d",
          "name": "Guijin Son",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d19e",
          "name": "Jiwoo Hong",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d19f",
          "user": {
            "_id": "63e087b6a98d931aa90c1b9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e087b6a98d931aa90c1b9c/96c6IT3f1pWGLbRdRDB2U.png",
            "isPro": false,
            "fullname": "Hyunwoo Ko",
            "user": "Cartinoe5930",
            "type": "user"
          },
          "name": "Hyunwoo Ko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:12.933Z",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d1a0",
          "name": "James Thorne",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:36:15.000Z",
      "title": "측정 시의 스케일링의 언어학적 일반화성에 관한 내용",
      "summary": "スケーリング予習計算은 다언어성 달성에 효과적이라는 사실은 입증되어 있지만, 테스트 시 스케일링에서 동일한 효과를 보임 여부는 불분명합니다. 본 논문에서는 MCLM(Multilingual Math Benchmark)을 소개합니다. MCLM은 55언어로 구성된 컴퍼티션 수준의 문제를 다루는 다언어 버전입니다. Qwen2.5-1.5B Math와 MR1-1.5B에 대해, Outcome Reward Modeling(ORM), Process Reward Modeling(ORM), Budget Forcing(BF)의 3가지 테스트 시 스케일링 방법들을 검증했습니다. Qwen2.5-1.5B Math와 ORM을 사용하면 MCLM에서 점수는 35.8점으로, BF를 MR1-1.5B에 적용하면 35.2점으로 나타났습니다. \"Thinking LLMs\"는 최근 주목을 받고 있지만, 추론 FLOPs가 동일한 수준으로 제한된 경우, 그 성능은 전통적인 스케일링 방법과 비교하여 상대적으로 좋습니다. 또한, BF는 영어의 AIME에서 20점 상승을 보여주지만, 다른 언어에서는 평균 1.94점 상승만 보였습니다. 이 패턴은 다른 테스트 시 스케일링 방법들도 동일합니다. 테스트 시 스케일링은 다언어 태스크에 대해서는 더 효과적으로 확장되지 않습니다. 발전을 위해, MCLM, MR1-1.5B, 그리고 평가 결과를 공개합니다.",
      "upvotes": 9,
      "discussionId": "67bd48d5becb766415a5d1e9"
    },
    "publishedAt": "2025-02-24T23:37:53.138Z",
    "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17407.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d3e619b8448e1785bbda2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
      "fullname": "GUIJIN SON",
      "name": "amphora",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17110",
      "authors": [
        {
          "_id": "67bd3936daef22cbce6d7ef2",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef3",
          "user": {
            "_id": "645b10e80c73ea27d13f7aca",
            "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
            "isPro": false,
            "fullname": "xuhaiyang",
            "user": "xhyandwyy",
            "type": "user"
          },
          "name": "Haiyang Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:41.528Z",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef4",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef5",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef6",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef7",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef8",
          "name": "Jitao Sang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T12:51:23.000Z",
      "title": "Mobile-Agent-V: 비디오 가이드로드의 다 에이전트 협조에 의한 모바일 장치의 동작 학습",
      "summary": "モバイルデバイス의 사용량이 급격히 증가함에 따라, 끊임없는 작업 관리를 위한 개선된 자동화가 필요하게 됩니다. 그러나 많은 AI를 주도하는 프레임워크들은 운영 지식의 부족으로 어려움을 겪고 있습니다. 손으로 작성된 지식은 도움이 됩니다만, 노동비가 높고 효율이 좋지 않습니다. 이러한 문제를 해결하기 위해, Mobile-Agent-V라는 프레임워크를 소개합니다. 이 프레임워크는 비디오 가이드를 사용하여, 풍부한, 비용 효율적인 이동 자동화에 대한 운영 지식 제공을 합니다. Mobile-Agent-V는 특훈이나 전처리가 필요하지 않으며, 비디오 입력을 사용하여 작업 수행 능력을 향상시킵니다. Mobile-Agent-V는 슬라이딩 윈도우 전략을 채택하고, 비디오 에이전트와 깊은 리플렉션 에이전트를 조합하여, 행동이 사용자의 지시에 일치하도록 합니다. 이 혁신적인 접근 방식에 의해, 사용자는 지도를 받아 작업 프로세스를 기록할 수 있으며, 시스템가 자동으로 학습하고 효율적으로 작업을 수행할 수 있습니다. 실험 결과를 통해, Mobile-Agent-V는 현재의 프레임워크와 비교하여 30%의 성능 향상을 달성하고 있습니다.",
      "upvotes": 8,
      "discussionId": "67bd3938daef22cbce6d7f9d"
    },
    "publishedAt": "2025-02-24T22:31:17.771Z",
    "title": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/mshxtP77rrnN07f6ux6_0.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17110.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16922",
      "authors": [
        {
          "_id": "67bd3d6b60186d7478467208",
          "user": {
            "_id": "6643261b8876db14227eeb19",
            "avatarUrl": "/avatars/67428c9e37a2273697c0547e1783ec6b.svg",
            "isPro": false,
            "fullname": "Zhenglin Wang",
            "user": "wzl0228",
            "type": "user"
          },
          "name": "Zhenglin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:15.633Z",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d7478467209",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720a",
          "name": "Pengfei LI",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720b",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720c",
          "name": "Deyu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T07:27:54.000Z",
      "title": "중국 시대별 시간계열 기준 및 어레이멘트 벤치마크",
      "summary": "시간 추론은 인간의 인지의 기초이며, 다양한 실세계의 응용에서 중요합니다. 최근의 대규모 언어 모델의 발전은 시간 추론에 대한 기대할 수 있는 능력을 보여주었지만, 현재의 벤치마크는 주로 규칙 기반으로 구축되어 있으며, 맥락의 깊이가 부족하고, 시간의 엔티티의 범위가 제한되어 있습니다. 이러한 제한을 해결하기 위해, 우리는 중국 시간 추론(CTM)을 소개합니다. CTM은 중국의 시대 순서의 광범위한 범위에서 시간 추론을 평가하기 위해 설계된 벤치마크입니다. CTM은 크로스 엔티티 관계, 파스 와이스 엔드 시간의 어라인, 맥락화된 및 문화적으로 기반된 추론을 강조하고, 세부적인 평가를 제공합니다. 확장된 실험 결과를 통해 CTM에 의한 문제를 명확히 하고, 개선의 가능성 있는 길을 제시합니다.",
      "upvotes": 7,
      "discussionId": "67bd3d6c60186d7478467249"
    },
    "publishedAt": "2025-02-24T22:48:30.357Z",
    "title": "Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16922.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15894",
      "authors": [
        {
          "_id": "67bd3bd26faf9f04b2170f61",
          "name": "Min Zhao",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f62",
          "name": "Guande He",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f63",
          "name": "Yixiao Chen",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f64",
          "user": {
            "_id": "64c269a52d73768f07ac266c",
            "avatarUrl": "/avatars/d497a960f8aef6a974907b68ed750c1c.svg",
            "isPro": false,
            "fullname": "Zhu Hongzhou",
            "user": "zhuhz22",
            "type": "user"
          },
          "name": "Hongzhou Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:23.502Z",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f65",
          "name": "Chongxuan Li",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f66",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T19:28:05.000Z",
      "title": "RIFLEx: 이미지 분화에서 길이 추정을 위한 무료 Launch\nTransformers",
      "summary": "최근의 이미지 생성의 발전은 모델이 고품질의 1분간의 이미지를 합성할 수 있게 되었습니다. 그러나 긴 이미지 생성 및 시간적 연속성을 유지하는 것은 큰 문제로, 현재의 길이 추정법은 시간적 재현 또는 움직임의 느려짐으로 문제가 발생합니다. 본 논문에서는 위치埋め의 주파수 성분의 역할을 체계적으로 분석하고, 길이 추정 행동을 주로 제어하는 고유의 주파수를 특정했습니다. 이 통찰에 기반하여, RIFLEx라는 최소한의, 원래 효과적인 접근 방식을 제안했습니다. RIFLEx는 재현을 억제하면서 움직임의 일관성을 유지하기 위해 고유의 주파수를 저감하여 추가적인 변경이 필요하지 않습니다. RIFLEx는 최신의 이미지 디퓨저 트랜스포머를 통해 완전한 훈련 제한으로 최고 품질의 2배 길이 추정을 실현합니다. 또한 최소한의 미세 조정으로 3배 길이 추정이 가능하며, 긴 이미지가 필요하지 않습니다. 프로젝트 페이지와 코드는 아래 URL에 있습니다.",
      "upvotes": 6,
      "discussionId": "67bd3bd66faf9f04b21710d1"
    },
    "publishedAt": "2025-02-25T00:09:04.483Z",
    "title": "RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15894.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6205
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16707",
      "authors": [
        {
          "_id": "67bd3bcc797e4d53ce0bc70d",
          "user": {
            "_id": "64f8fbd95515d7dcceb906b1",
            "avatarUrl": "/avatars/1c7d034de408930b166592465e65fc31.svg",
            "isPro": false,
            "fullname": "Yunhai Feng",
            "user": "yunhaif",
            "type": "user"
          },
          "name": "Yunhai Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:31.085Z",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc70e",
          "user": {
            "_id": "62318c0386753f5f41d0e261",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
            "isPro": false,
            "fullname": "Jiaming Han",
            "user": "csuhan",
            "type": "user"
          },
          "name": "Jiaming Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:28.772Z",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc70f",
          "name": "Zhuoran Yang",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc710",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc711",
          "name": "Sergey Levine",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc712",
          "name": "Jianlan Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T20:42:15.000Z",
      "title": "리플렉시블 계획링: 비젼-언어 모델을 이용한 다단계 장기 로봇 작동",
      "summary": "복잡한 장기적인 로봇 조작 문제를 해결하기 위해, 높은 계획 능력, 물리 세계의 이해 능력, 그리고 장기적인 예측 오류 누적 문제를 해결하기 위한 장기적인 예측 능력이 필요합니다. 시각 언어 모델(VLMs)은 인터넷 데이터에 pretraining되어 있어, 이러한 문제를 해결하는 프레임워크를 제공할 수 있습니다. 그러나 현재의 형태에서, VLMs는 기계어 조작에 필요한 복잡한 물리 설명의 이해를 하지 못하고, 장기적인 예측 오류 누적 문제를 해결하기 위한 장기적인 예측 능력도 갖지 않습니다. 본 논문에서는, VLMs의 물리적 설명 능력을 강화하기 위한 새로운 테스트 시 계산 프레임워크를 통해, 다단계 조작 태스크를 해결하는 것을 보고합니다. 그 핵심은, \"반사\" 구조를 사용하여 학습된 VLM을 반복적으로 개선하는 접근법입니다. 이는 미래의 세계 상태를 상상하는 생성 모델을 사용하며, 이러한 예측을 동작 선택에 가이드하고 잠재적인 부적절성을 비판적으로 개선하는 것입니다. 실험 결과를 통해, 우리의 방법론은 여러 선진적인 상업 VLMs 및 Monte Carlo 트리 탐색(MCTS) 등 후 학습 접근법을 크게 초월할 수 있음을 보여줍니다. 비디오는 https://reflect-vlm.github.io에서 이용할 수 있습니다.",
      "upvotes": 5,
      "discussionId": "67bd3bcf797e4d53ce0bc7ff"
    },
    "publishedAt": "2025-02-25T01:02:05.395Z",
    "title": "Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16707.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f8cb8ed04a890f5380d9a4",
      "avatarUrl": "/avatars/d6fdfdbb0c10141aa3b4c832d928121b.svg",
      "fullname": "Jianlan Luo",
      "name": "jianlanluo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15987",
      "authors": [
        {
          "_id": "67bd46ea3e090b402d70f1f4",
          "user": {
            "_id": "64dfbcb18e2084e1d7b51b46",
            "avatarUrl": "/avatars/fafe30beea2d7e8eec3f3ba985c582f7.svg",
            "isPro": false,
            "fullname": "Kushal Raj Bhandari",
            "user": "KBhandari11",
            "type": "user"
          },
          "name": "Kushal Raj Bhandari",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-25T04:30:32.676Z",
          "hidden": false
        },
        {
          "_id": "67bd46ea3e090b402d70f1f5",
          "name": "Pin-Yu Chen",
          "hidden": false
        },
        {
          "_id": "67bd46ea3e090b402d70f1f6",
          "name": "Jianxi Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T22:52:19.000Z",
      "title": "Hugging Face의 Open-Weight AI 모델의 성장예측",
      "summary": "開放중량의 AI 맵스케일가 발전하고 모델 개발, 대규모 투자 및 사용자의 관심이 증가하는 가운데, 모델이 최종적으로 혁신을 주도하고 AI 생태계를 형성하는 것이 중요해졌습니다. 과학 문헌의 인용 동향과 유사성을 기반으로, 开放중량 모델의 영향이 어떻게 진화하는 지를 정량화하기 위한 프레임워크를 제안합니다. 특히, Wang et al.가 도입한 과학 인용 모델을 적용하여, 开放중량 모델의 미세 조정 모델의 누적 수를 추적하기 위한 immediacy, longevity, relative fitness 세 가지 요인을 사용합니다. 우리 조사 결과, 인용별로 접근하는 방법은 开放중량 모델의 다양한 采用 경로를 효과적으로 파악하고, 대부분의 모델이 이 경로에 적합하며, 아우터저는 특징적인 패턴이나 사용량의 급격한 증가를 나타냅니다.",
      "upvotes": 4,
      "discussionId": "67bd46ee3e090b402d70f317"
    },
    "publishedAt": "2025-02-24T23:30:36.556Z",
    "title": "Forecasting Open-Weight AI Model Growth on Hugging Face",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e67bdd61009063689407479/kQHArNjaT0CM1KCujtDc1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15987.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "5e67bdd61009063689407479",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857146757-5e67bdd61009063689407479.jpeg",
      "fullname": "Clem 🤗",
      "name": "clem",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isMod": false,
      "followerCount": 2052
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16701",
      "authors": [
        {
          "_id": "67bd31d6bf6d46017e515a58",
          "user": {
            "_id": "62543749b777cd32720675c2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658760912583-62543749b777cd32720675c2.jpeg",
            "isPro": false,
            "fullname": "Irene Solaiman",
            "user": "irenesolaiman",
            "type": "user"
          },
          "name": "Irene Solaiman",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-25T03:43:21.348Z",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a59",
          "name": "Rishi Bommasani",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5a",
          "name": "Dan Hendrycks",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5b",
          "name": "Ariel Herbert-Voss",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5c",
          "name": "Yacine Jernite",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5d",
          "name": "Aviya Skowron",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5e",
          "name": "Andrew Trask",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T20:06:12.000Z",
      "title": "Release보다 멀리：생성 AI 시스템의 액세스 검토",
      "summary": "生成형 AI의 릴리즈 결정은 시스템의 구성 요소가 사용 가능한지 여부에 대한 결정이지만, 릴리즈는 사용자가 시스템과 어떻게 협력할 수 있는 변화된 여러 요소를 처리하지 못합니다. 릴리즈보다, 시스템의 구성 요소에 액세스가 제공되면 잠재적인 위험과 이익에 대한 정보를 제공합니다. 액세스는 구성 요소의 사용에 필요한 실용적인 요구 사항, 인프라, 기술적, 사회적 조건을 포함합니다. 액세스는 3가지 축으로 분해되어 있습니다: 리소스, 기술적 사용 가능도, 유용성. 각 카테고리 내에서는 시스템의 구성 요소에 대한 설정된 연속 변수가 손실을 설명합니다. 예를 들어, 리소스는 모델의 가중치를 서버로 하기 위해 계산 인프라에 액세스해야 합니다. 또한, 4개의 고성능 언어 모델의 액세스성을 비교하고 2개의 오픈 웨이트 모델과 2개의 클로즈 웨이트 모델을 포함하며, 모든 모델에 대해 동일한 고려 사항을 적용하는 것을 보여줍니다. 액세스 변수는 사용자에게 액세스를 확장하거나 증가시킬 수 있는 구조를 만드는 기초로, 액세스의 규모와 그 규모가 위험 관리와 위험 대처에 미치는 영향을 조사합니다. 이 프레임워크는 시스템의 릴리즈의 위험과 이익을 더 잘 이해하는 것을 목표로, 시스템의 릴리즈 결정, 연구, 그리고 정책에 정보를 제공합니다.",
      "upvotes": 4,
      "discussionId": "67bd31d7bf6d46017e515a7e"
    },
    "publishedAt": "2025-02-24T21:59:15.571Z",
    "title": "Beyond Release: Access Considerations for Generative AI Systems",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62543749b777cd32720675c2/LwZmJUoXiJriC_c1DZ7qM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16701.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62543749b777cd32720675c2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658760912583-62543749b777cd32720675c2.jpeg",
      "fullname": "Irene Solaiman",
      "name": "irenesolaiman",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 79
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17258",
      "authors": [
        {
          "_id": "67bd515c0417e7f92283d3b8",
          "name": "Xiangpeng Yang",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3b9",
          "name": "Linchao Zhu",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3ba",
          "name": "Hehe Fan",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3bb",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T15:39:14.000Z",
      "title": "VideoGrain: 시간과 공간의 注意력을 조절한 다粒도 비디오 편집",
      "summary": "최근의 Difeasion 모델의 발전은 영화의 생성과 편집 능력을 크게 향상시켰습니다. 그러나 클래스 수준, 인스턴스 수준, 파티션 수준의 변경을 포함하는 다중 그린드 영화 편집은 어려운 도전입니다. 다중 그린드 편집의 주요 난점은 문장을 영역으로의 제어의 의미적 조정과 Difeasion 모델 내의 특성량의 결합입니다. 이러한 난점을 해결하기 위해, 우리는 VideoGrain를 제안합니다. VideoGrain는 공간 시간(교차 및 자기) 注意机构을 조절하여, 영화 콘텐츠에 미세한 제어를 구현하는 0샷 접근입니다. 우리는 문장을 영역으로의 제어를 강화하고, 교차 注意에서 무관한 영역과의 상호작용을 최소화합니다. 또한, 자기 注意에서 영역 간의 간섭을 줄이고, 영역 내의 인식을 강화합니다. 광범위한 실험은 우리의 방법が 현실적인 시나리오에서 가장 先端의 성능을 달성하는 것을 보여줍니다. 우리의 코드, 데이터, demo는 https://knightyxp.github.io/VideoGrain_project_page/ 에 공개되어 있습니다.",
      "upvotes": 3,
      "discussionId": "67bd51620417e7f92283d4e9"
    },
    "publishedAt": "2025-02-25T00:13:12.214Z",
    "title": "VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6205
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14132",
      "authors": [
        {
          "_id": "67b86819d00e69f10c1f31b9",
          "user": {
            "_id": "6231d3ce86753f5f41d39c6f",
            "avatarUrl": "/avatars/9b18f368e5f80cfc935b2e339d42a85f.svg",
            "isPro": false,
            "fullname": "Nadav Borenstein",
            "user": "Nadav",
            "type": "user"
          },
          "name": "Nadav Borenstein",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:52.278Z",
          "hidden": false
        },
        {
          "_id": "67b86819d00e69f10c1f31ba",
          "user": {
            "_id": "6698cffdb2ebada9f4a7e7d7",
            "avatarUrl": "/avatars/e66d946c14595d3b008185f2be8d2f57.svg",
            "isPro": false,
            "fullname": "Greta Warren",
            "user": "gretawarren",
            "type": "user"
          },
          "name": "Greta Warren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T14:42:45.791Z",
          "hidden": false
        },
        {
          "_id": "67b86819d00e69f10c1f31bb",
          "name": "Desmond Elliott",
          "hidden": false
        },
        {
          "_id": "67b86819d00e69f10c1f31bc",
          "name": "Isabelle Augenstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T22:26:39.000Z",
      "title": "커뮤니티 노트가 전문적인 사실검증자를 대체할 수 있는지?",
      "summary": "소셜 미디어에서 오신도 증가를 방지하기 위해 일반적으로 사용되는 두 가지 전략은 (i) 전문적인 조직이 수행하는 사실 체크와 (ii) 플랫폼 사용자가 수행하는 커뮤니티 Moderation입니다. Twitter/X와 Meta의 정책 변경은 사실 체크 단체와의 파트너십을 통해 커뮤니티 노트와의 협업에 대한 의존관계가 증가함에 따라 전환되어 있습니다. 그러나 사실 체크와 커뮤니티 노트 사이의 의존관계의 정도와 특성은 명확하지 않습니다. 이러한 문제를 해결하기 위해 언어 모델을 사용하여, 토픽, 인용된 소스, 노트가 광범위한 오신도 설명에 관련된 주장을 부정하는 등의 속성을 유형화한 Twitter/X의 대규모 코퍼스를 기록했습니다. 분석 결과는 커뮤니티 노트가 사실 체크 소스를 인용하는 횟수는 이전 보고서보다 5배 이상 증가하고, 사실 체크는 광범위한 설명에 관련된 포스트의 노트에서 특히 중요하며, 다른 소스보다 2배 이상의 확률로 사실 체크 소스를 참조하고 있습니다. 결론적으로, 우리의 결과를 통해 성공적인 커뮤니티 Moderation이 전문적인 사실 체크에 강한 의존성을 가지고 있음을 보여줍니다.",
      "upvotes": 2,
      "discussionId": "67b8681bd00e69f10c1f3267"
    },
    "publishedAt": "2025-02-25T04:11:18.915Z",
    "title": "Can Community Notes Replace Professional Fact-Checkers?",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6231d3ce86753f5f41d39c6f/CwWaf1c9-jOzJ-gD5lvCH.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/6231d3ce86753f5f41d39c6f/WrrBClUkuDsXHcfxP_N8B.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14132.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6231d3ce86753f5f41d39c6f",
      "avatarUrl": "/avatars/9b18f368e5f80cfc935b2e339d42a85f.svg",
      "fullname": "Nadav Borenstein",
      "name": "Nadav",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15122",
      "authors": [
        {
          "_id": "67bbd6d5ba0bb31293e11210",
          "user": {
            "_id": "675f68e3074ff89c5c078bf3",
            "avatarUrl": "/avatars/e3b78d90f032659d411761f47c3cf43e.svg",
            "isPro": false,
            "fullname": "Angus",
            "user": "angus924",
            "type": "user"
          },
          "name": "Angus Dempster",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-24T02:18:57.914Z",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11211",
          "name": "Navid Mohammadi Foumani",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11212",
          "name": "Chang Wei Tan",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11213",
          "name": "Lynn Miller",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11214",
          "name": "Amish Mishra",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11215",
          "name": "Mahsa Salehi",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11216",
          "name": "Charlotte Pelletier",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11217",
          "name": "Daniel F. Schmidt",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11218",
          "name": "Geoffrey I. Webb",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T00:54:40.000Z",
      "title": "몬스터: 모나스 스케라블な 타이밍 시리즈 평가 리포지토리\n\n(注意: \"ス케라블\"는 \"스케일블\"의 번역이며, \"ス케라블\"이 더 합당한 표현일 수 있습니다. 따라서, \"스케일블\"로 대체할 수 있습니다.)\n\n몬스터: 모나스 스케일블な 타이밍 시리즈 평가 리포지토리",
      "summary": "몬스터（MONSTER）-MONASHスカラブルタイムシリーズ 평가 리포지토리-、시계열 분류 분야의 대규모 데이터 세트의 수집을 소개합니다. 시계열 분류 분야는 UCR와 UEA가 설정한 공통 벤치마크에 따라 피드백을 받고 있습니다. 그러나 이러한 벤치마크에 포함된 데이터 세트는 작으며, 각각의 중앙값은 217과 255개입니다. 이로 인해, 광범위한 작은 데이터 세트에서 낮은 분류 오류를 달성하는 모델의 범위가 좁아지는 경향이 있습니다. 이는 분산을 최소화하고, 계산 문제와 같은 스케일러블성 측면에 약간의 중점을 두지 않는 모델을 우선시하는 경향이 때문입니다. 우리의 희망은, 이러한 벤치마크를 대규모 데이터 세트를 사용하여 구축하여 분야를 다양화하는 것입니다. 우리는 대규모 데이터에서 학습하는 이론적 및 실용적인 도전에 참여하여, 분야에 새로운 발전의 큰 잠재력을 믿습니다.",
      "upvotes": 2,
      "discussionId": "67bbd6d6ba0bb31293e11258"
    },
    "publishedAt": "2025-02-25T00:37:53.138Z",
    "title": "MONSTER: Monash Scalable Time Series Evaluation Repository",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15122.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "675f68e3074ff89c5c078bf3",
      "avatarUrl": "/avatars/e3b78d90f032659d411761f47c3cf43e.svg",
      "fullname": "Angus",
      "name": "angus924",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17414",
      "authors": [
        {
          "_id": "67bd526001d5bfa0abfcc5ba",
          "name": "Zeyuan Chen",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bb",
          "name": "Hongyi Xu",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bc",
          "name": "Guoxian Song",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bd",
          "name": "You Xie",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5be",
          "name": "Chenxu Zhang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bf",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c0",
          "name": "Chao Wang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c1",
          "name": "Di Chang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c2",
          "name": "Linjie Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:47:54.000Z",
      "title": "X-Dancer: 표현의 음악과 인간들의 춤의 영상 생성",
      "summary": "X-Dancer는 새로운 Zero-Shot 음악 Drove Image Animation Pipeline로, 하나의 고정 이미지에서 다양한 긴 거리의 생생한 인간의 춤 영상으로 생성하는 기술입니다. 그 핵심은 하나의 통합 채널 분산 프레임워크로, 자동 복원 채널 모델을 적용하여 2D의 몸체, 머리, 손의 포즈를 구성하는 확장된 음악 동기화 Terminal Sequence를 합성하는 것입니다. 이러한 시퀀스는 분산 모델을 통해, 현실적인 춤 영상 프레임을 생성합니다. 기존의 방법과 달리, 3D인체의 움직임을 주로 생성하는 것이 아니라, X-Dancer는 데이터의 제한을 해결하고, 2D의 춤의 움직임의 넓은 범위를 모델링하고, 단색 비디오에서 음악의 박자를 微妙하게 일치시키는 것으로 스케일러빌리티를 향상시킵니다. 이를 실현하기 위해, 먼저 2D인체의 포즈 라벨과 키 포인트의 신뢰도를 결합한 공간적 구조적인 Terminal 표현을 구축합니다. 그리고 음악으로부터 동작으로의 채널 모델을 설계하고, 음악 동기화된 춤 포즈 Terminal Sequence를 자동 복원적으로 생성하고, 음악의 스타일과 선행의 동작 컨텍스트를 포함하는 글로벌 어텐션을 적용합니다. 마지막으로, 이러한 합성된 포즈 Terminal을 Reference Image에 Animation을 추가하여, 완전히 미분 가능한 최종적인 프레임워크를 구축합니다. 실험 결과는 X-Dancer가 다양한 특징付き의 춤 영상을 생성할 수 있으며, 다양성, 표현력, 리알리티에 있어서 가장 선진적인 방법을 크게 초월하는 것을 증명합니다. 코드와 모델은 연구를 위해 사용 가능합니다.",
      "upvotes": 2,
      "discussionId": "67bd526101d5bfa0abfcc62c"
    },
    "publishedAt": "2025-02-25T00:17:51.431Z",
    "title": "X-Dancer: Expressive Music to Human Dance Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17414.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6205
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13074",
      "authors": [
        {
          "_id": "67bd8759fdecc637bd621e6b",
          "name": "Omer Angel",
          "hidden": false
        },
        {
          "_id": "67bd8759fdecc637bd621e6c",
          "name": "Emmanuel Jacob",
          "hidden": false
        },
        {
          "_id": "67bd8759fdecc637bd621e6d",
          "name": "Brett Kolesnik",
          "hidden": false
        },
        {
          "_id": "67bd8759fdecc637bd621e6e",
          "name": "Grégory Miermont",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T17:21:44.000Z",
      "title": "브라운의 뿔 속의 뱀",
      "summary": "ブローナースプフェア는 두차원 초면과 동형적인 랜덤 메트릭스 공간에서, 여러 종류의 랜덤 평면 맵의 일반적인 스케일링 극한으로 나타나는 현상입니다. ブローナースプフェア의 직접적인 구성은 연속적인 ANALOGY에 의한 Cori--Vauquelin--Schaeffer (CVS) 대응으로 수행됩니다. CVS 대응은 표준화된 나무를 평면 맵에 대응시키고, 연속적인 BARON-LABELED ALDOUS의 연속 시간 랜덤 나무(ブローナースネーク)를 ブローナースプフェア에 대응시키는 데 사용됩니다. 본 논문에서는, 연속적인 CVS 대응의 역변환을 설명하기 위해, ブローナースプフェア를 측정 함수로서의 ブローナースネーク로 구축하는 방법을 설명합니다. ブローナースプフェア의 방향 처리에 특별한 주의가 필요합니다.",
      "upvotes": 0,
      "discussionId": "67bd875afdecc637bd621e95"
    },
    "publishedAt": "2025-02-25T04:03:39.758Z",
    "title": "The snake in the Brownian sphere",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13074.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636d12455aaed143cd665607",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679399015950-636d12455aaed143cd665607.png",
      "fullname": "ZLW",
      "name": "ZarkLngeW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15167",
      "authors": [
        {
          "_id": "67bc7ea06f88ef9a2b8283d3",
          "name": "Chuan Cui",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d4",
          "name": "Kejiang Chen",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d5",
          "name": "Zhihua Wei",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d6",
          "name": "Wen Shen",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d7",
          "name": "Weiming Zhang",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d8",
          "name": "Nenghai Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T03:05:45.000Z",
      "title": "M3-AGIQA: 멀티모드, 멀티로운드, 멀티アスペクト ビジョン AI 생성 이미지의 품질 평가",
      "summary": "AI 생성 이미지（AGI） 모델의 급속한 발전은 그 품질 평가에 중대한 문제를 일으키고 있습니다. 이러한 문제를 해결하기 위해, 우리는 M3-AGIQA, 즉 다중모달, 다중 라운드, 다중 측면 평가를 수행하기 위한 평가 프레임워크를 제안합니다. 우리의 접근 방식은 다중모달 대형 언어 모델（MLLMs）의 능력을 활용하여, JOINT 텍스트와 이미지 인코더로 작동시키고, Low-Rank Adaptation（LoRA）를 통해 온라인 MLLMs의 높은 캡처 능력이 로컬 모델에 통합됩니다. 이 프레임워크는 구조화된 다턴 평가 구조를 포함하며, 중간 이미지 설명을 생성하고 품질, 대응, 수용성 측면에서 깊은 전망을 제공합니다. 인간 시각적 판단에 맞는 예측을 위해, xLSTM과 리지셰션 헤드를 구성한 예측기를 사용하며, 순서 로지스틱을 처리하고 Mean Opinion Scores（MOSs）를 예측합니다. 여러 벤치마크 데이터 세트에서 확장되고, 교차 데이터 검증에서 강력한 일반화 성능을 나타냅니다. 코드는 https://github.com/strawhatboy/M3-AGIQA에 공개되어 있습니다.",
      "upvotes": 0,
      "discussionId": "67bc7ea26f88ef9a2b828473"
    },
    "publishedAt": "2025-02-25T03:36:50.480Z",
    "title": "M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 771
    },
    "isAuthorParticipating": false
  }
]