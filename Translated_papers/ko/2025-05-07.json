[
  {
    "paper": {
      "id": "2505.03318",
      "authors": [
        {
          "_id": "681aac4fd31f567552f0cc0e",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc0f",
          "name": "Zhimin Li",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc10",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:33.194Z",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc11",
          "name": "Chunyu Wang",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc12",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc13",
          "name": "Cheng Jin",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc14",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T08:46:41.000Z",
      "submittedOnDailyAt": "2025-05-07T00:19:35.245Z",
      "title": "강화학습에 의한 통합 다모달 코사톨 보상 모델의 미세 조정",
      "submittedOnDailyBy": {
        "_id": "654c6845bac6e6e49895a5b5",
        "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
        "isPro": false,
        "fullname": "Yibin Wang",
        "user": "CodeGoat24",
        "type": "user"
      },
      "summary": "최근의 다모둠의 보상 모델(RM)의 발전은 시각 모델과 인간의 취향을 일치시키기 위한 보상 신호의 제공에 있어 상당한 기대를 나타내고 있습니다. 그러나 현재의 RM은 직접적인 응답을 제공하거나 얕은 사유 과정으로 인해 깊이를 제한하고 있으며, 이는 보상 신호의 불정확성을招く 경우가 많습니다. 우리는 보상 사유 과정에 긴 계층의 생각(CoT)을 명시적으로 포함하여 그 신뢰성과 강도를 크게 강화할 수 있음을 주장합니다. 또한 RM이 CoT 사유를 내부화한 후, 직접적인 응답의 정확도도 은닉한 사유 능력에 의해 개선될 수 있다는 것을 믿습니다. 이점에 대해 본 논문에서는 첫 번째 통일된 다모둠의 CoT 기반의 보상 모델인 UnifiedReward-Think를 제안합니다. 이 모델은 시각 이해와 생성 보상 태스크의 두 가지 영역에서 다차원으로 단계별로 긴 계층의 사유를 수행할 수 있습니다. 특히, 우리는 탐험을 주도하는 강화학습의 미세 조정 접근 방식을 채택하여 모델의 잠재적인 복잡한 사유 능력을 발휘하고 보상을 촉발시키는 것을 목표로 합니다: (1) 먼저, GPT-4o의 사유 과정을 학습시키기 위해少量의 이미지 생성의 취향 데이터를 사용하며, 이를 모델의 냉동의 시작으로 사용하며, CoT 사유의 형식과 구조를 학습하는 것을 목표로 합니다. (2) 그 후, 모델의 사전 지식과 일반화 능력을 활용하여 큰 규모의 통일된 다모둠의 취향 데이터를 준비하고, 시각 태스크의 광범위한 범위에서 모델의 사유 과정을 발휘합니다. 이 단계에서는, 정확한 사유 출력을 유지하며, 거부 샘플링을 사용하여 모델을 훈련합니다. (3) 반면, 불정확한 예측 샘플은 최종적으로 그룹 상대 정책 최적화(GRPO)에 기반한 강화학습의 미세 조정을 사용하여 다양한 사유 패스를 탐색하고, 정확한 및 강인한 결정을 최적화하는 것을 가능하게 합니다. 시각 보상 태스크의 광범위한 범위에서 확장된 실험은 본 논문의 모델의 우수성을 보여줍니다.",
      "upvotes": 50,
      "discussionId": "681aac50d31f567552f0cc5d",
      "projectPage": "https://codegoat24.github.io/UnifiedReward/think",
      "githubRepo": "https://github.com/CodeGoat24/UnifiedReward",
      "ai_keywords": [
        "Multimodal Reward Models (RMs)",
        "long chains of thought (CoT)",
        "UnifiedReward-Think",
        "exploration-driven reinforcement fine-tuning",
        "small amount of image generation preference data",
        "GPT-4o",
        "large-scale unified multimodal preference data",
        "rejection sampling",
        "Group Relative Policy Optimization (GRPO)",
        "reinforcement fine-tuning"
      ]
    },
    "publishedAt": "2025-05-06T04:46:41.000Z",
    "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning",
    "summary": "Recent advances in multimodal Reward Models (RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallow reasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the reward reasoning process can significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt an\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill the reasoning process of GPT-4o,\nwhich is then used for the model's cold start to learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model's reasoning process across various vision\ntasks. During this phase, correct reasoning outputs are retained for rejection\nsampling to refine the model (3) while incorrect predicted samples are finally\nused for Group Relative Policy Optimization (GRPO) based reinforcement\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\nfor correct and robust solutions. Extensive experiments across various vision\nreward tasks demonstrate the superiority of our model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03318.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654c6845bac6e6e49895a5b5",
      "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
      "fullname": "Yibin Wang",
      "name": "CodeGoat24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03335",
      "authors": [
        {
          "_id": "681ab9b8f43603c60cab87a3",
          "name": "Andrew Zhao",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a4",
          "user": {
            "_id": "647eb24118274bce0308b2b8",
            "avatarUrl": "/avatars/463e49a89b61164ccfad85ced10658b2.svg",
            "isPro": false,
            "fullname": "Yiran Wu",
            "user": "kevinwyr",
            "type": "user"
          },
          "name": "Yiran Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:25.939Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a5",
          "user": {
            "_id": "649d475111592b1a765ac1a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
            "isPro": false,
            "fullname": "Yang Yue",
            "user": "Yang130",
            "type": "user"
          },
          "name": "Yang Yue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:23.436Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a6",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a7",
          "name": "Quentin Xu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a8",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a9",
          "name": "Matthieu Lin",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87aa",
          "user": {
            "_id": "6486dde1f74857df3f1a5828",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
            "isPro": false,
            "fullname": "Shenzhi Wang",
            "user": "shenzhi-wang",
            "type": "user"
          },
          "name": "Shenzhi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:28.623Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ab",
          "name": "Qingyun Wu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ac",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-07T01:39:06.052Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ad",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T09:08:00.000Z",
      "submittedOnDailyAt": "2025-05-07T00:10:48.130Z",
      "title": "「0 데이터 기반의 강화 학습 사이트 플레이 추론」",
      "submittedOnDailyBy": {
        "_id": "630482fbce6b12280b18971d",
        "avatarUrl": "/avatars/b07f31fd970d736bdf574d56da7a5634.svg",
        "isPro": false,
        "fullname": "Andrew Zhao",
        "user": "andrewzh",
        "type": "user"
      },
      "summary": "실현 가능한 보상에 기반한 강화학습(RLVR)은 결과 기반의 보상을 통해 대규모의 언어 모델의 논리적 능력 향상이 가능한 점을 보여주고 있습니다. 최근의 RLVR 연구는 0 설정에서 동작하여 논리적 프로세스 라벨링에 대한 서브젝션을 피할 수 있지만, 아직도 자동으로 편집된 품질 좋은 질문과 대답의 수집이 테스트 데이터에 의존하고 있습니다. 고품질, 인간이 만든 예의 부족은 장기적인 스케일라빌리테이티에 대한 인간이 서브젝션을 의존하는 것에 대한 우려를 불러일으키고 있습니다. 또한, 미래의 AI가 인간의 지능을 초월하는 경우, 인간이 제공하는 태스크는 초지능 시스템에 제한된 학습 잠재력을 제공할 가능성이 제한될 수 있습니다. 이러한 우려에 대처하기 위해, 우리는 하나의 모델이 스스로 학습 진보를 최대화하는 태스크를 제안하고 이를 해결함으로써 논리적 능력 향상을 목표로 하는 새로운 RLVR 패러다임 'Absolute Zero'를 제안하고 있습니다. 이 패러다임 아래, 우리는 제안된 코드 논리적 태스크를 코드 젝스퍼터를 사용하여 검증하고 답을 확인함으로써 논리적 능력과 훈련 코스를 자기 진화시키는 시스템 'Absolute Zero Reasoner (AZR)'를 소개하고 있습니다. AZR는 외부 데이터를 완전히 의존하지 않지만, 코드와 수학적인 논리적 태스크에서 가장 先端의 성능을 달성하고, 현재의 0 설정 모델을 초월하고 있습니다. 또한, AZR는 다른 모델 규모와 모델 클래스의 적용성을 보여주고, 효과적으로 적용할 수 있음을 보여줍니다.",
      "upvotes": 43,
      "discussionId": "681ab9baf43603c60cab881a",
      "projectPage": "https://andrewzh112.github.io/absolute-zero-reasoner/",
      "githubRepo": "https://github.com/LeapLabTHU/Absolute-Zero-Reasoner",
      "ai_keywords": [
        "Reinforcement learning with verifiable rewards (RLVR)",
        "zero setting",
        "outcome-based rewards",
        "manually curated collections",
        "superintelligent system",
        "Absolute Zero",
        "AzR (Absolute Zero Reasoner)",
        "training curriculum",
        "code executor",
        "verifiable reward",
        "open-ended yet grounded learning",
        "SOTA performance",
        "coding and mathematical reasoning tasks"
      ]
    },
    "publishedAt": "2025-05-06T05:08:00.000Z",
    "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03335.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630482fbce6b12280b18971d",
      "avatarUrl": "/avatars/b07f31fd970d736bdf574d56da7a5634.svg",
      "fullname": "Andrew Zhao",
      "name": "andrewzh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03730",
      "authors": [
        {
          "_id": "681abf3759155282c1cb2306",
          "name": "Shiyi Zhang",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2307",
          "user": {
            "_id": "64970d3d9c3b29dca8633f87",
            "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
            "isPro": false,
            "fullname": "JunhaoZhuang",
            "user": "JunhaoZhuang",
            "type": "user"
          },
          "name": "Junhao Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:58.214Z",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2308",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2309",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb230a",
          "name": "Yansong Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:58:02.000Z",
      "submittedOnDailyAt": "2025-05-07T00:37:06.442Z",
      "title": "FlexiAct: 휘발한 액션 제어를 heterógeneous한 시나리오에 적용하기\n\n(Note: The translation provided is a direct literal translation. For a more natural and contextually appropriate translation, consider the following:\n\nFlexiAct: 휘발한 액션 제어를 다양한 시나리오에 적용하기\n\nThis version maintains the flexibility and adaptability of the original term \"heterogeneous\" while being more commonly used in Korean for diverse or varied scenarios.)",
      "submittedOnDailyBy": {
        "_id": "6315d306a9456afe2b9bf34a",
        "avatarUrl": "/avatars/7285b4e7d84b528d1a50f8ee4eb10727.svg",
        "isPro": false,
        "fullname": "ElevenZ",
        "user": "shiyi0408",
        "type": "user"
      },
      "summary": "アクションカスタマイズメント는, シェープガイドされたまたはグローバルモーションカスタマイズメントを使用して, 入力制御シグナルによって指示されたアクションを実行するビデオを生成することである。現在の方法は, スペクトラル構造の厳格な制約（例えば, ラウタイ、スケルタル、視点の一貫性）により, 多様な主題とシナリオに対する適応性が低下している。これらの制限を克服するために, 私たちはFlexiActを提案しています。FlexiActは, 参照ビデオからアクションをターゲット画像に転送することを目的としています。現在の方法と違い, FlexiActは参照ビデオの主題とターゲット画像の間にスペクトラル構造（ラウタイ、スケルタル、視点）の変化を許容し, アクションの一貫性を維持することができます。これを実現するには, アクションの精密な制御、スペクトラル構造の適応、アクションの一貫性の維持が必要です。これに向けて, 私たちはRefAdapterを導入しています。RefAdapterは, スペクトラル適応と一貫性の維持に特化し, 現在の方法を超えて, アプエアンコンシステンシーと構造的な柔軟性のバランスを実現することができます。また, 私たちの観察から, デノイズプロセスは, 時間ステップごとに動き（低周波）とアプエアン詳細（高周波）に対する注意が異なることがわかりました。そこで, 私たちはFAE（周波数達知アクション抽出）を提案しています。FAEは, 現在の方法と違い, 別々のスペクトラル・タイムステップアーキテクチャを使用しないで, デノイズプロセス中でアクション抽出を直接実現します。実験は, 我々の方法が, 多様なラウタイ、スケルタル、視点の主題にアクションを有効に転送することを示しています。我々は, コードとモデル重みを公開して, さらなる研究のためのサポートを提供します. https://shiyi-zh0408.github.io/projectpages/FlexiAct/",
      "upvotes": 17,
      "discussionId": "681abf3859155282c1cb23fa",
      "projectPage": "https://shiyi-zh0408.github.io/projectpages/FlexiAct/",
      "githubRepo": "https://github.com/shiyi-zh0408/FlexiAct",
      "ai_keywords": [
        "FlexiAct",
        "RefAdapter",
        "image-conditioned adapter",
        "spatial adaptation",
        "consistency preservation",
        "FAE",
        "Frequency-aware Action Extraction",
        "denoising process",
        "spatial-temporal architectures",
        "action extraction"
      ]
    },
    "publishedAt": "2025-05-06T13:58:02.000Z",
    "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios",
    "summary": "Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03730.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6315d306a9456afe2b9bf34a",
      "avatarUrl": "/avatars/7285b4e7d84b528d1a50f8ee4eb10727.svg",
      "fullname": "ElevenZ",
      "name": "shiyi0408",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02922",
      "authors": [
        {
          "_id": "681abdbef8feaef23b543877",
          "name": "Yaoqi Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543878",
          "name": "Jinkai Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543879",
          "user": {
            "_id": "667135bdcca06def1c2599a6",
            "avatarUrl": "/avatars/d94ab99265e1970852605d344b4d69e9.svg",
            "isPro": false,
            "fullname": "Baotong Lu",
            "user": "baotonglu",
            "type": "user"
          },
          "name": "Baotong Lu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-07T01:56:16.240Z",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387a",
          "user": {
            "_id": "66e96f58de9ebeee86f5e27f",
            "avatarUrl": "/avatars/e7d692aa47f02f1858186f47186166ad.svg",
            "isPro": false,
            "fullname": "Qianxi Zhang",
            "user": "qianxizhang",
            "type": "user"
          },
          "name": "Qianxi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:00.341Z",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387b",
          "name": "Chengruidong Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387c",
          "name": "Jingjia Luo",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387d",
          "name": "Di Liu",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387e",
          "name": "Huiqiang Jiang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387f",
          "name": "Qi Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543880",
          "name": "Jing Liu",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543881",
          "name": "Bailu Ding",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543882",
          "name": "Xiao Yan",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543883",
          "name": "Jiawei Jiang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543884",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543885",
          "name": "Mingxing Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543886",
          "name": "Yuqing Yang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543887",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543888",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T18:01:17.000Z",
      "submittedOnDailyAt": "2025-05-07T00:36:48.274Z",
      "title": "RetroInfer: 스케일러블한 긴 문맥 LLM의 추론에 대한 벡터 저장 방법\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "6278bd42541f3d2dfa77ea70",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
        "isPro": true,
        "fullname": "Huiqiang Jiang",
        "user": "iofu728",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)의 컨텍스트 길이가 증가함에 따라, GPU 메모리와 Bandwidth의 제약으로 효율적인 추론에 중대한 문제를 야기하고 있습니다. 우리는 긴 컨텍스트 LLM의 추론을 가속화하기 위해, 고유의 注意의 희소성을 활용한 벡터 스택 시스템으로서의 키 밸류(KV) 캐시를 재 개념화하여 새로운 시스템 RetroInfer를 소개합니다. 그 핵심은 wave index입니다. wave index는 세 분의 注意 근사, 정확도 제한된 注意 추정, 분리된 클러스터링 등 방법들을 활용하여, 중요 토큰의 효율적이고 정확한 검색을 가능하게 합니다. wave buffer와 결합하여, KV 캐시의 배치를 협조하고, GPU와 CPU의 계산과 데이터 전송을 병렬화하여, 높은 트랜스포트 유지합니다. 선행의 희소성 기반의 방법과 달리, RetroInfer는 토큰 선택과 하드웨어의 협조에 어려움을 갖지 않도록, 모델의 정확도를 희생하지 않고 강력한 성능을 제공합니다. 긴 컨텍스트 벤치마크에서의 실험은, GPU 메모리의 제한 내에서 전체 注意보다 4.5배의 속도 향상을 달성하고, KV 캐시를 CPU 메모리에 확장하여, 희소 注意 기반 라인에 대한 10.5배의 속도 향상을 달성하며, 전체 注意 수준의 정확도를 유지합니다.",
      "upvotes": 16,
      "discussionId": "681abdc0f8feaef23b543926",
      "ai_keywords": [
        "RetrInfer",
        "key-value (KV) cache",
        "vector storage system",
        "wave index",
        "Attention-aWare VEctor index",
        "tripartite attention approximation",
        "accuracy-bounded attention estimation",
        "segmented clustering",
        "wave buffer",
        "GPU memory",
        "token selection",
        "hardware coordination"
      ]
    },
    "publishedAt": "2025-05-05T14:01:17.000Z",
    "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
    "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02922.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6278bd42541f3d2dfa77ea70",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
      "fullname": "Huiqiang Jiang",
      "name": "iofu728",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03005",
      "authors": [
        {
          "_id": "681ac8a09f4ed2ece10fac18",
          "user": {
            "_id": "647f4bac45baf21ad709fcd0",
            "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
            "isPro": false,
            "fullname": "Dan Goldstein",
            "user": "SmerkyG",
            "type": "user"
          },
          "name": "Daniel Goldstein",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:53.268Z",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac19",
          "name": "Eric Alcaide",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac1a",
          "name": "Janna Lu",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac1b",
          "name": "Eugene Cheah",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/sfcDfWZfka9sy8siQC6MV.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/yXcUOgY48NS_nfIZN0Kqq.png"
      ],
      "publishedAt": "2025-05-05T20:03:28.000Z",
      "submittedOnDailyAt": "2025-05-07T01:19:36.031Z",
      "title": "RADLADS: 스케일에서 선형 어타치의 빠른 어텐션 스타일화",
      "submittedOnDailyBy": {
        "_id": "647f4bac45baf21ad709fcd0",
        "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
        "isPro": false,
        "fullname": "Dan Goldstein",
        "user": "SmerkyG",
        "type": "user"
      },
      "summary": "라플라티나 에프센션 변환을 렐러어 어텐션 디코더에 확장한 RADLADS(Scale에서 빠른 어텐션 분馏를 렐러어 어텐션 디코더로)를 소개합니다. 이는 소프트맥스 어텐션 트랜스포머를 빠르게 렐러어 어텐션 디코더 모델로 변환하는 프로토콜로, 두 가지 새로운 RWKV-variant 아키텍처와, 전문적인 Qwen2.5 오픈 소스 모델(7B, 32B, 72B 사이즈)을 포함합니다. 변환 프로세스에는 원의 교사 모델의 토큰 카운트의 0.005% 정도인 350-700M 토큰이 필요합니다. 현재 가격에서 72B 렐러어 어텐션 모델의 변환 비용은 2,000 USD 미만입니다が, 추론 시의 질의는 원의 트랜스포머와 거의 유사합니다. 이러한 모델은 렐러어 어텐션 모델의 사이즈의 표준 벤치마크에서 가장 선진한 하류 성능을 달성합니다. 모든 모델은 Apache 2.0 라이센스의 기반으로 HuggingFace에서 릴리즈되며, 72B 모델은 Qwen License Agreement에 따라 같은 제한을 받습니다.\n\n모델은 다음과 같은 URL에서 공개됩니다:\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\n\n훈련 코드는 다음과 같은 URL에서 공개됩니다:\nhttps://github.com/recursal/RADLADS-paper",
      "upvotes": 14,
      "discussionId": "681ac8a19f4ed2ece10fac75",
      "ai_keywords": [
        "Rapid Attention Distillation",
        "RADLADS",
        "softmax attention transformers",
        "linear attention decoder models",
        "RWKV-variant",
        "Qwen2.5",
        "token count",
        "linear attention models",
        "HuggingFace",
        "Apache 2.0 license",
        "Qwen License Agreement"
      ]
    },
    "publishedAt": "2025-05-05T16:03:28.000Z",
    "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale",
    "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/sfcDfWZfka9sy8siQC6MV.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/yXcUOgY48NS_nfIZN0Kqq.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03005.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f4bac45baf21ad709fcd0",
      "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
      "fullname": "Dan Goldstein",
      "name": "SmerkyG",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02872",
      "authors": [
        {
          "_id": "681b03a33bf166767107c74b",
          "name": "Cfir Avraham Hadar",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74c",
          "name": "Omer Shubi",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74d",
          "name": "Yoav Meiri",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74e",
          "name": "Yevgeni Berzak",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T13:23:48.000Z",
      "submittedOnDailyAt": "2025-05-07T05:24:49.077Z",
      "title": "로드잉 읽기 중 눈의 이동으로부터 개방적인 정보 탐색의 목적의 밝혀짐",
      "submittedOnDailyBy": {
        "_id": "60ef001bed64a34082bfa0dd",
        "avatarUrl": "/avatars/78e4daeac169edbf4dc42fbed9b50d59.svg",
        "isPro": false,
        "fullname": "Omer Shubi",
        "user": "scaperex",
        "type": "user"
      },
      "summary": "독서하는 과정에서 특정 정보에 관심이 많은 경우가 종종 있습니다. 예를 들어, 이 논문을 읽고 대두연결 모델(LLMs)에 대한 관심이 있는 경우, 실험 설계에 대한 관심 있는 경우, 또는 \"이것은 효과적일까?\"라는 질문에 대한 관심 있는 경우 등이 있습니다. 광범위한 범위에서 일상 생활에서도, 독서에 대한 여러 텍스트 고유의 목표가 독서 행동을 지도하고 있습니다. 이 연구에서는, 처음으로 개방된 독서 목표가 독서 시의 눈동자 이동으로부터 자동적으로 해석될 수 있는지를 조사하는 시도를 합니다. 이 문제를 해결하기 위해, 목표 분류 및 목표 구성 태스크와 평가 프레임워크를 제안하고, 영어 독서 데이터에 대해 많은 텍스트 고유의 정보 탐색 태스크를 포함하는 대규모 눈동자 추적 데이터로 사용합니다. 목표 분류와 목표 구성에 대한 분리 및 생성적인 다모달 LLMs를 개발하고 비교합니다. 이러한 실험은 두 태스크에서 상당한 성공을 보여주고, LLMs가 독서 시의 눈동자 이동으로부터 독서자의 텍스트 고유의 목표에 대한 유익한 정보를 추출할 수 있음을 보여줍니다.",
      "upvotes": 11,
      "discussionId": "681b03a43bf166767107c787",
      "ai_keywords": [
        "goal classification",
        "goal reconstruction",
        "discriminative models",
        "generative models",
        "multimodal LLMs",
        "large-scale eye tracking",
        "text-specific information seeking"
      ]
    },
    "publishedAt": "2025-05-04T09:23:48.000Z",
    "title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in\n  Reading",
    "summary": "When reading, we often have specific information that interests us in a text.\nFor example, you might be reading this paper because you are curious about LLMs\nfor eye movements in reading, the experimental design, or perhaps you only care\nabout the question ``but does it work?''. More broadly, in daily life, people\napproach texts with any number of text-specific goals that guide their reading\nbehavior. In this work, we ask, for the first time, whether open-ended reading\ngoals can be automatically decoded from eye movements in reading. To address\nthis question, we introduce goal classification and goal reconstruction tasks\nand evaluation frameworks, and use large-scale eye tracking for reading data in\nEnglish with hundreds of text-specific information seeking tasks. We develop\nand compare several discriminative and generative multimodal LLMs that combine\neye movements and text for goal classification and goal reconstruction. Our\nexperiments show considerable success on both tasks, suggesting that LLMs can\nextract valuable information about the readers' text-specific goals from eye\nmovements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02872.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60ef001bed64a34082bfa0dd",
      "avatarUrl": "/avatars/78e4daeac169edbf4dc42fbed9b50d59.svg",
      "fullname": "Omer Shubi",
      "name": "scaperex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02214",
      "authors": [
        {
          "_id": "6819905519b420a2f91aa231",
          "user": {
            "_id": "64612660933afb0106a9dee3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
            "isPro": false,
            "fullname": "Xingyu Zheng",
            "user": "Xingyu-Zheng",
            "type": "user"
          },
          "name": "Xingyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:43.859Z",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa232",
          "name": "Yuye Li",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa233",
          "user": {
            "_id": "68164883244ee586cb159268",
            "avatarUrl": "/avatars/856ca8731ca156f616529e427d8fd76a.svg",
            "isPro": false,
            "fullname": "初浩然",
            "user": "HaoranChu",
            "type": "user"
          },
          "name": "Haoran Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:41.155Z",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa234",
          "name": "Yue Feng",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa235",
          "name": "Xudong Ma",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa236",
          "name": "Jie Luo",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa237",
          "name": "Jinyang Guo",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa238",
          "name": "Haotong Qin",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa239",
          "name": "Michele Magno",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa23a",
          "name": "Xianglong Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64612660933afb0106a9dee3/uQV5mzdTqoNatTz5fk6xC.png"
      ],
      "publishedAt": "2025-05-04T18:43:44.000Z",
      "submittedOnDailyAt": "2025-05-07T02:59:22.920Z",
      "title": "Qwen3의 실험적인 축소 연구",
      "submittedOnDailyBy": {
        "_id": "64612660933afb0106a9dee3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
        "isPro": false,
        "fullname": "Xingyu Zheng",
        "user": "Xingyu-Zheng",
        "type": "user"
      },
      "summary": "Qwen 系列는 오픈 소스의 대형 언어 모델(LLMs)의 선도적인 가족으로 나타났으며, 자연어 이해 태스크에서 뛰어난 능력을 보여주고 있습니다. 최근의 Qwen3의 릴리즈에서는 다양한 벤치마크에서 최상위의 성능을 발휘하며, 리소스 제한된 환경에서 이러한 모델을 효율적으로 처리하는 데 대한 관심이 증가하고 있습니다. 저 비트 수의 퀀텀화는 유망한 해결책으로 제안되어 있지만, Qwen3의 성능에 미치는 영향은 아직 자세히 조사되지 않았습니다.本研究에서는, Qwen3의 퀀텀화 설정에 따른 강건성을 체계적으로 평가하고, 이 최尖端 모델의 압축에 대한 기회와 문제점을 밝혀내는 것을 목표로 합니다. Qwen3에 대해 5가지 현재의 클래식적인 후 학습 퀀텀화 기술에 엄격하게 평가하고, 1~8 비트의 비트 폭을 범위로, 여러 데이터 세트를 통해 그 효과를 평가했습니다. 우리의 발견은, Qwen3은 중도에 비트 폭에서 상대적으로 경쟁적인 성능을 유지하지만, 초 저 정밀도의 언어 태스크에서 뚜렷한 하락을 확인하며, LLM의 압축에 대한 지속적인 문제점을 밝혀냅니다. 이러한 결과를 통해, 극한 퀀텀화 시나리오에서 성능 손실을 줄이기 위한 연구의 필요성을 강조합니다. 우리는, 이 실험적 분석이 Qwen3과 미래의 LLMs에 적합한 퀀텀화 방법의 발전에 연결되는 구체적인 이점을 제공하기를 기대하고 있습니다. 이 프로젝트는, https://github.com/Efficient-ML/Qwen3-Quantization과 https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b에서 릴리즈되었습니다.",
      "upvotes": 7,
      "discussionId": "6819905519b420a2f91aa27c",
      "githubRepo": "https://github.com/Efficient-ML/Qwen3-Quantization",
      "ai_keywords": [
        "Low-bit quantization",
        "Post-training quantization techniques",
        "Bit-widths",
        "Linguistic tasks",
        "LLM compression",
        "Quantization methods"
      ]
    },
    "publishedAt": "2025-05-04T14:43:44.000Z",
    "title": "An Empirical Study of Qwen3 Quantization",
    "summary": "The Qwen series has emerged as a leading family of open-source Large Language\nModels (LLMs), demonstrating remarkable capabilities in natural language\nunderstanding tasks. With the recent release of Qwen3, which exhibits superior\nperformance across diverse benchmarks, there is growing interest in deploying\nthese models efficiently in resource-constrained environments. Low-bit\nquantization presents a promising solution, yet its impact on Qwen3's\nperformance remains underexplored. This study conducts a systematic evaluation\nof Qwen3's robustness under various quantization settings, aiming to uncover\nboth opportunities and challenges in compressing this state-of-the-art model.\nWe rigorously assess 5 existing classic post-training quantization techniques\napplied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their\neffectiveness across multiple datasets. Our findings reveal that while Qwen3\nmaintains competitive performance at moderate bit-widths, it experiences\nnotable degradation in linguistic tasks under ultra-low precision, underscoring\nthe persistent hurdles in LLM compression. These results emphasize the need for\nfurther research to mitigate performance loss in extreme quantization\nscenarios. We anticipate that this empirical analysis will provide actionable\ninsights for advancing quantization methods tailored to Qwen3 and future LLMs,\nultimately enhancing their practicality without compromising accuracy. Our\nproject is released on https://github.com/Efficient-ML/Qwen3-Quantization and\nhttps://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64612660933afb0106a9dee3/uQV5mzdTqoNatTz5fk6xC.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02214.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64612660933afb0106a9dee3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
      "fullname": "Xingyu Zheng",
      "name": "Xingyu-Zheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03735",
      "authors": [
        {
          "_id": "681acb9c285636e830d37c8e",
          "user": {
            "_id": "6625ce8074ae2df4e3effa92",
            "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
            "isPro": false,
            "fullname": "Jiayuan Rao 饶珈源",
            "user": "Homie0609",
            "type": "user"
          },
          "name": "Jiayuan Rao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:50.372Z",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c8f",
          "name": "Zifeng Li",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c90",
          "user": {
            "_id": "632c7a0d1d303f5f9acf01b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
            "isPro": false,
            "fullname": "Haoning Wu",
            "user": "haoningwu",
            "type": "user"
          },
          "name": "Haoning Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-07T02:56:09.106Z",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c91",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c92",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c93",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:59:31.000Z",
      "submittedOnDailyAt": "2025-05-07T01:26:13.756Z",
      "title": "전체적인 축구 이해를 위한 다 에이전트 시스템\n\n(Note: The translation is provided as requested, without any additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "6625ce8074ae2df4e3effa92",
        "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
        "isPro": false,
        "fullname": "Jiayuan Rao 饶珈源",
        "user": "Homie0609",
        "type": "user"
      },
      "summary": "최근, AI를 주도한 축구 이해 분야의 발전은 빠르게 진행되고 있지만, 현재의 연구는 주로 분리된 요소에 집중하고, 좁은 태스크에 초점을 맞추고 있습니다. 이러한 간극을 메우기 위해, 축구 전반에 대한 이해를 위한 엄격한 프레임워크를 제안합니다. 구체적인로, 이 논문에서는 다음과 같은 기여를 수행합니다: (i) SoccerWiki를 구축합니다. 이는 처음의 규모가 큰 다모델의 축구 지식베이스이며, 선수, 팀, 裁判, 피어스폿 등 다양한 분야의 풍부한 지식들을 통합하고, 지식 기반의 추론을 가능하게 합니다. (ii) SoccerBench를 소개합니다. 이는 가장 엄격한 축구 고유의 벤치마크이며, 13가지의 다양한 이해 태스크에 대해 약 10K의 표준화된 다모델(텍스트, 이미지, 영상)의 다선택 문제 쌍을 제공하며, 자동 피일틴과 손으로 확인을 통해 검증되었습니다. (iii) SoccerAgent를 소개합니다. 이는 새로운 다 에이전트 시스템이며, 복잡한 축구 문제를 협업된 추론을 통해 분해하고, SoccerWiki에서 영역专用의 지식을 활용하여 강력한 성능을 달성합니다. (iv) 매우 광범위한 평가와 제거 시험을 수행하며, SoccerBench에서 가장 先端의 MLLM을 벤치마크로 하고, 제안된 에이전트 시스템의 우월성을 명확히 합니다. 모든 데이터와 코드는 아래 URL에서 공개되어 있습니다: https://jyrao.github.io/SoccerAgent/",
      "upvotes": 5,
      "discussionId": "681acb9e285636e830d37d4b",
      "projectPage": "https://jyrao.github.io/SoccerAgent/",
      "githubRepo": "https://github.com/jyrao/SoccerAgent",
      "ai_keywords": [
        "multimodal",
        "knowledge base",
        "domain knowledge",
        "multimodal (text, image, video) multi-choice QA pairs",
        "multi-agent system",
        "collaborative reasoning",
        "state-of-the-art MLLMs"
      ]
    },
    "publishedAt": "2025-05-06T13:59:31.000Z",
    "title": "Multi-Agent System for Comprehensive Soccer Understanding",
    "summary": "Recent advancements in AI-driven soccer understanding have demonstrated rapid\nprogress, yet existing research predominantly focuses on isolated or narrow\ntasks. To bridge this gap, we propose a comprehensive framework for holistic\nsoccer understanding. Specifically, we make the following contributions in this\npaper: (i) we construct SoccerWiki, the first large-scale multimodal soccer\nknowledge base, integrating rich domain knowledge about players, teams,\nreferees, and venues to enable knowledge-driven reasoning; (ii) we present\nSoccerBench, the largest and most comprehensive soccer-specific benchmark,\nfeaturing around 10K standardized multimodal (text, image, video) multi-choice\nQA pairs across 13 distinct understanding tasks, curated through automated\npipelines and manual verification; (iii) we introduce SoccerAgent, a novel\nmulti-agent system that decomposes complex soccer questions via collaborative\nreasoning, leveraging domain expertise from SoccerWiki and achieving robust\nperformance; (iv) extensive evaluations and ablations that benchmark\nstate-of-the-art MLLMs on SoccerBench, highlighting the superiority of our\nproposed agentic system. All data and code are publicly available at:\nhttps://jyrao.github.io/SoccerAgent/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6625ce8074ae2df4e3effa92",
      "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
      "fullname": "Jiayuan Rao 饶珈源",
      "name": "Homie0609",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03368",
      "authors": [
        {
          "_id": "681b05ab8e325b4097318969",
          "user": {
            "_id": "661e841972c7030f3fd57eb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661e841972c7030f3fd57eb9/PysszxrOl__sZ-IToZfJU.jpeg",
            "isPro": false,
            "fullname": "Stef De Sabbata",
            "user": "sdesabbata",
            "type": "user"
          },
          "name": "Stef De Sabbata",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:24.829Z",
          "hidden": false
        },
        {
          "_id": "681b05ab8e325b409731896a",
          "name": "Stefano Mizzaro",
          "hidden": false
        },
        {
          "_id": "681b05ab8e325b409731896b",
          "name": "Kevin Roitero",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T09:40:06.000Z",
      "submittedOnDailyAt": "2025-05-07T05:33:25.973Z",
      "title": "지오스펙트럼 기계적 설명성의 대규모 언어 모델",
      "submittedOnDailyBy": {
        "_id": "620cca6f06a4320dbf3b50d8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654011487582-620cca6f06a4320dbf3b50d8.png",
        "isPro": false,
        "fullname": "Kevin Roitero",
        "user": "kevinr",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)은 다양한 자연어 처리 태스크에서 전례가 없는 능력을 보여주고 있습니다. 이들이 텍스트와 코드를 처리하고 생성하는 능력은 여러 분야에서 가장 광범위하게 사용되고 있지만, 지식 기반과 '론론' 툴로서의 활용은 지속적인 연구의 영역입니다. 지구학에서 LLMs의 지구적 지식과 공간론의 능력에 대한 평가에 초점을 맞추는 문헌이 증가하고 있지만, 이러한 모델의 내부 기능에 대해, 특히 지구적 데이터를 어떻게 처리하는지에 대한 구체적인 지식은 아직 부족합니다.\n\n이 장에서는, 새로운 프레임워크를 구축하여 지구 공간 구조의 설명 가능성에 대한 연구를 수행합니다. 이는 공간 분석을 사용하여 LLMs가 지구적 데이터를 어떻게 처리하는지를 역학으로 수행하기 위해 사용됩니다. 우리의 목표는 이러한 복잡한 모델이 지구적 데이터를 처리하여 생성하는 내부 표현을 이해하는 데 집중됩니다. 이러한 모델이 지구적 데이터에 어떻게 생각하는지를 이 프레임워크가 필요로 하는 것과 같이 고려할 수 있습니다만, 이는 과도한 인간화는 아닙니다.\n\n먼저, LLMs의 내부 구조를 밝혀내기 위한 폼을 설명합니다. 다음으로, 구조적 설명 가능성 분야를 통해 스펙트럼 디오디온론과 희소 오토인코더의 역할을 논의합니다. 이러한 모델의 다중 의미적인 내부 표현을 밝혀, 설명 가능한 단일 의미적인 특징으로 분해하기 위해 그 역할을 설명합니다. 실험에서는 공간 자기상관성을 사용하여, 지역명과 관련된 특징이 공간 패턴에 관련이 있는 것을 보여주고, 이러한 모델이 지구적 데이터를 어떻게 처리하는지에 대한 더 깊은 이해를 제공합니다. 마지막으로, 우리의 프레임워크가 지구학의 기초 모델의 연구와 사용에 어떻게 도움을 줄 수 있는지 논의합니다.",
      "upvotes": 3,
      "discussionId": "681b05ad8e325b4097318a3d",
      "ai_keywords": [
        "probing",
        "mechanistic interpretability",
        "superposition hypothesis",
        "sparse autoencoders",
        "polysemantic",
        "monosemantic",
        "spatial autocorrelation",
        "placenames",
        "geospatial patterns"
      ]
    },
    "publishedAt": "2025-05-06T05:40:06.000Z",
    "title": "Geospatial Mechanistic Interpretability of Large Language Models",
    "summary": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03368.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620cca6f06a4320dbf3b50d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654011487582-620cca6f06a4320dbf3b50d8.png",
      "fullname": "Kevin Roitero",
      "name": "kevinr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03164",
      "authors": [
        {
          "_id": "681ac5d2626f5a368b2a7103",
          "name": "Ji Won Chung",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7104",
          "name": "Tongyu Zhou",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7105",
          "name": "Ivy Chen",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7106",
          "name": "Kevin Hsu",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7107",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7108",
          "name": "Alexa Siu",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7109",
          "name": "Shunan Guo",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710a",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:55.782Z",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710b",
          "name": "James Tompkin",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710c",
          "name": "Jeff Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T04:18:42.000Z",
      "submittedOnDailyAt": "2025-05-07T01:00:46.546Z",
      "title": "InfoVids: 시청자의 비디오 경험에 대한 재평가를 수행하고, 다른 시각화 방법과 프레젠테이션 간의 관계를 활용하여 재설계를 수행합니다.",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "전통적인 데이터 표현은 프레젠테이터와 시각화를 서로 다른 두 공간에 분리하여 있습니다 - 3차원 세계와 2차원 스크린 - 이를 통해 시각화 엔지니어의 이야기가 강화됩니다. 인간적인 시청자 경험으로 만들기 위해, 우리는 InfoVids를 사용하여 프레젠테이터와 시각화를 더 공정하게 연결합니다. 이 정보 그래픽 비디오는 프레젠테이터와 시각화의 관계를 재설정하기 위해 제작되었습니다. InfoVids를 설계하는 과정에서, 라우어, 포어, 인터랙션의 사용으로 시청자 경험이 어떻게 변하는지 조사하고 있습니다. 30명의 참가자와 9개의 포인트 준비로, 기준적인 2차원 슬라이드와 비교하여, 자기소개적인 시각에서 실용적인 장기적인 정보 제공을 제공합니다. 혼합 방법의 분석에 의해, 이 패러다임은 시청자의 注意 분열을 줄이고, 시각화에서 프레젠테이터로의 집중을 변경하고, 시청자에게 더 상호작용적이고 자연스럽고 즐거운 전신적인 데이터 성능을 실현했습니다. 최종적으로, InfoVids는 시청자에게 프레젠테이터와 시각화의 전통적인 동기를 재평가시킵니다.",
      "upvotes": 3,
      "discussionId": "681ac5d4626f5a368b2a71f8"
    },
    "publishedAt": "2025-05-06T00:18:42.000Z",
    "title": "InfoVids: Reimagining the Viewer Experience with Alternative\n  Visualization-Presenter Relationships",
    "summary": "Traditional data presentations typically separate the presenter and\nvisualization into two separate spaces--the 3D world and a 2D screen--enforcing\nvisualization-centric stories. To create a more human-centric viewing\nexperience, we establish a more equitable relationship between the\nvisualization and the presenter through our InfoVids. These\ninfographics-inspired informational videos are crafted to redefine\nrelationships between the presenter and visualizations. As we design InfoVids,\nwe explore how the use of layout, form, and interactions affects the viewer\nexperience. We compare InfoVids against their baseline 2D `slides' equivalents\nacross 9 metrics with 30 participants and provide practical, long-term insights\nfrom an autobiographical perspective. Our mixed methods analyses reveal that\nthis paradigm reduced viewer attention splitting, shifted the focus from the\nvisualization to the presenter, and led to more interactive, natural, and\nengaging full-body data performances for viewers. Ultimately, InfoVids helped\nviewers re-imagine traditional dynamics between the presenter and\nvisualizations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03164.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03739",
      "authors": [
        {
          "_id": "681b232d7167935b1e979e64",
          "name": "Zuwei Long",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e65",
          "name": "Yunhang Shen",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e66",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e67",
          "name": "Heting Gao",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e68",
          "name": "Lijiang Li",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e69",
          "name": "Peixian Chen",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6a",
          "name": "Mengdan Zhang",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6b",
          "name": "Hang Shao",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6c",
          "name": "Jian Li",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6d",
          "name": "Jinlong Peng",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6e",
          "name": "Haoyu Cao",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6f",
          "name": "Ke Li",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e70",
          "name": "Rongrong Ji",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e71",
          "name": "Xing Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:59:53.000Z",
      "submittedOnDailyAt": "2025-05-07T08:07:03.177Z",
      "title": "VITA-Audio: 효율적인 대규모 음성・언어 모델에서 고속 교차 모델 토큰 생성",
      "submittedOnDailyBy": {
        "_id": "6483143902f98c3f05aff915",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6483143902f98c3f05aff915/ZhWFFgrlRsQf4MXiInh5p.jpeg",
        "isPro": true,
        "fullname": "沈云航 Yunhang Shen",
        "user": "shenyunhang",
        "type": "user"
      },
      "summary": "자연스러운 인간-컴퓨터 상호작용의 요구가 증가하는 가운데, 스peech-based 시스템은 일상적인 커뮤니케이션의 가장 일반적인 형태인 스peech를 활용하여 주목을 집중 받고 있습니다. 그러나 현재의 스peech 모델은 스트리밍 시 처음의 음성 토큰을 생성하는 과정에서 높은 latency를 인정하고, 이를 채택함에 있어 중대한 붕괴로 작용하고 있습니다. 이러한 문제를 해결하기 위해, VITA-Audio라는 단말기부터 단말기까지의 대규모 스peech 모델을 제안합니다. 특히, 가벼운 Multiple Cross-modal Token Prediction (MCTP) 모듈을 도입하여, 한 번의 모델의 전파 과정에서 효율적으로 다수의 음성 토큰을 생성함으로써, 추론 속도를 가속화하고, 스트리밍 시나리오에서 처음의 음성의 생성 latency를 크게 줄입니다. 또한, 4단계의 진보적인 훈련 전략을 검토하여, 음성의 품질의 최소한의 손실로 모델의 가속화를 실현합니다. 우리의 지식에 따르면, VITA-Audio는 처음의 멀티 모델의 대규모 언어 모델이며, 처음의 전파 과정에서 음성 출력을 생성할 수 있으며, 최소한의 latency로 실시간 커뮤니케이션 능력이 허용됩니다. VITA-Audio는 완전하게 재현 가능한 모델이며, 데이터셋은 오픈 소스입니다. 실험 결과는, 우리의 모델은 7B 파라미터 크기로 3~5배의 추론 속도 업그레이드를 달성하고, 자동 언어 인식 (ASR), 문맥 생성 (TTS), 대화형 질의응답 (SQA) 등 여러 벤치마크에서 유사한 규모의 오픈 소스 모델보다 뚜렷하게 뛰어납니다.",
      "upvotes": 2,
      "discussionId": "681b232e7167935b1e979ec6",
      "ai_keywords": [
        "end-to-end large speech model",
        "Multiple Cross-modal Token Prediction (MCTP)",
        "model forward pass",
        "inference",
        "streaming scenarios",
        "four-stage progressive training strategy",
        "multi-modal large language model",
        "real-time conversational capabilities",
        "inference speedup",
        "automatic speech recognition (ASR)",
        "text-to-speech (TTS)",
        "spoken question answering (SQA)"
      ]
    },
    "publishedAt": "2025-05-06T13:59:53.000Z",
    "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model",
    "summary": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03739.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6483143902f98c3f05aff915",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6483143902f98c3f05aff915/ZhWFFgrlRsQf4MXiInh5p.jpeg",
      "fullname": "沈云航 Yunhang Shen",
      "name": "shenyunhang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02311",
      "authors": [
        {
          "_id": "681a06459c99f4b5ce5f2838",
          "user": {
            "_id": "658e85bb5b7553ca5c29ba89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
            "isPro": false,
            "fullname": "Jihao Zhao",
            "user": "Robot2050",
            "type": "user"
          },
          "name": "Jihao Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T16:51:29.234Z",
          "hidden": false
        },
        {
          "_id": "681a06459c99f4b5ce5f2839",
          "name": "Chunlai Zhou",
          "hidden": false
        },
        {
          "_id": "681a06459c99f4b5ce5f283a",
          "name": "Biao Qin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T01:45:56.000Z",
      "submittedOnDailyAt": "2025-05-07T00:06:14.265Z",
      "title": "インバーカーインターフェイス는 필요에 따라 호출하는 것: 대규모 언어 모델의 적응적인 호출을 통한 문제 해결",
      "submittedOnDailyBy": {
        "_id": "658e85bb5b7553ca5c29ba89",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
        "isPro": false,
        "fullname": "Jihao Zhao",
        "user": "Robot2050",
        "type": "user"
      },
      "summary": "대규모와 소규모의 언어 모델(LM)의 협업 패러다임은 성능과 비용의 균형을 효과적으로 조정할 수 있지만, 소규모의 LM에서 헛소리의 발생 시점을 정확히 파악하는 것은 중요한 문제입니다. 기존의 최적화 노력을 주로 후처리 방법들에 초점을 맞추며, 이들은 LM의 설명 과정에서 분리되어, 높은 계산 비용과 제한된 효과성을 동반했습니다. 본 논문에서는, 소규모의 LM의 생성 과정에서 헛소리의 축적과 전파를 계산하기 위한 실용적인 호출 평가 지표인 \"AttenHScore\"를 제안하고, 이 지표를 사용하여 잠재적인 설명 오류를 연속적으로 확장시켜서, 대규모의 LM의 실시간 호출을 더욱 정확하게 수행할 수 있습니다. 또한, 소규모의 LM의 설명 능력의 한계에 대해 고려하고, 불확실성에 대한 지식 재구성을 활용하여, 서로 다른 텍스트 챗넨크에서 중요한 정보를 더 효과적으로 추출할 수 있습니다. 확장된 실험은, 여러 QA 데이터셋에서 실시간 헛소리 감지 능력을 향상시키기 위해, \"AttenHScore\"가 기준보다 더 뛰어난 것을 명확히 보여주고, 특히 복잡한 질문을 처리하는 경우 특히 효과적이라는 것을 보여줍니다. 또한, 본 논문의 전략은 추가 모델 훈련의 필요성을 제거하고, 다양한 Transformer 기반의 LM에 대한 적응성을 보여주고 있습니다.",
      "upvotes": 2,
      "discussionId": "681a06469c99f4b5ce5f28a3",
      "ai_keywords": [
        "large language models (LMs)",
        "small language models (LMs)",
        "hallucinations",
        "post-processing techniques",
        "AttenHScore",
        "generation process",
        "reasoning errors",
        "dynamic adjustment",
        "real-time invocation",
        "reasoning capacity",
        "uncertainty-aware knowledge reorganization",
        "QA datasets",
        "complex queries",
        "transformer-based LMs"
      ]
    },
    "publishedAt": "2025-05-04T21:45:56.000Z",
    "title": "Invoke Interfaces Only When Needed: Adaptive Invocation for Large\n  Language Models in Question Answering",
    "summary": "The collaborative paradigm of large and small language models (LMs)\neffectively balances performance and cost, yet its pivotal challenge lies in\nprecisely pinpointing the moment of invocation when hallucinations arise in\nsmall LMs. Previous optimization efforts primarily focused on post-processing\ntechniques, which were separate from the reasoning process of LMs, resulting in\nhigh computational costs and limited effectiveness. In this paper, we propose a\npractical invocation evaluation metric called AttenHScore, which calculates the\naccumulation and propagation of hallucinations during the generation process of\nsmall LMs, continuously amplifying potential reasoning errors. By dynamically\nadjusting the detection threshold, we achieve more accurate real-time\ninvocation of large LMs. Additionally, considering the limited reasoning\ncapacity of small LMs, we leverage uncertainty-aware knowledge reorganization\nto assist them better capture critical information from different text chunks.\nExtensive experiments reveal that our AttenHScore outperforms most baseline in\nenhancing real-time hallucination detection capabilities across multiple QA\ndatasets, especially when addressing complex queries. Moreover, our strategies\neliminate the need for additional model training and display flexibility in\nadapting to various transformer-based LMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02311.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658e85bb5b7553ca5c29ba89",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
      "fullname": "Jihao Zhao",
      "name": "Robot2050",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.21650",
      "authors": [
        {
          "_id": "681b258f23dbf6b59c5fc735",
          "name": "Haiyang Zhou",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc736",
          "user": {
            "_id": "63f095be6309c84d5f48848a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f095be6309c84d5f48848a/pL2CKi-r-0mMfIGhYSAsm.jpeg",
            "isPro": false,
            "fullname": "Wangbo Yu",
            "user": "Drexubery",
            "type": "user"
          },
          "name": "Wangbo Yu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-07T09:19:13.250Z",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc737",
          "name": "Jiawen Guan",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc738",
          "name": "Xinhua Cheng",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc739",
          "name": "Yonghong Tian",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc73a",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T13:55:28.000Z",
      "submittedOnDailyAt": "2025-05-07T07:52:52.459Z",
      "title": "ホロタイム：フィードバックモデル을 사용하여 フラットディープラーニング에 대한 パノラマ4Dスケーン의 생성을 실현하는 방법\n\n(注意：原始文本中的“フラットディープラーニング”在韩语中没有直接对应的词汇，因此保留了日语原文。如果需要完全的韩语翻译，可以考虑使用“Flat Deep Learning”或“2D Deep Learning”等术语。)",
      "submittedOnDailyBy": {
        "_id": "66eeda3676a8038cb448f11d",
        "avatarUrl": "/avatars/8d6e61f4c9354c6720ccaa7be0fe1d9f.svg",
        "isPro": false,
        "fullname": "Haiyang Zhou",
        "user": "Marblueocean",
        "type": "user"
      },
      "summary": "ディフュージョンモデル의 급격한 발전은 VR와 AR 기술의 적용에 대한 혁신적인 가능성을 보여주고 있습니다. 일반적으로, 이러한 기술은 사용자 경험 측면에서, 스케인 수준의 4D 자산이 필요합니다. 그러나 현재의 ディフュージョンモデル은 주로 정적 3D 스케인이나 객체 수준의 동작을 모델링하고 있으므로, 진정한 만족스러운 경험을 제공할 수 있는 한계가 있습니다. 이러한 문제를 해결하기 위해, 우리는 HoloTime을 제안하고 있습니다. 이 프레임워크는 비디오 ディフュージョンモデル을 통합하여, 하나의 Prompt 또는 참조 이미지에서 360도 4D 스케인 재구성 방법을 결합하여, 생성된 360도 비디오를 4D 자산으로 변환하여, 사용자가 완전한 4D 경험을 제공받을 수 있는 것입니다. 특히, 비디오 ディフュージョンモデル을 360도 비디오의 생성에 적용하기 위해, 우리는 360World 데이터셋을 소개하고 있습니다. 이는 4D 스케인 재구성 작업에 적합한 360도 비디오의 첫 번째 상세한 집합입니다. 이 집합을 기반으로, 우리는 Panoramic Animator를 제안하고 있습니다. 이는 2 단계의 이미지에서 비디오 ディフュージョンモデル을 사용하여, 고품질의 360도 영상을 360도 비디오로 변환할 수 있는 것입니다. 다음으로, Panoramic Space-Time Reconstruction을 소개합니다. 이는 공간-시간의 깊이 추정 방법을 사용하여, 생성된 360도 비디오를 4D 포인트 클러스터로 변환하여, 시간적으로 일치하는 4D 스케인의 재구성을 가능하게 합니다. 우리의 방법의 효과를 평가하기 위해, 현재의 방법과 비교 분석을 수행하고, 360도 비디오의 생성과 4D 스케인 재구성 모두에서 우위를 보여주었습니다. 이는 우리의 방법, 사용자가 흥미를 계속 불러일으키며, 실제적인 만족스러운 환경을 만들 수 있는 능력, VR와 AR 애플리케이션의 사용자 경험 향상을 보여주고 있습니다.",
      "upvotes": 1,
      "discussionId": "681b259123dbf6b59c5fc7cf",
      "projectPage": "https://zhouhyocean.github.io/holotime/",
      "githubRepo": "https://github.com/PKU-YuanGroup/HoloTime",
      "ai_keywords": [
        "diffusion models",
        "HoloTime",
        "video diffusion models",
        "panoramic videos",
        "360-degree 4D scene reconstruction",
        "360World dataset",
        "Panoramic Animator",
        "image-to-video diffusion model",
        "Panoramic Space-Time Reconstruction",
        "space-time depth estimation",
        "4D point clouds",
        "Gaussian Splatting",
        "4D scenes",
        "spatially and temporally consistent"
      ]
    },
    "publishedAt": "2025-04-30T09:55:28.000Z",
    "title": "HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene\n  Generation",
    "summary": "The rapid advancement of diffusion models holds the promise of\nrevolutionizing the application of VR and AR technologies, which typically\nrequire scene-level 4D assets for user experience. Nonetheless, existing\ndiffusion models predominantly concentrate on modeling static 3D scenes or\nobject-level dynamics, constraining their capacity to provide truly immersive\nexperiences. To address this issue, we propose HoloTime, a framework that\nintegrates video diffusion models to generate panoramic videos from a single\nprompt or reference image, along with a 360-degree 4D scene reconstruction\nmethod that seamlessly transforms the generated panoramic video into 4D assets,\nenabling a fully immersive 4D experience for users. Specifically, to tame video\ndiffusion models for generating high-fidelity panoramic videos, we introduce\nthe 360World dataset, the first comprehensive collection of panoramic videos\nsuitable for downstream 4D scene reconstruction tasks. With this curated\ndataset, we propose Panoramic Animator, a two-stage image-to-video diffusion\nmodel that can convert panoramic images into high-quality panoramic videos.\nFollowing this, we present Panoramic Space-Time Reconstruction, which leverages\na space-time depth estimation method to transform the generated panoramic\nvideos into 4D point clouds, enabling the optimization of a holistic 4D\nGaussian Splatting representation to reconstruct spatially and temporally\nconsistent 4D scenes. To validate the efficacy of our method, we conducted a\ncomparative analysis with existing approaches, revealing its superiority in\nboth panoramic video generation and 4D scene reconstruction. This demonstrates\nour method's capability to create more engaging and realistic immersive\nenvironments, thereby enhancing user experiences in VR and AR applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21650.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66eeda3676a8038cb448f11d",
      "avatarUrl": "/avatars/8d6e61f4c9354c6720ccaa7be0fe1d9f.svg",
      "fullname": "Haiyang Zhou",
      "name": "Marblueocean",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.18373",
      "authors": [
        {
          "_id": "681af7a58f41e99d8ca05074",
          "user": {
            "_id": "6454d2ee1a543cf97b1ba671",
            "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
            "isPro": false,
            "fullname": "shen",
            "user": "lorashen",
            "type": "user"
          },
          "name": "Lei Shen",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-07T06:09:12.039Z",
          "hidden": false
        },
        {
          "_id": "681af7a58f41e99d8ca05075",
          "name": "Xiaoyu Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-25T14:17:47.000Z",
      "submittedOnDailyAt": "2025-05-07T04:49:41.738Z",
      "title": "Auto-SLURP: 스마트 프라이빗 보조자인 다 에이전트 프레임워크 평가용 벤치마크 데이터셋",
      "submittedOnDailyBy": {
        "_id": "6454d2ee1a543cf97b1ba671",
        "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
        "isPro": false,
        "fullname": "shen",
        "user": "lorashen",
        "type": "user"
      },
      "summary": "최근, 대규모 언어 모델(LLMs)를 기반으로 한 다 에이전트 프레임워크가 급격히 발전하고 있습니다. 이 발전 중, 특히 LLM 기반의 다 에이전트 프레임워크의 성능 평가에 적합한 벤치마크 데이터 세트가 부족했습니다. 이를 채워주기 위해, 우리는 스마트한 개인 보조의 배경에서 LLM 기반의 다 에이전트 프레임워크의 성능을 평가하기 위한 벤치마크 데이터 세트인 \"Auto-SLURP\"를 소개합니다. Auto-SLURP는 초기에 개발된 원형 SLURP 데이터 세트를 확장하고, 데이터의 재 라벨링과 컴퓨터 서버 및 외부 서비스의 통합을 통해 생성되었습니다. 이 확장에서는 언어 이해, 태스크 실행, 응답 생성을 모두 평가하기 위한 일관된 엔드 엔드 평가 프로세스를 구현했습니다. 우리의 실험은 현재의 최尖端 프레임워크에 대한 가장 중요한 도전을 상징하며, 진정으로 신뢰할 수 있는 스마트한 다 에이전트 개인 보조는 아직 진행 중임을 명확히 합니다. 데이터 세트와 관련된 코드는 https://github.com/lorashen/Auto-SLURP/ 에 공개되어 있습니다.",
      "upvotes": 0,
      "discussionId": "681af7a68f41e99d8ca050ba",
      "githubRepo": "https://github.com/lorashen/Auto-SLURP/",
      "ai_keywords": [
        "multi-agent frameworks",
        "large language models (LLMs)",
        "benchmark datasets",
        "natural language understanding tasks",
        "simulated servers",
        "external services",
        "end-to-end evaluation pipeline",
        "language understanding",
        "task execution",
        "response generation"
      ]
    },
    "publishedAt": "2025-04-25T10:17:47.000Z",
    "title": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant",
    "summary": "In recent years, multi-agent frameworks powered by large language models\n(LLMs) have advanced rapidly. Despite this progress, there is still a notable\nabsence of benchmark datasets specifically tailored to evaluate their\nperformance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset\naimed at evaluating LLM-based multi-agent frameworks in the context of\nintelligent personal assistants. Auto-SLURP extends the original SLURP dataset\n-- initially developed for natural language understanding tasks -- by\nrelabeling the data and integrating simulated servers and external services.\nThis enhancement enables a comprehensive end-to-end evaluation pipeline,\ncovering language understanding, task execution, and response generation. Our\nexperiments demonstrate that Auto-SLURP presents a significant challenge for\ncurrent state-of-the-art frameworks, highlighting that truly reliable and\nintelligent multi-agent personal assistants remain a work in progress. The\ndataset and related code are available at\nhttps://github.com/lorashen/Auto-SLURP/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18373.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6454d2ee1a543cf97b1ba671",
      "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
      "fullname": "shen",
      "name": "lorashen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]