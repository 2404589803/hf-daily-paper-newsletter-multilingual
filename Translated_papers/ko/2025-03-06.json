[
  {
    "paper": {
      "id": "2503.00865",
      "authors": [
        {
          "_id": "67c666245e2443d7d5e9b76a",
          "user": {
            "_id": "64802face9ff472e30dc1ceb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64802face9ff472e30dc1ceb/bcwTlgpaUrU7m2RMB5zCc.png",
            "isPro": false,
            "fullname": "Yiran Zhao",
            "user": "Yiran0924",
            "type": "user"
          },
          "name": "Yiran Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:51:21.231Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76b",
          "user": {
            "_id": "61657b0b20606e5e73f611cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61657b0b20606e5e73f611cc/6ZPne2GYlWkxrx35ND1P8.png",
            "isPro": false,
            "fullname": "CHAOQUN LIU",
            "user": "lukecq",
            "type": "user"
          },
          "name": "Chaoqun Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:27:33.956Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76c",
          "name": "Yue Deng",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76d",
          "user": {
            "_id": "671609f7664f44a151f1f0e8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fEQLuH1kdW5Pd9Y_J64hN.png",
            "isPro": false,
            "fullname": "jiahao ying",
            "user": "jhying",
            "type": "user"
          },
          "name": "Jiahao Ying",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:39.926Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76e",
          "user": {
            "_id": "6539c87ba318a98bf0d15dd8",
            "avatarUrl": "/avatars/beb9ba6eeacb61addc5897836bd59f55.svg",
            "isPro": false,
            "fullname": "Mahani Aljunied",
            "user": "maljunied",
            "type": "user"
          },
          "name": "Mahani Aljunied",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:33.285Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76f",
          "name": "Zhaodonghui Li",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b770",
          "user": {
            "_id": "6454685a548f22be598414c4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg",
            "isPro": false,
            "fullname": "Lidong Bing",
            "user": "LidongBing",
            "type": "user"
          },
          "name": "Lidong Bing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:19.611Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b771",
          "user": {
            "_id": "604f67ef0fe8ff3ec13d71ef",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
            "isPro": false,
            "fullname": "Hou Pong (Ken) Chan",
            "user": "kenchan0226",
            "type": "user"
          },
          "name": "Hou Pong Chan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:50.272Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b772",
          "name": "Yu Rong",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b773",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b774",
          "user": {
            "_id": "60dff6ae19a362a8c27862aa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60dff6ae19a362a8c27862aa/LIYzLB3cdPh-B3XIBgBCC.jpeg",
            "isPro": false,
            "fullname": "Wenxuan Zhang",
            "user": "isakzhang",
            "type": "user"
          },
          "name": "Wenxuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:27:36.769Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T11:53:55.000Z",
      "title": "바벨： 개방형 몬이거럴 대 언어 모델 90% 이상의 세계의 사용자를 서비스 합니다.",
      "summary": "대 언어 모델(LLMs)는 자연어 처리(NLP)에 혁신을 가져왔지만, 오픈 소스의 다언어 LLMs는 희귀하며, 기존 모델들은 언어 커버 면적에 제한이 있습니다. 이러한 모델들은 일반적으로 자원 풍부한 언어를 우선시하고 널리 사용되지만 자원 부족의 언어는 버려집니다. 이러한 차이를 해결하기 위해, 우리는 Babel라는 오픈 소스의 다언어 LLMs를 소개합니다. 이 모델은 25대 언어를 커버하고, 세계 인구의 90%를 초과하는 사람들의 언어를 지원하며, 다른 오픈 소스의 다언어 LLMs가 버려진 많은 언어를 포함합니다. 이러한 모델들은 일반적인 연속적인 예측 학습 접근 방식과 다릅니다. 파라미터 수를 층 확장 기술에 의해 확장하여 성능의 한계를 높입니다. 우리는 Babel-9B와 Babel-83B의 두 버전을 소개합니다. 첫 번째는 효율적인 추론과 미세 조정을 목표로 하며, 두 번째는 오픈 소스의 다언어 LLMs의 새로운 기준을 세팅합니다. 다언어 태스크에 대한 광범위한 평가에서, 상대적으로 같은 크기의 오픈 소스의 LLMs와 비교하여 높은 성능을 나타냅니다. 또한, 오픈 소스의 슈퍼 바이버 디닝 피니티닝 데이터 세트를 사용하여, Babel은 뛰어난 성능을 달성하며, Babel-9B-Chat은 10B 사이즈의 LLMs 중에서 가장 뛰어난 모델이며, Babel-83B-Chat은 다언어 태스크에서 새로운 기준을 세팅하고, 비즈니스 모델 수준에 도달합니다.",
      "upvotes": 32,
      "discussionId": "67c666255e2443d7d5e9b7b3",
      "projectPage": "https://babel-llm.github.io/babel-llm/",
      "githubRepo": "https://github.com/babel-llm/babel-llm"
    },
    "publishedAt": "2025-03-05T21:49:03.700Z",
    "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00865.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64802face9ff472e30dc1ceb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64802face9ff472e30dc1ceb/bcwTlgpaUrU7m2RMB5zCc.png",
      "fullname": "Yiran Zhao",
      "name": "Yiran0924",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.00329",
      "authors": [
        {
          "_id": "67c755f898a2e37274c62c96",
          "name": "Benjamin Schneider",
          "hidden": false
        },
        {
          "_id": "67c755f898a2e37274c62c97",
          "name": "Florian Kerschbaum",
          "hidden": false
        },
        {
          "_id": "67c755f898a2e37274c62c98",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:50:07.881Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-01T03:29:02.000Z",
      "title": "ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC:",
      "summary": "ビジュアルエンベッディングモデル은, ビジュアル検索や分類などのゼロショットタスクに優れています。しかし, これらのモデルは, 構想不明やユーザーの指示が必要なタスクに使用できません。これらのタスクは, ビジュアルと自然言語の入力を組み合わせたエンブディングを出力するための多モーダルエンブディングモデルが必要となります。現在のCLIPベースのアプローチは, 画像とテキストを独立にエンブディングし、結果を融合しています。私たちは, これがモーダル間の弱い相互作用とユーザーの表現制御の悪い性能を帯びていることを見出しました。私たちは, ビジョンラベルモデルバックボードを使用して画像特徴と自然言語の指示を深く統合するための開放ソース多モーダルエンブディングモデルABCを紹介します。ABCは, MSCOCO画像からのテキスト検索で最も良い性能を達成し、マジックマルチモーダルエンブディングベンチマークで分類やVQAタスクで最も優れています。強力なビジョンラベル表現を持つABCは, 自然言語を使って軽微なやつらやその可能性がある構想不明なビジュアル検索問題を解決できます。この能力を評価するために, 私たちは, 正しい検索を求めるためにテキストインストラクションと画像内容を交互にするベンチマークCtrlBenchを設計しました。ABCは, 高品質の表現と柔軟な自然言語制御を提供して, 多モーダルエンブディングの状態を進めます。私たちのモデルとデータセットは, プロジェクトページで提供されています。",
      "upvotes": 10,
      "discussionId": "67c7560298a2e37274c6311d",
      "projectPage": "https://tiger-ai-lab.github.io/ABC/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/ABC"
    },
    "publishedAt": "2025-03-05T21:33:37.945Z",
    "title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/b2vg-4UWwvcEboAZgK-Sv.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00329.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03278",
      "authors": [
        {
          "_id": "67c94e5f8c4ef8be73583f4b",
          "name": "Jun Li",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4c",
          "user": {
            "_id": "631b9ff5824f2502e3557c7e",
            "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
            "isPro": false,
            "fullname": "liu",
            "user": "che111",
            "type": "user"
          },
          "name": "Che Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:03:06.882Z",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4d",
          "name": "Wenjia Bai",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4e",
          "name": "Rossella Arcucci",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4f",
          "name": "Cosmin I. Bercea",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f50",
          "name": "Julia A. Schnabel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T09:02:33.000Z",
      "title": "知識 설명을 사용한 시각 언어 모델의 이상성 기반화 향상",
      "summary": "Visual Language Models (VLMs)는 시각 기반의 태스크에서 놀라운 능력을 보여주고 있습니다. 그러나 의료 분야에서 효과성과 특히 의료 영상 내의 이상 검출과 국소화에 대한 조사는 아직 심도 있는 상태입니다. 주요 문제점은 의료용 용어의 복잡성과 추상성이며, 병학적 이상팀과 시각적 특징과의 직접적인 연관성이 어려워서입니다. 본 논문에서는 분해된 의료 지식의 활용을 통해 VLM의 의료 이상 검출과 국소화에 대한 성능을 향상시키는 새로운 접근을 제안합니다. 특정 이상에 대한 모델의 직접적인 인식보다는 의료 개념을 기본적인 속성 및 공통적인 시각적 패턴으로 분해하는 것을 중점으로 합니다. 이 전략은 설명과 시각적 특징 사이의 더 강한 일치를 촉진하고, 의료 영상 내의 이상 인식과 국소화에 모두 개선합니다. 우리 방법은 0.23B Florence-2 base model에서 평가되었으며, 7B LLaVA 기반의 의료용 VLM과 비교하여 유사한 성능을 달성합니다. 특히, 이 모델은 데이터의 1.5%만 학습하였더라도 이상 검출의 성능이 향상됩니다. 실험 결과를 통해, 기존의 이상과 이전에未见한 이상에도 우리의 접근의 효과성과 강력한 일반화 능력을 보여주고 있습니다.",
      "upvotes": 9,
      "discussionId": "67c94e608c4ef8be73583f7b",
      "projectPage": "https://lijunrio.github.io/AG-KD/"
    },
    "publishedAt": "2025-03-06T02:29:15.964Z",
    "title": "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631b9ff5824f2502e3557c7e",
      "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
      "fullname": "liu",
      "name": "che111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03751",
      "authors": [
        {
          "_id": "67c912b1b5903dd437cc2370",
          "user": {
            "_id": "658529d61c461dfe88afe8e8",
            "avatarUrl": "/avatars/a22c1b07d28c2662833c462c6537d835.svg",
            "isPro": false,
            "fullname": "Xuanchi Ren",
            "user": "xrenaa",
            "type": "user"
          },
          "name": "Xuanchi Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:55:04.321Z",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2371",
          "name": "Tianchang Shen",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2372",
          "name": "Jiahui Huang",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2373",
          "name": "Huan Ling",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2374",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2375",
          "name": "Merlin Nimier-David",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2376",
          "name": "Thomas Müller",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2377",
          "name": "Alexander Keller",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2378",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2379",
          "name": "Jun Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T18:59:50.000Z",
      "title": "GEN3C: 3D 정보에 의한 세계적인 동화 생성 및 정확한 카메라 제어",
      "summary": "GEN3C는 구조적인 카메라 제어와 시간의 3D 일관성을 가진 생성적인 이미지 모델입니다. 기존의 이미지 모델은 현실적인 이미지를 생성하지만, 3D 정보가 적고, 객체의 존재 여부 등 불확실성이 많았습니다. 카메라 제어는 카메라 파라미터가 신경망의 입력으로, 이러한 카메라로 이미지가 어떻게 변하는지 추론하는 것이 필요하여 정확한 제어가 어려워졌습니다. GEN3C는 3D 캐치 버퍼를 가이드로 사용합니다: 시드 이미지 또는 이전에 생성된 프레임의 픽셀별 깊이를 예측한 포인트 클러스터로, 다음 프레임을 생성할 때 사용자가 제공된 새로운 카메라 트라이アク션과 3D 캐치 버퍼의 2D 렌더링에 기반합니다. 중요한 점은 GEN3C는 이전에 생성한 것을 기억할 필요가 없고, 카메라의 자세에서 이미지의 구조를 추론하는 필요도 없으며, 모든 생성력을 이전에 본 적이 없는 영역으로 집중할 수 있습니다. 모델은 이전에 본 적이 없는 영역으로의 생성력을 다발적으로 발휘하고, 다음 프레임으로 시뮬레이션을 진행할 수 있습니다. 우리의 결과는 기존 기술보다 더 정확한 카메라 제어를 보여주고, 희귀 뷰의 새로운 뷰 합성에도 최尖端의 결과를 보여주고 있습니다. 특히, 회전장면과 단조감소 이미지 등 어려운 설정에서도 우수한 성능을 보여주고 있습니다. 결과는 이미지에서 가장 잘 볼 수 있습니다. 홈페이지를 확인하세요! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
      "upvotes": 9,
      "discussionId": "67c912b9b5903dd437cc2505"
    },
    "publishedAt": "2025-03-05T22:13:22.552Z",
    "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03751.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6288
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02951",
      "authors": [
        {
          "_id": "67c907ea7568a12737ad4535",
          "user": {
            "_id": "653df1323479e9ebbe3eb6cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
            "isPro": true,
            "fullname": "Zhangchen Xu",
            "user": "flydust",
            "type": "user"
          },
          "name": "Zhangchen Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:26:50.636Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4536",
          "user": {
            "_id": "637c88b6d55081513c5690d8",
            "avatarUrl": "/avatars/6766e23ebf46b46d6c8b48351c571907.svg",
            "isPro": false,
            "fullname": "Yang Liu",
            "user": "nlpyang",
            "type": "user"
          },
          "name": "Yang Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-06T02:26:54.940Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4537",
          "user": {
            "_id": "605e8dfd5abeb13e714c4c18",
            "avatarUrl": "/avatars/bc27a0ed17b2bd4311e89d3028fa327b.svg",
            "isPro": false,
            "fullname": "yueqin yin",
            "user": "yyqoni",
            "type": "user"
          },
          "name": "Yueqin Yin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:26:48.614Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4538",
          "user": {
            "_id": "653b2524b77b5e255f2d29d2",
            "avatarUrl": "/avatars/f69aea8de84c435295e7638bad5bd82e.svg",
            "isPro": false,
            "fullname": "Mingyuan Zhou",
            "user": "mingyuanzhou",
            "type": "user"
          },
          "name": "Mingyuan Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:03:56.474Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4539",
          "name": "Radha Poovendran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T19:17:36.000Z",
      "title": "KodCode: 다양성, 어려움, 검증 가능한 합성 데이터 세트입니다.",
      "summary": "KodCode는 ラージェットラングアウェイモデル 학습 시의 고품질으로 확인 가능한 훈련 데이터의 획득에 대한 장기적인 문제를 해결하기 위한 합성 데이터 세트입니다. 현재의 코딩 포커스 리소스는 광범위한 커버리지(예: 간단한 코딩 태스크부터 고난도 알고리즘 문제까지) 또는 확인 가능한 정확성(예: 유닛 테스트)의 두 가지를 모두 보장할 수 없습니다. 반면에, KodCode는 자동 인증 프로세스에서 체계적으로 인증된 문제-해결책-테스트 테일러를 구성하고 있습니다. 우리의 파이프라인은 광범위한 코딩 문제를 합성하고, 더 어려운 문제를 대처하기 위해 추가적인 실험을 분배하여 해결책과 테스트 케이스를 생성합니다. 마지막으로, 훈련 후의 데이터 합성은 DeepSeek R1의 이유 모델로부터 테스트 베이스의 거부 샘플링 프로세스를 통해 질의를 변경하고 대답을 생성합니다. 이 파이프라인은 규모적, 강건한 다양성 있는 코딩 데이터 세트를 생성합니다. KodCode는 규범적인 훈련을 가능하게 하며, 쌍의 유닛 테스트와 RL 조정에 큰 잠재력을 제공합니다. 코딩 벤치마크(HumanEval(+)、MBPP(+)、BigCodeBench、LiveCodeBench)에서의 훈련 실험은 KodCode로 조정된 모델이 최선 성능을 달성하고, Qwen2.5-Coder-32B-Instruct 및 DeepSeek-R1-Distill-Llama-70B보다 뛰어난 성능을 보여주었습니다.",
      "upvotes": 7,
      "discussionId": "67c907ee7568a12737ad4633",
      "projectPage": "https://kodcode-ai.github.io/",
      "githubRepo": "https://github.com/KodCode-AI/kodcode"
    },
    "publishedAt": "2025-03-05T21:31:01.626Z",
    "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "flydust",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01836",
      "authors": [
        {
          "_id": "67c94c32dd505e6a4db201a2",
          "user": {
            "_id": "65d9a453f20e4fc19480afba",
            "avatarUrl": "/avatars/27bfa034a13a5e7cb5fc3b647515a201.svg",
            "isPro": false,
            "fullname": "yisen li",
            "user": "yisenL",
            "type": "user"
          },
          "name": "Yisen Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:04:09.687Z",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a3",
          "name": "Lingfeng Yang",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a4",
          "name": "Wenxuan Shen",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a5",
          "name": "Pan Zhou",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a6",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a7",
          "name": "Weiwei Lin",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a8",
          "user": {
            "_id": "643be8879f5d314db2d9ed23",
            "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
            "isPro": false,
            "fullname": "Chen Dongping",
            "user": "shuaishuaicdp",
            "type": "user"
          },
          "name": "Dongping Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:09:00.496Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T18:56:44.000Z",
      "title": "CrowdSelect: 합성된 명령어 데이터 선택에 의한 다 LLM 지식",
      "summary": "大語言モデル의 指導則遵守能力を選択された サブセットを使用して 小さな モデルに 導出する手法が モデル訓練の 主流となっています。現在の 合成的な 指導データ選択戦略は 主に 単次元の 信号に 基づいています（例えば、報酬スコア、モデルの 難易度），それらは 多様な 分野での 指導則遵守の 複雑性を 捉えずに います。そこで、我々は、複雑な 指導応答ペアの 特性を 捉えるために、より 多様な 信号を 検討し、Multi-LLMの 知識を 活用した 3つの 基盤メトリックを 提案します。これらの 基盤メトリックに 基づいて、我々は クラスタリングベースの アプローチを 採用した CrowdSelectを 提案します。我々の 検証結果に よると、4つの 基盤モデルを 通じて MT-benchと Arena-Hardでの 性能が 一致していることが 示されます。CrowdSelectは、すべての メトリックを 効率的に 統合し、Llama-3.2-3b-instructでは Arena-Hardで 4.81%、MT-benchで 11.1%の 向上を 示します。我々の 研究結果は、将来の 研究に おいて 有効な エリアを 提供しようとします。コードは、https://github.com/listentm/crowdselectに 公開されています。",
      "upvotes": 6,
      "discussionId": "67c94c33dd505e6a4db201f6",
      "githubRepo": "https://github.com/listentm/crowdselect"
    },
    "publishedAt": "2025-03-06T02:20:38.735Z",
    "title": "CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01836.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.18860",
      "authors": [
        {
          "_id": "67c95acf88c3b4201c10b9e9",
          "name": "Md Mehrab Tanjim",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ea",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9eb",
          "name": "Mike Rimer",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ec",
          "name": "Xiang Chen",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ed",
          "name": "Sungchul Kim",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ee",
          "name": "Vaishnavi Muppala",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ef",
          "name": "Tong Yu",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f0",
          "name": "Zhengmian Hu",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f1",
          "name": "Ritwik Sinha",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f2",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f3",
          "name": "Iftikhar Ahamath Burhanuddin",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f4",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:24:24.968Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T06:05:29.000Z",
      "title": "응답: 대화 태스크의 차이에 따라 개발된 리트리이팅 접근법",
      "summary": "회화사소cian은 과거의 상호작용의 일부를 활용하여, 사용자의 질문이나 요구에 대한 의미 있는(정확한) 답변을 제공하기 위해 질문 변경 알고리즘을 필요로 하는 경우가 많습니다. 그러나 정확한 변경 접근 방식은 대화사소cian이 지원하는 태스크나 다른 제약에 따라 다릅니다. 본 논문에서는 두 가지 근본적으로 다른 생성 태스크에서 두 가지 다른 접근 방식인 \"변경\"과 \"합성\"에 대해 체계적으로 조사합니다. 이러한 태스크는 텍스트로부터 텍스트 생성 태스크와, 텍스트를 입력으로 하여 사용자의 질문에 대답하는 시각화 또는 데이터 테이블을 생성하는 다모델 생성 태스크입니다. 우리의 결과를 통해 특정한 변경 또는 합성 접근 방식은 가벼운 사용 사례와 생성 태스크에 크게 의존합니다. 특히, 대화사소cian의 경우, 질문 변경 접근 방식이 가장 적합하며, 데이터 분석사소cian의 경우, 질문 합성 접근 방식이 가장 적합합니다. 특히, 데이터 분석사소cian의 사용 사례에 대해, 짧은 데이터 세트와 긴 데이터 세트의 두 가지를 조사한 결과, 질문 합성은 항상 더 좋은 성능을 나타내며, 대화 텍스트 기반의 질문 대답에 대해서는 질문 변경 접근 방식이 가장 적합합니다.",
      "upvotes": 2,
      "discussionId": "67c95acf88c3b4201c10ba22"
    },
    "publishedAt": "2025-03-06T03:20:40.127Z",
    "title": "Exploring Rewriting Approaches for Different Conversational Tasks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18860.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.03044",
      "authors": [
        {
          "_id": "67c94e6ad325e95d82f23433",
          "user": {
            "_id": "5e7749883d77a72421292d07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
            "isPro": false,
            "fullname": "Gabriele Sarti",
            "user": "gsarti",
            "type": "user"
          },
          "name": "Gabriele Sarti",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:24:30.601Z",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23434",
          "name": "Vilém Zouhar",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23435",
          "name": "Grzegorz Chrupała",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23436",
          "name": "Ana Guerberof-Arenas",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23437",
          "name": "Malvina Nissim",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23438",
          "name": "Arianna Bisazza",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T22:50:17.000Z",
      "title": "QE4PE: 언어 수준의 품질 평가 도구의 인간 후 편집용\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "summary": "단어 수준의 품질 평가(QE)는 기계 번역의 오류를 감지하고, 인간의 후 편집을 지도하고 촉진합니다. 단어 수준의 QE 시스템의 정확도는 광범위하게 평가되어 있습니다; 그러나 그 활용 가능성과, 인간의 후 편집의 속도, 품질, 편집 선택에 미치는 영향은 아직 조사가 부족합니다. 우리 연구팀의 QE4PE 연구는 2가지 번역 방향의 42명의 전문 후 편집자가 참여한 실용적인 설정에서, 기계 번역(MT)의 후 편집 중 단어 수준의 QE의 영향을 조사하고 있습니다. 우리는 가장 先端의 뉴럴 MT 모델의 출력에 존재하는 잠재적인 오류를 특정하기 위해, 4가지 오류 스팬의 하이라이트 모드를 비교하고 있습니다. 후 편집의 노력을 생산성으로 평가하고, 품질의 향상은 단어 수준과 문장 수준의 인간의 Annotation에 의해 평가됩니다. 우리는 필드, 언어 및 편집자의 속도가 하이라이트의 효과성을 결정하는 중요한 요인임을 발견하였으며, 인간의 하이라이트와 자동화된 QE 하이라이트 사이에 약간의 차이는 전문적인 작업 흐름 내에서 정확성과 활용 가능성 사이의 간극을 보여주는 것을 발견하였습니다.",
      "upvotes": 2,
      "discussionId": "67c94e6fd325e95d82f23524"
    },
    "publishedAt": "2025-03-06T02:30:17.431Z",
    "title": "QE4PE: Word-level Quality Estimation for Human Post-Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03044.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e7749883d77a72421292d07",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
      "fullname": "Gabriele Sarti",
      "name": "gsarti",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 213
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20317",
      "authors": [
        {
          "_id": "67c95b2f1fcfdc62ba3a620b",
          "name": "Yongjia Lei",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620c",
          "name": "Haoyu Han",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620d",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620e",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:24:22.970Z",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620f",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a6210",
          "user": {
            "_id": "637c6d95a8716d642050b50f",
            "avatarUrl": "/avatars/0955a10113807348f24db968c7bd7c7a.svg",
            "isPro": false,
            "fullname": "Mahantesh Halappanavar",
            "user": "mhalappa",
            "type": "user"
          },
          "name": "Mahantesh M Halappanavar",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-06T08:22:09.090Z",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a6211",
          "name": "Jiliang Tang",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a6212",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T17:42:52.000Z",
      "title": "그래프 구조와 맥락의 병합 검색\n지식 기반",
      "summary": "그래프 지식 기반 (TG-KB)은 문맥과 구조적 지식을 제공하여 질문을 답변하는 데 중요하게 되었습니다. 그러나 현재의 검색 방법은 이러한 두 가지 지식의 상호적인 강화를 고려하지 않고, 각각을 분리하여 검색합니다. 또한 일부 하이브리드 방법은 구조적 검색을 완전히 회避하기 위해 인접한 집합 후로 검색합니다. 이러한 공간에 채워주기 위해, 여기는 구조적 및 문맥적 검색을 계획・理由・조직화 프레임워크를 통해 혼합하여 검색하는 Mixture of Structural-and-Textual Retrieval (MoR)를 제안합니다. 계획 단계에서, MoR는 질문을 답변하기 위한 논리를 명확히 하기 위해 문맥적 계획 그래프를 생성합니다. 그 후, 이유 단계에서, MoR는 구조적 탐색과 문맥적 매칭을 조합하여 TG-KB에서 후보를 얻습니다. 그리고 조직화 단계에서, MoR는 구조적 트래픽에 기반하여 얻은 후보를 추가로 재스코어합니다. 광범위한 실험은 MoR가 구조적 및 문맥적 검색을 조화시키는 우수한 성능을 보여주며, 질문의 논리에 따라 검색 성능의 불균형성과 구조적 트래픽의 통합에 의한 후보의 재스코어의 이점이 포함됩니다. 코드는 https://github.com/Yoega/MoR에 공개되어 있습니다.",
      "upvotes": 1,
      "discussionId": "67c95b311fcfdc62ba3a62a5"
    },
    "publishedAt": "2025-03-06T03:22:14.664Z",
    "title": "Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20317.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01763",
      "authors": [
        {
          "_id": "67c92e9c746bbcdbdfa8ebd4",
          "name": "Zhengliang Shi",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd5",
          "name": "Yuhan Wang",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd6",
          "name": "Lingyong Yan",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd7",
          "name": "Pengjie Ren",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd8",
          "name": "Shuaiqiang Wang",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd9",
          "name": "Dawei Yin",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebda",
          "name": "Zhaochun Ren",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T17:37:16.000Z",
      "title": "レビュアルモデル는 특정 도구에 대한 지식이 부족합니다：대 언어 모델의 도구 평가를 벤치마크로 평가하는 데 사용됩니다。",
      "summary": "Tool Learning의 목적은 다양한 도구를 가진 대형 언어 모델(LLMs)을 활용하는 데에 더하여, 실질적인 태스크를 해결하는 에이전트 역할을 하는 것입니다. 도구를 사용하는 LLMs의 컨텍스트 길이 제한을 고려하여, 정보 검색(IR) 모델을 사용하여 대규모 도구 세트에서 유용한 도구를 선택하는 것은 중요한 초기 단계입니다. 그러나 IR 모델이 도구 검색 태스크에 대한 성능은 자세히 조사되어 있지 않습니다. 도구 사용 벤치마크에서는, 이를 단순화하기 위해, 각 태스크에 관련된 작은 도구 세트를 사전에 자동화하여 사용하지만, 실제 세계적인 시나리오로부터 멀리 떨어져 있습니다. 이 논문에서는, 7.6k 종류의 다양한 검색 태스크와 43k의 도구 코퍼스를 구성한 다양한 도구 검색 벤치마크 \"ToolRet\"를 제안합니다. ToolRet에서 6가지의 모델을 벤치마크 합니다. 놀라울 정도로, 전통적인 IR 벤치마크에서 강력한 성능을 보여주는 모델도 ToolRet에서 낮은 성능을 나타냅니다. 이 낮은 검색 품질은 도구 사용 LLMs의 태스크 통과율을 감소시킵니다. 또한, 200k 이상의 인스턴스를 포함하는 규모적인 훈련 데이터 세트를 제공하여, IR 모델의 도구 검색 능력을 크게 최적화합니다.",
      "upvotes": 1,
      "discussionId": "67c92e9e746bbcdbdfa8ec57"
    },
    "publishedAt": "2025-03-06T00:12:07.867Z",
    "title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01763.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01729",
      "authors": [
        {
          "_id": "67c92e8b5650d7efeba5b48c",
          "name": "Santiago Bou Betran",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48d",
          "name": "Alberta Longhini",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48e",
          "name": "Miguel Vasco",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48f",
          "name": "Yuchong Zhang",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b490",
          "name": "Danica Kragic",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T16:49:15.000Z",
      "title": "FLAME: 로봇 조작의 federated 학습 벤치마크\n\n(Note: The original text \"FLAME: ロボット操作のフェデレイドラーニングベンチマーク\" is a Japanese text. The translation provided is from Japanese to Korean. If you intended to translate from English to Korean, the correct translation would be \"FLAME: 로봇 조작의 Federated Learning Benchmark\".)",
      "summary": "최근의 로봇 조작의 발전은 다양한 환경에서 수집된 큰 규모의 데이터 세트에 의해 구동되고 있습니다. 이러한 데이터 세트를 통해 로봇 조작 정책의 훈련은 지금까지 중심화된 모티브로 이루어져 있었으며, scalability, adaptability, 데이터 프라이버시와 관련된 우려가 있었다. 반면, federated learning은 분산적이고, 학습을 통해 프라이버시를 보호할 수 있기 때문에 로봇 조작에서도 적용이 남아 있습니다. 우리는 로봇 조작에 적합한 federated learning의 첫 번째 벤치마크인 FLAME(Manipulation Environments에서 federated learning)를 소개합니다. FLAME는 다음과 같은 두 가지 구성 요소를 포함합니다: (i) 160,000개 이상의 다양한 조작 태스크의 전문가示范의 큰 규모의 데이터 세트; (ii) 로봇 정책 학습의 federated learning 설정에서 훈련과 평가 프레임워크. FLAME에서 표준의 federated learning 알고리즘을 평가하고, 분산된 정책 학습의 가능성과 중요한 문제를 밝혀줍니다. 이 벤치마크는 scalable하고 adaptable한 로봇 학습의 기반을 구축합니다.",
      "upvotes": 1,
      "discussionId": "67c92e8d5650d7efeba5b519"
    },
    "publishedAt": "2025-03-06T00:11:48.501Z",
    "title": "FLAME: A Federated Learning Benchmark for Robotic Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01729.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01449",
      "authors": [
        {
          "_id": "67c92e738d5fe8c860571103",
          "name": "Ting Zhang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571104",
          "name": "Chengran Yang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571105",
          "name": "Yindu Su",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571106",
          "name": "Martin Weyssow",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571107",
          "name": "Hung Nguyen",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571108",
          "name": "Tan Bui",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571109",
          "name": "Hong Jin Kang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110a",
          "name": "Yikun Li",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110b",
          "name": "Eng Lieh Ouh",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110c",
          "name": "Lwin Khin Shar",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110d",
          "name": "David Lo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T11:56:00.000Z",
      "title": "버전 체크 파일을 다운로드 하세요.",
      "summary": "최근의 생성 AI의 발전은 소프트웨어 엔지니어링 분야에서 대규모 언어 모델(LLMs)의 광범위한 도입을 촉발시키고, 오랜 기간 동안 해결되지 않은 여러 문제들을 해결했습니다. 그러나 소프트웨어 보안의 중요한 한 부분인 소프트웨어 취약점 탐출(SVD)에서 LLMs의 능력을 자세히 조사하는 구체적인 연구는 지금까지 거의 발견되지 않았습니다. 현재의 연구는 주로 C/C++ 데이터 세트를 사용하여 LLMs를 평가하고 있습니다. 일반적으로, 프로젝트 엔지니어링, 명령어 트레이닝, 순서 클래스 최종 최종 중 1개 또는 2개를 조사하고 있습니다. 이러한 연구의 결과, LLMs가 다양한 프로그래밍 언어로 취약점을 탐출하는 효과성에 대한 지식 부족이 크게 남아 있습니다. 이러한 지식 부족을 해결하기 위해, LLMs의 SVD 작업의 성능을 평가하기 위한 구체적인 실험 연구를 수행하고 있습니다. 프로젝트를 준비하고 있습니다. Python으로 8,260 함수, Java로 7,505 함수, JavaScript로 28,983 함수의 취약한 함수를 포함하는 세부적인 데이터 세트를 준비하고 있습니다. 5개의 프로젝트 소스 LLMs를 프로젝트 엔지니어링, 명령어 트레이닝, 순서 클래스 최종 등 여러 가지 접근법으로 평가합니다. 이러한 LLMs는 5개의 최종 최종 된 소규모 언어 모델과 2개의 프로젝트 소스 정적 애플리케이션 보안 테스트 도구와 비교됩니다. 또한, LLMs의 SVD 작업의 성능을 향상시키기 위해, 데이터의 시각과 모델의 시각의 두 가지 접근법을 조사합니다. 데이터의 시각에서, 데이터를 다운 샘플링한 균형 데이터 세트를 사용하여 모델을 재학습합니다. 모델의 시각에서, 여러 LLMs의 예측을 결합하는 앙상블 학습의 방법을 조사합니다. 구체적인 실험에 따라, SVD는 LLMs의 어려운 작업으로 남아 있는 것을 보여줍니다. 이 연구는 LLMs가 SVD에서 수행하는 역할을 자세히 이해하고, 생성 AI를 활용하여 소프트웨어 보안의 실용적인 실습을 향상시키는 실질적인 가이드라인을 제공합니다.",
      "upvotes": 1,
      "discussionId": "67c92e748d5fe8c860571142"
    },
    "publishedAt": "2025-03-06T00:11:25.013Z",
    "title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01449.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01378",
      "authors": [
        {
          "_id": "67c92e537ae0115c7a7b9fa3",
          "name": "Artem Lykov",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa4",
          "name": "Valerii Serpiva",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa5",
          "name": "Muhammad Haris Khan",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa6",
          "name": "Oleg Sautenkov",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa7",
          "name": "Artyom Myshlyaev",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa8",
          "name": "Grik Tadevosyan",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa9",
          "name": "Yasheerah Yaqoot",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9faa",
          "name": "Dzmitry Tsetserukou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:21:36.000Z",
      "title": "코카고든：UAV에서 실시간 인지 태스크 해결과 논리론의 VLA 모델 및 평가 벤치마크",
      "summary": "이 논문에서는 복잡한 무인항공기(UAV) 작업에 적합한 새로운 Vision-Language-Action(VLA) 모델인 \"CognitiveDrone\"을 소개합니다. 이 모델은 8,000 이상의 훈련 데이터 세트를 기반으로, 인간 인식, 기호 이해, 논리론리 3가지 카테고리에 대해, 시각 입력과 문자열 지시에 따라 시퀀스적으로 4차원 행동 명령을 생성합니다. 복잡한 시나리오에서 성능을 향상시키기 위해, 추가적인 Vision-Language Model(VLM) 논리론리 모듈을 포함하고, 고주파 제어 전에 작업 지시를 단순화하기 위해 \"CognitiveDrone-R1\"을 제안합니다. 우리의 오픈 소스 벤치마크 \"CognitiveDroneBench\"을 사용하여 수행된 실험 평가에 따르면, 레이스专用 모델(RaceVLA)의 전체 성공율은 31.3%였지만, 기본적인 CognitiveDrone 모델은 59.6%, CognitiveDrone-R1은 77.2%의 성공율을 달성했습니다. 이러한 결과를 통해, 중요한 인지 작업에서 약 30%의 향상을 보여주고, 무인항공기 제어 시스템에 진화적인 논리론리 능력을 통합하는 효과를 강조합니다. 우리의 기여는 가장 선진적인 VLA 모델의 개발과, 무인항공기 조작의 인지 작업 평가에 대한 처음의专用 벤치마크의 도입입니다. 완전한 리포지토리는 cognitivedrone.github.io에 접근할 수 있습니다.",
      "upvotes": 1,
      "discussionId": "67c92e547ae0115c7a7b9fe6"
    },
    "publishedAt": "2025-03-06T00:10:56.364Z",
    "title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.00502",
      "authors": [
        {
          "_id": "67c8427047c2aa135346dced",
          "user": {
            "_id": "66d3290364c1e9b73208af82",
            "avatarUrl": "/avatars/e0ea5f2b366927c7b146f248028a2e59.svg",
            "isPro": false,
            "fullname": "Shiyu Fang",
            "user": "FanGShiYuu",
            "type": "user"
          },
          "name": "Shiyu Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-05T15:46:48.691Z",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcee",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcef",
          "name": "Chengkai Xu",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf0",
          "name": "Chen Lv",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf1",
          "name": "Peng Hang",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf2",
          "name": "Jian Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-01T14:15:52.000Z",
      "title": "Interact, Instruct to Improve: LLM Driven Dual-Agent Framework for Enhancing Autonomous Vehicle Interactions",
      "summary": "自動運転車(AVs)는 상업화 단계에 진입하며, 그 상호작용과 의도 표현의 제한된 능력은 인간운전차(HVs)와의 상호작용에서 문제가 남아 있습니다. 대규모 언어 모델(LLMs)의 최근 발전은 양방향인 인간 기계 통신을 가능하게 했지만, 느린 추론 속도와 실시간 결정의 필요로 인해 실질적인 적용에 문제가 있습니다. 이러한 문제를 해결하기 위해, 본 논문에서는 여러 시나리오에서 명확한 양방향적인 AV-HV 상호작용을 가능하게 하는 병렬 액터 레지언어 프레임워크를 도입합니다. 먼저, LLM 구동의 레지언과 다른 HVs와의 상호작용을 촉진하여 상호작용 메모리 데이터베이스인 액터로 이름을 붙입니다. 다음으로, 메모리 파티션 모듈과 2층 메모리 검색 모듈을 도입하여 액터가 다양한 HVs를 처리할 수를 크게 향상시킵니다. 액터 레지언어 프레임워크의 제안은 안정성과 효율성을 크게 향상시키는 것을 추론 연구와 다른 결정론 방법과의 비교에서 입증했습니다. 마지막으로, 레지언의 이유에 따라 외부인간 기계 인터페이스(eHMI) 정보와 액터에서 적용 가능한 행동 결정을 조합하여 제안된 액터 레지언어의 효과를 여러 시나리오에서野外 상호작용에서 확인되었습니다. 코드는 https://github.com/FanGShiYuu/Actor-Reasoner에 공개됩니다.",
      "upvotes": 1,
      "discussionId": "67c8427247c2aa135346dd84",
      "projectPage": "https://fangshiyuu.github.io/Actor-Reasoner/",
      "githubRepo": "https://github.com/FanGShiYuu/Actor-Reasoner"
    },
    "publishedAt": "2025-03-05T21:37:18.981Z",
    "title": "Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d3290364c1e9b73208af82",
      "avatarUrl": "/avatars/e0ea5f2b366927c7b146f248028a2e59.svg",
      "fullname": "Shiyu Fang",
      "name": "FanGShiYuu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01372",
      "authors": [
        {
          "_id": "67c6bd6e8f3e7fd471affd06",
          "name": "Joel Niklaus",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd07",
          "name": "Jakob Merane",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd08",
          "name": "Luka Nenadic",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd09",
          "name": "Sina Ahmadi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0a",
          "name": "Yingqiang Gao",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0b",
          "name": "Cyrill A. H. Chevalley",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0c",
          "name": "Claude Humbel",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0d",
          "name": "Christophe Gösken",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0e",
          "name": "Lorenzo Tanzi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0f",
          "name": "Thomas Lüthi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd10",
          "name": "Stefan Palombo",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd11",
          "name": "Spencer Poff",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd12",
          "name": "Boling Yang",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd13",
          "name": "Nan Wu",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd14",
          "name": "Matthew Guillod",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd15",
          "name": "Robin Mamié",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd16",
          "name": "Daniel Brunner",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd17",
          "name": "Julio Pereyra",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd18",
          "name": "Niko Grupen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:10:30.000Z",
      "title": "SwiLTra-Bench: 썸플 법규 번역 벤치마크",
      "summary": "スイス에서 법령 번역은 국가의 4가지 공식 언어와 다언어 법문서 요구에 따라 특별히 중요합니다. 그러나 이 과정은 법률 전문가와 능숙한 번역자를 모두 필요로 하여 전문인력을 의존하고, 붉은점(瓶頸)을 발생시키고 공정적이고 효과적인 접근에 영향을 미칩니다. 이러한 도전을 해결하기 위해, 우리는 모든 스웨덴어(영어 포함)의 법령, 헤드 노트, 프레스 릴리즈를 포함하여 180K 이상의 대응 번역 쌍을 포함하는 상세한 다언어 벤치마크인 SwiLTra-Bench를 소개합니다. 이 벤치마크는 LLM 기반의 번역 시스템 평가에 설계되었습니다. 로지스틱 평가에 따라 선진 모델은 모든 문서 유형에서 상위 번역 성능을 보여주고, 전문 번역 시스템은 특히 법에서 뛰어나지만 헤드 노트에서는 떨어집니다. 엄격한 테스트와 인간 전문가의 평가에 의해, 우리는 오픈 시스템의 미세 조정이 번역 품질을 크게 향상시키는 것을 보여주고, 이는 0 shotPrompting된 최신 모델(예: Claude-3.5-Sonnet)과 비교하여 떨어졌습니다. 또한 SwiLTra-Judge라는 전문 LLM 평가 시스템도 소개합니다. 이 시스템은 인간 전문가의 평가에 가장 일치합니다.",
      "upvotes": 0,
      "discussionId": "67c6bd708f3e7fd471affd5d"
    },
    "publishedAt": "2025-03-06T00:10:21.173Z",
    "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01372.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  }
]