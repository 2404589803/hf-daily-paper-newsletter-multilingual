[
  {
    "paper": {
      "id": "2502.01237",
      "authors": [
        {
          "_id": "67a1c1428747511e7b9a1965",
          "user": {
            "_id": "62897fce5d9e25c10e4f319d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg",
            "isPro": false,
            "fullname": "Alexey Gorbatovski",
            "user": "Myashka",
            "type": "user"
          },
          "name": "Alexey Gorbatovski",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:00.767Z",
          "hidden": false
        },
        {
          "_id": "67a1c1428747511e7b9a1966",
          "name": "Boris Shaposhnikov",
          "hidden": false
        },
        {
          "_id": "67a1c1428747511e7b9a1967",
          "user": {
            "_id": "6416272d986557e8cac64ece",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6416272d986557e8cac64ece/s3CLjNN_pGj-vJDcENFD2.jpeg",
            "isPro": false,
            "fullname": "Viacheslav",
            "user": "ummagumm-a",
            "type": "user"
          },
          "name": "Viacheslav Sinii",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:38:52.039Z",
          "hidden": false
        },
        {
          "_id": "67a1c1428747511e7b9a1968",
          "user": {
            "_id": "636e71b2b0ebc04888157b71",
            "avatarUrl": "/avatars/957ba705d470e3a01792741d7f0ff038.svg",
            "isPro": false,
            "fullname": "Alexey Malakhov",
            "user": "ZeL1k7",
            "type": "user"
          },
          "name": "Alexey Malakhov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:38:54.121Z",
          "hidden": false
        },
        {
          "_id": "67a1c1428747511e7b9a1969",
          "user": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "isPro": false,
            "fullname": "Daniil Gavrilov",
            "user": "kefirski",
            "type": "user"
          },
          "name": "Daniil Gavrilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:38:57.087Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T10:54:14.000Z",
      "title": "직렬한 어레이먼트 알고리즘의 차이는 브루프에 비슷합니다.",
      "summary": "Direct Alignment Algorithms (DAAs)은 Reinforcement Learning from Human Feedback (RLHF)의 강화학습(RL)과 보상 모델링(RM)을 직접 정책 최적화로 대체하여 언어 모델의 어레이멘트를 간단히 한다. DAAs는 랭킹 손실(pairwise vs. pointwise)、이 손실에 사용되는 보상(예: 정책과 참조 정책의 확률비율 또는 확률비율)、또는 지도 학습 미세 조정(SFT)의 스텝이 필요 여부(two-stage vs. one-stage)에 따라 분류된다. 먼저, one-stage의 방법이 two-stage의 방법이나보다 낮은 성능을 나타내는 것을 보여주고, 명시적인 SFT 스텝을 포함하고 beta 파라미터의 도입(정책의 선호 최적화의 강도를 제어하는)을 one-stage의 ORPO와 ASFT에 적용하여, 이 개선은 Alpaca Eval 2에서 +3.46(ORPO)와 +8.27(ASFT)의 성능 개선으로 two-stage의 방법과 DPO와 같은 높은 성능을 보여주는 것을 보여준다. 더 나아가, 분석은 pointwise 또는 pairwise의 목표를 사용하는 여부가 특정한 은닉 보상이나 손실 함수에 따라는 특정한 값이 아닌 것을 보여주고, 이러한 결과를 통해 과도한 성능 향상이나 전체적인 어레이멘트 알고리즘의 우수성을 주장하는 것을 피하기 위해 정밀한 평가의 중요성을 강조한다.",
      "upvotes": 33,
      "discussionId": "67a1c1438747511e7b9a19ae"
    },
    "publishedAt": "2025-02-04T03:10:49.348Z",
    "title": "The Differences Between Direct Alignment Algorithms are a Blur",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62897fce5d9e25c10e4f319d/ndKErkZSfT5LvqKfIrC7f.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01237.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "62897fce5d9e25c10e4f319d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg",
      "fullname": "Alexey Gorbatovski",
      "name": "Myashka",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01456",
      "authors": [
        {
          "_id": "67a19d705efa4fab15497775",
          "user": {
            "_id": "650eba9555dc1e841746f132",
            "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
            "isPro": false,
            "fullname": "Ganqu Cui",
            "user": "ganqu",
            "type": "user"
          },
          "name": "Ganqu Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:23.889Z",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497776",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497777",
          "name": "Zefan Wang",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497778",
          "user": {
            "_id": "6321152b8c0da827c72c7c16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678783813705-6321152b8c0da827c72c7c16.jpeg",
            "isPro": false,
            "fullname": "Hanbin Wang",
            "user": "hanbin",
            "type": "user"
          },
          "name": "Hanbin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:25.869Z",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497779",
          "name": "Wendi Li",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777a",
          "name": "Bingxiang He",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777b",
          "name": "Yuchen Fan",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777c",
          "name": "Tianyu Yu",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777d",
          "name": "Qixin Xu",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777e",
          "name": "Weize Chen",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777f",
          "name": "Jiarui Yuan",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497780",
          "name": "Huayu Chen",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497781",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497782",
          "name": "Xingtai Lv",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497783",
          "name": "Shuo Wang",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497784",
          "name": "Yuan Yao",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497785",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497786",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497787",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497788",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497789",
          "name": "Maosong Sun",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549778a",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549778b",
          "name": "Ning Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T15:43:48.000Z",
      "title": "숨겨진 보상에 의한 강화 처리",
      "summary": "밀집 보상은 대규모 언어 모델(LLMs)의 추론 시 스케일링에서 결과 수준의 희소 보상에 비해 복잡한 다단계 로직이 필요한 태스크에서 효과적인 대체로 증명되어 있습니다. 그러나 LLMs의 강화 학습(RL)에서도 이러한 미세한 보상이 결과 보상의 본질적인 문제를 해결할 수 있는 것을 보여주는 것으로서 매력적이며, 그러나 이러한 가능성은 아직 크게 실현되지 않았습니다. 이는 현재의 방법론에서 프로세스 보상 모델(PRMs)의 온라인 훈련에서 고품질의 프로세스 라벨의 수집이 비싸고 보상 힙킹에 취약한 데 의한 것입니다. 이러한 문제를 해결하기 위해 PRIME(IMplicit 보상을 통한 프로세스 강화 학습)을 제안합니다. PRIME은 정책 로일 아웃과 결과 라벨을 사용하여 온라인 PRM 업데이트를 가능하게 합니다. PRIME은 현재의 방법론에서 필요로 하는专用 보상 모델의 훈련 단계를 생략하고, 장치 오버헤드를 크게 줄입니다. PRIME의 효과는 상대적인 수학과 코딩에서 나타납니다. Qwen2.5-Math-7B-Base부터 시작하여 PRIME은 SFT 모델을 초월할 수 있습니다. 특히, 우리에게 얻은 모델인 Eurus-2-7B-PRIME은 Qwen2.5-Math-7B-Instruct을 초월하고, 훈련 데이터의 10%를 사용합니다.",
      "upvotes": 26,
      "discussionId": "67a19d705efa4fab154977d0"
    },
    "publishedAt": "2025-02-04T00:02:39.922Z",
    "title": "Process Reinforcement through Implicit Rewards",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01456.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6321152b8c0da827c72c7c16",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678783813705-6321152b8c0da827c72c7c16.jpeg",
      "fullname": "Hanbin Wang",
      "name": "hanbin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01534",
      "authors": [
        {
          "_id": "67a1ad77d797fac51fa80770",
          "name": "Dawei Li",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80771",
          "user": {
            "_id": "653a195b0da86d726c9c580c",
            "avatarUrl": "/avatars/61649e1d600fdc1edc50ead0dfa99fdd.svg",
            "isPro": false,
            "fullname": "Renliang Sun",
            "user": "RLSNLP",
            "type": "user"
          },
          "name": "Renliang Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:11.035Z",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80772",
          "name": "Yue Huang",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80773",
          "name": "Ming Zhong",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80774",
          "name": "Bohan Jiang",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80775",
          "name": "Jiawei Han",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80776",
          "name": "Xiangliang Zhang",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80777",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80778",
          "name": "Huan Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T17:13:03.000Z",
      "title": "편향 누설: LLM-as-a-judge에서 오염 문제",
      "summary": "대 언어 모델(LLMs)를 판단자와서의 역할과 LLM 기반의 데이터 합성은 모델 개발의 두 가지 기본적인 LLM 주도 데이터 설명 방법 중의 하나가 되었습니다. 이러한 조합은 모델의 훈련과 평가의 효율 향상에 큰 기여를 하였지만, 이 새로운 모델 개발 패러다임에 따른 잠재적인 오염에 대해 거의 주목을 기울여 있지 않았습니다. 본 논문에서는 LLM을 판단자와서의 편향과 합성 데이터 생성자와 LLM 기반의 평가자의 관련성으로 인한 오염 문제를 밝혀냅니다. 이러한 문제를 연구하기 위해 먼저 데이터 생성용 LLM과 판단용 LLM 사이에서 3가지의 공통된 관련성을 정의합니다: 같은 모델이거나, 계승 관계를 가지거나, 같은 모델족에 속하는 것입니다. 확장된 실험을 통해, 설명 편향이 취향 편향으로 판단자가 관련된 학생 모델에 대한 편향을 인식하고 있다고 실험적으로 확인합니다. 진행된 분석은 취향 편향은 이전에 인식된 LLM을 판단자와서의 경우의 편향과 비교하여 감지하기 어려운 광범위한 문제임을 보여줍니다. 이러한 모든 발견은 취향 편향은 LLM을 판단자와서의 영역에서 광범위하게 해결하기 어려운 문제임을 의미합니다. 모든 코드와 데이터는 아래 URL에서 공개됩니다: https://github.com/David-Li0406/Preference-Leakage.",
      "upvotes": 11,
      "discussionId": "67a1ad78d797fac51fa807c1"
    },
    "publishedAt": "2025-02-04T01:04:33.630Z",
    "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01534.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6474e1afb68461d5cf7c41cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
      "fullname": "Dawei Li",
      "name": "wjldw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01061",
      "authors": [
        {
          "_id": "67a1a7a166a8a88726963ef4",
          "name": "Gaojie Lin",
          "hidden": false
        },
        {
          "_id": "67a1a7a166a8a88726963ef5",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "67a1a7a166a8a88726963ef6",
          "name": "Jiaqi Yang",
          "hidden": false
        },
        {
          "_id": "67a1a7a166a8a88726963ef7",
          "name": "Zerong Zheng",
          "hidden": false
        },
        {
          "_id": "67a1a7a166a8a88726963ef8",
          "name": "Chao Liang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T05:17:32.000Z",
      "title": "OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman",
      "summary": "端到端의 인간 애니메이션, 예를 들어 음주 Drove Table 인간 생성에 대해, 최근 몇 년 동안 눈에 띄는 발전이 보입니다. 그러나 현재의 방법들은, 대규모의 일반적인 비디오 생성 모델로 확장할 수 없기 때문에, 현실적인 응용 분야에서의 가능성은 제한되어 있습니다. 본 논문에서는, 데이터의 확장을 효과적으로 수행하기 위해, 동작과 관련된 조건을 학습 단계에 섞어 Scalable 프레임워크를 제안합니다. 이를 통해, 동작과 관련된 조건에 대한 두 가지 학습 원칙을 도입하고, 그에 따른 모델 구조와 추론 전략을 제안합니다. 이러한 설계는 OmniHuman이 데이터 Drove 동작 생성을 최대한 활용하여, 고품질의 인간 비디오 생성을 실현할 수 있게 됩니다. 더욱 중요한 것은, OmniHuman은 얼굴의 근접, 사진, 반체, 전체 등 다양한 사진 콘텐츠들을 지원하며, 대화음과 노래를 지원하고, 인간과 객체의 상호작용과 어려운 신체 상태들을 처리하며, 다양한 이미지 스타일을 처리할 수 있습니다. 현재의 엔드 투 엔드 음주 Drove 방법과 비교하여, OmniHuman은 현실적인 비디오를 생성하며, 입력의 유연성이 높아집니다. 또한, 여러 Drove 모드(음주 Drove, 비디오 Drove, 결합 Drove 신호)를 지원합니다. 비디오 샘플은 ttfamily 프로젝트 페이지(https://omnihuman-lab.github.io)에서 제공됩니다.",
      "upvotes": 11,
      "discussionId": "67a1a7a466a8a88726963f90"
    },
    "publishedAt": "2025-02-04T00:37:57.949Z",
    "title": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01061.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18636",
      "authors": [
        {
          "_id": "67a1bfc314cba2eba6da4b2b",
          "name": "Xun Liang",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b2c",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b2d",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b2e",
          "name": "Sensen Zhang",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b2f",
          "user": {
            "_id": "669e0b93c7cb0568dac6e92e",
            "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
            "isPro": false,
            "fullname": "hanyu Wang",
            "user": "UglyToilet",
            "type": "user"
          },
          "name": "Hanyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:04.452Z",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b30",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b31",
          "name": "Jason Zhaoxin Fan",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b32",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b33",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b34",
          "name": "Mengwei Wang",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b35",
          "name": "Jiawei Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T17:01:31.000Z",
      "title": "SafeRAG: 안전 레비ュー 아우ガーデーション에서 보안 벤치마크\n\n(Note: The original text contains a mix of English and Japanese characters. The translation above maintains the original structure while translating the English part to Korean and keeping the Japanese characters as they are, assuming they are part of the original text's intended meaning.)",
      "summary": "レコード検索・生成パラダイム의 레コード付き生成（RAG）は、大規模な言語モデル（LLMs）に外部キャンバスを統合して知識密集型タスクを解決するために非常に成功しています。しかし、外部キャンバスと未確認キャンバスの統合によりLLMsの脆弱性が増加し、攻撃者は知識を操作して攻撃タスクを行うことができます。本論文では、RAGセキュリティを評価するためのベンチマーク「SafeRAG」を介して、RAGセキュリティを評価します。まず、攻撃タスクを銀色ノイズ、コンテキスト間の衝突、ソフトアド、ホワイトデニアル・オブ・サービスと分類します。次に、各タスクに対して手動で主にRAGセキュリティ評価データセット（即、SafeRAGデータセット）を構築します。そして、SafeRAGデータセットを用いてRAGが遭遇するさまざまな攻撃シナリオをシミュレートします。14つの代表的なRAGコンポーネントに対して実験を行い、RAGはすべての攻撃タスクに脆弱であり、最も明らかな攻撃タスクは既存のレコード検索モジュール、フィルターまたは進歩的なLLMsを経由して簡単に通過することができ、RAGサービスの品質が低下することを示します。コードは以下のURLから利用可能です：https://github.com/IAAR-Shanghai/SafeRAG。",
      "upvotes": 8,
      "discussionId": "67a1bfc414cba2eba6da4b63"
    },
    "publishedAt": "2025-02-04T03:22:06.520Z",
    "title": "SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18636.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62a155e615eeab266b2f2243",
      "avatarUrl": "/avatars/e89ef156e73af028e3ce3664e6cb4e62.svg",
      "fullname": "Zhiyu Li",
      "name": "jimi888",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01068",
      "authors": [
        {
          "_id": "67a1a75f6aa8429da4945eeb",
          "user": {
            "_id": "639ffbc6beb95d698de9640d",
            "avatarUrl": "/avatars/7ef1aaadd5b378d00e17dc548e42cb7e.svg",
            "isPro": false,
            "fullname": "Dongwon Jo",
            "user": "dongwonjo",
            "type": "user"
          },
          "name": "Dongwon Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:16.125Z",
          "hidden": false
        },
        {
          "_id": "67a1a75f6aa8429da4945eec",
          "user": {
            "_id": "662672eaebdfec5cfdf1d034",
            "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
            "isPro": false,
            "fullname": "Jiwon Song",
            "user": "jiwonsong",
            "type": "user"
          },
          "name": "Jiwon Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:14.253Z",
          "hidden": false
        },
        {
          "_id": "67a1a75f6aa8429da4945eed",
          "name": "Yulhwa Kim",
          "hidden": false
        },
        {
          "_id": "67a1a75f6aa8429da4945eee",
          "name": "Jae-Joon Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T05:25:09.000Z",
      "title": "FastKV: KV 캐시 압축을 통한 빠른 긴 컨텍스트 처리와 토큰 선택적 전파\n\n(FastKV: KV 캐시 압축을 통한 빠른 긴 컨텍스트 처리와 토큰 선택적 전파)",
      "summary": "대 언어 모뎀（LLMs）는 긴 문맥 시퀀스의 처리에 뛰어난 성능을 가지고 있지만, 이들은 문맥 정보를 저장하기 위해 큰 키 값(KV) 캐시를 필요로 하고, 이는 계산 효율성과 메모리 사용량에 큰 부담을 가집니다. 기존의 캐시 압축 시도들은 메모리 요구량을 줄이기만 목표였지만, 지연 시간을 개선하는 데 제한되어 있었습니다. 이러한 문제를 해결하기 위해, FastKV라는 긴 문맥 시퀀스의 지연 시간을 개선하는 캐시 압축 방법을 소개합니다. 처리 속도를 향상시키면서 정확도를 유지하기 위해, FastKV는 LLMs의 초기 계층에서 모든 문맥 정보를 유지하고, 더 깊은 계층에서 일부를 선택적으로 전파하는 새로운 토큰 선택적 전파(TSP) 접근 방식을 채택합니다. 또한, FastKV는 그룹화 쿼리 처리(GQA)에 관련된 KV 캐시 압축을 채택하고, GQA의 메모리와 계산 효율성의 장점을 활용합니다. 실험 결과를 통해, FastKV는 가장 선진한 KV 캐시 압축 방법인 HeadKV와 비교하여, 시간 至 일 모드(TTFT)과 트랜소르프(Transtorp)에서 각각 2.00배와 1.40배의 향상을 달성했습니다. 또한, FastKV는 긴 문맥 벤치마크에서 정확도를 유지하면서 기준과 비교하여 성능을 실현했습니다. 코드는 https://github.com/dongwonjo/FastKV에 공개되어 있습니다.",
      "upvotes": 7,
      "discussionId": "67a1a7616aa8429da4945f95"
    },
    "publishedAt": "2025-02-04T00:45:45.545Z",
    "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01068.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639ffbc6beb95d698de9640d",
      "avatarUrl": "/avatars/7ef1aaadd5b378d00e17dc548e42cb7e.svg",
      "fullname": "Dongwon Jo",
      "name": "dongwonjo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.00094",
      "authors": [
        {
          "_id": "67a185ab908f4534beb94b8c",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:32.712Z",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b8d",
          "name": "Sara Ghaboura",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b8e",
          "name": "Omkar Thawkar",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b8f",
          "name": "Fahad Shahbaz Khan",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b90",
          "name": "Hisham Cholakkal",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b91",
          "name": "Rao Muhammad Anwer",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b92",
          "name": "Salman Khan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-31T18:58:20.000Z",
      "title": "AIN: 아랍 인클루션 대규모 다양화 모델",
      "summary": "LLMs의 급속한 발전과 그 반대편의 대형 멀티모달 모델(LMMs)의 진화 사이에서, 영어나 중국어와 같은 자원 풍부한 언어에서는 눈에 띄게 발전이 이루어지고 있습니다. 반면에, 阿拉伯語의 LLMs는 눈에 띄게 발전하지만, LMMs는 아직 많은 면에서 조사가 되지 않고, 특히 특정 언어의 면이나 시각적 이해에 초점을 맞추고 있습니다. 이 공백을 메기 위해, 阿拉伯語의 단일모달 모델은 아직 조사가 되지 않았습니다. AIN- 阿拉伯語의 단일모달 모델은 아직 조사가 되지 않았습니다. AIN는 영어와 阿拉伯語를 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문적으로 전문",
      "upvotes": 7,
      "discussionId": "67a185b0908f4534beb94c49"
    },
    "publishedAt": "2025-02-03T22:22:44.375Z",
    "title": "AIN: The Arabic INclusive Large Multimodal Model",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mmf9V_8rdsi9hN-QdFZV8.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/uLq0E1qq75-P4P1KV4xWF.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/1eixiKjHGNVm6RaJpdWeq.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/XVJSPAgIQcQn8Zi4gUVwi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00094.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01081",
      "authors": [
        {
          "_id": "67a1a56d83c3565727d22f0c",
          "name": "Vernon Y. H. Toh",
          "hidden": false
        },
        {
          "_id": "67a1a56d83c3565727d22f0d",
          "name": "Yew Ken Chia",
          "hidden": false
        },
        {
          "_id": "67a1a56d83c3565727d22f0e",
          "name": "Deepanway Ghosal",
          "hidden": false
        },
        {
          "_id": "67a1a56d83c3565727d22f0f",
          "name": "Soujanya Poria",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T05:47:04.000Z",
      "title": "ジャンプイング・レジニング・クローブ? 다모돗 모델의 미지의 해법으로 발전하는 추적\nGPT-[n]과 o-[n] 모델에서 인론 성능의 발전을 조사\n\n이 번역은 원문의 전문성과 정확성을 유지하며, 한국어로 변환되었습니다.",
      "summary": "OpenAI의 o1과 o3의 릴리즈는 대규모 언어 모델에서 발전적인 논리 능력의 향상에 대한 중요한 패러다임 전환을 기록하고 있습니다. 특히, o3는 인공 일반 지능(AGI)의 추상화와 논리 corpus에서 새로운 문제를 해결하고 기술 습득에 있어서 인간을 초월한 성능을 보여주었습니다. 그러나 이 벤치마크는 기호적인 패턴에만 제한되어 있으며, 인간은 이러한 패턴을 포함하는 시각과 언어 데이터로 구성된 다양한 시나리오에서 인식과 논리를 수행하는 경우가 많기 때문에, 다양한 논리 능력의 발전을 조사하는 필요성이 급격히 제기되었습니다. 따라서, GPT-[n]과 o-[n] 시리즈 모델의 진화 과정을 추적하고, 복잡한 다양한 패턴의 퍼즐에서 시각 인식과 추상적 또는 알고리즘적인 논리를 필요로 하는 것을 통해 논리 능력의 향상을 조사하고 있습니다. o1의 상위 성능은 GPT-4o의 계산 비용에 가까운 750배이며, 이에 대한 효율성에 대한 의문이 있습니다. 우리의 결과에 따르면 모델의 반복 과정에서 명확한 논리 능력의 향상의 경향이 나타났으며, GPT 시리즈 모델에서 o1으로 뚜렷한 성능의 점프가 보입니다. 그러나 o1 모델은 단순한 다양한 패턴의 퍼즐에서 추상적 논리에 필요한 경우 고충을 보입니다. 또한 알고리즘적인 퍼즐에서 성능이 아직 좋지 않은 것을 알 수 있습니다. 우리는 이 시리즈의 새로운 모델을 계속해서 추적하고 이 논문에 따라 결과를 업데이트하는 것을 계획하고 있습니다. 이 평가에 사용된 모든 리소스는 공개적으로 사용할 수 있습니다(https://github.com/declare-lab/LLM-PuzzleTest).",
      "upvotes": 5,
      "discussionId": "67a1a57083c3565727d22fc6"
    },
    "publishedAt": "2025-02-04T00:28:35.436Z",
    "title": "The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01081.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01142",
      "authors": [
        {
          "_id": "67a1b4630e9634919de9bc52",
          "user": {
            "_id": "643407dd4b34368fdb0149e8",
            "avatarUrl": "/avatars/9477b9267d5692a4fe59e30590e9639d.svg",
            "isPro": false,
            "fullname": "Xinyan Guan",
            "user": "xinyan233333",
            "type": "user"
          },
          "name": "Xinyan Guan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:08.849Z",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc53",
          "name": "Jiali Zeng",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc54",
          "name": "Fandong Meng",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc55",
          "name": "Chunlei Xin",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc56",
          "name": "Yaojie Lu",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc57",
          "name": "Hongyu Lin",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc58",
          "name": "Xianpei Han",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc59",
          "name": "Le Sun",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc5a",
          "name": "Jie Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T08:22:45.000Z",
      "title": "DeepRAG: 단계별로 검색 단계를 고려하기 위한 대규모 언어 모델",
      "summary": "대 언어 모델（LLMs）는 시간성, 정확성, 커버의 효율성에 따라 사실적인 대화에 어려움을 겪으며, 논리적 논술의 가능성에 대한 가능성을 보여주고 있습니다. 반면, 논리적 논술과 검색 어우게生成（RAG）의 통합은 유효한 태스크 분해와冗長한 검색으로 노이즈의 유입과 답변의 질의 저하를 원인으로 하여 어려움을 겪습니다. 본 논문에서는 검색 어우게生成 논리를马尔코프 결정 프로세스（MDP）로 모델링하고, 전략적이고 적응적인 검색을 가능하게 하는 DeepRAG 프레임워크를 제안합니다. DeepRAG는 질문을 연속적으로 분해하고, 각 단계에서 외부 지식 검색을 수행할지 또는 파라메트릭 논리에 의존할지 동적으로 결정합니다. 실험 결과를 통해 DeepRAG는 검색 효율을 향상시키며, 답변의 정확성을 21.99% 높일 수 있으며, 검색 어우게生成 논리의 최적화에 효과적이라는 것을 보여주고 있습니다.",
      "upvotes": 3,
      "discussionId": "67a1b4640e9634919de9bc8b"
    },
    "publishedAt": "2025-02-04T04:35:57.149Z",
    "title": "DeepRAG: Thinking to Retrieval Step by Step for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643407dd4b34368fdb0149e8",
      "avatarUrl": "/avatars/9477b9267d5692a4fe59e30590e9639d.svg",
      "fullname": "Xinyan Guan",
      "name": "xinyan233333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01100",
      "authors": [
        {
          "_id": "67a1a649f4aecd0dfc96ebf4",
          "user": {
            "_id": "607f666a4ad99100d63ce35c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/607f666a4ad99100d63ce35c/QxhxnvfeV6efkxwUFHwjI.png",
            "isPro": false,
            "fullname": "Bill Yuchen Lin",
            "user": "yuchenlin",
            "type": "user"
          },
          "name": "Bill Yuchen Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:17.972Z",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf5",
          "user": {
            "_id": "635049104e753c9940fefd71",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635049104e753c9940fefd71/HgR43XIFw3dneY5ufrAE8.jpeg",
            "isPro": false,
            "fullname": "Ronan Le Bras",
            "user": "ronanlb",
            "type": "user"
          },
          "name": "Ronan Le Bras",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-04T05:31:56.722Z",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf6",
          "name": "Kyle Richardson",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf7",
          "name": "Ashish Sabharwal",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf8",
          "name": "Radha Poovendran",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf9",
          "name": "Peter Clark",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebfa",
          "name": "Yejin Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T06:44:49.000Z",
      "title": "제브라로지엿：LLM의 수리논리 추론의 스케일링 한계",
      "summary": "ZebraLogic라는 상세한 평가 프레임워크를 도입하여, ロジックグリッドパズル(Logic Grid Puzzle)에서 얻은 制約満足問題(Constraint Satisfaction Problem, CSP)의 LLM의 논리성능을 평가합니다. ZebraLogic은 구조조절 가능한, quantitative하게 복잡한 패즐을 생성할 수 있으며, Llama, o1 모델, DeepSeek-R1 등 다양한 모델의 스케일링 한계에 대한 체계적인 연구를 촉진합니다. 검색공간의 복잡성과 다양한 로직적 제약을 고려하여 평가 환경을 구축하고, 난이도가 증가함에 따라 논리성을 평가하기 위한 구조화된 환경을 제공합니다.\n\n우리의 결과를 통해, 문제의 복잡도가 증가하면 정확도가 크게 떨어지는 현상이 명확히 나타납니다. 이 현상을 \"complexity curse\"라고 부르며, 현재 LLM의 논리성능에 존재하는 고유한 한계를 보여주고 있습니다. 또한, 로직신텍스 향상에 대한 전략을 검토하고, Best-of-N 샘플링, 백트래킹 구조, 자동 증명 프롬프트 등 방법을 제시합니다. 우리의 발견은 LLM의 논리성능의 스케일링에 대한 중요한 통찰을 제공하며, 기본적인 한계를 밝혀, 개선의 가능성을 제시합니다.",
      "upvotes": 3,
      "discussionId": "67a1a64cf4aecd0dfc96ecb8"
    },
    "publishedAt": "2025-02-04T00:32:03.929Z",
    "title": "ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01100.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01441",
      "authors": [
        {
          "_id": "67a189e8fbbab3ce03462fb3",
          "user": {
            "_id": "63e083e6f351dc0745745d17",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e083e6f351dc0745745d17/N0GE4uLrkm14blAQMnm2E.jpeg",
            "isPro": false,
            "fullname": "Quan Dao",
            "user": "quandao10",
            "type": "user"
          },
          "name": "Quan Dao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:30.529Z",
          "hidden": false
        },
        {
          "_id": "67a189e8fbbab3ce03462fb4",
          "name": "Khanh Doan",
          "hidden": false
        },
        {
          "_id": "67a189e8fbbab3ce03462fb5",
          "name": "Di Liu",
          "hidden": false
        },
        {
          "_id": "67a189e8fbbab3ce03462fb6",
          "user": {
            "_id": "66db7db231e772c5ec4c5576",
            "avatarUrl": "/avatars/aa0eb054bd6c881054431a22daf1aea1.svg",
            "isPro": false,
            "fullname": "Trung Le",
            "user": "trungleuc",
            "type": "user"
          },
          "name": "Trung Le",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-04T03:30:50.175Z",
          "hidden": false
        },
        {
          "_id": "67a189e8fbbab3ce03462fb7",
          "name": "Dimitris Metaxas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T15:25:58.000Z",
      "title": "개량된 잠재적 일관성 모델의 훈련 방법",
      "summary": "일관성 모델은 새로운 생성 모델의 가족 중, 단일 스텝 또는 복수 스텝으로 고품질의 샘플을 생성할 수 있는 새로운 모델입니다. 최근, 일관성 모델은 놀라운 성능을 보여주며, 픽셀 공간에서의 결과를 구현하며, 분산 모델과 동일한 결과를 달성했습니다. 그러나 일관성 훈련을 큰 데이터 세트에 확장하는 성공은 특히 텍스트에서 이미지 및 영상 생성 태스크에 있어서, 잠재 공간에서의 성능에 따라 결정됩니다. 본 논문에서는 픽셀 공간과 잠재 공간의 통계적 차이를 분석하고, 잠재 데이터가 높은 인풋스탯의 아웃라이어를 포함하고 있으며, 잠재 공간에서의 일관성 모델의 성능을 크게 저하시키는 것을 발견했습니다. 이를 해결하기 위해, Pseudo-Huber 손실을 Cauchy 손실로 대체하여 아웃라이어의 영향을 효과적으로 줄였습니다. 또한 초기 단계에서 분산 손실을 도입하고, 최적송달(OT) 복사를 사용하여 성능을 향상시켰습니다. 마지막으로, 적응 스케줄러를 도입하고, 비 스케일링 LayerNorm을 구조에 도입하여 특성량의 통계를 더 잘 파악하고, 아웃라이어의 영향을 줄였습니다. 이러한 전략을 통해, 단일 또는 복수 스텝으로 고품질의 샘플링을 가능하게 하고, 잠재 일관성 모델과 분산 모델의 성능 차이를 크게 좁펠습니다. 구현은 다음과 같은 링크에서 공개되었습니다: https://github.com/quandao10/sLCT/",
      "upvotes": 2,
      "discussionId": "67a189eafbbab3ce0346300b"
    },
    "publishedAt": "2025-02-03T22:32:23.956Z",
    "title": "Improved Training Technique for Latent Consistency Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01441.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e083e6f351dc0745745d17",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e083e6f351dc0745745d17/N0GE4uLrkm14blAQMnm2E.jpeg",
      "fullname": "Quan Dao",
      "name": "quandao10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01636",
      "authors": [
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb1",
          "user": {
            "_id": "64e8f4a24f3f7b0b84834315",
            "avatarUrl": "/avatars/242bb68c7ccffe5061c2d1c229ea3b0b.svg",
            "isPro": false,
            "fullname": "Akshat Gupta",
            "user": "akshat57",
            "type": "user"
          },
          "name": "Akshat Gupta",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-04T05:53:11.213Z",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb2",
          "name": "Phudish Prateepamornkul",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb3",
          "name": "Maochuan Lu",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb4",
          "name": "Ahmed Alaa",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb5",
          "name": "Thomas Hartvigsen",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb6",
          "name": "Gopala Anumanchipalli",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:59:14.000Z",
      "title": "Lifelong Sequential Knowledge Editing without Model Degradation\n\n한국어로 번역하면:\n\n수명주기 순차적 지식 편집에 모델 저하 없이\n\n(수명주기 순차적 지식 편집에 모델 성능 저하 없이)",
      "summary": "이전 연구에서 파라미터 변경된 지식 편집에 대해, 대규모 순서적인 편집은 모델의 손상에 의한 중대한 영향을 보여주는 것이 명확히 밝혀졌습니다. 본 논문에서는 이러한 현상의 원인을 조사하고, 원 모델의 하류 성능을 유지하면서 10,000회의 순서적인 지식 편집을 실현하는 시도를 합니다. 먼저, \"위치 지정하여 편집\"의 지식 편집 방법론이 편집된 사실에 과적합을 일으키는 것을 보여줍니다. 또한, 이러한 방법론을 연속적으로 사용하면 편집된 행렬의 노ル름의 불균형한 성장을 일으키는 것을 명확히 합니다. 다음으로, \"위치 지정하여 편집\" 방법론의 내부 기능에 대한 중요한 복잡한 관점을 제공합니다. 노ル름의 성장은 이러한 방법론이 제공하는 \"중요성 퀄리티\"로서, 편집된 레이어에서 생성된 출력 액티브에 의해 큰 중요성을 부여하는 은닉 기술로 나타납니다. 이 \"중요성 퀄리티\"에 의해, 편집된 레이어는 모델의 출력에 큰 기여를 제공합니다. 이러한 문제를 완화하기 위해, ENCORE(초기 중단과 노ル름 제한을 받는 강인한 지식 편집)을 제안합니다. ENCORE는 과적합과 노ル름의 불균형한 성장을 억제하고, 장기적인 순서적인 편집을 가능하게 하며, 하류 성능의 손실을 피하여 10,000회의 순서적인 편집을 실현할 수 있습니다. 또한, Llama3-8B에서 MEMIT보다 61% 빠르고, AlphaEdit보다 64% 빠르며 효율적으로 작동합니다.",
      "upvotes": 1,
      "discussionId": "67a1aa5fc7fa0ccf0a32cf90"
    },
    "publishedAt": "2025-02-04T00:50:46.370Z",
    "title": "Lifelong Sequential Knowledge Editing without Model Degradation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01636.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e8f4a24f3f7b0b84834315",
      "avatarUrl": "/avatars/242bb68c7ccffe5061c2d1c229ea3b0b.svg",
      "fullname": "Akshat Gupta",
      "name": "akshat57",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01637",
      "authors": [
        {
          "_id": "67a1a51e6aa8429da493d0b5",
          "name": "Da Yu",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0b6",
          "name": "Edith Cohen",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0b7",
          "name": "Badih Ghazi",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0b8",
          "name": "Yangsibo Huang",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0b9",
          "name": "Pritish Kamath",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0ba",
          "name": "Ravi Kumar",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0bb",
          "name": "Daogao Liu",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0bc",
          "name": "Chiyuan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:59:32.000Z",
      "title": "스케일링 내장 레이어즈인 레이지 모델",
      "summary": "スコーン(スケーラブル, 컨텍스트付き, 오프라이드, N-gram 埋め込み)를 제안합니다.スコーン는 레이어 크기가 확장되는 때 언어 모델의 성능을 향상시키는 방법입니다. 일반성 있는 N-gram의埋め込み를 추가하면서,원래의 단어 벡터를 유지합니다. これらの埋め込みは, 各入力トークンに対してコンテキスト付きの表現を提供し, 別のモデルで訓練されます。推論時には, 最小限の推論スピードの影響を与えないように, オフライドメモリに事前計算され, 格納されます。スコーンは, 2つの新しいスケーリング戦略を可能にします: キャッシュされたN-gram埋め込みの数を増やすことと, それらを学習するモデルのスケーリング、すべて推論時間のFLOPSを固定して行います。両方のスケーリングを行うことで,スコーンは, 1.9Bパラメータのベースラインを超えることを示し, その他多様なコーパスでも, 推論時間のFLOPSを半分に抑えながら, より優れた性能を示します。",
      "upvotes": 1,
      "discussionId": "67a1a51e6aa8429da493d0d5"
    },
    "publishedAt": "2025-02-04T00:27:13.960Z",
    "title": "Scaling Embedding Layers in Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01637.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01591",
      "authors": [
        {
          "_id": "67a1a4b72bf092a7612b36eb",
          "name": "Antoine Dedieu",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36ec",
          "name": "Joseph Ortiz",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36ed",
          "name": "Xinghua Lou",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36ee",
          "name": "Carter Wendelken",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36ef",
          "name": "Wolfgang Lehrach",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36f0",
          "name": "J Swaroop Guntupalli",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36f1",
          "name": "Miguel Lazaro-Gredilla",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36f2",
          "name": "Kevin Patrick Murphy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:25:17.000Z",
      "title": "Transformerワールドモデル를 데이터에 대한 RL에 최적화하기 위한 개선",
      "summary": "모형 기반의 RL에 적합한 접근 방식을 제안하여, 어려운 Craftax-classic 벤치마크에서 새로운 최강 성능을 구현합니다. 이 오픈 월드의 2D 생존 게임에서, 일반적인 강력한 능력을 보여주는 것이 필요합니다. 우리는 샘플 에피소드를 향상시키기 위해 조정을 수행했으며, MBRL 알고리즘에서 1M 환경 스텝에서도 67.4%의 보상을 달성하여, DreamerV3의 53.2%를 크게 초월하고, 처음으로 인간 성능 65.0%를 초월했습니다. 우리의 방법은 SOTA의 모델 없는 모델 기반 선을 구축하고, 새로운 정책 아키텍처를 사용합니다. 그 후, 표준의 MBRL 세팅에 3가지 개선을 추가합니다: (a) \"Dyna with warmup\" - 실제 데이터와 상상 데이터로 정책을 학습하는 것, (b) \"nearest neighbor tokenizer\" - 이미지 패치에 대해 Transformer World Model (TWM)의 입력을 생성하는 구조를 개선하는 것, (c) \"block teacher forcing\" - TWM가 다음 시간 스텝의 미래 토큰을 함께 설명하는 것입니다.",
      "upvotes": 1,
      "discussionId": "67a1a4b82bf092a7612b371b"
    },
    "publishedAt": "2025-02-04T00:25:52.071Z",
    "title": "Improving Transformer World Models for Data-Efficient RL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01591.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01584",
      "authors": [
        {
          "_id": "67a1e658a68ad21bcdffead6",
          "name": "Carolyn Jane Anderson",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffead7",
          "name": "Joydeep Biswas",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffead8",
          "name": "Aleksander Boruch-Gruszecki",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffead9",
          "name": "Federico Cassano",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffeada",
          "name": "Molly Q Feldman",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffeadb",
          "name": "Arjun Guha",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffeadc",
          "name": "Francesca Lucchetti",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffeadd",
          "name": "Zixuan Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:10:38.000Z",
      "title": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models",
      "summary": "현재의 선도 모델의 벤치마크는, 전문가가 이해하기 어려운 \"파이ン드라이버레벨\"의 전문 지식에 대한 측정을 하고 있습니다. 대조적으로, 우리는, 일반적인 지식을 필요로 하는 NPR의 일요일의 퍼즐 도전에 기반한 벤치마크를 제공합니다. 우리의 벤치마크는 인간과 모델 모두에게 어려움을 줄지만, 정확한 해결책은 쉽게 확인될 수 있고, 모델의 오류는 쉽게 검출할 수 있습니다.\n\n우리의 연구는, 현재의 벤치마크에서 밝혀지지 않은 능력의 결함이 밝혀집니다: OpenAI o1은, 전문 지식에 대한 벤치마크에서 다른 이유 모델과 동일한 수준의 다른 모델보다 크게 우세하여 있습니다. 더욱이, 이유의 출력 분석에서 새로운 필드의 실패를 발견했습니다. DeepSeek R1의 예로, \"더 이상 계획하지 않습니다\"라고 알려진 것처럼, 친구 간의 잘못된 대답을 제공하기 전에 \"더 이상 계획하지 않습니다\"라고 알려진 것입니다. R1은 물론, 출력에도 \"불안\"이라는 것이 놀라울 정도로 놀라울 것입니다. 그리고, 코사이드의 경우, \"생각을 끝내지 않았습니다\"라고 하여, 모델이 추론 시의 방법론을 \"끝을 눕는\" 필요에 대한 것을 보여줍니다. 또한, R1과 Gemini Thinking의 장기적인 이유의 효과를 정량화하고, 벤치마크에서 정확도를 높이기 위해 장기적인 이유가 필요할 때의 경계를 특정하기 위해 시도했습니다.",
      "upvotes": 0,
      "discussionId": "67a1e659a68ad21bcdffeb04"
    },
    "publishedAt": "2025-02-04T05:06:50.415Z",
    "title": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d8315bad693a1a962864b3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664332914111-62d8315bad693a1a962864b3.png",
      "fullname": "Arjun Guha",
      "name": "arjunguha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18055",
      "authors": [
        {
          "_id": "67a197099b2f48315e74dcde",
          "user": {
            "_id": "67225dd94201755d88e104c4",
            "avatarUrl": "/avatars/6da69788ce0cd41c86f9dd0bf8d092aa.svg",
            "isPro": false,
            "fullname": "Edwin D. de Jong",
            "user": "EdwinDdeJong",
            "type": "user"
          },
          "name": "Edwin D. de Jong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:28.120Z",
          "hidden": false
        },
        {
          "_id": "67a197099b2f48315e74dcdf",
          "name": "Eric Marcus",
          "hidden": false
        },
        {
          "_id": "67a197099b2f48315e74dce0",
          "name": "Jonas Teuwen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-29T23:38:14.000Z",
      "title": "현재의 병리학 기반 모델은 의료기관의 차이에 취약합니다.",
      "summary": "病理학 기반 모형(FM)은 의료에서 큰 가능성을 가지고 있습니다. 이들이 임상실践에 사용되기 전에는 의료기관 간의 차이를 강하게 유지하는 것이 중요합니다. FM이 세포나 암의 종류의 생물학적 특징에 초점을 맞추고 있거나, 아니면 염색법과 다른 차이로 인한 의료기관의 신호에 초점을 맞추고 있는지 평가합니다. Robustness Index를 도입합니다. 이 새로운 Robustness 메트릭은 생물학적 특징이 신호 특징에 대해 어떤 程度의 우선권을 차지하고 있는지 반영합니다. 현재 공개된 10개의病理학 FM을 평가합니다. 현재의 FM은 의료기관의 특징을 강하게 표현하고 있습니다. Robustness Index에 대한 유의미한 차이를 보입니다. 지금까지는 그렇게 Robust하지는 않았지만, 생물학적 특징이 신호 특징에 대해 어느 정도의 우선권을 차지하는 모델은 없습니다. 의료기관의 차이가 FM 기반의 예측 성능에 미치는 영향을 정량적으로 평가하는 방법을 설명합니다. Robustness Fix Model의 불변성이 하류 모델의 분류 성능에 미치는 영향을 분석하고, 암의 종류의 분류 오류는 랜덤하지 않고, 같은 의료기관의 신호에 특히 대응하고 있음을 확인합니다: 같은 의료기관에서 다른 클래스의 이미지. FM의 매핑 공간을 시각화하고, 이는 생물학적 원인보다 의료기관에 의해 더 강하게 조직되어 있습니다. 그 결과, 의료기관의 원천이 조직의 원과 암의 종류보다 더 정확하게 예측됩니다. 이 곳에서 도입된 Robustness Fix Performance Index는 강력한, 신뢰할 수 있는 임상 적용을 위한病理학 FM의 발전을 촉진하는 데 제공됩니다.",
      "upvotes": 0,
      "discussionId": "67a1970b9b2f48315e74dd5d"
    },
    "publishedAt": "2025-02-04T04:59:22.696Z",
    "title": "Current Pathology Foundation Models are unrobust to Medical Center Differences",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67225dd94201755d88e104c4/oD8gcxl4D9G3FPXWGVGiz.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67225dd94201755d88e104c4/_jrPyZDKwbr3K9-Q4_sCH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67225dd94201755d88e104c4",
      "avatarUrl": "/avatars/6da69788ce0cd41c86f9dd0bf8d092aa.svg",
      "fullname": "Edwin D. de Jong",
      "name": "EdwinDdeJong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.00314",
      "authors": [
        {
          "_id": "67a1d1ca167bea74d520eb59",
          "name": "Moein Heidari",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5a",
          "name": "Ehsan Khodapanah Aghdam",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5b",
          "name": "Alexander Manzella",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5c",
          "name": "Daniel Hsu",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5d",
          "name": "Rebecca Scalabrino",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5e",
          "name": "Wenjin Chen",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5f",
          "name": "David J. Foran",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb60",
          "name": "Ilker Hacihaliloglu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-01T04:25:28.000Z",
      "title": "U-Net의 개선된 버전의 성능에 대한 연구 - 후腹膜암의 분할에 관한 연구",
      "summary": "腹膜후부에는 다양한 종양이 존재하며, 빈번하지 않은 benign 및 malignant 종양이 포함되어 있으며, 이러한 종양의 희귀성과 중요한 구조와의 근접으로 진단과 치료에 문제가 발생합니다. 종양의 크기를 추정하는 것은 형상의 불균질성으로 인해 어려워집니다. 손동으로 분리하는 것은 시간이 걸립니다. U-Net과 같은 모델을 사용하여 자동 분리는 계산량의 높은 부담을 가지고 있지만, 기대되는 성과를 보이고 있습니다. 이를 해결하기 위해, Mamba State Space Model (SSM)과 Extended Long-Short Term Memory (xLSTM)과 같은 구조는 긴 거리 의존성을 처리하기 위해 자원 소비를 줄이는 효율적인 해결책을 제공하고 있습니다. 본 연구에서는, 새로운 내부 CT 데이터셋과 공개된 기관 분리 데이터셋에서 U-Net의 확장 버전 (CNN, ViT, Mamba, xLSTM)을 평가합니다. 제안된 ViLU-Net 모델은 Vi-blocks을 사용하여 분리를 개선하고 있습니다. 결과적으로 xLSTM은 U-Net 프레임워크에서 효율성을 보여주는 것을 명확히 알 수 있습니다. 코드는 GitHub에서 공개되어 있습니다.",
      "upvotes": 0,
      "discussionId": "67a1d1cd167bea74d520ebf6"
    },
    "publishedAt": "2025-02-04T03:38:34.899Z",
    "title": "A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00314.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61ba19bf6122a4fd29049371",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1639586194527-noauth.jpeg",
      "fullname": "Moein Heidari",
      "name": "moein99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]