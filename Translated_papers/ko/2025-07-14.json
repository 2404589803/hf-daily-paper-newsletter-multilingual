[
  {
    "paper": {
      "id": "2507.01951",
      "authors": [
        {
          "_id": "6868daac213f123a1f88b9c8",
          "name": "Zixiao Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9c9",
          "name": "Yuxin Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9ca",
          "name": "Xiaorui Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cb",
          "name": "Mengting Xing",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cc",
          "name": "Jie Gao",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cd",
          "name": "Jianjun Xu",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9ce",
          "name": "Guangcan Liu",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cf",
          "name": "Chenhui Jin",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9d0",
          "name": "Zhuo Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9d1",
          "name": "Shengzhuo Zhang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9d2",
          "name": "Hongtao Xie",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638700c723da90491eb72722/mRIN68aSkxejrT4UqCaKz.jpeg"
      ],
      "publishedAt": "2025-07-02T17:58:01.000Z",
      "submittedOnDailyAt": "2025-07-14T00:42:27.924Z",
      "title": "테스트 시간 스케일링에 의한 리플렉티브 생성 모델",
      "submittedOnDailyBy": {
        "_id": "638700c723da90491eb72722",
        "avatarUrl": "/avatars/6dcddca5c31121b60e45aab0816e11be.svg",
        "isPro": false,
        "fullname": "Yuxin Wang",
        "user": "wangyuxin87",
        "type": "user"
      },
      "summary": "우리는 자기지원 프로세스 보상 모델(SPRM)을 사용하여 OpenAI o3의 성능을 실현한 첫 번째 반사적 생성 모델인 MetaStone-S1을 소개합니다. SPRM은 백본 네트워크를 공유하고, 다음 토큰 예측과 프로세스 스코어 평가에 대한 작업 전문 헤드를 사용하며, 무용적인 프로세스 설명이 필요하지 않고, 정책 모델과 프로세스 보상 모델(PRM)을 하나의 통합 피드백으로 통합하여 PRM 파라미터를 99% 이상 줄임으로써 효율적인 추론에 적합한 모델로 성공했습니다. SPRM을 탑재한 MetaStone-S1은 자연스럽고 테스트 시 스케일링(TTS)에 적합합니다. 우리는 제어 가능한 사고 길이에 기반하여, 낮은, 중간의, 높은 3가지 이유를 위한 노력을 제공하고 있습니다. 또한, 전체적인 사고 계산과 TTS 성능의 관계를 명확히 하기 위해 실험적으로 스케일러를 구축했습니다. 실험 결과, 우리의 MetaStone-S1은 32B 파라미터 크기로만 OpenAI-o3-mini의 시리즈와 비교할 수 있는 성능을 달성했습니다. 연구 커뮤니티의 지원을 위해, MetaStone-S1을 공개 소스로 GitHub에서 제공합니다(https://github.com/MetaStone-AI/MetaStone-S1).",
      "upvotes": 58,
      "discussionId": "6868daac213f123a1f88b9d3",
      "githubRepo": "https://github.com/MetaStone-AI/MetaStone-S1",
      "ai_summary": "MetaStone-S1, a reflective generative model using a self-supervised process reward model, achieves efficient reasoning and scalable performance with fewer parameters compared to existing models.",
      "ai_keywords": [
        "reflective generative model",
        "self-supervised process reward model",
        "backbone network",
        "task-specific heads",
        "policy model",
        "process reward model",
        "test time scaling",
        "controllable thinking length",
        "scaling law",
        "total thinking computation"
      ],
      "githubStars": 41
    },
    "publishedAt": "2025-07-02T13:58:01.000Z",
    "title": "Test-Time Scaling with Reflective Generative Model",
    "summary": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638700c723da90491eb72722/mRIN68aSkxejrT4UqCaKz.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01951.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "638700c723da90491eb72722",
      "avatarUrl": "/avatars/6dcddca5c31121b60e45aab0816e11be.svg",
      "fullname": "Yuxin Wang",
      "name": "wangyuxin87",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08776",
      "authors": [
        {
          "_id": "68745e0f257d4f0435370288",
          "name": "Zhengqing Wang",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f0435370289",
          "name": "Yuefan Wu",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f043537028a",
          "name": "Jiacheng Chen",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f043537028b",
          "name": "Fuyang Zhang",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f043537028c",
          "name": "Yasutaka Furukawa",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64d97c5bfd0b55d501ba00cf/xQVrjtR_Sb4SOz-GghYdp.mp4"
      ],
      "publishedAt": "2025-07-11T17:38:52.000Z",
      "submittedOnDailyAt": "2025-07-14T00:07:38.676Z",
      "title": "CLiFT: 압축 라이트 필드 토큰에 대한 계산 효율성과 적응성을 도모합니다.",
      "submittedOnDailyBy": {
        "_id": "64d97c5bfd0b55d501ba00cf",
        "avatarUrl": "/avatars/47505f2a573acea7176a96f538226ecb.svg",
        "isPro": false,
        "fullname": "Zhengqing Wang",
        "user": "EricW123456",
        "type": "user"
      },
      "summary": "이 논문은 \"압축 광장 토큰 (CLiFTs)\"라는 표현 기법을 제안하고, 화면의 풍부한 외관과 기하 정보를 유지하는 데 사용됩니다. CLiFT는 토큰의 압축으로 계산 효율적인 렌더링을 가능하게 하며, 같은 훈련 네트워크를 사용하여 화면의 토큰 수를 변경하여 새로운 시각을 렌더링할 수 있습니다. 구체적으로는, 이미지 세트를 제공하면, 다점 카메라의 자세로 이미지를 토큰화합니다. 잠재 공간의 K-means法是 감소된 빛의 집합을 클러스터 중심 토큰으로 선택하는 데 토큰을 사용합니다. 다점의 \"압축기\"는 모든 토큰의 정보를 클러스터 중심 토큰에 압축하여 CLiFTs를 구축합니다. 테스트 시에는, 목표의 시각과 계산 버킷 (CLiFTs의 수)를 제공하면, 지정된 수의 가까운 토큰을 모으고, 계산 효율적인 렌더링 프로세스를 통해 새로운 시각을 합성합니다. RealEstate10K와 DL3DV 데이터 세트의 실험은, quantitative 및 qualitative 평가로 우리의 접근 방식을 검증하고, 비교적으로 렌더링 품질에서도 전체적인 렌더링 점수를 달성하며, 데이터 크기, 렌더링 품질, 렌더링 속도의 보완을 제공합니다.",
      "upvotes": 38,
      "discussionId": "68745e10257d4f043537028d",
      "projectPage": "https://clift-nvs.github.io/",
      "githubRepo": "https://github.com/eric-zqwang/CLiFT",
      "ai_summary": "A neural rendering method uses compressed light-field tokens to efficiently represent scenes and render novel views with varying compute budgets.",
      "ai_keywords": [
        "neural rendering",
        "compressed light-field tokens",
        "CLiFTs",
        "multi-view encoder",
        "latent-space K-means",
        "condenser",
        "compute-adaptive renderer",
        "RealEstate10K",
        "DL3DV datasets"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-07-11T13:38:52.000Z",
    "title": "CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive\n  Neural Rendering",
    "summary": "This paper proposes a neural rendering approach that represents a scene as\n\"compressed light-field tokens (CLiFTs)\", retaining rich appearance and\ngeometric information of a scene. CLiFT enables compute-efficient rendering by\ncompressed tokens, while being capable of changing the number of tokens to\nrepresent a scene or render a novel view with one trained network. Concretely,\ngiven a set of images, multi-view encoder tokenizes the images with the camera\nposes. Latent-space K-means selects a reduced set of rays as cluster centroids\nusing the tokens. The multi-view ``condenser'' compresses the information of\nall the tokens into the centroid tokens to construct CLiFTs. At test time,\ngiven a target view and a compute budget (i.e., the number of CLiFTs), the\nsystem collects the specified number of nearby tokens and synthesizes a novel\nview using a compute-adaptive renderer. Extensive experiments on RealEstate10K\nand DL3DV datasets quantitatively and qualitatively validate our approach,\nachieving significant data reduction with comparable rendering quality and the\nhighest overall rendering score, while providing trade-offs of data size,\nrendering quality, and rendering speed.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64d97c5bfd0b55d501ba00cf/xQVrjtR_Sb4SOz-GghYdp.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d97c5bfd0b55d501ba00cf",
      "avatarUrl": "/avatars/47505f2a573acea7176a96f538226ecb.svg",
      "fullname": "Zhengqing Wang",
      "name": "EricW123456",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08800",
      "authors": [
        {
          "_id": "6874615e257d4f043537028f",
          "name": "Luke Rivard",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370290",
          "name": "Sun Sun",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370291",
          "name": "Hongyu Guo",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370292",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370293",
          "name": "Yuntian Deng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/8bwvqBSzlNYlfcdn-pVWu.mp4"
      ],
      "publishedAt": "2025-07-11T17:59:40.000Z",
      "submittedOnDailyAt": "2025-07-14T01:10:17.755Z",
      "title": "NeuralOS: Neural Generator Model을 활용한 Operating System의 시뮬레이션에 관한 연구",
      "submittedOnDailyBy": {
        "_id": "63081e15a670ed10f9d44229",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
        "isPro": true,
        "fullname": "Yuntian Deng",
        "user": "yuntian-deng",
        "type": "user"
      },
      "summary": "뉴럴로스, 네트워크 프레임워크를 소개합니다. 이는 운영 시스템의 그래픽 사용자 인터페이스(GUI)를 직접 예측하여 마우스의 이동, 클릭, 키보드 이벤트 등 사용자 입력에 대응하는 데 사용됩니다. 뉴럴로스는 컴퓨터의 상태를 추적하는 리카렌트 뉴럴 네트워크(RNN)와 분기 기반의 뉴럴 렌더링 모델을 조합하여 구성되어 있습니다. 모델은 사용자 인터페이스의 데이터 세트로 훈련되어 있으며, 이는 AI 에이전트가 생성한 리アル샷な 인터랙션과 랜덤으로 생성된 인터랙션을 모두 포함합니다. 실험은 뉴럴로스가 현실적인 GUI 시퀀스를 성공적으로 렌더링하고 마우스 인터랙션을 정확하게 인식하고 애플리케이션의 시작 등 상태 턴스샹을 신뢰적으로 예측하는 것을 보여주고 있습니다. 그러나 키보드 인터랙션의 미세한 구조를 정밀하게 모델화하는 것은 어렵지만, 뉴럴로스는 인간 컴퓨팅 인터페이스 시스템의 완벽히 적응 가능한 생성 가능한 뉴럴 인터페이스의 개발을 위해 출발하고 있습니다.",
      "upvotes": 23,
      "discussionId": "6874615e257d4f0435370294",
      "projectPage": "https://neural-os.com/",
      "githubRepo": "https://github.com/yuntian-group/neural-os",
      "ai_summary": "NeuralOS uses a combination of RNNs and diffusion-based rendering to simulate OS GUIs by predicting screen frames from user inputs, demonstrating realistic GUI rendering and state transitions.",
      "ai_keywords": [
        "recurrent neural network",
        "RNN",
        "diffusion-based neural renderer",
        "GUI",
        "user inputs",
        "mouse interactions",
        "keyboard events",
        "state transitions",
        "application launches"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-11T13:59:40.000Z",
    "title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative\n  Models",
    "summary": "We introduce NeuralOS, a neural framework that simulates graphical user\ninterfaces (GUIs) of operating systems by directly predicting screen frames in\nresponse to user inputs such as mouse movements, clicks, and keyboard events.\nNeuralOS combines a recurrent neural network (RNN), which tracks computer\nstate, with a diffusion-based neural renderer that generates screen images. The\nmodel is trained on a large-scale dataset of Ubuntu XFCE recordings, which\ninclude both randomly generated interactions and realistic interactions\nproduced by AI agents. Experiments show that NeuralOS successfully renders\nrealistic GUI sequences, accurately captures mouse interactions, and reliably\npredicts state transitions like application launches. Although modeling\nfine-grained keyboard interactions precisely remains challenging, NeuralOS\noffers a step toward creating fully adaptive, generative neural interfaces for\nfuture human-computer interaction systems.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/8bwvqBSzlNYlfcdn-pVWu.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08800.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63081e15a670ed10f9d44229",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
      "fullname": "Yuntian Deng",
      "name": "yuntian-deng",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 245
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05397",
      "authors": [
        {
          "_id": "6874a1ff257d4f0435370344",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370345",
          "name": "Jie Xia",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370346",
          "name": "Xiaopeng Peng",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370347",
          "name": "Wangbo Zhao",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370348",
          "name": "Zilong Ye",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370349",
          "name": "Zekai Li",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034a",
          "name": "Suorong Yang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034b",
          "name": "Jiadong Pan",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034c",
          "name": "Yuanxiang Chen",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034d",
          "name": "Ziqiao Wang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034e",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034f",
          "name": "Qian Zheng",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370350",
          "name": "Xiaojun Chang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370351",
          "name": "Gang Pan",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370352",
          "name": "Shurong Dong",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370353",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370354",
          "name": "Yang You",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/KhRuhMNBeK8P8MfGydldo.png"
      ],
      "publishedAt": "2025-07-07T18:31:50.000Z",
      "submittedOnDailyAt": "2025-07-14T04:53:28.591Z",
      "title": "뉴럴네트워크 Drove Image Editing",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "전통적인 이미지 편집은 일반적으로 손동작 프로노퓰터를 의존하며, 노동비가 높고, 손동작 제어 능력이나 언어 능력이 제한된 사람들에게 접근할 수 없었습니다. 최근의 뇌-컴퓨터 인터페이스(BCI)와 생성 모델의 발전을 활용하여, LoongX라는 손없이 이미지 편집 접근 방식을 제안합니다. LoongX는 23,928개의 이미지 편집 쌍의 최신 Difusion 모델을 사용하여, 동기화된 전자신경그래피(EEG), 기능적 근적외선 스펙트로그래피(fNIRS), 포터티시모그래피(PPG), 머리운동 신호와 함께 사용자의 의도를 파악합니다. 이러한 신호의 다양성을 효과적으로 처리하기 위해, LoongX는 두 개의 핵심 모듈을 조합하고 있습니다. 크로스 스케일 상태 공간(CS3) 모듈은 정보 모델의 특정 특징을 인코딩합니다. 동적 게이트 융합(DGF) 모듈은 이러한 특징을 하나의 통합 라틴스페이스로 합칩니다. 이 라틴스페이스를 Difusion 트랜스포머(DiT)의 미세 조정으로 편집 세ман틱과 일치시킵니다. 또한, 컨트랙트 훈련을 사용하여 인코더를 사전 훈련하고, 인지 상태와 임베디된 자연어로부터의 세ман틱의 의도를 일치시킵니다. 확장된 실험에 따르면, LoongX는 텍스트 주도 메소드와 같은 성능을 달성하며, 신경 신호와 음성의 조합으로 이을 수 있었습니다(CLIP-T: 0.2588 vs. 0.2549). 이러한 결과를 통해, 신경 구동 생성 모델이 접근 가능한 직감적인 이미지 편집을 가능하게 하고, 인지 구동 컨테이너 기술의 새로운 방향을 개척하는 것을 보여주었습니다. 데이터셋과 코드는 향후 연구와 이 새로운 분야의 발전을 촉진하기 위해 공개됩니다.",
      "upvotes": 16,
      "discussionId": "6874a200257d4f0435370355",
      "projectPage": "https://loongx1.github.io/",
      "githubRepo": "https://github.com/LanceZPF/loongx",
      "ai_summary": "LoongX uses multimodal neurophysiological signals and diffusion models for hands-free image editing, achieving performance comparable to text-driven methods and outperforming them when combined with speech.",
      "ai_keywords": [
        "diffusion models",
        "electroencephalography (EEG)",
        "functional near-infrared spectroscopy (fNIRS)",
        "photoplethysmography (PPG)",
        "head motion signals",
        "cross-scale state space (CS3) module",
        "dynamic gated fusion (DGF) module",
        "diffusion transformer (DiT)",
        "contrastive learning"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-07-07T14:31:50.000Z",
    "title": "Neural-Driven Image Editing",
    "summary": "Traditional image editing typically relies on manual prompting, making it\nlabor-intensive and inaccessible to individuals with limited motor control or\nlanguage abilities. Leveraging recent advances in brain-computer interfaces\n(BCIs) and generative models, we propose LoongX, a hands-free image editing\napproach driven by multimodal neurophysiological signals. LoongX utilizes\nstate-of-the-art diffusion models trained on a comprehensive dataset of 23,928\nimage editing pairs, each paired with synchronized electroencephalography\n(EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography\n(PPG), and head motion signals that capture user intent. To effectively address\nthe heterogeneity of these signals, LoongX integrates two key modules. The\ncross-scale state space (CS3) module encodes informative modality-specific\nfeatures. The dynamic gated fusion (DGF) module further aggregates these\nfeatures into a unified latent space, which is then aligned with edit semantics\nvia fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train\nthe encoders using contrastive learning to align cognitive states with semantic\nintentions from embedded natural language. Extensive experiments demonstrate\nthat LoongX achieves performance comparable to text-driven methods (CLIP-I:\n0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural\nsignals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results\nhighlight the promise of neural-driven generative models in enabling\naccessible, intuitive image editing and open new directions for\ncognitive-driven creative technologies. Datasets and code will be released to\nsupport future work and foster progress in this emerging area.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/KhRuhMNBeK8P8MfGydldo.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05397.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08799",
      "authors": [
        {
          "_id": "6874ac1e257d4f0435370389",
          "name": "Max Belitsky",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038a",
          "name": "Dawid J. Kopiczko",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038b",
          "name": "Michael Dorkenwald",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038c",
          "name": "M. Jehanzeb Mirza",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038d",
          "name": "Cees G. M. Snoek",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038e",
          "name": "Yuki M. Asano",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/637d21239a5217b88b7549c3/w5UPBRgGM_-9_ELxgOBlL.png"
      ],
      "publishedAt": "2025-07-11T17:59:36.000Z",
      "submittedOnDailyAt": "2025-07-14T05:40:13.268Z",
      "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
      "submittedOnDailyBy": {
        "_id": "637d21239a5217b88b7549c3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d21239a5217b88b7549c3/LrIGPiva5VGVZG87rTAJz.jpeg",
        "isPro": false,
        "fullname": "Yuki Asano",
        "user": "yukimasano",
        "type": "user"
      },
      "summary": "우리는 캐시 조향을 제안합니다. 이는 키-밸류 캐시에 직접 한 번의 간섭을 적용하여 언어 모델을 비시위적으로 조향하는 경량화 방법입니다. 캐시 조향의 효과성을 검증하기 위해, 작은 언어 모델에 캐시 조향을 적용하여 체인의 사고를 유도합니다. 우리의 접근법은 GPT-4o가 생성한 추론 트래스들을 활용하여 조향 벡터를 구축하여, 미세 조정이나 프롬프트 수정 없이 모델 행동을 더 명확하고 다단계적인 추론으로 조향시키는 데 사용됩니다. 다양한 추론 벤치마크에서 실험적 평가는 캐시 조향이 모델 추론의 질적 구조와 quantitative 태스크 성능을 모두 개선함을 보여주었습니다. 이전에 요구하는 지속적인 간섭을 필요로 하는 활성화 조향 기술과 비교하여, 한 번의 캐시 조향은 하이퍼파라미터 안정성, 추론 시간 효율성과 통합의 쉬움에 대한 상당한 이점을 제공합니다. 이로 인해, 제어된 생성에 대한 더 강건하고 실용적인 솔루션으로 간주될 수 있습니다.",
      "upvotes": 15,
      "discussionId": "6874ac1e257d4f043537038f",
      "ai_summary": "Cache steering improves reasoning in language models through a single intervention in the key-value cache, enhancing both reasoning structure and task performance.",
      "ai_keywords": [
        "cache steering",
        "key-value cache",
        "chain-of-thought reasoning",
        "GPT-4o",
        "steering vectors",
        "multi-step reasoning",
        "activation steering",
        "hyperparameter stability",
        "inference-time efficiency",
        "ease of integration",
        "controlled generation"
      ]
    },
    "publishedAt": "2025-07-11T13:59:36.000Z",
    "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
    "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/637d21239a5217b88b7549c3/w5UPBRgGM_-9_ELxgOBlL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08799.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "637d21239a5217b88b7549c3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d21239a5217b88b7549c3/LrIGPiva5VGVZG87rTAJz.jpeg",
      "fullname": "Yuki Asano",
      "name": "yukimasano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08801",
      "authors": [
        {
          "_id": "68746fc3257d4f04353702ce",
          "name": "Hangjie Yuan",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702cf",
          "name": "Weihua Chen",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d0",
          "name": "Jun Cen",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d1",
          "name": "Hu Yu",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d2",
          "name": "Jingyun Liang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d3",
          "name": "Shuning Chang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d4",
          "name": "Zhihui Lin",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d5",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d6",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d7",
          "name": "Jiazheng Xing",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d8",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d9",
          "name": "Jiasheng Tang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702da",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702db",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:59:42.000Z",
      "submittedOnDailyAt": "2025-07-14T01:18:10.056Z",
      "title": "Lumos-1: 통일 모형에서의 자동 복원 비디오 생성의 관점",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "自動회귀형 대 언어 모델(LLMs)은 여러 언어 작업들을 통일시키고, 자동회귀형 비디오 생성에 초기적인 노력을 촉발시켰습니다. 현재의 자동회귀형 비디오 생성기는 표준 LLM 아키텍처로부터 벗어난 반면, 큰 외부 텍스트 인코더에 의존하며, 다음 토큰의 해석에 의한 지연이 문제로 되었습니다. 본 논문에서는 최소한의 변경으로 LLM 아키텍처를 유지하는 자동회귀형 비디오 생성기 Lumos-1을 소개합니다. LLMs에 공간-시간적인 상관성을 注入하기 위해 3D RoPE의 효과를 인식하고, 주파수 스펙트럼의 불균형 범위를 진단했습니다. 이에 따라 MM-RoPE를 제안합니다. MM-RoPE는 원본 텍스트의 RoPE를 유지하면서, 여러 모델화를 위한 확장된 주파수 스펙트럼과 3D 위치 정보를 제공합니다. 또한 Lumos-1은 시간적인 원인성을 준수하는 때의 토큰의 의존관계를 채택합니다. 이 의존관계에 기반하여, 공간 정보의 반복성으로 인한 프레임별 손실의 불균형 문제를 식별하고, 자동회귀형 이산 디피로마션 포싱(AR-DF)을 제안합니다. AR-DF는 훈련 시 시간적인 튜브 마스크를 사용하며, 추론 시의 마스크 정책과 호환하도록 품질 저하를 방지합니다. 메모리 효율적인 훈련 기술로, Lumos-1은 48 그래픽스 상에서 예측 및 编集中, GenEval에서 EMU3와 같은 성능, VBench-I2V에서 COSMOS-Video2World와 같은 성능, VBench-T2V에서 OpenSoraPlan과 같은 성능을 달성했습니다. 코드와 모델은 https://github.com/alibaba-damo-academy/Lumos에서 사용 가능합니다.",
      "upvotes": 13,
      "discussionId": "68746fc3257d4f04353702dc",
      "githubRepo": "https://github.com/alibaba-damo-academy/Lumos",
      "ai_summary": "Lumos-1 is an autoregressive video generator that uses a modified LLM architecture with MM-RoPE and AR-DF to address spatiotemporal correlation and frame-wise loss imbalance, achieving competitive performance with fewer resources.",
      "ai_keywords": [
        "autoregressive large language models",
        "LLMs",
        "autoregressive video generation",
        "3D RoPE",
        "MM-RoPE",
        "token dependency strategy",
        "intra-frame bidirectionality",
        "inter-frame temporal causality",
        "frame-wise loss imbalance",
        "Autoregressive Discrete Diffusion Forcing",
        "AR-DF",
        "temporal tube masking",
        "GenEval",
        "COSMOS-Video2World",
        "VBench-I2V",
        "OpenSoraPlan",
        "VBench-T2V"
      ],
      "githubStars": 26
    },
    "publishedAt": "2025-07-11T13:59:42.000Z",
    "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective",
    "summary": "Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08801.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08772",
      "authors": [
        {
          "_id": "68746c95257d4f04353702b7",
          "name": "Shaocong Dong",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702b8",
          "name": "Lihe Ding",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702b9",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702ba",
          "name": "Yaokun Li",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bb",
          "name": "Yuxin Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bc",
          "name": "Yucheng Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bd",
          "name": "Qi Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702be",
          "name": "Jaehyeok Kim",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bf",
          "name": "Chenjian Gao",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c0",
          "name": "Zhanpeng Huang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c1",
          "name": "Zibin Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c2",
          "name": "Tianfan Xue",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c3",
          "name": "Dan Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:33:18.000Z",
      "submittedOnDailyAt": "2025-07-14T01:05:21.741Z",
      "title": "온네오트 모ー드：3D 생성의 컨텍스트付き 파르츠 잠재\n\n（注意：此处的翻译是直接从日文翻译成韩文，因为原始文本是日文，而非英文。如果需要英文到韩文的翻译，请提供英文文本。）",
      "submittedOnDailyBy": {
        "_id": "63ae91af2314b93f9e6dde42",
        "avatarUrl": "/avatars/792ce138cbee85b8754fdcec7fb1ff52.svg",
        "isPro": false,
        "fullname": "Shaocong Dong",
        "user": "dscdyc",
        "type": "user"
      },
      "summary": "최근의 3D 생성 기술은 3D 고유의 잠재 확산 프레임워크로 전환되었으며, 이는 실제 데이터에서 추론된 기하학적 정보에 기반합니다. 이러한 발전에도 불구하고, 3D 생성에 대한 세 가지 주요 한계가 남아 있습니다: 1) 단일 잠재 표현은 복잡한 다중 기하학을 포착하지 못하여 세부적인 정보 손실이 발생합니다; 2) 전체적인 잠재 코딩은 구성 설계에 필요한 부분 독립성과 상호관계를 잃습니다; 3) 글로벌 조건 부여 구조는 미소한 연동성을 갖지 않습니다. 3D 설계 작업 흐름을 모델링하고, CoPart - 3D 오브젝트를 컨텍스트에 대한 부분의 잠재 표현으로 분해하여, 분산적인 다중 부분 생성을 가능하게 하는 부분 인식의 확산 프레임워크를 제안합니다. 이 패러다임은 세 가지 장점을 제공합니다: i) 부분 분해로 인코딩 복잡성을 줄입니다; ii) 명시적인 부분 관계 모델링을 가능하게 합니다; iii) 부분 수준의 조건 부여를 지원합니다. 또한, 관련 신경망을 사용하여, 사전 훈련된 DifFence 모델을 공통의 부분의 잠재 노이즈에 최적화하기 위한 상호 가이드 전략을 개발합니다. 새로운 3D 부분 데이터 세트를 구축하여, Partverse - Objaverse에서 자동 마스 분할과 인간 확인된 어노테이션을 통해, 대규모 훈련을 가능하게 합니다. 광범위한 실험에서, CoPart의 부분 수준의 편집, 아키텍처 오브젝트의 생성, 그리고 전례가 없는 연동성을 갖춘 시나리오의 구성에 있어서, 높은 성능을 보여주었습니다.",
      "upvotes": 9,
      "discussionId": "68746c96257d4f04353702c4",
      "projectPage": "https://hkdsc.github.io/project/copart/",
      "githubRepo": "https://github.com/hkdsc/copart",
      "ai_summary": "A part-aware diffusion framework, CoPart, enhances 3D generation by decomposing objects into contextual parts, improving complexity handling, relationship modeling, and part-level conditioning.",
      "ai_keywords": [
        "latent diffusion frameworks",
        "geometric priors",
        "single-latent representations",
        "holistic latent coding",
        "part independence",
        "interrelationships",
        "compositional design",
        "global conditioning mechanisms",
        "fine-grained controllability",
        "human 3D design workflows",
        "part-aware diffusion framework",
        "contextual part latents",
        "coherent multi-part generation",
        "encoding complexity",
        "part relationship modeling",
        "part-level conditioning",
        "mutual guidance strategy",
        "joint part latent denoising",
        "geometric coherence",
        "foundation model priors",
        "Partverse",
        "Objaverse",
        "automated mesh segmentation",
        "human-verified annotations",
        "part-level editing",
        "articulated object generation",
        "scene composition"
      ],
      "githubStars": 38
    },
    "publishedAt": "2025-07-11T13:33:18.000Z",
    "title": "From One to More: Contextual Part Latents for 3D Generation",
    "summary": "Recent advances in 3D generation have transitioned from multi-view 2D\nrendering approaches to 3D-native latent diffusion frameworks that exploit\ngeometric priors in ground truth data. Despite progress, three key limitations\npersist: (1) Single-latent representations fail to capture complex multi-part\ngeometries, causing detail degradation; (2) Holistic latent coding neglects\npart independence and interrelationships critical for compositional design; (3)\nGlobal conditioning mechanisms lack fine-grained controllability. Inspired by\nhuman 3D design workflows, we propose CoPart - a part-aware diffusion framework\nthat decomposes 3D objects into contextual part latents for coherent multi-part\ngeneration. This paradigm offers three advantages: i) Reduces encoding\ncomplexity through part decomposition; ii) Enables explicit part relationship\nmodeling; iii) Supports part-level conditioning. We further develop a mutual\nguidance strategy to fine-tune pre-trained diffusion models for joint part\nlatent denoising, ensuring both geometric coherence and foundation model\npriors. To enable large-scale training, we construct Partverse - a novel 3D\npart dataset derived from Objaverse through automated mesh segmentation and\nhuman-verified annotations. Extensive experiments demonstrate CoPart's superior\ncapabilities in part-level editing, articulated object generation, and scene\ncomposition with unprecedented controllability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08772.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63ae91af2314b93f9e6dde42",
      "avatarUrl": "/avatars/792ce138cbee85b8754fdcec7fb1ff52.svg",
      "fullname": "Shaocong Dong",
      "name": "dscdyc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08794",
      "authors": [
        {
          "_id": "68747a58257d4f04353702f9",
          "name": "Yulai Zhao",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fa",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fb",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fc",
          "name": "S. Y. Kung",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fd",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fe",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:55:22.000Z",
      "submittedOnDailyAt": "2025-07-14T02:04:25.605Z",
      "title": "1 토큰으로 LLM-as-a-Judge를 속이는 방법",
      "submittedOnDailyBy": {
        "_id": "62d58fd53bf5e059f7cc3245",
        "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
        "isPro": false,
        "fullname": "Dian Yu",
        "user": "yudian",
        "type": "user"
      },
      "summary": "生成 보상 모델 (또는 LLMs-as-judges)은 대규모 언어 모델 (LLMs)를 사용하여 답변의 품질을 평가하는 모델로, 확실한 보상을 포함하는 강화 학습 (RLVR)에 의해 증가하여 사용되고 있습니다. 규칙 기반 메트릭보다 선호되며, 특히, 자유형 출력을 포함하는 복잡한 이유론의 태스크에 있어서 특히 선호됩니다. 이 패러다임에서 LLM은 일반적으로 후보 답변과 사실적인 참조를 비교하여 정확성을 보여주는 이진 보상을 부여합니다. 이 비교 태스크의 눈에 보이는 간단한 점과는 달리, 생성 보상 모델은 표면적인 동작에 취약합니다: 비단어 기호 (예: 「:」 또는 「.」) 및 이유론의 시작과 같은 「생각의 과정:」 또는 「이 문제를 단계별로 해결하겠습니다:」가 잘못된 정답 보상을 자주 불러일으키는 경우가 많습니다. 이 취약점이 LLMs, 데이터셋, 엑스퓸포트 형식의 광범위한 범위에서 확장되어 있으며, 보상 모델을 기반으로 한 핵심 알고리즘 패러다임 (예: 거부 샘플링, 취향 최적화, RLVR)에 대해 엄격한 위협을 줍니다. 이 문제를 완화하기 위해, 우리는 간단하고 효과적인 데이터 확장 전략을 도입하고, 새로운 생성 보상 모델을 학습하여 크게 개선된 강건성을 갖춘 것을 만들었습니다. 우리의 발견은 더 신뢰할 수 있는 LLM 기반 평가 방법의 긴급한 필요성을 강조합니다. 우리는 https://huggingface.co/sarosavo/Master-RM과 https://huggingface.co/datasets/sarosavo/Master-RM에서 강건하고 일반 영역의 보상 모델과 그 합성 데이터를 공개합니다.",
      "upvotes": 7,
      "discussionId": "68747a58257d4f04353702ff",
      "ai_summary": "Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.",
      "ai_keywords": [
        "generative reward models",
        "LLMs-as-judges",
        "large language models",
        "reinforcement learning with verifiable rewards",
        "RLVR",
        "binary reward",
        "rejection sampling",
        "preference optimization"
      ]
    },
    "publishedAt": "2025-07-11T13:55:22.000Z",
    "title": "One Token to Fool LLM-as-a-Judge",
    "summary": "Generative reward models (also known as LLMs-as-judges), which use large\nlanguage models (LLMs) to evaluate answer quality, are increasingly adopted in\nreinforcement learning with verifiable rewards (RLVR). They are often preferred\nover rigid rule-based metrics, especially for complex reasoning tasks involving\nfree-form outputs. In this paradigm, an LLM is typically prompted to compare a\ncandidate answer against a ground-truth reference and assign a binary reward\nindicating correctness. Despite the seeming simplicity of this comparison task,\nwe find that generative reward models exhibit surprising vulnerabilities to\nsuperficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning\nopeners like \"Thought process:\" and \"Let's solve this problem step by step.\"\ncan often lead to false positive rewards. We demonstrate that this weakness is\nwidespread across LLMs, datasets, and prompt formats, posing a serious threat\nfor core algorithmic paradigms that rely on generative reward models, such as\nrejection sampling, preference optimization, and RLVR. To mitigate this issue,\nwe introduce a simple yet effective data augmentation strategy and train a new\ngenerative reward model with substantially improved robustness. Our findings\nhighlight the urgent need for more reliable LLM-based evaluation methods. We\nrelease our robust, general-domain reward model and its synthetic training data\nat https://huggingface.co/sarosavo/Master-RM and\nhttps://huggingface.co/datasets/sarosavo/Master-RM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08794.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d58fd53bf5e059f7cc3245",
      "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
      "fullname": "Dian Yu",
      "name": "yudian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08441",
      "authors": [
        {
          "_id": "6874b80f257d4f04353703a8",
          "name": "Anlin Zheng",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703a9",
          "name": "Xin Wen",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703aa",
          "name": "Xuanyang Zhang",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ab",
          "name": "Chuofan Ma",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ac",
          "name": "Tiancai Wang",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ad",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ae",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703af",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T09:32:45.000Z",
      "submittedOnDailyAt": "2025-07-14T06:26:25.095Z",
      "title": "비전 펀드 모델을 자동 복원 이미지 생성의 효과적인 시각 토큰나이저로 취급하기 위한 관점",
      "submittedOnDailyBy": {
        "_id": "63483629ac5172169929da0e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665676793089-noauth.jpeg",
        "isPro": false,
        "fullname": "Xin Wen",
        "user": "xwen99",
        "type": "user"
      },
      "summary": "レビューフォービジョンファンダメンタルモデル의 강력한 표현을 활용하고, 이 모델 위에 직접적인 이미지 토큰라이너를 구축하는 새로운 방향을 검토합니다. 특히, 이 분야는 크게 탐색되지 않았습니다. 구체적으로는, 자유드レビューフォービジョンファンダメンタルモデル을 토큰라이너의 인코더로 사용합니다. 이러한 모델의 효과를 향상시키기 위해, 2가지 주요 구성 요소를 도입합니다. 1. 2D 그리드 상의 예측된 특성의冗長성을 줄이기 위한 영역별 감소 프레임워크. 2. 토큰라이너의 출력을 Fundamental Model의 표현과 일치시키고, 문맥적 의존성을 유지하는 문맥적 재구성 오브젝트. 이러한 설계에 기반하여 제안된 이미지 토큰라이너, VFMTok는 이미지 재구성 및 생성의 품질에 큰 향상을 입히고, 토큰링의 효율성도 향상합니다. 또한, 자동 복원(AR) 생성을 촉진하고, ImageNet 벤치마크에서 gFID가 2.07을 달성하며, 모델의 수렴을 3배로 가속화하고, 클래스 조건付き의 고품질 합성을 가능하게 합니다. 코드는 공개적으로 릴리즈되며, 커뮤니티에 이익을 제공하고 있습니다.",
      "upvotes": 4,
      "discussionId": "6874b80f257d4f04353703b0",
      "ai_summary": "A novel image tokenizer built on pre-trained vision foundation models improves image reconstruction, generation quality, and token efficiency, enhancing autoregressive generation and class-conditional synthesis.",
      "ai_keywords": [
        "pre-trained vision foundation models",
        "image tokenizer",
        "region-adaptive quantization framework",
        "semantic reconstruction objective",
        "VFMTok",
        "gFID",
        "autoregressive generation",
        "class-conditional synthesis",
        "classifier-free guidance"
      ]
    },
    "publishedAt": "2025-07-11T05:32:45.000Z",
    "title": "Vision Foundation Models as Effective Visual Tokenizers for\n  Autoregressive Image Generation",
    "summary": "Leveraging the powerful representations of pre-trained vision foundation\nmodels -- traditionally used for visual comprehension -- we explore a novel\ndirection: building an image tokenizer directly atop such models, a largely\nunderexplored area. Specifically, we employ a frozen vision foundation model as\nthe encoder of our tokenizer. To enhance its effectiveness, we introduce two\nkey components: (1) a region-adaptive quantization framework that reduces\nredundancy in the pre-trained features on regular 2D grids, and (2) a semantic\nreconstruction objective that aligns the tokenizer's outputs with the\nfoundation model's representations to preserve semantic fidelity. Based on\nthese designs, our proposed image tokenizer, VFMTok, achieves substantial\nimprovements in image reconstruction and generation quality, while also\nenhancing token efficiency. It further boosts autoregressive (AR) generation --\nachieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model\nconvergence by three times, and enabling high-fidelity class-conditional\nsynthesis without the need for classifier-free guidance (CFG). The code will be\nreleased publicly to benefit the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08441.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63483629ac5172169929da0e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665676793089-noauth.jpeg",
      "fullname": "Xin Wen",
      "name": "xwen99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06952",
      "authors": [
        {
          "_id": "68749c03257d4f043537033e",
          "name": "Keyon Vafa",
          "hidden": false
        },
        {
          "_id": "68749c03257d4f043537033f",
          "name": "Peter G. Chang",
          "hidden": false
        },
        {
          "_id": "68749c03257d4f0435370340",
          "name": "Ashesh Rambachan",
          "hidden": false
        },
        {
          "_id": "68749c03257d4f0435370341",
          "name": "Sendhil Mullainathan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T15:36:15.000Z",
      "submittedOnDailyAt": "2025-07-14T04:27:13.989Z",
      "title": "기초 모델은 무엇에 주목했나요? 추론적 편향을 활용하여 세계 모델을 탐색합니다.",
      "submittedOnDailyBy": {
        "_id": "64d98ef7a4839890b25eb78b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
        "isPro": true,
        "fullname": "Fangyuan Yu",
        "user": "Ksgk-fy",
        "type": "user"
      },
      "summary": "기초 모델은 순서 예측이 깊은 도메인 이해를 명확히 하는 데 기반하고 있습니다. 이는 카이프터의 행성 운동을 예측하는 것과 뉴턴의 역학의 발견이 연결된 것과 같이입니다. 그러나 이러한 모델이 본질적인 구조를 정확히 이해하고 있는지 평가하는 것은 어렵습니다. 우리는 기초 모델 평가에 적합한 기술을 개발하고, 이 기술은 모델이 설정된 세계 모델로부터 생성된 합성 데이터 세트에 어떻게 적응하는지를 조사합니다. 이 기술은 기초 모델의 인덱스 브레이스가 세계 모델에 일치하는지 평가하고, 이를 위해 인덱스 브레이스 프로브라고 합니다. 여러 도메인에서 기초 모델은 훈련 태스크에서 뛰어난 성능을 보였지만, 새로운 태스크에 적응할 때, 세계 모델에 대한 인덱스 브레이스 프로브를 개발하지 않았음을 발견했습니다. 특히,궤도 프로젝트로 훈련된 기초 모델은 새로운 물리 태스크에 적응할 때 뉴턴의 역학을 적용하지 않는 것을 일관되게 발견했습니다. 또한, 이러한 모델은 태스크에 적합한 휴리스틱을 개발하고, 이들이 일반화되지 못하도록 행동하는 것을 명확히 알 수 있었습니다.",
      "upvotes": 4,
      "discussionId": "68749c03257d4f0435370342",
      "ai_summary": "Foundation models, despite excelling in training tasks, often fail to generalize to new tasks due to task-specific heuristics rather than capturing underlying world models.",
      "ai_keywords": [
        "sequence prediction",
        "foundation models",
        "inductive bias",
        "synthetic datasets",
        "world model",
        "inductive bias probe",
        "Newtonian mechanics",
        "task-specific heuristics",
        "generalization"
      ]
    },
    "publishedAt": "2025-07-09T11:36:15.000Z",
    "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for\n  World Models",
    "summary": "Foundation models are premised on the idea that sequence prediction can\nuncover deeper domain understanding, much like how Kepler's predictions of\nplanetary motion later led to the discovery of Newtonian mechanics. However,\nevaluating whether these models truly capture deeper structure remains a\nchallenge. We develop a technique for evaluating foundation models that\nexamines how they adapt to synthetic datasets generated from some postulated\nworld model. Our technique measures whether the foundation model's inductive\nbias aligns with the world model, and so we refer to it as an inductive bias\nprobe. Across multiple domains, we find that foundation models can excel at\ntheir training tasks yet fail to develop inductive biases towards the\nunderlying world model when adapted to new tasks. We particularly find that\nfoundation models trained on orbital trajectories consistently fail to apply\nNewtonian mechanics when adapted to new physics tasks. Further analysis reveals\nthat these models behave as if they develop task-specific heuristics that fail\nto generalize.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d98ef7a4839890b25eb78b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
      "fullname": "Fangyuan Yu",
      "name": "Ksgk-fy",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08771",
      "authors": [
        {
          "_id": "68747a17257d4f04353702ef",
          "name": "Chenyang Song",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f0",
          "name": "Weilin Zhao",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f1",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f2",
          "name": "Chaojun Xiao",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f3",
          "name": "Yingfa Chen",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f4",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f5",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f6",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:28:56.000Z",
      "submittedOnDailyAt": "2025-07-14T03:24:41.458Z",
      "title": "BlockFFN: チャンクレベル 활성화 스패시티에 의한 종단 측 가속에 적합한 Mixed of Experts oriented\n\n(注意：由于原文中包含了一些技术术语，翻译时尽量保持了专业性和准确性，但某些术语可能需要根据具体上下文进行调整。)",
      "submittedOnDailyBy": {
        "_id": "64c09684e56520a63d35ec87",
        "avatarUrl": "/avatars/86a338e8d122a66a94143bbb9bf3ebf8.svg",
        "isPro": false,
        "fullname": "Chenyang Song",
        "user": "Raincleared",
        "type": "user"
      },
      "summary": "라르지 라ン그 언어 모델(LLMs)의 계산 부담을 줄이기 위해, 활성화의 희소성을 갖는 아키텍처, 특히 믹스 오브 에픽스퍼트(MoE)를 대표하는 것들이 주목받고 있습니다. 그러나, 베이스 모델의 MoE의 미분 불변성과 유연성의 부족이 모델의 성능을 저하시켰습니다. 또한, 각 토큰이 그少数의 파라미터를 활성화하는 데만 충분한 것은, 희소하게 활성화된 아키텍처가 토큰 레벨의 희소성이 낮은 것을 밝혀냅니다. 이는 연속된 다수의 토큰의 활성화 파라미터의 비율이 큰 것을 나타냅니다. 이 희소성의 패턴은, 저 리소스 조건(예: 단말 장치)에서의 가속이 적합하지 않으며,主流의 가속 기술(예: 추론 해석)과도 적합하지 않습니다. 이러한 문제를 해결하기 위해, 우리는 새로운 MoE 아키텍처, BlockFFN을 도입하고, 효율적인 훈련 및 배치手法를 제안합니다. 특히, ReLU 활성화와 RMSNorm을 조합한 루터를 사용함으로써, 미분 가능한 및 유연한 루팅을 실현합니다. 다음으로, 토큰 레벨의 희소성(TLS)와 토큰 클럭 레벨의 희소성(CLS)을 모두 촉진하기 위해, CLS에 대한 훈련 객젝티브를 설계하고, BlockFFN을 가속에 적합하게 만듭니다. 마지막으로, 활성화의 희소성과 추론 해석을 조합한 첫 번째 유효한 가속 캐너를 구현합니다. 실험 결과는, BlockFFN이 다른 MoE 기반 라인에 대항하여 높은 성능을 보여주고, 80% 이상의 TLS와 70% 이상의 8 토큰의 CLS를 달성했습니다. 우리의 캐너는 실제 단말 장치에서 3.67배의 속도 업그레이드를 달성했습니다. 모든 코드와 체크포인트는 공개되어 있습니다(https://github.com/thunlp/BlockFFN).",
      "upvotes": 2,
      "discussionId": "68747a17257d4f04353702f7",
      "githubRepo": "https://github.com/thunlp/BlockFFN",
      "githubStars": 3
    },
    "publishedAt": "2025-07-11T13:28:56.000Z",
    "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity",
    "summary": "To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67times speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08771.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c09684e56520a63d35ec87",
      "avatarUrl": "/avatars/86a338e8d122a66a94143bbb9bf3ebf8.svg",
      "fullname": "Chenyang Song",
      "name": "Raincleared",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07151",
      "authors": [
        {
          "_id": "68707d75c8391850d6097823",
          "user": {
            "_id": "66b34647a29e5c00011d34c3",
            "avatarUrl": "/avatars/ae39f9b84f9379423fa3a8509fbcc94e.svg",
            "isPro": false,
            "fullname": "Zongmeng Zhang",
            "user": "ustc-zhangzm",
            "type": "user"
          },
          "name": "Zongmeng Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:24.241Z",
          "hidden": false
        },
        {
          "_id": "68707d75c8391850d6097824",
          "name": "Wengang Zhou",
          "hidden": false
        },
        {
          "_id": "68707d75c8391850d6097825",
          "name": "Jie Zhao",
          "hidden": false
        },
        {
          "_id": "68707d75c8391850d6097826",
          "name": "Houqiang Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b34647a29e5c00011d34c3/Zj-2S7r3SlJnKipepoQxm.png"
      ],
      "publishedAt": "2025-07-09T11:18:38.000Z",
      "submittedOnDailyAt": "2025-07-14T01:45:01.699Z",
      "title": "강한 다모달 대언어 모델로 モデルフリックトレッド 대책",
      "submittedOnDailyBy": {
        "_id": "66b34647a29e5c00011d34c3",
        "avatarUrl": "/avatars/ae39f9b84f9379423fa3a8509fbcc94e.svg",
        "isPro": false,
        "fullname": "Zongmeng Zhang",
        "user": "ustc-zhangzm",
        "type": "user"
      },
      "summary": "다모달 대언어 모댈(MLLM)은 시각 언어 태스크에서 뛰어난 능력을 보여주지만, 실세계 상황에서는 호라이즈션(Hallucination)이 쉽게 발생합니다. 본 논문에서는 모델의 응답과 입력 사이의 충돌에 초점을 맞추어, MLLM의 호라이즈션 현상을 조사합니다. 기존 연구들은 모델의 응답과 입력의 충돌을 중점으로 하지만, 우리는 모델이 호라이즈션을 직접招く 입력의 고유한 충돌을 연구합니다. 모델 충돌을 공식적으로 정의하고, 시각 언어 태스크에서 이 현상을 시뮬레이션하기 위해, Multimodal Modality Conflict (MMMC) 데이터 세트를 구축했습니다. Prompt 엔지니어링, 규범적 조정, 강화학습 기반의 3가지 방법을 제안하여, 모델 충돌에 의한 호라이즈션을 완화시키는 것을 목표로 합니다. MMMC 데이터 세트를 확장하여 실험을 수행하고, 이러한 방법의 장점과 단점을 분석했습니다. 우리의 결과를 통해, 강화학습 방법은 모델 충돌 아래 호라이즈션을 완화시키는 최고의 성능을 달성하고, 규범적 조정 방법은 바람직한 성능을 나타냅니다. 우리의 연구는 호라이즈션을招く 미주의 모델 충돌을 명확히 하고, MLLM의 강건성을 더 깊은 이해를 제공합니다.",
      "upvotes": 2,
      "discussionId": "68707d76c8391850d6097827",
      "projectPage": "https://github.com/zmzhang2000/MMMC",
      "githubRepo": "https://github.com/zmzhang2000/MMMC",
      "ai_summary": "Investigation of modality conflict in multimodal large language models reveals its role in causing hallucinations, with reinforcement learning emerging as the most effective mitigation strategy.",
      "ai_keywords": [
        "multimodal large language models",
        "vision-language tasks",
        "hallucinations",
        "modality conflict",
        "prompt engineering",
        "supervised fine-tuning",
        "reinforcement learning",
        "Multimodal Modality Conflict (MMMC)"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-09T07:18:38.000Z",
    "title": "Robust Multimodal Large Language Models Against Modality Conflict",
    "summary": "Despite the impressive capabilities of multimodal large language models\n(MLLMs) in vision-language tasks, they are prone to hallucinations in\nreal-world scenarios. This paper investigates the hallucination phenomenon in\nMLLMs from the perspective of modality conflict. Unlike existing works focusing\non the conflicts between model responses and inputs, we study the inherent\nconflicts in inputs from different modalities that place MLLMs in a dilemma and\ndirectly lead to hallucinations. We formally define the modality conflict and\nconstruct a dataset named Multimodal Modality Conflict (MMMC) to simulate this\nphenomenon in vision-language tasks. Three methods based on prompt engineering,\nsupervised fine-tuning, and reinforcement learning are proposed to alleviate\nthe hallucination caused by modality conflict. Extensive experiments are\nconducted on the MMMC dataset to analyze the merits and demerits of these\nmethods. Our results show that the reinforcement learning method achieves the\nbest performance in mitigating the hallucination under modality conflict, while\nthe supervised fine-tuning method shows promising and stable performance. Our\nwork sheds light on the unnoticed modality conflict that leads to\nhallucinations and provides more insights into the robustness of MLLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b34647a29e5c00011d34c3/Zj-2S7r3SlJnKipepoQxm.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07151.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b34647a29e5c00011d34c3",
      "avatarUrl": "/avatars/ae39f9b84f9379423fa3a8509fbcc94e.svg",
      "fullname": "Zongmeng Zhang",
      "name": "ustc-zhangzm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]