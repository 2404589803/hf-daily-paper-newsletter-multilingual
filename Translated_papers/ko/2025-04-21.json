[
  {
    "paper": {
      "id": "2504.13835",
      "authors": [
        {
          "_id": "6805b38355d3c792e1a9d0dd",
          "name": "Yicheng Chen",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0de",
          "name": "Yining Li",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0df",
          "name": "Kai Hu",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0e0",
          "name": "Zerun Ma",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0e1",
          "name": "Haochen Ye",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0e2",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T17:59:46.000Z",
      "submittedOnDailyAt": "2025-04-21T01:39:08.191Z",
      "title": "MIG: 정보수익을 최대화하기 위한 자동적인 데이터 선택에 의한 세ман틱스페이스에서의 유전학조정",
      "submittedOnDailyBy": {
        "_id": "649988726677f66c2b486392",
        "avatarUrl": "/avatars/b649a77370660e129726e29504daba34.svg",
        "isPro": false,
        "fullname": "Yining Li",
        "user": "ly015",
        "type": "user"
      },
      "summary": "데이터의 품질과 다양성은 유효한 인스트럭션 튜닝 데이터 세트의 구축에 중요한 요소입니다. 오픈 소스 인스트럭션 튜닝 데이터 세트의 활용이 증가함에 따라, 많은 데이터에서 고품질과 다양성을 가진 서브셋을 자동적으로 선택하는 것이 유리합니다. 현재의 방법은 일반적으로 인스턴스의 품질을 우선시하고 휴리스틱 규칙을 사용하여 다양성을 유지하지만, 이러한 방법들은 데이터 세트 전체의 일관된 시각을 갖지 않기 때문에, 최적의 결과를 얻을 수 있는 경우가 있습니다. 또한 휴리스틱 규칙은 일반적으로 내장 공간 내의 거리나 클러스터링을 중시하기 때문에, 복잡한 인스트럭션의 의도를 정확히 감지하는 것이 불가능합니다. 이러한 gap을 메우기 위해, 데이터 세트의 정보량을 정량화하기 위한 통일된 방법을 제안합니다. 이 방법은 라벨 그래프를 구축하여 기호적인 공간을 모델화하고, 그래프 내의 정보 분포에 기반하여 다양성을 정량화합니다. 이러한 평가에 따라, 데이터 샘플링 방법을 효율적으로 도입하고, 반복적으로 데이터 샘플을 선택하여 기호적인 공간에서 정보 손실을 최소화하고, 정보 gain을 최대화하는 것을 목표로 합니다. 다양한 데이터 세트와 기초 모델에 대한 실험은 이 방법이 가장 先端한 방법(state-of-the-art)을 초월하는 것을 보여주고 있습니다. 특히, MIG로 샘플링 된 5%의 Tulu3 데이터를 사용하여 미세 조정된 모델은 전체 데이터 세트에서 훈련된 공식의 SFT 모델과 비교하여 상대적인 성능을 나타내고, AlpacaEval에서 +5.73%와 Wildbench에서 +6.89%의 개선을 보였습니다.",
      "upvotes": 26,
      "discussionId": "6805b38555d3c792e1a9d155",
      "projectPage": "https://yichengchen24.github.io/projects/mig",
      "githubRepo": "https://github.com/yichengchen24/MIG",
      "ai_keywords": [
        "label graph",
        "semantic space",
        "information content",
        "Maximize the Information Gain (MIG)"
      ]
    },
    "publishedAt": "2025-04-18T13:59:46.000Z",
    "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space",
    "summary": "Data quality and diversity are key to the construction of effective\ninstruction-tuning datasets. % With the increasing availability of open-source\ninstruction-tuning datasets, it is advantageous to automatically select\nhigh-quality and diverse subsets from a vast amount of data. % Existing methods\ntypically prioritize instance quality and use heuristic rules to maintain\ndiversity. % However, this absence of a comprehensive view of the entire\ncollection often leads to suboptimal results. % Moreover, heuristic rules\ngenerally focus on distance or clustering within the embedding space, which\nfails to accurately capture the intent of complex instructions in the semantic\nspace. % To bridge this gap, we propose a unified method for quantifying the\ninformation content of datasets. This method models the semantic space by\nconstructing a label graph and quantifies diversity based on the distribution\nof information within the graph. % Based on such a measurement, we further\nintroduce an efficient sampling method that selects data samples iteratively to\nMaximize the Information Gain (MIG) in semantic\nspace. % Experiments on various datasets and base models demonstrate that MIG\nconsistently outperforms state-of-the-art methods. % Notably, the model\nfine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance\nto the official SFT model trained on the full dataset, with improvements of\n+5.73\\% on AlpacaEval and +6.89\\% on Wildbench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13835.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649988726677f66c2b486392",
      "avatarUrl": "/avatars/b649a77370660e129726e29504daba34.svg",
      "fullname": "Yining Li",
      "name": "ly015",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13837",
      "authors": [
        {
          "_id": "6805b9ec7c5fa8020f595642",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595643",
          "name": "Zhiqi Chen",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595644",
          "name": "Rui Lu",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595645",
          "name": "Andrew Zhao",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595646",
          "name": "Zhaokai Wang",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595647",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595648",
          "name": "Shiji Song",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595649",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/2KWQqFdVDUCAu-kSu87fa.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/rF4cAa6DAI3EaejDV_dgG.mp4"
      ],
      "publishedAt": "2025-04-18T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-21T01:54:36.096Z",
      "title": "강화학습은 기초 모델을 초월하여 LLM의 논리 능력에 대한 이유론을 촉발시키는가?",
      "submittedOnDailyBy": {
        "_id": "649d475111592b1a765ac1a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
        "isPro": false,
        "fullname": "Yang Yue",
        "user": "Yang130",
        "type": "user"
      },
      "summary": "RLVR（Verifiable Rewards）는 최근 LLM의 추론 능력을 향상시키기 위한 데에서 상당한 성공을 보였으며, 특히 수학과 프로그래밍 태스크에서 뛰어난 성능을 보였다. 일반적으로 RLVR는 LLM이 지속적으로 자기 개선할 수 있도록 해준다고 생각되어 있으며, 기존의 기본 모델보다 새로운 추론 능력을 얻는 것을 기대하고 있다. 그러나 본 연구에서는 이러한 가정을 비판적으로 재검토하고, 큰 k 값을 사용한 pass@k 메트릭을 평가하고 다양한 모델 familes와 벤치마크에서 모델의 추론 능력을 조사하고 있다. 놀라울 정도로, RL은 새로운 추론 패턴을 근본적으로 추출하지 않는다는 것을 알게 되었다. RL로 훈련된 모델은 작은 k 값（예를 들어 k=1）에서 기본 모델보다 우수한 성능을 보지만, 큰 k 값에서는 기본 모델이 상대적으로 또는 더 높은 pass@k 점수를 달성하는 것을 관찰할 수 있다. RL로 훈련된 모델이 생성하는 추론 경로는 기본 모델의 샘플링 분포에 이미 포함되어 있는 것을 보여주고, 이러한 모델이 얻은 추론 능력은 기본 모델에서 이미 존재하는 것임을 보여준다. 또한, RL 훈련은 보상을 쉽게 주는 경로로 모델의 출력 분포를 편향시키고, 정확한 응답을 효율적으로 샘플링하는 데에 성능을 향상시킬 수 있는 것을 알게 되었다. 그러나 이 방법은 기본 모델과 비교했을 때 추론 능력의 경계가 좁아지는 것을 관찰할 수 있다. RLVR를 사용한 시각화 추론 태스크에서도 유사한 결과를 얻었다. 또한, RLVR와 달리 새로운 지식을 모델에 소개할 수 있다는 것을 확인했다. 이러한 발견은 RLVR가 LLM의 추론 능력을 향상시키기 위한 중요한 한계를 보여주고, 추론 LLM의 RL 훈련의 영향과 더 좋은 패러다임의 필요성을 근본적으로 재검토하는 것을 요구하고 있다. 프로젝트 페이지: https://limit-of-RLVR.github.io",
      "upvotes": 24,
      "discussionId": "6805b9ed7c5fa8020f59568c",
      "projectPage": "https://limit-of-rlvr.github.io/",
      "githubRepo": "https://github.com/LeapLabTHU/limit-of-RLVR",
      "ai_keywords": [
        "Reinforcement Learning",
        "Verifiable Rewards",
        "RLVR",
        "LLMs (Large Language Models)",
        "reasoning capabilities",
        "mathematics",
        "programming tasks",
        "pass@\\textit{k}",
        "benchmark",
        "RL-trained models",
        "base models",
        "reasoning paths",
        "sampling distribution",
        "performance",
        "biasing",
        "output distribution",
        "visual reasoning tasks",
        "distillation"
      ]
    },
    "publishedAt": "2025-04-18T13:59:56.000Z",
    "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@k metric with large values of k to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does not, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of k (\\eg, k=1), base models can achieve a comparable or\neven higher pass@k score compared to their RL counterparts at large k\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/2KWQqFdVDUCAu-kSu87fa.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/rF4cAa6DAI3EaejDV_dgG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13837.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "649d475111592b1a765ac1a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
      "fullname": "Yang Yue",
      "name": "Yang130",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11833",
      "authors": [
        {
          "_id": "6805bb01747a412bca737b53",
          "name": "Changjiang Gao",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b54",
          "name": "Xu Huang",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b55",
          "name": "Wenhao Zhu",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b56",
          "name": "Shujian Huang",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b57",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b58",
          "name": "Fei Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T07:45:10.000Z",
      "submittedOnDailyAt": "2025-04-21T01:57:09.327Z",
      "title": "언어의 다양성이 대규모 언어 모델의 추론 능력을 강화 할 수 있는지 생각합니다.",
      "submittedOnDailyBy": {
        "_id": "65fed45b08d35929362dd651",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
        "isPro": false,
        "fullname": "FeiYuan",
        "user": "FeYuan",
        "type": "user"
      },
      "summary": "이전의 연구에 따르면, 대규모 언어 모델은 상당한 \"영어를 편향하는\" 현상을 나타내고, 영어로 태스크를 제공하는 경우 훨씬 더 좋은 성능을 보여주는 경우가 많습니다. 흥미롭게도, 우리는 특정 언어를 사용하여 논리 태스크에서 더 좋은 성능을 얻을 수 있음을 관찰했습니다. 그러나 이러한 현상은 아직 조사가 부족합니다. 본 논문에서는 논리 태스크에서의 다언어 활용의 한계를 평가하고, 영어로만 이루어진 논리와 비교하여, 다언어 논리는 약 10 Acc@k 포인트 정도의 효과적인 향상과 강인한 (번역 품질 및 언어 선택의 변화에 대한 견고성) 한계를 높일 수 있음을 보여줍니다. 또한 한계의 원인과 이를 달성하는 과정에서의 문제점을 분석하고, 일반적인 답안 선택 방법들이 이러한 한계를 달성하지 못하는 것을 밝혀줍니다. 이러한 시사점은 LLM의 다언어 논리의 모든 가능성을 활용하기 위한 미래 연구에 도움이 될 것으로 기대합니다.",
      "upvotes": 14,
      "discussionId": "6805bb02747a412bca737b7e",
      "githubRepo": "https://github.com/CONE-MT/multilingual_reasoning"
    },
    "publishedAt": "2025-04-16T03:45:10.000Z",
    "title": "Could Thinking Multilingually Empower LLM Reasoning?",
    "summary": "Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@k points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11833.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fed45b08d35929362dd651",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
      "fullname": "FeiYuan",
      "name": "FeYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13072",
      "authors": [
        {
          "_id": "6805bfc5e332a61dd90160b0",
          "name": "Wenqi Dong",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b1",
          "name": "Bangbang Yang",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b2",
          "name": "Zesong Yang",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b3",
          "name": "Yuan Li",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b4",
          "name": "Tao Hu",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b5",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b6",
          "name": "Yuewen Ma",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b7",
          "name": "Zhaopeng Cui",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63d748ff6f49aa82306b7e48/DNTOfy5oOkjTJwLBeGOUH.qt"
      ],
      "publishedAt": "2025-04-17T16:33:39.000Z",
      "submittedOnDailyAt": "2025-04-21T03:26:56.809Z",
      "title": "하이Scene: 3D 스키밍 작업 중 등각 뷰 생성에 의한 계층적 스키밍 제작",
      "submittedOnDailyBy": {
        "_id": "63d748ff6f49aa82306b7e48",
        "avatarUrl": "/avatars/9f0b8b8a09b14d76e52ed1bd312e6b63.svg",
        "isPro": false,
        "fullname": "BB Yang",
        "user": "ybbbbt",
        "type": "user"
      },
      "summary": "3D 생성은 멀티미디어와 컴퓨터 그래픽에서 중요한 경계면으로, 현재의 방법들은 대상의 물체 분류의 제한과 상호작용 애플리케이션의 편집 유연성에 약점을 가지고 있습니다. 본 논문에서는 2D 이미지 생성과 3D 물체 생성 사이에 gap을 줄이는 새로운 단계별 프레임워크 \"HiScene\"를 소개합니다. 이 프레임워크는 구성적인 인식과 예술적인 시네마 콘텐츠를 가진 고정밀도의 시선을 생성합니다. 우리의 주요 아이디어는 등거리 변환 뷰에서 시선을 단계별 \"물체\"로 취급하는 것입니다. 이를 통해 방은 더 복잡한 물체로 더 분해가 가능하며, 더 분해가 가능한 항목으로 분해될 수 있습니다. 이 단계별 접근 방식에 의해 3D 콘텐츠와 2D 표현을 일치시키면서 구성적 구조를 유지할 수 있습니다. 각 분해된 인스턴스의 완전성과 공간적 정렬을 보장하기 위해, 밴드 처리와 그림 처리를 효과적으로 수행하는 비디오 디퓨저 기반의 모델 완성 기술과 함께, 공간적 일관성을 보장하기 위해 형상 우선 注入를 도입합니다. 실험 결과를 통해, 우리의 방법은 자연스러운 물체 배치와 완전한 물체 인스턴스를 생성하며, 물리적 합리성과 사용자 입력과의 일치를 유지합니다.",
      "upvotes": 5,
      "discussionId": "6805bfc9e332a61dd901618b",
      "ai_keywords": [
        "hierarchical framework",
        "2D image generation",
        "3D object generation",
        "video-diffusion-based amodal completion",
        "occlusions",
        "shadows",
        "shape prior injection",
        "spatial coherence",
        "natural object arrangements",
        "complete object instances",
        "interactive applications",
        "physical plausibility",
        "user inputs"
      ]
    },
    "publishedAt": "2025-04-17T12:33:39.000Z",
    "title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation",
    "summary": "Scene-level 3D generation represents a critical frontier in multimedia and\ncomputer graphics, yet existing approaches either suffer from limited object\ncategories or lack editing flexibility for interactive applications. In this\npaper, we present HiScene, a novel hierarchical framework that bridges the gap\nbetween 2D image generation and 3D object generation and delivers high-fidelity\nscenes with compositional identities and aesthetic scene content. Our key\ninsight is treating scenes as hierarchical \"objects\" under isometric views,\nwhere a room functions as a complex object that can be further decomposed into\nmanipulatable items. This hierarchical approach enables us to generate 3D\ncontent that aligns with 2D representations while maintaining compositional\nstructure. To ensure completeness and spatial alignment of each decomposed\ninstance, we develop a video-diffusion-based amodal completion technique that\neffectively handles occlusions and shadows between objects, and introduce shape\nprior injection to ensure spatial coherence within the scene. Experimental\nresults demonstrate that our method produces more natural object arrangements\nand complete object instances suitable for interactive applications, while\nmaintaining physical plausibility and alignment with user inputs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63d748ff6f49aa82306b7e48/DNTOfy5oOkjTJwLBeGOUH.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13072.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d748ff6f49aa82306b7e48",
      "avatarUrl": "/avatars/9f0b8b8a09b14d76e52ed1bd312e6b63.svg",
      "fullname": "BB Yang",
      "name": "ybbbbt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13157",
      "authors": [
        {
          "_id": "6804392129303a3402c4f38e",
          "user": {
            "_id": "631bfb21f6bc4be4a6592afc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
            "isPro": false,
            "fullname": "Khiem Vuong",
            "user": "kvuong2711",
            "type": "user"
          },
          "name": "Khiem Vuong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-20T15:01:35.224Z",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f38f",
          "name": "Anurag Ghosh",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f390",
          "name": "Deva Ramanan",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f391",
          "name": "Srinivasa Narasimhan",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f392",
          "name": "Shubham Tulsiani",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/x2dR6H7Gl0l8qbXY9YxGP.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/Kf60HymR1YXGVqoSmQNYb.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/O0wHR9zr6E56IE7sjs0UI.jpeg"
      ],
      "publishedAt": "2025-04-17T17:57:05.000Z",
      "submittedOnDailyAt": "2025-04-21T01:28:10.375Z",
      "title": "AerialMegaDepth: 飞行-지면 재구성 및 시각점 합성 학습",
      "submittedOnDailyBy": {
        "_id": "631bfb21f6bc4be4a6592afc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
        "isPro": false,
        "fullname": "Khiem Vuong",
        "user": "kvuong2711",
        "type": "user"
      },
      "summary": "우리는 기하학적 재구성의 과제를 조사합니다. 현재의 최尖端의 학습 기반의 접근 방식은 기공으로부터의 사진과 그리드으로부터의 사진의 극단적인 시각적 변화를 처리할 수 없습니다. 우리의 가설은, 고품질의 동변 기록된 기공으로부터의 그리드의 데이터 세트가 이 실패의 주요 원인입니다. 이 데이터는 어떤 방법으로도 구축하기 어렵기 때문에, 확장 가능한 구축이 어렵습니다. 이 도전을 극복하기 위해, 우리는 3D 도시 워크스페이스에서의 포맷 바이트 데이터(예: Google Earth)와 실제 그리드 레벨의 컴포드 샘플링 데이터(예: MegaDepth)를 조합하여 확장 가능한 프레임워크를 제안합니다. 포맷 바이트 데이터는 광범위한 기공으로부터의 시각을 시뮬레이션하고, 실제 컴포드 샘플링 데이터는 맵 기반의 포맷 바이트 데이터가 충분한 세부 정보가 없는 경우 그리드 레벨의 시각적 정확도를 향상시키고, 실상 이미지와 포맷 바이트 데이터의 영역 간 간극을 닫습니다. 이 하이브리드 데이터 세트를 사용하여, 우리는 여러 최尖端의 알고리즘을 조정하여 실제 세계적인, 0샷의 기공으로부터의 그리드의 작업에서 뚜렷한 향상을 달성합니다. 예를 들어, 기본 DUSt3R는 카메라 회전 오류가 5도 이내인 기공으로부터의 그리드의 페어를 5% 미만으로 위치하는 반면, 우리의 데이터로 조정하면 정확도가 근사 56%에 도달하며, 주요 실패점을 해결합니다. 카메라 측정과 스케나 재구성을 제외한 우리의 데이터는, 어려운 기공으로부터의 그리드의 스케나 합성과 같은 하류 작업의 성능을 향상시키고, 실제 세계적인 애플리케이션에서 우리의 접근 방식의 실용적인 가치를 보여주는 것입니다.",
      "upvotes": 4,
      "discussionId": "6804392329303a3402c4f3e8",
      "projectPage": "https://aerial-megadepth.github.io/",
      "githubRepo": "https://github.com/kvuong2711/aerial-megadepth",
      "ai_keywords": [
        "geometric reconstruction",
        "learning-based approaches",
        "extreme viewpoint variation",
        "co-registered",
        "aerial-ground datasets",
        "pseudo-synthetic renderings",
        "3D city-wide meshes",
        "crowd-sourced images",
        "visual fidelity",
        "domain gap",
        "mesh-based renderings",
        "fine-tuning",
        "DUSt3R",
        "camera rotation error",
        "scene reconstruction",
        "novel-view synthesis",
        "downstream tasks"
      ]
    },
    "publishedAt": "2025-04-17T13:57:05.000Z",
    "title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis",
    "summary": "We explore the task of geometric reconstruction of images captured from a\nmixture of ground and aerial views. Current state-of-the-art learning-based\napproaches fail to handle the extreme viewpoint variation between aerial-ground\nimage pairs. Our hypothesis is that the lack of high-quality, co-registered\naerial-ground datasets for training is a key reason for this failure. Such data\nis difficult to assemble precisely because it is difficult to reconstruct in a\nscalable way. To overcome this challenge, we propose a scalable framework\ncombining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google\nEarth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The\npseudo-synthetic data simulates a wide range of aerial viewpoints, while the\nreal, crowd-sourced images help improve visual fidelity for ground-level images\nwhere mesh-based renderings lack sufficient detail, effectively bridging the\ndomain gap between real images and pseudo-synthetic renderings. Using this\nhybrid dataset, we fine-tune several state-of-the-art algorithms and achieve\nsignificant improvements on real-world, zero-shot aerial-ground tasks. For\nexample, we observe that baseline DUSt3R localizes fewer than 5% of\naerial-ground pairs within 5 degrees of camera rotation error, while\nfine-tuning with our data raises accuracy to nearly 56%, addressing a major\nfailure point in handling large viewpoint changes. Beyond camera estimation and\nscene reconstruction, our dataset also improves performance on downstream tasks\nlike novel-view synthesis in challenging aerial-ground scenarios, demonstrating\nthe practical value of our approach in real-world applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/x2dR6H7Gl0l8qbXY9YxGP.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/Kf60HymR1YXGVqoSmQNYb.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/O0wHR9zr6E56IE7sjs0UI.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13157.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631bfb21f6bc4be4a6592afc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
      "fullname": "Khiem Vuong",
      "name": "kvuong2711",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11544",
      "authors": [
        {
          "_id": "6804ca9fd8538baa1c39ca93",
          "name": "Tianyang Xu",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca94",
          "name": "Haojie Zheng",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca95",
          "name": "Chengze Li",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca96",
          "name": "Haoxiang Chen",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca97",
          "name": "Yixin Liu",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca98",
          "name": "Ruoxi Chen",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca99",
          "name": "Lichao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T18:24:00.000Z",
      "submittedOnDailyAt": "2025-04-21T01:38:46.380Z",
      "title": "NodeRAG: ホモジュラス ノード를 가진 그래프 기반의 RAG의 구조화",
      "submittedOnDailyBy": {
        "_id": "6610fb736504d9bed5890d58",
        "avatarUrl": "/avatars/832b186fc51c639f1709025d442b3f4b.svg",
        "isPro": false,
        "fullname": "Tianyang Xu",
        "user": "TerryXu666",
        "type": "user"
      },
      "summary": "レタイブレーディングアウガーエンジャン(RAG)는 큰 언어 모델이 외부와 개인적인 코퍼스를 액세스할 수 있도록 하여, 특정 분야에서 사실적으로 일치하는 답변을 제공하는 방법입니다. 그래프 기반의 RAG 방법들은 코퍼스의 내적 구조를 활용하여, 그래프 인덱스를 구축하고, 그래프의 구조적 특성을 활용하여 이 과정을 진행합니다. 그러나 현재의 그래프 기반의 RAG 접근법은 그래프 구조의 설계를 우선하지 않습니다. 설계가 부족한 그래프는 다양한 그래프 알고리즘의 쉽게 통합을 방해하고, 작업 흐름의 불연속성과 성능 저하를招きます. 그래프의 잠재력을 더욱 발휘하기 위해, 노드 RAG라는 그래프 중심적인 프레임워크를 제안합니다. 이 프레임워크는 다양한 그래프 구조를 도입하고, 그래프 기반의 머신 학습을 RAG 작업 흐름에 쉽게 통합할 수 있게 합니다. LLM의 능력을 밀접하게 맞춘다면, 이 프레임워크는 일관된 효율적인 프로세스를 보장합니다. 구체적인 실험을 통해, 노드 RAG는 이전 방법과 비교하여, 인덱스 시간, 쿼리 시간, 저장 효율성, 그리고 단계별 벤치마크 및 개방된 헤드온 엔드 평가에서 우수한 질문 대답 성능을 보여주며, 성능이 우수한 측면을 보여줍니다. 우리의 GitHub 리포지토리는 https://github.com/Terry-Xu-666/NodeRAG에 있습니다.",
      "upvotes": 4,
      "discussionId": "6804caa0d8538baa1c39cac2",
      "projectPage": "https://terry-xu-666.github.io/NodeRAG_web/",
      "githubRepo": "https://github.com/Terry-Xu-666/NodeRAG",
      "ai_keywords": [
        "Retrieval-augmented generation (RAG)",
        "external and private corpus",
        "factually consistent responses",
        "knowledge graph index",
        "graph-based RAG methods",
        "heterogeneous graph structures",
        "seamless and holistic integration",
        "end-to-end process",
        "question-answering performance",
        "multi-hop benchmarks",
        "open-ended head-to-head evaluations",
        "retrieval tokens"
      ]
    },
    "publishedAt": "2025-04-15T14:24:00.000Z",
    "title": "NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes",
    "summary": "Retrieval-augmented generation (RAG) empowers large language models to access\nexternal and private corpus, enabling factually consistent responses in\nspecific domains. By exploiting the inherent structure of the corpus,\ngraph-based RAG methods further enrich this process by building a knowledge\ngraph index and leveraging the structural nature of graphs. However, current\ngraph-based RAG approaches seldom prioritize the design of graph structures.\nInadequately designed graph not only impede the seamless integration of diverse\ngraph algorithms but also result in workflow inconsistencies and degraded\nperformance. To further unleash the potential of graph for RAG, we propose\nNodeRAG, a graph-centric framework introducing heterogeneous graph structures\nthat enable the seamless and holistic integration of graph-based methodologies\ninto the RAG workflow. By aligning closely with the capabilities of LLMs, this\nframework ensures a fully cohesive and efficient end-to-end process. Through\nextensive experiments, we demonstrate that NodeRAG exhibits performance\nadvantages over previous methods, including GraphRAG and LightRAG, not only in\nindexing time, query time, and storage efficiency but also in delivering\nsuperior question-answering performance on multi-hop benchmarks and open-ended\nhead-to-head evaluations with minimal retrieval tokens. Our GitHub repository\ncould be seen at https://github.com/Terry-Xu-666/NodeRAG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11544.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6610fb736504d9bed5890d58",
      "avatarUrl": "/avatars/832b186fc51c639f1709025d442b3f4b.svg",
      "fullname": "Tianyang Xu",
      "name": "TerryXu666",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.09621",
      "authors": [
        {
          "_id": "6800ef5509eaa9d1d87a6eaf",
          "name": "Jiuchen Chen",
          "hidden": false
        },
        {
          "_id": "6800ef5509eaa9d1d87a6eb0",
          "user": {
            "_id": "6672c01fa6eb488f049ecb80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
            "isPro": false,
            "fullname": "Xinyu Yan",
            "user": "fengyanzi",
            "type": "user"
          },
          "name": "Xinyu Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-19T15:17:09.181Z",
          "hidden": false
        },
        {
          "_id": "6800ef5509eaa9d1d87a6eb1",
          "name": "Qizhi Xu",
          "hidden": false
        },
        {
          "_id": "6800ef5509eaa9d1d87a6eb2",
          "name": "Kaiqi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T15:41:25.000Z",
      "submittedOnDailyAt": "2025-04-21T05:45:47.747Z",
      "title": "토크나이즈 이미지 패치：대화상의 효과적인 노이즈 제거의 글로벌 맥락 융합",
      "submittedOnDailyBy": {
        "_id": "6672c01fa6eb488f049ecb80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
        "isPro": false,
        "fullname": "Xinyu Yan",
        "user": "fengyanzi",
        "type": "user"
      },
      "summary": "글로벌 컨텍스트 정보와 지역적인 세부 특징은 흐음 제거 태스크에서 필수적이다. 딥러닝 모델은 작은, 저해상도 이미지에 능숙하지만, 큰, 고해상도 이미지에 대해 GPU 메모리의 제한으로 어려움을 겪는다. 이에 보완하여, 이미지 슬라이딩이나 다운 샘플링을 선택하게 된다. 그러나 이 방법은 전역 정보를 줄이고, 고주파의 세부 정보를 버리는 단점이 있다. 이러한 문제를 해결하기 위해, 우리는 DehazeXL이라는 흐음 제거 방법을 제안한다. 이는 전역 컨텍스트와 지역적인 특징 추출을 더 잘 조화시키며,主流의 GPU 하드웨어에서 큰 이미지의 시작부터 끝까지의 모델링을 가능하게 한다. 또한, 흐음 제거 태스크의 특성에 맞는 시각적인 속성 부여 방법을 설계하고, 전역 컨텍스트의 효율적 활용을 평가하기 위해 도움을 준다. 마지막으로, 큰 이미지의 흐음 제거에 대한 벤치마크 데이터 세트의 부족을 인식하여, 8KDehaze라는 초고해상도의 흐음 제거 데이터 세트를 개발했다. 이는 10000 쌍의 청결한 및 흐음 있는 원격 관측 이미지를 포함하며, 각 이미지의 크기는 8192×8192 픽셀이다. 확장된 실험은 DehazeXL은 21GB의 메모리로 최대 10240×10240 픽셀의 이미지를 추론할 수 있으며, 모든 평가된 방법 중 가장 先端한 결과를 구현한다. 소스 코드와 실험 데이터 세트는 https://github.com/CastleChen339/DehazeXL에서 이용할 수 있다.",
      "upvotes": 4,
      "discussionId": "6800ef5709eaa9d1d87a6f76",
      "projectPage": "https://castlechen339.github.io/DehazeXL.github.io/",
      "githubRepo": "https://github.com/CastleChen339/DehazeXL",
      "ai_keywords": [
        "haze removal",
        "image slicing",
        "downsampling",
        "DehazeXL",
        "global context",
        "local feature extraction",
        "end-to-end modeling",
        "visual attribution",
        "8KDehaze",
        "ultra-high-resolution haze removal dataset",
        "remote sensing images"
      ]
    },
    "publishedAt": "2025-04-13T11:41:25.000Z",
    "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal\n  in Large Images",
    "summary": "Global contextual information and local detail features are essential for\nhaze removal tasks. Deep learning models perform well on small, low-resolution\nimages, but they encounter difficulties with large, high-resolution ones due to\nGPU memory limitations. As a compromise, they often resort to image slicing or\ndownsampling. The former diminishes global information, while the latter\ndiscards high-frequency details. To address these challenges, we propose\nDehazeXL, a haze removal method that effectively balances global context and\nlocal feature extraction, enabling end-to-end modeling of large images on\nmainstream GPU hardware. Additionally, to evaluate the efficiency of global\ncontext utilization in haze removal performance, we design a visual attribution\nmethod tailored to the characteristics of haze removal tasks. Finally,\nrecognizing the lack of benchmark datasets for haze removal in large images, we\nhave developed an ultra-high-resolution haze removal dataset (8KDehaze) to\nsupport model training and testing. It includes 10000 pairs of clear and hazy\nremote sensing images, each sized at 8192 times 8192 pixels. Extensive\nexperiments demonstrate that DehazeXL can infer images up to 10240 times\n10240 pixels with only 21 GB of memory, achieving state-of-the-art results\namong all evaluated methods. The source code and experimental dataset are\navailable at https://github.com/CastleChen339/DehazeXL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6672c01fa6eb488f049ecb80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
      "fullname": "Xinyu Yan",
      "name": "fengyanzi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13626",
      "authors": [
        {
          "_id": "6805fa66fddd500b98039425",
          "name": "Yule Liu",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b98039426",
          "name": "Jingyi Zheng",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b98039427",
          "name": "Zhen Sun",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b98039428",
          "name": "Zifan Peng",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b98039429",
          "name": "Wenhan Dong",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b9803942a",
          "name": "Zeyang Sha",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b9803942b",
          "name": "Shiwen Cui",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b9803942c",
          "name": "Weiqiang Wang",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b9803942d",
          "name": "Xinlei He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T11:07:19.000Z",
      "submittedOnDailyAt": "2025-04-21T06:29:28.134Z",
      "title": "로그인하여 자세한 답변을 받기로 합니다.",
      "submittedOnDailyBy": {
        "_id": "63da3d7ae697e5898cb86854",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675246771355-noauth.jpeg",
        "isPro": false,
        "fullname": "Talha Rüzgar Akkuş",
        "user": "Q-bert",
        "type": "user"
      },
      "summary": "최근의 대규모 인지 모델(LRMs)의 발전은 테스트 시의 계산량을 스케일링하여 다양한 태스크의 인지 능력을 향상시키는 효과성을 보여주고 있습니다. 그러나 LRMs는 일반적으로 \"초과 인지\" 문제를 겪습니다, 모델이 제한된 성능 효과를 얻기 위해 매우 많은 인지 단계를 생성하는 경우가 있습니다. 현재의 연구는 과도 인지를 완화시키기 위해 미세 조정을 의존하고 있지만, 이는 추가 데이터, 비정규적인 훈련 설정, 안전성 조정에 대한 위험, 그리고 일반화 능력의 저하를 동반합니다.\n\n실험적 분석을 통해 LRM의 행동의 중요한 특징을 밝혀내어, 작은 모델이 생성한 외부의 CoTs를 기억 토큰(<think>과 <think> 사이에 삽입함으로써, 모델이 다수의 기억을 생성하는 것을 억제할 수 있음을 밝혀졌습니다. 이 아이디어에 기반하여 ThoughtMani라는 간단하고 효율적인 파이프라인을 제안하고, LRMs가 불필요한 중간 단계를 스킵하여 계산 비용을 크게 줄일 수 있음을 허용합니다. 실험을 광범위하게 수행하여 ThoughtMani의 효용성과 효율성을 증명했습니다. 예를 들어, LiveBench/Code 데이터셋에 QwQ-32B에 적용한 경우, ThoughtMani는 원래의 성능을 유지하면서 출력 토큰 수를 약 30% 줄였으며, CoT 생성기에서의 오버헤드도 줄였습니다. 또한 ThoughtMani는 안전성 조정을 평균 10% 향상시켰습니다. 모델의 비즈니스 포지션은 일반적으로 다른 크기의 모델을 동시에 제공하기 때문에, ThoughtMani는 현실적인 애플리케이션에 더 효율적이고 접근 가능한 LRMs를 구축하는 효과적인 방법을 제공합니다.",
      "upvotes": 3,
      "discussionId": "6805fa67fddd500b98039461",
      "ai_keywords": [
        "large reasoning models (LRMs)",
        "overthinking problems",
        "fine-tuning",
        "thinking token",
        "external CoTs (Chain of Thought)",
        "ThoughtMani",
        "unnecessary intermediate steps",
        "computational costs",
        "LiveBench/Code dataset",
        "output token counts",
        "safety alignment"
      ]
    },
    "publishedAt": "2025-04-18T07:07:19.000Z",
    "title": "Thought Manipulation: External Thought Can Be Efficient for Large\n  Reasoning Models",
    "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities in multiple tasks. However, LRMs typically suffer from\n\"overthinking\" problems, where models generate significantly redundant\nreasoning steps while bringing limited performance gains. Existing work relies\non fine-tuning to mitigate overthinking, which requires additional data,\nunconventional training setups, risky safety misalignment, and poor\ngeneralization.\n  Through empirical analysis, we reveal an important characteristic of LRM\nbehaviors that placing external CoTs generated by smaller models between the\nthinking token (<think> and </think>) can effectively\nmanipulate the model to generate fewer thoughts. Building on these insights, we\npropose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass\nunnecessary intermediate steps and reduce computational costs significantly. We\nconduct extensive experiments to validate the utility and efficiency of\nThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code\ndataset, ThoughtMani keeps the original performance and reduces output token\ncounts by approximately 30%, with little overhead from the CoT generator.\nFurthermore, we find that ThoughtMani enhances safety alignment by an average\nof 10%. Since model vendors typically serve models of different sizes\nsimultaneously, ThoughtMani provides an effective way to construct more\nefficient and accessible LRMs for real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13626.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63da3d7ae697e5898cb86854",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675246771355-noauth.jpeg",
      "fullname": "Talha Rüzgar Akkuş",
      "name": "Q-bert",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 89
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13173",
      "authors": [
        {
          "_id": "6805c4dab15a57fcb59b6f08",
          "name": "Ali Behrouz",
          "hidden": false
        },
        {
          "_id": "6805c4dab15a57fcb59b6f09",
          "name": "Meisam Razaviyayn",
          "hidden": false
        },
        {
          "_id": "6805c4dab15a57fcb59b6f0a",
          "name": "Peilin Zhong",
          "hidden": false
        },
        {
          "_id": "6805c4dab15a57fcb59b6f0b",
          "name": "Vahab Mirrokni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:59:33.000Z",
      "submittedOnDailyAt": "2025-04-21T02:39:28.607Z",
      "title": "모든 것이 연결되어 있습니다: 테스트 타임 메모리 제시션, 주의 바이어스, 저장, 온라인 최적화의 여정",
      "submittedOnDailyBy": {
        "_id": "65cccd5134a5d74cbaa9446c",
        "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
        "isPro": false,
        "fullname": "Ali Behrouz",
        "user": "AliBehrouz",
        "type": "user"
      },
      "summary": "高い設計효율성과 효과적인 기술적인 기반 아키텍처는 기초 모델의 성능을 향상시키기 위한 연구의 핵심이었다. 인간이 특정 이벤트나 자극에 대한 注意偏差를 반영하여, Transformers, Titans, 그리고 현대의 선형 재귀 신경망의 재개념을 통해, 내부 객체로 지식 편향을 사용하는 연관 기억 모듈을 구축했다. 놀라울 정도로, 현재의 많은 시퀀스 모델은 (1) dot-product 유사성, 또는 (2) L2 회귀 객체를 지식 편향으로 활용하는 것을 발견했다. 이러한 객체를 초과하여, 지식 편향의 선택지와 효과적인 근사치를 제공하며, 훈련 프로세스의 안정화를 도모했다. 그리고 현대의 심층 학습 아키텍처에서의 잊기 구조를 리텐션 정규화로 재해석하고, 시퀀스 모델에 새로운 잊기 게이트를 제공했다. 이러한 관점에서, Miras라는 일반적인 프레임워크를 제안하고, (i) 연관 기억 아키텍처, (ii) 지식 편향 객체, (iii) 리텐션 게이트, (iv) 기억 학습 알고리즘의 4가지 선택지를 기반으로 심층 학습 아키텍처를 설계했다. Moneta, Yaad, Memora라는 3가지 새로운 시퀀스 모델을 제안하며, 현재의 선형 RNN의 능력을 초과하면서, 고속의 병렬 가능한 훈련 프로세스를 유지했다. 실험은 Miras의 다른 설계 선택에 따라, 강도가 다른 모델을 얻을 수 있음을 보여주며, 예를 들어, 언어 모델링, 일반적인 추론, 그리고 기억 강조 작업 등 특별한 작업에서 Transformers나 현대의 선형 재귀 모델을 초과하는 특별한 성능을 나타내는 것을 보여줬다.",
      "upvotes": 3,
      "discussionId": "6805c4dbb15a57fcb59b6f3d",
      "ai_keywords": [
        "Transformers",
        "Titans",
        "linear recurrent neural networks",
        "associative memory modules",
        "attentional bias",
        "dot-product similarity",
        "L2 regression",
        "retention regularization",
        "forget gates",
        "Miras",
        "Moneta",
        "Yaad",
        "Memora",
        "parallelizable training process",
        "language modeling",
        "commonsense reasoning",
        "recall intensive tasks"
      ]
    },
    "publishedAt": "2025-04-17T13:59:33.000Z",
    "title": "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization",
    "summary": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13173.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cccd5134a5d74cbaa9446c",
      "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
      "fullname": "Ali Behrouz",
      "name": "AliBehrouz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]