[
  {
    "paper": {
      "id": "2506.15675",
      "authors": [
        {
          "_id": "6853946599bf39f9665c79e0",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e1",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e2",
          "name": "Xiaofeng Mao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e3",
          "name": "Shaoheng Lin",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e4",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e5",
          "name": "Shitian Zhao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e6",
          "name": "Zhaopan Xu",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e7",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e8",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e9",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ea",
          "name": "Zizhen Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79eb",
          "name": "Fanrui Zhang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ec",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ed",
          "name": "Zhixiang Wang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ee",
          "name": "Yuwei Wu",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ef",
          "name": "Tong He",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f0",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f1",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f2",
          "name": "Yunde Jia",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f3",
          "user": {
            "_id": "63527f4e7d071f23d085ad45",
            "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
            "isPro": false,
            "fullname": "KAIPENG ZHANG",
            "user": "kpzhang",
            "type": "user"
          },
          "name": "Kaipeng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-19T10:09:38.100Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/GFrjhPvZnILgeTd_w2kvc.mp4"
      ],
      "publishedAt": "2025-06-18T17:57:06.000Z",
      "submittedOnDailyAt": "2025-06-19T03:16:13.113Z",
      "title": "세계: 세계탐사 목적의 비디오 데이터셋",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "비디오 생성 기술은 놀라운 발전을 거둔 것으로, 상호작용 세계 탐험의 기초가 되어야 합니다. 그러나 현재의 비디오 생성 데이터셋은 세계 탐험 훈련에 적합하지 않습니다. 이들은 제한된 장소, 짧은 시간, 정적 장면, 탐험 및 세계에 대한 설명의 부족 등 제한을 보여줍니다. 본 논문에서는, 「세계」라는 의미의 「Sekai」라는 고품질의 첫人称 관점의 세계 비디오 데이터셋을 소개합니다. 이는 100개 이상의 국가 및 지역으로부터 750개 이상의 도시에서 걷기 및 드라이너의 관점(FPV와 UVA)의 비디오를 5,000시간 이상으로 수집하여, 풍부한 세계 탐험에 대한 설명을 포함하는 것입니다. 비디오의 수집, 전처리, 설명에 의한 어노테이션에 효율적이고 효과적인 도구 박스를 개발했습니다. 데이터셋의 품질을 보여주는 실험은, 일부를 사용하여 상호작용 비디오 세계 탐험 모델을 「YUME」(「꿈」이라는 의미의 일본어)이라는 이름으로 훈련했습니다. Sekai은 비디오 생성 및 세계 탐험 분야에 도움이 되고, 가치 있는 애플리케이션을 촉진하는 것을 믿습니다.",
      "upvotes": 26,
      "discussionId": "6853946599bf39f9665c79f4",
      "projectPage": "https://lixsp11.github.io/sekai-project/",
      "githubRepo": "https://github.com/Lixsp11/sekai-codebase",
      "ai_summary": "Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.",
      "ai_keywords": [
        "first-person view",
        "worldwide video dataset",
        "rich annotations",
        "FPV",
        "UVA",
        "video collection",
        "pre-processing",
        "camera trajectories",
        "interactive video world exploration model"
      ]
    },
    "publishedAt": "2025-06-18T13:57:06.000Z",
    "title": "Sekai: A Video Dataset towards World Exploration",
    "summary": "Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/GFrjhPvZnILgeTd_w2kvc.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15675.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15681",
      "authors": [
        {
          "_id": "68536fc899bf39f9665c7961",
          "user": {
            "_id": "657152eb12f162153b50ec9d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
            "isPro": false,
            "fullname": "Byung-Kwan Lee",
            "user": "BK-Lee",
            "type": "user"
          },
          "name": "Byung-Kwan Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-19T09:10:25.415Z",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7962",
          "user": {
            "_id": "65b33e5f7cd0069ad648c4e8",
            "avatarUrl": "/avatars/1a746ea535cffa92ea08006e05ea414a.svg",
            "isPro": false,
            "fullname": "Ryo Hachiuma",
            "user": "rhachiuma",
            "type": "user"
          },
          "name": "Ryo Hachiuma",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-19T02:02:49.546Z",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7963",
          "name": "Yong Man Ro",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7964",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7965",
          "name": "Yueh-Hua Wu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/2xNUivkkqkJWJLWtIPNvb.mp4"
      ],
      "publishedAt": "2025-06-18T17:59:49.000Z",
      "submittedOnDailyAt": "2025-06-19T00:36:10.331Z",
      "title": "GenRecal: 크기에서 작은쪽으로 재조정 후의 생성\n비전-언어 모델",
      "submittedOnDailyBy": {
        "_id": "657152eb12f162153b50ec9d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
        "isPro": false,
        "fullname": "Byung-Kwan Lee",
        "user": "BK-Lee",
        "type": "user"
      },
      "summary": "최근의 시각 언어 모델(VLMs)의 발전은 GPT-4V와 같은 폐쇄 소스 시스템과 같은 성능을 달성하기 위해 대규모 언어 모델(LLMs)을 활용하고 있습니다. 그러나 이러한 모델이 실제 현실 모델에서 작동하는 것을 특히 자원 제한된 장치에서 작동하는 것을 실현하는 것은 대규모 계산 요구에 의해 어려워졌습니다. 이로 인해, 지식의 대규모 VLMs에서 작은 규모로 줄이고 효율적으로 하는 것이 흥미를 불러일으키는 것입니다. VLMs의 구조의 다양성이 여기서 중요한 문제로 나타납니다. 그것은 서로 다른 LLMs를 기반으로 구축되어 있으며, 단어 크기, 단어 분할, 단어 인덱스 순서가 다른 타겟 타입의 변화를 포함하는 것입니다. 특정 VLM 유형에 한정된 제한을 해결하기 위해, 우리는 Generation after Recalibration(GenRecal)을 소개합니다. GenRecal은, 서로 다른 VLMs 사이에서 특징 표현을 조정하고 적응시킬 리커레이터를 포함하는 새로운 일반적인 용도의 발열 프레임워크입니다. GenRecal은 많은 어려운 벤치마크에서 확산된 실험을 통해 기준 성능을 크게 향상시키고, 최종적으로, 대규모의 오픈 소스 및 폐쇄 소스의 VLMs을 초월하는 것을 보여주었습니다.",
      "upvotes": 13,
      "discussionId": "68536fc899bf39f9665c7966",
      "projectPage": "https://byungkwanlee.github.io/GenRecal-page/",
      "ai_summary": "GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.",
      "ai_keywords": [
        "vision-language models",
        "large language models",
        "distillation framework",
        "GenerRecal",
        "recalibration",
        "feature representations",
        "heterogeneous VLMs"
      ]
    },
    "publishedAt": "2025-06-18T13:59:49.000Z",
    "title": "GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models",
    "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/2xNUivkkqkJWJLWtIPNvb.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657152eb12f162153b50ec9d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
      "fullname": "Byung-Kwan Lee",
      "name": "BK-Lee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.15677",
      "authors": [
        {
          "_id": "68536b2399bf39f9665c794c",
          "name": "Yining Hong",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794d",
          "name": "Rui Sun",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794e",
          "name": "Bingxuan Li",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794f",
          "name": "Xingcheng Yao",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7950",
          "name": "Maxine Wu",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7951",
          "name": "Alexander Chien",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7952",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7953",
          "name": "Ying Nian Wu",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7954",
          "name": "Zhecan James Wang",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7955",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T17:58:17.000Z",
      "submittedOnDailyAt": "2025-06-19T00:14:22.240Z",
      "title": "물체와 디지털 영역을 연결하는 통합적 에이전트 지능",
      "submittedOnDailyBy": {
        "_id": "6431b64df76c34519e93d1ba",
        "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
        "isPro": true,
        "fullname": "Yining Hong",
        "user": "evelynhong",
        "type": "user"
      },
      "summary": "현재의 AI 에이전트는 주로 코드화되어 있습니다: 이는 온라인에서 얻을 수 있는 큰 디지털 정보와 지식을 검색하고 논리적으로 설명할 수 있는 능력; 또한 경험적인 인식, 계획, 행동을 통해 물리적 세계와 상호작용할 수 있는 능력이 있지만, 두 가지 모두를 수행할 수 있는 경우가 많지 않습니다. 이 분리는 물리적 및 디지털 지능을 통합하여 태스크를 해결하는 능력을 제한하고 있습니다. 예를 들어, 온라인 레시피를 기반으로 요리하는 것, 동적인 맵 데이터를 사용하여 노비게이션하는 것, 웹 데이터를 사용하여 리アル워ル의 지표를 해석하는 것 등입니다. 우리는 경험적인 지식과 홈셀러 규모의 이유를 자연스럽게 연결하는 새로운 패러다임에 도입합니다. 이 개념을 구현하기 위해, 우선, 현실적인 3D 실내와 외부 환경과 기능적인 홈셀러 인터페이스를 밀접하게 결합한 통합적인 시뮬레이션 플랫폼을 개발합니다. 이 플랫폼에 기반하여, 우리는 요리, 노비게이션, 구매, 관광, 위치 등의 다양한 태스크를 구축하고 릴리스합니다. 이러한 태스크는 물리적 및 디지털 영역을 통합한 이유를 필요로 하며, 가로지르는 영역의 지능의 체계적인 평가가 필요합니다. 실험 결과를 통해, 가장 선진한 AI 시스템과 인간 능력 사이에 큰 성능 차이를 보여주며, 경험적인 인식과 홈셀러 규모의 지식 접근의 교차점에서의 문제와 기회를 밝혀줍니다. 모든 데이터셋, 코드, 웹 사이트는 우리 프로젝트 페이지 (https://embodied-web-agent.github.io/)에서 공개되어 있습니다.",
      "upvotes": 7,
      "discussionId": "68536b2399bf39f9665c7956",
      "ai_summary": "Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.",
      "ai_keywords": [
        "Embodied Web Agents",
        "task environments",
        "simulation platform",
        "3D indoor and outdoor environments",
        "functional web interfaces",
        "Embodied Web Agents Benchmark",
        "systematic assessment",
        "cross-domain intelligence",
        "embodied cognition"
      ]
    },
    "publishedAt": "2025-06-18T13:58:17.000Z",
    "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence",
    "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15677.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6431b64df76c34519e93d1ba",
      "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
      "fullname": "Yining Hong",
      "name": "evelynhong",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15068",
      "authors": [
        {
          "_id": "68536bf399bf39f9665c7958",
          "name": "Zongxia Li",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c7959",
          "name": "Yapei Chang",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795a",
          "name": "Yuhang Zhou",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795b",
          "name": "Xiyang Wu",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795c",
          "name": "Zichao Liang",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795d",
          "name": "Yoo Yeon Sung",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795e",
          "name": "Jordan Lee Boyd-Graber",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T02:16:53.000Z",
      "submittedOnDailyAt": "2025-06-19T04:04:25.155Z",
      "title": "Semantics에 대한 관심과 보상의 설정, 그리고 자유형식의 개방적인 학습을 위한 R1 트레이닝에서의 대응",
      "submittedOnDailyBy": {
        "_id": "64ea62f918d79efd533c93fe",
        "avatarUrl": "/avatars/9985a789ce11b788de2cba12adfb72fc.svg",
        "isPro": false,
        "fullname": "Xiyang Wu",
        "user": "wuxiyang",
        "type": "user"
      },
      "summary": "開放終端의 긴 문장 생성 평가는 어렵습니다. 이는 좋은 출력과 나쁜 출력을 명확하게 구분하기 어렵기 때문입니다. 현재의 평가 방법은 일관성, 스타일, 관련성 등 주요 요소를 잘못 평가하고 있으며, 또한 학습 데이터에 따라 편향되어 있습니다. 이러한 이유로, 开放終端의 긴 문장 생성 평가는 조사 부족의 문제입니다. 이를 해결하기 위해, 우리는 PrefBERT를 제안합니다. PrefBERT는 GRPO에서 开放終端의 긴 문장 생성을 평가하는 점수 모델로, 좋은 출력과 나쁜 출력에 대해 다른 보상을 제공하여 학습을 가이드합니다. PrefBERT는 두 가지 다양한 긴 문장 스타일의 답변 평가 데이터 세트와 Likert 평가의 품질을 통해 훈련되었습니다. 이로 인해, PrefBERT는 로컬 또는 BERTScore보다 더 좋은 의미적인 보상 피드백을 제공하며, GRPO를 효과적으로 지원합니다. LLM-as-a-judge, 인간 평가, 그리고 질적 분석을 통해, PrefBERT는 여러 문장과 문장 길이에 따라 훈련되어 있으며, 긴 패스케이스에서도 안정적이고 GRPO에 필요한 확인 가능한 보상과 일치합니다. 인간 평가는 PrefBERT를 보상 신호로 사용하여 학습 정책 모델을 수행함으로써, 인간의 취향에 맞게 잘된 응답을 얻을 수 있음을 확인했습니다. 코드는 https://github.com/zli12321/long_form_rl에 공개되어 있습니다.",
      "upvotes": 7,
      "discussionId": "68536bf399bf39f9665c795f",
      "ai_summary": "PrefBERT, a scoring model, improves open-ended long-form generation by providing better semantic reward feedback than traditional metrics.",
      "ai_keywords": [
        "PrefBERT",
        "GRPO",
        "multi-sentence responses",
        "paragraph-length responses",
        "Likert-rated quality",
        "LLM-as-a-judge",
        "human ratings",
        "qualitative analysis",
        "verifiable rewards"
      ]
    },
    "publishedAt": "2025-06-17T22:16:53.000Z",
    "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation",
    "summary": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15068.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ea62f918d79efd533c93fe",
      "avatarUrl": "/avatars/9985a789ce11b788de2cba12adfb72fc.svg",
      "fullname": "Xiyang Wu",
      "name": "wuxiyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15569",
      "authors": [
        {
          "_id": "68537ca999bf39f9665c799a",
          "name": "Chengye Wang",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799b",
          "name": "Yifei Shen",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799c",
          "name": "Zexi Kuang",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799d",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799e",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun Zhao",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-19T09:10:23.732Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T15:43:26.000Z",
      "submittedOnDailyAt": "2025-06-19T01:28:40.934Z",
      "title": "SciVer: 과학 프로테스트의 다모달 구조 모델 평가",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "SciVer은, 과학적 컨텍스트에서 주장의 증명 능력을 평가하기 위해 설계된 첫 번째 벤치마크입니다. SciVer은 1,113 논문에 대한 3,000개의 전문가의 어노테이션을 포함하며, 과학적 주장의 증명에서 공통의 논리론의 4개의 서브셋을 포함하고 있습니다. 핸들링 평가에 도움이 될 수 있도록, 각 예시에 전문의 어노테이션된 보조 증거를 포함합니다. 21개의 가장 先端의 다 모델의 성능을 평가합니다. o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, Qwen2.5-VL을 포함합니다. SciVer에서 모델과 인간 전문가 사이에서는 큰 성능 간격을 볼 수 있습니다. 검색 어거시 생성(RAG)의 상세한 분석과 인간이 한한을 평가한 오류를 통해, 현재의 오픈 소스 모델의 중요한 한계를 특정하고, 다 모델 과학 논문 작업에서 모델의 이해와 논리론의 발전에 대한 핵심적인 힌트를 제공합니다.",
      "upvotes": 6,
      "discussionId": "68537ca999bf39f9665c799f",
      "githubRepo": "https://github.com/QDRhhhh/SciVer",
      "ai_summary": "A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.",
      "ai_keywords": [
        "retrieval-augmented generation (RAG)"
      ]
    },
    "publishedAt": "2025-06-18T11:43:26.000Z",
    "title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification",
    "summary": "We introduce SciVer, the first benchmark specifically designed to evaluate\nthe ability of foundation models to verify claims within a multimodal\nscientific context. SciVer consists of 3,000 expert-annotated examples over\n1,113 scientific papers, covering four subsets, each representing a common\nreasoning type in multimodal scientific claim verification. To enable\nfine-grained evaluation, each example includes expert-annotated supporting\nevidence. We assess the performance of 21 state-of-the-art multimodal\nfoundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and\nQwen2.5-VL. Our experiment reveals a substantial performance gap between these\nmodels and human experts on SciVer. Through an in-depth analysis of\nretrieval-augmented generation (RAG), and human-conducted error evaluations, we\nidentify critical limitations in current open-source models, offering key\ninsights to advance models' comprehension and reasoning in multimodal\nscientific literature tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15569.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.15050",
      "authors": [
        {
          "_id": "68539c6199bf39f9665c79f6",
          "name": "Tiantian Fan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f7",
          "name": "Lingjun Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f8",
          "name": "Yu Yue",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f9",
          "name": "Jiaze Chen",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fa",
          "name": "Chengyi Wang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fb",
          "name": "Qiying Yu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fc",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fd",
          "name": "Zhiqi Lin",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fe",
          "name": "Ruofei Zhu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79ff",
          "name": "Yufeng Yuan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a00",
          "name": "Xiaochen Zuo",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a01",
          "name": "Bole Ma",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a02",
          "name": "Mofan Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a03",
          "name": "Gaohong Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a04",
          "name": "Ru Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a05",
          "name": "Haotian Zhou",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a06",
          "name": "Cong Xie",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a07",
          "name": "Ruidong Zhu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a08",
          "name": "Zhi Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a09",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0a",
          "name": "Mingxuan Wang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0b",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0c",
          "name": "Yonghui Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T01:21:38.000Z",
      "submittedOnDailyAt": "2025-06-19T03:43:49.620Z",
      "title": "Truncated Proximal Policy Optimization\n\n(단어 그대로 번역)",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "최근, 검증 시 스케일링 라ー지 란그 란그 란 모델(LLMs)는 과학 및 전문적인 업무에서 긴 코ン시컷(CoT)를 생성하여 비상정의 논리 능력을 보여주고 있습니다. 이러한 논리 모델의 개발의 중요한 구성 요소 중 하나는, 근접 정책 최적화(PPO) 및 그의 변형체를 나타내는 강화 학습(RL)이 모델이 시도 오류로 학습하는 것을 허용하는 것입니다. 그러나 PPO는 고유의 on-policy 이므로 시간이 걸리기 때문에, 긴 답의 길이가 증가하여 더욱 악화되는 문제점이 있습니다. 본 논문에서는, 컷 Truncated Proximal Policy Optimization(T-PPO)를 제안합니다. T-PPO는 정책 업데이트와 길이 제한된 답의 생성을 스트리밍으로 수행하여, PPO의 새로운 확장으로 학습 효율성을 향상시키는 것입니다. T-PPO는 완전 동기화 된 긴 생성 프로세스의 고유한 단점인 낮은 하드웨어 사용율 문제를 완화합니다. 기여는 두 가지입니다. 첫째, 불완전한 답으로부터의 평가의 추정을 수행하는 Extended Generalized Advantage Estimation(EGAE)를 제안하여 정책 학습의 통합을 유지합니다. 둘째, 정책 모델과 가치 모델의 독립적인 최적화를 가능하게 하며, 계산적으로 최적화 된 구조를 설계합니다. 이 구조는 선택적 프롬프트와 토큰의 필터링을 통해冗长的 계산을 줄이고, 학습 프로세스를 가속화할 수 있는 동시에 수렴 성능을 희생하지 않습니다. AIME 2024에서의 실험 결과로부터, T-PPO는 논리 LLMs의 학습 효율성을 2.5배 정도 향상시키고, 현재의 컴퓨터에서 볼 수 없는 효과를 나타냅니다.",
      "upvotes": 4,
      "discussionId": "68539c6199bf39f9665c7a0d",
      "ai_summary": "T-PPO, an extension of PPO, improves training efficiency for Large Language Models by optimizing policy updates and utilizing hardware resources more effectively.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "chains-of-thought (CoT)",
        "reinforcement learning (RL)",
        "Proximal Policy Optimization (PPO)",
        "Truncated Proximal Policy Optimization (T-PPO)",
        "Extended Generalized Advantage Estimation (EGAE)",
        "advantage estimation",
        "policy and value models",
        "independent optimization",
        "prompt and truncated tokens",
        "AIME 2024",
        "base model"
      ]
    },
    "publishedAt": "2025-06-17T21:21:38.000Z",
    "title": "Truncated Proximal Policy Optimization",
    "summary": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated\nexceptional reasoning capabilities across scientific and professional tasks by\ngenerating long chains-of-thought (CoT). As a crucial component for developing\nthese reasoning models, reinforcement learning (RL), exemplified by Proximal\nPolicy Optimization (PPO) and its variants, allows models to learn through\ntrial and error. However, PPO can be time-consuming due to its inherent\non-policy nature, which is further exacerbated by increasing response lengths.\nIn this work, we propose Truncated Proximal Policy Optimization (T-PPO), a\nnovel extension to PPO that improves training efficiency by streamlining policy\nupdate and length-restricted response generation. T-PPO mitigates the issue of\nlow hardware utilization, an inherent drawback of fully synchronized\nlong-generation procedures, where resources often sit idle during the waiting\nperiods for complete rollouts. Our contributions are two-folds. First, we\npropose Extended Generalized Advantage Estimation (EGAE) for advantage\nestimation derived from incomplete responses while maintaining the integrity of\npolicy learning. Second, we devise a computationally optimized mechanism that\nallows for the independent optimization of the policy and value models. By\nselectively filtering prompt and truncated tokens, this mechanism reduces\nredundant computations and accelerates the training process without sacrificing\nconvergence performance. We demonstrate the effectiveness and efficacy of T-PPO\non AIME 2024 with a 32B base model. The experimental results show that T-PPO\nimproves the training efficiency of reasoning LLMs by up to 2.5x and\noutperforms its existing competitors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15050.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 88
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06279",
      "authors": [
        {
          "_id": "6850e0285e07650ecce890f3",
          "user": {
            "_id": "637f22d27119bd030dfd4af8",
            "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
            "isPro": false,
            "fullname": "Shi Liu",
            "user": "CLLBJ16",
            "type": "user"
          },
          "name": "Shi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-18T12:16:48.071Z",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f4",
          "user": {
            "_id": "63e4562f9db5da2dc1f3b520",
            "avatarUrl": "/avatars/f4eecf1396b05e1c72436e7026d85cef.svg",
            "isPro": false,
            "fullname": "Weijie Su",
            "user": "jackroos",
            "type": "user"
          },
          "name": "Weijie Su",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-19T09:10:33.557Z",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f5",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f6",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f7",
          "name": "Jifeng Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:59:06.000Z",
      "submittedOnDailyAt": "2025-06-19T00:24:25.743Z",
      "title": "CoMemo: LVLMs는 이미지 컨텍스트와 이미지 메모리를 필요로 한다.",
      "submittedOnDailyBy": {
        "_id": "637f22d27119bd030dfd4af8",
        "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
        "isPro": false,
        "fullname": "Shi Liu",
        "user": "CLLBJ16",
        "type": "user"
      },
      "summary": "최근의 대형 시각 언어 모델(LVLM)은 대규모 언어 모델(LLM)에 기반하여 발전하여 시각 특징과 LLM의 표현을 일치시키는 주요 패러다임에 입각하여 확립되었습니다. 그러나 이러한 LVLM은 최적의 다형 처리 구조를 갖지 않는 경향이 있습니다. 우선, LVLM은 두 개의 분포를示して 어텐션 할당에 대해 나타내고, 컨텍스트가 확장될수록 중간 시각 콘텐츠가 진행적으로 시각 정보에 무시될 수 있습니다. 또한 단순한 위치 인코딩 시나프스는 동적인 고해상도 이미지 처리에서 2차원 구조적 관계를 유지하는 데 실패합니다. 이러한 제한을 해결하기 위해, 우리는 CoMemo라는 이중 패스 구조를 제안합니다. 이는 컨텍스트 이미지 패스와 이미지 메모 패스를 결합한 것입니다. 시각 정보의 무시를 효과적으로 줄일 수 있습니다. 또한, 우리는 RoPE-DHR라는 새로운 위치 인코딩 기제를 도입합니다. 이는 얕은 이미지 기반의 위치 어그레시션을 사용하여 긴 시퀀스 동안의 거리적 감쇠를 억제하면서 2차원 공간 인식을 유지합니다. 7가지의 벤치마크(장대 컨텍스트 이해, 다장 이미지 추론, 시각 질문응답 테스트 등)에서 평가 결과, CoMemo는 전통적인 LVLM 구조에 비해 높은 성능을 나타냅니다. 프로젝트 페이지는 https://lalbj.github.io/projects/CoMemo/에서 이용할 수 있습니다.",
      "upvotes": 4,
      "discussionId": "6850e0295e07650ecce890f8",
      "projectPage": "https://lalbj.github.io/projects/CoMemo/",
      "githubRepo": "https://github.com/LALBJ/CoMemo",
      "ai_summary": "CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.",
      "ai_keywords": [
        "Large Vision-Language Models",
        "Large Language Models",
        "multimodal processing",
        "bimodal distribution",
        "attention allocation",
        "middle visual content",
        "positional encoding",
        "visual processing",
        "image Memory path",
        "RoPE-DHR",
        "positional aggregation",
        "2D structural relationships",
        "spatial awareness",
        "remote decay",
        "long-context comprehension",
        "multi-image reasoning",
        "visual question answering"
      ]
    },
    "publishedAt": "2025-06-06T13:59:06.000Z",
    "title": "CoMemo: LVLMs Need Image Context with Image Memory",
    "summary": "Recent advancements in Large Vision-Language Models built upon Large Language\nModels have established aligning visual features with LLM representations as\nthe dominant paradigm. However, inherited LLM architectural designs introduce\nsuboptimal characteristics for multimodal processing. First, LVLMs exhibit a\nbimodal distribution in attention allocation, leading to the progressive\nneglect of middle visual content as context expands. Second, conventional\npositional encoding schemes fail to preserve vital 2D structural relationships\nwhen processing dynamic high-resolution images. To address these limitations,\nwe propose CoMemo - a dual-path architecture that combines a Context image path\nwith an image Memory path for visual processing, effectively alleviating visual\ninformation neglect. Additionally, we introduce RoPE-DHR, a novel positional\nencoding mechanism that employs thumbnail-based positional aggregation to\nmaintain 2D spatial awareness while mitigating remote decay in extended\nsequences. Evaluations across seven benchmarks,including long-context\ncomprehension, multi-image reasoning, and visual question answering,\ndemonstrate CoMemo's superior performance compared to conventional LVLM\narchitectures. Project page is available at\nhttps://lalbj.github.io/projects/CoMemo/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f22d27119bd030dfd4af8",
      "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
      "fullname": "Shi Liu",
      "name": "CLLBJ16",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.15672",
      "authors": [
        {
          "_id": "6853bbcc99bf39f9665c7a50",
          "name": "Yao Zhang",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a51",
          "name": "Chenyang Lin",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a52",
          "name": "Shijie Tang",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a53",
          "name": "Haokun Chen",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a54",
          "name": "Shijie Zhou",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a55",
          "name": "Yunpu Ma",
          "hidden": false
        },
        {
          "_id": "6853bbcc99bf39f9665c7a56",
          "name": "Volker Tresp",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T17:54:55.000Z",
      "submittedOnDailyAt": "2025-06-19T06:26:51.897Z",
      "title": "세워움인텔리전스 기반의 완전 자동화 아게닉 시스템 생성에 대한向け\n\n(Note: The term \"向け\" in the original text seems to be a typographical error or an incomplete term. It might be intended to be \"의向\" which means \"direction\" or \"purpose\" in Korean. If this is the case, the correct translation would be \"세워움인텔리전스 기반의 완전 자동화 아게닉 시스템 생성의 방향\" or \"세워움인텔리전스 기반의 완전 자동화 아게닉 시스템 생성의 목적\".)",
      "submittedOnDailyBy": {
        "_id": "648cbea3dee03837c823cbf2",
        "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
        "isPro": false,
        "fullname": "Shuo Chen",
        "user": "ShuoChen99",
        "type": "user"
      },
      "summary": "대 언어 모델의 급속한 발전은 결정, 협조, 태스크 실행에 대한 아웃풋 시스템의 발전을 촉진하고 있습니다. 그러나 현재의 아웃풋 시스템의 생성 프레임워크는 완전한 자율성을 갖지 않으며, 스크래치부터 아웃풋의 생성, 자동 최적화의 아웃풋 기능, 협업의 부족으로 인해 적응성 및 scalability를 제한하고 있습니다. 우리는 SwarmAgentic 프레임워크를 제안하여 완전한 자동화된 아웃풋 시스템의 생성을 실현하고자 합니다. 이 프레임워크는 언어를 주도한 탐색을 통해 스크래치부터 아웃풋 시스템의 구축을 수행하고, 아웃풋 기능과 협업을 상호 의존하는 구성 요소로 공동 최적화를 수행합니다. 시스템 수준 구조의 효율적인 탐색을 가능하게 하기 위해, SwarmAgentic은 후보 시스템의 집단을 유지하고, 피드백 유도된 업데이트로 진화시키고, Particle Swarm Optimization(PSO)의 영감을 받습니다. 우리의 방법은 높은 수준의 계획, 시스템 수준의 협조, 창의적인 이유에 대한 6개의 현실적인, 개방적, 탐색적인 태스크에 대해 평가됩니다. 태스크의 설명과 목적 함수만 제공하면, SwarmAgentic은 모든 baseline을 초과하고, TravelPlanner 벤치마크에서 ADAS에 대한 +261.8%의 상대적인 개선을 달성하며, 구조적으로 제한되지 않은 태스크에서 완전한 자동화의 효과를 강조합니다. 이 프레임워크는 교환성과 자율성을 갖춘 아웃풋 시스템의 설계에 대한 중요한 단계를 보여주며, 집단 지식과 시스템 다 아웃풋 생성을 연결합니다. 우리의 코드는 https://yaoz720.github.io/SwarmAgentic/ 에서 공개되어 있습니다.",
      "upvotes": 2,
      "discussionId": "6853bbcc99bf39f9665c7a57",
      "ai_summary": "SwarmAgentic is a framework for automated agentic system generation that optimize agent functionality and collaboration through language-driven exploration, outperforming existing baselines in unconstrained tasks.",
      "ai_keywords": [
        "Large Language Models",
        "agentic systems",
        "from-scratch agent generation",
        "self-optimizing agent functionality",
        "collaboration",
        "Particle Swarm Optimization (PSO)",
        "TravelPlanner benchmark",
        "system-level coordination",
        "creative reasoning"
      ]
    },
    "publishedAt": "2025-06-18T13:54:55.000Z",
    "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via\n  Swarm Intelligence",
    "summary": "The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648cbea3dee03837c823cbf2",
      "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
      "fullname": "Shuo Chen",
      "name": "ShuoChen99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14435",
      "authors": [
        {
          "_id": "68537b2a99bf39f9665c7990",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7991",
          "name": "Jiayu Xu",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7992",
          "name": "Ruiping Wang",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7993",
          "name": "Yan Feng",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7994",
          "name": "Yitao Zhai",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7995",
          "name": "Peng Pei",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7996",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7997",
          "name": "Xilin Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T11:53:49.000Z",
      "submittedOnDailyAt": "2025-06-19T01:22:11.793Z",
      "title": "MoTE: 메모리 효과적인 대규모 다모달 모델을 위한 삼원식의 익스퍼트 혼합\n\n(Note: The translation is provided as requested, maintaining professionalism and accuracy. The original text appears to be a technical term, possibly related to machine learning or artificial intelligence, and the translation is intended to convey the same meaning in Korean.)",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "대규모 다모달 혼합 익스퍼트(MoEs)는 모델 크기를 늘리며 성능을 향상시키기 위해 활성 파라미터를 고정하고 있습니다. 그러나 이전의 연구에서는, 스パ어스 업사이클링의 때에 주로 전체 정밀도의 익스퍼트를 사용했습니다. 이들은 종결 태스크에서 높은 성능을 보여주지만, 많은 익스퍼트를 메모리 피트프린트에 높게 시키고, 에지 디바이스의 도입에 큰 문제점을 초래합니다. 본 논문에서는, MoTE(Ternary Experts의 혼합)를 제안합니다. 이것은 밀도 체크포인트에서 학습하는 데 사용할 수 있는 scalable 및 메모리 효과적인 접근입니다. 고 정밀도 익스퍼트를 학습하는 것이 아니라, 저 정밀도 익스퍼트를 늘리고 스パ어스 업사이클링을 수행합니다. 특히, 사전 훈련된 FFN을 공유 익스퍼트으로, 파라미터가 {-1, 0, 1}의 세 값 루트 익스퍼트를 학습합니다. 확장된 실험은, 우리의 접근 방식이 모델 크기에 따라 원하는 스케일링 테ン드를 보여줍니다. MoTE는, 전체 정밀도 기반 baseline MoE-LLaVA와 비교하여相当의 성능을 보여주고, 더 작은 메모리 피트프린트를 제공합니다. 또한, 우리의 접근 방식은 후 학습 축소 방법과 호환하며, 메모리 제약이 낮아질 때 우위를 더 크게 합니다. 같은 익스퍼트 메모리 피트프린트(3.4GB)를 유지하면서, 후 학습 축소와 결합하여, MoTE는 종결 태스크의 평균 정확도를 4.3% 증가시켰으며, 메모리 제약된 디바이스의 효과를 보여주고 있습니다.",
      "upvotes": 2,
      "discussionId": "68537b2b99bf39f9665c7998",
      "ai_summary": "MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "MoEs",
        "sparse up-cycling",
        "low-precision",
        "ternary experts",
        "shared expert",
        "FFN",
        "pre-trained",
        "post-training quantization",
        "memory-constrained",
        "end tasks"
      ]
    },
    "publishedAt": "2025-06-17T07:53:49.000Z",
    "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models",
    "summary": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size\nto boost performance while maintaining fixed active parameters. However,\nprevious works primarily utilized full-precision experts during sparse\nup-cycling. Despite they show superior performance on end tasks, the large\namount of experts introduces higher memory footprint, which poses significant\nchallenges for the deployment on edge devices. In this work, we propose MoTE, a\nscalable and memory-efficient approach to train Mixture-of-Ternary-Experts\nmodels from dense checkpoint. Instead of training fewer high-precision experts,\nwe propose to train more low-precision experts during up-cycling. Specifically,\nwe use the pre-trained FFN as a shared expert and train ternary routed experts\nwith parameters in {-1, 0, 1}. Extensive experiments show that our approach has\npromising scaling trend along model size. MoTE achieves comparable performance\nto full-precision baseline MoE-LLaVA while offering lower memory footprint.\nFurthermore, our approach is compatible with post-training quantization methods\nand the advantage further amplifies when memory-constraint goes lower. Given\nthe same amount of expert memory footprint of 3.4GB and combined with\npost-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3%\naverage accuracy on end tasks, demonstrating its effectiveness and potential\nfor memory-constrained devices.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14866",
      "authors": [
        {
          "_id": "6853db4199bf39f9665c7ae5",
          "name": "Thomas Kuntz",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae6",
          "name": "Agatha Duzan",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae7",
          "name": "Hao Zhao",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae8",
          "name": "Francesco Croce",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7ae9",
          "name": "Zico Kolter",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7aea",
          "name": "Nicolas Flammarion",
          "hidden": false
        },
        {
          "_id": "6853db4199bf39f9665c7aeb",
          "name": "Maksym Andriushchenko",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T17:59:31.000Z",
      "submittedOnDailyAt": "2025-06-19T08:15:06.478Z",
      "title": "OS-Harm: 컴퓨터 사용 에이전트의 보안성을 측정하기 위한 벤치마크",
      "submittedOnDailyBy": {
        "_id": "64c225f0129617dbaba5ae88",
        "avatarUrl": "/avatars/b12db433cd6b37c8e5299e575bdf98f9.svg",
        "isPro": false,
        "fullname": "Maksym Andriushchenko",
        "user": "MaksymAndriushchenko",
        "type": "user"
      },
      "summary": "컴퓨터 사용 에이전트는 스크린샷이나 엑세스 가능 트리를 처리하여 그래픽 사용자 인터페이스와 직접적으로 상호작용하는 LLM 기반의 에이전트입니다. 이러한 시스템은 확산을 시작하며 있지만, 그 보안성은 크게 뺏겨져 있으며, 유해한 행동의 가능성에 대한 평가가 중요합니다. 이를 위해 OS-Harm라는 새로운 벤치마크를 소개합니다. OS-Harm은 OSWorld 환경 위에 구축되어 있으며, 의도적인 사용자의 부정적 사용, 프로ン프트 인젝션 공격, 모델의 부정적 행동을 포함하는 3가지의 헝리 카테고리로 모델을 검증합니다. 이를 위해 150개의 태스크를 만들고, 여러 사이트 에이전트와의 상호작용을 요구합니다 (메일 클라이언트, 코드 편집기, 브라우저 등). 또한 정확성과 보안성을 평가하기 위해 자동 체크박스를 제안하고, 인간 Annotation과 높은 일치율(F1 스코어 0.76과 0.79)을 달성합니다. OS-Harm은 o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro 등 선진 모델을 기반으로 컴퓨터 사용 에이전트의 보안성을 평가하며, 특히 모든 모델은 의도적인 부정적 사용에 대한 질문에 직접적으로 대응하며, 동적 프로ン프트 인젝션에 상대적으로 취약하고, 때로는 불안정한 행동을 합니다. OS-Harm 벤치마크는 https://github.com/tml-epfl/os-harm에서 사용 가능합니다.",
      "upvotes": 1,
      "discussionId": "6853db4199bf39f9665c7aec",
      "githubRepo": "https://github.com/tml-epfl/os-harm",
      "ai_summary": "A new benchmark called OS-Harm measures the safety of computer use agents interacting with GUIs, evaluating their susceptibility to misuse, prompt injection attacks, and misbehavior across various safety violations and applications.",
      "ai_keywords": [
        "LLM-based agents",
        "OS-Harm",
        "OSWorld environment",
        "deliberate user misuse",
        "prompt injection attacks",
        "model misbehavior",
        "harassment",
        "copyright infringement",
        "disinformation",
        "data exfiltration",
        "automated judge",
        "F1 score",
        "GUI"
      ]
    },
    "publishedAt": "2025-06-17T13:59:31.000Z",
    "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents",
    "summary": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14866.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c225f0129617dbaba5ae88",
      "avatarUrl": "/avatars/b12db433cd6b37c8e5299e575bdf98f9.svg",
      "fullname": "Maksym Andriushchenko",
      "name": "MaksymAndriushchenko",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14824",
      "authors": [
        {
          "_id": "6853c3af99bf39f9665c7a89",
          "name": "Yao Zhang",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8a",
          "name": "Hewei Gao",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8b",
          "name": "Haokun Chen",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8c",
          "name": "Weiguo Li",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8d",
          "name": "Yunpu Ma",
          "hidden": false
        },
        {
          "_id": "6853c3af99bf39f9665c7a8e",
          "name": "Volker Tresp",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:50:50.000Z",
      "submittedOnDailyAt": "2025-06-19T06:31:06.085Z",
      "title": "FedNano: Lightweight Federated Learning for Pre-trained Models - Advances in Progress",
      "submittedOnDailyBy": {
        "_id": "648cbea3dee03837c823cbf2",
        "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
        "isPro": false,
        "fullname": "Shuo Chen",
        "user": "ShuoChen99",
        "type": "user"
      },
      "summary": "다모뎔 모델 대 언어 모델(MLLMs)은 다모뎔 모델 추론과 크로스 모델 검색 등 복잡한 작업에서 뛰어난 성능을 보입니다が, 실제 환경에서 기능에 있어서는 분산된 다모뎔 데이터와 엄격한 프라이버시 요구로 인한 기능 장애가 있습니다. Federated Learning(FL)은 데이터 중앙화 없이 모델의 공동 학습을 가능하게 하여 해결책을 제공하지만, MLLM의 FL 구현에는 높은 계산 요구, 클라이언트 용량의 한계, 큰 통신 비용, 그리고 헤티노믹 클라이언트 데이터로 인한 문제가 있습니다. 현재의 FL 방법들은 클라이언트 측에서 Full Model의 기계 학습을 수행하는 가정으로 되어 있으며, MLLM의 크기와 통신 요구로 이러한 가정이 파괴되어 있습니다. 이러한 제한을 해결하기 위해, FedNano라는 첫 번째 FL 프레임워크를 제안하고 있습니다. FedNano는 서버 측에서 LLM을 중앙화하고, 클라이언트 측의 특정 적응을 위한 NanoEdge라는 가벼운 모듈을 도입합니다. NanoEdge는 모델 고유의 인코더, 연결자, 그리고 저랭크 적응터를 사용하여, LLM의 클라이언트 측 기계 학습의 필요성을 제거하고, 클라이언트 측의 저장을 95% 줄이고, 모델 파라미터의 통신 비용을 0.01% 이하로 억제할 수 있습니다. NanoAdapter만 전송하는 방식으로, FedNano는 헤티노믹 클라이언트 데이터와 자원의 한계에 대한 처리, 프라이버시를 유지하면서, MLLM 크기와 FL의 가능성 사이의 간격을 채우고, 교환 가능한 분산된 다모뎔 AI 시스템의 가능성을 제공합니다. 실험은 FedNano가 기존의 FL 기반 라인 shot을 초과하고, MLLM 크기와 FL의 가능성 사이의 간격을 채우고, 교환 가능한 분산된 다모뎔 AI 시스템의 가능성을 제공합니다.",
      "upvotes": 1,
      "discussionId": "6853c3af99bf39f9665c7a8f",
      "ai_summary": "FedNano is a federated learning framework that centralizes large language models on servers and uses NanoEdge modules for client-specific adaptation, addressing scalability and privacy issues.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Federated Learning",
        "NanoEdge",
        "modality-specific encoders",
        "connectors",
        "NanoAdapters",
        "low-rank adaptation",
        "client-specific adaptation",
        "compact NanoAdapter updates",
        "decentralized multimodal AI systems"
      ]
    },
    "publishedAt": "2025-06-12T13:50:50.000Z",
    "title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal\n  Large Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal\nreasoning and cross-modal retrieval but face deployment challenges in\nreal-world scenarios due to distributed multimodal data and strict privacy\nrequirements. Federated Learning (FL) offers a solution by enabling\ncollaborative model training without centralizing data. However, realizing FL\nfor MLLMs presents significant challenges, including high computational\ndemands, limited client capacity, substantial communication costs, and\nheterogeneous client data. Existing FL methods assume client-side deployment of\nfull models, an assumption that breaks down for large-scale MLLMs due to their\nmassive size and communication demands. To address these limitations, we\npropose FedNano, the first FL framework that centralizes the LLM on the server\nwhile introducing NanoEdge, a lightweight module for client-specific\nadaptation. NanoEdge employs modality-specific encoders, connectors, and\ntrainable NanoAdapters with low-rank adaptation. This design eliminates the\nneed to deploy LLM on clients, reducing client-side storage by 95%, and\nlimiting communication overhead to only 0.01% of the model parameters. By\ntransmitting only compact NanoAdapter updates, FedNano handles heterogeneous\nclient data and resource constraints while preserving privacy. Experiments\ndemonstrate that FedNano outperforms prior FL baselines, bridging the gap\nbetween MLLM scale and FL feasibility, and enabling scalable, decentralized\nmultimodal AI systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14824.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648cbea3dee03837c823cbf2",
      "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
      "fullname": "Shuo Chen",
      "name": "ShuoChen99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]