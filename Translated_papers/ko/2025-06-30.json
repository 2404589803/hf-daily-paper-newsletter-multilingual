[
  {
    "paper": {
      "id": "2506.17450",
      "authors": [
        {
          "_id": "68620adf9e7509383d29ab98",
          "user": {
            "_id": "655bca95360e4f90cb61ba83",
            "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
            "isPro": true,
            "fullname": "Jiacheng Chen",
            "user": "cccjc",
            "type": "user"
          },
          "name": "Jiacheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-30T06:22:01.226Z",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab99",
          "name": "Ramin Mehran",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9a",
          "name": "Xuhui Jia",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9b",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9c",
          "name": "Sanghyun Woo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T19:38:34.000Z",
      "submittedOnDailyAt": "2025-06-30T02:33:26.106Z",
      "title": "BlenderFusion: 3D 기반의 시각화 편집 및 생성적 합성",
      "submittedOnDailyBy": {
        "_id": "655bca95360e4f90cb61ba83",
        "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
        "isPro": true,
        "fullname": "Jiacheng Chen",
        "user": "cccjc",
        "type": "user"
      },
      "summary": "BlenderFusion는 물체, 카메라, 배경을 재조합하여 새로운 셈을 합성하는 생성적인 시각적 합성 프레임워크입니다. 이 프로세스는 (i) 시각적 입력을 시각적 3D 엔티티로 변환하는 (레이어 처리), (ii) Blender에서 3D 기반의 제어로 편집하는 (편집), (iii) 생성적 합성기를 사용하여 일관된 셈을 생성하는 (합성)으로 구성되어 있습니다. 우리의 생성적 합성기는 pretrained diffusion 모델을 확장하여 원본 (소스)과 편집된 (타겟) 셈을 병렬로 처리할 수 있습니다. 이는 (i) 소스 마스크, (ii) 시뮬레이션된 물체 젝을 사용한 두 가지 주요 학습 전략으로 미세 조정되어 있습니다. BlenderFusion는 복잡한 합성적 셈 편집 태스크에서 기존 방법보다 크게 뛰어납니다.",
      "upvotes": 28,
      "discussionId": "68620adf9e7509383d29ab9d",
      "projectPage": "https://blenderfusion.github.io/",
      "ai_summary": "A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.",
      "ai_keywords": [
        "diffusion model",
        "source masking",
        "simulated object jittering"
      ]
    },
    "publishedAt": "2025-06-20T15:38:34.000Z",
    "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing",
    "summary": "We present BlenderFusion, a generative visual compositing framework that\nsynthesizes new scenes by recomposing objects, camera, and background. It\nfollows a layering-editing-compositing pipeline: (i) segmenting and converting\nvisual inputs into editable 3D entities (layering), (ii) editing them in\nBlender with 3D-grounded control (editing), and (iii) fusing them into a\ncoherent scene using a generative compositor (compositing). Our generative\ncompositor extends a pre-trained diffusion model to process both the original\n(source) and edited (target) scenes in parallel. It is fine-tuned on video\nframes with two key training strategies: (i) source masking, enabling flexible\nmodifications like background replacement; (ii) simulated object jittering,\nfacilitating disentangled control over objects and camera. BlenderFusion\nsignificantly outperforms prior methods in complex compositional scene editing\ntasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17450.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655bca95360e4f90cb61ba83",
      "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
      "fullname": "Jiacheng Chen",
      "name": "cccjc",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.21862",
      "authors": [
        {
          "_id": "6861eea79e7509383d29ab2f",
          "name": "Boyuan Sun",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab30",
          "name": "Jiaxing Zhao",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab31",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab32",
          "name": "Qibin Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T02:29:58.000Z",
      "submittedOnDailyAt": "2025-06-30T00:31:12.107Z",
      "title": "LLaVA-Scissor: 토큰 압축에서 의미적인 연결 요소를 활용한\n  비디오 LLM",
      "submittedOnDailyBy": {
        "_id": "6686044047f2a33570e59e31",
        "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
        "isPro": false,
        "fullname": "Jiaxing Zhao",
        "user": "StarJiaxing",
        "type": "user"
      },
      "summary": "이 논문에서는 비디오 다 타입 대 언어 모델에 적합한 토큰 압축 전략 LLaVA-Scissor를 제안합니다. 기존의 방법은 주로 어텐션 스코어에 기반하여 토큰을 압축하지만, 모든 семанти적 영역을 효과적으로捉え지 못하였으며, 토큰의冗長성이 발생했습니다. 또한, семанти적 연결 요소(SCC) 접근을 활용하여 토큰 세트 내의 다른 семанти적 영역에 토큰을 할당하여 전체적인 семанти적인 커버를 보장합니다. 결과적으로, 스펙트럼과 시간적인 영역에서 SCC를 활용하는 2단계의 스펙트럴・타임스펙트럴 토큰 압축 전략이 실현되었습니다. 이 전략은 전체적인 비디오를 표현하기 위해 중복되지 않는 семанти적 토큰의 세트로 토큰을 효과적으로 압축할 수 있습니다. LLaVA-Scissor의 토큰 압축 능력을 평가하기 위해 비디오 질문응답, 긴 비디오 이해, 세부적인 다 선택 벤치마크를 포함하는 다양한 비디오 이해 벤치마크를 통해 검증되었습니다. 실험 결과를 통해 제안된 LLaVA-Scissor는 다른 토큰 압축 방법보다 뛰어난 성능을 보였으며, 낮은 토큰 유지 비율에서도 다양한 비디오 이해 벤치마크에서 상위 성능을 발휘했습니다. 프로젝트 페이지: https://github.com/HumanMLLM/LLaVA-Scissor.",
      "upvotes": 26,
      "discussionId": "6861eea89e7509383d29ab33",
      "ai_summary": "LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.",
      "ai_keywords": [
        "token compression strategy",
        "Semantic Connected Components (SCC)",
        "spatio-temporal token compression strategy",
        "video question answering",
        "long video understanding",
        "comprehensive multi-choice benchmarks"
      ]
    },
    "publishedAt": "2025-06-26T22:29:58.000Z",
    "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
    "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21862.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6686044047f2a33570e59e31",
      "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
      "fullname": "Jiaxing Zhao",
      "name": "StarJiaxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21416",
      "authors": [
        {
          "_id": "685e084071131fa43be08acc",
          "user": {
            "_id": "6361dd166945df7441b893fa",
            "avatarUrl": "/avatars/b3ae6888a41aab8c2a7ef9f7320565c4.svg",
            "isPro": false,
            "fullname": "Bowen Chen ",
            "user": "chenbowen",
            "type": "user"
          },
          "name": "Bowen Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-30T06:22:45.351Z",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08acd",
          "name": "Mengyi Zhao",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ace",
          "name": "Haomiao Sun",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08acf",
          "name": "Li Chen",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad0",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad1",
          "name": "Kang Du",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad2",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T16:04:16.000Z",
      "submittedOnDailyAt": "2025-06-30T04:43:14.606Z",
      "title": "XVerse: 일관된 다변수 제어를 수행하는 DiT 조정 - 식별 및 세ман틱 속성 일치\n\n(Note: The translation provided above is a direct translation of the given text. If \"XVerse\" is a specific term or brand name, it may be more appropriate to keep it in its original form or provide a more contextually accurate translation if available. However, without additional context, the direct translation is provided as requested.)",
      "submittedOnDailyBy": {
        "_id": "6498038ece9190ebb8693034",
        "avatarUrl": "/avatars/06ec2457932e05572d917ba286cdef25.svg",
        "isPro": false,
        "fullname": "Zhao",
        "user": "Mengyi",
        "type": "user"
      },
      "summary": "Achieving fine-grained control over subject identity and semantic attributes (pose, style, lighting) in text-to-image generation, particularly for multiple subjects, often undermines the editability and coherence of Diffusion Transformers (DiTs). Many approaches introduce artifacts or suffer from attribute entanglement. To overcome these challenges, we propose a novel multi-subject controlled generation model XVerse. By transforming reference images into offsets for token-specific text-stream modulation, XVerse allows for precise and independent control for specific subjects without disrupting image latents or features. Consequently, XVerse offers high-fidelity, editable multi-subject image synthesis with robust control over individual subject characteristics and semantic attributes. This advancement significantly improves personalized and complex scene generation capabilities.",
      "upvotes": 18,
      "discussionId": "685e084071131fa43be08ad3",
      "projectPage": "https://bytedance.github.io/XVerse/",
      "githubRepo": "https://github.com/bytedance/XVerse",
      "ai_summary": "XVerse enhances text-to-image generation by enabling precise and independent control over multiple subjects using token-specific text-stream modulation, improving image coherence and fidelity.",
      "ai_keywords": [
        "Diffusion Transformers",
        "DiTs",
        "text-to-image generation",
        "multi-subject controlled generation",
        "reference images",
        "token-specific text-stream modulation",
        "image latents",
        "multi-subject image synthesis",
        "semantic attributes"
      ],
      "githubStars": 68
    },
    "publishedAt": "2025-06-26T12:04:16.000Z",
    "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT Modulation",
    "summary": "Achieving fine-grained control over subject identity and semantic attributes\n(pose, style, lighting) in text-to-image generation, particularly for multiple\nsubjects, often undermines the editability and coherence of Diffusion\nTransformers (DiTs). Many approaches introduce artifacts or suffer from\nattribute entanglement. To overcome these challenges, we propose a novel\nmulti-subject controlled generation model XVerse. By transforming reference\nimages into offsets for token-specific text-stream modulation, XVerse allows\nfor precise and independent control for specific subject without disrupting\nimage latents or features. Consequently, XVerse offers high-fidelity, editable\nmulti-subject image synthesis with robust control over individual subject\ncharacteristics and semantic attributes. This advancement significantly\nimproves personalized and complex scene generation capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21416.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6498038ece9190ebb8693034",
      "avatarUrl": "/avatars/06ec2457932e05572d917ba286cdef25.svg",
      "fullname": "Zhao",
      "name": "Mengyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21356",
      "authors": [
        {
          "_id": "6861fb7a9e7509383d29ab4b",
          "name": "Hongbo Liu",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4c",
          "name": "Jingwen He",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4d",
          "name": "Yi Jin",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4e",
          "name": "Dian Zheng",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4f",
          "name": "Yuhao Dong",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab50",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab51",
          "name": "Ziqi Huang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab52",
          "name": "Yinan He",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab53",
          "name": "Yangguang Li",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab54",
          "name": "Weichao Chen",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab55",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab56",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab57",
          "name": "Shengjie Zhao",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab58",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T15:09:21.000Z",
      "submittedOnDailyAt": "2025-06-30T04:32:34.261Z",
      "title": "ShotBench: 영화적인 시각 언어 모델로, 전문가 수준의 시각 이해를 갖춘 모델입니다.",
      "submittedOnDailyBy": {
        "_id": "652965773a416e1f2173443b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
        "isPro": false,
        "fullname": "Yuhao Dong",
        "user": "THUdyh",
        "type": "user"
      },
      "summary": "映画의 기본적인 시각적 언어인 영화 기록 기술은 이야기의 전달, 감정, 미술적 질의의 전달에 필수적이다. 최근의 시각적 언어 모델(VLMs)은 광범위한 시각적 이해력을 보여주지만, 개별의샷 내의 微妙한 영화 기록 문법을 이해하는 데는 큰 조사가 없고 강한 평가가 부족하다. 이 중요한 결함이 미세한 시각적 이해와 AI에 의한 영상 생성의 정확도를 제한하고 있다. 이에 대처하여, 우리는 영화 기록 언어 이해에 특화된 상세한 벤치마크인 ShotBench를 소개한다. 이는 200편 이상의 평가된 영화(주要是 오스카 수상 후보 영화)에서 추출한 8개의 중요한 영화 기록의 차원을 통해, 3500개 이상의 전문가가 채점된 QA 쌍을 준비하고 있다. ShotBench에서 24개의 선진적인 VLMs를 평가함으로써, 그들이 가진 큰 한계가 명확히 드러났다: 가장 높은 성능을 보이는 모델도 평균 정답률이 60% 미만이고, 특히 미세한 시각적 코드와 복잡한 공간적인 논리론에 어려움을 겪는다. 이 분야의 발전을 촉진하기 위해, 우리는 약 70,000개 QA 쌍을 포함하는 대규모 다양한 데이터셋인 ShotQA를 구축한다. ShotQA를 활용하여, 우리는 감독적 조정과 그룹 상대적 정책 최적화를 통해 ShotVL을 개발한다. ShotVL은 모든 현재의 오픈 소스 및 소유권 모델을 초과하고, ShotBench에서 새로운 최선 성능을 설정한다. 우리는 모델, 데이터, 코드를 공개하여, AI에 의한 영화 기록의 이해와 생성의 중요한 분야의 빠른 발전을 촉진한다.",
      "upvotes": 15,
      "discussionId": "6861fb7a9e7509383d29ab59",
      "projectPage": "https://vchitect.github.io/ShotBench-project/",
      "githubRepo": "https://github.com/Vchitect/ShotBench/tree/main",
      "ai_summary": "ShotBench and ShotQA datasets, along with ShotVL model, enhance AI's understanding and generation capabilities by specifically targeting nuanced cinematic language comprehension.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "ShotBench",
        "QA pairs",
        "cinematic grammar",
        "fine-grained visual comprehension",
        "AI-assisted video generation",
        "ShotQA",
        "multimodal dataset",
        "supervised fine-tuning",
        "Group Relative Policy Optimization",
        "ShotVL",
        "AI-driven cinematic understanding",
        "state-of-the-art performance"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-06-26T11:09:21.000Z",
    "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language\n  Models",
    "summary": "Cinematography, the fundamental visual language of film, is essential for\nconveying narrative, emotion, and aesthetic quality. While recent\nVision-Language Models (VLMs) demonstrate strong general visual understanding,\ntheir proficiency in comprehending the nuanced cinematic grammar embedded\nwithin individual shots remains largely unexplored and lacks robust evaluation.\nThis critical gap limits both fine-grained visual comprehension and the\nprecision of AI-assisted video generation. To address this, we introduce\nShotBench, a comprehensive benchmark specifically designed for cinematic\nlanguage understanding. It features over 3.5k expert-annotated QA pairs from\nimages and video clips, meticulously curated from over 200 acclaimed\n(predominantly Oscar-nominated) films and spanning eight key cinematography\ndimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their\nsubstantial limitations: even the top-performing model achieves less than 60%\naverage accuracy, particularly struggling with fine-grained visual cues and\ncomplex spatial reasoning. To catalyze advancement in this domain, we construct\nShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic\nQA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning\nand Group Relative Policy Optimization. ShotVL significantly outperforms all\nexisting open-source and proprietary models on ShotBench, establishing new\nstate-of-the-art performance. We open-source our models, data, and code to\nfoster rapid progress in this crucial area of AI-driven cinematic understanding\nand generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21356.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652965773a416e1f2173443b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
      "fullname": "Yuhao Dong",
      "name": "THUdyh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20279",
      "authors": [
        {
          "_id": "686218679e7509383d29abb3",
          "name": "Changliang Xia",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb4",
          "name": "Chengyou Jia",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb5",
          "name": "Zhuohang Dang",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb6",
          "name": "Minnan Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T09:40:50.000Z",
      "submittedOnDailyAt": "2025-06-30T03:24:47.090Z",
      "title": "理想에서 현실로: 현실 세계의 스캐닝에 대한 데이터 효율적인 통일의 밀집 예측",
      "submittedOnDailyBy": {
        "_id": "6602548a68d519ed324b47c5",
        "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
        "isPro": false,
        "fullname": "ChengyouJia",
        "user": "ChengyouJia",
        "type": "user"
      },
      "summary": "Dense prediction タスク는 컴퓨터 비전 분야에서 중요한 역할을 수행하며, 입력 이미지의 각 픽셀에 대한 라벨링을 학습하는 것을 목표로 합니다. 이 분야에서 발전이 있었지만, 현재의 방법들은 ideale 조건에 초점을 맞추고, 현실적인 시나리오에 한정된 일반화 능력을 가지고 있으며, 실제 데이터의 부족에 직면하고 있습니다. 이러한 문제를 체계적으로 연구하기 위해, DenseWorld라는 벤치마크를 소개합니다. DenseWorld는 25가지 효과적인 실제적인 애플리케이션에 대응하는 광범위한 범위의 밀도 예측 タスク를 구성하고, 전체적인 통일 평가의 특징을 가지고 있습니다. 다음으로, DenseDiT라는 모델을 제안합니다. DenseDiT은 생성 모델의 시각적 선두를 최대한 활용하여, 한 가지 통일적인 전략으로 다양한 실제적인 밀도 예측 タスク를 수행하는 것을 목표로 합니다. DenseDiT은 파라미터 재사용 구조와 두 개의 가벼운 브랜치를 조합하여, 0.1% 미만의 추가 파라미터를 사용하여 작동합니다. DenseWorld에서 평가는 현재 일반적인 및 특화된 베이스라인의 성능 저하를 명확히 하고, 그들의 현실적인 일반화 능력을 한계를 보여줍니다. 반면, DenseDiT은 기준 데이터의 0.01% 미만의 사용으로 우수한 결과를 얻으며, 실제적인 기능의 실천적 가치를 강조합니다. 데이터, 체크포인트 및 코드는 https://xcltql666.github.io/DenseDiTProj에서 이용할 수 있습니다.",
      "upvotes": 13,
      "discussionId": "686218689e7509383d29abb7",
      "projectPage": "https://xcltql666.github.io/DenseDiTProj/",
      "githubRepo": "https://github.com/xcltql666/DenseDiT",
      "ai_summary": "DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.",
      "ai_keywords": [
        "dense prediction",
        "generative models",
        "visual priors",
        "parameter-reuse mechanism",
        "lightweight branches",
        "multi-scale context",
        "DenseWorld",
        "DenseDiT"
      ],
      "githubStars": 15
    },
    "publishedAt": "2025-06-25T05:40:50.000Z",
    "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios",
    "summary": "Dense prediction tasks hold significant importance of computer vision, aiming\nto learn pixel-wise annotated label for an input image. Despite advances in\nthis field, existing methods primarily focus on idealized conditions, with\nlimited generalization to real-world scenarios and facing the challenging\nscarcity of real-world data. To systematically study this problem, we first\nintroduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction\ntasks that correspond to urgent real-world applications, featuring unified\nevaluation across tasks. Then, we propose DenseDiT, which maximally exploits\ngenerative models' visual priors to perform diverse real-world dense prediction\ntasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism\nand two lightweight branches that adaptively integrate multi-scale context,\nworking with less than 0.1% additional parameters. Evaluations on DenseWorld\nreveal significant performance drops in existing general and specialized\nbaselines, highlighting their limited real-world generalization. In contrast,\nDenseDiT achieves superior results using less than 0.01% training data of\nbaselines, underscoring its practical value for real-world deployment. Our\ndata, and checkpoints and codes are available at\nhttps://xcltql666.github.io/DenseDiTProj",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602548a68d519ed324b47c5",
      "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
      "fullname": "ChengyouJia",
      "name": "ChengyouJia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22434",
      "authors": [
        {
          "_id": "686205ad9e7509383d29ab80",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab81",
          "name": "Mingkang Zhu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab82",
          "name": "Shaoteng Liu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab83",
          "name": "Xiaoyang Wu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab84",
          "name": "Xiaogang Xu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab85",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab86",
          "name": "Xiang Bai",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab87",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T17:59:27.000Z",
      "submittedOnDailyAt": "2025-06-30T02:04:48.511Z",
      "title": "복수 이미지 비교를 이용한 강화 시각 설명",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "이 연구는 여러 이미지 간의 Chain-of-Thought (CoT) 추론을 가능하게 하는 방법을 조사하고 있습니다. 직관적인 해결책으로는 Vision-Language Models (VLMs)에 적용할 수 있는 규칙 기반의 강화학습을 고려할 수 있습니다. 그러나 이러한 방법들은 일반적으로 자동적으로 준비된 질문-답변 쌍을 기반으로 하기 때문에, 이미지의 복잡한 시각적 세부 사항이나 이미지 간의 복잡한 논리 처리에 특히 어려움이 있습니다. 자동 조정된 시각적 표현 학습을 모델로 참고하여, 이미지가 내재적 제약을 가지고 있다고 발견했습니다. 이 지침에 따라 동일한 이미지의 두 가지 확장된 시각과 세 번째로 유사한 다른 이미지로 구성된 이미지 튜플을 구축했습니다. 학습 과정에서 모델은 이러한 이미지를 비교하기 위한 이유를 생성하도록 촉구합니다 (즉, 같은지 다른지 판단하는 것입니다). 그리고 규칙 기반의 강화학습을 사용하여 모델을 최적화합니다. 높은 시각적 유사성과 확장의 존재로, 모델은 미묘한 시각적 변화를 관심을 가지며 논리적인 논리를 수행해야 합니다. 실험은 시각적 비교 태스크에만 학습된 이유를 배우는 것을 보여주고, 이는 광범위한 질문에 광범위하게 일반화합니다. 인간 기록된 질문-답변 쌍에 의존하지 않도록, 이 방법은 여러 이미지의 논리를 처리하는 벤치마크에서 뚜렷한 개선을 달성하고, 일반적인 시각적 태스크에 강력한 성능을 보여주고 있습니다.",
      "upvotes": 8,
      "discussionId": "686205ad9e7509383d29ab88",
      "ai_summary": "Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.",
      "ai_keywords": [
        "Vision-Language Models",
        "self-supervised learning",
        "image triplets",
        "reasoning ability",
        "multi-image reasoning benchmarks",
        "general vision tasks"
      ]
    },
    "publishedAt": "2025-06-27T13:59:27.000Z",
    "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
    "summary": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual\ncues across multiple images. A straightforward solution is to adapt rule-based\nreinforcement learning for Vision-Language Models (VLMs). However, such methods\ntypically rely on manually curated question-answer pairs, which can be\nparticularly challenging when dealing with fine grained visual details and\ncomplex logic across images. Inspired by self-supervised visual representation\nlearning, we observe that images contain inherent constraints that can serve as\nsupervision. Based on this insight, we construct image triplets comprising two\naugmented views of the same image and a third, similar but distinct image.\nDuring training, the model is prompted to generate a reasoning process to\ncompare these images (i.e., determine same or different). Then we optimize the\nmodel with rule-based reinforcement learning. Due to the high visual similarity\nand the presence of augmentations, the model must attend to subtle visual\nchanges and perform logical reasoning to succeed. Experiments show that,\nalthough trained solely on visual comparison tasks, the learned reasoning\nability generalizes effectively to a wide range of questions. Without relying\non any human-annotated question-answer pairs, our method achieves significant\nimprovements on multi-image reasoning benchmarks and shows strong performance\non general vision tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22434.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21656",
      "authors": [
        {
          "_id": "6861f2b89e7509383d29ab35",
          "name": "Yifan Shen",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab36",
          "name": "Yuanzhe Liu",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab37",
          "name": "Jingyuan Zhu",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab38",
          "name": "Xu Cao",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab39",
          "name": "Xiaofeng Zhang",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3a",
          "name": "Yixiao He",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3b",
          "name": "Wenming Ye",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3c",
          "name": "James Matthew Rehg",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3d",
          "name": "Ismini Lourentzou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T18:00:00.000Z",
      "submittedOnDailyAt": "2025-06-30T00:44:37.025Z",
      "title": "Fine-Grained Preference Optimization은 VLMs의 공간 인식 능력을 향상시키는 데 사용됩니다.",
      "submittedOnDailyBy": {
        "_id": "65e387095132c2edd193ae49",
        "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
        "isPro": false,
        "fullname": "Yifan Shen",
        "user": "SivanSX",
        "type": "user"
      },
      "summary": "현재의 시각 언어 모델(VLMs)은 특히 여러 단계의 로직과 정확한 공간적 alignment가 필요할 때, 미세한 공간적 논리론을 다루는 것이 어려워진다. 본 연구에서는 이러한 한계를 해결하기 위해 SpatialReasoner-R1이라는 시각 언어 논리론 모델을 도입하고 있습니다. 고품질의 공간적 논리론의 규범을 구축하기 위해, 다 모델 모테카르로 트리 탐색(M3CTS) 메소드를 설계하고, 다양한, 로직적으로 일관된 긴 연속 컨셉(LongCoT)의 논리론 트래지즌을 생성합니다. 또한, 미세한 직접적인 취미 최적화(fDPO)를 제안하고, 관측 기반과 로직적인 논리론에 대한 분할 고유의 취미의 거니러티를 도입하며, 공간적 보상 메커니즘에 의해 시각적 일관성, 공간적 기반, 로직적인 일관성을 기반으로 후보 응답을 평가합니다. 실험 결과를 통해, fDPO는 공간적 품질 태스크에서 표준 DPO보다 평균적으로 4.1%의 향상을 달성하고, 공간적 양 태스크에서는 9.0%의 효과를 발휘합니다. fDPO로 훈련된 SpatialReasoner-R1은 SPATIALRGPT-Bench에서 새로운 SoTA를 설정하고, 가장 강력한 기준과 비교하여 평균적인 정확도를 9.8%의 향상을 거뒀으며, 일반적인 시각 언어 태스크에서도 경쟁적인 성능을 유지합니다.",
      "upvotes": 6,
      "discussionId": "6861f2b99e7509383d29ab3e",
      "ai_summary": "SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.",
      "ai_keywords": [
        "vision-language models",
        "SpatialReasoner-R1",
        "Multi-Model Monte Carlo Tree Search",
        "M3CTS",
        "Long Chain-of-Thought",
        "LongCoT",
        "fine-grained Direct Preference Optimization",
        "fDPO",
        "segment-specific preference granularity",
        "descriptive grounding",
        "logical reasoning",
        "spatial reward mechanism",
        "visual consistency",
        "spatial grounding",
        "logical coherence",
        "SPATIALRGPT-Bench"
      ]
    },
    "publishedAt": "2025-06-26T14:00:00.000Z",
    "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
    "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e387095132c2edd193ae49",
      "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
      "fullname": "Yifan Shen",
      "name": "SivanSX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21628",
      "authors": [
        {
          "_id": "686261739e7509383d29ac6e",
          "name": "Magnus Dierking",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac6f",
          "name": "Christopher E. Mower",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac70",
          "name": "Sarthak Das",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac71",
          "name": "Huang Helong",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac72",
          "name": "Jiacheng Qiu",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac73",
          "name": "Cody Reading",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac74",
          "name": "Wei Chen",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac75",
          "name": "Huidong Liang",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac76",
          "name": "Huang Guowei",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac77",
          "name": "Jan Peters",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac78",
          "name": "Quan Xingyue",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac79",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac7a",
          "name": "Haitham Bou-Ammar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T20:23:39.000Z",
      "submittedOnDailyAt": "2025-06-30T08:38:41.700Z",
      "title": "Ark: 로봇 학습을 위한 오픈 소스 Python 프레임워크",
      "submittedOnDailyBy": {
        "_id": "631c375768f7da9ad2496bf6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg",
        "isPro": false,
        "fullname": "Haitham Bou Ammar",
        "user": "hba123",
        "type": "user"
      },
      "summary": "로봇기술은 놀라운 하드웨어의 발전을 거듭해왔습니다. DARPA의 도시와 로봇 도전, 그리고 최초의 인간형 로봇 캐프보크싱 대회를 포함합니다. 그러나, 상업적 비정상성은 기계 학습의 발전에 따라잡을 수 없습니다. 로직 블록으로서의 소프트웨어는 큰 블록을 형성하고 있습니다. 현재의 로봇 스택은, 높은 학습 곡선, 저급 C/C++의 지식, 트로이닝의 분산, 복잡한 하드웨어 인테리어를 요구하고 있습니다. 이는, Python을 중심으로 작성된, 세부적으로 작성된 생태계와 대비되어, 현대의 AI를 구동하는 것이 아닙니다. ARK라는 오픈 소스, Python을 우선으로 하는 로봇 기술 프레임워크를 소개합니다. ARK는, 데이터의 수집, 전처리, 프로젝트의 학습을 위해 가장 先端의 학습 알고리즘(예: ACT, Diffusion Policy)을 사용하여, Gym 타입의 환경 인터페이스를 제공하여, 고정밀의 시뮬레이션과 물리적인 로봇 사이에서 쉼없이 전환할 수 있습니다. 가벼운 클라이언트 서버 아키텍처는 네트워크 프로바이더, 서브 서브 시hariabari 통신을 제공하며, 선택 가능한 C/C++ 바이ン딩도 실시간 성능을 보장합니다. ARK는, 제어, SLAM, 이동 계획, 시스템 식별, 시각화를 위한 재사용 가능한 모듈을 포함하고, ROS의 자연스러운 교환성도 제공합니다. 상세한 설명과 사례 연구를 포함하여, 동작 및 이동 검색의 프로토 타입, 무통의 하드웨어의 교환, 엔드 포인트에서 구축된 파이프라인을 보여주며,主流의 기계 학습 작업 흐름의 편리성을匹敵합니다. ARK는, Python을 공유 플랩 하에서 로봇과 AI의 실용을 통합하고, 자동 로봇의 연구와 상업적 배치의 입문 가치를 낮추고, 가속합니다.",
      "upvotes": 4,
      "discussionId": "686261739e7509383d29ac7b",
      "ai_summary": "ARK is an open-source Python-first framework that integrates modern imitation-learning algorithms and seamless simulation-physical robot interactions to simplify robotics development and deployment.",
      "ai_keywords": [
        "Gym-style environment interface",
        "imitation-learning algorithms",
        "ACT",
        "Diffusion Policy",
        "lightweight client-server architecture",
        "publisher-subscriber communication",
        "reusable modules",
        "control",
        "SLAM",
        "motion planning",
        "system identification",
        "visualization",
        "native ROS interoperability",
        "end-to-end pipelines"
      ]
    },
    "publishedAt": "2025-06-24T16:23:39.000Z",
    "title": "Ark: An Open-source Python-based Framework for Robot Learning",
    "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21628.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631c375768f7da9ad2496bf6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg",
      "fullname": "Haitham Bou Ammar",
      "name": "hba123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19741",
      "authors": [
        {
          "_id": "686209869e7509383d29ab92",
          "name": "Yihong Luo",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab93",
          "name": "Shuchen Xue",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab94",
          "name": "Tianyang Hu",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab95",
          "name": "Jing Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T15:58:55.000Z",
      "submittedOnDailyAt": "2025-06-30T02:20:48.977Z",
      "title": "노이즈 일관성 트레이닝: 한 단계 제네레이터의 원생 접근 방식에서 추가 제어 학습",
      "submittedOnDailyBy": {
        "_id": "65f7e6856bd4bac5b6a4ecc3",
        "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
        "isPro": false,
        "fullname": "Yihong Luo",
        "user": "Luo-Yihong",
        "type": "user"
      },
      "summary": "고품질 콘텐츠 생성의 효율화와 제어 가능성의 추구는 인공지능 생성 콘텐츠(AIGC)의 핵심적인 문제입니다. 한 스텝 제네레이터는 확산 열 증발 기법을 통해 우수한 생성 품질과 계산 효율성을 제공하지만, 새로운 제어 조건에 대한 적응(구조적 제약, 문법 가이드라인, 외부 입력 등)에 큰 문제점이 있습니다. 전통적인 접근 방식은 기초 모델의 높은 계산량을 필요로 하고 후속의 증발 열 증발이 필요합니다. 본 논문에서는 새로운 제어 신호를 직접 적용할 수 있는 새로운, 가벼운 접근 방식인 노이즈 일관성 훈련(NCT)을 소개합니다. NCT는 원의 훈련 이미지의 액세스나 기초 증발 모델의 재훈련을 필요로 하지 않습니다. NCT는 적응 모듈을 삽입하고 제네레이터의 노이즈 공간에서 노이즈 일관성 손실을 사용하여 효과적으로 구현합니다. 이 손실은 제어 조건에 따라 程度 다른 조건付き 노이즈에 대한 적응 모델의 생성 행동을 일치시키고 새로운 제어에 따라 암뜻적으로 가이드합니다. 이론적으로는 이 훈련 목표는 적응 모델과 새로운 조건에 의한 조건付き 분포의 분포 차이의 최소화로 이해될 수 있습니다. NCT는 모듈화되어 있으며 데이터 효율적이고 간단하게 구현할 수 있으며, 이를 위해 전처리된 한 스텝 제네레이터와 제어 신호 모델을 의존합니다. 세부적인 실험은 NCT는 한 스텝에 가장 先端의 제어 가능한 생성을 실현하고 생성 품질과 계산 효율성에서 현재의 다 스텝이나 증발 열 증발 기반의 방법들을 초월함을 보여줍니다. 코드는 https://github.com/Luo-Yihong/NCT 에 액세스할 수 있습니다.",
      "upvotes": 4,
      "discussionId": "686209869e7509383d29ab96",
      "ai_summary": "A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.",
      "ai_keywords": [
        "diffusion distillation",
        "Noise Consistency Training",
        "NCT",
        "one-step generators",
        "adapter module",
        "noise consistency loss",
        "noise space",
        "conditional distribution",
        "generative modeling",
        "data-efficient",
        "computational efficiency"
      ]
    },
    "publishedAt": "2025-06-24T11:58:55.000Z",
    "title": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls",
    "summary": "The pursuit of efficient and controllable high-quality content generation\nremains a central challenge in artificial intelligence-generated content\n(AIGC). While one-step generators, enabled by diffusion distillation\ntechniques, offer excellent generation quality and computational efficiency,\nadapting them to new control conditions--such as structural constraints,\nsemantic guidelines, or external inputs--poses a significant challenge.\nConventional approaches often necessitate computationally expensive\nmodifications to the base model and subsequent diffusion distillation. This\npaper introduces Noise Consistency Training (NCT), a novel and lightweight\napproach to directly integrate new control signals into pre-trained one-step\ngenerators without requiring access to original training images or retraining\nthe base diffusion model. NCT operates by introducing an adapter module and\nemploys a noise consistency loss in the noise space of the generator. This loss\naligns the adapted model's generation behavior across noises that are\nconditionally dependent to varying degrees, implicitly guiding it to adhere to\nthe new control. Theoretically, this training objective can be understood as\nminimizing the distributional distance between the adapted generator and the\nconditional distribution induced by the new conditions. NCT is modular,\ndata-efficient, and easily deployable, relying only on the pre-trained one-step\ngenerator and a control signal model. Extensive experiments demonstrate that\nNCT achieves state-of-the-art controllable generation in a single forward pass,\nsurpassing existing multi-step and distillation-based methods in both\ngeneration quality and computational efficiency. Code is available at\nhttps://github.com/Luo-Yihong/NCT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19741.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f7e6856bd4bac5b6a4ecc3",
      "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
      "fullname": "Yihong Luo",
      "name": "Luo-Yihong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21411",
      "authors": [
        {
          "_id": "686237f69e7509383d29abe9",
          "user": {
            "_id": "64d5deb154bb9eb704f83122",
            "avatarUrl": "/avatars/86ce09bcca903319051e2307581a43f4.svg",
            "isPro": false,
            "fullname": "Yehui Tang",
            "user": "tangyehui",
            "type": "user"
          },
          "name": "Yehui Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:35.262Z",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abea",
          "name": "Xiaosong Li",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abeb",
          "user": {
            "_id": "64b78295479b934973e2c40e",
            "avatarUrl": "/avatars/9213e385964132fa50859264a838d891.svg",
            "isPro": false,
            "fullname": "liu",
            "user": "Fangcheng2",
            "type": "user"
          },
          "name": "Fangcheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:52.170Z",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abec",
          "name": "Wei Guo",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abed",
          "name": "Hang Zhou",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abee",
          "name": "Yaoyuan Wang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abef",
          "name": "Kai Han",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf0",
          "name": "Xianzhi Yu",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf1",
          "name": "Jinpeng Li",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf2",
          "name": "Hui Zang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf3",
          "name": "Fei Mi",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf4",
          "name": "Xiaojun Meng",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf5",
          "name": "Zhicheng Liu",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf6",
          "name": "Hanting Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf7",
          "name": "Binfan Zheng",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf8",
          "name": "Can Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf9",
          "name": "Youliang Yan",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfa",
          "name": "Ruiming Tang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfb",
          "name": "Peifeng Qin",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfc",
          "name": "Xinghao Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfd",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfe",
          "user": {
            "_id": "658bdf7b925aadd43304f05c",
            "avatarUrl": "/avatars/64d9e9dea27c376c3bc7b2a54efc2a46.svg",
            "isPro": false,
            "fullname": "Yunhe Wang",
            "user": "MightyCrane",
            "type": "user"
          },
          "name": "Yunhe Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:59.146Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T16:40:21.000Z",
      "submittedOnDailyAt": "2025-06-30T05:41:23.309Z",
      "title": "Pangu Pro MoE: 그룹별 전문가의 혼잡으로 인한 효율적인 희소성",
      "submittedOnDailyBy": {
        "_id": "63a369d98c0c89dcae3b8329",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
        "isPro": true,
        "fullname": "Adina Yakefu",
        "user": "AdinaY",
        "type": "user"
      },
      "summary": "ミックスオブエキスパート(MoE)가 대규모 언어 모델에 등장하며, 모델 파라미터 수와 학습 능력의 대폭적 증가로 인해 실행 비용의 작은 대체를 기대받는 것으로 나타난다. 그러나 이러한 Expert의 활성화 횟수가 편향되어 있고, 서로 다른 장치에서 병렬 실행 시 시스템의 효율화가 通常로 잃어지는 것을 관찰하고 있다. 이에 따라, ミックスオブグループエキスパート(MoGE)를 도입하여, Expert의 선택 시 그룹화하여 MoE에 비해 Expert의 부담을 더 균등하게 조정할 수 있음을 보여준다. MoGE는 각 예약된 Expert 그룹 내 동일한 수의 Expert를 활성화시키며, 모델 실행이 여러 장치에 분배될 경우, 이 아키텍처 설계는 장치 간 계산부하의 균형을 유지하고, 특히 추론 단계에서의 트랜스포트를 크게 향상시킬 수 있음을 보장한다. 또한, Ascend NPUs에서 Pangu Pro MoE를 구축하고, MoGE에 기초한 희소 모델로 총 파라미터 수 720억, 각 토큰에서 활성화되는 파라미터 수 160억을 만족하고 있다. Pangu Pro MoE의 설계는 Ascend 300I Duo와 800I A2를 통해 확장된 시스템 시뮬레이션 연구에 의해 최적화되어 있다. 실험 결과로부터, MoGE는 모델의 훈련 및 추론에서 Expert의 부담의 균형을 더욱 잘 조절하여 효율적인 실행을 실현하는 것을 보여준다. Pangu Pro MoE의 추론 성능은 1 카드당 1148 토큰/초를 달성하며, 스펙트럴 엑셀レ레이션으로 더욱 1528 토큰/초에 도달하여 밀집 모델을 초과할 수 있다. 또한, Ascend 300I Duo에서 모델 추론의 비용 성능 비율은 매우 좋으며, Pangu Pro MoE는 Magic Space Parallelization을 사용하여 훈련이 가능하며, 100B 총 파라미터 클래스 내에서 선두 모델로 자리잡고, GLM-Z1-32B와 Qwen3-32B 등 오픈 소스 모델을 초과하는 것을 보여준다.",
      "upvotes": 4,
      "discussionId": "686237f79e7509383d29abff",
      "ai_summary": "Mixture of Grouped Experts (MoGE) improves expert load balancing and execution efficiency for large language models, enhancing throughput and cost-to-performance on Ascend NPUs.",
      "ai_keywords": [
        "Mixture of Experts (MoE)",
        "Mixture of Grouped Experts (MoGE)",
        "large language models",
        "expert load balancing",
        "computational load",
        "inference phase",
        "sparse model",
        "Ascend NPUs",
        "system simulation",
        "speculative acceleration",
        "Dense models",
        "GLM-Z1-32B",
        "Qwen3-32B"
      ]
    },
    "publishedAt": "2025-05-27T12:40:21.000Z",
    "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
    "summary": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo. Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 774
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22419",
      "authors": [
        {
          "_id": "686229249e7509383d29abd0",
          "name": "Bingchen Zhao",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd1",
          "name": "Despoina Magka",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd2",
          "name": "Minqi Jiang",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd3",
          "name": "Xian Li",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd4",
          "name": "Roberta Raileanu",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd5",
          "name": "Tatiana Shavrina",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd6",
          "name": "Jean-Christophe Gagnon-Audet",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd7",
          "name": "Kelvin Niu",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd8",
          "name": "Shagun Sodhani",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd9",
          "name": "Michael Shvartsman",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abda",
          "name": "Andrei Lupu",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdb",
          "name": "Alisia Lupidi",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdc",
          "name": "Edan Toledo",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdd",
          "name": "Karen Hambardzumyan",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abde",
          "name": "Martin Josifoski",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdf",
          "name": "Thomas Foster",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe0",
          "name": "Lucia Cipolina-Kun",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe1",
          "name": "Abhishek Charnalia",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe2",
          "name": "Derek Dunfield",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe3",
          "name": "Alexander H. Miller",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe4",
          "name": "Oisin Mac Aodha",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe5",
          "name": "Jakob Foerster",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe6",
          "name": "Yoram Bachrach",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T17:44:32.000Z",
      "submittedOnDailyAt": "2025-06-30T06:29:15.385Z",
      "title": "自動화 LLM 속도 훈련 벤치마크: NanoGPT의 재현과 개선",
      "submittedOnDailyBy": {
        "_id": "62dcd71075e9787ec5aa41ba",
        "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
        "isPro": true,
        "fullname": "Bingchen Zhao",
        "user": "tennant",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)의 급격한 발전은 과학의 발전에 도움이 될 수 있습니다. 이 시도의 중요한 능력은 기존의 일을 재현하는 능력입니다. AI 에이전트가 활발한 연구 분야에서 결과를 재현할 수 있는 능력을 평가하기 위해, NanoGPT speedrun의 연구 커뮤니티의 기여를 활용하여, 우리는 자동화된 LLM speedrunning 벤치마크를 도입했습니다. 이 대회에서, GPT-2 모델을 가장 빠른 시간 내에 훈련하는 것을 목표로 합니다. 19개의 speedrun 태스크 각각에 대해, 에이전트는 이전 기록과 훈련 스크립트를 제공받고, 그 중 3가지의 힌트 포맷 중 하나를 선택적으로 부여받습니다. 이 힌트는 프로그래밍 언어의 코드에서 논문처럼 기술된 범위가 있습니다. 기록은 설계적으로 빠르게 실행되며, speedrun의 개선은 알고리즘의 높은 수준의 발전부터 하드웨어에 관련된 최적화까지 다양한 코드 수준의 변경을 포함합니다. 이러한 기능은 LLM의 훈련을 개선하는 선도적인 문제에 대한 가치가 있고 실용적이라는 것을 보여줍니다. 최근의 모델과 SoTA 스키프트의 조합이, 힌트를 제공받더라도 우리 벤치마크에서 이미 알려진 혁신을 재실행하는 것이 어렵습니다. 이 벤치마크는 LLM의 자동화 과학 재현 능력이 간단히, 필요한(하지만 충분한 것이 아닙니다) 기술임을 보여줍니다.",
      "upvotes": 2,
      "discussionId": "686229249e7509383d29abe7",
      "ai_summary": "An Automated LLM Speedrunning Benchmark evaluates AI agents' ability to reproduce scientific results by leveraging NanoGPT speedrun tasks, indicating that even recent reasoning LLMs struggle with re-implementing known improvements.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "AI agents",
        "Automated LLM Speedrunning Benchmark",
        "NanoGPT speedrun",
        "GPT-2",
        "high-level algorithmic advancements",
        "hardware-aware optimizations"
      ]
    },
    "publishedAt": "2025-06-27T13:44:32.000Z",
    "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements",
    "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62dcd71075e9787ec5aa41ba",
      "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
      "fullname": "Bingchen Zhao",
      "name": "tennant",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21594",
      "authors": [
        {
          "_id": "68625a4c9e7509383d29ac4c",
          "name": "Ahmed M. Adly",
          "hidden": false
        },
        {
          "_id": "68625a4c9e7509383d29ac4d",
          "name": "Mostafa Samy",
          "hidden": false
        },
        {
          "_id": "68625a4c9e7509383d29ac4e",
          "name": "Amr Fawzy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T09:44:21.000Z",
      "submittedOnDailyAt": "2025-06-30T08:07:09.494Z",
      "title": "가젤-R1: 가장 선진적인 의료논리론을 달성하기 위한 파라미터 효율적인 2단계 훈련\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "63aca106e3b217fb36cf1950",
        "avatarUrl": "/avatars/b37cc9102f875b6ce0c55a294c052078.svg",
        "isPro": false,
        "fullname": "Ahmed Mostafa",
        "user": "AhmedMostafa",
        "type": "user"
      },
      "summary": "GAZAL-R1, 320억 파라미터의 언어 모델을 소개합니다. 이 모델은 의료 논리의 최상위 성능을 달성하고, 임상 판단을 위해 단계별로 설명을 제공합니다. Qwen3 32B에 기반한 이 모델은 전략적인 훈련이 전문 분야에서 중간 규모의 모델이 크게 큰 컴포넌트를 초과하는 것을 보여주었습니다. 새로운 2단계 훈련 프로이퀘트를 개발했습니다. 처음으로, 107,033개의 합성적인 의료 논리 예의 컬렉션에 의한 서브 프로모시션 조정을 수행하고, 구조화된 임상적인 사고를 가르치며, Weight-Decomposed Low-Rank Adaptation (DoRA) 및 Rank-Stabilized LoRA (rsLoRA) 등 파라미터 효율적인 기술이 추가되었습니다. 다음으로, Group Relative Policy Optimization (GRPO)를 사용하여 강화학습을 수행하고, 정확성, 형식의 준수, 논리의 품질을 향상시키는 복잡한 다 컴포넌트 상영 시스템이 사용되었습니다. GAZAL-R1은 의료 벤치마크에서 특히 뛰어난 성능을 기록하며, MedQA에서 87.1%, MMLU Pro (Medical)에서 81.6%, PubMedQA에서 79.6%의 점수를 달성했으며, 12배 이상 큰 모델을 초과합니다. 이 연구는 전문 분야에서 논리적 인력의 모델의 훈련에 대한 문제에 대한 자세한 통찰을 제공하며, 상영 팍킹, 훈련의 불안정성, 사실적인 기억과 상세한 논리의 기본적인 긴장관계 등 문제에 대해 설명합니다. 우리의 방법론은 성능, 효율, 설명성을 균형을 이루는 고성능의 전문 분야의 언어 모델의 개발에 재현 가능한 프레임워크를 제공합니다.",
      "upvotes": 2,
      "discussionId": "68625a4c9e7509383d29ac4f",
      "ai_summary": "Gazal-R1, a 32-billion-parameter language model, achieves top performance in medical reasoning through strategic training, including advanced parameter-efficient techniques and reinforcement learning, providing detailed explanations for clinical decisions.",
      "ai_keywords": [
        "Weight-Decomposed Low-Rank Adaptation (DoRA)",
        "Rank-Stabilized LoRA (rsLoRA)",
        "Group Relative Policy Optimization (GRPO)",
        "MedQA",
        "MMLU Pro (Medical)",
        "PubMedQA",
        "reasoning-capable models",
        "reward hacking",
        "training instability"
      ]
    },
    "publishedAt": "2025-06-18T05:44:21.000Z",
    "title": "Gazal-R1: Achieving State-of-the-Art Medical Reasoning with\n  Parameter-Efficient Two-Stage Training",
    "summary": "We present Gazal-R1, a 32-billion-parameter language model that achieves\nstate-of-the-art performance in medical reasoning while providing transparent,\nstep-by-step explanations for clinical decision-making. Built upon Qwen3 32B,\nour model demonstrates that strategic training can enable mid-sized models to\noutperform significantly larger counterparts in specialized domains. We\ndeveloped a novel two-stage training pipeline: first, supervised fine-tuning on\na carefully curated dataset of 107,033 synthetic medical reasoning examples\nthat teaches structured clinical thinking, enhanced by advanced\nparameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation\n(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using\nGroup Relative Policy Optimization (GRPO) with a sophisticated multi-component\nreward system that refines accuracy, format adherence, and reasoning quality.\nGazal-R1 achieves exceptional performance across medical benchmarks, scoring\n87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing\nmodels up to 12x larger. Beyond its strong empirical results, this work\nprovides detailed insights into the challenges of training reasoning-capable\nmodels in specialized domains, including issues with reward hacking, training\ninstability, and the fundamental tension between factual recall and detailed\nreasoning. Our methodology offers a reproducible framework for developing\nhigh-capability, domain-specific language models that balance performance,\nefficiency, and explainability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21594.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63aca106e3b217fb36cf1950",
      "avatarUrl": "/avatars/b37cc9102f875b6ce0c55a294c052078.svg",
      "fullname": "Ahmed Mostafa",
      "name": "AhmedMostafa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22149",
      "authors": [
        {
          "_id": "68625f5d9e7509383d29ac62",
          "name": "Ronald Fecso",
          "hidden": false
        },
        {
          "_id": "68625f5d9e7509383d29ac63",
          "name": "José Morano",
          "hidden": false
        },
        {
          "_id": "68625f5d9e7509383d29ac64",
          "name": "Ursula Schmidt-Erfurth",
          "hidden": false
        },
        {
          "_id": "68625f5d9e7509383d29ac65",
          "name": "Hrvoje Bogunović",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T11:53:54.000Z",
      "submittedOnDailyAt": "2025-06-30T08:31:01.610Z",
      "title": "RetFiner: 시각 언어의 향상 전략에 의한 젖막기 기반 모델\n\n(Note: The translation is provided as requested, without any additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "655b3383ed8df831286969f0",
        "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
        "isPro": false,
        "fullname": "José Morano",
        "user": "j-morano",
        "type": "user"
      },
      "summary": "비디오 인센싱 기술의 발전과 딥러닝(DL)의 발전으로 의료기관원과 연구자들은 결막병의 단계 분류를 스트리밍으로 수행할 수 있게 되었습니다. 딥러닝을 학습하기 위해 필요한 프라이버시 기반 학습(SSL)이 인기를 끌고 있으며, 이는 무 라벨 데이터의 사용으로 비용을 줄일 수 있습니다. SSL은 기초 모델(FM)의 개발을 가능하게 했지만, 현재의 OCT용 FM은 이미지 데이터만 사용하여 훈련되어 있어 이미지의 세부적인 의미 이해가 부족합니다. 이는 하류 데이터의 성능(특히 복잡한 태스크에 대해)에서 분명히 드러납니다. 이러한 FM은 특정 애플리케이션이나 인구에 적합하도록 해야하기 때문에, 체크 피드백(가능하지만 구현할 수 없는 경우도ある)이 필요합니다. 이를 해결하기 위해 우리는 RetFiner를 제안합니다. RetFiner는 SSL을 사용한 시각 언어 미세 조정 셈으로, 현재의 FM의 표현을 개선하고 특정 인구에 효율적으로 직접 적용할 수 있도록 합니다. 우리 방법은 풍부한 텍스트 데이터에서 풍부한 서브 객체 신호를 활용하여 다양한 훈련 객체를 사용합니다. RetFiner는 OCT용 FM의 RETFound, UrFound, VisionFM에 대해 검증되었으며, 7가지의 매우 다양한 OCT 분류 태스크에 대해 선형 프로비닝 성능에 상당한 향상을 나타냅니다. 평균적으로는 기준 데이터보다 각각 5.8, 3.9, 2.1 퍼센트 포인트의 증가를 나타냅니다. 우리 코드와 모델 가중치는 https://github.com/ronnief1/RetFiner에서 공개되어 있습니다.",
      "upvotes": 1,
      "discussionId": "68625f5d9e7509383d29ac66",
      "githubRepo": "https://github.com/ronnief1/RetFiner",
      "ai_summary": "RetFiner, a vision-language refinement scheme, enhances self-supervised foundation models for OCT by leveraging textual data, improving their downstream performance in retinal disease classification tasks.",
      "ai_keywords": [
        "optical coherence tomography (OCT)",
        "deep learning (DL)",
        "self-supervised learning (SSL)",
        "foundation models (FMs)",
        "supervised fine-tuning",
        "RetFiner",
        "vision-language refinement",
        "linear probing performance"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-06-27T07:53:54.000Z",
    "title": "RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation\n  Models",
    "summary": "The rise of imaging techniques such as optical coherence tomography (OCT) and\nadvances in deep learning (DL) have enabled clinicians and researchers to\nstreamline retinal disease staging. A popular DL approach is self-supervised\nlearning (SSL), where models learn from vast amounts of unlabeled data,\navoiding costly annotation. SSL has allowed the development of foundation\nmodels (FMs), large models that can be used for a variety of downstream tasks.\nHowever, existing FMs for OCT, trained solely on image data, lack a\ncomprehensive and robust semantic understanding of images, as evidenced by\ntheir downstream performance (especially for complex tasks), and thus require\nsupervised fine-tuning (which may be unfeasible) to better adapt to specific\napplications and populations. To address this, we propose RetFiner, an SSL\nvision-language refinement scheme that improves the representations of existing\nFMs and enables their efficient and direct adaptation to specific populations\nfor improved downstream performance. Our method uses a diverse set of training\nobjectives which take advantage of the rich supervisory signal found in textual\ndata. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM,\nshowing significant improvements in linear probing performance on seven highly\ndiverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1\npercentage points over their baselines, respectively. Our code and model\nweights are publicly available at https://github.com/ronnief1/RetFiner.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22149.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655b3383ed8df831286969f0",
      "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
      "fullname": "José Morano",
      "name": "j-morano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]