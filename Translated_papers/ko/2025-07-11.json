[
  {
    "paper": {
      "id": "2507.07966",
      "authors": [
        {
          "_id": "68706bdcc8391850d60977eb",
          "name": "Yukang Chen",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977ec",
          "name": "Wei Huang",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977ed",
          "name": "Baifeng Shi",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977ee",
          "name": "Qinghao Hu",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977ef",
          "name": "Hanrong Ye",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f0",
          "name": "Ligeng Zhu",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f1",
          "name": "Zhijian Liu",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f2",
          "name": "Pavlo Molchanov",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f3",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f4",
          "name": "Xiaojuan Qi",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f5",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f6",
          "name": "Hongxu Yin",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f7",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f8",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62919485a29097b211bc7b83/Tu__SuWZeWyCBPoK8YUyh.mp4"
      ],
      "publishedAt": "2025-07-10T17:47:40.000Z",
      "submittedOnDailyAt": "2025-07-11T00:13:53.988Z",
      "title": "스케일링 RL 롱비디오에",
      "submittedOnDailyBy": {
        "_id": "62919485a29097b211bc7b83",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653710384819-62919485a29097b211bc7b83.png",
        "isPro": false,
        "fullname": "YukangChen",
        "user": "Yukang",
        "type": "user"
      },
      "summary": "우리는 reinforcement learning을 활용하여 긴 비디오에서 시각-언어 모델(VLMs)의 추론력을 확장하는 완전스택 프레임워크를 소개합니다. 긴 비디오 추론의 독특한 도전을 해결하기 위해 세 가지 핵심 요소를 통합합니다: (1) 큰 규모의 데이터셋인 LongVideo-Reason, 이는 스포츠, 게임, vlog 등 다양한 분야에서 고품질의 추론적 코딩을 포함하는 52K 긴 비디오 QA 쌍; (2) VLMs를 확장하는 두 단계 훈련 파이프라인, 이는 chain-of-thought supervised fine-tuning (CoT-SFT)와 reinforcement learning (RL)을 결합합니다; (3) 긴 비디오 RL의 훈련 기반 구조인 Multi-modal Reinforcement Sequence Parallelism (MR-SP), 이는 시퀀스 병렬성과 vLLM 기반 엔진을 사용하여 긴 비디오를 위한 캐시된 비디오 임베딩을 사용하여 효율적인 rollout과 prefilling을 수행합니다. 실험 결과에 따르면 LongVILA-R1-7B은 VideoMME 등 긴 비디오 QA 벤치마크에서 강력한 성능을 나타냅니다. 또한, 우리의 MR-SP 시스템은 긴 비디오 RL 훈련에서 최대 2.1배의 속도 향상을 달성합니다. LongVILA-R1은 입력 비디오 프레임의 수를 확장할 때 일관된 성능 향상을 보여주며, VLMs에서 긴 비디오 추론에 대한 확고한 단계를 나타냅니다. 또한, 우리는 RL 훈련이 다양한 모드(영상, 텍스트, 오디오), 다양한 모델(VILA와 Qwen 시리즈), 그리고 이미지 및 비디오 생성 모델을 지원하는 공개에 사용할 수 있는 훈련 시스템을 공개합니다. 단일 A100 노드(8 GPU)에서, 예를 들어, 3,600 프레임(약 256k 토큰)의 1시간 길이의 비디오의 RL 훈련을 지원합니다.",
      "upvotes": 69,
      "discussionId": "68706bdcc8391850d60977f9",
      "projectPage": "https://github.com/NVlabs/Long-RL",
      "githubRepo": "https://github.com/NVlabs/Long-RL",
      "ai_summary": "A framework for scaling vision-language models to long videos using reinforcement learning, achieving strong performance on various reasoning tasks with a specialized training infrastructure.",
      "ai_keywords": [
        "vision-language models",
        "reinforcement learning",
        "chain-of-thought supervised fine-tuning",
        "CoT-SFT",
        "Multi-modal Reinforcement Sequence Parallelism",
        "MR-SP",
        "sequence parallelism",
        "vLLM",
        "long video QA",
        "VideoMME",
        "LongVideo-Reason-eval",
        "temporal reasoning",
        "goal and purpose reasoning",
        "spatial reasoning",
        "plot reasoning",
        "RL training",
        "image and video generation models"
      ],
      "githubStars": 216
    },
    "publishedAt": "2025-07-10T13:47:40.000Z",
    "title": "Scaling RL to Long Videos",
    "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62919485a29097b211bc7b83/Tu__SuWZeWyCBPoK8YUyh.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07966.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62919485a29097b211bc7b83",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653710384819-62919485a29097b211bc7b83.png",
      "fullname": "YukangChen",
      "name": "Yukang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 61
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05964",
      "authors": [
        {
          "_id": "6870b8b5c8391850d60978e0",
          "name": "Vera Soboleva",
          "hidden": false
        },
        {
          "_id": "6870b8b5c8391850d60978e1",
          "user": {
            "_id": "66680c6451545a8b46c6fd21",
            "avatarUrl": "/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg",
            "isPro": false,
            "fullname": "Aibek Alanov",
            "user": "ai-alanov",
            "type": "user"
          },
          "name": "Aibek Alanov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T07:59:56.813Z",
          "hidden": false
        },
        {
          "_id": "6870b8b5c8391850d60978e2",
          "name": "Andrey Kuznetsov",
          "hidden": false
        },
        {
          "_id": "6870b8b5c8391850d60978e3",
          "name": "Konstantin Sobolev",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T13:14:10.000Z",
      "submittedOnDailyAt": "2025-07-11T05:42:42.057Z",
      "title": "T-LoRA: 1개의 이미지의 Difﬁculty Model을 사용하여 과적합을 방지하는 방법의 맞춤화",
      "submittedOnDailyBy": {
        "_id": "66680c6451545a8b46c6fd21",
        "avatarUrl": "/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg",
        "isPro": false,
        "fullname": "Aibek Alanov",
        "user": "ai-alanov",
        "type": "user"
      },
      "summary": "While diffusion model fine-tuning offers a powerful approach for customizing pre-trained models to generate specific objects, it frequently suffers from overfitting when training samples are limited, compromising both generalization capability and output diversity. This paper tackles the challenging yet most impactful task of adapting a diffusion model using just a single concept image, as single-image customization holds the greatest practical potential. We introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework specifically designed for diffusion model personalization. In our work, we show that higher diffusion timesteps are more prone to overfitting than lower ones, necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates two key innovations: (1) a dynamic fine-tuning strategy that adjusts rank-constrained updates based on diffusion timesteps, and (2) a weight parametrization technique that ensures independence between adapter components through orthogonal initialization. Extensive experiments show that T-LoRA and its individual components outperform standard LoRA and other diffusion model personalization techniques. They achieve a superior balance between concept fidelity and text alignment, highlighting the potential of T-LoRA in data-limited and resource-constrained scenarios. Code is available at https://github.com/ControlGenAI/T-LoRA.",
      "upvotes": 53,
      "discussionId": "6870b8b5c8391850d60978e4",
      "githubRepo": "https://github.com/ControlGenAI/T-LoRA",
      "ai_summary": "T-LoRA, a timestep-dependent low-rank adaptation framework, enhances diffusion model personalization with a dynamic fine-tuning strategy and orthogonal initialization, achieving better concept fidelity and text alignment in data-limited settings.",
      "ai_keywords": [
        "diffusion model fine-tuning",
        "overfitting",
        "generalization capability",
        "output diversity",
        "single-image customization",
        "T-LoRA",
        "timestep-dependent low-rank adaptation",
        "dynamic fine-tuning strategy",
        "rank-constrained updates",
        "diffusion timesteps",
        "weight parametrization",
        "orthogonal initialization",
        "concept fidelity",
        "text alignment",
        "data-limited scenarios"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-07-08T09:14:10.000Z",
    "title": "T-LoRA: Single Image Diffusion Model Customization Without Overfitting",
    "summary": "While diffusion model fine-tuning offers a powerful approach for customizing\npre-trained models to generate specific objects, it frequently suffers from\noverfitting when training samples are limited, compromising both generalization\ncapability and output diversity. This paper tackles the challenging yet most\nimpactful task of adapting a diffusion model using just a single concept image,\nas single-image customization holds the greatest practical potential. We\nintroduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework\nspecifically designed for diffusion model personalization. In our work we show\nthat higher diffusion timesteps are more prone to overfitting than lower ones,\nnecessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates\ntwo key innovations: (1) a dynamic fine-tuning strategy that adjusts\nrank-constrained updates based on diffusion timesteps, and (2) a weight\nparametrization technique that ensures independence between adapter components\nthrough orthogonal initialization. Extensive experiments show that T-LoRA and\nits individual components outperform standard LoRA and other diffusion model\npersonalization techniques. They achieve a superior balance between concept\nfidelity and text alignment, highlighting the potential of T-LoRA in\ndata-limited and resource-constrained scenarios. Code is available at\nhttps://github.com/ControlGenAI/T-LoRA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05964.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66680c6451545a8b46c6fd21",
      "avatarUrl": "/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg",
      "fullname": "Aibek Alanov",
      "name": "ai-alanov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07999",
      "authors": [
        {
          "_id": "68706dcdc8391850d60977fb",
          "user": {
            "_id": "6499809cf19fc795e7724e43",
            "avatarUrl": "/avatars/4b3adce8c85e2f3ef05318ded6c89c3e.svg",
            "isPro": false,
            "fullname": "HaochenWang",
            "user": "HaochenWang",
            "type": "user"
          },
          "name": "Haochen Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:30.986Z",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d60977fc",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:28.856Z",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d60977fd",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d60977fe",
          "name": "Anran Wang",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d60977ff",
          "user": {
            "_id": "64d201b1c2bd235422fb1d14",
            "avatarUrl": "/avatars/e50581aa66391cedae94e116e759b9ec.svg",
            "isPro": false,
            "fullname": "wang",
            "user": "stormthunder",
            "type": "user"
          },
          "name": "Jiacong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:26.581Z",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d6097800",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d6097801",
          "user": {
            "_id": "64531f631a57e1179c203e6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64531f631a57e1179c203e6b/C_J7pXFLqoJoHYPPhK3J9.jpeg",
            "isPro": false,
            "fullname": "zjn",
            "user": "garlicisnotmyfavor",
            "type": "user"
          },
          "name": "Jiani Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:33.749Z",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d6097802",
          "name": "Sule Bai",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d6097803",
          "name": "Zijian Kang",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d6097804",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d6097805",
          "name": "Zhuochen Wang",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d6097806",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T17:59:58.000Z",
      "submittedOnDailyAt": "2025-07-11T00:29:18.339Z",
      "title": "跡의 있는 증거를 강화한 시각화 그래프 라인 추론: 평가와 방법론",
      "submittedOnDailyBy": {
        "_id": "6499809cf19fc795e7724e43",
        "avatarUrl": "/avatars/4b3adce8c85e2f3ef05318ded6c89c3e.svg",
        "isPro": false,
        "fullname": "HaochenWang",
        "user": "HaochenWang",
        "type": "user"
      },
      "summary": "모델이 OpenAI-o3와 같이 시각 영역을 동적으로 참조하여 시각에 기반한 논리론을 선구하는 것을 창조하고 있습니다. 그러나 이러한 능력을 전반적으로 평가하는 벤치마크는 존재하지 않습니다. 이를 채워내기 위해, 우리는 Traceable Evidence Evaluation Benchmark을 TreeBench라는 이름으로 제안합니다. 이는 3가지 원칙에 기반하여 구축되어 있습니다: (1) 복잡한 스케네에서 시각적인 관찰, (2) Bounding Box 평가에 의한 증거의 흔적의 증명, (3) 2차원 논리론을 사용하여 물체 간의 상호작용과 공간의 계층을 검증하는 것입니다. 복잡한 물체가 밀집적으로 배치된 이미지에 우선시하여, SA-1B에서 1K枚의 고품질의 이미지를 처음에 샘플링하고, 8명의 LMM 전문가를 통해 각 이미지에 대한 질문, 후보 선택肢, 답을 직접 설명합니다. 3단계의 질량 관리를 통해, TreeBench는 405개의 어려운 시각적인 질문 대답 쌍을 구성하고, 가장 선진한 모델들도 이 벤치마크에 어려움을 겪습니다. 그 중, OpenAI-o3의 점수는 54.87이며, 모두 60%의 정확도를 달성하지 않습니다. 또한, 우리는 TreeVGR(증빙의 흔적을 강화 학습과 통합한 시각적인 논리론)을 도입합니다. 이는 설명적인 논리론의 패스웨이를 가능하게 하며, 정확한 설명과 논리론의 패스웨이를 가능하게 합니다. Qwen2.5-VL-7B에서 초기화되어, V* Bench (+16.8), MME-RealWorld (+12.6), TreeBench (+13.4)를 개선하고, 증빙의 흔적이 시각적인 논리론을 진보하는 핵심 역할을 하는 것을 보여줍니다. 코드는 https://github.com/Haochen-Wang409/TreeVGR에 접근할 수 있습니다.",
      "upvotes": 32,
      "discussionId": "68706dcec8391850d6097807",
      "projectPage": "https://github.com/Haochen-Wang409/TreeVGR",
      "githubRepo": "https://github.com/Haochen-Wang409/TreeVGR",
      "ai_summary": "TreeBench evaluates visual grounded reasoning through subtle target detection, traceable evidence, and second-order reasoning, while TreeVGR enhances this with joint localization and reasoning using reinforcement learning.",
      "ai_keywords": [
        "visual grounded reasoning",
        "bounding box evaluation",
        "second-order reasoning",
        "TreeBench",
        "TreeVGR",
        "reinforcement learning",
        "localization",
        "reasoning pathways",
        "V* Bench",
        "MME-RealWorld"
      ],
      "githubStars": 19
    },
    "publishedAt": "2025-07-10T13:59:58.000Z",
    "title": "Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and\n  Methodology",
    "summary": "Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically\nreferencing visual regions, just like human \"thinking with images\". However, no\nbenchmark exists to evaluate these capabilities holistically. To bridge this\ngap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a\ndiagnostic benchmark built on three principles: (1) focused visual perception\nof subtle targets in complex scenes, (2) traceable evidence via bounding box\nevaluation, and (3) second-order reasoning to test object interactions and\nspatial hierarchies beyond simple object localization. Prioritizing images with\ndense objects, we initially sample 1K high-quality images from SA-1B, and\nincorporate eight LMM experts to manually annotate questions, candidate\noptions, and answers for each image. After three stages of quality control,\nTreeBench consists of 405 challenging visual question-answering pairs, even the\nmost advanced models struggle with this benchmark, where none of them reach 60%\naccuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR\n(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to\nsupervise localization and reasoning jointly with reinforcement learning,\nenabling accurate localizations and explainable reasoning pathways. Initialized\nfrom Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and\nTreeBench (+13.4), proving traceability is key to advancing vision-grounded\nreasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07999.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6499809cf19fc795e7724e43",
      "avatarUrl": "/avatars/4b3adce8c85e2f3ef05318ded6c89c3e.svg",
      "fullname": "HaochenWang",
      "name": "HaochenWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07984",
      "authors": [
        {
          "_id": "687088a6c8391850d6097874",
          "name": "JingLi Lin",
          "hidden": false
        },
        {
          "_id": "687088a6c8391850d6097875",
          "name": "Chenming Zhu",
          "hidden": false
        },
        {
          "_id": "687088a6c8391850d6097876",
          "name": "Runsen Xu",
          "hidden": false
        },
        {
          "_id": "687088a6c8391850d6097877",
          "name": "Xiaohan Mao",
          "hidden": false
        },
        {
          "_id": "687088a6c8391850d6097878",
          "name": "Xihui Liu",
          "hidden": false
        },
        {
          "_id": "687088a6c8391850d6097879",
          "name": "Tai Wang",
          "hidden": false
        },
        {
          "_id": "687088a6c8391850d609787a",
          "name": "Jiangmiao Pang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6433aba4546e16f17a0f19f6/pNjo9cepo_BTSUgc_ZLsg.mp4"
      ],
      "publishedAt": "2025-07-10T17:56:07.000Z",
      "submittedOnDailyAt": "2025-07-11T04:12:08.963Z",
      "title": "OST-Bench: 온라인 스펙트럼 시간 공간 장소 이해에 대한 MLLM의 능력 평가",
      "submittedOnDailyBy": {
        "_id": "6433aba4546e16f17a0f19f6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433aba4546e16f17a0f19f6/3hlNy4_Suy0bO__Qa9amL.jpeg",
        "isPro": false,
        "fullname": "Chenming Zhu",
        "user": "ChaimZhu",
        "type": "user"
      },
      "summary": "최근의 다 모델 대 언어 모델(MLLM)의 발전은 복잡한 이유에 의한 시각과 언어의 통합에 놀라워하는 능력을 보여주고 있습니다. 기존의 벤치마크에서는 고정된 미리 기록된 입력을 사용하여 모델을 평가하고 있지만, 우리는 에이전트가 현장에서 적극적으로 탐색하는 시각과 시간적인 이해를 평가하기 위한 벤치마크를 소개합니다. 온라인에서는 증가하는 관찰을 처리하고 이유를 찾는 것이 필요로 하며, 시간적인 구성에서는 현재의 시각 입력과 역사의 메모리를 통합하여 동적인 공간적인 이유를 지지하는 것이 필요합니다. OST-Bench는 현실적인 구체적인 인식의 문제를 더 정확하게 반영하고 있습니다. 효율적인 데이터 수집 프로이프라인에 기반하여 구축되어 있으며, OST-Bench는 ScanNet, Matterport3D, ARKitScenes에서 1.4k의 현장과 10k의 질문·답변 쌍을 수집하고 있습니다. 우리는 여러 선진적인 MLLM을 OST-Bench에서 평가하고, 복잡한 시간적인 이유에 필요한 작업에서 이러한 모델이 부족한 것을 발견했습니다. 온라인 설정에서 탐색의 경계가 확장되고 메모리가 증가함에 따라 정확도가 떨어집니다. 추가적인 실험적 분석을 통해 모델 간에 공유되는 오류 패턴을 특정하고, 복잡한 순환 기반의 공간적인 이유의 요구와 장기적인 메모리의 추출 요구가 모델의 성능을 크게 떨어뜨리고, 두 가지 다른 축에 따라 개선을 위해 필요한 핵심적인 문제를 밝혀줍니다. 피드백을 위해 연구와 개발을 위한 우리의 코드, 데이터셋, 벤치마크는 사용 가능합니다. 우리의 프로젝트 페이지는 https://rbler1234.github.io/OSTBench.github.io/ 입니다.",
      "upvotes": 24,
      "discussionId": "687088a6c8391850d609787b",
      "projectPage": "https://rbler1234.github.io/OSTBench.github.io/",
      "githubRepo": "https://github.com/OpenRobotLab/OST-Bench",
      "ai_summary": "OST-Bench evaluates multimodal large language models in online spatio-temporal reasoning tasks, revealing challenges in handling complex spatial cues and long-term memory in real-world scenarios.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "OST-Bench",
        "Online Spatio-Temporal understanding",
        "ScanNet",
        "Matterport3D",
        "ARKitScenes",
        "complex spatio-temporal reasoning",
        "long-term memory retrieval"
      ],
      "githubStars": 36
    },
    "publishedAt": "2025-07-10T13:56:07.000Z",
    "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online\n  Spatio-temporal Scene Understanding",
    "summary": "Recent advances in multimodal large language models (MLLMs) have shown\nremarkable capabilities in integrating vision and language for complex\nreasoning. While most existing benchmarks evaluate models under offline\nsettings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\nbenchmark designed to evaluate Online Spatio-Temporal understanding from the\nperspective of an agent actively exploring a scene. The Online aspect\nemphasizes the need to process and reason over incrementally acquired\nobservations, while the Spatio-Temporal component requires integrating current\nvisual inputs with historical memory to support dynamic spatial reasoning.\nOST-Bench better reflects the challenges of real-world embodied perception.\nBuilt on an efficient data collection pipeline, OST-Bench consists of 1.4k\nscenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\nARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\nthey fall short on tasks requiring complex spatio-temporal reasoning. Under the\nonline setting, their accuracy declines as the exploration horizon extends and\nthe memory grows. Through further experimental analysis, we identify common\nerror patterns across models and find that both complex clue-based spatial\nreasoning demands and long-term memory retrieval requirements significantly\ndrop model performance along two separate axes, highlighting the core\nchallenges that must be addressed to improve online embodied reasoning. To\nfoster further research and development in the field, our codes, dataset, and\nbenchmark are available. Our project page is:\nhttps://rbler1234.github.io/OSTBench.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6433aba4546e16f17a0f19f6/pNjo9cepo_BTSUgc_ZLsg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6433aba4546e16f17a0f19f6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433aba4546e16f17a0f19f6/3hlNy4_Suy0bO__Qa9amL.jpeg",
      "fullname": "Chenming Zhu",
      "name": "ChaimZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07990",
      "authors": [
        {
          "_id": "68708156c8391850d6097869",
          "user": {
            "_id": "6513030fb3a463e17df56edd",
            "avatarUrl": "/avatars/867bd4316b2de758654ad3a84ea868c1.svg",
            "isPro": false,
            "fullname": "Hyun, Jeongseok",
            "user": "js-hyun",
            "type": "user"
          },
          "name": "Jeongseok Hyun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:01.307Z",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d609786a",
          "name": "Sukjun Hwang",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d609786b",
          "name": "Su Ho Han",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d609786c",
          "name": "Taeoh Kim",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d609786d",
          "name": "Inwoong Lee",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d609786e",
          "name": "Dongyoon Wee",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d609786f",
          "name": "Joon-Young Lee",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d6097870",
          "name": "Seon Joo Kim",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d6097871",
          "name": "Minho Shim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T17:59:02.000Z",
      "submittedOnDailyAt": "2025-07-11T05:04:15.008Z",
      "title": "다粒도 시공간 마킹 병합을 이용한 훈련없이 비디오 LLM 가속화",
      "submittedOnDailyBy": {
        "_id": "6513030fb3a463e17df56edd",
        "avatarUrl": "/avatars/867bd4316b2de758654ad3a84ea868c1.svg",
        "isPro": false,
        "fullname": "Hyun, Jeongseok",
        "user": "js-hyun",
        "type": "user"
      },
      "summary": "Video large language models (LLMs) achieve strong video understanding by leveraging a large number of spatio-temporal tokens, but suffer from quadratic computational scaling with token count. To address this, we propose a training-free spatio-temporal token merging method, named STTM. Our key insight is to exploit local spatial and temporal redundancy in video data which has been overlooked in prior work. STTM first transforms each frame into multi-granular spatial tokens using a coarse-to-fine search over a quadtree structure, then performs directed pairwise merging across the temporal dimension. This decomposed merging approach outperforms existing token reduction methods across six video QA benchmarks. Notably, STTM achieves a 2times speed-up with only a 0.5% accuracy drop under a 50% token budget, and a 3times speed-up with just a 2% drop under a 30% budget. Moreover, STTM is query-agnostic, allowing KV cache reuse across different questions for the same video. The project page is available at https://www.jshyun.me/projects/sttm.",
      "upvotes": 19,
      "discussionId": "68708157c8391850d6097872",
      "projectPage": "https://www.jshyun.me/projects/sttm",
      "githubRepo": "https://github.com/HYUNJS/STTM",
      "ai_summary": "A spatio-temporal token merging method improves video LLM efficiency by exploiting redundancy, achieving significant speed-ups with minimal accuracy loss.",
      "ai_keywords": [
        "spatio-temporal tokens",
        "quadratic computational scaling",
        "token merging method",
        "STTM",
        "multi-granular spatial tokens",
        "quadtree structure",
        "directed pairwise merging",
        "video QA benchmarks",
        "token budget",
        "query-agnostic",
        "KV cache reuse"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-07-10T13:59:02.000Z",
    "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
    "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2times speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3times speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07990.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6513030fb3a463e17df56edd",
      "avatarUrl": "/avatars/867bd4316b2de758654ad3a84ea868c1.svg",
      "fullname": "Hyun, Jeongseok",
      "name": "js-hyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07998",
      "authors": [
        {
          "_id": "687068dec8391850d60977e2",
          "user": {
            "_id": "62c66504031996c36c86976a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
            "isPro": true,
            "fullname": "steve z",
            "user": "stzhao",
            "type": "user"
          },
          "name": "Shitian Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:02:29.862Z",
          "hidden": false
        },
        {
          "_id": "687068dec8391850d60977e3",
          "user": {
            "_id": "67ff7f687351095d4b606b84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67ff7f687351095d4b606b84/KhNPmbBC3zghuP5h1MK-c.png",
            "isPro": false,
            "fullname": "Haoquan Zhang",
            "user": "haoquan03",
            "type": "user"
          },
          "name": "Haoquan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:02:26.572Z",
          "hidden": false
        },
        {
          "_id": "687068dec8391850d60977e4",
          "name": "Shaoheng Lin",
          "hidden": false
        },
        {
          "_id": "687068dec8391850d60977e5",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "687068dec8391850d60977e6",
          "name": "Qilong Wu",
          "hidden": false
        },
        {
          "_id": "687068dec8391850d60977e7",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "687068dec8391850d60977e8",
          "name": "Chen Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T17:59:55.000Z",
      "submittedOnDailyAt": "2025-07-11T00:33:22.135Z",
      "title": "PyVision: 동적인 도구를 보유한 에이전트식 시각\n\n(Note: The translation provided is a direct translation of the given text. The term \"Agentic\" is often used in AI and robotics to describe systems that exhibit autonomous or self-directed behavior, while \"Dynamic\" refers to something that is constantly changing or in motion. \"Vision\" is a common term for the ability to see or perceive visual information. The translation aims to capture these nuances while maintaining the original text's structure and meaning.)",
      "submittedOnDailyBy": {
        "_id": "62c66504031996c36c86976a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
        "isPro": true,
        "fullname": "steve z",
        "user": "stzhao",
        "type": "user"
      },
      "summary": "LLMs는 계획, 이유, 외부 도구의 동적인 호출을 가능하게 하는 에이전트로 증가하고 있습니다. 그러나 시각적 이유로선, 기존의 접근 방식은 주로 특정 작업 흐름과 정적 도구 세트에 제한되어 있습니다. 본 보고서에서는 PyVision라는 상호작용적이고 단계별 프레임워크를 제안합니다. 이는 MLLM이 순서에 맞는 Python 기반의 도구를 자동으로 생성, 실행, 개선할 수 있게 하고, 유연하고 해석 가능한 문제 해결을 해제합니다. PyVision이 생성한 도구의 트래킹 로직을 개발하고 다양한 벤치마크에서 사용에 대한 분석을 진행합니다. 양적으로는, PyVision은 안정적인 성능 향상을 달성했으며, V*에서 GPT-4.1을 +7.8% 상승, VLMsAreBlind-mini에서 Claude-4.0-Sonnet을 +31.1% 상승시켰습니다. 이러한 결과를 통해, 다양한 변화를 보여주고 있습니다: 동적인 도구 설계는 모델이 도구를 사용하는 것만으로는 아니지만, 도구를 개발할 수 있게 하고, 더 에이전트적인 시각적 이유로 진화하는 것을 보여줍니다.",
      "upvotes": 17,
      "discussionId": "687068dec8391850d60977e9",
      "projectPage": "https://agent-x.space/pyvision/",
      "githubRepo": "https://github.com/agents-x-project/PyVision",
      "ai_summary": "PyVision, an interactive framework, enables LLMs to autonomously create and refine Python-based tools for visual reasoning, achieving significant performance improvements across benchmarks.",
      "ai_keywords": [
        "LLMs",
        "agents",
        "visual reasoning",
        "predefined workflows",
        "static toolsets",
        "interactive framework",
        "multi-turn framework",
        "autonomously generate",
        "execute",
        "refine",
        "Python-based tools",
        "taxonomy",
        "benchmarks",
        "GPT-4.1",
        "Claude-4.0-Sonnet",
        "V*",
        "VLMsAreBlind-mini",
        "dynamic tooling",
        "agentic visual reasoning"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-07-10T13:59:55.000Z",
    "title": "PyVision: Agentic Vision with Dynamic Tooling",
    "summary": "LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07998.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c66504031996c36c86976a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
      "fullname": "steve z",
      "name": "stzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07982",
      "authors": [
        {
          "_id": "68707ef2c8391850d6097860",
          "user": {
            "_id": "6590f7880c993129053a2344",
            "avatarUrl": "/avatars/d08049493234edb8e23f1c1531e386d3.svg",
            "isPro": false,
            "fullname": "Haoyu wu",
            "user": "Haoyuwu",
            "type": "user"
          },
          "name": "Haoyu Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:05.153Z",
          "hidden": false
        },
        {
          "_id": "68707ef2c8391850d6097861",
          "name": "Diankun Wu",
          "hidden": false
        },
        {
          "_id": "68707ef2c8391850d6097862",
          "user": {
            "_id": "619b7b1cab4c7b7f16a7d59e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619b7b1cab4c7b7f16a7d59e/6TvXaAqBghAMYO1-j5l4v.jpeg",
            "isPro": false,
            "fullname": "Tianyu He",
            "user": "deeptimhe",
            "type": "user"
          },
          "name": "Tianyu He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:03.023Z",
          "hidden": false
        },
        {
          "_id": "68707ef2c8391850d6097863",
          "name": "Junliang Guo",
          "hidden": false
        },
        {
          "_id": "68707ef2c8391850d6097864",
          "name": "Yang Ye",
          "hidden": false
        },
        {
          "_id": "68707ef2c8391850d6097865",
          "name": "Yueqi Duan",
          "hidden": false
        },
        {
          "_id": "68707ef2c8391850d6097866",
          "name": "Jiang Bian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T17:55:08.000Z",
      "submittedOnDailyAt": "2025-07-11T02:06:14.101Z",
      "title": "ジェオメトリー 중심: 영상 분할과 3D 표현의 결합으로 일관된 세계 모델링",
      "submittedOnDailyBy": {
        "_id": "6577fba2eb02736add6377f5",
        "avatarUrl": "/avatars/3e486dd36021feaa4a0259cd89c1eee9.svg",
        "isPro": false,
        "fullname": "Wu",
        "user": "Diankun",
        "type": "user"
      },
      "summary": "동영상은 동적인 3차원 세계를 2차원으로 투영하여 표현된다. 그러나 우리의 분석에 따르면, 단순히 널리 있는 동영상 데이터로 훈련된 동영상 확산 모델은 학습된 표현에 의미 있는 3차원 기하학적 구조를 인식할 수 있는 경우가 많다. 이러한 동영상 확산 모델과 물리적인 세계의 잠재적인 3차원 특성 사이의 간극을 메우는 데, 우리는 Geometry Forcing을 제안한다. 이는 간단하고 효과적인 방법이며, 동영상 확산 모델이 잠재적인 3차원 표현을 내부적으로 인식하도록 유도하는 데 사용된다. 우리의 주요 견해는, 예측된 기하학적 기저 모델의 특징을 사용하여 모델의 중간 표현을 기하학적 구조에 맞추어 조정하는 것이다. 여기서, 우리는 두 가지 보간을 위한 조정 목표를 도입한다: Angular Alignment는 코사인 유사도를 사용하여 방향 일관성을 강제하고, Scale Alignment는 정규화된 확산 표현으로부터 비정규화된 기하학적 특징량의 회귀를 통해 스케일 관련 정보를 보존하는 것이다. Geometry Forcing은 카메라의 관점 조건과 행동 조건에 따른 동영상 생성 태스크에서 평가된다. 실험 결과는, baseline 모델에 비해 시각적 품질과 3차원 일관성이 크게 향상된 것을 보여주며, 프로젝트 페이지는 https://GeometryForcing.github.io이다.",
      "upvotes": 16,
      "discussionId": "68707ef2c8391850d6097867"
    },
    "publishedAt": "2025-07-10T13:55:08.000Z",
    "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for\n  Consistent World Modeling",
    "summary": "Videos inherently represent 2D projections of a dynamic 3D world. However,\nour analysis suggests that video diffusion models trained solely on raw video\ndata often fail to capture meaningful geometric-aware structure in their\nlearned representations. To bridge this gap between video diffusion models and\nthe underlying 3D nature of the physical world, we propose Geometry Forcing, a\nsimple yet effective method that encourages video diffusion models to\ninternalize latent 3D representations. Our key insight is to guide the model's\nintermediate representations toward geometry-aware structure by aligning them\nwith features from a pretrained geometric foundation model. To this end, we\nintroduce two complementary alignment objectives: Angular Alignment, which\nenforces directional consistency via cosine similarity, and Scale Alignment,\nwhich preserves scale-related information by regressing unnormalized geometric\nfeatures from normalized diffusion representation. We evaluate Geometry Forcing\non both camera view-conditioned and action-conditioned video generation tasks.\nExperimental results demonstrate that our method substantially improves visual\nquality and 3D consistency over the baseline methods. Project page:\nhttps://GeometryForcing.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07982.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6577fba2eb02736add6377f5",
      "avatarUrl": "/avatars/3e486dd36021feaa4a0259cd89c1eee9.svg",
      "fullname": "Wu",
      "name": "Diankun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07136",
      "authors": [
        {
          "_id": "68708cb9c8391850d609788f",
          "name": "Wanhua Li",
          "hidden": false
        },
        {
          "_id": "68708cb9c8391850d6097890",
          "name": "Yujie Zhao",
          "hidden": false
        },
        {
          "_id": "68708cb9c8391850d6097891",
          "name": "Minghan Qin",
          "hidden": false
        },
        {
          "_id": "68708cb9c8391850d6097892",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "68708cb9c8391850d6097893",
          "name": "Yuanhao Cai",
          "hidden": false
        },
        {
          "_id": "68708cb9c8391850d6097894",
          "name": "Chuang Gan",
          "hidden": false
        },
        {
          "_id": "68708cb9c8391850d6097895",
          "name": "Hanspeter Pfister",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/tfzmPoFtikY2cpVZVd5Rt.mp4"
      ],
      "publishedAt": "2025-07-09T00:19:58.000Z",
      "submittedOnDailyAt": "2025-07-11T02:43:05.954Z",
      "title": "LangSplatV2: 고차원 3D 언어 가우스 스플레드 기술로 450점 이상의 FPS를 실현합니다.",
      "submittedOnDailyBy": {
        "_id": "658bb7e47459b6e471b9d2e6",
        "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
        "isPro": false,
        "fullname": "Wanhua Li",
        "user": "EthanTaylor",
        "type": "user"
      },
      "summary": "이 논문에서는 LangSplatV2를 통해 고차원 특징 스플릿팅이 476.2 FPS로 실현되어, 고해상도 이미지에 대한 3D 开语詞 텍스트 검색이 384.6 FPS로 실현되어, LangSplat과 비교하여 42배의 속도 향상과 47배의 성능 향상을 얻으며, 더 나은 검색 정확도를 달성할 수 있습니다. LangSplat은 2D CLIP 언어 특징을 3D에 삽입하고, SAM의 의미로 정확한 3D 언어 필드를 학습하기 위해 가우시안 스플릿팅을 사용합니다. 이러한 3D 언어 필드의 발전은 복잡한 스케인 내의 언어 상호작용을 필요로 하는 애플리케이션에서 중요합니다. 그러나 LangSplat은 진화된 A100 GPU를 사용하더라도 실시간 추론 성능(8.2 FPS)을 달성하지 못하여, 광범위한 응용에 연결되지 않습니다. 이 논문에서는 LangSplat의 시간 분석을 자세히 수행하고, 가중 디코더가 주요 속도 저하 원인으로 식별되었습니다. 우리의 해결책은 LangSplatV2에서 각 가우시안이 글로벌 사전의 희소 코드로 기능하도록 가정하고, 3D 희소 계수 필드를 학습함으로써 가중 디코더의 필요성을 완전히 제거합니다. 이러한 희소성을 활용하여 CUDA 최적화를 이용한 효율적인 희소 계수 스플릿팅 방법을 제안하고, 고품질의 고차원 특징 맵을 그리는 것을 가능하게 하며, 스플릿팅의 초 저차원 특징에 해당하는 시간 비용이 부과되지 않도록 합니다. 실험 결과를 통해 LangSplatV2는 더 좋은 또는 경쟁적인 검색 정확도를 실현하고, 더 빠르게 동작합니다. 코드와 Demo는 프로젝트 페이지: https://langsplat-v2.github.io에서 제공됩니다.",
      "upvotes": 12,
      "discussionId": "68708cbac8391850d6097896",
      "ai_summary": "LangSplatV2 enhances 3D text querying speed and accuracy by replacing the heavyweight decoder with a sparse coefficient field and efficient CUDA optimization.",
      "ai_keywords": [
        "Gaussian Splatting",
        "CLIP language features",
        "3D language field",
        "SAM semantics",
        "sparse code",
        "sparse coefficient field",
        "sparse coefficient splatting",
        "CUDA optimization"
      ]
    },
    "publishedAt": "2025-07-08T20:19:58.000Z",
    "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+\n  FPS",
    "summary": "In this paper, we introduce LangSplatV2, which achieves high-dimensional\nfeature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6\nFPS for high-resolution images, providing a 42 times speedup and a 47\ntimes boost over LangSplat respectively, along with improved query accuracy.\nLangSplat employs Gaussian Splatting to embed 2D CLIP language features into\n3D, significantly enhancing speed and learning a precise 3D language field with\nSAM semantics. Such advancements in 3D language fields are crucial for\napplications that require language interaction within complex scenes. However,\nLangSplat does not yet achieve real-time inference performance (8.2 FPS), even\nwith advanced A100 GPUs, severely limiting its broader application. In this\npaper, we first conduct a detailed time analysis of LangSplat, identifying the\nheavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2\nassumes that each Gaussian acts as a sparse code within a global dictionary,\nleading to the learning of a 3D sparse coefficient field that entirely\neliminates the need for a heavyweight decoder. By leveraging this sparsity, we\nfurther propose an efficient sparse coefficient splatting method with CUDA\noptimization, rendering high-dimensional feature maps at high quality while\nincurring only the time cost of splatting an ultra-low-dimensional feature. Our\nexperimental results demonstrate that LangSplatV2 not only achieves better or\ncompetitive query accuracy but is also significantly faster. Codes and demos\nare available at our project page: https://langsplat-v2.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/tfzmPoFtikY2cpVZVd5Rt.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07136.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658bb7e47459b6e471b9d2e6",
      "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
      "fullname": "Wanhua Li",
      "name": "EthanTaylor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07202",
      "authors": [
        {
          "_id": "68707dfbc8391850d6097841",
          "user": {
            "_id": "659d164f4b29e5948c66b9f6",
            "avatarUrl": "/avatars/6b4531673a76f7d93b84402ecac74cbe.svg",
            "isPro": false,
            "fullname": "Mohamed Elmoghany",
            "user": "elmoghany",
            "type": "user"
          },
          "name": "Mohamed Elmoghany",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:12.603Z",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097842",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097843",
          "name": "Seunghyun Yoon",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097844",
          "name": "Subhojyoti Mukherjee",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097845",
          "name": "Eslam Bakr",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097846",
          "name": "Puneet Mathur",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097847",
          "name": "Gang Wu",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097848",
          "name": "Viet Dac Lai",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097849",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609784a",
          "name": "Ruiyi Zhang",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609784b",
          "name": "Varun Manjunatha",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609784c",
          "name": "Chien Nguyen",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609784d",
          "name": "Daksh Dangi",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609784e",
          "name": "Abel Salinas",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609784f",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Taesiri",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:08.440Z",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097850",
          "name": "Hongjie Chen",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097851",
          "name": "Xiaolei Huang",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097852",
          "name": "Joe Barrow",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097853",
          "name": "Nesreen Ahmed",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097854",
          "name": "Hoda Eldardiry",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097855",
          "name": "Namyong Park",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097856",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097857",
          "name": "Jaemin Cho",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097858",
          "name": "Anh Totti Nguyen",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097859",
          "name": "Zhengzhong Tu",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609785a",
          "name": "Thien Nguyen",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609785b",
          "name": "Dinesh Manocha",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609785c",
          "name": "Mohamed Elhoseiny",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609785d",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:10.400Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T18:20:33.000Z",
      "submittedOnDailyAt": "2025-07-11T01:30:02.686Z",
      "title": "장 비디오의 이야기 생성에 관한 조사： 아키텍처, 일관성 및 영화 품질",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "전자 비디오 생성 모델에서 적절한 발전이 이루어진에도 불구하고, 현재의 최尖端 방법들은 주로 5-16초의 긴 전자 비디오를 생성할 수 있으며, 이 것을 \"긴 전자 비디오\"로 주로 다루고 있습니다. 또한 16초 이상의 전자 비디오는 모든 노트에서 일치하는 인물의 외모와 공간의 배치를 유지하는 것이 어렵습니다. 특히, 여러 에피소드의 긴 전자 비디오는 인물의 일관성과 동작의 일관성을 유지하는 것이 어렵습니다. 반면에 일부 방법은 150초의 긴 전자 비디오를 생성할 수 있지만, 이들은 일반적으로 프레임의冗長성과 시간의 다양성 저하로 부정적으로 영향을 받습니다. 최근의 연구에서는 여러 에피소드의 긴 전자 비디오를 생성하고 설명의 일관성과 고품질의 세부 정보를 포함하는 것을 시도하고 있습니다. 우리는 전자 비디오 생성에 관한 32편의 논문을 검토하고, 이러한 질을 일관시키게 되는 핵심적인 기술적 구성 요소와 훈련 전략을 특정했습니다. 또한, 우리는 현재의 방법에 대한 상세한 새로운 기술을 구축하고, 기술적 설계와 성능의 특징을 바탕으로 논문을 분류한 비교 표를 제공합니다.",
      "upvotes": 9,
      "discussionId": "68707dfcc8391850d609785e"
    },
    "publishedAt": "2025-07-09T14:20:33.000Z",
    "title": "A Survey on Long-Video Storytelling Generation: Architectures,\n  Consistency, and Cinematic Quality",
    "summary": "Despite the significant progress that has been made in video generative\nmodels, existing state-of-the-art methods can only produce videos lasting 5-16\nseconds, often labeled \"long-form videos\". Furthermore, videos exceeding 16\nseconds struggle to maintain consistent character appearances and scene layouts\nthroughout the narrative. In particular, multi-subject long videos still fail\nto preserve character consistency and motion coherence. While some methods can\ngenerate videos up to 150 seconds long, they often suffer from frame redundancy\nand low temporal diversity. Recent work has attempted to produce long-form\nvideos featuring multiple characters, narrative coherence, and high-fidelity\ndetail. We comprehensively studied 32 papers on video generation to identify\nkey architectural components and training strategies that consistently yield\nthese qualities. We also construct a comprehensive novel taxonomy of existing\nmethods and present comparative tables that categorize papers by their\narchitectural designs and performance characteristics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07202.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06543",
      "authors": [
        {
          "_id": "686f9aad706a6ea465418a08",
          "user": {
            "_id": "67c6a1e75e2443d7d5f85cb3",
            "avatarUrl": "/avatars/0569b368520411ab828d46725bc3896a.svg",
            "isPro": false,
            "fullname": "Taekyung Kim",
            "user": "taekyung-k",
            "type": "user"
          },
          "name": "Taekyung Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:03:22.401Z",
          "hidden": false
        },
        {
          "_id": "686f9aad706a6ea465418a09",
          "user": {
            "_id": "660f8cc1a61244f3df3d4426",
            "avatarUrl": "/avatars/45d59766122bb3482f6dd7f9d98aa87a.svg",
            "isPro": false,
            "fullname": "Dongyoon Han",
            "user": "calintz",
            "type": "user"
          },
          "name": "Dongyoon Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:03:14.509Z",
          "hidden": false
        },
        {
          "_id": "686f9aad706a6ea465418a0a",
          "user": {
            "_id": "64b9feed96676e40d0fa89a7",
            "avatarUrl": "/avatars/2154a6ceb87677ad2c9d9620de5b18ec.svg",
            "isPro": false,
            "fullname": "Byeongho Heo",
            "user": "bhheo",
            "type": "user"
          },
          "name": "Byeongho Heo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:03:16.434Z",
          "hidden": false
        },
        {
          "_id": "686f9aad706a6ea465418a0b",
          "name": "Jeongeun Park",
          "hidden": false
        },
        {
          "_id": "686f9aad706a6ea465418a0c",
          "name": "Sangdoo Yun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T04:57:29.000Z",
      "submittedOnDailyAt": "2025-07-11T04:21:34.394Z",
      "title": "トークン 봇킹: 1 토큰으로 메모리 다이나믹스를 기억합니다.",
      "submittedOnDailyBy": {
        "_id": "64b9feed96676e40d0fa89a7",
        "avatarUrl": "/avatars/2154a6ceb87677ad2c9d9620de5b18ec.svg",
        "isPro": false,
        "fullname": "Byeongho Heo",
        "user": "bhheo",
        "type": "user"
      },
      "summary": "동적인 시선에서 압축적이고 시간적 관심 있는 시각적 표현을 추출하는 것은 시각적 추적, 로봇 조작 등 순차적인 시선 이해 태스크의 성공적인 실행에 중요합니다. 본 논문에서는 간단하고 직관적인 자동 경학 학습 파이프라인 \"Token Bottleneck (ToBo)\"를 소개합니다. 이 파이프라인은 시선을 돋보 토큰으로 압축하고, 최소한의 패치를 힌트로 사용하여 후속의 시선을 예측합니다. ToBo 파이프라인은 참고 시선을 압축된 돋보 토큰으로 압축하고, 순차적인 시선 표현 학습을 촉진합니다. 확장 단계에서 돋보 토큰과 일부 타겟 패치를 힌트로 사용하여 모델을 시간적 동작을 이해하도록 가이드합니다. 이 설계는 시각적 백본에 시간적 의존성을 내장하여 시선 간 동적인 트랜지션을 이해할 수 있습니다. 다양한 순차적 태스크의 확장 실험, 비디오 라벨 전파 및 로봇 조작 기록 환경에서 실험을 수행하여 ToBo의 우수한 성능을 보여주었습니다. 또한 물리적인 로봇에 플레이드라인 모델을 구현하고, 실세계 환경에서 강건성과 효율성을 확인했습니다. 또한 ToBo의 확장성을 다양한 모델 크기의 폭에 미치는 영향을 한 번 더 평가했습니다.",
      "upvotes": 9,
      "discussionId": "686f9aae706a6ea465418a0d",
      "projectPage": "https://token-bottleneck.github.io",
      "githubRepo": "https://github.com/naver-ai/tobo",
      "ai_summary": "ToBo is a self-supervised learning method that creates compact, temporally aware visual representations for sequential scene understanding tasks, outperforming baselines in both simulated and real-world environments.",
      "ai_keywords": [
        "Token Bottleneck",
        "self-supervised learning",
        "bottleneck token",
        "sequential scene representations",
        "temporal dependencies",
        "video label propagation",
        "robot manipulation"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-09T00:57:29.000Z",
    "title": "Token Bottleneck: One Token to Remember Dynamics",
    "summary": "Deriving compact and temporally aware visual representations from dynamic\nscenes is essential for successful execution of sequential scene understanding\ntasks such as visual tracking and robotic manipulation. In this paper, we\nintroduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised\nlearning pipeline that squeezes a scene into a bottleneck token and predicts\nthe subsequent scene using minimal patches as hints. The ToBo pipeline\nfacilitates the learning of sequential scene representations by conservatively\nencoding the reference scene into a compact bottleneck token during the squeeze\nstep. In the expansion step, we guide the model to capture temporal dynamics by\npredicting the target scene using the bottleneck token along with few target\npatches as hints. This design encourages the vision backbone to embed temporal\ndependencies, thereby enabling understanding of dynamic transitions across\nscenes. Extensive experiments in diverse sequential tasks, including video\nlabel propagation and robot manipulation in simulated environments demonstrate\nthe superiority of ToBo over baselines. Moreover, deploying our pre-trained\nmodel on physical robots confirms its robustness and effectiveness in\nreal-world environments. We further validate the scalability of ToBo across\ndifferent model scales.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06543.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64b9feed96676e40d0fa89a7",
      "avatarUrl": "/avatars/2154a6ceb87677ad2c9d9620de5b18ec.svg",
      "fullname": "Byeongho Heo",
      "name": "bhheo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07996",
      "authors": [
        {
          "_id": "68709572c8391850d60978b7",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "68709572c8391850d60978b8",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "68709572c8391850d60978b9",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T07:59:58.984Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T17:59:53.000Z",
      "submittedOnDailyAt": "2025-07-11T03:54:10.527Z",
      "title": "スキップレイヤー 아니면 루프? 테스트타임의 깊이 어댑터 프레딕트 레인드 LLMs",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "プレトレーンドニューラルネットワーク가 경험 후의 훈련에 의해 입력의 차이에 적응할 수 있는지 확인하는 것이 가능합니다? 단순한 태스크에서는 모든 레이어가 필요하며, 어려운 태스크에서는 충분한지 확인하는 것이 필요합니다. 우리는 プレトレーンド대규모 언어 모델(LLM)의 레이어들을 개별 모듈로 취급하여, 각 테스트 샘플에 대해 더 잘 적합한 큰 모델을 구축할 수 있음을 확인했습니다. 특히, プレトレーンド 모델의 각 레이어는 RNN(循環ニューラルネットワーク)로 스킵되거나 제거되거나, 여러 번 반복될 수 있으며, 임의의 순서로 다른 레이어와 스택으로 연결될 수 있으며, 각 샘플에 대해 연결된 레이어(CoLa)를 생성할 수 있습니다. 이 구성 공간은 기존의 연구의 범위를 크게 확장합니다. MCTS(몬테카를로 트리 탐색) 프로토콜을 개발하여, 수학과 일반 지식 논리 벤치마크에서 각 샘플에 대해 최적의 CoLa를 탐색하고 식별합니다. 고정 깊이의 정적 모델과 비교하여, CoLa는 짧은 패스(빠른 사고방식), 같은 레이어(サンス)의 재현(느린 사고방식), 그리고 두 가지의 조합으로, 입력에 맞는 유연하고 동적인 아키텍처를 제공합니다. MCTS 최적화된 CoLa에 대해 검토하여, 두 가지 주요 발견이 있었습니다: (1) 기본적으로 올바르게 예측한 샘플의 75% 이상에 대해, 짧은 패스를 가진 CoLa가 발견되어, 추론의 효율화를 크게 여겨지는 공간이 있음을 시사합니다; (2) 기본적으로 올바르지 않게 예측한 샘플의 60% 이상에 대해, 올바른 예측을 달성하는 CoLa가 발견되어, 성능 향상의 큰 공간이 시사됩니다. 이러한 결과를 통해, 고정 아키텍처의 プレトレーンド LLM이 다른 샘플에 대해 추론할 때의 단점을 명확히 하고, 측정 시의 깊이 변경의 일반화 능력을 해제하는 데 연결됩니다.",
      "upvotes": 8,
      "discussionId": "68709573c8391850d60978ba",
      "ai_summary": "A method using chain-of-layers (CoLa) derived from a pretrained large language model allows for dynamic architecture adaptation, improving efficiency and accuracy across diverse tasks through selective layer manipulation and Monte Carlo Tree Search optimization.",
      "ai_keywords": [
        "pretrained large language model (LLM)",
        "chain-of-layers (CoLa)",
        "Monte Carlo Tree Search (MCTS)",
        "looped/recurrent pretrained modules",
        "layer pruning",
        "early-exit networks",
        "test-time depth adaptation"
      ]
    },
    "publishedAt": "2025-07-10T13:59:53.000Z",
    "title": "Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs",
    "summary": "Can a pretrained neural network adapt its architecture to different inputs\nwithout any finetuning? Do we need all layers for simple tasks, and are they\nadequate for challenging tasks? We found that the layers of a pretrained large\nlanguage model (LLM) can be manipulated as separate modules to build a better\nand even shallower model customized for each test sample. In particular, each\nlayer from the pretrained model can be skipped/pruned or repeated multiple\ntimes as recurrent neural networks (RNN), and stacked with others in arbitrary\norders, yielding a chain-of-layers (CoLa) per sample. This compositional space\ngreatly expands the scope of existing works on looped/recurrent pretrained\nmodules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree\nSearch (MCTS) protocol to explore and identify the optimal CoLa for each sample\nfrom math and commonsense reasoning benchmarks. Compared to a static model of a\nfixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same\nlayer(s) (slow thinking), and combining both, offering more flexible, dynamic\narchitectures for different inputs. We conduct an extensive analysis of the\nMCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples\nwith correct predictions by the original LLM, we can find shorter CoLa,\nsuggesting a large space for improving inference efficiency; (2) For >60% of\nsamples with originally incorrect predictions, we can identify CoLa achieving\ncorrect predictions, suggesting a large space of performance enhancement. Our\nresults highlight the shortcomings of using a fixed architecture of pre-trained\nLLMs for inference on different samples and pave the way to unlock the\ngeneralization power of test-time depth adaptation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07484",
      "authors": [
        {
          "_id": "68708d7ac8391850d609789f",
          "name": "Kaiqu Liang",
          "hidden": false
        },
        {
          "_id": "68708d7ac8391850d60978a0",
          "name": "Haimin Hu",
          "hidden": false
        },
        {
          "_id": "68708d7ac8391850d60978a1",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "68708d7ac8391850d60978a2",
          "name": "Dawn Song",
          "hidden": false
        },
        {
          "_id": "68708d7ac8391850d60978a3",
          "name": "Thomas L. Griffiths",
          "hidden": false
        },
        {
          "_id": "68708d7ac8391850d60978a4",
          "name": "Jaime Fernández Fisac",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T07:11:57.000Z",
      "submittedOnDailyAt": "2025-07-11T02:36:07.699Z",
      "title": "머신배지샷：대규모 언어 모델에서 진실의 무시의 특징화",
      "submittedOnDailyBy": {
        "_id": "6275a465597c70eb8949fce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
        "isPro": false,
        "fullname": "Xuandong Zhao",
        "user": "Xuandong",
        "type": "user"
      },
      "summary": "바우스히트, 포스트랜드의 철학자 헤리・프란키ン그에 의해 개념화된 것은, 사실의 가치에 대한 고려를 하지 않고 설명할 수 있는 것을 지칭합니다. 선행의 연구는 대규모 언어 모델(LLM)의 해리네샬과 서브피어샹에 대한 조사를 진행 중이었지만, 우리는 '마시ン 바우스히트'라는 개념적인 프레임워크를 제안하고, 연구자들이 LLM의 사실성 손실을 특징화할 수 있는 기초적인 구조를 밝혀내는 것을 목표로 합니다. 우리는 '바우스히트 인덱스'라는 새로운 메트릭을 소개하고, LLM의 사실성에 대한 불변성을 정량화하고, 공언, 파터링, 워셀워드, 균형없는 주장 등 네 가지 질적 바우스히트의 형식을 분석할 수 있는 타ク시나미를 제안합니다. 시장플레이스 데이터 세트, 정치적 중립 데이터 세트, 그리고 우리의 새로운 바우스히트 에바르벤치마크(2,400개의 시나리오로 100개의 AI 보조기를 경험)에 대해 실험적 평가를 실시했습니다. 이러한 결과는, RLHF에 의한 모델의 微調校은 바우스히트를 뚜렷하게 확대하고, 추론 시의 체인오프 시그널 최적화는 특히 공언과 파터링의 특정 바우스히트의 형식을 뚜렷하게 확대합니다. 또한 정치적 맥락에서, 워셀워드는 주요 전략으로 사용됩니다. 이러한 발견은 AI의 일관성의 체계적인 문제를 밝혀내고, 사실성이 있는 LLM의 행동을 목표로 하는 새로운 시각을 제공합니다.",
      "upvotes": 3,
      "discussionId": "68708d7ac8391850d60978a5",
      "ai_summary": "Machine bullshit, characterized by LLMs' indifference to truth, is quantified and analyzed through a new framework, revealing that RLHF and CoT prompting exacerbate certain bullshit forms.",
      "ai_keywords": [
        "Bullshit Index",
        "reinforcement learning from human feedback (RLHF)",
        "chain-of-thought (CoT) prompting",
        "empty rhetoric",
        "paltering",
        "weasel words",
        "unverified claims",
        "AI alignment"
      ]
    },
    "publishedAt": "2025-07-10T03:11:57.000Z",
    "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in\n  Large Language Models",
    "summary": "Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to\nstatements made without regard to their truth value. While previous work has\nexplored large language model (LLM) hallucination and sycophancy, we propose\nmachine bullshit as an overarching conceptual framework that can allow\nresearchers to characterize the broader phenomenon of emergent loss of\ntruthfulness in LLMs and shed light on its underlying mechanisms. We introduce\nthe Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and\npropose a complementary taxonomy analyzing four qualitative forms of bullshit:\nempty rhetoric, paltering, weasel words, and unverified claims. We conduct\nempirical evaluations on the Marketplace dataset, the Political Neutrality\ndataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI\nassistants) explicitly designed to evaluate machine bullshit. Our results\ndemonstrate that model fine-tuning with reinforcement learning from human\nfeedback (RLHF) significantly exacerbates bullshit and inference-time\nchain-of-thought (CoT) prompting notably amplify specific bullshit forms,\nparticularly empty rhetoric and paltering. We also observe prevalent machine\nbullshit in political contexts, with weasel words as the dominant strategy. Our\nfindings highlight systematic challenges in AI alignment and provide new\ninsights toward more truthful LLM behavior.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07484.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6275a465597c70eb8949fce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
      "fullname": "Xuandong Zhao",
      "name": "Xuandong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07574",
      "authors": [
        {
          "_id": "6870c2d3c8391850d60978e6",
          "user": {
            "_id": "63aadfe9a4bdd629b7ea7692",
            "avatarUrl": "/avatars/9cc4eb5d4090ce84a590ec195b70e545.svg",
            "isPro": false,
            "fullname": "Enrico",
            "user": "envomp",
            "type": "user"
          },
          "name": "Enrico Vompa",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T07:59:54.331Z",
          "hidden": false
        },
        {
          "_id": "6870c2d3c8391850d60978e7",
          "name": "Tanel Tammet",
          "hidden": false
        },
        {
          "_id": "6870c2d3c8391850d60978e8",
          "name": "Mohit Vaishnav",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T09:23:32.000Z",
      "submittedOnDailyAt": "2025-07-11T06:38:11.205Z",
      "title": "라이너 세파라티비리티의 천지 위로 올라가다",
      "submittedOnDailyBy": {
        "_id": "63aadfe9a4bdd629b7ea7692",
        "avatarUrl": "/avatars/9cc4eb5d4090ce84a590ec195b70e545.svg",
        "isPro": false,
        "fullname": "Enrico",
        "user": "envomp",
        "type": "user"
      },
      "summary": "가장 선진적인 Visual-Language Models (VLMs)는 시각적인 정보의 선형 분리성을 추상적인 논리구조에 의해 제한될 것으로 보입니다. 본 연구에서는 이러한 \"선형적인 이유의 붕어빵\"을 조사하고, VLM의 시각적인 정보에 대한 단순한 선형 분류기 성능인 Linear Separability Ceiling (LSC)를 제시합니다. 우리는 이 붕어빵이 광범위하게 존재하며, 시각적 인식의 저하가 아니라 언어 모델의 이유의 경로의 실패로 인한 것입니다. 이 문제는 해결할 수 있는 어레이멘트 문제임을 보여줍니다. 그러나 필요한 인핌은 태스크에 따라 다릅니다: 의미적인 개념에 대해 기존의 경로를 활성화하면 충분하지만, 복잡한 관계적인 이유론은 모델의 핵심 가중치 조정이 필요합니다. 후속 조정을 방법론적 제어로 사용함으로써, VLM 내에 강력한, 잠정적으로 있는 이유의 경로가 있음을 강력히 입증합니다. 그러나 복잡한 관계적인 태스크에 필요한 깊은 조정을 요구하는 경우, 표현의 질을 명시적으로 향상시키려 하더라도, 모델이 실패하는 새로운 Prompt 형식으로 모델의 정보가 잘 분리될 수 있다는 것을 반전시키게 됩니다. 최종적으로, 본 연구는 VLM 분석에 새로운 렌즈를 제공하며, 강력한 이유론은 목표적 어레이멘트이며, 단순히 표현 학습의 향상이 아니라는 것을 보여줍니다.",
      "upvotes": 2,
      "discussionId": "6870c2d4c8391850d60978e9",
      "githubRepo": "https://github.com/envomp/Beyond-the-Linear-Separability-Ceiling",
      "ai_summary": "The study identifies a linear reasoning bottleneck in Visual-Language Models and proposes the Linear Separability Ceiling as a metric to evaluate it, suggesting targeted alignment rather than improved representation learning as a solution.",
      "ai_keywords": [
        "Visual-Language Models",
        "VLMs",
        "linear separability",
        "Linear Separability Ceiling",
        "linear classifier",
        "visual embeddings",
        "abstract reasoning tasks",
        "reasoning pathways",
        "postfix tuning",
        "representation quality",
        "prompt formats"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-07-10T05:23:32.000Z",
    "title": "Beyond the Linear Separability Ceiling",
    "summary": "Most state-of-the-art Visual-Language Models (VLMs) are seemingly limited by\nthe linear separabilty of their visual embeddings on abstract reasoning tasks.\nThis work investigates this \"linear reasoning bottleneck\" by introducing the\nLinear Separability Ceiling (LSC), the performance of a simple linear\nclassifier on a VLM's visual embeddings. We find this bottleneck is widespread\nand stems not from poor perception, but from failures in the language model's\nreasoning pathways. We demonstrate this is a solvable alignment issue. The\nrequired intervention, however, is task-dependent: activating existing pathways\nsuffices for semantic concepts, while complex relational reasoning requires\nadapting core model weights. Using postfix tuning as a methodological control,\nwe find strong evidence for powerful, dormant reasoning pathways within VLMs.\nHowever, for complex relational tasks requiring deeper adaptation, explicitly\nimproving representation quality causes the model to fail on new prompt formats\ndespite its embeddings remaining well separated. Ultimately, this work provides\na new lens for VLM analysis, showing that robust reasoning is a matter of\ntargeted alignment, not simply improved representation learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07574.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63aadfe9a4bdd629b7ea7692",
      "avatarUrl": "/avatars/9cc4eb5d4090ce84a590ec195b70e545.svg",
      "fullname": "Enrico",
      "name": "envomp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05241",
      "authors": [
        {
          "_id": "6870a7c6c8391850d60978ca",
          "name": "Jingyi Chai",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978cb",
          "name": "Shuo Tang",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978cc",
          "name": "Rui Ye",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978cd",
          "name": "Yuwen Du",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978ce",
          "name": "Xinyu Zhu",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978cf",
          "name": "Mengcheng Zhou",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978d0",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978d1",
          "name": "Weinan E",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978d2",
          "name": "Yuzhi Zhang",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978d3",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978d4",
          "name": "Siheng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T17:50:52.000Z",
      "submittedOnDailyAt": "2025-07-11T04:29:26.187Z",
      "title": "과학 AI 에이전트에 대한 접근 방법, 첫 번째 장. X-Master를 기반으로: 인간이 마지막의 시험에서 우리가 선두에 서야 하는가?",
      "submittedOnDailyBy": {
        "_id": "62d22496c58f969c152bcefd",
        "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
        "isPro": false,
        "fullname": "Tiezhen WANG",
        "user": "xianbao",
        "type": "user"
      },
      "summary": "AI 에이전트의 급격한 발전은 과학의 발견을 빠르게 촉진하기 위해 오랜 시간 간고하게 간직해온 야망을 불어들었다. 이 목표를 달성하기 위해 인간 지식의 선두에 깊은 이해가 필요합니다. 따라서 「인간의 최종 시험」(HLE)은 과학의 AI 에이전트의 평가에 특히 어려운 지표를 제공받는다. 이 연구에서는 일반적인 AI 에이전트의 기본 아키텍처를 구축하고, HLE에서 선두적인 실적을 통해 그 능력을 증명하고자 합니다. 이를 달성하기 위해, X-Master와 외부 도구를 유연하게 상호작용하는 인간 연구자를 모방하는 로직 에이전트를 사용합니다. 이 에이전트는 코드를 인터랙티브 언어로 생각하는 개념화에 의해 구축된 Python 라이브러리와 사용자 정의된 도구를 유연하게 사용할 수 있도록 설계되어 있습니다. 또한 X-Master와 확장된 에이전트 워크플로우를 통해 사유의 폭과 깊이를 체계적으로 향상시키고 그 능력을 확장하는 데 사용됩니다. 우리의 오픈 소스 솔루션, X-Masters는 HLE에서 32.1%의 점수로 새로운 최단거리 레벨을 설정하고, OpenAI와 Google의 Deep Research(26.6%와 26.9%)를 초과하여 30%의 슈리드(슈리드: 특정 기준을 초과하는 표현)를 처음으로 초과하는 것을 실현했습니다. 이 연구는 복잡한 태스크 해결에 대한 이해를 깊게 하고, 미래의 발전에 도움이 되는 유익한 경험을 모은 동시에 다음 모델 훈련에도 지도를 제공할 수 있습니다.",
      "upvotes": 1,
      "discussionId": "6870a7c6c8391850d60978d5"
    },
    "publishedAt": "2025-07-07T13:50:52.000Z",
    "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I.\n  X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
    "summary": "The rapid advancements of AI agents have ignited the long-held ambition of\nleveraging them to accelerate scientific discovery. Achieving this goal\nrequires a deep understanding of the frontiers of human knowledge. As such,\nHumanity's Last Exam (HLE) provides an exceptionally challenging touchstone for\nevaluating scientific AI agents. In this work, we aim to construct the\nfoundational architecture for general-purpose agents and validate the\ncapabilities through leading performance on HLE. To achieve this, we introduce\nX-Master, a tool-augmented reasoning agent designed to emulate human\nresearchers by interacting flexibly with external tools during its reasoning\nprocess. This agent, guided by the conceptualization of code as an interaction\nlanguage, can flexibly leverage built-in Python libraries and our customized\ntools to augment the reasoning. We further scale its capabilities through\nX-Masters, a scattered-and-stacked agentic workflow that systematically\nenhances breadth and depth of reasoning. Our open-source solution, X-Masters,\nsets a new state-of-the-art record on HLE with a score of 32.1%, surpassing\nOpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to\nexceed the 30% threshold. This work allows us to gain a deeper understanding of\ncomplex task-solving and accumulates valuable experience that can inform future\nadvancements, guiding subsequent model training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05241.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d22496c58f969c152bcefd",
      "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
      "fullname": "Tiezhen WANG",
      "name": "xianbao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 120
    },
    "isAuthorParticipating": false
  }
]