[
  {
    "paper": {
      "id": "2502.20730",
      "authors": [
        {
          "_id": "67c514aba3d873e41624a082",
          "user": {
            "_id": "63664c8fa2abcdf2fd6425ed",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
            "isPro": false,
            "fullname": "Li Zhuoqun",
            "user": "lzq2021",
            "type": "user"
          },
          "name": "Zhuoqun Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-03T08:07:26.218Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a083",
          "user": {
            "_id": "64a4ceda9a90f701134189b7",
            "avatarUrl": "/avatars/859a189c5d2ae2fcb9aa2d79104fbfe7.svg",
            "isPro": false,
            "fullname": "Haiyang Yu",
            "user": "yhycai",
            "type": "user"
          },
          "name": "Haiyang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-03T09:31:12.493Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a084",
          "user": {
            "_id": "63ef664304b0e373992a2633",
            "avatarUrl": "/avatars/cba554ff88bd8b68ae51bea8ee991d13.svg",
            "isPro": false,
            "fullname": "Xuanang Chen",
            "user": "xuanang",
            "type": "user"
          },
          "name": "Xuanang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:31.384Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a085",
          "user": {
            "_id": "6711c702f858a456b4b9f3a4",
            "avatarUrl": "/avatars/178e9567c3111ab22717c3c0dd003a6a.svg",
            "isPro": false,
            "fullname": "Hongyu  Lin",
            "user": "sanmusunrise",
            "type": "user"
          },
          "name": "Hongyu Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:28:09.791Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a086",
          "user": {
            "_id": "6216496a9b34d2fb49144599",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg",
            "isPro": false,
            "fullname": "Yaojie Lu",
            "user": "luyaojie",
            "type": "user"
          },
          "name": "Yaojie Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:38.957Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a087",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a088",
          "user": {
            "_id": "65e99a77e71555ed193609cf",
            "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
            "isPro": false,
            "fullname": "Xianpei Han",
            "user": "xphan",
            "type": "user"
          },
          "name": "Xianpei Han",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:51.007Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a089",
          "user": {
            "_id": "66641b2fd8e1e34bc621e688",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66641b2fd8e1e34bc621e688/csPETwnx2zCIHSWi9uAi-.png",
            "isPro": false,
            "fullname": "Yongbin Li",
            "user": "Yongbin-Li",
            "type": "user"
          },
          "name": "Yongbin Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:57.561Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a08a",
          "name": "Le Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-28T05:23:10.000Z",
      "title": "DeepSolution: 복잡한 공학 해결책 설계를 향상시키기 위한\n  나무 구조 탐색과 두 점 생각 방식의 사용 방법",
      "summary": "복잡한 공학 문제에 대한 해결책 설계는 인간 생산 활동에 있어서 중요하다. 그러나 과거의 Review-Augmented Generation (RAG) 분야의 연구는 복잡한 공학 해결책 설계에 관련된 태스크에 충분히 다루지 않았습니다. 이를 채워내기 위해, 우리는 공학 문제를 해결하는 완전한 가능한 해결책을 생성하는 시스템의 능력을 평가하기 위한 새로운 벤치마크인 SolutionBench를 소개합니다. 또한 복잡한 공학 해결책 설계의 발전을 촉진하기 위해, 우리는 트리 구조 탐색과 두점 사고 구조를 활용한 새로운 시스템인 SolutionRAG를 제안합니다. 확장된 실험 결과를 통해, SolutionRAG는 SolutionBench에서 가장 최신 (SOTA)의 성능을 달성하고, 실제 세계적인 애플리케이션에서 복잡한 공학 해결책 설계의 자동화와 신뢰성 향상에 잠재력을 보여주는 것을 명확히 알 수 있습니다.",
      "upvotes": 11,
      "discussionId": "67c514aca3d873e41624a10b"
    },
    "publishedAt": "2025-03-02T21:35:24.437Z",
    "title": "DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/y_kT4GP3xgm-5RdguMNV7.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/wDAS_USsxsVHbin1I5CEe.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/4lJgWp9V8pm4vDBUH4I5n.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20730.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63664c8fa2abcdf2fd6425ed",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
      "fullname": "Li Zhuoqun",
      "name": "lzq2021",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.18600",
      "authors": [
        {
          "_id": "67c0a8058589d8ecb79d472b",
          "user": {
            "_id": "6594b1bb57a556fbe162915e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594b1bb57a556fbe162915e/WuYxqbbvaJaT-xsk5KhoT.jpeg",
            "isPro": false,
            "fullname": "Silei Xu",
            "user": "sileixu",
            "type": "user"
          },
          "name": "Silei Xu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-27T18:01:14.543Z",
          "hidden": false
        },
        {
          "_id": "67c0a8058589d8ecb79d472c",
          "name": "Wenhao Xie",
          "hidden": false
        },
        {
          "_id": "67c0a8058589d8ecb79d472d",
          "name": "Lingxiao Zhao",
          "hidden": false
        },
        {
          "_id": "67c0a8058589d8ecb79d472e",
          "user": {
            "_id": "5efd09cf49ed724c8a135868",
            "avatarUrl": "/avatars/af12bc94657979677a9f26183f0c9727.svg",
            "isPro": false,
            "fullname": "Pengcheng He",
            "user": "DeBERTa",
            "type": "user"
          },
          "name": "Pengcheng He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:30:43.479Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T19:36:06.000Z",
      "title": "빠르게 생각하기 위해 적은 쪽의 쇄도장",
      "summary": "대 언어 모뎀(LLMs)는 Chain-of-Thought(CoT) 프로닝 등 구조를 통해 복잡한 이유론 문제 해결에서 뛰어난 성능을 보여주고 있습니다. 그러나 인간은 일반적으로 더 효율적인 전략을 선택하는 경우가 많습니다: 요약적인 중간적인 생각들을 작성하여 그 중 기본적인 정보만捉捉る 것입니다. 본 논문에서는 인간의 인지 프로세스를 영감을 받아 LLMs가 태스크를 해결하면서 최소한의 정보적인 중간적인 이유론을 출력하는 새로운 패러다임인 \"Chain of Draft(CoD)\"를 제안합니다. 이로써, 보시코라티를 줄이고 중요한 이견에 초점을 맞추어 CoT과 같은 수준의 정확한 결과를 얻을 수 있으며, 또한 복잡한 이유론 태스크에서 7.6%의 토큰만을 사용함으로써 비용과 라틴성을 크게 줄일 수 있습니다.",
      "upvotes": 6,
      "discussionId": "67c0a8078589d8ecb79d47ed"
    },
    "publishedAt": "2025-03-03T02:35:09.967Z",
    "title": "Chain of Draft: Thinking Faster by Writing Less",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18600.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63da3d7ae697e5898cb86854",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675246771355-noauth.jpeg",
      "fullname": "Talha Rüzgar Akkuş",
      "name": "Q-bert",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 85
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18017",
      "authors": [
        {
          "_id": "67bef5a6070ec160042d99f4",
          "user": {
            "_id": "657429d833e5a4bf5b278615",
            "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
            "isPro": false,
            "fullname": "QiuchenWang",
            "user": "autumncc",
            "type": "user"
          },
          "name": "Qiuchen Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-28T12:15:57.850Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f5",
          "name": "Ruixue Ding",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f6",
          "user": {
            "_id": "64892d31cbda0d1cdb956897",
            "avatarUrl": "/avatars/3cdafe03a8295124636347d15a099aaf.svg",
            "isPro": false,
            "fullname": "Zehui Chen",
            "user": "lovesnowbest",
            "type": "user"
          },
          "name": "Zehui Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:32:18.129Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f7",
          "user": {
            "_id": "65351cbe6141b3927afaed17",
            "avatarUrl": "/avatars/5abf5f2c4ab329e63a7f45c15c9dfb93.svg",
            "isPro": false,
            "fullname": "weiqi wu",
            "user": "vickywu",
            "type": "user"
          },
          "name": "Weiqi Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:32:12.075Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f8",
          "user": {
            "_id": "62e8efb14210d3fe69eacb42",
            "avatarUrl": "/avatars/2feadd75274bf353b910f4679ef72b39.svg",
            "isPro": false,
            "fullname": "Shihang Wang",
            "user": "shihang",
            "type": "user"
          },
          "name": "Shihang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:32:05.679Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f9",
          "user": {
            "_id": "63a091e42fabbbb89991f5ce",
            "avatarUrl": "/avatars/d55485b06461764c36c9edf9d6e8892c.svg",
            "isPro": false,
            "fullname": "pengjun xie",
            "user": "xpjandy",
            "type": "user"
          },
          "name": "Pengjun Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:31:59.813Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99fa",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T09:26:12.000Z",
      "title": "ViDoRAG: 시각적 문서 검색 확장 생성을 수행하는 동적인 반복적인 이유 аген트",
      "summary": "ビジュアル豊富な 문서에서 정보를 이해하는 것은 전통적인 Retrieval-Augmented Generation (RAG) 방법론에 있어서 중요한 문제입니다. 현재의 벤치마크는 주로 이미지 기반의 질문에 대한 대답 (QA)에 집중하고 있으며, 복잡한 시각적 문서 내에서 효율적인 검색, 이해, 그리고 기본적인 이유를 찾는 기본적인 문제를 무시하고 있습니다. 이러한 차이를 보완하기 위해, ViDoSeek라는 새로운 데이터셋을 도입하여, RAG의 성능을 평가하기 위해 복잡한 이유를 필요로 하는 시각적 풍부한 문서에 적합한 평가 방법을 제공했습니다. 이를 기반으로, 현재의 RAG 접근 방식의 주요한 한계점을 식별했습니다: (i) 단순한 시각적 검색 방법은 문자와 시각적 특징을 효과적으로 통합하는 것이 어렵습니다, (ii) 지금까지의 접근 방식은 일반적으로 충분한 이유 토큰을 할당하지 못하여, 그 효과성을 제한하고 있습니다. 이러한 문제를 해결하기 위해, ViDoRAG라는 새로운 다 에이전트 RAG 프레임워크를 제안합니다. ViDoRAG는 복잡한 시각적 문서에 필요한 이유를 필요로 하는 것이고, Gaussian Mixture Model (GMM)에 기반한 하이브리드 스테라티지 사용으로, 다 모델 검색을 효과적으로 처리할 수 있습니다. 모델의 이유 능력 향상을 위해, 탐색, 요약, 반성 포함하는 반복적인 에이전트 작업 흐름을 도입하고, RAG 분야에서 테스트 시간 스케일링을 위한 프레임워크를 제공합니다. ViDoSeek에서 확장된 실험은 우리의 접근 방식의 유효성과 일반화 능력을 증명했습니다. 특히, ViDoRAG는 현재의 방법과 경쟁적인 ViDoSeek 벤치마크에서 10% 이상의 효과를 보였습니다.",
      "upvotes": 4,
      "discussionId": "67bef5a7070ec160042d9a65"
    },
    "publishedAt": "2025-03-02T22:22:01.895Z",
    "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18017.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657429d833e5a4bf5b278615",
      "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
      "fullname": "QiuchenWang",
      "name": "autumncc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20545",
      "authors": [
        {
          "_id": "67c51b459d5807d6674b3d3c",
          "name": "Kechen Li",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d3d",
          "name": "Wenqi Zhu",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d3e",
          "name": "Coralia Cartis",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d3f",
          "user": {
            "_id": "64bb61e876a6e2efcc728e22",
            "avatarUrl": "/avatars/b0ed1c9f13fd1f2c99d202155001e39b.svg",
            "isPro": false,
            "fullname": "Tianbo Ji",
            "user": "jitianbo",
            "type": "user"
          },
          "name": "Tianbo Ji",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:35:49.782Z",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d40",
          "name": "Shiwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T21:41:43.000Z",
      "title": "SoS1: O1과 R1-Like Reasoning LLMs는 제곱합의 해결책자입니다.",
      "summary": "대 언어 모델(LLMs)는 다양한 태스크에서 인간 수준의 우수한 인지 능력을 달성하지만, 엄격한 수학 문제 해결 능력은 개방된 문제입니다. 본 연구에서는 기본적으로 계산적으로 어려운 문제를 다루며, 주어진 다변수 다항식의 음수가 아닌지 판단하는 문제를 다루었습니다. 이 문제는 힐버트의 17번째 문제와 밀접한 관계에 있으며, 전 세계적인 다항식 최적화에 중요한 역할을 하며, 다양한 분야에서도 적용되어 있습니다. 먼저, SoS-1K라는, 세분화되어 정리된 데이터셋을 소개합니다. 이 데이터셋에는 5가지 단계적으로 어려운 평가기준에 기반하여 전문가가 설계한 논리적 지침이 포함되어 있습니다. 이 데이터셋을 사용하여, 최신 LLMs를 평가한 결과, 구조화된 지침이 없는 경우, 모든 모델은 랜덤한 가지 기반 선 50%보다 조금만 초과하는 정도입니다. 그러나 고품질의 논리적 지침은 정확도를 크게 향상시키고, 성능을 81%까지 향상시킵니다. 또한, SoS-1K에서 4시간의 微調節을 통해 7B 모델인 SoS-7B는 DeepSeek-V3과 GPT-4o-mini를 초과하는 정확도를 갖으며, 각각의 모델에 필요한 계산 시간은 1.8%와 5% 밖에 필요하지 않습니다. 본 연구의 결과는 LLMs가 수학적인 논리적 지침의 경계를 뛰어넘으며, NP-hard 문제 해결 가능성에 대한 가능성을 보여주고 있습니다.",
      "upvotes": 2,
      "discussionId": "67c51b469d5807d6674b3d88"
    },
    "publishedAt": "2025-03-02T22:00:31.796Z",
    "title": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20545.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6260
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20396",
      "authors": [
        {
          "_id": "67c51d36c830dcb76bbb5994",
          "name": "Toru Lin",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5995",
          "name": "Kartik Sachdev",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5996",
          "name": "Linxi Fan",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5997",
          "user": {
            "_id": "65369a95605a07338de78ab0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sGFjOjLT2akN-sn5beVWL.jpeg",
            "isPro": false,
            "fullname": "Jitendra Malik ",
            "user": "jitendra1995",
            "type": "user"
          },
          "name": "Jitendra Malik",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:36:34.177Z",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5998",
          "name": "Yuke Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T18:59:52.000Z",
      "title": "시뮬레이션에서 리알 모댄의 강화 학습을 이용한 인형 모델 기반의 시각적 디텍터스마니피라팅",
      "summary": "강화학습은 다양한 문제 영역에서 인간이나 더 높은 능력을 달성하기 위해 원하는 결과를 얻고 있지만, 디ェテックな 로봇 조작의 성공은 제한되어 있습니다.本研究에서는 인형의 신체상상에 대한 접촉 풍부한 작업 태스크의 해결에 대한 강화 학습의 적용의 주요 문제점을 조사하고 있습니다.우리는 인지적인 보완을 수행하는 새로운 방법을 소개하고 실험적인 증명을 진행합니다.우리의 주요 기여는 실세계로부터 시뮬레이션 환경에 가까운 자동 조정 모듈, 장기간 접촉 풍부한 작업 태스크의 보상 설계의 단순화를 실현하는 일반화된 보상 설계 스케일, 어려운 탐색 문제의 샘플 효과성을 개선하면서 시뮬레이션으로부터 실세계의 성능을 유지하는 분할 공격 훈련 프로세스, 희소하고 밀린 물체 표현의 혼합을 사용하여 시뮬레이션으로부터 실세계의 인지 오류를 구체화하는 것입니다.우리는 3가지 인형의 디ェテックな 작업 태스크에 대해 원하는 결과를 보여주고 각 방법에 대한 소멸 연구를 진행합니다.우리의 연구는 시뮬레이션으로부터 실세계의 강화 학습을 활용한 인형의 디ェテックな 조작의 학습에 성공한 접근을 제시하고, 인간의 지도가 필요하지 않은 강력한 일반화와 높은 성능을 실현합니다.",
      "upvotes": 1,
      "discussionId": "67c51d39c830dcb76bbb5a1f"
    },
    "publishedAt": "2025-03-02T22:08:44.891Z",
    "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20396.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6260
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20811",
      "authors": [
        {
          "_id": "67c51c198d02783fa3a6249d",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a6249e",
          "name": "Jingyun Hua",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a6249f",
          "user": {
            "_id": "675a69699e086bd6250a36ef",
            "avatarUrl": "/avatars/95c72e3975d1a37f8655a2fe629746ec.svg",
            "isPro": false,
            "fullname": "Weihong Lin",
            "user": "lwher1996",
            "type": "user"
          },
          "name": "Weihong Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:42:30.547Z",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a0",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a1",
          "name": "Fuzheng Zhang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a2",
          "name": "Jianlong Wu",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a3",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a4",
          "name": "Liqiang Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-28T07:53:40.000Z",
      "title": "HAIC: 더 좋은 캡처를 사용하여 인간 행동 이해와 생성을 개선하는 다중모달 대 언어 모델에 대한 연구",
      "summary": "최근의 다중 모델 대 언어 모델(MLLMs)은 영화 이해에 있어서 큰 진전을 이루고 있습니다. 그러나 인간의 행동에 대한 영화 성능은 고품질 데이터의 부족으로 제한되어 있습니다. 이에 대처하여 우리는 2단계의 데이터 설명 프로세스를 도입합니다. 먼저, 인터넷에서 명확한 인간의 행동을 포함하는 영화를 수집하는 전략을 설계합니다. 다음으로, 인간의 특성을 사용하여 개인을 구분하고 행동과 상호작용을 시간적으로 상세하게 설명하는 표준적인 캡션 포맷으로 설명합니다. 이 프로세스를 통해 우리는 HAICTrain과 HAICBench의 두 개의 데이터 세트를 칫솔을 썼습니다. HAICTrain은 Gemini-Pro로 생성되어 학습용으로 검증된 126K개의 영화-캡션 쌍으로 구성됩니다. 반면에, HAICBench는 500개의 자동 설명된 영화-캡션 쌍과 1,400개의 QA 쌍을 포함하며, 인간의 행동 이해에 대한 상세한 평가 제공됩니다. 실험 결과는 HAICTrain을 사용하여 학습은 4개의 벤치마크에서 인간 이해 능력이 크게 향상되고, 영화 생성 결과를 개선할 수 있음을 보여주고 있습니다. HAICTrain과 HAICBench는 https://huggingface.co/datasets/KuaishouHAIC/HAIC에서 공개되어 있습니다.",
      "upvotes": 1,
      "discussionId": "67c51c1b8d02783fa3a62543"
    },
    "publishedAt": "2025-03-02T22:04:15.087Z",
    "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20811.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6260
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20583",
      "authors": [
        {
          "_id": "67c516998d02783fa3a52dc8",
          "user": {
            "_id": "6304ac1a412a1b9d381ca378",
            "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
            "isPro": false,
            "fullname": "Keisuke Kamahori",
            "user": "kamahori",
            "type": "user"
          },
          "name": "Keisuke Kamahori",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-03T08:07:02.986Z",
          "hidden": false
        },
        {
          "_id": "67c516998d02783fa3a52dc9",
          "user": {
            "_id": "62908273c740ebb981a6dba4",
            "avatarUrl": "/avatars/465f50369c367b07670f5209c83d65f2.svg",
            "isPro": false,
            "fullname": "Jungo Kasai",
            "user": "jungok",
            "type": "user"
          },
          "name": "Jungo Kasai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:43:49.097Z",
          "hidden": false
        },
        {
          "_id": "67c516998d02783fa3a52dca",
          "user": {
            "_id": "628c26a8b80bb09700d6af86",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653352051245-noauth.jpeg",
            "isPro": false,
            "fullname": "Noriyuki Kojima",
            "user": "kojimano",
            "type": "user"
          },
          "name": "Noriyuki Kojima",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:43:56.698Z",
          "hidden": false
        },
        {
          "_id": "67c516998d02783fa3a52dcb",
          "user": {
            "_id": "654132fe5a9a913c6c870e79",
            "avatarUrl": "/avatars/2f6807eddef1929c571977e9af35f952.svg",
            "isPro": false,
            "fullname": "Baris Kasikci",
            "user": "kasikci",
            "type": "user"
          },
          "name": "Baris Kasikci",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:44:04.084Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T22:52:21.000Z",
      "title": "LiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLiteASR: 효율적인 자동 음성 인식을 위한 저 순위 근사법 사용\n\nLite",
      "summary": "현대의 자동 언어 인식 모델(ASR)에서, OpenAI의 Whisper 등 모델은 깊은 인코더-디코더 아키텍처를 사용하며, 이러한 인코더는 효율적인 처리를 위해 높은 계산량으로 중요한 버퍼 역할을 하고 있습니다. 우리는 LiteASR라는 ASR 인코더의 저レン키 압축 기법을 소개합니다. 이 방법은 추론 비용의 절감과 동시에 읽기 정확도를 유지할 수 있습니다. 우리의 접근법은 중간 활성화의 강한 저レン키 특성을 활용합니다: 작은 조정 데이터 세트에서 주성분 분석(PCA)를 적용하고, 선형 변환을 저レン키 행렬 곱의 연속으로 근사하여, 더 나아가 자기 어텐션을 줄인 차원으로 동작시켜 최적화합니다. 평가 결과는 우리의 방법이 Whisper large-v3의 인코더 크기를 50% 이상 줄일 수 있고, Whisper medium의 크기와 같은 크기에서도 더 높은 읽기 정확도를 유지하며, 이로 인해 효율성과 성능의 새로운 Pareto 최적 경계를 확립할 수 있습니다. LiteASR의 코드는 https://github.com/efeslab/LiteASR에 공개되어 있습니다.",
      "upvotes": 1,
      "discussionId": "67c516998d02783fa3a52dfd"
    },
    "publishedAt": "2025-03-02T21:48:46.577Z",
    "title": "LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20583.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6304ac1a412a1b9d381ca378",
      "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
      "fullname": "Keisuke Kamahori",
      "name": "kamahori",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.19577",
      "authors": [
        {
          "_id": "67c42356054ae6d1c760b643",
          "user": {
            "_id": "66588b6fd22637bfab498709",
            "avatarUrl": "/avatars/9007f0d3b078bd6193912a5359107f24.svg",
            "isPro": false,
            "fullname": "Hugues Turbé",
            "user": "hturbe",
            "type": "user"
          },
          "name": "Hugues Turbé",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-02T20:15:04.391Z",
          "hidden": false
        },
        {
          "_id": "67c42356054ae6d1c760b644",
          "name": "Mina Bjelogrlic",
          "hidden": false
        },
        {
          "_id": "67c42356054ae6d1c760b645",
          "name": "Gianmarco Mengaldo",
          "hidden": false
        },
        {
          "_id": "67c42356054ae6d1c760b646",
          "name": "Christian Lovis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T21:40:30.000Z",
      "title": "以下是翻译结果：\n\n\"안녕하세요. 아래는 지정된 영어문서의 한국어 번역입니다.\"",
      "summary": "ビジュアルフォUNDATIONモデル（VFMs）는 최신 기술로 높은 성능을 자랑하며 인기가 높아지고 있습니다. 그러나 설명력은 중요합니다. 이 점에서, 자기 설명 모델（SEM）은 예측을 해석할 수 있는 개념의 가중합으로 분해하고 설명 가능한 클래스 분류기를 제공하는 것을 목표로 합니다. 최근의 연구에서 이러한 설명력의 부족에 대한 증거가 제시되어 있습니다. 본 연구에서는 VFMs와 새로운 원형 아키텍처, 특수화된 훈련 객체를 조합하여 ProtoFM라는 접근법을 제안합니다. VFMs 위에서 약간의 가중합（약 1M 파라미터）만 훈련하는 것으로, 이 접근법은 효율적이고 설명 가능한 해결책을 제공합니다. 평가 결과, 우리의 접근법은 설명력 지표에서 광범위한 범위에서 현재 모델을 초월하면서 상대적인 클래스 분류 성능을 달성합니다. 코드는 https://github.com/hturbe/proto-fm에 접근 가능합니다.",
      "upvotes": 0,
      "discussionId": "67c4235c054ae6d1c760b806"
    },
    "publishedAt": "2025-03-03T04:21:42.563Z",
    "title": "Tell me why: Visual foundation models as self-explainable classifiers",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66588b6fd22637bfab498709/4VG_eDtZKZ4kj1AdG_P14.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19577.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66588b6fd22637bfab498709",
      "avatarUrl": "/avatars/9007f0d3b078bd6193912a5359107f24.svg",
      "fullname": "Hugues Turbé",
      "name": "hturbe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]