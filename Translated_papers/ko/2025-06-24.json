[
  {
    "paper": {
      "id": "2506.18882",
      "authors": [
        {
          "_id": "685a163c0e4ad7e219758569",
          "name": "Hong Li",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856a",
          "name": "Houyuan Chen",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856b",
          "name": "Chongjie Ye",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856c",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856d",
          "name": "Bohan Li",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856e",
          "name": "Shaocong Xu",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856f",
          "name": "Xianda Guo",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758570",
          "name": "Xuhui Liu",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758571",
          "name": "Yikai Wang",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758572",
          "name": "Baochang Zhang",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758573",
          "name": "Satoshi Ikehata",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758574",
          "name": "Boxin Shi",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758575",
          "name": "Anyi Rao",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758576",
          "name": "Hao Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643a1f5b58cb07c2a3745116/4JHMhL80xxrkPBOIt9-Cg.mp4"
      ],
      "publishedAt": "2025-06-23T17:53:11.000Z",
      "submittedOnDailyAt": "2025-06-24T02:59:45.489Z",
      "title": "정상 광线下의 광 : 일반 광도 스튜디오 촬영을 위한 통일된 특징 표현\n\n(Note: The translation is provided as requested, without any additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "643a1f5b58cb07c2a3745116",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a1f5b58cb07c2a3745116/OiSDfgfcCUWu0X4-FiNm0.jpeg",
        "isPro": false,
        "fullname": "Hugo",
        "user": "chongjie",
        "type": "user"
      },
      "summary": "일반적인 사진 촬영법(PS)는 임의의 조명 조건 하에서 물체의 고품질의 표면 정규선(normal vector)을 복원하는 것을 목표로 합니다. 최근의 발전으로 SDM-UniPS와 Uni MS-PS가 등장하지만, 두 가지 기본적인 문제점이 남아 있습니다. 1) 변화하는 조명과 표면 정규선의 특징과 깊은 상관관계로, 관측되는 강도의 불확실성이 조명의 변화 또는 표면의 방향에 따라 밝기의 변화에 대해 판단하기 어렵습니다. 2) 복잡한 표면에서 고주파의 기오메트리(geometry)의 보존에서, 복잡한 기오메트리는 자기 그림, 상호 반사 및 微妙한 정규선의 변화를 일으키고, 전통적인 특징 처리 연산으로 정확하게 인식하기 어려운 것입니다.",
      "upvotes": 62,
      "discussionId": "685a163c0e4ad7e219758577",
      "githubRepo": "https://github.com/houyuanchen111/LINO_UniPS",
      "ai_summary": "Photometric stereo aims to recover high-quality surface normals under arbitrary lighting conditions, addressing challenges related to illumination-surface normal coupling and high-frequency geometric detail preservation.",
      "ai_keywords": [
        "photometric stereo",
        "deep coupling",
        "surface normals",
        "illumination conditions",
        "intensity variations",
        "self-shadowing",
        "inter-reflections",
        "subtle normal variations"
      ]
    },
    "publishedAt": "2025-06-23T13:53:11.000Z",
    "title": "Light of Normals: Unified Feature Representation for Universal\n  Photometric Stereo",
    "summary": "Universal photometric stereo (PS) aims to recover high-quality surface\nnormals from objects under arbitrary lighting conditions without relying on\nspecific illumination models. Despite recent advances such as SDM-UniPS and Uni\nMS-PS, two fundamental challenges persist: 1) the deep coupling between varying\nillumination and surface normal features, where ambiguity in observed intensity\nmakes it difficult to determine whether brightness variations stem from\nlighting changes or surface orientation; and 2) the preservation of\nhigh-frequency geometric details in complex surfaces, where intricate\ngeometries create self-shadowing, inter-reflections, and subtle normal\nvariations that conventional feature processing operations struggle to capture\naccurately.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643a1f5b58cb07c2a3745116/4JHMhL80xxrkPBOIt9-Cg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18882.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643a1f5b58cb07c2a3745116",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a1f5b58cb07c2a3745116/OiSDfgfcCUWu0X4-FiNm0.jpeg",
      "fullname": "Hugo",
      "name": "chongjie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18871",
      "authors": [
        {
          "_id": "685a0be90e4ad7e2197584f4",
          "user": {
            "_id": "65f19fa7f591e4538b65dea5",
            "avatarUrl": "/avatars/a38e65701e1d2eb3eb93335d6d0b937c.svg",
            "isPro": false,
            "fullname": "Chenyuan Wu",
            "user": "wcyno23",
            "type": "user"
          },
          "name": "Chenyuan Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:41:49.563Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f5",
          "name": "Pengfei Zheng",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f6",
          "user": {
            "_id": "661ac5b53d7248a6f20080c1",
            "avatarUrl": "/avatars/26aef5944759c2e4366a71eb8c7fc50a.svg",
            "isPro": false,
            "fullname": "Ruiran Yan",
            "user": "Ruiran",
            "type": "user"
          },
          "name": "Ruiran Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:41:58.820Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f7",
          "user": {
            "_id": "62612679bbcbd1c34f1638af",
            "avatarUrl": "/avatars/c0675d05a52192ee14e9ab1633353956.svg",
            "isPro": false,
            "fullname": "Xiao",
            "user": "Shitao",
            "type": "user"
          },
          "name": "Shitao Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:42:07.997Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f8",
          "user": {
            "_id": "641bd1737c21ab946bf69aff",
            "avatarUrl": "/avatars/83759075ad893a69a0c2cf5493d7e988.svg",
            "isPro": false,
            "fullname": "xin luo",
            "user": "sienna223",
            "type": "user"
          },
          "name": "Xin Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:42.720Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f9",
          "user": {
            "_id": "6458b59c7a7e192202df8fa0",
            "avatarUrl": "/avatars/33ee716477e5686da8723d01e199cd27.svg",
            "isPro": false,
            "fullname": "Yueze Wang",
            "user": "yzwang",
            "type": "user"
          },
          "name": "Yueze Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T09:33:03.979Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fa",
          "user": {
            "_id": "675bcb9ce16de4a95aac9950",
            "avatarUrl": "/avatars/a1d0a2fd96ddee9cdea4f97819233fe5.svg",
            "isPro": false,
            "fullname": "Wanli Li",
            "user": "liwanli",
            "type": "user"
          },
          "name": "Wanli Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:42:15.514Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fb",
          "user": {
            "_id": "674972973dc92067bd606877",
            "avatarUrl": "/avatars/8366448b45baf7d7f3d3d2b8793479ed.svg",
            "isPro": false,
            "fullname": "Jiang Xiyan",
            "user": "Emilia515",
            "type": "user"
          },
          "name": "Xiyan Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:42:29.622Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fc",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fd",
          "user": {
            "_id": "6564a2ceedae9c33b7654a1f",
            "avatarUrl": "/avatars/42f09356a1282896573ccb44830cd327.svg",
            "isPro": false,
            "fullname": "JUNJIE ZHOU",
            "user": "JUNJIE99",
            "type": "user"
          },
          "name": "Junjie Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:40.699Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fe",
          "user": {
            "_id": "66164f6245336ca774679611",
            "avatarUrl": "/avatars/9baf0ab475bc8d5997abda9ffe8cfa28.svg",
            "isPro": false,
            "fullname": "Ze Liu",
            "user": "marsh123",
            "type": "user"
          },
          "name": "Ze Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:38.616Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584ff",
          "user": {
            "_id": "6540617c7cadb2d1b42007c5",
            "avatarUrl": "/avatars/b1877fd0564c362a0d4a064d4ec43a73.svg",
            "isPro": false,
            "fullname": "Ziyi Xia",
            "user": "ZiyiXia",
            "type": "user"
          },
          "name": "Ziyi Xia",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:42:47.445Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758500",
          "name": "Chaofan Li",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758501",
          "name": "Haoge Deng",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758502",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758503",
          "name": "Kun Luo",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758504",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758505",
          "user": {
            "_id": "66ed026076a8038cb4ae6053",
            "avatarUrl": "/avatars/99b6527da6b66c6b5df3fc8261587322.svg",
            "isPro": false,
            "fullname": "Defu Lian",
            "user": "dove-ustc",
            "type": "user"
          },
          "name": "Defu Lian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:43:23.976Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758506",
          "name": "Xinlong Wang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758507",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758508",
          "name": "Tiejun Huang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758509",
          "name": "Zheng Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:38:54.000Z",
      "submittedOnDailyAt": "2025-06-24T01:06:04.763Z",
      "title": "OmniGen2: 첨단 모노라일 제네레이션 탐색",
      "submittedOnDailyBy": {
        "_id": "6564a2ceedae9c33b7654a1f",
        "avatarUrl": "/avatars/42f09356a1282896573ccb44830cd327.svg",
        "isPro": false,
        "fullname": "JUNJIE ZHOU",
        "user": "JUNJIE99",
        "type": "user"
      },
      "summary": "이 연구에서는 OmniGen2, 즉 광범위하고 오픈소스의 생성 모델을 소개합니다. 이 모델은 텍스트에서 이미지, 이미지 편집, 텍스트 프레임 내의 생성 등 다양한 생성 태스크에 대해 한 달의 해결책을 제공하는 것을 목표로 합니다. OmniGen v1과 달리, OmniGen2는 텍스트와 이미지의 모델 디오리터에 대해 두 가지 다른 확실 패스웨이를 채택하고, 공유된 파라미터와 결합된 이미지 토큰 나바어를 사용합니다. 이 설계는 OmniGen2가 기존의 다 모델 이해 모델에 기반하여 생성될 수 있게 하고, VAE의 입력을 재적용하는 필요성을 줄이는 것으로, 원래의 텍스트 생성 능력을 유지합니다. OmniGen2의 훈련을 지원하기 위해, 이미지 편집 및 텍스트 프레임 내의 생성 데이터를 포함하는 세부적인 데이터 구축 파이프라인을 개발했습니다. 또한, 이미지 생성 태스크에 적합한 반사 구조를 도입하고, OmniGen2에 기반한 특별한 반사 데이터 세트를 제작했습니다. OmniGen2는 텍스트에서 이미지 및 이미지 편집에서 상대적으로 파라미터 크기가 작지만, 여러 태스크 벤치마크에서 경쟁적인 결과를 얻습니다. 텍스트 프레임 내의 생성을 위한 평가를 위해, OmniContext라는 새로운 벤치마크를 도입했습니다. OmniGen2는 오픈 소스 모델에서 가장 선진적인 성능을 갖습니다. 미래의 연구를 지원하기 위해, 모델, 훈련 코드, 데이터 세트, 데이터 구축 파이프라인을 공개합니다. 프로젝트 페이지: https://vectorspacelab.github.io/OmniGen2; GitHub 링크: https://github.com/VectorSpaceLab/OmniGen2",
      "upvotes": 33,
      "discussionId": "685a0be90e4ad7e21975850a",
      "projectPage": "https://vectorspacelab.github.io/OmniGen2/",
      "githubRepo": "https://github.com/VectorSpaceLab/OmniGen2",
      "ai_summary": "OmniGen2, a versatile generative model, introduces dual decoding pathways for text and images, preserves original text generation, and achieves competitive results with a new subject-driven benchmark.",
      "ai_keywords": [
        "decoding pathways",
        "unshared parameters",
        "decoupled image tokenizer",
        "multimodal understanding models",
        "reflection mechanism",
        "reflection dataset",
        "OmniContext",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-06-23T13:38:54.000Z",
    "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
    "summary": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18871.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6564a2ceedae9c33b7654a1f",
      "avatarUrl": "/avatars/42f09356a1282896573ccb44830cd327.svg",
      "fullname": "JUNJIE ZHOU",
      "name": "JUNJIE99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18841",
      "authors": [
        {
          "_id": "685a0f330e4ad7e219758514",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758515",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758516",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:35.954Z",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758517",
          "name": "Roy Ka-Wei Lee",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758518",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T16:59:02.000Z",
      "submittedOnDailyAt": "2025-06-24T01:08:07.123Z",
      "title": "LongWriter-Zero: 긴 문장 생성 최적화를 리니포어밍 학습에 의한 방법으로 실현합니다.",
      "submittedOnDailyBy": {
        "_id": "63369da91ba5d5ece24118a4",
        "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
        "isPro": false,
        "fullname": "wuyuhao",
        "user": "mozhu",
        "type": "user"
      },
      "summary": "超장생성을 수행하는 대형 언어 모델(LLMs)은 광범위하게 요구되는 시나리오 중 하나입니다が, 최대 생성 길이의 제한과 길이가 증가하면서 질의 저하가 원인으로 실제적으로 중요한 문제로 자리잡고 있습니다. 과거의 접근 방식의 예로 LongWriter가 제시되어 있습니다 그러나 일반적으로 '교'에 기반합니다. 이는 합성의 긴 문장 출력에 대한 규칙적 조정 학습(SFT)을 수행하는 것입니다 그러나 이 전략은 합성의 SFT 데이터에 의존합니다. 이는 구축이 어려워 비용도 높습니다 또한 Collaboration과 일관성이 부족하며 과도하게 인공적이고 구조적으로 단조롭습니다. 본 연구에서는 완전히 0부터 시작하고 어떠한 注釈이나 합성 데이터도 의존하지 않는, 보상기준의 접근 방식을 제안합니다. 이는 강화 학습(RL)을 활용하여 LLMs에 의한 초장고품질의 문장 생성 능력을 촉진하는 것을 목표로 합니다. RL 학습을 위해 기초 모델부터 시작하여 작성 과정에서 계획과 개선을 촉진하기 위해 모델을 가이드합니다. 이를 위해 특별히 보상 모델을 사용하며, LLM을 길이의 제어, 문장의 질, 구조의 조형에 맞게 계속합니다. 실험적 평가에 따라 우리의 LongWriter-Zero 모델(Qwen2.5-32B에서 학습된 것)은 전통적인 SFT 방법보다 일관적으로 우수한 결과를 보입니다. WritingBench와 Arena-Write의 모든 메트릭에 대해 가장 先端의 결과를 실현하며, DeepSeek R1나 Qwen3-235B과 같은 100B 이상의 모델을 초월합니다. 우리는 https://huggingface.co/THU-KEG/LongWriter-Zero-32B 에서 데이터와 모델 체크포인트를 공개합니다.",
      "upvotes": 30,
      "discussionId": "685a0f340e4ad7e219758519",
      "ai_summary": "An incentivization-based reinforcement learning approach is used to develop a large language model capable of generating ultra-long, high-quality text without the need for synthetic data or supervised fine-tuning.",
      "ai_keywords": [
        "reinforcement learning",
        "reward models",
        "long-form text generation",
        "ultra-long generation",
        "large language models",
        "synthetic fine-tuning",
        "length control",
        "writing quality",
        "structural formatting",
        "WritingBench",
        "Arena-Write"
      ]
    },
    "publishedAt": "2025-06-23T12:59:02.000Z",
    "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
    "summary": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18841.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63369da91ba5d5ece24118a4",
      "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
      "fullname": "wuyuhao",
      "name": "mozhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18851",
      "authors": [
        {
          "_id": "685a0fb40e4ad7e219758528",
          "user": {
            "_id": "6304e2dabad6ce7fc0287d57",
            "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
            "isPro": false,
            "fullname": "Zhuowei_Chen",
            "user": "ZhuoweiChen",
            "type": "user"
          },
          "name": "Zhuowei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:44:49.590Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758529",
          "user": {
            "_id": "63b415037af2e415f2599c18",
            "avatarUrl": "/avatars/4afbe7d6d05a702f1beeed9c53e78153.svg",
            "isPro": false,
            "fullname": "Bingchuan Li",
            "user": "lbc402",
            "type": "user"
          },
          "name": "Bingchuan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:44:57.735Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852a",
          "user": {
            "_id": "6804ce31d205d72ddbeec8a0",
            "avatarUrl": "/avatars/772d20a653649063158cba166298801a.svg",
            "isPro": false,
            "fullname": "Tianxiang Ma",
            "user": "TianxiangMa",
            "type": "user"
          },
          "name": "Tianxiang Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:45:10.156Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852b",
          "name": "Lijie Liu",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852c",
          "user": {
            "_id": "619b404bab4c7b7f16a7d57d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619b404bab4c7b7f16a7d57d/coT_UGRfBOUAeSxjyhdlG.jpeg",
            "isPro": false,
            "fullname": "Mingcong Liu",
            "user": "onion-liu",
            "type": "user"
          },
          "name": "Mingcong Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:23.740Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852d",
          "name": "Yi Zhang",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852e",
          "name": "Gen Li",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852f",
          "user": {
            "_id": "6752cd83ffaeeb979db974ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
            "isPro": false,
            "fullname": "Xinghui Li",
            "user": "Crayon-Shinchan",
            "type": "user"
          },
          "name": "Xinghui Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:45:40.054Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758530",
          "name": "Siyu Zhou",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758531",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758532",
          "user": {
            "_id": "67bc6b515d9470ec64bdcc33",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dttL8Fb3bKCVG5zjg_02q.png",
            "isPro": false,
            "fullname": "Xinglong Wu",
            "user": "Xingzhe-xlwu",
            "type": "user"
          },
          "name": "Xinglong Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:45:50.807Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:11:56.000Z",
      "submittedOnDailyAt": "2025-06-24T01:12:12.451Z",
      "title": "Fantom Data: General Topic of Video Generation for Entity Consistency\nDataset",
      "submittedOnDailyBy": {
        "_id": "6304e2dabad6ce7fc0287d57",
        "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
        "isPro": false,
        "fullname": "Zhuowei_Chen",
        "user": "ZhuoweiChen",
        "type": "user"
      },
      "summary": "주제 비디오 생성은 최근에 큰 진전을 보이고 있습니다. 그러나 현재의 모델은 문맥 텍스트의 지시에 충실하게 따라야 하는 어려운 문제를 발견하고 있습니다. 이 제한, 일반적으로 알려진 복사 패스팅 문제는 대응된 페어의 훈련 패러다임이 광범위하게 사용되기 때문에 발생합니다. 이 접근법은 타겟 비디오와 같은 스케인에서 참조 이미지를 샘플링함으로써, 주제의 인식을 배경과 컨텍스트 속성으로 연결하는 것을 시도합니다. 이 문제를 해결하기 위해, 우리는 Phantom-Data를 소개합니다. Phantom-Data는 처음의 일반적인 주제 비디오의 일치성을 위한 데이터셋입니다. 이 데이터셋은 약 100만 개의 식별자 일치하는 페어를 포함하며, 다양한 카테고리에 걸쳐 구성되어 있습니다. 우리의 데이터셋은 3단계의 파이프라인으로 구축되어 있습니다. (1) 일반적인 입력에 맞는 주제 탐색 모듈, (2) 5300만 이상의 비디오와 30억 이상의 이미지에서 큰 규모의 크로스 컨텍스트 주제 검색, (3) 선두 체커 로이드를 통해 컨텍스트 변화의 시각적 일치성을 보장하는 식별 확인을 통해 구축되었습니다. 세부적인 실험은 Phantom-Data에서 훈련은 페어 내 기반 선과 비교하여 비슷한 수준의 식별자 일치성을 유지하는 동시에, 프로ン퓰트 어레이미션과 시각적 품질을 크게 향상시키는 것을 보여주고 있습니다.",
      "upvotes": 21,
      "discussionId": "685a0fb40e4ad7e219758535",
      "projectPage": "https://phantom-video.github.io/Phantom-Data/",
      "githubRepo": "https://github.com/Phantom-video/Phantom-Data",
      "ai_summary": "A cross-pair dataset called Phantom-Data improves subject-to-video generation by enhancing prompt alignment and visual quality while maintaining identity consistency.",
      "ai_keywords": [
        "Phantom-Data",
        "subject-to-video generation",
        "copy-paste problem",
        "in-pair training paradigm",
        "subject detection",
        "cross-context subject retrieval",
        "prior-guided identity verification"
      ]
    },
    "publishedAt": "2025-06-23T13:11:56.000Z",
    "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation\n  Dataset",
    "summary": "Subject-to-video generation has witnessed substantial progress in recent\nyears. However, existing models still face significant challenges in faithfully\nfollowing textual instructions. This limitation, commonly known as the\ncopy-paste problem, arises from the widely used in-pair training paradigm. This\napproach inherently entangles subject identity with background and contextual\nattributes by sampling reference images from the same scene as the target\nvideo. To address this issue, we introduce Phantom-Data, the first\ngeneral-purpose cross-pair subject-to-video consistency dataset, containing\napproximately one million identity-consistent pairs across diverse categories.\nOur dataset is constructed via a three-stage pipeline: (1) a general and\ninput-aligned subject detection module, (2) large-scale cross-context subject\nretrieval from more than 53 million videos and 3 billion images, and (3)\nprior-guided identity verification to ensure visual consistency under\ncontextual variation. Comprehensive experiments show that training with\nPhantom-Data significantly improves prompt alignment and visual quality while\npreserving identity consistency on par with in-pair baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18851.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6304e2dabad6ce7fc0287d57",
      "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
      "fullname": "Zhuowei_Chen",
      "name": "ZhuoweiChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18896",
      "authors": [
        {
          "_id": "685a02790e4ad7e2197584b2",
          "name": "Jiaru Zou",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b3",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b4",
          "name": "Jingwen Gu",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b5",
          "name": "Jiahao Qiu",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b6",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b7",
          "name": "Jingrui He",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b8",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:59:02.000Z",
      "submittedOnDailyAt": "2025-06-24T01:03:32.146Z",
      "title": "ReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM: 궤도에 관심있는 PRMs에서 긴 생각의 연속적인 추론\n\nReasonFlux-PRM:",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Process Reward Models (PRMs)은 최근 대규모 언어 모델(LLMs)의 중간적인 이유를 체크하기 위한 강력한 프레임워크로 발전하여 왔습니다. 지난 주의 PRMs는 주로 모델의 최종적인 출력에 대한 데이터로 훈련되어있고, 중간적인 사고의 경로를 강력한 평가하기가 어려웠으며, 특히 Deepseek-R1과 같은 발전된 이유 모델이 출력하는 경로-출력의 새로운 설정에서尤为如此。本研究에서는, ReasonFlux-PRM을 소개합니다. ReasonFlux-PRM은 경로에 관심 있는 새로운 PRM이며, 경로-출력의 이유 트래스 평가에 특화된 설계를 합니다. ReasonFlux-PRM은 단계 수준과 경로 수준의 서브젝션을 사용하며, 구조화된 chain-of-thought 데이터에 대응하는 미세한 보상의 배분이 가능합니다. ReasonFlux-PRM은 옵스레인 및 온라인의 두 가지 설정에서 보상의 서브젝션을 지원합니다. 구체적으로는, (i) 다운스트리밍의 훈련에서 고품질의 모델을 보장하는 데이터 선택, (ii) 강화학습 시 정책 최적화에 필요한 밀도가 높은 프로세스 수준의 보상 제공, (iii) 보상을 가이드로 하는 Best-of-N 테스트 시간 스케일링 가능하도록 합니다. AIME, MATH500, GPQA-Diamond 등 어려운 다운스트리밍 벤치마크에서의 실험 결과를 통해, ReasonFlux-PRM-7B는 강력한 PRMs(예: Qwen2.5-Math-PRM-72B)이나 인간 칼레이드 기반 라인 모델보다 고품질의 데이터 선택을 보여주며, 평균적인 수익률은 훈련 시 12.1%, 강화학습 시 4.5%, 테스트 시간 스케일링 시 6.3%를 획득했습니다. 또한, 리소스 제한된 애플리케이션이나 에지 데플로yment에 적합한 효율적인 ReasonFlux-PRM-1.5B를 릴리즈합니다. 프로젝트: https://github.com/Gen-Verse/ReasonFlux",
      "upvotes": 20,
      "discussionId": "685a027a0e4ad7e2197584b9",
      "projectPage": "https://huggingface.co/collections/Gen-Verse/reasonflux-prm-68463c73cf1c6a0ec6fafeb5",
      "githubRepo": "https://github.com/Gen-Verse/ReasonFlux",
      "ai_summary": "ReasonFlux-PRM, a novel trajectory-aware Process Reward Model, evaluates reasoning traces with step-level and trajectory-level supervision, enhancing performance in model distillation, reinforcement learning, and test-time scaling.",
      "ai_keywords": [
        "Process Reward Models",
        "trajectory-aware PRM",
        "trajectory-response outputs",
        "step-level supervision",
        "trajectory-level supervision",
        "chain-of-thought data",
        "model distillation",
        "policy optimization",
        "reinforcement learning",
        "Best-of-N test-time scaling",
        "AIME",
        "MATH500",
        "GPQA-Diamond"
      ]
    },
    "publishedAt": "2025-06-23T13:59:02.000Z",
    "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
    "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18896.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18898",
      "authors": [
        {
          "_id": "685a07510e4ad7e2197584c6",
          "user": {
            "_id": "62318c0386753f5f41d0e261",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
            "isPro": false,
            "fullname": "Jiaming Han",
            "user": "csuhan",
            "type": "user"
          },
          "name": "Jiaming Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:47.088Z",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584c7",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584c8",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584c9",
          "user": {
            "_id": "6365a174ad2c9e8b6731dd0f",
            "avatarUrl": "/avatars/2f4dd0eda92bca5a7464129fe7d961f9.svg",
            "isPro": false,
            "fullname": "Hanyu Wang",
            "user": "hywang66",
            "type": "user"
          },
          "name": "Hanyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:44.987Z",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584ca",
          "name": "Qi Zhao",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584cb",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584cc",
          "name": "Hao He",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584cd",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584ce",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:59:14.000Z",
      "submittedOnDailyAt": "2025-06-24T01:02:20.046Z",
      "title": "비전을 지역어로: 시각적 인식과 생성을 통합하기 위한 텍스트에 대응하는 표현",
      "submittedOnDailyBy": {
        "_id": "62318c0386753f5f41d0e261",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
        "isPro": false,
        "fullname": "Jiaming Han",
        "user": "csuhan",
        "type": "user"
      },
      "summary": "이 논문에서는 공통된 이산적인 의미 표현을 사용하여 이미지 이해와 생성을 통합하는 다양한 프레임워크를 제안합니다. 핵심 부분에는 텍스트 대응 토큰너퍼(TA-Tok)가 있으며, 큰 규모의 언어 모델(LLM)의 배포에 의해 투하된 텍스트 대응 코드 박스를 사용하여 이미지를 이산 토큰으로 변환합니다. 이 프레임워크에서 시각과 텍스트를 확장 배포로 통합하여, 모델 유형에 관계없이 공통적인 인터페이스를 통해 크로스 모델의 입력과 출력을 가능하게 합니다. 또한, 효과성과 시각적 세부 정보를 균형을 유지하기 위해 스케일 적응적인 인코딩과 디코딩을 제안하고, 고품질의 시각적 출력을 생성하기 위한 생성적인 데텍노나이저를 구현합니다. 디코딩의 다양한 필요를 충족하기 위해, 고속한 자동 회귀 모델과 분기 기반 모델의 두 가지 보간 데텍노나이저를 사용합니다. 모델 융합을 향상시키기 위해, 발전된 사전 학습 태스크를 검토하고, 시각적 이해와 생성 모두에서 향상을 나타냅니다. 벤치마크에서의 실험은, 현재 다양한 LLM 방법들을 추월하며, 더 빠른 수렴과 더 높은 학습 효과성을 달성합니다. 코드, 모델, 데이터는 아래 URL에서 사용 가능합니다. https://tar.csuhan.com",
      "upvotes": 18,
      "discussionId": "685a07520e4ad7e2197584cf",
      "projectPage": "https://tar.csuhan.com",
      "githubRepo": "https://github.com/csuhan/Tar",
      "ai_summary": "A multimodal framework uses a Text-Aligned Tokenizer (TA-Tok) to integrate vision and text into a unified space, employing a generative de-tokenizer with autoregressive and diffusion-based models for efficient and high-fidelity visual outputs.",
      "ai_keywords": [
        "Text-Aligned Tokenizer (TA-Tok)",
        "multimodal LLM",
        "Tar",
        "scale-adaptive encoding",
        "diffusion-based model",
        "autoregressive model",
        "modality fusion",
        "pre-training tasks"
      ]
    },
    "publishedAt": "2025-06-23T13:59:14.000Z",
    "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
    "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18898.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62318c0386753f5f41d0e261",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
      "fullname": "Jiaming Han",
      "name": "csuhan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18254",
      "authors": [
        {
          "_id": "685a39d60e4ad7e219758622",
          "name": "Tianyu Yu",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758623",
          "name": "Bo Ji",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758624",
          "name": "Shouli Wang",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758625",
          "name": "Shu Yao",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758626",
          "name": "Zefan Wang",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758627",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758628",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758629",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862a",
          "name": "Yuan Yao",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862b",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862c",
          "name": "Maosong Sun",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862d",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T02:56:36.000Z",
      "submittedOnDailyAt": "2025-06-24T04:14:54.081Z",
      "title": "RLPR: 무 유효 데이터에서 일반 영역에서의 RLVR 추론",
      "submittedOnDailyBy": {
        "_id": "64abc4aa6cadc7aca585dddf",
        "avatarUrl": "/avatars/736afea979cd0021c7a37f68731524ea.svg",
        "isPro": false,
        "fullname": "Tianyu Yu",
        "user": "Yirany",
        "type": "user"
      },
      "summary": "강화학습에 의한 신뢰 가능한 보상(RLVR)는 LLM의 논리 능력의 발전에 대한 희망적인 가능성을 보여주고 있습니다. 그러나 그 성공은 주로 수학과 코드의 분야에 제한되어 있습니다. 이 주요한 한계는 분야专門적인 데이터베이스의 중시로 인해 무한히 복잡하고 scalability의 한계로 인한 것입니다. 이러한 도전에 대처하기 위해 우리의 주요한 견해는 LLM이 정확한 자유형의 답을 생성하는 고유한 확률이 논리적인 보상(이론 과정이 정확한 답에 어떻게 유도하는지)을 자체적으로 평가하는 것을 보여줍니다. 이 통찰에 기반하여 RLPR(확률 기반 보상을 사용한 강화 학습)이라는 간단한 데이터베이스가 없는 프레임워크를 제안합니다. RLPR는 LLM의 고유한 토큰 확률 스코어를 참조답과して 보상 신호로 사용하며, 훈련 중의 기대 보상을 최대화합니다. 우리들은 이 노이즈가 많은 확률 보상의 높은 분산을 해결하는 것이 중요하다고 인식하고, LLM의 고유한 확률로부터 정확한 안정적인 보상을 보장하기 위해 prob-to-reward와 안정화 방법을 제안합니다. 4개의 일반적인 분야 벤치마크와 3개의 수학 벤치마크에서의 세부적인 실험은, Gemma, Llama, Qwen 기반의 모델의 두 개를 통해 논리 능력이 일관되게 향상되는 것을 보여주고 있습니다. 특히, TheoremQA에서 VeriFree를 7.6점, Minerva에서 7.5점을 초과하며, 7개의 벤치마크 전체에서 General-Reasoner의 평균 점수를 1.6점 초과했습니다.",
      "upvotes": 18,
      "discussionId": "685a39d80e4ad7e21975862e",
      "projectPage": "https://github.com/OpenBMB/RLPR",
      "githubRepo": "https://github.com/OpenBMB/RLPR",
      "ai_summary": "RLPR, a verifier-free framework using LLM's token probability scores as reward signals, enhances reasoning capabilities across both general and mathematical domains, outperforming other methods in various benchmarks.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "reasoning capabilities",
        "LLM",
        "RLPR",
        "token probability scores",
        "prob-to-reward",
        "stabilizing methods",
        "TheoremQA",
        "Minerva",
        "General-Reasoner"
      ]
    },
    "publishedAt": "2025-06-22T22:56:36.000Z",
    "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising\npotential in advancing the reasoning capabilities of LLMs. However, its success\nremains largely confined to mathematical and code domains. This primary\nlimitation stems from the heavy reliance on domain-specific verifiers, which\nresults in prohibitive complexity and limited scalability. To address the\nchallenge, our key observation is that LLM's intrinsic probability of\ngenerating a correct free-form answer directly indicates its own evaluation of\nthe reasoning reward (i.e., how well the reasoning process leads to the correct\nanswer). Building on this insight, we propose RLPR, a simple verifier-free\nframework that extrapolates RLVR to broader general domains. RLPR uses the\nLLM's own token probability scores for reference answers as the reward signal\nand maximizes the expected reward during training. We find that addressing the\nhigh variance of this noisy probability reward is crucial to make it work, and\npropose prob-to-reward and stabilizing methods to ensure a precise and stable\nreward from LLM intrinsic probabilities. Comprehensive experiments in four\ngeneral-domain benchmarks and three mathematical benchmarks show that RLPR\nconsistently improves reasoning capabilities in both areas for Gemma, Llama,\nand Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6\npoints on TheoremQA and 7.5 points on Minerva, and even surpasses strong\nverifier-model-dependent approaches General-Reasoner by 1.6 average points\nacross seven benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18254.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64abc4aa6cadc7aca585dddf",
      "avatarUrl": "/avatars/736afea979cd0021c7a37f68731524ea.svg",
      "fullname": "Tianyu Yu",
      "name": "Yirany",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15741",
      "authors": [
        {
          "_id": "685a48970e4ad7e219758662",
          "name": "He Zhu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758663",
          "user": {
            "_id": "64301abe450c0de9a1d3d18e",
            "avatarUrl": "/avatars/01b284874dadc7d21d656c53dcb77e42.svg",
            "isPro": false,
            "fullname": "tianrui",
            "user": "tianyue818",
            "type": "user"
          },
          "name": "Tianrui Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T07:14:35.823Z",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758664",
          "user": {
            "_id": "6578265ddea7e2122d02f6ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578265ddea7e2122d02f6ba/Bh6JjoVF5ceLSjV7Z7nTk.jpeg",
            "isPro": false,
            "fullname": "kang zhu",
            "user": "kangz",
            "type": "user"
          },
          "name": "King Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T09:33:02.082Z",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758665",
          "name": "Heyuan Huang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758666",
          "name": "Yeyi Guan",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758667",
          "name": "Jinxiang Xia",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758668",
          "name": "Yi Yao",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758669",
          "name": "Hanhao Li",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866a",
          "name": "Ningning Wang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866b",
          "name": "Pai Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866c",
          "name": "Tianhao Peng",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866d",
          "name": "Xin Gui",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866e",
          "name": "Xiaowan Li",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866f",
          "name": "Yuhui Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758670",
          "name": "Yuchen Eleanor Jiang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758671",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758672",
          "name": "Changwang Zhang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758673",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758674",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758675",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758676",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758677",
          "name": "Xitong Gao",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758678",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758679",
          "name": "Wangchunshu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T17:59:02.000Z",
      "submittedOnDailyAt": "2025-06-24T05:17:03.413Z",
      "title": "OAgents: 유효한 아가네트의 구축에 관한 실증 연구",
      "submittedOnDailyBy": {
        "_id": "628c8598ef14f971b698107f",
        "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
        "isPro": false,
        "fullname": "Zhou",
        "user": "Wangchunshu",
        "type": "user"
      },
      "summary": "최근, Agentic AI의 연구 분야가 증가하고 있습니다. 그러나 현재의 Agent 연구의 실용은 표준화와 과학성 부족으로, 방법 간 공정한 비교가 어려운 문제를 가지고 있습니다. 이로 인해, Agent 프레임워크의 다른 설계 선택이 어떤 효과를 줄 수 있는지는 알 수 없으며, 그 발전을 평가하는 것도 어려워집니다. 본 연구에서는 GAIA 벤치마크와 BrowseComp를 체계적으로 실험적으로 조사하고, 공정하고 엄격한 방법으로 대중적인 설계 선택의 영향을 살펴볼 것입니다. 우리는 이전의 연구, 오픈 소스를 포함한, 표준 평가 프로토콜의 결함이 있으며, 재현성이 없고, 랜덤 실험 간 극단적인 변동이 있음을 발견했습니다. 따라서, 우리는 안정화를 위해 더 강력한 평가 프로토콜을 도입했습니다. 우리의 연구는 어떤 구성 요소와 설계가 효과적인 Agent에 필수적인 것인지, 그 외는 비효율적이라는 것을 보여줍니다. 우리의 발견에 기반하여, 새로운 기반 Agent 프레임워크 OAgents를 구축하고 오픈 소스로 제공했습니다. OAgents는 다수의 Agent 구성 요소의 모듈화 설계를 제공하며, 미래의 Agentic AI의 연구를 촉진합니다.",
      "upvotes": 18,
      "discussionId": "685a48980e4ad7e21975867a"
    },
    "publishedAt": "2025-06-17T13:59:02.000Z",
    "title": "OAgents: An Empirical Study of Building Effective Agents",
    "summary": "Recently, Agentic AI has become an increasingly popular research field.\nHowever, we argue that current agent research practices lack standardization\nand scientific rigor, making it hard to conduct fair comparisons among methods.\nAs a result, it is still unclear how different design choices in agent\nframeworks affect effectiveness, and measuring their progress remains\nchallenging. In this work, we conduct a systematic empirical study on GAIA\nbenchmark and BrowseComp to examine the impact of popular design choices in key\nagent components in a fair and rigorous manner. We find that the lack of a\nstandard evaluation protocol makes previous works, even open-sourced ones,\nnon-reproducible, with significant variance between random runs. Therefore, we\nintroduce a more robust evaluation protocol to stabilize comparisons. Our study\nreveals which components and designs are crucial for effective agents, while\nothers are redundant, despite seeming logical. Based on our findings, we build\nand open-source OAgents, a new foundation agent framework that achieves\nstate-of-the-art performance among open-source projects. OAgents offers a\nmodular design for various agent components, promoting future research in\nAgentic AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15741.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "628c8598ef14f971b698107f",
      "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
      "fullname": "Zhou",
      "name": "Wangchunshu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18903",
      "authors": [
        {
          "_id": "685a1ce80e4ad7e2197585b7",
          "name": "Runjia Li",
          "hidden": false
        },
        {
          "_id": "685a1ce80e4ad7e2197585b8",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "685a1ce80e4ad7e2197585b9",
          "name": "Andrea Vedaldi",
          "hidden": false
        },
        {
          "_id": "685a1ce80e4ad7e2197585ba",
          "name": "Tomas Jakab",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638e29cf319f9c746b87ad4b/Gt6zbaM3ILEQIG4Cl1W2J.mp4"
      ],
      "publishedAt": "2025-06-23T17:59:56.000Z",
      "submittedOnDailyAt": "2025-06-24T02:12:37.233Z",
      "title": "VMem: 일관된 인터랙티브 비디오 시나리오의 생성을 위해 서플 인덱스를 사용하는 뷰메모리",
      "submittedOnDailyBy": {
        "_id": "638e29cf319f9c746b87ad4b",
        "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
        "isPro": true,
        "fullname": "Runjia Li",
        "user": "liguang0115",
        "type": "user"
      },
      "summary": "우리는 새로운 메모리 구조를 제안합니다. 이 구조를 통해 상호작용적으로 환경 탐색할 수 있는 비디오 생성기를 구축할 수 있습니다. 이전에는 2D 시각을 외부로 확장하면서 3D 기하학을 단계적으로 재구성하는 방법으로 유사한 결과를 달성했지만, 이 방법은 오류가 급격히 쌓입니다. 또한, 장기간 일관성을 유지하는 비디오 생성기에도 동일한 결과를 얻을 수 있었습니다. 이러한 제한을 해결하기 위해, 우리는 과거의 시각을 3D 표면 요소(surfels)에 기반하여 기하학적으로 인덱싱하고 기억하는 메커니즘인 Surfel-Indexed View Memory(VMem)를 도입했습니다. VMem은 새로운 시각을 생성할 때 가장 관련성이 높은 과거의 시각을 효율적으로 얻을 수 있습니다. 이렇게, 과거의 일부를 컨텍스트로 사용하는 것보다, 계산 비용이 적은 부분이 想象된 환경의 일관적인 탐색을 수행할 수 있습니다. 우리의 방법은 장기간 시각 합성 벤치마크에서 평가되었으며, 기존 방법과 비교하여 시각의 일관성과 카메라 제어에 대한 우수한 성능을 보여줍니다.",
      "upvotes": 8,
      "discussionId": "685a1ce80e4ad7e2197585bb",
      "projectPage": "https://v-mem.github.io/",
      "githubRepo": "https://github.com/runjiali-rl/vmem",
      "ai_summary": "A novel memory mechanism called Surfel-Indexed View Memory enhances video generation by efficiently remembering and retrieving relevant past views, improving long-term scene coherence and reducing computational cost.",
      "ai_keywords": [
        "memory mechanism",
        "video generators",
        "out-painting",
        "3D geometry",
        "context window",
        "Surfel-Indexed View Memory",
        "surfels",
        "scene coherence",
        "camera control",
        "long-term scene synthesis benchmarks"
      ]
    },
    "publishedAt": "2025-06-23T13:59:56.000Z",
    "title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed\n  View Memory",
    "summary": "We propose a novel memory mechanism to build video generators that can\nexplore environments interactively. Similar results have previously been\nachieved by out-painting 2D views of the scene while incrementally\nreconstructing its 3D geometry, which quickly accumulates errors, or by video\ngenerators with a short context window, which struggle to maintain scene\ncoherence over the long term. To address these limitations, we introduce\nSurfel-Indexed View Memory (VMem), a mechanism that remembers past views by\nindexing them geometrically based on the 3D surface elements (surfels) they\nhave observed. VMem enables the efficient retrieval of the most relevant past\nviews when generating new ones. By focusing only on these relevant views, our\nmethod produces consistent explorations of imagined environments at a fraction\nof the computational cost of using all past views as context. We evaluate our\napproach on challenging long-term scene synthesis benchmarks and demonstrate\nsuperior performance compared to existing methods in maintaining scene\ncoherence and camera control.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638e29cf319f9c746b87ad4b/Gt6zbaM3ILEQIG4Cl1W2J.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18903.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638e29cf319f9c746b87ad4b",
      "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
      "fullname": "Runjia Li",
      "name": "liguang0115",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18309",
      "authors": [
        {
          "_id": "685a2df10e4ad7e219758604",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758605",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758606",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758607",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758608",
          "name": "Jianfeng Liu",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758609",
          "name": "Yuefeng Zhan",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860a",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860b",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860c",
          "name": "Weiwei Deng",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860d",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860e",
          "name": "Feng Sun",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860f",
          "name": "Qi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T05:51:52.000Z",
      "submittedOnDailyAt": "2025-06-24T03:21:15.103Z",
      "title": "리티ン고： 추천 시스템의 사용자 프로파일 생성에 대한 탐색",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "유저 프로파일링은 추천 시스템에서 중요하며, 유저의 상호작용 데이터를 간결한 구조화된 표현으로 변환하여, 유저별 추천을 구동할 수 있습니다. 전통적인 임베딩 기반의 프로파일링은 해석성과 적응성에 부족하지만, 최근의 대규모 언어 모델(LLMs)의 발전으로, 의미적으로 풍부하고 투명한 텍스트 기반의 프로파일링이 가능해졌습니다. 그러나 현재의 방법들은 고정된 포맷에 의존하며, 유저의 행동의 전체 다양성을 파악하는 능력이 제한되어 있습니다. 본 논문에서는, LettinGo라는 새로운 프레임워크를 소개합니다. 이 프레임워크는, 다양성과 적응성을 가진 유저 프로파일링을 생성하기 위해, LLMs의 표현력을 활용하고, 하류의 추천 태스크에서 직접적인 피드백을 받습니다. 이 접근 방식은 정규적인 微調編集(SFT)으로 인한 刚성한 제약을 피할 수 있습니다. 대신, Direct Preference Optimization(DPO)를 사용하여, 프로파일링 생성자를 태스크의 특정 성능에 맞게, 프로파일링이 적응적이고 효과적이도록 보장합니다. LettinGo는 3단계로 동작합니다: 1. 다양한 유저 프로파일링을 탐색, 2. 추천 시스템에서의 영향에 기반한 프로파일링의 품질을 평가, 3. 태스크의 실적으로부터 얻은 비교적인 취향 데이터에 기반한 프로파일링 생성을 조정합니다. 실험 결과를 통해, 우리의 프레임워크가 추천 정확도, 유연성, 맥락 인식을 크게 향상시키는 것을 보여주고 있습니다. 이 연구는, 다음 세대의 추천 시스템의 프로파일링 생성의 중요한 혁신으로서 기능합니다.",
      "upvotes": 6,
      "discussionId": "685a2df20e4ad7e219758610",
      "ai_summary": "LettinGo enhances user profiling via diverse, adaptive profiles generated using LLMs and Direct Preference Optimization, improving recommendation accuracy and flexibility.",
      "ai_keywords": [
        "large language models (LLMs)",
        "semantically richer",
        "more transparent",
        "supervised fine-tuning (SFT)",
        "Direct Preference Optimization (DPO)",
        "profile generator",
        "pairwise preference data",
        "recommendation systems",
        "recommendation accuracy",
        "flexibility",
        "contextual awareness"
      ]
    },
    "publishedAt": "2025-06-23T01:51:52.000Z",
    "title": "LettinGo: Explore User Profile Generation for Recommendation System",
    "summary": "User profiling is pivotal for recommendation systems, as it transforms raw\nuser interaction data into concise and structured representations that drive\npersonalized recommendations. While traditional embedding-based profiles lack\ninterpretability and adaptability, recent advances with large language models\n(LLMs) enable text-based profiles that are semantically richer and more\ntransparent. However, existing methods often adhere to fixed formats that limit\ntheir ability to capture the full diversity of user behaviors. In this paper,\nwe introduce LettinGo, a novel framework for generating diverse and adaptive\nuser profiles. By leveraging the expressive power of LLMs and incorporating\ndirect feedback from downstream recommendation tasks, our approach avoids the\nrigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ\nDirect Preference Optimization (DPO) to align the profile generator with\ntask-specific performance, ensuring that the profiles remain adaptive and\neffective. LettinGo operates in three stages: (1) exploring diverse user\nprofiles via multiple LLMs, (2) evaluating profile quality based on their\nimpact in recommendation systems, and (3) aligning the profile generation\nthrough pairwise preference data derived from task performance. Experimental\nresults demonstrate that our framework significantly enhances recommendation\naccuracy, flexibility, and contextual awareness. This work enhances profile\ngeneration as a key innovation for next-generation recommendation systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18309.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18631",
      "authors": [
        {
          "_id": "685a1b390e4ad7e2197585a9",
          "name": "Chenxing Wei",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585aa",
          "name": "Jiarui Yu",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ab",
          "name": "Ying Tiffany He",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ac",
          "name": "Hande Dong",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ad",
          "name": "Yao Shu",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ae",
          "name": "Fei Yu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65ed3051492a7f35db21fea2/Oc7EWF5wDYqu1FEvfuAlq.png"
      ],
      "publishedAt": "2025-06-23T13:36:24.000Z",
      "submittedOnDailyAt": "2025-06-24T02:05:49.825Z",
      "title": "Redit: 보상 설계를 이용한 LLM 정책 최적화 향상",
      "submittedOnDailyBy": {
        "_id": "65ed3051492a7f35db21fea2",
        "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
        "isPro": false,
        "fullname": "Chenxing Wei",
        "user": "kittttttt",
        "type": "user"
      },
      "summary": "DeepSeek-R1는 규칙 기반의 보상 시스템(Reward System)을 사용하여 대규모 언어 모델(LLM)의 추론 능력을 성공적으로 향상시켰습니다. 이 보상 시스템은 \"완전\"하다는 것을 보여주며 보상 해킹을 효과적으로 억제하지만, 이 보상 함수는 일반적으로 이산적(discrete)입니다. 우리의 실험적 관찰에서 이 이산적인 보상은 경사도의 이상, 불안정한 최적화, 그리고 점차적인 수렴을招致합니다. 이러한 문제를 해결하기 위해, 우리는 ReDit(Reward Design)를 제안했습니다. ReDit은 간단한 랜덤성을 추가하여 이산적인 보상 신호를 디자이너하는 방법입니다. 이 섭동된 보상으로, 학습 프로세스 전체에서 탐험적 경사도가 계속적으로 제공되고, 평활한 경사도 업데이트가 가능하여 수렴을 가속화합니다. 注入된 랜덤성은 평탄한 보상 영역에서 독특성을 불러일으키며, 모델이 새로운 정책을 찾아 지역적 최적해에서逸脱하도록 촉구합니다. 다양한 태스크에서의 실험은 ReDit의 효과성과 효율성을 보여주었습니다. 평균적으로, ReDit은 GRPO와 비교하여 약 10%의 학습 단계를 통해相当한 성능을 달성하고, 같은 길이의 학습 기간에서 GRPO보다 4%의 성능 향상을 나타냅니다. ReDit에서 경사도 문제를 억제하는 큰 효과를 확인했습니다. 또한 이론적인 분석도 제공되어 이 우수한 점을 한 단계 더 증명합니다.",
      "upvotes": 5,
      "discussionId": "685a1b390e4ad7e2197585af",
      "githubRepo": "https://github.com/kithib/ReDit",
      "ai_summary": "ReDit, a reward dithering method, addresses issues in discrete reward systems by introducing noise, leading to smoother optimization and faster convergence compared to standard methods.",
      "ai_keywords": [
        "rule-based reward system",
        "reward hacking",
        "discrete rewards",
        "gradient anomaly",
        "unstable optimization",
        "slow convergence",
        "random noise",
        "exploratory gradients",
        "flat reward regions",
        "local optima",
        "vanilla GRPO",
        "performance improvement",
        "gradient issues"
      ]
    },
    "publishedAt": "2025-06-23T09:36:24.000Z",
    "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
    "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65ed3051492a7f35db21fea2/Oc7EWF5wDYqu1FEvfuAlq.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18631.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ed3051492a7f35db21fea2",
      "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
      "fullname": "Chenxing Wei",
      "name": "kittttttt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18527",
      "authors": [
        {
          "_id": "685a0cba0e4ad7e21975850c",
          "name": "JiaKui Hu",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e21975850d",
          "name": "Yuxiao Yang",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e21975850e",
          "name": "Jialun Liu",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e21975850f",
          "name": "Jinbo Wu",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e219758510",
          "name": "Chen Zhao",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e219758511",
          "name": "Yanye Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T11:28:37.000Z",
      "submittedOnDailyAt": "2025-06-24T03:17:08.044Z",
      "title": "自動逆転를 통해 다중점의 일치 이미지를 생성합니다.",
      "submittedOnDailyBy": {
        "_id": "64ccd5cc4726a3f833831087",
        "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
        "isPro": false,
        "fullname": "Hu",
        "user": "Jiakui",
        "type": "user"
      },
      "summary": "다중시점 이미지는 3D 콘텐츠 생성에서 중요한 요소입니다. 주요 문제점은 다중시점에서의 일치성과 다양한 조건에서 형태와텍스처의 유효한 합성입니다. 본 논문에서는 다중시점 자동회귀(MV-AR) 메소드를 제안하고, 자동회귀 모델을 활용하여 임의의 Prompt로부터 순차적으로 일치된 다중시점 이미지를 생성하는 것을 목표로 합니다. 먼저, AR 모델의 다음 토큰 예측 능력은 순차적인 다중시점 합성의 효율성을 크게 향상시킬 수 있습니다. 시점이 넓게 떨어져 있을 때, MV-AR는 모든 이전 시점을 활용하여 유효한 참조 정보를 추출할 수 있습니다. 다음으로, 다양한 Prompt를 처리할 수 있는 방법론을 마련하기 위해 구조 설계와 훈련 단계를 활용한 통합 모델을 제안합니다. 다양한 조건을 처리하기 위해, 맥락, 카메라의 자세, 이미지, 형태에 대한 조건 입력 모듈을 도입합니다. 다중 모델 조건을 동시에 관리하기 위해, 순차적인 훈련 단계를 활용합니다. 이 단계는 X-to-multi-view (X2mv) 모델의 개발을 지원하기 위해, 텍스트로부터 다중시점 이미지(t2mv) 모델을 기반으로 조건을 랜덤하게 떨어뜨리고 조합하여 구현할 수 있습니다. 마지막으로, 제한된 고품질 데이터로 인한 과적합 문제를 해결하기 위해, \"Shuffle View\" 데이터 확장 기법을 제안하여 훈련 데이터를 수배로 확대할 수 있습니다. 실험은 MV-AR의 성능과 다양성을 보여주고, 다양한 조건에서 일치된 다중시점 이미지를 생성하고, 선진적인 디퓨전 기반의 다중시점 이미지 생성 모델과 같은 성능을 보여주었습니다. 코드와 모델은 https://github.com/MILab-PKU/MVAR에서 공개 예정입니다.",
      "upvotes": 4,
      "discussionId": "685a0cbb0e4ad7e219758512",
      "ai_summary": "The Multi-View Auto-Regressive (MV-AR) method uses an auto-regressive model to generate consistent multi-view images from prompts, addressing challenges in shape and texture synthesis across diverse conditions.",
      "ai_keywords": [
        "Multi-View Auto-Regressive",
        "MV-AR",
        "auto-regressive model",
        "next-token-prediction",
        "condition injection modules",
        "text-to-multi-view",
        "X-to-multi-view",
        "progressive training strategy",
        "Shuffle View",
        "data augmentation"
      ]
    },
    "publishedAt": "2025-06-23T07:28:37.000Z",
    "title": "Auto-Regressively Generating Multi-View Consistent Images",
    "summary": "Generating multi-view images from human instructions is crucial for 3D\ncontent creation. The primary challenges involve maintaining consistency across\nmultiple views and effectively synthesizing shapes and textures under diverse\nconditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR)\nmethod, which leverages an auto-regressive model to progressively generate\nconsistent multi-view images from arbitrary prompts. Firstly, the\nnext-token-prediction capability of the AR model significantly enhances its\neffectiveness in facilitating progressive multi-view synthesis. When generating\nwidely-separated views, MV-AR can utilize all its preceding views to extract\neffective reference information. Subsequently, we propose a unified model that\naccommodates various prompts via architecture designing and training\nstrategies. To address multiple conditions, we introduce condition injection\nmodules for text, camera pose, image, and shape. To manage multi-modal\nconditions simultaneously, a progressive training strategy is employed. This\nstrategy initially adopts the text-to-multi-view (t2mv) model as a baseline to\nenhance the development of a comprehensive X-to-multi-view (X2mv) model through\nthe randomly dropping and combining conditions. Finally, to alleviate the\noverfitting problem caused by limited high-quality data, we propose the\n\"Shuffle View\" data augmentation technique, thus significantly expanding the\ntraining data by several magnitudes. Experiments demonstrate the performance\nand versatility of our MV-AR, which consistently generates consistent\nmulti-view images across a range of conditions and performs on par with leading\ndiffusion-based multi-view image generation models. Code and models will be\nreleased at https://github.com/MILab-PKU/MVAR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ccd5cc4726a3f833831087",
      "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
      "fullname": "Hu",
      "name": "Jiakui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18349",
      "authors": [
        {
          "_id": "685a02460e4ad7e2197584a9",
          "user": {
            "_id": "659c6a50615d5e661222fe16",
            "avatarUrl": "/avatars/8a946482e49a821dbe397dc3898f22c5.svg",
            "isPro": false,
            "fullname": "Zichong Li",
            "user": "Pearush",
            "type": "user"
          },
          "name": "Zichong Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:56.270Z",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584aa",
          "name": "Chen Liang",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ab",
          "name": "Zixuan Zhang",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ac",
          "name": "Ilgee Hong",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ad",
          "name": "Young Jin Kim",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ae",
          "name": "Weizhu Chen",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584af",
          "name": "Tuo Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T07:15:59.000Z",
      "submittedOnDailyAt": "2025-06-24T02:19:52.155Z",
      "title": "SlimMoE: 큰 MoE 모델의 구조화 압축에 의한 전문가 슬라이밍과 열처리",
      "submittedOnDailyBy": {
        "_id": "63e6b5e22d2c508de9001afd",
        "avatarUrl": "/avatars/43cec9e8b8d490bd259e383954846a1e.svg",
        "isPro": false,
        "fullname": "Chen Liang",
        "user": "cliang1453",
        "type": "user"
      },
      "summary": "Mixture of Experts (MoE) 아키텍처는 대규모 언어 모델(LLMs)를 확장할 때 추론 효율성을 유지하기 위해 강력한 패러다임으로 등장하게 되었다. 그러나 그 큰 메모리 요구는 자원 제한된 환경에서 미세 조정이나 배포에 부담을 줍니다. 이러한 도전에 대응하기 위해 우리는 SlimMoE를 소개합니다. SlimMoE는 확장된 MoE 모델을 크게 줄임으로써 효율적인 버전으로 변환하는 다단계 압축 프레임워크이며, 모델의 시작부터 학습하는 데 금지적인 비용 부담을 피하기 위해 설계되었습니다. 우리 방식은 엑시퍼트를 슬라이미화하고 중간 단계에서 지식을 전달함으로써 파라미터 카운트를 체계적으로 줄이고, 한꺼번에 플레이싱 접근으로 인한 성능 저하를 억제할 수 있습니다. 이 프레임워크를 사용하면 Phi 3.5-MoE(총 41.9B/활성화 16.6B 파라미터)를 Phi-mini-MoE(총 7.6B/활성화 2.4B 파라미터)와 Phi-tiny-MoE(총 3.8B/활성화 1.1B 파라미터)로 압축할 수 있습니다. 이러한 압축 모델은 1개 GPU(Phi-mini-MoE의 A100, Phi-tiny-MoE의 A6000)를 사용하여 미세 조정이 가능하며, 학술 분야나 자원 제한된 환경에서 고품질의 적용성을 제공합니다. 실험 결과를 통해 이러한 압축 모델은 같은 크기의 모델보다 뛰어나며, 큰 모델과 비교하여 경쟁력을 가지고 있습니다. 예를 들어, Phi-mini-MoE는 활성 파라미터의 2/3을 사용하며 Phi-3-mini와 같은 성능을 달성하거나 더 좋은 성능을 나타내며, Llama 3.1 8B와 비교하여 MMLU 점수가相当하며, 상당한 저비용을 받습니다. 우리의 발견은 구조적인 플레이싱과 단계별 디스틸레이션을 조합하여 고품질의 작은 MoE 모델을 만들 수 있는 유효한 방법임을 보여줍니다, MoE 아키텍처의 광범위한 도입을 촉진합니다. 우리 모델은 다음과 같은 URL에서 공개됩니다.\n\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct",
      "upvotes": 4,
      "discussionId": "685a02460e4ad7e2197584b0",
      "ai_summary": "SlimMoE compresses large MoE models into smaller, efficient variants using multi-stage compression without full retraining, maintaining competitive performance with significantly fewer resources.",
      "ai_keywords": [
        "Mixture of Experts (MoE)",
        "large language models (LLMs)",
        "parameter counts",
        "knowledge transfer",
        "one-shot pruning",
        "Phi 3.5-MoE",
        "Phi-mini-MoE",
        "Phi-tiny-MoE",
        "structured pruning",
        "staged distillation",
        "MMLU scores",
        "latency"
      ]
    },
    "publishedAt": "2025-06-23T03:15:59.000Z",
    "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming\n  and Distillation",
    "summary": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm\nfor scaling large language models (LLMs) while maintaining inference\nefficiency. However, their enormous memory requirements make them prohibitively\nexpensive to fine-tune or deploy in resource-constrained environments. To\naddress this challenge, we introduce SlimMoE, a multi-stage compression\nframework for transforming large MoE models into much smaller, efficient\nvariants without incurring the prohibitive costs of training from scratch. Our\nmethod systematically reduces parameter counts by slimming experts and\ntransferring knowledge through intermediate stages, effectively mitigating the\nperformance degradation common in one-shot pruning approaches. Using this\nframework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to\ncreate Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE\n(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of\nthe original model's training data. These compressed models can be fine-tuned\non a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them\nhighly suitable for academic and resource-limited settings. Our experiments\ndemonstrate that these compressed models outperform others of similar size and\nremain competitive with larger models. For instance, Phi-mini-MoE achieves\nsimilar or better performance to Phi-3-mini using only 2/3 of the activated\nparameters and yields comparable MMLU scores to Llama 3.1 8B despite having\nsignificantly lower latency. Our findings demonstrate that structured pruning\ncombined with staged distillation offers an effective path to creating\nhigh-quality, compact MoE models, paving the way for broader adoption of MoE\narchitectures. We make our models publicly available at\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct and\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18349.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e6b5e22d2c508de9001afd",
      "avatarUrl": "/avatars/43cec9e8b8d490bd259e383954846a1e.svg",
      "fullname": "Chen Liang",
      "name": "cliang1453",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16962",
      "authors": [
        {
          "_id": "6858d907c0c8e29df8ea3ce2",
          "user": {
            "_id": "67547707f168984215451697",
            "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
            "isPro": false,
            "fullname": "manglu",
            "user": "manglu3935",
            "type": "user"
          },
          "name": "Haoran Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-23T15:52:05.938Z",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce3",
          "name": "Yankai Jiang",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce4",
          "name": "Wenjie Lou",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce5",
          "name": "Yujie Zhang",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce6",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce7",
          "name": "Lilong Wang",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce8",
          "name": "Mianxin Liu",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce9",
          "name": "Lei Liu",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3cea",
          "name": "Xiaosong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T12:51:19.000Z",
      "submittedOnDailyAt": "2025-06-24T02:20:40.889Z",
      "title": "각 단계별 마이크로 라이브러리 모델의 의료 논리 검증 가능성 향상",
      "submittedOnDailyBy": {
        "_id": "67547707f168984215451697",
        "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
        "isPro": false,
        "fullname": "manglu",
        "user": "manglu3935",
        "type": "user"
      },
      "summary": "다모달대언어모듈(MLLMs)은 일반적인 태스크에서 강력한 논리론 능력을 보여주는 데 시작하지만, 의료 분야에서의 적용은 초기 단계입니다. 의료 MLLM의 논리론 능력을 강화하기 위해, Chain-of-Thought(CoT) 훈련 데이터의 구축이 중요합니다. 그러나 현재의 접근 방식은, 중요한 진단에 대한 유효한 논리론 패스의 검색과 평가를 위해 하나의 프레임워크를 제공하지 않습니다. 이러한 도전을 해결하기 위해, 우리는 Mentor-Intern Collaboration Search(MICS)를 제안합니다. MICS는, 엄격하고 유효한 의료 CoT 데이터를 생성하기 위한 새로운 논리론 패스 검색 스키ーم입니다. MICS는, 우선 Majority Model을 사용하여 논리론을 시작시키고, 그 시작된 패스에 따라 각 프로젝트 모델을 촉발시키고, 최종적으로, 여러 프로젝트 모델의 전체 논리론 성능에 기반하여 가장 적절한 논리론 패스를 선택합니다. 논리론 성능은 MICS-Score에 의해 평가되어, 생성된 논리론 패스의 질을 평가합니다. 최종적으로, 우리는, 순서가 있는 어려운 다 태스크 의료 논리론 데이터 세트 MMRP와, 학科教學 전략을 활용하여 새로운 의료 MLLM인 Chiron-o1을 구축했습니다. Chiron-o1은, MICS를 사용하여 구축된 CoT 데이터에 기반하여 훈련되어, 의료 시각화 질문과 논리론 벤치마크에서 가장 先端의 성능을 달성했습니다. 코드는, GitHub - manglu097/Chiron-o1: MLLM에서 단계별 및 검증 가능한 의료 논리론의 향상을 실현하는 코드가 있습니다.",
      "upvotes": 4,
      "discussionId": "6858d907c0c8e29df8ea3ceb",
      "githubRepo": "https://github.com/manglu097/Chiron-o1",
      "ai_summary": "MICS, a novel reasoning-path searching scheme, enhances medical MLLMs like Chiron-o1 with robust generalizable reasoning and visual question-answering capabilities through comprehensive chain-of-thought data generation.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "chain-of-thought",
        "Mentor-Intern Collaborative Search",
        "MICS",
        "mentor models",
        "intern models",
        "MICS-Score",
        "multi-task medical reasoning dataset",
        "MMRP",
        "curriculum learning",
        "medical visual question answering",
        "reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-06-20T08:51:19.000Z",
    "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
    "summary": "Multimodal large language models (MLLMs) have begun to demonstrate robust\nreasoning capabilities on general tasks, yet their application in the medical\ndomain remains in its early stages. Constructing chain-of-thought (CoT)\ntraining data is essential for bolstering the reasoning abilities of medical\nMLLMs. However, existing approaches exhibit a deficiency in offering a\ncomprehensive framework for searching and evaluating effective reasoning paths\ntowards critical diagnosis. To address this challenge, we propose Mentor-Intern\nCollaborative Search (MICS), a novel reasoning-path searching scheme to\ngenerate rigorous and effective medical CoT data. MICS first leverages mentor\nmodels to initialize the reasoning, one step at a time, then prompts each\nintern model to continue the thinking along those initiated paths, and finally\nselects the optimal reasoning path according to the overall reasoning\nperformance of multiple intern models. The reasoning performance is determined\nby an MICS-Score, which assesses the quality of generated reasoning paths.\nEventually, we construct MMRP, a multi-task medical reasoning dataset with\nranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum\nlearning strategy, with robust visual question-answering and generalizable\nreasoning capabilities. Extensive experiments demonstrate that Chiron-o1,\ntrained on our CoT dataset constructed using MICS, achieves state-of-the-art\nperformance across a list of medical visual question answering and reasoning\nbenchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing\nStep-by-Step and Verifiable Medical Reasoning in MLLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16962.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67547707f168984215451697",
      "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
      "fullname": "manglu",
      "name": "manglu3935",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18792",
      "authors": [
        {
          "_id": "685a72a00e4ad7e219758702",
          "name": "Michal Nazarczuk",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758703",
          "name": "Sibi Catley-Chandar",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758704",
          "name": "Thomas Tanay",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758705",
          "name": "Zhensong Zhang",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758706",
          "name": "Gregory Slabaugh",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758707",
          "name": "Eduardo Pérez-Pellitero",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T16:01:15.000Z",
      "submittedOnDailyAt": "2025-06-24T08:15:41.084Z",
      "title": "ViDAR: 4차원 재구성을 위한 카메라의 관심사 - 비디오 분화\n\n(Note: The translation provided above is a direct translation of the given text. The term \"4차원 재구성\" is used to maintain the original meaning of \"4次元重建\" while ensuring the translation is clear and accurate in Korean. The term \"카메라의 관심사\" is used to convey the idea of \"cameras of interest\" in the context of video differentiation.)",
      "submittedOnDailyBy": {
        "_id": "66f44a3df9252e0f50b59fdb",
        "avatarUrl": "/avatars/4234203c77a0e6f594f3de26bfe8c649.svg",
        "isPro": false,
        "fullname": "Michal Nazarczuk",
        "user": "michaal94",
        "type": "user"
      },
      "summary": "동적 뉴럴뷰 합성은 임의의 시각에서 움직이는 주체의 사진을 생성하는 것을 목표로 하고 있습니다. 이 작업은 단관 비디오를 신뢰할 때 특히 어려워서, 구조를 움직이기 시작하여 분리하는 것이 어렵고, 서브젝션이 부족합니다. 우리는 개인화 디퓨저 모델을 활용하여, 4D 재구성 프레임워크인 Video Diffusion-Aware Reconstruction (ViDAR)를 제시하여, 피즈마 다중 뷰의 서브젝션 신호를 합성하는 데 사용합니다. ViDAR는 시각적 품질과 구조적 일관성을 유지하면서, 단일 관점에서 인한 불확실성을 줄이는 데 사용됩니다. 디퓨저 기반의 서브젝션의 공간 시간 불확실성을 해결하기 위해, 우리는 합성 뷰와 시장의 기본 구조를 일관시키기 위한 디퓨저 워즈 함수와 카메라 자세 최적화 스텔라티지(Stereo)를 제안합니다. DyCheck라는 어려운 벤치마크에서 수행한 실험은, ViDAR가 시각적 품질과 구조적 일관성에서 가장 先端의 베이스라인을 모두 초월함을 보여줍니다. 또한, ViDAR는 동적인 영역에서 강력한 개선을 나타내며, 시장의 움직임의 풍부한 부분을 재구성 성능을 비교하기 위해 새로운 벤치마크를 제공합니다. 프로젝트 페이지는, https://vidar-4d.github.io 입니다.",
      "upvotes": 3,
      "discussionId": "685a72a10e4ad7e219758708",
      "ai_summary": "ViDAR uses diffusion-aware reconstruction to generate high-quality novel views of dynamic scenes from monocular video, outperforming existing methods in visual quality and geometric consistency.",
      "ai_keywords": [
        "Video Diffusion-Aware Reconstruction",
        "ViDAR",
        "Gaussian splatting",
        "diffusion models",
        "spatio-temporal inconsistency",
        "diffusion-aware loss function",
        "camera pose optimisation",
        "DyCheck benchmark"
      ]
    },
    "publishedAt": "2025-06-23T12:01:15.000Z",
    "title": "ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs",
    "summary": "Dynamic Novel View Synthesis aims to generate photorealistic views of moving\nsubjects from arbitrary viewpoints. This task is particularly challenging when\nrelying on monocular video, where disentangling structure from motion is\nill-posed and supervision is scarce. We introduce Video Diffusion-Aware\nReconstruction (ViDAR), a novel 4D reconstruction framework that leverages\npersonalised diffusion models to synthesise a pseudo multi-view supervision\nsignal for training a Gaussian splatting representation. By conditioning on\nscene-specific features, ViDAR recovers fine-grained appearance details while\nmitigating artefacts introduced by monocular ambiguity. To address the\nspatio-temporal inconsistency of diffusion-based supervision, we propose a\ndiffusion-aware loss function and a camera pose optimisation strategy that\naligns synthetic views with the underlying scene geometry. Experiments on\nDyCheck, a challenging benchmark with extreme viewpoint variation, show that\nViDAR outperforms all state-of-the-art baselines in visual quality and\ngeometric consistency. We further highlight ViDAR's strong improvement over\nbaselines on dynamic regions and provide a new benchmark to compare performance\nin reconstructing motion-rich parts of the scene. Project page:\nhttps://vidar-4d.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18792.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66f44a3df9252e0f50b59fdb",
      "avatarUrl": "/avatars/4234203c77a0e6f594f3de26bfe8c649.svg",
      "fullname": "Michal Nazarczuk",
      "name": "michaal94",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17538",
      "authors": [
        {
          "_id": "685a33c50e4ad7e219758612",
          "name": "Yile Gu",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758613",
          "name": "Rohan Kadekodi",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758614",
          "name": "Hoang Nguyen",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758615",
          "user": {
            "_id": "6304ac1a412a1b9d381ca378",
            "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
            "isPro": false,
            "fullname": "Keisuke Kamahori",
            "user": "kamahori",
            "type": "user"
          },
          "name": "Keisuke Kamahori",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:07:52.905Z",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758616",
          "name": "Yiyu Liu",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758617",
          "name": "Baris Kasikci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-21T01:32:22.000Z",
      "submittedOnDailyAt": "2025-06-24T03:49:17.409Z",
      "title": "ConsumerBench: 소비자 벤치마크 사용자 기기에 있는 생성 AI 앱의 벤치마크",
      "submittedOnDailyBy": {
        "_id": "6304ac1a412a1b9d381ca378",
        "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
        "isPro": false,
        "fullname": "Keisuke Kamahori",
        "user": "kamahori",
        "type": "user"
      },
      "summary": "최근, Generative AI (GenAI) 애플리케이션이 클라우드 환경에서 사용자의 기기로 이동하면서 리소스 관리, 시스템 엔지니어링 및 사용자 경험에 새로운 문제를 제기하고 있습니다. 본 논문에서는, GenAI 모델이 실행되는 사용자 기기의 시스템 엔지니어링 및 응답 시간 평가를 위해 확장된 벤치마크 프레임워크 \"ConsumerBench\"를 제시합니다. 현재 벤치마크와 달리, 특정 GPU에서 고유의 모델 접근을 가정하는 것이 아닙니다. ConsumerBench는 제한된 하드웨어에서 병렬 실행되는 실제 다중 애플리케이션 시나리오를 시뮬레이션합니다. 또한, ConsumerBench는 복잡한 작업에 필요한 다수의 애플리케이션의 협업을 시뮬레이션하는 가변적인 작업 흐름을 지원합니다. ConsumerBench는 애플리케이션 수준의 메트릭스(엔드포인트, 서비스 수준 목표(SLO)의 달성)와 시스템 수준의 메트릭스(CPU/GPU의 사용률, 메모리 밴드폭)을 모두 고려합니다. 확장된 실험을 통해, ConsumerBench는 리소스 공유의 적절성, 극한 분배의 불평등성, 정적 모델 서버의 성능의 극점점을 밝혀줍니다. 또한, 모델 개발자와 시스템 설계자에게 실질적인 아이디어를 제공하며, 소비자 수준의 GPU 아키텍처에 맞게 조정된 커스텀 커널의 이점과 SLO에 대한 스케줄링 전략의 가치를 밝혀줍니다.",
      "upvotes": 3,
      "discussionId": "685a33c70e4ad7e219758618",
      "githubRepo": "https://github.com/efeslab/ConsumerBench",
      "ai_summary": "ConsumerBench evaluates GenAI system efficiency and response time on end-user devices through a comprehensive benchmarking framework, emphasizing realistic multi-application scenarios and customizable workflows.",
      "ai_keywords": [
        "Generative AI",
        "ConsumerBench",
        "system efficiency",
        "response time",
        "benchmarking framework",
        "multi-application scenarios",
        "application-level metrics",
        "latency",
        "Service Level Objective",
        "SLO",
        "system-level metrics",
        "CPU utilization",
        "GPU utilization",
        "memory bandwidth",
        "greedy allocation",
        "static model server configurations",
        "custom kernels",
        "SLO-aware scheduling strategies"
      ]
    },
    "publishedAt": "2025-06-20T21:32:22.000Z",
    "title": "ConsumerBench: Benchmarking Generative AI Applications on End-User\n  Devices",
    "summary": "The recent shift in Generative AI (GenAI) applications from cloud-only\nenvironments to end-user devices introduces new challenges in resource\nmanagement, system efficiency, and user experience. This paper presents\nConsumerBench, a comprehensive benchmarking framework designed to evaluate the\nsystem efficiency and response time of GenAI models running on end-user\ndevices. Unlike existing benchmarks that assume exclusive model access on\ndedicated GPUs, ConsumerBench simulates realistic multi-application scenarios\nexecuting concurrently on constrained hardware. Furthermore, ConsumerBench\nsupports customizable workflows that simulate complex tasks requiring\ncoordination among multiple applications. ConsumerBench captures both\napplication-level metrics, including latency and Service Level Objective (SLO)\nattainment, and system-level metrics like CPU/GPU utilization and memory\nbandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies\nin resource sharing, unfair scheduling under greedy allocation, and performance\npitfalls of static model server configurations. The paper also provides\npractical insights for model developers and system designers, highlighting the\nbenefits of custom kernels tailored to consumer-grade GPU architectures and the\nvalue of implementing SLO-aware scheduling strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6304ac1a412a1b9d381ca378",
      "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
      "fullname": "Keisuke Kamahori",
      "name": "kamahori",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18879",
      "authors": [
        {
          "_id": "685a1a090e4ad7e21975859c",
          "name": "Junyan Li",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e21975859d",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e21975859e",
          "name": "Muhammad Yusuf Hassan",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e21975859f",
          "name": "Talha Chafekar",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a0",
          "name": "Tianle Cai",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a1",
          "name": "Zhile Ren",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a2",
          "name": "Pengsheng Guo",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a3",
          "name": "Foroozan Karimzadeh",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a4",
          "name": "Colorado Reed",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a5",
          "name": "Chong Wang",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a6",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:50:11.000Z",
      "submittedOnDailyAt": "2025-06-24T01:53:55.727Z",
      "title": "CommVQ: 교환 가능한 벡터 썸네일링을 이용한 KV 캐시 압축",
      "submittedOnDailyBy": {
        "_id": "62d09eb86a61a88ea0d83918",
        "avatarUrl": "/avatars/81b511d94cced304ffca058caff662d4.svg",
        "isPro": false,
        "fullname": "Junyan Li",
        "user": "senfu",
        "type": "user"
      },
      "summary": "대 언어 모형(LLMs)은 긴 컨텍스트 길이를 요구하는 애플리케이션에서 사용된 횟수가 증가하고 있지만, GPU에서 컨텍스트가 커질 때마다 키-값(KV) 캐쉬가 메모리 버튼 뀔 수 밖에 없습니다. 이를 대처하기 위해, コミュティーバイエンド ベクトルキャップティゼーション(CommVQ)을 제안하여, 긴 컨텍스트의 LLM 추론의 메모리 사용량을 크게 줄이려는 목표를 가지고 있습니다. 먼저, 가벼운 인코더와 코드북을 사용하여 가산 캐쉬티제이션을 통해 KV 캐쉬를 압축시키고, 간단한 행렬 곱으로 디코딩이 가능한 형태로 만듭니다. 또한, 디코딩 시의 계산 비용을 줄이기 위해, Rotary Position Embedding(RoPE)과 コミュティーバイ에 설계된 Expectation-Maximization(EM) 알고리즘을 사용하여 훈련합니다. 이로써, 자동 注意 구조에 효율적으로 디코딩을 통합할 수 있습니다. 우리의 접근 방식은, RoPE-コミュティーバイエンド 코ード북을 사용하여 가산 캐쉬티제이션을 통해 높은 정확도를 유지하고, 낮은 오버헤드를 보여주도록 합니다. 긴 컨텍스트 벤치마크와 GSM8K에서 실험을 통해, 2 비트 캐쉬티제이션으로 FP16 KV 캐쉬 크기를 87.5% 줄였으며, 최신의 KV 캐쉬 캐쉬티제이션 방법과 비교하여 뛰어난 성능을 보입니다. 특히, 1 비트 캐쉬티제이션을 가능하게 하여, 최소한의 정확도 손실을 동반하면서 LLaMA-3.1 8B 모델을 128K 컨텍스트 길이를 사용하여 하나의 RTX 4090 GPU에서 실행할 수 있습니다. 소스 코드는 다음 URL에 공개되어 있습니다: https://github.com/UMass-Embodied-AGI/CommVQ.",
      "upvotes": 2,
      "discussionId": "685a1a090e4ad7e2197585a7",
      "ai_summary": "Commutative Vector Quantization (CommVQ) reduces memory usage in long-context LLM inference by compressing the KV cache with additive quantization and integration of Rotary Position Embedding (RoPE).",
      "ai_keywords": [
        "Commutative Vector Quantization",
        "CommVQ",
        "additive quantization",
        "codebook",
        "Rotary Position Embedding",
        "RoPE",
        "Expectation-Maximization",
        "self-attention",
        "FP16",
        "KV cache quantization",
        "GSM8K",
        "LLaMA-3.1 8B model",
        "RTX 4090 GPU"
      ]
    },
    "publishedAt": "2025-06-23T13:50:11.000Z",
    "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
    "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d09eb86a61a88ea0d83918",
      "avatarUrl": "/avatars/81b511d94cced304ffca058caff662d4.svg",
      "fullname": "Junyan Li",
      "name": "senfu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17673",
      "authors": [
        {
          "_id": "685a5c8e0e4ad7e2197586c7",
          "user": {
            "_id": "64105805928400b416439f10",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
            "isPro": false,
            "fullname": "Seonglae Cho",
            "user": "seonglae",
            "type": "user"
          },
          "name": "Seonglae Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:09:45.757Z",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586c8",
          "name": "Harryn Oh",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586c9",
          "name": "Donghyun Lee",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586ca",
          "name": "Luis Eduardo Rodrigues Vieira",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586cb",
          "name": "Andrew Bermingham",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586cc",
          "name": "Ziad El Sayed",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-21T10:18:25.000Z",
      "submittedOnDailyAt": "2025-06-24T06:38:36.202Z",
      "title": "FaithfulSAE: 희소한 자기 자동 인코더를 사용하여 외부 데이터 세트의 의존성을 제거하여 충실한 특성을 추출합니다.",
      "submittedOnDailyBy": {
        "_id": "64105805928400b416439f10",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
        "isPro": false,
        "fullname": "Seonglae Cho",
        "user": "seonglae",
        "type": "user"
      },
      "summary": "Sparse Autoencoders (SAEs)는 대규모 언어 모델의 표현을 해석할 수 있는 특징을 분해하는 잠재적인 해결책으로 등장했습니다. 그러나 Paulo와 Belrose(2025)는 초기화 시드에 따라 불안정함을 지적하고, Heap 등(2025)는 SAEs가 모델의 내부 특징을 파악하지 못하는 가능성을 지적했습니다. 이러한 문제들은 SAEs를 외부 데이터 세트로 훈련하는 데 기인할 수 있습니다. 이러한 데이터 세트는 웹에서 수집된 것 또는 다른 모델이 생성된 것이며, 모델의 일반화 능력을 초과하는 OOD(Offensive Data)를 포함할 수 있습니다. 이는 모델의 내부 활성을 부정확하게 표현하는 \"부실 특징\"을 생성하는 데 연결됩니다. 이러한 문제를 대처하기 위해 우리는 FaithfulSAE라는 방법을 제안하고 있습니다. 이것은 모델의 자체 합성 데이터 세트를 사용하여 SAEs를 훈련하는 방법입니다. FaithfulSAE를 사용함으로써, 우리는 OOD 명령 데이터 세트에서 SAEs를 훈련하는 것은 다른 시드에 따라 SAEs의 불안정함을 줄일 수 있다는 것을 보여주었습니다. 특히, FaithfulSAE는 웹 기반 데이터 세트로 훈련된 SAEs를 초과하여, 7개의 모델 중 5개의 모델의 부실 특징 비율이 낮아졌습니다. 전반적으로, 우리의 접근 방식은 외부 데이터 세트의 의존성을 제거하고, 모델의 내부 특징을 더 잘 파악하여 해석성을 향상시키고, SAEs의 훈련 데이터 세트의 중요성을 강조합니다.",
      "upvotes": 1,
      "discussionId": "685a5c8e0e4ad7e2197586cd",
      "ai_summary": "FaithfulSAE improves Sparse Autoencoder stability and interpretability by training on synthetic datasets generated by the model itself, reducing the occurrence of fake features and out-of-distribution data issues.",
      "ai_keywords": [
        "Sparse Autoencoders",
        "SAEs",
        "interpretability",
        "instability",
        "initialization seeds",
        "model-internal features",
        "out-of-distribution",
        "OOD",
        "Fake Features",
        "SAE probing task",
        "synthetic dataset"
      ]
    },
    "publishedAt": "2025-06-21T06:18:25.000Z",
    "title": "FaithfulSAE: Towards Capturing Faithful Features with Sparse\n  Autoencoders without External Dataset Dependencies",
    "summary": "Sparse Autoencoders (SAEs) have emerged as a promising solution for\ndecomposing large language model representations into interpretable features.\nHowever, Paulo and Belrose (2025) have highlighted instability across different\ninitialization seeds, and Heap et al. (2025) have pointed out that SAEs may not\ncapture model-internal features. These problems likely stem from training SAEs\non external datasets - either collected from the Web or generated by another\nmodel - which may contain out-of-distribution (OOD) data beyond the model's\ngeneralisation capabilities. This can result in hallucinated SAE features,\nwhich we term \"Fake Features\", that misrepresent the model's internal\nactivations. To address these issues, we propose FaithfulSAE, a method that\ntrains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we\ndemonstrate that training SAEs on less-OOD instruction datasets results in SAEs\nbeing more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained\non web-based datasets in the SAE probing task and exhibit a lower Fake Feature\nRatio in 5 out of 7 models. Overall, our approach eliminates the dependency on\nexternal datasets, advancing interpretability by better capturing\nmodel-internal features while highlighting the often neglected importance of\nSAE training datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17673.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64105805928400b416439f10",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
      "fullname": "Seonglae Cho",
      "name": "seonglae",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16929",
      "authors": [
        {
          "_id": "6858b7c0c0c8e29df8ea3c29",
          "name": "Mohon Raihan",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2a",
          "name": "Plabon Kumar Saha",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2b",
          "user": {
            "_id": "67a3002c637d195f3c4bf371",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
            "isPro": false,
            "fullname": "Rajan Das Gupta",
            "user": "rajandasgupta",
            "type": "user"
          },
          "name": "Rajan Das Gupta",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:15:03.417Z",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2c",
          "name": "A Z M Tahmidul Kabir",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2d",
          "name": "Afia Anjum Tamanna",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2e",
          "name": "Md. Harun-Ur-Rashid",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2f",
          "name": "Adnan Bin Abdus Salam",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c30",
          "name": "Md Tanvir Anjum",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c31",
          "name": "A Z M Ahteshamul Kabir",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Kx6P69oj873BKEjH3euNc.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/0S6fvcxnAdj4DiU-NEhtz.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/EBuiK2wVbSXnLVWmxDiZ5.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Id__vCzRydC1Bmxv-j1W0.png"
      ],
      "publishedAt": "2025-06-20T11:44:48.000Z",
      "submittedOnDailyAt": "2025-06-24T08:29:18.461Z",
      "title": "소아의 신생아 사망예측을 위한 딥러닝 및 기계학습의 접근법",
      "submittedOnDailyBy": {
        "_id": "67a3002c637d195f3c4bf371",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
        "isPro": false,
        "fullname": "Rajan Das Gupta",
        "user": "rajandasgupta",
        "type": "user"
      },
      "summary": "신생아 사망은 여전히 개발 중 국가甚은 일부 발전된 국가에서 우려되는 현실입니다. Macro Trades의 글로벌 데이터에 따르면, 1,000명의 신생아 중 26.693명이 사망합니다. 이 숫자를 줄이기 위해 위험한 아이를 조기에 예측하는 것이 중요합니다. 이러한 예측은 아이와 어머니를 충분히 돌보는 기회를 제공하고, 조기 아동 사망을 피할 수 있습니다. 이러한 배경에서 신생아가 위험에 처한지를 결정하기 위해 기계 학습을 사용합니다. 140만명의 신생아의 역사 데이터를 사용하여 예측 모델을 훈련했습니다. 로지스틱 회귀, K-최근접 이웃, 랜덤 포레스트 분류기, 극단 경사 부상 (XGBoost), 컨볼루션 신경망 (CNN) 및 긴 시급 기억 (LSTM) 등 기계 학습과 딥 학습 기술을 사용하여 신생아 사망률 예측을 정확하게 수행할 수 있는 모델을 식별했습니다. 기계 학습 알고리즘에서 XGBoost와 랜덤 포레스트 분류기는 94%의 최고 정확도를 달성하였으며, 딥 학습 모델에서 LSTM은 99%의 최고 정확도를 달성하였습니다. 따라서, LSTM을 사용하여 아이가 예방 조치가 필요할 지 예측하는 데 가장 적합한 방법이라고 할 수 있습니다.",
      "upvotes": 1,
      "discussionId": "6858b7c1c0c8e29df8ea3c32",
      "ai_summary": "Deep learning, specifically LSTM, outperforms other machine learning techniques in predicting neonatal mortality using historical data.",
      "ai_keywords": [
        "logical regression",
        "K-nearest neighbor",
        "random forest classifier",
        "extreme gradient boosting (XGBoost)",
        "convolutional neural network",
        "long short-term memory (LSTM)"
      ]
    },
    "publishedAt": "2025-06-20T07:44:48.000Z",
    "title": "A deep learning and machine learning approach to predict neonatal death\n  in the context of São Paulo",
    "summary": "Neonatal death is still a concerning reality for underdeveloped and even some\ndeveloped countries. Worldwide data indicate that 26.693 babies out of 1,000\nbirths die, according to Macro Trades. To reduce this number, early prediction\nof endangered babies is crucial. Such prediction enables the opportunity to\ntake ample care of the child and mother so that early child death can be\navoided. In this context, machine learning was used to determine whether a\nnewborn baby is at risk. To train the predictive model, historical data of 1.4\nmillion newborns was used. Machine learning and deep learning techniques such\nas logical regression, K-nearest neighbor, random forest classifier, extreme\ngradient boosting (XGBoost), convolutional neural network, and long short-term\nmemory (LSTM) were implemented using the dataset to identify the most accurate\nmodel for predicting neonatal mortality. Among the machine learning algorithms,\nXGBoost and random forest classifier achieved the best accuracy with 94%, while\namong the deep learning models, LSTM delivered the highest accuracy with 99%.\nTherefore, using LSTM appears to be the most suitable approach to predict\nwhether precautionary measures for a child are necessary.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Kx6P69oj873BKEjH3euNc.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/0S6fvcxnAdj4DiU-NEhtz.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/EBuiK2wVbSXnLVWmxDiZ5.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Id__vCzRydC1Bmxv-j1W0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16929.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a3002c637d195f3c4bf371",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
      "fullname": "Rajan Das Gupta",
      "name": "rajandasgupta",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.17323",
      "authors": [
        {
          "_id": "685a6c720e4ad7e2197586f2",
          "name": "Tamas Bisztray",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f3",
          "user": {
            "_id": "64d3db80aea0ccb1b4975d95",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
            "isPro": false,
            "fullname": "Bilel Cherif",
            "user": "Neo111x",
            "type": "user"
          },
          "name": "Bilel Cherif",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T09:32:57.540Z",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f4",
          "name": "Richard A. Dubniczky",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f5",
          "name": "Nils Gruschka",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f6",
          "name": "Bertalan Borsos",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f7",
          "name": "Mohamed Amine Ferrag",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f8",
          "name": "Attila Kovacs",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f9",
          "name": "Vasileios Mavroeidis",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586fa",
          "name": "Norbert Tihanyi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T19:49:41.000Z",
      "submittedOnDailyAt": "2025-06-24T07:45:13.848Z",
      "title": "이 Entry는 전문적인 어휘와 정확성을 유지하며 그대로 번역 결과를 반환합니다.\n\n「저는 여름의 마지막 날에 작성한 LLM가 누군가를 알고 있습니다: LLM 생성 코드\n레이터 속성 식별의 스タイロメトリー」",
      "submittedOnDailyBy": {
        "_id": "64d3db80aea0ccb1b4975d95",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
        "isPro": false,
        "fullname": "Bilel Cherif",
        "user": "Neo111x",
        "type": "user"
      },
      "summary": "AI 코드, deepfakes 및 기타 합성 콘텐츠의 감지는新兴する 연구 과제입니다. LLM (Large Language Models)에서 생성되는 코드가 더 일반화 될 때, 각 샘플의 특정 모델을 식별하는 것이 중요해졌습니다. 본 논문에서는, C 프로그래밍의 LLM의 저작권 식별을 체계적으로 연구합니다. CodeT5-Authorship라는 새로운 모델을 공개합니다. 이 모델은 원본 CodeT5의 encoder-decoder 아키텍처의 encoder 유닛만 사용하며, decoder를 버리고 분류에 초점을 맞추었습니다. 모델의 encoder의 출력 (첫 번째 토큰)은 GELU 활성화 함수와 드롭아웃을 포함하는 2층의 분류 헤드를 통해, 가능한 저자의 확률 분포를 생성합니다. 모델의 평가에는 8개의 최단 LLM에서 생성된 32,000 개의 프로그램으로 구성된 LLM-AuthorBench를 사용합니다. 7개의 전통적인 ML 분류기 및 8개의 미세 조정된 transformer 모델 (BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3, Longformer, LoRA-fine-tuned Qwen2-1.5B)과 비교합니다. 이진 분류에서, 유사한 모델 (GPT-4.1과 GPT-4o)에서 생성된 C 프로그래밍을 구분하기 위해 97.56%의 정확도를 달성하였으며, 5개의 최단 LLM (Gemini 2.5 Flash, Claude 3.5 Haiku, GPT-4.1, Llama 3.3, DeepSeek-V3)의 다 클래스 식별에서 95.40%의 정확도를 달성하였습니다. 과학의 개방을 지원하기 위해, CodeT5-Authorship의 아키텍처, LLM-AuthorBench의 벤치마크, 그리고 GitHub에서 모든 관련 Google Colab 스크립트를 공개합니다.",
      "upvotes": 1,
      "discussionId": "685a6c720e4ad7e2197586fb",
      "projectPage": "https://github.com/LLMauthorbench",
      "githubRepo": "https://github.com/LLMauthorbench/LLMauthorbench",
      "ai_summary": "A novel model, CodeT5-Authorship, is introduced to classify the authorship of C programs generated by Large Language Models, achieving high accuracy compared to traditional and transformer-based classifiers.",
      "ai_keywords": [
        "Large Language Models",
        "LLM authorship attribution",
        "CodeT5-Authorship",
        "encoder-decoder architecture",
        "GELU activation",
        "dropout",
        "LLM-AuthorBench",
        "traditional ML classifiers",
        "BERT",
        "RoBERTa",
        "CodeBERT",
        "ModernBERT",
        "DistilBERT",
        "DeBERTa-V3",
        "Longformer",
        "LoRA",
        "Qwen2-1.5B",
        "binary classification",
        "multi-class attribution"
      ]
    },
    "publishedAt": "2025-06-18T15:49:41.000Z",
    "title": "I Know Which LLM Wrote Your Code Last Summer: LLM generated Code\n  Stylometry for Authorship Attribution",
    "summary": "Detecting AI-generated code, deepfakes, and other synthetic content is an\nemerging research challenge. As code generated by Large Language Models (LLMs)\nbecomes more common, identifying the specific model behind each sample is\nincreasingly important. This paper presents the first systematic study of LLM\nauthorship attribution for C programs. We released CodeT5-Authorship, a novel\nmodel that uses only the encoder layers from the original CodeT5\nencoder-decoder architecture, discarding the decoder to focus on\nclassification. Our model's encoder output (first token) is passed through a\ntwo-layer classification head with GELU activation and dropout, producing a\nprobability distribution over possible authors. To evaluate our approach, we\nintroduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs\ngenerated by eight state-of-the-art LLMs across diverse tasks. We compare our\nmodel to seven traditional ML classifiers and eight fine-tuned transformer\nmodels, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,\nLongformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model\nachieves 97.56% accuracy in distinguishing C programs generated by closely\nrelated models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class\nattribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,\nGPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the\nCodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant\nGoogle Colab scripts on GitHub: https://github.com/LLMauthorbench/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17323.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d3db80aea0ccb1b4975d95",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
      "fullname": "Bilel Cherif",
      "name": "Neo111x",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16507",
      "authors": [
        {
          "_id": "685a752f0e4ad7e21975870a",
          "name": "Pragya Srivastava",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870b",
          "name": "Harman Singh",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870c",
          "name": "Rahul Madhavan",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870d",
          "name": "Gandharv Patil",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870e",
          "name": "Sravanti Addepalli",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870f",
          "name": "Arun Suggala",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758710",
          "name": "Rengarajan Aravamudhan",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758711",
          "name": "Soumya Sharma",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758712",
          "name": "Anirban Laha",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758713",
          "name": "Aravindan Raghuveer",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758714",
          "name": "Karthikeyan Shanmugam",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758715",
          "name": "Doina Precup",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T17:59:47.000Z",
      "submittedOnDailyAt": "2025-06-24T08:24:29.234Z",
      "title": "강건한 보상 모델링을 위해 카우스룰을 적용합니다.",
      "submittedOnDailyBy": {
        "_id": "639ccab166106be1436e1640",
        "avatarUrl": "/avatars/1e3806e18ac427be20e93e5400f153d4.svg",
        "isPro": false,
        "fullname": "Pragya Srivastava",
        "user": "pragsri8",
        "type": "user"
      },
      "summary": "보상 모델（RMs）는 인간의 반응을 기반으로 대규모 언어 모델（LLMs）를 조정하는 데 필수적인 요소이지만, 일반적으로 보상 해킹에 직면하여 어려움을 겪습니다. 이들은 예를 들어 답의 길이나 형식 등 표면적이나 불확실한 속성에 의존하며, 이러한 속성을 학습한 것을 질의의 진정한 원인으로 잘못 판단합니다（예로, 사실성, 관련성). 이는 표준 훈련 목적 함수가 이러한 요소를 구분할 수 없기 때문에 취약한 RMs와 부적절한 정책이 발생합니다. 우리는 보상 해킹을 완화하기 위한 새로운 프레임워크인 Crome（유효한 보상 모델링）를 소개합니다. Crome은 명시적인因果 모델에 기반하여, 훈련 중 다음과 같은 합성적인 특정한 증강을 사용합니다: 1)因果 증강은 특정한因果 속성에 따라 다른 쌍으로, 각因果 속성의 개별적인 민감도를 강제합니다. 2) 중립 증강은 주로 불확실한 속성을 통해 결과를 연결하는 쌍으로, 불확실한 속성의 변동을 강제합니다. 특히, 우리 증강은因果 규칙에만 따르는 답의 인터벌을 통해 불확실한 원인에 대한 지식이 없는 상태에서 생성됩니다. 실험적으로는 Crome은 RewardBench 상에서 표준의 베이스라인보다 크게 뛰어납니다, 평균 정확도를 5.4%까지 올린 반면, 특정 카테고리에서 13.2%까지, 7.2%까지의 효과를 달성합니다. Crome의 강건성은 N을 늘린 Best-of-N 추론 설정에서 다양한 벤치마크에서 일관된 효과를 계속 가져오고 있으며, 인기 있는 RewardBench（댓글, 댓글-하드, 안전, 이유 태스크), 안전에 초점을 둔 WildGuardTest, 이유에 초점을 둔 GSM8k을 포함하는 벤치마크에 의해 더욱 증명되었습니다.",
      "upvotes": 0,
      "discussionId": "685a752f0e4ad7e219758716",
      "ai_summary": "Crome, a novel reward modeling framework using causal and neutral augmentations, significantly improves the robustness and accuracy of reward models against reward hacking.",
      "ai_keywords": [
        "Reward models",
        "Large Language Models",
        "reward hacking",
        "causal model",
        "causal augmentations",
        "neutral augmentations",
        "answer interventions",
        "oracle LLM",
        "RewardBench",
        "WildGuardTest",
        "GSM8k",
        "Best-of-N inference"
      ]
    },
    "publishedAt": "2025-06-19T13:59:47.000Z",
    "title": "Robust Reward Modeling via Causal Rubrics",
    "summary": "Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)\nvia human feedback, yet they often suffer from reward hacking. They tend to\nlatch on to superficial or spurious attributes, such as response length or\nformatting, mistaking these cues learned from correlations in training data for\nthe true causal drivers of quality (e.g., factuality, relevance). This occurs\nbecause standard training objectives struggle to disentangle these factors,\nleading to brittle RMs and misaligned policies. We introduce Crome (Causally\nRobust Reward Modeling), a novel framework grounded in an explicit causal model\ndesigned to mitigate reward hacking. Crome employs the following synthetic\ntargeted augmentations during training: (1) Causal Augmentations, which are\npairs that differ along specific causal attributes, to enforce sensitivity\nalong each causal attribute individually, and (2) Neutral Augmentations, which\nare tie-label pairs varying primarily in spurious attributes, to enforce\ninvariance along spurious attributes. Notably, our augmentations are produced\nwithout any knowledge of spurious factors, via answer interventions only along\ncausal rubrics, that are identified by querying an oracle LLM. Empirically,\nCrome significantly outperforms standard baselines on RewardBench, improving\naverage accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in\nspecific categories. The robustness of Crome is further testified by the\nconsistent gains obtained in a Best-of-N inference setting across increasing N,\nacross various benchmarks, including the popular RewardBench (covering chat,\nchat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and\nthe reasoning-specific GSM8k.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639ccab166106be1436e1640",
      "avatarUrl": "/avatars/1e3806e18ac427be20e93e5400f153d4.svg",
      "fullname": "Pragya Srivastava",
      "name": "pragsri8",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10597",
      "authors": [
        {
          "_id": "685a0fb40e4ad7e219758522",
          "name": "Xunguang Wang",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758523",
          "name": "Zhenlan Ji",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758524",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758525",
          "name": "Zongjie Li",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758526",
          "name": "Daoyuan Wu",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758527",
          "name": "Shuai Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T11:42:40.000Z",
      "submittedOnDailyAt": "2025-06-24T01:16:03.239Z",
      "title": "SoK: 대규모 언어 모델의 젓가락 브레이크 가드레일 평가",
      "submittedOnDailyBy": {
        "_id": "6601853162471e0981261241",
        "avatarUrl": "/avatars/ccd1c5ce9d2f6fe7c2aff80fd9c39270.svg",
        "isPro": false,
        "fullname": "XunguangWang",
        "user": "xunguangwang",
        "type": "user"
      },
      "summary": "대 언어 모형(LLMs)는 놀라운 진전을 달성했지만, 그 도입으로 안전 기관에 대한 중요한 취약성을 드러내고, 특히 젓가락 브레이크 공격에 대해 안전 기관을 회피할 수 있습니다. 젓가락 브레이크 공격에 대응하기 위해 외부 방어 구조로서의 겉장대(Guardrail)이 등장했습니다. 그러나 현재의 LLM 겉장대의 상태는 분리되어 있으며, 통일적인 탤러노미(Taxonomy)과 완전한 평가 프레임워크가 부족합니다. 이 Systematization of Knowledge(SoK) 논문에서는 LLM의 젓가락 브레이크 공격에 대한 첫 번째 하드웨어 시크스 분석을 수행합니다. 우리는 새로운, 다양한 탤러노미를 제안하고, 보안성, 효율성, 유용성 평가 프레임워크를 통해 실용적인 효과를 평가하는 것을 목표로 합니다. 엄격한 분석과 실험을 통해 현재의 겉장대 접근법의 강점과 한계를 밝혀, 공격 유형의 광범위한 범위에서의 일반성을 조사하고, 방어 조합의 최적화를 위한 피드백을 제공합니다. 우리의 연구는 향후 연구와 개발에 대한 구조적인 기초를 제공하며, 강한 LLM 겉장대의 원리적 진전과 도입을 가이드하는 것을 목표로 합니다. 코드는 https://github.com/xunguangwang/SoK4JailbreakGuardrails에서 사용 가능합니다.",
      "upvotes": 0,
      "discussionId": "685a0fb40e4ad7e219758533",
      "ai_summary": "A systematic analysis and evaluation framework for jailbreak guardrails in Large Language Models is presented, categorizing and assessing their effectiveness and optimization potential.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "jailbreak attacks",
        "guardrails",
        "Security-Efficiency-Utility framework",
        "multi-dimensional taxonomy"
      ]
    },
    "publishedAt": "2025-06-12T07:42:40.000Z",
    "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models",
    "summary": "Large Language Models (LLMs) have achieved remarkable progress, but their\ndeployment has exposed critical vulnerabilities, particularly to jailbreak\nattacks that circumvent safety mechanisms. Guardrails--external defense\nmechanisms that monitor and control LLM interaction--have emerged as a\npromising solution. However, the current landscape of LLM guardrails is\nfragmented, lacking a unified taxonomy and comprehensive evaluation framework.\nIn this Systematization of Knowledge (SoK) paper, we present the first holistic\nanalysis of jailbreak guardrails for LLMs. We propose a novel,\nmulti-dimensional taxonomy that categorizes guardrails along six key\ndimensions, and introduce a Security-Efficiency-Utility evaluation framework to\nassess their practical effectiveness. Through extensive analysis and\nexperiments, we identify the strengths and limitations of existing guardrail\napproaches, explore their universality across attack types, and provide\ninsights into optimizing defense combinations. Our work offers a structured\nfoundation for future research and development, aiming to guide the principled\nadvancement and deployment of robust LLM guardrails. The code is available at\nhttps://github.com/xunguangwang/SoK4JailbreakGuardrails.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10597.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601853162471e0981261241",
      "avatarUrl": "/avatars/ccd1c5ce9d2f6fe7c2aff80fd9c39270.svg",
      "fullname": "XunguangWang",
      "name": "xunguangwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]