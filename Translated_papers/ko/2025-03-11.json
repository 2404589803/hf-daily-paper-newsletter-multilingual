[
  {
    "paper": {
      "id": "2503.03601",
      "authors": [
        {
          "_id": "67cbfff12cc05acaab147f07",
          "name": "Kristian Kuznetsov",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f08",
          "user": {
            "_id": "636254dc2691058b19d9276a",
            "avatarUrl": "/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg",
            "isPro": false,
            "fullname": "Kushnareva",
            "user": "Kushnareva",
            "type": "user"
          },
          "name": "Laida Kushnareva",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:23:18.630Z",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f09",
          "name": "Polina Druzhinina",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0a",
          "user": {
            "_id": "6172aaeec8e66e2aa84c06b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6172aaeec8e66e2aa84c06b9/ZdRZSp3P1SU6CIDbvQwkv.jpeg",
            "isPro": false,
            "fullname": "Anton Razzhigaev",
            "user": "razzant",
            "type": "user"
          },
          "name": "Anton Razzhigaev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:23:21.197Z",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0b",
          "name": "Anastasia Voznyuk",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0c",
          "name": "Irina Piontkovskaya",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0d",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0e",
          "name": "Serguei Barannikov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T15:33:52.000Z",
      "title": "특징 수준에서 인공적으로 작성된 문헌을 감지하는 sparse 自動 エンコーダー에 대한 지식",
      "summary": "인공 텍스트 탐지(ATD)은 고수준의 대규모 언어 모델(LLMs)의 증가에 따라 중요성이 높아집니다. 그러나 여러 노력의 합으로도, 서로 다른 유형의 처음 본 텍스트에 대해 일관된 좋은 성능을 보여주는 것은 불가능하며, 새로운 LLMs에 대한 유효한 확장을 보장하는 것도 불가능합니다. 해석성은 이 목표를 달성하기 위해 중요한 역할을 합니다. 본 연구에서는, Sparse Autoencoders(SAE)를 사용하여, Gemma-2-2b의 잔차 스트리밍으로부터 특징을 추출하고, ATD의 해석성을 향상시킵니다. 두 가지 해석적이고 효율적인 특징을 특정하고, 도메인 및 모델 고유의 통계, 스태링 접근법, 수동 또는 LLM 기반의 해석을 통해, 그 의미와 연관성을 분석합니다. 우리의 방법은, 서로 다른 모델에서 텍스트가 인간이 쓴 것과 어떻게 다른지에 대해 유익한 정보를 제공합니다. 또한, 현재의 LLMs은 정보 밀집한 도메인에서 특히 다른 표현을 가지고 있으며, 프로마티카적인 프로ン퓰트를 사용하여 인간처럼 출력을 생성할 수 있습니다.",
      "upvotes": 85,
      "discussionId": "67cbfff22cc05acaab147f4d",
      "ai_keywords": [
        "Sparse Autoencoders",
        "Gemma-2-2b",
        "residual stream",
        "interpretability",
        "domain-specific statistics",
        "model-specific statistics",
        "steering approach",
        "LLM-based interpretation",
        "writing style",
        "information-dense domains",
        "human-like outputs"
      ]
    },
    "publishedAt": "2025-03-05T10:33:52.000Z",
    "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
    "summary": "Artificial Text Detection (ATD) is becoming increasingly important with the\nrise of advanced Large Language Models (LLMs). Despite numerous efforts, no\nsingle algorithm performs consistently well across different types of unseen\ntext or guarantees effective generalization to new LLMs. Interpretability plays\na crucial role in achieving this goal. In this study, we enhance ATD\ninterpretability by using Sparse Autoencoders (SAE) to extract features from\nGemma-2-2b residual stream. We identify both interpretable and efficient\nfeatures, analyzing their semantics and relevance through domain- and\nmodel-specific statistics, a steering approach, and manual or LLM-based\ninterpretation. Our methods offer valuable insights into how texts from various\nmodels differ from human-written content. We show that modern LLMs have a\ndistinct writing style, especially in information-dense domains, even though\nthey can produce human-like outputs with personalized prompts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03601.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07365",
      "authors": [
        {
          "_id": "67cf9cd037bc7273882147a3",
          "name": "Fanqing Meng",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a4",
          "name": "Lingxiao Du",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a5",
          "name": "Zongkai Liu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a6",
          "name": "Zhixiang Zhou",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a7",
          "name": "Quanfeng Lu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a8",
          "name": "Daocheng Fu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a9",
          "name": "Botian Shi",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147aa",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ab",
          "name": "Junjun He",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ac",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ad",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ae",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147af",
          "name": "Qiaosheng Zhang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147b0",
          "name": "Wenqi Shao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T14:23:12.000Z",
      "title": "MM-Eureka: 규칙 기반의 규모적인 강화학습을 통한 시각적인 \"아픈 모멘트\" 탐색",
      "summary": "MM-Eureka는 다 모델 논리 모델이며, 대규모 규칙 기반의 강화학습(RL)을 다 모델 논리로 확장하여 성공한 것입니다. 규칙 기반의 RL은 LLM의 논리 능력을 문맥 영역에서 향상시키기 위해 놀라운 성공을 보였지만, 다 모델 설정에 적용하기가 어려웠습니다. 우리 연구는 DeepSeek-R1과 같은 문맥 기반의 RL 시스템의 주요 특성을 다 모델 공간에 재현하고, 정확도 상승, 답변 길이의 안정적인 상승, 반성 행동의 발생 등을 포함하는 것입니다. 우리는 규칙 기반의 RL을 사용하여, 교사 제한 없이 인스탠트 튜닝 모델과 사전 학습 모델이 강력한 다 모델 논리 능력을 개발할 수 있음을 보여주고, 다른 접근 방식에 비해 더 높은 데이터 효율성을 보입니다. 우리는 https://github.com/ModalMinds/MM-EUREKA에서 완전한 파이프라인을 오픈 소스로化和, 이 분야의 발전을 촉진합니다. 우리 모든 코드, 모델, 데이터 등은 릴리스하고 있습니다.",
      "upvotes": 38,
      "discussionId": "67cf9cd137bc7273882147e2",
      "ai_keywords": [
        "multimodal reasoning",
        "rule-based reinforcement learning (RL)",
        "large-scale rule-based reinforcement learning (RL)",
        "DeepSeek-R1",
        "multimodal space",
        "accuracy reward",
        "response length",
        "reflection behaviors",
        "instruction-tuned",
        "pre-trained models",
        "multimodal reasoning capabilities",
        "rule-based RL",
        "supervised fine-tuning",
        "data efficiency"
      ]
    },
    "publishedAt": "2025-03-10T10:23:12.000Z",
    "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning",
    "summary": "We present MM-Eureka, a multimodal reasoning model that successfully extends\nlarge-scale rule-based reinforcement learning (RL) to multimodal reasoning.\nWhile rule-based RL has shown remarkable success in improving LLMs' reasoning\nabilities in text domains, its application to multimodal settings has remained\nchallenging. Our work reproduces key characteristics of text-based RL systems\nlike DeepSeek-R1 in the multimodal space, including steady increases in\naccuracy reward and response length, and the emergence of reflection behaviors.\nWe demonstrate that both instruction-tuned and pre-trained models can develop\nstrong multimodal reasoning capabilities through rule-based RL without\nsupervised fine-tuning, showing superior data efficiency compared to\nalternative approaches. We open-source our complete pipeline to foster further\nresearch in this area. We release all our codes, models, data, etc. at\nhttps://github.com/ModalMinds/MM-EUREKA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07365.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07605",
      "authors": [
        {
          "_id": "67cfa0c1edb742caa3572982",
          "name": "Xun Liang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572983",
          "user": {
            "_id": "669e0b93c7cb0568dac6e92e",
            "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
            "isPro": false,
            "fullname": "hanyu Wang",
            "user": "UglyToilet",
            "type": "user"
          },
          "name": "Hanyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:46.104Z",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572984",
          "name": "Huayi Lai",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572985",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572986",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572987",
          "name": "Jiawei Yang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572988",
          "name": "Jihao Zhao",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572989",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa357298a",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa357298b",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:59:03.000Z",
      "title": "SEAP: 훈련 없이 스패스 엑스プーラー 액티브 학습 준비로 대규모 언어 모델의 능력을 발휘する",
      "summary": "대 언어 모델은 다양한 자연어 처리 태스크에서 놀라운 성공을 거두지만, 추론 시 높은 계산 비용이 주요한 장애물로 남아 있습니다. 본 논문에서는稀疏 Expert Activation Pruning (SEAP)라는 훈련 없이의 파라미터 축소 기법을 통해, 추론 오버헤드를 줄이기 위해 태스크 관련 파라미터를 선택적으로 유지합니다. LLM의 은닉 상태와 활성화의 클러스터링 패턴에 의해 영감을 받아, SEAP는 태스크 고유의 Expert Activation 패턴을 특정하고, 모델을 축소하면서 태스크 성능을 유지하면서 계산 효율성을 향상시킵니다. 실험 결과에 따르면, SEAP는 계산 오버헤드를 크게 줄일 수 있으며, 상대적인 정확도를 유지하는 것을 보여주었습니다. 특히, 50%의 축소율에서 WandA와 FLAP을 초과하는 20% 이상의 효과를 보여주고, 20%의 축소율에서 밀집 모델에 대한 2.2%의 성능 저하만 보였습니다. 이러한 발견은 SEAP의 scalability와 효율성을 명확히 하고, 대규모 LLM의 최적화에 대한 바람직한 접근 방식의 가능성을 보여줍니다.",
      "upvotes": 36,
      "discussionId": "67cfa0c2edb742caa35729dc",
      "githubRepo": "https://github.com/IAAR-Shanghai/SEAP",
      "ai_keywords": [
        "Sparse Expert Activation Pruning (SEAP)",
        "hidden states",
        "activations",
        "task-specific expert activation patterns",
        "computational efficiency"
      ]
    },
    "publishedAt": "2025-03-10T13:59:03.000Z",
    "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models",
    "summary": "Large Language Models have achieved remarkable success across various natural\nlanguage processing tasks, yet their high computational cost during inference\nremains a major bottleneck. This paper introduces Sparse Expert Activation\nPruning (SEAP), a training-free pruning method that selectively retains\ntask-relevant parameters to reduce inference overhead. Inspired by the\nclustering patterns of hidden states and activations in LLMs, SEAP identifies\ntask-specific expert activation patterns and prunes the model while preserving\ntask performance and enhancing computational efficiency. Experimental results\ndemonstrate that SEAP significantly reduces computational overhead while\nmaintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both\nWandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2%\nperformance drop compared to the dense model. These findings highlight SEAP's\nscalability and effectiveness, making it a promising approach for optimizing\nlarge-scale LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07605.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07002",
      "authors": [
        {
          "_id": "67cfa814d212c9c5048845a0",
          "name": "Jiazheng Liu",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a1",
          "name": "Sipeng Zheng",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a2",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:32.095Z",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a3",
          "name": "Zongqing Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T07:32:53.000Z",
      "title": "노트를 씀으로써 집중력이 올라갈까? 　다툼다모달디아로지에 대한向けて\n\n학습\n\nNote-taking and concentration - A multi-modal dialogue approach\n\nLearning",
      "summary": "다턴 다모델 데이터셋 MMDiag를 소개합니다. 이 데이터셋은 특별히 설계된 규칙과 GPT의 도움을 통해 공동 생성되었으며, 질문 간, 질문과 이미지 간, 그리고 다른 이미지 영역 간에 강한 상관관계를 유지함으로써 현실적인 시나리오에 더 가까운 것을 만들었습니다. MMDiag는 다턴 다모델 데이터셋의 학습에 강력한 벤치마크 역할을 하며, MLLM의 기초와 논리력 능력에 대한 더 큰 도전을 제공합니다. 또한, 인간의 시각 처리에 의해 모델을 이어받아 DiagNote라는 MLLM을 소개합니다. DiagNote은 Chain-of-Thought와 注釈를 수행하기 위한 두 개의 모듈(계획과 시각)이 상호작용하여 있습니다. DiagNote은 현재의 MLLM에 의해 더 우수한 기초와 시각 정보 및 언어 정보의 조합으로 논리력을 수행하는 것을 실험적으로 보여주는 것입니다.",
      "upvotes": 29,
      "discussionId": "67cfa818d212c9c504884689",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "vision towers",
        "multi-turn vision question-answering tasks",
        "multi-turn multimodal dialogue dataset (MMDiag)",
        "GPT assistant",
        "multimodal dialogue learning",
        "grounding",
        "reasoning capabilities",
        "Deliberate module",
        "Gaze module",
        "Chain-of-Thought"
      ]
    },
    "publishedAt": "2025-03-10T03:32:53.000Z",
    "title": "Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue\n  Learning",
    "summary": "Multimodal large language models (MLLMs), built on large-scale pre-trained\nvision towers and language models, have shown great capabilities in multimodal\nunderstanding. However, most existing MLLMs are trained on single-turn vision\nquestion-answering tasks, which do not accurately reflect real-world human\nconversations. In this paper, we introduce MMDiag, a multi-turn multimodal\ndialogue dataset. This dataset is collaboratively generated through\ndeliberately designed rules and GPT assistance, featuring strong correlations\nbetween questions, between questions and images, and among different image\nregions; thus aligning more closely with real-world scenarios. MMDiag serves as\na strong benchmark for multi-turn multimodal dialogue learning and brings more\nchallenges to the grounding and reasoning capabilities of MLLMs. Further,\ninspired by human vision processing, we present DiagNote, an MLLM equipped with\nmultimodal grounding and reasoning capabilities. DiagNote consists of two\nmodules (Deliberate and Gaze) interacting with each other to perform\nChain-of-Thought and annotations respectively, throughout multi-turn dialogues.\nWe empirically demonstrate the advantages of DiagNote in both grounding and\njointly processing and reasoning with vision and language information over\nexisting MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07002.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07314",
      "authors": [
        {
          "_id": "67cfa750c8f2a661dc9798fe",
          "name": "Weijia Wu",
          "hidden": false
        },
        {
          "_id": "67cfa750c8f2a661dc9798ff",
          "name": "Zeyu Zhu",
          "hidden": false
        },
        {
          "_id": "67cfa750c8f2a661dc979900",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T13:33:27.000Z",
      "title": "自動화 영화 생성에 의한 다 에이전트 컨텍스트 계획",
      "summary": "현재의 긴 비디오 생성 프레임워크는 자동 계획이 부족하여, 이야기, 장면, 영상 디자인, 캐릭터 간의 상호작용에 대한 손동 입력이 필요하며, 높은 비용과 적절하지 않은 효율을招く。 이러한 문제를 해결하기 위해, 우리는 MovieAgent를 소개합니다. MovieAgent는 효과적인 Chain of Thought(CoT) 계획을 사용하여 자동 영화 생성 프레임워크입니다. MovieAgent는 두 가지 주요 장점을 제공합니다: 1) 우리는 초기에 자동화된 영화/긴 비디오 생성의 패러다임을 탐구하고 정의합니다. 스크립트와 캐릭터 백터를 제공하면, 우리의 MovieAgent는 연결된 나로그를 유지하면서, 캐릭터의 일관성, 동기화된 자막, 비디오 전체의 서브 나로그를 보장하여, 다양한 장면과 각도에서의 긴 비디오를 생성합니다. 2) MovieAgent는 장면의 구성, 카메라 설정, 영상 디자인을 자동적으로 구축하기 위해 휴리스틱한 CoT 기반의 이유 과정을 도입하여, 사람의 노력을 크게 줄입니다. 영화 감독, 스크린 리테이팅, 스토리보드 아티스트, 로케션 매니저의 역할을 모방하여, 여러 LLM 에이전트를 사용함으로써, MovieAgent는 생산 프로세스를 스트리밍화합니다. 실험은 스크립트 의존성, 캐릭터의 일관성, 나로그의 연결성을 새로운 최단거리의 결과를 달성했습니다. 우리의 휴리스틱 프레임워크는 완전한 자동화된 영화 생성에 대해 새로운 통찰을 제공합니다. 코드와 프로젝트 웹 사이트는 다음 URL에서 사용 가능합니다: https://github.com/showlab/MovieAgent 와 https://weijiawu.github.io/MovieAgent.",
      "upvotes": 24,
      "discussionId": "67cfa752c8f2a661dc9799b8",
      "ai_keywords": [
        "MovieAgent",
        "Chain of Thought (CoT)",
        "automated movie/long-video generation",
        "multi-scene, multi-shot long-form videos",
        "coherent narrative",
        "character consistency",
        "synchronized subtitles",
        "stable audio",
        "hierarchical CoT-based reasoning",
        "multiple LLM agents",
        "director",
        "screenwriter",
        "storyboard artist",
        "location manager",
        "script faithfulness",
        "narrative coherence",
        "fully automated movie generation"
      ]
    },
    "publishedAt": "2025-03-10T09:33:27.000Z",
    "title": "Automated Movie Generation via Multi-Agent CoT Planning",
    "summary": "Existing long-form video generation frameworks lack automated planning,\nrequiring manual input for storylines, scenes, cinematography, and character\ninteractions, resulting in high costs and inefficiencies. To address these\nchallenges, we present MovieAgent, an automated movie generation via\nmulti-agent Chain of Thought (CoT) planning. MovieAgent offers two key\nadvantages: 1) We firstly explore and define the paradigm of automated\nmovie/long-video generation. Given a script and character bank, our MovieAgent\ncan generates multi-scene, multi-shot long-form videos with a coherent\nnarrative, while ensuring character consistency, synchronized subtitles, and\nstable audio throughout the film. 2) MovieAgent introduces a hierarchical\nCoT-based reasoning process to automatically structure scenes, camera settings,\nand cinematography, significantly reducing human effort. By employing multiple\nLLM agents to simulate the roles of a director, screenwriter, storyboard\nartist, and location manager, MovieAgent streamlines the production pipeline.\nExperiments demonstrate that MovieAgent achieves new state-of-the-art results\nin script faithfulness, character consistency, and narrative coherence. Our\nhierarchical framework takes a step forward and provides new insights into\nfully automated movie generation. The code and project website are available\nat: https://github.com/showlab/MovieAgent and\nhttps://weijiawu.github.io/MovieAgent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07314.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07216",
      "authors": [
        {
          "_id": "67cfa6fcd77496ce0c154bdc",
          "name": "Sangwoo Park",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bdd",
          "name": "Seanie Lee",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bde",
          "name": "Byungjoo Kim",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bdf",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T11:55:50.000Z",
      "title": "FedRand: 랜덤화 LoRA를 통해 협동 학습의 개인 정보 보호 향상을 위한 업데이트 단위",
      "summary": "Federated Learning (FL)는 분산된 모델 훈련을 수행하기 위해 광범위하게 사용되고 있는 프레임워크입니다. 이 방법은 중앙 서버가 지역 컴퓨터에서 직접 데이터에 액세스하지 않도록 설계되어 있습니다. 그러나 이 접근 방식은 지역 컴퓨터에서 모델이 중앙 서버에 집중될 때 데이터 프라이버시를 완전히 보호할 수 없기 때문에 문제가 있습니다. 이 문제는 Vision Language Model (VLM)을 FL로 훈련하는 경우 더욱 중요합니다. VLM은 훈련 데이터의 인스턴스를 쉽게 기억하고, 멤버십 추론 공격(MIAs)에 취약하게 될 수 있습니다. 이러한 도전에 대처하기 위해 FedRand 프레임워크를 제안합니다. 이 프레임워크에서는 모든 컴퓨터 파라미터를 공개하지 않도록 설계되어 있습니다. 이 프레임워크에서 각 컴퓨터는 서버에서 Low-Rank Adaptation(LoRA)의 서브 파라미터를 랜덤으로 선택하고, 나머지 LoRA의 가중치를 비공개 파라미터로 유지합니다. 이 프레임워크에서 컴퓨터의 비공개 데이터 세트에서 두 가지 파라미터를 모두 훈련한 후, 비공개 컴퓨터 파라미터만 서버에 전송되어 집중됩니다. 이 접근 방식은 컴퓨터 측의 VLM 파라미터를 노출하는 위험을 줄이고, 데이터 프라이버시를 향상시킬 수 있습니다. 실험적으로는 FedRand는 관련 기준과 비교하여 MIAs에 대한 강건성을 향상시키고, 훈련 데이터를 완전히 LoRA 파라미터로 전송하는 방법과 비교하여 다양한 벤치마크 데이터 세트에서 정확도가相当함을 입증했습니다.",
      "upvotes": 22,
      "discussionId": "67cfa6fdd77496ce0c154c18",
      "ai_keywords": [
        "Federated Learning (FL)",
        "vision-language models (VLMs)",
        "membership inference attacks (MIAs)",
        "FedRand framework",
        "Low-Rank Adaptation (LoRA)",
        "subparameters",
        "non-private client parameters",
        "client parameters",
        "aggregation",
        "robustness"
      ]
    },
    "publishedAt": "2025-03-10T07:55:50.000Z",
    "title": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
    "summary": "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07216.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07067",
      "authors": [
        {
          "_id": "67cfa99b7c95194db8d75468",
          "user": {
            "_id": "64b7628af902508f0d7ae112",
            "avatarUrl": "/avatars/83c155254486e80c1dfd14676fdf9215.svg",
            "isPro": false,
            "fullname": "Jongwoo Ko",
            "user": "jongwooko",
            "type": "user"
          },
          "name": "Jongwoo Ko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:29.622Z",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d75469",
          "user": {
            "_id": "64ad94f05a4a60156925ec96",
            "avatarUrl": "/avatars/643bdb076e703bfcc89cec6fccb756c6.svg",
            "isPro": false,
            "fullname": "Tianyi Chen",
            "user": "tianyic",
            "type": "user"
          },
          "name": "Tianyi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:27.139Z",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546a",
          "name": "Sungnyun Kim",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546b",
          "name": "Tianyu Ding",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546c",
          "name": "Luming Liang",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546d",
          "name": "Ilya Zharkov",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546e",
          "name": "Se-Young Yun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T08:51:32.000Z",
      "title": "DistiLLM-2: 대비적인 접근을 통해 LLM의 스타일화 수준을 향상시키기 위한 방법",
      "summary": "그 성공을 얻은 대규모 언어 모델(LLMs)의 디스틸레이션(Distillation)에도 불구하고, 거의 모든 전행 연구는 교사 모델과 학생 모델의 생성 데이터에 동일한 손실 함수를 적용하고 있습니다. 이러한 전략들은 손실 표현과 데이터 유형의 연계를 간과하고, 학생 모델의 성능 향상에 있어 최적의 결과를 얻을 수 없다고 여겨지는 것을 간과하고 있습니다. 이에대해, 우리는 교사의 답의 확률을 높일 때 학생의 답의 확률을 낮추는 방식으로 이 연계를 활용하는 비교적 접근을 제안합니다. 우리의 광범위한 실험에 의하면, DistiLLM-2는 지시와 코드 생성 등 다양한 문제를 해결할 수 있는 고성능의 학생 모델을 구축할 수 있음을 보여주고, 취향 조정과 시각 언어 확장 등 다양한 애플리케이션을 지원합니다. 이러한 발견은, 비교적 접근이 교사 모델과 학생 모델의 연계를 효과적으로 실현하고, 다양한 데이터 유형에 있어 LLM의 디스틸레이션의 효과를 향상시킬 수 있는 가능성을 밝혀줍니다.",
      "upvotes": 19,
      "discussionId": "67cfa99c7c95194db8d754bf",
      "githubRepo": "https://github.com/jongwooko/distillm-2",
      "ai_keywords": [
        "contrastive approach",
        "likelihood",
        "DistiLLM-2",
        "instruction-following",
        "code generation",
        "preference alignment",
        "vision-language extensions"
      ]
    },
    "publishedAt": "2025-03-10T04:51:32.000Z",
    "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs",
    "summary": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07067.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06680",
      "authors": [
        {
          "_id": "67cf94d9f2b1fe815db6db40",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db41",
          "user": {
            "_id": "641a9a4b05290a135041a3ed",
            "avatarUrl": "/avatars/95d66ac607973abe95bd3558c6c93739.svg",
            "isPro": false,
            "fullname": "Pluto",
            "user": "CharonBony",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T01:41:47.194Z",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db42",
          "name": "Zhongxin Guo",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db43",
          "name": "Shaoguang Mao",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db44",
          "name": "Wen Luo",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db45",
          "name": "Guangyue Peng",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db46",
          "name": "Yangyu Huang",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db47",
          "name": "Houfeng Wang",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db48",
          "name": "Scarlett Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T16:11:57.000Z",
      "title": "FEA-Bench: FEA-Bench는 FEA(Finite Element Analysis)의 특성 구현의 리포지토리 수준의 코드 생성을 평가하기 위해 사용되는 벤치마크입니다.",
      "summary": "리포지토리 수준의 코드 기반에서 새로운 기능 구현은 코드 생성 모델의 중요한 응용 분야로 꼽히는 데 있습니다. 그러나 현재의 벤치마크는 이 능력에 대한 전문적인 평가 프레임워크를 부족하게 합니다. 이를 채워주기 위해 FEA-Bench라는 벤치마크를 소개합니다. FEA-Bench는 대규모 언어 모델(LLMs)가 리포지토리 내의 진행 개발을 수행하는 능력을 평가하기 위해 설계되었습니다. 83 페이지의 GitHub 리포지토리에서 Pull Request를 수집하고 규칙 기반과 의도 기반의 필터링을 사용하여 새로운 기능 개발에 초점을 맞추어 Task Instance를 구축했습니다. 각 Task Instance에 포함된 코드 변경은 관련된 단위 테스트 파일과 함께 해결책의 검증이 가능합니다. 새로운 기능 구현에는 LLMs는 새로운 컴포넌트의 코드 완성 기능과 리포지토리 내의 다른 관련 부분의 코드 편집 능력이 동시에 필요합니다. LLMs의 자동 경량 제작 소프트웨어 능력을 더욱 상세하게 평가하는 방법을 제공합니다. 실험 결과를 따르면 LLMs는 FEA-Bench에서 유의미하게 낮은 성능을 보였고, 리포지토리 수준의 진행 개발에 있어 상당한 문제점들이 밝혀졌습니다.",
      "upvotes": 16,
      "discussionId": "67cf94dbf2b1fe815db6db9e"
    },
    "publishedAt": "2025-03-09T12:11:57.000Z",
    "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation",
    "summary": "Implementing new features in repository-level codebases is a crucial\napplication of code generation models. However, current benchmarks lack a\ndedicated evaluation framework for this capability. To fill this gap, we\nintroduce FEA-Bench, a benchmark designed to assess the ability of large\nlanguage models (LLMs) to perform incremental development within code\nrepositories. We collect pull requests from 83 GitHub repositories and use\nrule-based and intent-based filtering to construct task instances focused on\nnew feature development. Each task instance containing code changes is paired\nwith relevant unit test files to ensure that the solution can be verified. The\nfeature implementation requires LLMs to simultaneously possess code completion\ncapabilities for new components and code editing abilities for other relevant\nparts in the code repository, providing a more comprehensive evaluation method\nof LLMs' automated software engineering capabilities. Experimental results show\nthat LLMs perform significantly worse in the FEA-Bench, highlighting\nconsiderable challenges in such repository-level incremental code development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06680.png",
    "numComments": 5,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07027",
      "authors": [
        {
          "_id": "67cf98fd59dbba733d8c531e",
          "user": {
            "_id": "636b3f9ce3ad78bc68b67541",
            "avatarUrl": "/avatars/2b7e745953ae39e01222e99fb63b279e.svg",
            "isPro": false,
            "fullname": "yuxuan",
            "user": "zzyx",
            "type": "user"
          },
          "name": "Yuxuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:57.021Z",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c531f",
          "name": "Yirui Yuan",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5320",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5321",
          "user": {
            "_id": "637745113a63a2983ffbde13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
            "isPro": false,
            "fullname": "Haofan Wang",
            "user": "wanghaofan",
            "type": "user"
          },
          "name": "Haofan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:54.821Z",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5322",
          "name": "Jiaming Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T08:07:17.000Z",
      "title": "EasyControl: 확산 모델에 효율적이고 유연한 제어를 추가하는 시스템\nTransformer",
      "summary": "최근 Unet 기반의 diffusion 모델의 발전으로 ControlNet과 IP-Adapter 등 효과적인 공간과 주제의 제어 구조가 도입되었습니다が DiT(Diffusion Transformer) 아키텍처는 효율적이고 유연한 제어에 어려움을 겪고 있습니다. 이러한 문제를 해결하기 위해 EasyControl라는 새로운 프레임워크를 제안합니다. 이 프레임워크는 조건에 의해 유도된 diffusion transformer를 통합하여 높은 효율성과 유연성을 갖습니다. 프레임워크는 3가지의 핵심 인нова션에 기반하여 구축되었습니다. 먼저, 라운드 웨이트의 Condition Injection LoRA 모듈을 도입합니다. 이 모듈은 조건 신호를 분리하여 처리하며, 플러그인 및 플레이인 솔루션으로 작동합니다. 기본 모델의 가중치를 변경하지 않도록, 사용자 정의 모델과의 호환성을 보장하고 다양한 조건의 유연한 주입을 가능하게 합니다. 특히, 이 모듈은 단일 조건 데이터로 학습된 경우에도, 조화적이고 강력한 0-shot 다조건 확장성을 지원합니다. 다음으로, Position-Aware Training Paradigm을 제안합니다. 이 접근 방식은 입력 조건을 고정 레지션에 표준화하여, 임의의 비율과 유연한 레지션의 이미지 생성을 가능하게 합니다. 동시에, 계산 효율성을 최적화하고, 실제 세계적인 애플리케이션에 대한 실용성을 갖습니다. 마지막으로, Causal Attention Mechanism과 KV Cache 기술이 결합된 조건에 의한 생성 태스크에 적용됩니다. 이 인нова션은 이미지 합성의 지연을 크게 줄이고, 전체 프레임워크의 효율성을 향상시킵니다. 확장된 실험에서, EasyControl는 다양한 애플리케이션 시나리오에서 뛰어난 성능을 발휘하고 있습니다. 이러한 인нова션은 프레임워크의 높은 효율성, 유연성과 다양한 태스크에 적합한 특성을 공유합니다.",
      "upvotes": 15,
      "discussionId": "67cf990359dbba733d8c545d",
      "ai_keywords": [
        "Unet-based diffusion models",
        "ControlNet",
        "IP-Adapter",
        "DiT (Diffusion Transformer)",
        "Condition Injection LoRA Module",
        "Condition Injection",
        "zero-shot multi-condition generalization",
        "Position-Aware Training Paradigm",
        "Position-Aware",
        "Causal Attention Mechanism",
        "KV Cache",
        "conditional generation tasks",
        "image synthesis"
      ]
    },
    "publishedAt": "2025-03-10T04:07:17.000Z",
    "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
    "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07027.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07602",
      "authors": [
        {
          "_id": "67cfb2efb77bc8e7d415f904",
          "name": "Yujie Wei",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f905",
          "name": "Shiwei Zhang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f906",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:32.780Z",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f907",
          "name": "Biao Gong",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f908",
          "user": {
            "_id": "6492a0d8d4ae24c933ace44d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DXIky2sdPwmiCOR9p-JBQ.png",
            "isPro": false,
            "fullname": "Longxiang Tang",
            "user": "lloong",
            "type": "user"
          },
          "name": "Longxiang Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:30.700Z",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f909",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90a",
          "name": "Haonan Qiu",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90b",
          "name": "Hengjia Li",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90c",
          "name": "Shuai Tan",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90d",
          "name": "Yingya Zhang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90e",
          "name": "Hongming Shan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:03.000Z",
      "title": "DreamRelation: 관계센터의 비디오 맞춤형 서비스",
      "summary": "관계 비디오 카스템이 사용자 지정의 두 가지 주제 간의 관계를 묘사하는 개인화된 비디오의 제작이며, 사진 콘텐츠 이해에 중요한 작업입니다. 기존의 방법은 주제의 외모와 동작을 전문화할 수 있지만, 복잡한 관계 비디오 카스템에 적응하지 못합니다. 정확한 관계 모델링과 주제 카테고리의 높은 일반화가 중요합니다. 주요 문제점은 관계에 고유한 복잡한 공간 배치, 순서 변화, 시간적 움직임의 복잡성으로 인해 발생합니다. 현재의 모델은 의미있는 상호작용을 감지하지 못하고, 관계의 무관한 시각적 세부 사항에 과도하게 중점을 둡니다. 이러한 문제를 해결하기 위해, 우리는 DreamRelation을 제안합니다. DreamRelation은 작은 샘플 비디오를 사용하여 관계를 전문화하는 새로운 접근입니다. 두 가지 주요 구성 요소를 사용합니다: 관계 디코딩 학습과 관계 다이나믹스 아키텍처. 먼저, 관계 디코딩 학습에서, 관계 로라터 튜플과 결합된 마스크 훈련 전략을 사용하여 주제의 외모로부터 관계를 분리하고, 다양한 관계에서 더 좋은 일반화를 보장합니다. 또한, MM-DiT의 액션 구조의 쿼리, 키, 밸류 특성의 다른 역할을 분석하여 관계 로라터 튜플의 최적 설계를 결정하고, DreamRelation은 설명 가능한 첫 번째 관계 비디오 생성 프레임워크로 이루어집니다. 다음으로, 관계 다이나믹스 아키텍처에서, 공간적 관계의 비교 손실을 도입하여 관계 다이나믹스를 우선시하고, 주제의 상세한 외모에 의존하지 않도록 합니다. 확장 검증은 DreamRelation이 상태의 최전단 방법에서 관계 비디오 카스템을 표현하는 데 뛰어납니다. 코드와 모델은 공개적으로 제공됩니다.",
      "upvotes": 10,
      "discussionId": "67cfb2f1b77bc8e7d415f96b",
      "ai_keywords": [
        "Relational Decoupling Learning",
        "Relational Dynamics Enhancement",
        "relation LoRA triplet",
        "hybrid mask training strategy",
        "attention mechanism",
        "space-time relational contrastive loss",
        "MM-DiT"
      ]
    },
    "publishedAt": "2025-03-10T13:58:03.000Z",
    "title": "DreamRelation: Relation-Centric Video Customization",
    "summary": "Relational video customization refers to the creation of personalized videos\nthat depict user-specified relations between two subjects, a crucial task for\ncomprehending real-world visual content. While existing methods can personalize\nsubject appearances and motions, they still struggle with complex relational\nvideo customization, where precise relational modeling and high generalization\nacross subject categories are essential. The primary challenge arises from the\nintricate spatial arrangements, layout variations, and nuanced temporal\ndynamics inherent in relations; consequently, current models tend to\noveremphasize irrelevant visual details rather than capturing meaningful\ninteractions. To address these challenges, we propose DreamRelation, a novel\napproach that personalizes relations through a small set of exemplar videos,\nleveraging two key components: Relational Decoupling Learning and Relational\nDynamics Enhancement. First, in Relational Decoupling Learning, we disentangle\nrelations from subject appearances using relation LoRA triplet and hybrid mask\ntraining strategy, ensuring better generalization across diverse relationships.\nFurthermore, we determine the optimal design of relation LoRA triplet by\nanalyzing the distinct roles of the query, key, and value features within\nMM-DiT's attention mechanism, making DreamRelation the first relational video\ngeneration framework with explainable components. Second, in Relational\nDynamics Enhancement, we introduce space-time relational contrastive loss,\nwhich prioritizes relational dynamics while minimizing the reliance on detailed\nsubject appearances. Extensive experiments demonstrate that DreamRelation\noutperforms state-of-the-art methods in relational video customization. Code\nand models will be made publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07602.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06580",
      "authors": [
        {
          "_id": "67cfa71827c7f0b2db19f7c2",
          "user": {
            "_id": "645b4a2978730bcc103dfe4d",
            "avatarUrl": "/avatars/de544de899897fd0a83506ff287123bc.svg",
            "isPro": false,
            "fullname": "Yuxiang Zhang",
            "user": "TokerZ",
            "type": "user"
          },
          "name": "Yuxiang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:40.336Z",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c3",
          "name": "Yuqi Yang",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c4",
          "name": "Jiangming Shu",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c5",
          "name": "Xinyan Wen",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c6",
          "name": "Jitao Sang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T12:19:47.000Z",
      "title": "アウトプット:\nAgentモデル: 행동 생성의 연속을 논리적 이유로 내포하는 모델",
      "summary": "전통적인 에이전트 작업 흐름은 외부의 Prompt를 사용하여 도구와 환경 간의 상호작용을 관리하며, 이는 추론 모델의 자율성을 제한합니다. 우리는 내부화하는 Chain-of-Action(CoA) 생성을 통해 자율적으로 결정한 시간과 방법에 따라 외부 도구를 사용하는 LAMs(라일지 제네트 모델)을 제안하고 있습니다. 제안된 AutoCoA 프레임워크는 정규 학습(SFT)과 강화 학습(RL)을 조합하여 모델이 추론과 행동을 끊임없이 진행하면서 환경과의 상호작용을 효율적으로 관리할 수 있게 합니다. 주요 구성 요소는 단계별 행동 트라이거, 트래지레벨의 CoA 최적화, 그리고 내부의 월드 모델입니다. 개방 영역 QA 태스크의 평가에 따르면, AutoCoA에 의해 훈련된 에이전트 모델은 특히 장기적인 이유와 다단계 행동이 필요한 태스크에서 ReAct 기반의 작업 흐름을 크게 초과하는 것을 보여줍니다. 코드와 데이터 세트는 https://github.com/ADaM-BJTU/AutoCoA에서 사용 가능합니다.",
      "upvotes": 10,
      "discussionId": "67cfa71927c7f0b2db19f817",
      "githubRepo": "https://github.com/ADaM-BJTU/AutoCoA",
      "ai_keywords": [
        "Large Agent Models (LAMs)",
        "Chain-of-Action (CoA)",
        "AutoCoA framework",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "step-level action triggering",
        "trajectory-level CoA optimization",
        "internal world model"
      ]
    },
    "publishedAt": "2025-03-09T08:19:47.000Z",
    "title": "Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models",
    "summary": "Traditional agentic workflows rely on external prompts to manage interactions\nwith tools and the environment, which limits the autonomy of reasoning models.\nWe position Large Agent Models (LAMs) that internalize the generation of\nChain-of-Action (CoA), enabling the model to autonomously decide when\nand how to use external tools. Our proposed AutoCoA framework combines\nsupervised fine-tuning (SFT) and reinforcement learning (RL), allowing the\nmodel to seamlessly switch between reasoning and action while efficiently\nmanaging environment interactions. Main components include step-level action\ntriggering, trajectory-level CoA optimization, and an internal world model to\nreduce real-environment interaction costs. Evaluations on open-domain QA tasks\ndemonstrate that AutoCoA-trained agent models significantly outperform\nReAct-based workflows in task completion, especially in tasks that require\nlong-term reasoning and multi-step actions. Code and dataset are available at\nhttps://github.com/ADaM-BJTU/AutoCoA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06580.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07608",
      "authors": [
        {
          "_id": "67cfa5bcb17ca92d24da9033",
          "user": {
            "_id": "65a4a180c8a09bd5e8e900b8",
            "avatarUrl": "/avatars/c135db68f6ff2c40119acd2e9ddce968.svg",
            "isPro": false,
            "fullname": "Bo Jiang",
            "user": "rb93dett",
            "type": "user"
          },
          "name": "Bo Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:43.665Z",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9034",
          "name": "Shaoyu Chen",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9035",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9036",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9037",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:59:42.000Z",
      "title": "AlphaDrive: 자율주행에서 VLMs의 역량을 풀어내는 강화학습과 논리론",
      "summary": "OpenAI o1와 DeepSeek R1은 수학 및 과학의 복잡한 분야에서 강화학습(RL)과 논리론이 중요한 역할을 하며, 인간 전문의 수준의 성능을 달성하거나 초월할 수 있습니다. 자동주행 분야에서는 최근부터 모델은 계획 성능을 크게 향상시켰지만, 제한된 지식과 논리론 능력으로 인해 긴 꼬리 문제를 처리할 수 있는 문제가 남아 있습니다. 일부 연구는 시각 언어 모델(VLMs)을 통합하여 자동주행을 위한 연구를 진행하고 있지만, 일반적으로 단순한 지도 학습(SFT)에 의존하며, 특히 계획에 적합한 최적화 전략을 시도하지 않습니다. 본 논문에서는 자동주행의 VLMs에 대한 RL과 논리론의 프레임워크를 제안하고 있습니다. AlphaDrive는 계획에 적합한 GRPO 기반의 RL 보상을 4개 도입하고, SFT와 RL의 조합을 통해 2단계 계획 논리론 학습 전략을 사용합니다. 이로 인해 AlphaDrive는 SFT만 사용하거나 논리론을 포함하지 않는 경우보다 계획 성능과 학습 효율을 크게 향상시켰습니다. 또한, RL 학습 후 AlphaDrive는 운전 안전성과 효율 향상에 중요한 다양한 계획 능력을 발견할 수 있다는 것을 흥미로 합니다. 우리 지식의 한계로 인해 AlphaDrive는 GRPO 기반의 RL과 계획 논리론을 통합한 자동주행을 위한 모델임을 알고 있습니다. 코드는 향후 연구를 촉진하기 위해 공개됩니다.",
      "upvotes": 9,
      "discussionId": "67cfa5bdb17ca92d24da9064",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "reasoning",
        "end-to-end models",
        "vision-language models (VLMs)",
        "supervised fine-tuning (SFT)",
        "GRPO-based RL rewards",
        "two-stage planning reasoning training strategy",
        "emergent multimodal planning capabilities"
      ]
    },
    "publishedAt": "2025-03-10T13:59:42.000Z",
    "title": "AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via\n  Reinforcement Learning and Reasoning",
    "summary": "OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level\nperformance in complex domains like mathematics and science, with reinforcement\nlearning (RL) and reasoning playing a crucial role. In autonomous driving,\nrecent end-to-end models have greatly improved planning performance but still\nstruggle with long-tailed problems due to limited common sense and reasoning\nabilities. Some studies integrate vision-language models (VLMs) into autonomous\ndriving, but they typically rely on pre-trained models with simple supervised\nfine-tuning (SFT) on driving data, without further exploration of training\nstrategies or optimizations specifically tailored for planning. In this paper,\nwe propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous\ndriving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning\nand employs a two-stage planning reasoning training strategy that combines SFT\nwith RL. As a result, AlphaDrive significantly improves both planning\nperformance and training efficiency compared to using only SFT or without\nreasoning. Moreover, we are also excited to discover that, following RL\ntraining, AlphaDrive exhibits some emergent multimodal planning capabilities,\nwhich is critical for improving driving safety and efficiency. To the best of\nour knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning\nreasoning into autonomous driving. Code will be released to facilitate future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07608.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05244",
      "authors": [
        {
          "_id": "67cfebe18a4265f3656a50aa",
          "user": {
            "_id": "642d430a7f9efee76b8713c0",
            "avatarUrl": "/avatars/4981f166a6df8e2ea60cd4c41c2f44d4.svg",
            "isPro": false,
            "fullname": "YuningWu",
            "user": "AQuarterMile",
            "type": "user"
          },
          "name": "Yuning Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:24:29.900Z",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ab",
          "name": "Jiahao Mei",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ac",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ad",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ae",
          "name": "SHaopeng Lai",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50af",
          "name": "Yuran Ren",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b0",
          "name": "Zijia Wang",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b1",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b2",
          "name": "Mengyue Wu",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b3",
          "name": "Qin Jin",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b4",
          "name": "Fei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T08:56:20.000Z",
      "title": "WritingBench: 복잡한 ジェネレータ 사진의 위한 벤치마크\n\n(Note: The original text \"WritingBench: ジェネレータ写真のためのコンプレックスなベンチマーク\" was translated to Korean as \"WritingBench: 복잡한 ジェネレータ 사진의 위한 벤치마크\". The translation maintains the original meaning and structure while ensuring accuracy and professionalism.)",
      "summary": "최근의 대규모 언어 모델(LLMs)의 발전은 텍스트 생성 능력이 크게 향상되었지만, 생성된 문장의 성능 평가는 어려운 문제입니다. 현재의 벤치마크는 주로 일반적인 텍스트 생성이나 특정한 문장 작업에 초점을 맞추고 있지만, 고품질의 다양한 문장 콘텐츠의 요구를 이해하지 않습니다. 이를 해결하기 위해, 우리는 WritingBench를 소개합니다. 이것은 6개의 핵심 문장 디레Й와 100개의 서브디레イン으로 구성되어 있으며, 창의적이고 설명적인, 정보적인, 기술적인 문장을 포함하는 상세한 벤치마크입니다. 또한, LLMs가 인스턴스 고유의 평가 기준을 동적으로 생성할 수 있는 질문에 의존하는 평가 프레임워크를 제안합니다. 이 프레임워크는 평가 기준에 대한 평가 모델로 보완되어, 스타일, 형식, 길이의 평가가 가능합니다. 이 프레임워크의 유효성은 7B 파라미터 모델이 최신(SOTA) 성능에 근접하는 것을 보여주고 있습니다. 우리는 이 벤치마크, 평가 도구, 모듈화된 프레임워크 컴포넌트를 공개하여 LLMs의 문장 개발에 기여합니다.",
      "upvotes": 9,
      "discussionId": "67cfebe38a4265f3656a5136",
      "githubRepo": "https://github.com/X-PLUG/WritingBench",
      "ai_keywords": [
        "large language models (LLMs)",
        "text generation",
        "generative writing",
        "benchmarks",
        "writing domains",
        "subdomains",
        "creative writing",
        "persuasive writing",
        "informative writing",
        "technical writing",
        "query-dependent evaluation framework",
        "instance-specific assessment criteria",
        "critic model",
        "criteria-aware scoring",
        "data curation"
      ]
    },
    "publishedAt": "2025-03-07T03:56:20.000Z",
    "title": "WritingBench: A Comprehensive Benchmark for Generative Writing",
    "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05244.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04629",
      "authors": [
        {
          "_id": "67cfbab6607797f40c6d4164",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4165",
          "name": "Shiyang Feng",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4166",
          "name": "Jiakang Yuan",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4167",
          "name": "Renqiu Xia",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4168",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4169",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d416a",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T17:15:48.000Z",
      "title": "슬라이드 포어징: 오프라인 휴리리스틱, 메모리 드라이브 제네레이션, 그리고 자동화 조사서 작성의 다차원 평가",
      "summary": "조사 논문은 과학 연구에 중요한 역할을 하고 있으며, 특히 연구 논문의 급격한 증가의 背景에서 그 중요성이 더욱 높아져 있습니다. 최근, 연구자들은 LLM을 사용하여 조사의 생성을 자동화하고 더 효율적인 연구를 실현하기 위해 노력을 기울이고 있습니다. 그러나 LLM으로 생성된 조사 논문과 인간이 작성한 조사 논문 사이에서, 특히 대목의 질과 인용의 정확성에서 명확한 질의 차이가 남아 있습니다. 이러한 차이를 메우기 위해, 우리는 SurveyForge를 소개합니다. SurveyForge는 인간이 작성한 대목의 논리적 구조를 분석하고, 검색된 분야 관련 논문을 참조하여 대목을 생성합니다. 다음으로, 우리 학문 네비게이터에서 검색된 고품질의 논문을 활용하여 생성된 논문의 내용을 자동으로 생성하고 개선합니다. 또한, 전체적인 평가를 실현하기 위해, 우리는 SurveyBench를 구축하여 100편의 인간이 작성한 조사 논문을 포함하며, 승률 비교를 수행하여 AI가 생성된 조사 논문을 대목, 내용의 질의 3차원으로 평가합니다. 실험은 SurveyForge가 이전의 작업보다 뛰어난 결과를 보여주고 있습니다.",
      "upvotes": 9,
      "discussionId": "67cfbab9607797f40c6d4206",
      "ai_keywords": [
        "LLMs (Large Language Models)",
        "SurveyForge",
        "SurveyBench",
        "AutoSurvey"
      ]
    },
    "publishedAt": "2025-03-06T12:15:48.000Z",
    "title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing",
    "summary": "Survey paper plays a crucial role in scientific research, especially given\nthe rapid growth of research publications. Recently, researchers have begun\nusing LLMs to automate survey generation for better efficiency. However, the\nquality gap between LLM-generated surveys and those written by human remains\nsignificant, particularly in terms of outline quality and citation accuracy. To\nclose these gaps, we introduce SurveyForge, which first generates the outline\nby analyzing the logical structure of human-written outlines and referring to\nthe retrieved domain-related articles. Subsequently, leveraging high-quality\npapers retrieved from memory by our scholar navigation agent, SurveyForge can\nautomatically generate and refine the content of the generated article.\nMoreover, to achieve a comprehensive evaluation, we construct SurveyBench,\nwhich includes 100 human-written survey papers for win-rate comparison and\nassesses AI-generated survey papers across three dimensions: reference,\noutline, and content quality. Experiments demonstrate that SurveyForge can\noutperform previous works such as AutoSurvey.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04629.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04812",
      "authors": [
        {
          "_id": "67ce5542818e1825dea7440b",
          "user": {
            "_id": "6626449503e1f561573d30e9",
            "avatarUrl": "/avatars/e7f9720ccd01bae32d0a03a1b0dacab5.svg",
            "isPro": false,
            "fullname": "Zhibin Lan",
            "user": "zhibinlan",
            "type": "user"
          },
          "name": "Zhibin Lan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:54.535Z",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440c",
          "user": {
            "_id": "635239137d071f23d083b056",
            "avatarUrl": "/avatars/1f1a0ed38d8de499d4b78922801c6d95.svg",
            "isPro": false,
            "fullname": "liqiang niu",
            "user": "lqniu",
            "type": "user"
          },
          "name": "Liqiang Niu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:51.713Z",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440d",
          "name": "Fandong Meng",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440e",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440f",
          "name": "Jinsong Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T10:21:57.000Z",
      "title": "LLaVE: 규모 언어와 시각의 내장 모델에서의 어려움 가중치付의 비교 학습",
      "summary": "Universal multimodal embedding models play a critical role in tasks such as interleaved image-text retrieval, multimodal RAG, and multimodal clustering. However, our empirical results indicate that existing LMM-based embedding models trained with the standard InfoNCE loss exhibit a high degree of overlap in similarity distribution between positive and negative pairs, making it challenging to distinguish hard negative pairs effectively. To deal with this issue, we propose a simple yet effective framework that dynamically improves the embedding model's representation learning for negative pairs based on their discriminative difficulty. Within this framework, we train a series of models, named LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks and 36 datasets. Experimental results show that LLaVE establishes stronger baselines that achieve state-of-the-art (SOTA) performance while demonstrating strong scalability and efficiency. Specifically, LLaVE-2B surpasses the previous SOTA 7B models, while LLaVE-7B achieves a further performance improvement of 6.2 points. Although LLaVE is trained on image-text data, it can generalize to text-video retrieval tasks in a zero-shot manner and achieve strong performance, demonstrating its remarkable potential for transfer to other embedding tasks.",
      "upvotes": 9,
      "discussionId": "67ce5543818e1825dea74480",
      "githubRepo": "https://github.com/DeepLearnXMU/LLaVE",
      "ai_keywords": [
        "multimodal embedding models",
        "interleaved image-text retrieval",
        "multimodal RAG",
        "multimodal clustering",
        "LMM-based embedding models",
        "InfoNCE loss",
        "similarity distribution",
        "hard negative pairs",
        "representation learning",
        "LLaVE",
        "MMEB benchmark",
        "meta-tasks",
        "datasets",
        "state-of-the-art (SOTA)",
        "scalability",
        "efficiency",
        "text-video retrieval tasks",
        "zero-shot manner",
        "transfer"
      ]
    },
    "publishedAt": "2025-03-04T05:21:57.000Z",
    "title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted\n  Contrastive Learning",
    "summary": "Universal multimodal embedding models play a critical role in tasks such as\ninterleaved image-text retrieval, multimodal RAG, and multimodal clustering.\nHowever, our empirical results indicate that existing LMM-based embedding\nmodels trained with the standard InfoNCE loss exhibit a high degree of overlap\nin similarity distribution between positive and negative pairs, making it\nchallenging to distinguish hard negative pairs effectively. To deal with this\nissue, we propose a simple yet effective framework that dynamically improves\nthe embedding model's representation learning for negative pairs based on their\ndiscriminative difficulty. Within this framework, we train a series of models,\nnamed LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks\nand 36 datasets. Experimental results show that LLaVE establishes stronger\nbaselines that achieve state-of-the-art (SOTA) performance while demonstrating\nstrong scalability and efficiency. Specifically, LLaVE-2B surpasses the\nprevious SOTA 7B models, while LLaVE-7B achieves a further performance\nimprovement of 6.2 points. Although LLaVE is trained on image-text data, it can\ngeneralize to text-video retrieval tasks in a zero-shot manner and achieve\nstrong performance, demonstrating its remarkable potential for transfer to\nother embedding tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04812.png",
    "numComments": 2,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07459",
      "authors": [
        {
          "_id": "67cfd1934fed2b7e3e4cbb34",
          "user": {
            "_id": "63357c608adfa81faf2ac180",
            "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
            "isPro": false,
            "fullname": "Xiangru Tang",
            "user": "RTT1",
            "type": "user"
          },
          "name": "Xiangru Tang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T06:00:52.457Z",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb35",
          "name": "Daniel Shao",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb36",
          "name": "Jiwoong Sohn",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb37",
          "name": "Jiapeng Chen",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb38",
          "name": "Jiayi Zhang",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb39",
          "name": "Jinyu Xiang",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3a",
          "name": "Fang Wu",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3b",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3c",
          "name": "Chenglin Wu",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3d",
          "user": {
            "_id": "65cae89119683f9817c049ea",
            "avatarUrl": "/avatars/b08b10d7c72e2cf1108147e659411b32.svg",
            "isPro": false,
            "fullname": "Wenqi Shi",
            "user": "wshi83",
            "type": "user"
          },
          "name": "Wenqi Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:16.321Z",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3e",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3f",
          "name": "Mark Gerstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T15:38:44.000Z",
      "title": "MedAgentsBench: 복잡한 의료 윤리 평가를 위한 사고 모형과 에이전트 프레임워크의 벤치마크",
      "summary": "대 언어 모델(LLMs)은 현재의 의료 질문 평가에서 놀라운 성능을 보여주고 있습니다. 이러한 높은 성능으로, 의미적인 평가와 발전된 방법의 차이를 명확히 하는 것이 어려워졌습니다. 우리는 현재의 모델이 표준 테스트에서 강력한 성능을 보여주지만, 여러 단계의 임상적인 이유, 진단의 구성, 치료 계획의 시나리오에 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추는 데 초점을 맞추",
      "upvotes": 7,
      "discussionId": "67cfd1944fed2b7e3e4cbb81",
      "ai_keywords": [
        "MedAgentsBench",
        "multi-step clinical reasoning",
        "diagnosis formulation",
        "treatment planning",
        "MedAgentsBench",
        "DeepSeek R1",
        "OpenAI o3",
        "advanced search-based agent methods"
      ]
    },
    "publishedAt": "2025-03-10T11:38:44.000Z",
    "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning",
    "summary": "Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https://github.com/gersteinlab/medagents-benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07459.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06749",
      "authors": [
        {
          "_id": "67cfb6495944a8e54f24cd9a",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9b",
          "name": "Bohan Jia",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9c",
          "name": "Zijie Zhai",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9d",
          "name": "Shaosheng Cao",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9e",
          "name": "Zheyu Ye",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9f",
          "name": "Fei Zhao",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cda0",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cda1",
          "name": "Shaohui Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T20:06:45.000Z",
      "title": "비전-R1: 다 모델 대 언어 모델의 이유 능력을 촉진하는 모델\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "summary": "DeepSeek-R1-Zero는 강화학습(RL)을 통해 LLM의 논리적인 인지능력을 발견하는 데 성공했습니다. 이 혁신을 통해 RL이 MLLM의 논리적인 인지능력을 향상시킬 수 있는지 조사했습니다. 그러나 직접 RL을 사용한 학습은 MLLM에서 복잡한 논리적인 인지능력(예: 질문과 반성)을 활성화시키는 것이 어려워서 인지했습니다. 이 문제를 해결하기 위해 논리적인 MLLM, Vision-R1을 제안했습니다. 특히, 기존의 MLLM과 DeepSeek-R1을 모델교류와 데이터필터링을 사용하여 고품질의 다형 논리적 데이터셋을 구축하고 200K의 다형 논리적 데이터셋, Vision-R1-cold 데이터셋을 만들었습니다. 이 데이터셋은 Vision-R1의 초기 데이터로 사용됩니다. 초기 단계에서의 과도한 생각에 의한 최적화 문제를 완화하기 위해 발전적인 사고제어 학습(PTST) 전략을 제안하고, 그룹적 정책 최적화(GRPO)와 엄격한 형식화된 결과 보상 함수를 사용하여 10K의 다형 수학 데이터셋을 사용하여 정확한 복잡한 논리적 프로세스 학습 능력을 점차적으로 훈련시켰습니다. 세부적인 실험은 우리의 모델이 다형 수학 논리적 벤치마크에서 평균 6% 정도의 향상을 달성했습니다. Vision-R1-7B은 MathVista 벤치마크에서 73.5%의 정확도를 달성했으며, 논리적 모델 중 가장 높은 수준을 나타내며, OpenAI O1보다 0.4% 낮은 성능을 나타냅니다. 데이터셋과 코드는 아래 URL에서 공개됩니다: https://github.com/Osilly/Vision-R1",
      "upvotes": 7,
      "discussionId": "67cfb64f5944a8e54f24cf33",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "MLLMs",
        "multimodal reasoning",
        "CoT dataset",
        "modality bridging",
        "data filtering",
        "Vision-R1-cold dataset",
        "Progressive Thinking Suppression Training (PTST)",
        "Group Relative Policy Optimization (GRPO)",
        "hard formatting result reward function",
        "multimodal math dataset",
        "MathVista benchmark",
        "OpenAI O1"
      ]
    },
    "publishedAt": "2025-03-09T16:06:45.000Z",
    "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models",
    "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning\ncapabilities in LLMs purely through Reinforcement Learning (RL). Inspired by\nthis breakthrough, we explore how RL can be utilized to enhance the reasoning\ncapability of MLLMs. However, direct training with RL struggles to activate\ncomplex reasoning capabilities such as questioning and reflection in MLLMs, due\nto the absence of substantial high-quality multimodal reasoning data. To\naddress this issue, we propose the reasoning MLLM, Vision-R1, to improve\nmultimodal reasoning capability. Specifically, we first construct a\nhigh-quality multimodal CoT dataset without human annotations by leveraging an\nexisting MLLM and DeepSeek-R1 through modality bridging and data filtering to\nobtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as\ncold-start initialization data for Vision-R1. To mitigate the optimization\nchallenges caused by overthinking after cold start, we propose Progressive\nThinking Suppression Training (PTST) strategy and employ Group Relative Policy\nOptimization (GRPO) with the hard formatting result reward function to\ngradually refine the model's ability to learn correct and complex reasoning\nprocesses on a 10K multimodal math dataset. Comprehensive experiments show our\nmodel achieves an average improvement of sim6% across various multimodal\nmath reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely\nused MathVista benchmark, which is only 0.4% lower than the leading reasoning\nmodel, OpenAI O1. The datasets and code will be released in:\nhttps://github.com/Osilly/Vision-R1 .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06749.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07507",
      "authors": [
        {
          "_id": "67cfa44c3a9d50150f59ffe1",
          "name": "Jie Hu",
          "hidden": false
        },
        {
          "_id": "67cfa44c3a9d50150f59ffe2",
          "name": "Shizun Wang",
          "hidden": false
        },
        {
          "_id": "67cfa44c3a9d50150f59ffe3",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T16:29:10.000Z",
      "title": "PE3R: 3차원 재구성의 관측 효율성\n\n(Note: The original text \"PE3R: 観測効率的3次元再構築\" translates to \"PE3R: Efficiency of Observation for 3D Reconstruction\" in English. However, since the instruction was to translate into Korean without adding explanations, the translation provided above is in Korean.)",
      "summary": "최근 2D에서 3D인식의 발전은 2D 이미지에서 3D 스케네를 이해するために 크게 개선되었습니다. 그러나 현재의 방법들은 스케네 간의 유한한 일반화, 인식 정확도의 저하, 재구성 속도의 느림 등 중요한 문제점을 드러냅니다. 이러한 제한을 해결하기 위해, 우리는 정확성과 효율성을 동시에 달성할 수 있는 새로운 프레임워크 \"Perception-Efficient 3D Reconstruction (PE3R)\"를 제안합니다. PE3R는 3D 의미 필드의 고속 재구성을 가능하게 하는 전방적 아키텍처를 사용합니다. 이 프레임워크는 다양한 스케네와 물체에 대한 강한 0-shot 일반화를 보여주고, 재구성 속도를 크게 향상시킬 수 있습니다. 2D에서 3D의 오픈 벡터 분할 및 3D 재구성에서 분산적인 실험은 PE3R의 효과와 광범위성을 입증했습니다. 이 프레임워크는 3D 의미 필드의 재구성 속도에 9배 이상의 속도 업을 달성하고, 인식 정확도와 재구성 정확도에도 큰 효과를 보였으며, 이 분야에서 새로운 기준을 세웠습니다. 코드는 https://github.com/hujiecpp/PE3R에서 공개되어 있습니다.",
      "upvotes": 5,
      "discussionId": "67cfa4503a9d50150f5a0137",
      "ai_keywords": [
        "feed-forward architecture",
        "3D semantic field reconstruction",
        "zero-shot generalization",
        "2D-to-3D open-vocabulary segmentation",
        "perception accuracy",
        "reconstruction precision",
        "speedup"
      ]
    },
    "publishedAt": "2025-03-10T12:29:10.000Z",
    "title": "PE3R: Perception-Efficient 3D Reconstruction",
    "summary": "Recent advancements in 2D-to-3D perception have significantly improved the\nunderstanding of 3D scenes from 2D images. However, existing methods face\ncritical challenges, including limited generalization across scenes, suboptimal\nperception accuracy, and slow reconstruction speeds. To address these\nlimitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel\nframework designed to enhance both accuracy and efficiency. PE3R employs a\nfeed-forward architecture to enable rapid 3D semantic field reconstruction. The\nframework demonstrates robust zero-shot generalization across diverse scenes\nand objects while significantly improving reconstruction speed. Extensive\nexperiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction\nvalidate the effectiveness and versatility of PE3R. The framework achieves a\nminimum 9-fold speedup in 3D semantic field reconstruction, along with\nsubstantial gains in perception accuracy and reconstruction precision, setting\nnew benchmarks in the field. The code is publicly available at:\nhttps://github.com/hujiecpp/PE3R.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07507.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07197",
      "authors": [
        {
          "_id": "67cfa76cf36e4221c5009654",
          "user": {
            "_id": "624f909eac5dd186b01ac3f5",
            "avatarUrl": "/avatars/71a5c93c491064ef9e1eda80fda90665.svg",
            "isPro": false,
            "fullname": "Zebin You",
            "user": "yyyou",
            "type": "user"
          },
          "name": "Zebin You",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:38.059Z",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009655",
          "name": "Jingyang Ou",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009656",
          "name": "Xiaolu Zhang",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009657",
          "name": "Jun Hu",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009658",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009659",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T11:27:12.000Z",
      "title": "효과적인 및 효율적인 마스크 이미지 생성 모델",
      "summary": "隠れコードモデル와 隠れ拡散モデル는 서로 다른 동기와 목적에 의해 설계되어 있습니다만, 우리는 이들을 하나의 프레임워크 내로 통합할 수 있음을 발견했습니다. 이 관점에서, 훈련과 샘플링의 설계 공간에 대해 자세히 조사하고, 성능과 효율에 기여하는 요인을 식별했습니다. 이 조사 중 얻은 개선을 기반으로, 우리 모델을 eMIGM(Enhanced Mixture of Implicit Generative Models)로 개발했습니다. 실험적으로는, eMIGM은 ImageNet 생성 성능에 강력한 효과를 보였습니다. 특히, ImageNet 256x256에서, 유사한 함수 평가 횟수(NFEs)와 모델 파라미터 수를 사용하여 eMIGM은 최신 VAR을 초과했습니다. NFE와 모델 파라미터 수가 증가함에 따라, eMIGM은 연속적인 拡散モデル과 같은 성능을 달성하지만, 40% 미만의 NFE를 필요로 합니다. 또한, ImageNet 512x512에서, 약 60%의 NFE로 eMIGM은 연속적인 拡散モデル을 초과했습니다.",
      "upvotes": 5,
      "discussionId": "67cfa76df36e4221c5009686",
      "ai_keywords": [
        "masked image generation models",
        "masked diffusion models",
        "training and sampling",
        "Fr\\'echet Inception Distance (FID)",
        "function evaluations (NFEs)",
        "VAR",
        "continuous diffusion models"
      ]
    },
    "publishedAt": "2025-03-10T07:27:12.000Z",
    "title": "Effective and Efficient Masked Image Generation Models",
    "summary": "Although masked image generation models and masked diffusion models are\ndesigned with different motivations and objectives, we observe that they can be\nunified within a single framework. Building upon this insight, we carefully\nexplore the design space of training and sampling, identifying key factors that\ncontribute to both performance and efficiency. Based on the improvements\nobserved during this exploration, we develop our model, referred to as eMIGM.\nEmpirically, eMIGM demonstrates strong performance on ImageNet generation, as\nmeasured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet\n256x256, with similar number of function evaluations (NFEs) and model\nparameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model\nparameters increase, eMIGM achieves performance comparable to the\nstate-of-the-art continuous diffusion models while requiring less than 40% of\nthe NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE,\neMIGM outperforms the state-of-the-art continuous diffusion models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07197.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06520",
      "authors": [
        {
          "_id": "67cf990ca80a73999cc816c3",
          "name": "Yuqi Liu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c4",
          "name": "Bohao Peng",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c5",
          "name": "Zhisheng Zhong",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c6",
          "name": "Zihao Yue",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c7",
          "name": "Fanbin Lu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c8",
          "name": "Bei Yu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c9",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T08:48:51.000Z",
      "title": "Seg-Zero: 인지 강화에 의한 이유 연결 가이드가 있는 분할",
      "summary": "전통적인 추론 분할 방법들은 분류 라벨과 간단한 설명을 사용하여 규범화된 미세 조정에 의존하며, 영역 외의 일반화 능력을 제한하고 명시적인 추론 프로세스를 부족하게 한다. 이러한 제한을 해결하기 위해, 우리는 Seg-Zero라는 새로운 프레임워크를 제안합니다. 이 프레임워크는 놀라운 일반화 능력을 보여주며, 인지 강화를 통해 명시적인 추론 체인을 얻을 수 있습니다. Seg-Zero는 이유 모델과 분할 모델로 구성된 디코더 로드 아키텍처를 도입하고 있습니다. 이유 모델은 사용자의 의도를 이해하고 명시적인 추론 체인을 생성하며, 다음으로 분할 모델은 픽셀 수준의 마스크를 생성하기 위해 위치 프론트 프로ン탤을 생성합니다. 우리는 형식과 정확도를 포함하는 복잡한 보상 구조를 설계하고, GRPO를 사용하여 독특하게 훈련하며, 명시적인 추론 데이터를 필요로 하지 않아, Seg-Zero는 강력한 0-shot의 일반화 능력을 보여주며, 검증 시의 추론 능력을 발견합니다. 실험은 Seg-Zero-7B는 Zero-shot 성능이 57.5가 되고, 지난주의 LISA-7B를 18% 초과하여 나타내며, 이 놀라운 향상은 Seg-Zero가 영역을 가로지르고 일반화하는 능력을 보여주며, 명시적인 추론 프로세스를 제공함을 보여줍니다. 코드는 https://github.com/dvlab-research/Seg-Zero에서 사용할 수 있습니다.",
      "upvotes": 5,
      "discussionId": "67cf990da80a73999cc81723",
      "ai_keywords": [
        "Seg-Zero",
        "decoupled architecture",
        "reasoning model",
        "segmentation model",
        "positional prompts",
        "pixel-level masks",
        "cognitive reinforcement",
        "reward mechanism",
        "reinforcement learning",
        "GRPO",
        "zero-shot generalization",
        "ReasonSeg benchmark",
        "emergent test-time reasoning"
      ]
    },
    "publishedAt": "2025-03-09T04:48:51.000Z",
    "title": "Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement",
    "summary": "Traditional methods for reasoning segmentation rely on supervised fine-tuning\nwith categorical labels and simple descriptions, limiting its out-of-domain\ngeneralization and lacking explicit reasoning processes. To address these\nlimitations, we propose Seg-Zero, a novel framework that demonstrates\nremarkable generalizability and derives explicit chain-of-thought reasoning\nthrough cognitive reinforcement. Seg-Zero introduces a decoupled architecture\nconsisting of a reasoning model and a segmentation model. The reasoning model\ninterprets user intentions, generates explicit reasoning chains, and produces\npositional prompts, which are subsequently used by the segmentation model to\ngenerate precious pixel-level masks. We design a sophisticated reward mechanism\nthat integrates both format and accuracy rewards to effectively guide\noptimization directions. Trained exclusively via reinforcement learning with\nGRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot\ngeneralization and exhibits emergent test-time reasoning capabilities.\nExperiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on\nthe ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant\nimprovement highlights Seg-Zero's ability to generalize across domains while\npresenting an explicit reasoning process. Code is available at\nhttps://github.com/dvlab-research/Seg-Zero.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06520.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06121",
      "authors": [
        {
          "_id": "67cfa436d37b8309603da1ee",
          "user": {
            "_id": "66de61d7174e9c6971dbb253",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sM0xfS7HAkf_6GmkEGjDk.png",
            "isPro": false,
            "fullname": "Alic Li",
            "user": "Alic-Li",
            "type": "user"
          },
          "name": "Li weile",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T04:29:57.207Z",
          "hidden": false
        },
        {
          "_id": "67cfa436d37b8309603da1ef",
          "user": {
            "_id": "6176b32847ee6431f632981e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg",
            "isPro": false,
            "fullname": "IvanD",
            "user": "xiaol",
            "type": "user"
          },
          "name": "Liu Xiao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T03:33:06.087Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-08T08:31:18.000Z",
      "title": "블라ックゴーストリマー：RWKV-7를 단순하고 뛰어난 Transformers의 대체로 사용하며, 큰 규모의 시간 시퀀스 모델링에 적합한 방법입니다.",
      "summary": "시계열 모델은 대규모의 복잡한 데이터셋을 처리하기 위해, 대규모 언어 모델(LLMs)와 같은 규모를 실현하기 위해 중대한 문제를 제기하고 있습니다. 시계열 데이터의 특성과 모델 규모의 계산 요구에 따라, 새로운 접근법이 필요하게 됩니다. 연구자들은 Transformers, LSTMs, GRUs 등 다양한 아키텍처를 사용하여 이러한 문제를 해결하기 위해 조사를 수행했지만, RWKV-7을 사용하여 메타 학습을 도입한 새로운 해결책을 제안합니다. RWKV-7의 시간 미克斯와 채널 미克斯 컴포넌트를 Transformer 기반의 시계열 모델 Timer에 통합하여, 1/23의 파라미터 수를 사용하여 약 1.13배에서 43.3배의 성능 향상과 훈련 시간의 4.5배 감소를 실현했습니다. 우리 코드와 모델 가중치는 https://github.com/Alic-Li/BlackGoose_Rimer에서 공개되어 있으며, 발전하는 연구와 개발에 제공될 수 있습니다.",
      "upvotes": 5,
      "discussionId": "67cfa437d37b8309603da253",
      "ai_keywords": [
        "Transformers",
        "LSTMs",
        "GRUs",
        "RWKV-7",
        "meta-learning",
        "state update mechanism",
        "time mix",
        "channel mix",
        "Timer",
        "performance improvement",
        "training time",
        "parameters"
      ]
    },
    "publishedAt": "2025-03-08T03:31:18.000Z",
    "title": "BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement\n  for Transformers in Large-Scale Time Series Modeling",
    "summary": "Time series models face significant challenges in scaling to handle large and\ncomplex datasets, akin to the scaling achieved by large language models (LLMs).\nThe unique characteristics of time series data and the computational demands of\nmodel scaling necessitate innovative approaches. While researchers have\nexplored various architectures such as Transformers, LSTMs, and GRUs to address\nthese challenges, we propose a novel solution using RWKV-7, which incorporates\nmeta-learning into its state update mechanism. By integrating RWKV-7's time mix\nand channel mix components into the transformer-based time series model Timer,\nwe achieve a substantial performance improvement of approximately 1.13 to 43.3x\nand a 4.5x reduction in training time with 1/23 parameters, all while utilizing\nfewer parameters. Our code and model weights are publicly available for further\nresearch and development at https://github.com/Alic-Li/BlackGoose_Rimer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06121.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03499",
      "authors": [
        {
          "_id": "67cb02680a2a716f25805cb4",
          "name": "Wonjun Kang",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb5",
          "name": "Kevin Galim",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb6",
          "name": "Yuchen Zeng",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb7",
          "name": "Minjae Lee",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb8",
          "name": "Hyung Il Koo",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb9",
          "name": "Nam Ik Cho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T13:44:42.000Z",
      "title": "상태 오프셋 튜닝: 상태 기반 파라미터 효율적인 미세 조정을 위한 상태 공간 모델",
      "summary": "ステートスペースモデル（SSMs）는 Transformers의 두 번째 계산 비용을 줄이기 위해 더 효율적인 대체 방법이 되었습니다. 그러나 SSMs에 대한 Parameter-Efficient Fine-Tuning（PEFT）메소드의 적용은 크게 조사되지 않았습니다. 특히, Transformers에서 광범위하게 사용되는 Prompt Tuning과 Prefix-Tuning은 SSMs에서 우수한 성능을 보여주지 못합니다. 이에대해, 우리는 Prompt Tuning에 대한 좋은 대체 방법이 될 수 있는 상태 기반 방법을 제안합니다. 이 새로운 메소드의 가족은 SSMs의 구조적 특성에서 자연스럽게 생겨났습니다. 상태 기반 방법은 외부 Prompt에 의존하지 않고, 직접 상태 관련 특성을 조정합니다. 또한, 우리는 새로운 상태 기반의 PEFT 메소드인 State-offset Tuning을 소개합니다. 시간 단계별로, 우리의 방법은 현재 단계의 상태를 직접 영향을 미칩니다. 다양한 데이터 세트를 구성한 실험을 통해, 우리의 방법의 효율성을 입증합니다. 코드는 https://github.com/furiosa-ai/ssm-state-tuning에 접근할 수 있습니다.",
      "upvotes": 3,
      "discussionId": "67cb02690a2a716f25805cfd",
      "githubRepo": "https://github.com/furiosa-ai/ssm-state-tuning",
      "ai_keywords": [
        "State Space Models (SSMs)",
        "Parameter-Efficient Fine-Tuning (PEFT)",
        "Prompt Tuning",
        "Prefix-Tuning",
        "State-based methods",
        "State-offset Tuning",
        "timesteps",
        "state-related features",
        "state-at-the-current-step"
      ]
    },
    "publishedAt": "2025-03-05T08:44:42.000Z",
    "title": "State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models",
    "summary": "State Space Models (SSMs) have emerged as efficient alternatives to\nTransformers, mitigating their quadratic computational cost. However, the\napplication of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains\nlargely unexplored. In particular, prompt-based methods like Prompt Tuning and\nPrefix-Tuning, which are widely used in Transformers, do not perform well on\nSSMs. To address this, we propose state-based methods as a superior alternative\nto prompt-based methods. This new family of methods naturally stems from the\narchitectural characteristics of SSMs. State-based methods adjust state-related\nfeatures directly instead of depending on external prompts. Furthermore, we\nintroduce a novel state-based PEFT method: State-offset Tuning. At every\ntimestep, our method directly affects the state at the current step, leading to\nmore effective adaptation. Through extensive experiments across diverse\ndatasets, we demonstrate the effectiveness of our method. Code is available at\nhttps://github.com/furiosa-ai/ssm-state-tuning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03499.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07603",
      "authors": [
        {
          "_id": "67cfc310f2b1fe815dc24ebf",
          "name": "Sedrick Keh",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec0",
          "name": "Jean Mercat",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec1",
          "name": "Samir Yitzhak Gadre",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec2",
          "name": "Kushal Arora",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec3",
          "name": "Igor Vasiljevic",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec4",
          "name": "Benjamin Burchfiel",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec5",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec6",
          "name": "Russ Tedrake",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec7",
          "name": "Thomas Kollar",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec8",
          "name": "Ludwig Schmidt",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec9",
          "name": "Achal Dave",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:19.000Z",
      "title": "VLMs는 이미지 데이터를 사전 훈련해야 합니다?",
      "summary": "전처리된 LLMs은 이미지 데이터와의进一步的 학습을 통해 시각 언어 태스크에서 우수한 성능을 나타냅니다. 두 번째 학습 단계에서 이미지를 추가함으로써 이러한 능력을 효과적으로 발휘할 수 있지만, 이 2 단계 파이프라인이 이미지의 빠른 통합을 수행한 VLMs과 비교하여 어떤 효과를 보였는지, 어떤 손실이 발생했는지는 명확하지 않습니다. 이를 조사하기 위해, 데이터셋, 규모, 이미지 텍스트 비율, 전처리를 수행한 양을 범위로 제한하여 모델을 학습하고, 이후 이러한 모델을 미세 조정하여 시각 언어 태스크와 텍스트만 있는 태스크의 하위 성능을 평가합니다. 그리고 이미지와 텍스트 데이터의 혼합으로 전처리를 수행한 모델은 텍스트만 평가에서도 강력한 성능을 유지하면서, 시각 언어 태스크에서 더 좋은 성능을 나타냅니다. 6가지 다양한 태스크의 평균값에서, 1B 모델에서 전처리의 80% 단계에서 이미지 토큰을 추가함으로써 전처리된 모델에 이미지 토큰을 추가하는 데 따른 평균적인 개선율은 2%입니다.",
      "upvotes": 2,
      "discussionId": "67cfc314f2b1fe815dc24fe3",
      "ai_keywords": [
        "pre-trained LLMs",
        "vision-language tasks",
        "fine-tune",
        "vision tokens"
      ]
    },
    "publishedAt": "2025-03-10T13:58:19.000Z",
    "title": "Should VLMs be Pre-trained with Image Data?",
    "summary": "Pre-trained LLMs that are further trained with image data perform well on\nvision-language tasks. While adding images during a second training phase\neffectively unlocks this capability, it is unclear how much of a gain or loss\nthis two-step pipeline gives over VLMs which integrate images earlier into the\ntraining process. To investigate this, we train models spanning various\ndatasets, scales, image-text ratios, and amount of pre-training done before\nintroducing vision tokens. We then fine-tune these models and evaluate their\ndownstream performance on a suite of vision-language and text-only tasks. We\nfind that pre-training with a mixture of image and text data allows models to\nperform better on vision-language tasks while maintaining strong performance on\ntext-only evaluations. On an average of 6 diverse tasks, we find that for a 1B\nmodel, introducing visual tokens 80% of the way through pre-training results in\na 2% average improvement over introducing visual tokens to a fully pre-trained\nmodel.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07603.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06885",
      "authors": [
        {
          "_id": "67cfc1c5182d970d40896a5e",
          "user": {
            "_id": "655b813476e4fad5529f3256",
            "avatarUrl": "/avatars/73d83e45d921531f9830a0ea80f76491.svg",
            "isPro": false,
            "fullname": "Yan Yang",
            "user": "HelloKKMe",
            "type": "user"
          },
          "name": "Yan Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:28.574Z",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a5f",
          "user": {
            "_id": "6357362f811ee2fa05070f64",
            "avatarUrl": "/avatars/2cf37efb80f5cfb3e4e9d08674de6dd1.svg",
            "isPro": false,
            "fullname": "Dongxu Li",
            "user": "dxli1",
            "type": "user"
          },
          "name": "Dongxu Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T04:53:27.837Z",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a60",
          "name": "Haoning Wu",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a61",
          "name": "Bei Chen",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a62",
          "name": "Liu Liu",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a63",
          "name": "Liyuan Pan",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a64",
          "name": "Junnan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T03:29:18.000Z",
      "title": "ProBench: 오픈 종목의 다모달기반 모델의 평가에 대한 다 분야 전문가 태스크에서의 평가",
      "summary": "전문사 수준의 다모달 태스크는 일반적인 지능의 핵심 마크스입니다. 다모달 대 언어 모델(MLLMs)의 능력이 지속적으로 향상되는 가운데, 그 발전된 다모달 지능을 평가하는 것이 필요하고, 어려워집니다. 본 논문에서는, 전문 지식과 발전된 이유를 필요로 하는 개방적인 사용자 쿼리의 벤치마크를 통해 ProBench를 소개합니다. ProBench는 10 분야와 56 서브 분야의 광범위한 범위에서, 과학, 예술, 인문학, 코딩, 수학, 그리고 창의적인 책에 이르륵, 광범위한 범위를 커버합니다. 실험적으로, 24개의 최신 모델을 MLLM-as-a-Judge를 사용하여 평가하고 비교했습니다. 결과적으로, 오픈 소스 모델의 가장 좋은 것은 프로프라이어 모델과 대결 중이지만, ProBench는 시각적 지식, 문맥의 이해, 분야의 지식, 발전된 이유에 대해 중요한 문제를 제시하고, 향후 다모달 AI 연구의 방향을 제공합니다.",
      "upvotes": 2,
      "discussionId": "67cfc1c7182d970d40896b1d",
      "projectPage": "https://yan98.github.io/ProBench/",
      "githubRepo": "https://github.com/Yan98/ProBench_eval",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "benchmark",
        "user queries",
        "professional expertise",
        "advanced reasoning",
        "high-quality samples",
        "daily productivity demands",
        "fields",
        "sub-fields",
        "visual perception",
        "textual understanding",
        "domain knowledge"
      ]
    },
    "publishedAt": "2025-03-09T23:29:18.000Z",
    "title": "ProBench: Judging Multimodal Foundation Models on Open-ended\n  Multi-domain Expert Tasks",
    "summary": "Solving expert-level multimodal tasks is a key milestone towards general\nintelligence. As the capabilities of multimodal large language models (MLLMs)\ncontinue to improve, evaluation of such advanced multimodal intelligence\nbecomes necessary yet challenging. In this work, we introduce ProBench, a\nbenchmark of open-ended user queries that require professional expertise and\nadvanced reasoning. ProBench consists of 4,000 high-quality samples\nindependently submitted by professionals based on their daily productivity\ndemands. It spans across 10 fields and 56 sub-fields, including science, arts,\nhumanities, coding, mathematics, and creative writing. Experimentally, we\nevaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal\nthat although the best open-source models rival the proprietary ones, ProBench\npresents significant challenges in visual perception, textual understanding,\ndomain knowledge and advanced reasoning, thus providing valuable directions for\nfuture multimodal AI research efforts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06885.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02199",
      "authors": [
        {
          "_id": "67c90dad6f3ef3c2c77689b0",
          "name": "Ailin Deng",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b1",
          "name": "Tri Cao",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b2",
          "user": {
            "_id": "67cbb6ea2cc05acaab023f75",
            "avatarUrl": "/avatars/79272c8889a8c472cf75172ead72daea.svg",
            "isPro": false,
            "fullname": "Zhirui Chen",
            "user": "ryanchen42",
            "type": "user"
          },
          "name": "Zhirui Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:03:34.042Z",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b3",
          "name": "Bryan Hooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:21:07.000Z",
      "title": "Words or Vision: 언어와 시각: 언어를 믿는 것인가, 시각을 믿는 것인가?",
      "summary": "비슷한 이미지 데이터와 다양한 문맥 입력을 위한 Vision-Language Models(VLMs)는 이미지와 문맥의 정보를 통합하는 데 뛰어난 성능을 보입니다. 그러나 모델 간 비대칭성 균형을 조사한 연구는 부족한 것으로 알려져 있습니다. 이미지 데이터와 다양한 문맥 입력을 위한 VLMs의 모델 간 선호도를 조사하기 위해, 4가지의 비슷한 이미지 작업에서 문맥의 변화를 적용하여 10가지의 Vision-Language Models(VLMs)를 평가했습니다. \"문맥에 무시\" 현상을 발견했습니다: VLMs은 비대칭성이 존재할 때, 이미지 데이터보다 문맥 데이터에 과도한 신뢰를 기울이며, 오염된 문맥에 의한 성능 저하와 안전성 의문이 발생합니다. 문맥의 편향을 유발하는 요인을 분석했습니다. 명령 계획, 언어 모델의 크기, 문맥의 관련성, 토큰의 순서, 이미지와 문맥의 신뢰도 상호작용 등이 포함됩니다. 언어 모델의 위치 편향을 유발하기 때문에, 토큰의 순서 등 요인은 문맥의 편향을 악화시킬 수 있습니다. 이 문제를 해결하기 위해, 문맥의 확장과 슈퍼바이다스 최종 훈련을 시도하고, 그 효과를 보여주었습니다. 또한 이론적인 분석을 제공하여, 문맥에 무시 현상은 훈련 중의 단순한 문맥과 다양한 모델 데이터의 불균형을 원인으로 보입니다. 이러한 발견은 VLMs에서 모델 간 상호작용의 조정과 훈련의 균형의 중요성을 강조하며, 다양한 모델 데이터의 비대칭성을 처리함으로써 강건성과 신뢰성을 향상시키기에 필요합니다.",
      "upvotes": 2,
      "discussionId": "67c90dae6f3ef3c2c77689ec",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "blind faith in text",
        "modality preferences",
        "textual variations",
        "vision-centric tasks",
        "text bias",
        "instruction prompts",
        "language model size",
        "token order",
        "positional biases",
        "multi-modal data",
        "supervised fine-tuning",
        "text augmentation",
        "balanced training",
        "modality interactions"
      ]
    },
    "publishedAt": "2025-03-03T21:21:07.000Z",
    "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
    "summary": "Vision-Language Models (VLMs) excel in integrating visual and textual\ninformation for vision-centric tasks, but their handling of inconsistencies\nbetween modalities is underexplored. We investigate VLMs' modality preferences\nwhen faced with visual data and varied textual inputs in vision-centered\nsettings. By introducing textual variations to four vision-centric tasks and\nevaluating ten Vision-Language Models (VLMs), we discover a ``blind faith\nin text'' phenomenon: VLMs disproportionately trust textual data over visual\ndata when inconsistencies arise, leading to significant performance drops under\ncorrupted text and raising safety concerns. We analyze factors influencing this\ntext bias, including instruction prompts, language model size, text relevance,\ntoken order, and the interplay between visual and textual certainty. While\ncertain factors, such as scaling up the language model size, slightly mitigate\ntext bias, others like token order can exacerbate it due to positional biases\ninherited from language models. To address this issue, we explore supervised\nfine-tuning with text augmentation and demonstrate its effectiveness in\nreducing text bias. Additionally, we provide a theoretical analysis suggesting\nthat the blind faith in text phenomenon may stem from an imbalance of pure text\nand multi-modal data during training. Our findings highlight the need for\nbalanced training and careful consideration of modality interactions in VLMs to\nenhance their robustness and reliability in handling multi-modal data\ninconsistencies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02199.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07595",
      "authors": [
        {
          "_id": "67cfa440aff9c98bb3f45a56",
          "user": {
            "_id": "64b3fc1fa24816979609dcb3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b3fc1fa24816979609dcb3/cHRMs4YegRcgbZO8_bBaZ.jpeg",
            "isPro": false,
            "fullname": "Sinclair Schneider",
            "user": "SinclairSchneider",
            "type": "user"
          },
          "name": "Sinclair Schneider",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T02:48:03.261Z",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a57",
          "name": "Florian Steuber",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a58",
          "name": "Joao A. G. Schneider",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a59",
          "name": "Gabi Dreo Rodosek",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:56:25.000Z",
      "title": "대 언어 모뎀의 감지 회피 기술",
      "summary": "대규모 언어 모델의 확산은 광범위한 사용에 이어졌지만, 다른 위험도 증가하고 있으며, 사실의 확산을 가능하게 하는 등 여러 문제를 야기하고 있습니다. 이에 따라 DetectGPT와 같은 분류 시스템의 개발이 중요해졌습니다. 이러한 Detector는 실험 시리즈에서 보여준 것처럼, 에바지온 기술에 취약합니다. 생성 모델의 온도의 체계적인 변경으로, 얕은 학습 Detector가 가장 신뢰도가 낮은 것으로 되었습니다. 재강化学습으로 생성 모델을 미세 조정하면, BERT 기반 Detector를 피할 수 있었습니다. 마지막으로, 재구성은 DetectGPT와 같은 0-shot Detector에 대해 90% 이상의 에바지온을 실현했지만, 문장은 원래의 것과 고차적으로 유사했습니다. 기존 연구와 비교하여, 제시된 방법의 더 좋은 성능이 명확히 드러났습니다. 사회에 미치는 영향과 연구의 가능성에 대한 논의도 진행되고 있습니다.",
      "upvotes": 1,
      "discussionId": "67cfa441aff9c98bb3f45a95",
      "ai_keywords": [
        "large language models",
        "fake news",
        "classification systems",
        "DetectGPT",
        "evasion techniques",
        "generative models",
        "temperature",
        "shallow learning-detectors",
        "fine-tuning",
        "reinforcement learning",
        "BERT-based-detectors",
        "zero-shot-detectors",
        "rephrasing"
      ]
    },
    "publishedAt": "2025-03-10T13:56:25.000Z",
    "title": "Detection Avoidance Techniques for Large Language Models",
    "summary": "The increasing popularity of large language models has not only led to\nwidespread use but has also brought various risks, including the potential for\nsystematically spreading fake news. Consequently, the development of\nclassification systems such as DetectGPT has become vital. These detectors are\nvulnerable to evasion techniques, as demonstrated in an experimental series:\nSystematic changes of the generative models' temperature proofed shallow\nlearning-detectors to be the least reliable. Fine-tuning the generative model\nvia reinforcement learning circumvented BERT-based-detectors. Finally,\nrephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT,\nalthough texts stayed highly similar to the original. A comparison with\nexisting work highlights the better performance of the presented methods.\nPossible implications for society and further research are discussed.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07595.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07465",
      "authors": [
        {
          "_id": "67cfaaed7f229132171f596b",
          "name": "Ao Wang",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596c",
          "name": "Lihao Liu",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596d",
          "name": "Hui Chen",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596e",
          "name": "Zijia Lin",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596f",
          "name": "Jungong Han",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f5970",
          "name": "Guiguang Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T15:42:59.000Z",
      "title": "YOLOE: 시간 효율적인, 그 것을 볼 수 있는\n\n(Note: The translation provided is a direct literal translation. For a more natural and contextually appropriate translation, consider the following: \"YOLOE: 시간 효율적인, 그 것을 쉽게 확인할 수 있는\")",
      "summary": "물체 검출과 분할은 컴퓨터 비전 애플리케이션에서 광범위하게 사용되고 있으나, YOLO 시리즈와 같은 전통적인 모델은 결정적인 카테고리에 의해 제한되어 개방적 시나리오에서의 적응성이 저하됩니다. 최근의 개방 세트 방법은 텍스트 프롬프트, 시각 커트, 또는 프롬프트 없는 패러다임을 활용하여 이러한 문제를 극복하고 있지만, 높은 계산량 요구와 배치 복잡성에 의해 성능과 효율성에 대한 손실이 발생합니다. 본 논문에서는 YOLOE라는 높은 효율성을 가진 모델 내부에서 다양한 개방 프롬프트 구조를 활용하여 물체 검출과 분할을 통합하고 실시간으로 볼 수 있는 방법을 제안합니다. 텍스트 프롬프트에 대해 Re-parameterizable Region-Text Alignment (RepRTA) 스테라타지제를 제안합니다. 이는 재파라미터 가능한 가벼운 보조 네트워크를 활용하여 사전 학습된 텍스트 엔코딩을 정밀화하고, 시각 텍스트 어레이먼트를 강화합니다. 시각 프롬프트에 대해 Semantic-Activated Visual Prompt Encoder (SAVPE)를 제시합니다. 이는 분리된 의미와 활성 베르시온을 활용하여 최소한의 복잡성을 가진 개선된 시각 엔코딩과 정확도를 구현합니다. 프롬프트 없는 시나리오에 대해 Lazy Region-Prompt Contrast (LRPC) 스테라지제를 제시합니다. 이는 구축된 큰 vocabulary와 전문적인 엔코딩을 활용하여 모든 물체를 인식할 수 있도록 합니다. 확장된 실험은 YOLOE의 특별한 0샷 성능, 높은 추론 효율성과 낮은 훈련 비용에 대해 보여줍니다. 특히, LVIS에서 3배 적은 훈련 비용과 1.4배의 추론 속도 업그레이드를 설정하고, YOLOE-v8-S는 YOLO-Worldv2-S를 3.5AP 초과합니다. COCO에서 YOLOE-v8-L은 폐쇄 세트의 YOLOv8-L보다 약 4배 적은 훈련 시간으로 0.6AP^b와 0.4AP^m의 효과를 보입니다. 코드와 모델은 https://github.com/THU-MIG/yoloe에 제공됩니다.",
      "upvotes": 0,
      "discussionId": "67cfaaf27f229132171f5ab4",
      "ai_keywords": [
        "Object detection",
        "Segmentation",
        "YOLO series",
        "Open-set methods",
        "Text prompts",
        "Visual cues",
        "Prompt-free paradigm",
        "YOLOE",
        "Rep-parameterizable Region-Text Alignment (RepRTA)",
        "Pretrained textual embeddings",
        "Re-parameterizable lightweight auxiliary network",
        "Semantic-Activated Visual Prompt Encoder (SAVPE)",
        "Decoupled semantic and activation branches",
        "Visual embedding",
        "Lazy Region-Prompt Contrast (LRPC)",
        "Large vocabulary",
        "Specialized embedding",
        "LVIS",
        "Zero-shot performance",
        "Transferability",
        "Inference efficiency",
        "Training cost",
        "AP",
        "COCO",
        "Closed-set YOLOv8-L",
        "Inference speedup",
        "Training time"
      ]
    },
    "publishedAt": "2025-03-10T11:42:59.000Z",
    "title": "YOLOE: Real-Time Seeing Anything",
    "summary": "Object detection and segmentation are widely employed in computer vision\napplications, yet conventional models like YOLO series, while efficient and\naccurate, are limited by predefined categories, hindering adaptability in open\nscenarios. Recent open-set methods leverage text prompts, visual cues, or\nprompt-free paradigm to overcome this, but often compromise between performance\nand efficiency due to high computational demands or deployment complexity. In\nthis work, we introduce YOLOE, which integrates detection and segmentation\nacross diverse open prompt mechanisms within a single highly efficient model,\nachieving real-time seeing anything. For text prompts, we propose\nRe-parameterizable Region-Text Alignment (RepRTA) strategy. It refines\npretrained textual embeddings via a re-parameterizable lightweight auxiliary\nnetwork and enhances visual-textual alignment with zero inference and\ntransferring overhead. For visual prompts, we present Semantic-Activated Visual\nPrompt Encoder (SAVPE). It employs decoupled semantic and activation branches\nto bring improved visual embedding and accuracy with minimal complexity. For\nprompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy.\nIt utilizes a built-in large vocabulary and specialized embedding to identify\nall objects, avoiding costly language model dependency. Extensive experiments\nshow YOLOE's exceptional zero-shot performance and transferability with high\ninference efficiency and low training cost. Notably, on LVIS, with 3times\nless training cost and 1.4times inference speedup, YOLOE-v8-S surpasses\nYOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6\nAP^b and 0.4 AP^m gains over closed-set YOLOv8-L with nearly 4times less\ntraining time. Code and models are available at\nhttps://github.com/THU-MIG/yoloe.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07465.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07426",
      "authors": [
        {
          "_id": "67cff321f2b1fe815dce3722",
          "name": "Junkang Wu",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3723",
          "name": "Kexin Huang",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3724",
          "name": "Xue Wang",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3725",
          "name": "Jinyang Gao",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3726",
          "name": "Bolin Ding",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3727",
          "name": "Jiancan Wu",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3728",
          "name": "Xiangnan He",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3729",
          "user": {
            "_id": "65fca775fa59bdf4737b1a84",
            "avatarUrl": "/avatars/a161b510bde8f57e7686cbb0b4aa6a52.svg",
            "isPro": false,
            "fullname": "Xiang Wang",
            "user": "xiangwang1223",
            "type": "user"
          },
          "name": "Xiang Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T08:24:02.839Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T15:11:07.000Z",
      "title": "RePO: ReLU 기반의 선호도 최적화",
      "summary": "LLM의 인간적인 성향을 맞추는 것은 현실적인 기능에 있어서 중요하지만, 현재의 방법들은 계산과 안정성 문제를 가지고 있습니다. DPO는 단일 파라미터 beta를 사용하여 오프라인 패러다임을 구축하고 있지만, SimPO와 같은 후속 방법들은 복잡성을 다시 도입하고, 이중 파라미터 (beta, gamma)를 사용합니다. 우리는 ReLU 기반의 성향 최적화 (RePO)의 스트리밍 알고리즘을 제안합니다. RePO는 (1) SimPO의 참조없이의 마르지널을 유지하면서, 그래픽 분석에 의해 beta를 제거하고, (2) ReLU 기반의 max-margin 손실을 도입하여 자연스럽게 가벼운 파서 쌍을 필터링함으로써 beta를 제거합니다. 이론적으로는 RePO는 SimPO의 극한값으로 특징이 되고, 로지스틱의 가중치가 이진 임계값으로 수렴하고, 0-1 손실의 Convex hull를 형성합니다. AlpacaEval 2와 Arena-Hard의 실험 결과를 통해 RePO는 여러 기반 모델에서 DPO와 SimPO를 초과하고, 하나의 파라미터 조정만 필요함을 시사합니다.",
      "upvotes": 0,
      "discussionId": "67cff322f2b1fe815dce3787",
      "ai_keywords": [
        "LLMS",
        "RLHF",
        "DPO",
        "SimPO",
        "ReLU-based Preference Optimization (RePO)",
        "reference-free margins",
        "gradient analysis",
        "ReLU-based max-margin loss",
        "convex envelope",
        "0-1 loss"
      ]
    },
    "publishedAt": "2025-03-10T11:11:07.000Z",
    "title": "RePO: ReLU-based Preference Optimization",
    "summary": "Aligning large language models (LLMs) with human preferences is critical for\nreal-world deployment, yet existing methods like RLHF face computational and\nstability challenges. While DPO establishes an offline paradigm with single\nhyperparameter beta, subsequent methods like SimPO reintroduce complexity\nthrough dual parameters (beta, gamma). We propose {ReLU-based Preference\nOptimization (RePO)}, a streamlined algorithm that eliminates beta via two\nadvances: (1) retaining SimPO's reference-free margins but removing beta\nthrough gradient analysis, and (2) adopting a ReLU-based max-margin loss that\nnaturally filters trivial pairs. Theoretically, RePO is characterized as\nSimPO's limiting case (beta to infty), where the logistic weighting\ncollapses to binary thresholding, forming a convex envelope of the 0-1 loss.\nEmpirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO\nand SimPO across multiple base models, requiring only one hyperparameter to\ntune.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07426.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06362",
      "authors": [
        {
          "_id": "67cffd119f703990a8e25925",
          "name": "Umberto Cappellazzo",
          "hidden": false
        },
        {
          "_id": "67cffd119f703990a8e25926",
          "name": "Minsu Kim",
          "hidden": false
        },
        {
          "_id": "67cffd119f703990a8e25927",
          "name": "Stavros Petridis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T00:02:10.000Z",
      "title": "Adaptive Audio-Visual Speech Recognition with Matrizon-Based Monomodal LLMs",
      "summary": "음성 시각 인식(AVSR)는 음성과 시각의 두 가지 모델을 활용하여, 노이즈가 많은 환경에서 음성 인식의 강건성을 향상시키는 기술입니다. 대규모 언어 모델(LLMs)의 최근 발전은 음성 인식에서도 효과적이며, AVSR에서도 마찬가지입니다. 그러나 음성 표현의 길이가 길기 때문에, LLMs와 직접적인 통합은 계산 비용이 크게 듭니다. 기존의 접근법은 이러한 문제를 해결하기 위해, LLMs에 입력하기 전에 음성 표현을 압축하는 방법을 사용합니다. 그러나 높은 압축 비율이 성능 저하를招き、계산 비용과 인식 정확도의 트레이드오프가 필요로 합니다. 이러한 도전을 해결하기 위해, 우리는 AVSR의 첫 번째 MATRIO-SK 기반의 다모달 LLM, Llama-MTSK를 제안합니다. 이것은 특정 계산 제약에 따라 음성 시각 토큰의 할당을 유연하게 조정할 수 있는 방식으로, 높은 성능을 유지합니다. 우리의 접근법은 MATRIO-SK 표현 학습을 모델로, 모델 내부에서 멀티Granularity로 음성 시각 표현을 인코딩하고, 다른 압축 수준에 대응하는 다양한 모델을 훈련하는 필요성을 제거합니다. 또한, LLM의 효율적인 미세 조정에 대해, 글로벌 및 스케일 고유의 LoRA 모듈을 사용한 3가지의 LoRA 기반의 MATRIO-SK 전략을 도입합니다. 두 개의 최대 AVSR 데이터 세트에서의 검증은 Llama-MTSK가 가장 先端의 결과를 구현하고, 고정된 압축 수준에서 독립적으로 훈련된 모델의 결과를 초월함을 보여줍니다.",
      "upvotes": 0,
      "discussionId": "67cffd129f703990a8e25990",
      "ai_keywords": [
        "Audio-Visual Speech Recognition (AVSR)",
        "Large Language Models (LLMs)",
        "speech representations",
        "computational costs",
        "audio-visual token allocation",
        "Matryoshka-based Multimodal LLM",
        "Matryoshka Representation Learning",
        "global LoRA modules",
        "scale-specific LoRA modules",
        "LoRA-based Matryoshka strategies"
      ]
    },
    "publishedAt": "2025-03-08T19:02:10.000Z",
    "title": "Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs",
    "summary": "Audio-Visual Speech Recognition (AVSR) leverages both audio and visual\nmodalities to enhance speech recognition robustness, particularly in noisy\nenvironments. Recent advancements in Large Language Models (LLMs) have\ndemonstrated their effectiveness in speech recognition, including AVSR.\nHowever, due to the significant length of speech representations, direct\nintegration with LLMs imposes substantial computational costs. Prior approaches\naddress this by compressing speech representations before feeding them into\nLLMs. However, higher compression ratios often lead to performance degradation,\nnecessitating a trade-off between computational efficiency and recognition\naccuracy. To address this challenge, we propose Llama-MTSK, the first\nMatryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation of\nthe audio-visual token allocation based on specific computational constraints\nwhile preserving high performance. Our approach, inspired by Matryoshka\nRepresentation Learning, encodes audio-visual representations at multiple\ngranularities within a single model, eliminating the need to train separate\nmodels for different compression levels. Moreover, to efficiently fine-tune the\nLLM, we introduce three LoRA-based Matryoshka strategies using global and\nscale-specific LoRA modules. Extensive evaluations on the two largest AVSR\ndatasets demonstrate that Llama-MTSK achieves state-of-the-art results,\nmatching or surpassing models trained independently at fixed compression\nlevels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06362.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05283",
      "authors": [
        {
          "_id": "67cef721e5ab8ec0550b7a66",
          "name": "Souhail Hadgi",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a67",
          "name": "Luca Moschella",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a68",
          "user": {
            "_id": "5e8ef1f14957053f606489e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
            "isPro": false,
            "fullname": "Andrea Santilli",
            "user": "teelinsan",
            "type": "user"
          },
          "name": "Andrea Santilli",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T14:35:44.397Z",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a69",
          "name": "Diego Gomez",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6a",
          "name": "Qixing Huang",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6b",
          "name": "Emanuele Rodolà",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6c",
          "name": "Simone Melzi",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6d",
          "name": "Maks Ovsjanikov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T09:51:56.000Z",
      "title": "3D와 텍스트 잠재 공간의 어레이멘션으로 탈출한 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은 낡은",
      "summary": "최근의 연구는, 규모에 따른 훈련을 수행할 때, 단일모달 2D 시각 엔코더와 텍스트 엔코더가 학습된 특징은, 다른 표현으로부터 발생하는 것을 무관하게, 놀라운 구조적인 특징을 공유하는 것을 보여줍니다. 그러나, 3D 엔코더가 다른 모달에 대한 역할은 조사되지 않았습니다. 또한, 현재의 3D Fundamental 모델은, 큰 데이터 세트를 사용하여 훈련되어 있지만, 일반적으로, 다른 표현으로부터 자유로워진 엔코더와 명시적인 alignment 객체에 기반하여 훈련되어 있습니다. 본 연구에서는, 단일모달 3D 엔코더에서 얻은 표현과 텍스트 기반의 특징 공간과 후처리 alignment의 가능성에 대한 조사를 수행합니다. 나ïve의 후처리 훈련 특징 alignment에 의한 단일모달 텍스트와 3D 엔코더의 성능은 제한되어 있습니다. 그 후, 대응하는 특징 공간의 부분 공간을 추출하고, 학습된 표현을 선택된 저차원 부분 공간에 투영하여 alignment의 품질이 크게 향상되고, 매칭과 검색 태스크에서 정확도가 향상됩니다. 우리의 분석은, 이러한 공유 부분 공간의 특성을 밝혀, 그들은 주로 세ман틱 및 기오메트리 데이터 표현과 구분되는 것을 보여줍니다. 전체적으로, 우리의 연구는, 후처리 훈련의 3D 단일모달과 텍스트 특징 공간의 alignment의 기준을 확립하는 것을 시도하고, 3D 데이터와 다른 표현과 비교하여 공유되는 일반적인 특징과 다른 특징을 밝혀냅니다.",
      "upvotes": 0,
      "discussionId": "67cef723e5ab8ec0550b7ac8",
      "ai_keywords": [
        "uni-modal 2D vision",
        "text encoders",
        "learned features",
        "3D encoders",
        "3D foundation models",
        "alignment objectives",
        "feature alignment",
        "subspaces",
        "lower-dimensional subspaces",
        "semantic data representations",
        "geometric data representations",
        "matching tasks",
        "retrieval tasks"
      ]
    },
    "publishedAt": "2025-03-07T04:51:56.000Z",
    "title": "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent\n  Spaces",
    "summary": "Recent works have shown that, when trained at scale, uni-modal 2D vision and\ntext encoders converge to learned features that share remarkable structural\nproperties, despite arising from different representations. However, the role\nof 3D encoders with respect to other modalities remains unexplored.\nFurthermore, existing 3D foundation models that leverage large datasets are\ntypically trained with explicit alignment objectives with respect to frozen\nencoders from other representations. In this work, we investigate the\npossibility of a posteriori alignment of representations obtained from\nuni-modal 3D encoders compared to text-based feature spaces. We show that naive\npost-training feature alignment of uni-modal text and 3D encoders results in\nlimited performance. We then focus on extracting subspaces of the corresponding\nfeature spaces and discover that by projecting learned representations onto\nwell-chosen lower-dimensional subspaces the quality of alignment becomes\nsignificantly higher, leading to improved accuracy on matching and retrieval\ntasks. Our analysis further sheds light on the nature of these shared\nsubspaces, which roughly separate between semantic and geometric data\nrepresentations. Overall, ours is the first work that helps to establish a\nbaseline for post-training alignment of 3D uni-modal and text feature spaces,\nand helps to highlight both the shared and unique properties of 3D data\ncompared to other representations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05283.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03511",
      "authors": [
        {
          "_id": "67cd7ace999766d8cd73fb18",
          "user": {
            "_id": "6732f2c24c2f18a60e76b915",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6732f2c24c2f18a60e76b915/W6oozAjM-zu7E3SL9uQ97.jpeg",
            "isPro": false,
            "fullname": "Fan",
            "user": "KianYale",
            "type": "user"
          },
          "name": "Qingyu Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:41.233Z",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb19",
          "name": "Yinghao Cai",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1a",
          "name": "Chao Li",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1b",
          "name": "Wenzhe He",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1c",
          "name": "Xudong Zheng",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1d",
          "name": "Tao Lu",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1e",
          "name": "Bin Liang",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1f",
          "name": "Shuo Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T13:57:37.000Z",
      "title": "NeuGrasp: 背景 プロフィール을 활용한 일반화 가능한 뉴럴 표면 구축에 의한 물질 무관한 물체의 갓 프로덕션 감지",
      "summary": "로봇이 투명하고 반사성이 높은 물체를 잡을 때 정확한 깊이 정보를 기반으로 하는 방법은 큰 문제를 가지고 있습니다. 본 논문에서는 배경 프로젝트를 활용하여 재료와 상관없는 잡음 동작을 수행하는 뉴格拉스프(NeuGrasp)를 소개합니다. NeuGrasp은 transformers와 글로벌 프로젝트를 조합하여 공간 인코딩을 사용하여 다각점 특징량을 축적하고, 좁은 혹은 희귀한 관점 조건에서도 강력한 표면 재구성을 가능하게 합니다. 포로콘 물체를 중심으로 잔차 특징을 강화하고, 공간 인식을 정확화하기 위해 占有 전설 프로젝트를 사용합니다. 투명하고 반사성이 높은 물체를 잡을 때도 뛰어난 성능을 보입니다. 자세한 설명은 https://neugrasp.github.io/ 에서 확인하실 수 있습니다.",
      "upvotes": 0,
      "discussionId": "67cd7ad0999766d8cd73fb77",
      "ai_keywords": [
        "neural surface reconstruction",
        "background priors",
        "material-agnostic grasp detection",
        "transformers",
        "global prior volumes",
        "multi-view features",
        "spatial encoding",
        "narrow and sparse viewing conditions",
        "residual feature enhancement",
        "occupancy-prior volume",
        "transparent objects",
        "specular surfaces"
      ]
    },
    "publishedAt": "2025-03-05T08:57:37.000Z",
    "title": "NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection",
    "summary": "Robotic grasping in scenes with transparent and specular objects presents\ngreat challenges for methods relying on accurate depth information. In this\npaper, we introduce NeuGrasp, a neural surface reconstruction method that\nleverages background priors for material-agnostic grasp detection. NeuGrasp\nintegrates transformers and global prior volumes to aggregate multi-view\nfeatures with spatial encoding, enabling robust surface reconstruction in\nnarrow and sparse viewing conditions. By focusing on foreground objects through\nresidual feature enhancement and refining spatial perception with an\noccupancy-prior volume, NeuGrasp excels in handling objects with transparent\nand specular surfaces. Extensive experiments in both simulated and real-world\nscenarios show that NeuGrasp outperforms state-of-the-art methods in grasping\nwhile maintaining comparable reconstruction quality. More details are available\nat https://neugrasp.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03511.png",
    "numComments": 1,
    "isAuthorParticipating": true
  }
]