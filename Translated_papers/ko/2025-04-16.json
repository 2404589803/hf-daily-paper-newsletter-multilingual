[
  {
    "paper": {
      "id": "2504.08672",
      "authors": [
        {
          "_id": "67fcb7294a92187863e805ee",
          "user": {
            "_id": "64e6cf78ecce34cb442dc889",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
            "isPro": false,
            "fullname": "Fangzhi Xu",
            "user": "xufangzhi",
            "type": "user"
          },
          "name": "Fangzhi Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:16.537Z",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805ef",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f0",
          "name": "Chang Ma",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f1",
          "name": "Haiteng Zhao",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f2",
          "user": {
            "_id": "6064a0eeb1703ddba0d458b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
            "isPro": false,
            "fullname": "Qiushi",
            "user": "QiushiSun",
            "type": "user"
          },
          "name": "Qiushi Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:24:08.107Z",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f3",
          "name": "Kanzhi Cheng",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f4",
          "name": "Junxian He",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f5",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f6",
          "name": "Zhiyong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T16:26:23.000Z",
      "submittedOnDailyAt": "2025-04-16T05:46:28.754Z",
      "title": "天才：일반화되어 있으며, 완전한 무마니얼의 자기 학습 프레임워크\n  진보적인 이유에 적합하다.",
      "submittedOnDailyBy": {
        "_id": "64e6cf78ecce34cb442dc889",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
        "isPro": false,
        "fullname": "Fangzhi Xu",
        "user": "xufangzhi",
        "type": "user"
      },
      "summary": "LLM의 이유론의 발전은 광범위한 관심을 불러일으키고 있습니다. 그러나 현재의 훈련 후 기술은 결과의 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브적 서브",
      "upvotes": 35,
      "discussionId": "67fcb72a4a92187863e8061b",
      "projectPage": "https://github.com/xufangzhi/Genius",
      "githubRepo": "https://github.com/xufangzhi/Genius",
      "ai_keywords": [
        "self-training framework",
        "Genius",
        "stepwise foresight re-sampling strategy",
        "advantage-calibrated optimization (ACO) loss function"
      ]
    },
    "publishedAt": "2025-04-11T12:26:23.000Z",
    "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
    "summary": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6cf78ecce34cb442dc889",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
      "fullname": "Fangzhi Xu",
      "name": "xufangzhi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10481",
      "authors": [
        {
          "_id": "67fdc1b41d1bc292f7b9358e",
          "user": {
            "_id": "64e18e9ec20c27fcc8df384e",
            "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
            "isPro": false,
            "fullname": "Ding Chen",
            "user": "Hush-cd",
            "type": "user"
          },
          "name": "Ding Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:22.449Z",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b9358f",
          "user": {
            "_id": "6455ff584095c967f9a847bb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6455ff584095c967f9a847bb/A5wjtWsudC73fLVmgASBr.jpeg",
            "isPro": false,
            "fullname": "Qingchen Yu",
            "user": "Duguce",
            "type": "user"
          },
          "name": "Qingchen Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:45.275Z",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93590",
          "name": "Pengyuan Wang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93591",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93592",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93593",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93594",
          "name": "Xinchi Li",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93595",
          "name": "Minchuan Yang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93596",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:59:36.000Z",
      "submittedOnDailyAt": "2025-04-16T00:53:50.942Z",
      "title": "xVerify: 효율적인 이유 모형 평가에 대한 답 검증 도구",
      "submittedOnDailyBy": {
        "_id": "64e18e9ec20c27fcc8df384e",
        "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
        "isPro": false,
        "fullname": "Ding Chen",
        "user": "Hush-cd",
        "type": "user"
      },
      "summary": "o1モデル의 릴리스에 따라, 쉼타임 스텝을 적용한 추론 모형이 점차 등장했습니다. 이러한 모형은 복잡한 추론, 중간 단계, 그리고 자기감각을 포함하여 생성하는 답변이므로, 기존 평가 방법들이 많은 경우 부족한 것으로 평가됩니다. 이들은 LLM의 출력이 실제 정답과 동일한지 판단할 수 없거나, 긴 복잡한 답변에서 최종적인 답을 추출할 수 없기 때문에 문제가 있습니다. 이러한 문제를 해결하기 위해, xVerify라는 추론 모형의 평가에 대한 효율적인 답변 검증 기능을 제안합니다. xVerify는 등호 판단에 강한 능력을 보여주며, 이유론 모형가 생성한 답변이 정답과 등호인지를 다양한 목적의 질문에 대해 효과적으로 판단할 수 있도록 설계되었습니다. xVerify의 훈련 및 평가에는 다양한 데이터셋에서 생성된 질문·답변 쌍과, 특히 이유론 모형의 평가에 대한 평가 세트를 활용한 VAR 데이터셋을 사용했습니다. 여러 회의 注釈 프로세스를 사용하여 라벨의 정확성을 보장합니다. VAR 데이터셋을 기반으로 다양한 크기의 여러 xVerify 모형을 훈련합니다. 테스트 세트와 일반화 세트의 평가 실험에서, 모든 xVerify 모형은 전체적인 F1 스코어와 정확도가 95%를 초과했습니다. 특히, 최소 크기의 버전인 xVerify-0.5B-I는 GPT-4o를 제외한 모든 평가 방법에서 초월하였고, xVerify-3B-Ib는 전체적인 성능에서 GPT-4o를 초월했습니다. 이러한 결과를 통해 xVerify의 효과와 일반화 능력이 입증됩니다.",
      "upvotes": 28,
      "discussionId": "67fdc1b51d1bc292f7b935e8",
      "githubRepo": "https://github.com/IAAR-Shanghai/xVerify",
      "ai_keywords": [
        "reasoning models",
        "o1 model",
        "slow thinking strategies",
        "complex reasoning",
        "intermediate steps",
        "self-reflection",
        "evaluation methods",
        "LLM output",
        "reference answer",
        "final answer",
        "xVerify",
        "equivalence judgment",
        "VAR dataset",
        "multi-round annotation process",
        "F1 scores",
        "xVerify-0.5B-I",
        "xVerify-3B-Ib",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-04-14T13:59:36.000Z",
    "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
    "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10481.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e18e9ec20c27fcc8df384e",
      "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
      "fullname": "Ding Chen",
      "name": "Hush-cd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10465",
      "authors": [
        {
          "_id": "67ff26c3414c03ebc1d42529",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252a",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:53.598Z",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252b",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252c",
          "name": "Yanwei Li",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252d",
          "name": "Weixian Lei",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252e",
          "name": "Xueqing Deng",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252f",
          "name": "Shihao Chen",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d42530",
          "name": "Shunping Ji",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d42531",
          "name": "Jiashi Feng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:52:22.000Z",
      "submittedOnDailyAt": "2025-04-16T02:11:29.898Z",
      "title": "Pixel-SAIL: 픽셀 기반의 이해를 실현하는 Single Transformer",
      "submittedOnDailyBy": {
        "_id": "63958b4414513eaf9029ebf1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
        "isPro": false,
        "fullname": "Xiangtai Li",
        "user": "LXT",
        "type": "user"
      },
      "summary": "다모달 대언어 모델(MLLMs)은 미세한 픽셀 수준의 이해 태스크에 대해 놀라운 성능을 달성합니다. 그러나 모든 연구는 시각 인코더(CLIP), 분할 엑스플래이 등 추가 기능에 중대한 의존을 하고 있으며, 이는 시스템 복잡성을 높임으로써 모델의 확장이 제한되어 있습니다. 본 연구에서는 추가 기능을 도입하지 않고 고차원으로 단순화된 MLLM을 탐색하는 것을 목표로 합니다. 본 연구는 최근의 Single Transformer as a unified Vision-Language Model(SAIL) 설계에 기반합니다. 이러한 연구에서는 transformers를 사용하여 시각 토큰과 텍스트 토큰을 함께 학습하는 것을 수행합니다. Pixel-SAIL은 픽셀 수준의 MLLM 태스크에 대한 단일 transformer를 소개합니다. 특히, 기본적인 베이스라인에서 3가지 기술적인 개선을 수행합니다. 먼저, 시각 토큰 특징을 정밀화하기 위한 학습 가능한 업샘플링 모듈을 설계합니다. 다음으로, 단일 transformer가 시각 프로ン퓰 입력을 이해하고 시각 프로ン퓰 인베딩과 시각 토큰의 초기 융합으로 이익을 얻을 수 있도록 새로운 시각 프로ン퓰 注入 전략을 제안합니다. 마지막으로, 시각 엑스플래이의 정확한 특성 추출 능력을 효율적으로 향상시키기 위한 시각 엑스플래이의 경험적 발달 전략을 도입합니다. 또한, 손동 체크를 사용하여 수집한 세부적인 픽셀 이해 벤치마크(PerBench)를 소개합니다. 이는 세부적인 물체 설명, 시각 프로ン퓰 기반의 질문 대답, 시각 문어 참조 분할의 3가지 태스크를 포함합니다. 4가지의 참조 분할 벤치마크, 1가지의 시각 프로ン퓰 벤치마크와 우리의 PerBench에 대해 확장된 실험을 수행하며, 우리의 Pixel-SAIL이 더 간단한 파이프라인으로 상대적으로 나쁨이나 더 좋은 결과를 얻는 것을 보여주었습니다. 코드와 모델은 https://github.com/magic-research/Sa2VA에서 공개됩니다.",
      "upvotes": 21,
      "discussionId": "67ff26c6414c03ebc1d425de",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "pixel-level understanding",
        "vision encoder (CLIP)",
        "segmentation experts",
        "single transformer as a unified vision-language model (SAIL)",
        "pixel-wise MLLM tasks",
        "learnable upsampling module",
        "visual prompt injection",
        "visual prompt embeddings",
        "vision expert distillation",
        "pixel understanding benchmark (PerBench)",
        "detailed object description",
        "visual prompt-based question answering",
        "visual-text referring segmentation",
        "referring segmentation benchmarks",
        "visual prompt benchmark"
      ]
    },
    "publishedAt": "2025-04-14T13:52:22.000Z",
    "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding",
    "summary": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for\nfine-grained pixel-level understanding tasks. However, all the works rely\nheavily on extra components, such as vision encoder (CLIP), segmentation\nexperts, leading to high system complexity and limiting model scaling. In this\nwork, our goal is to explore a highly simplified MLLM without introducing extra\ncomponents. Our work is motivated by the recent works on Single trAnsformer as\na unified vIsion-Language Model (SAIL) design, where these works jointly learn\nvision tokens and text tokens in transformers. We present Pixel-SAIL, a single\ntransformer for pixel-wise MLLM tasks. In particular, we present three\ntechnical improvements on the plain baseline. First, we design a learnable\nupsampling module to refine visual token features. Secondly, we propose a novel\nvisual prompt injection strategy to enable the single transformer to understand\nvisual prompt inputs and benefit from the early fusion of visual prompt\nembeddings and vision tokens. Thirdly, we introduce a vision expert\ndistillation strategy to efficiently enhance the single transformer's\nfine-grained feature extraction capability. In addition, we have collected a\ncomprehensive pixel understanding benchmark (PerBench), using a manual check.\nIt includes three tasks: detailed object description, visual prompt-based\nquestion answering, and visual-text referring segmentation. Extensive\nexperiments on four referring segmentation benchmarks, one visual prompt\nbenchmark, and our PerBench show that our Pixel-SAIL achieves comparable or\neven better results with a much simpler pipeline. Code and model will be\nreleased at https://github.com/magic-research/Sa2VA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10465.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63958b4414513eaf9029ebf1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
      "fullname": "Xiangtai Li",
      "name": "LXT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10337",
      "authors": [
        {
          "_id": "67fddae99a03686367721718",
          "user": {
            "_id": "6471a24381ded91f253ceb1c",
            "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
            "isPro": false,
            "fullname": "Wesley Shi",
            "user": "WesleyShi",
            "type": "user"
          },
          "name": "Wenlei Shi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-15T06:43:40.277Z",
          "hidden": false
        },
        {
          "_id": "67fddae99a03686367721719",
          "name": "Xing Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T15:46:33.000Z",
      "submittedOnDailyAt": "2025-04-16T00:52:23.733Z",
      "title": "하이미더：생성 인식의 검증 시의 스케일링 테스트",
      "submittedOnDailyBy": {
        "_id": "6471a24381ded91f253ceb1c",
        "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
        "isPro": false,
        "fullname": "Wesley Shi",
        "user": "WesleyShi",
        "type": "user"
      },
      "summary": "AI 시스템은 지식의 생성과 유지가 가능한 한, 그 지식의 확인이 가능한 한, 그 지식의 확인이 가능하다. 장기간 Chain-of-Thought reasoning에 대한 최근 연구는 LLM(대규모 언어 모델)이 경쟁적인 문제를 해결하는 데 큰 잠재력을 보여주지만, 그 확인 능력은 약하고, 충분히 조사되어 있지 않습니다. 본 논문에서는, Chain-of-Thought reasoning의 확인을 수행하는 LLM, Heimdall을 제안합니다. Heimdall은 해결책의 정확성을 정확히 판단할 수 있습니다. 강화학습을 우선으로, 경쟁적인 수학 문제에서 확인 정확도를 62.5%에서 94.5%로 향상시켰습니다. 재현 샘플링을 사용하여 스케일링하면, 정확도가 더 97.5%에 도달했습니다. 인간 평가에 따르면, Heimdall은 인상적인 일반화 능력을 보여주고, 훈련 중 포함되지 않은 어려운 수학 증명의 많은 문제를 올바르게 감지했습니다. 또한, Pessimistic Verification을 제안하고, Heimdall의 기능을 문제 해결의 스케일링에 확장시켰습니다. Pessimistic Verification은 해결 모델로부터의 해결책을 Heimdall에 판단하고, 가장 낮은 신뢰도의 해결책을 선택합니다. DeepSeek-R1-Distill-Qwen-32B를 해결 모델로 사용하였을 때, Pessimistic Verification은 AIME2025의 해결 정확도를 54.2%에서 70.0%로, 16배의 계산 바켓을 사용하였을 때 83.3%로 향상시켰습니다. 2.5 Pro의 강력한 해결 모델을 사용하였을 때, 점수는 93.0%에 도달했습니다. 마지막으로, 자동적인 지식발견 시스템의 프로토타イ프를 구축했습니다. 이 시스템에서, 첫 번째 소스가 문제를 제안하고, 두 번째 소스가 해결책을 제공하고, 세 번째 소스가 해결책을 확인합니다. NuminaMath의 데이터 합성 워크를 첫 번째 2 컴포넌트으로 사용하였으며, Heimdall은 데이터 세트 내의 문제의 레코드를 효과적으로 감지하고, 거의 절반의 데이터가 탓을 것을 밝혀냅니다. 이는 NuminaMath의 최근의 축소 연구와 깊은 일치를 나타냅니다.",
      "upvotes": 21,
      "discussionId": "67fddaea9a03686367721776",
      "ai_keywords": [
        "Chain-of-Thought reasoning",
        "LLMs (Large Language Models)",
        "Heimdall",
        "long CoT verification",
        "pure reinforcement learning",
        "synthetic math problems",
        "human evaluation",
        "generalization capabilities",
        "Pessimistic Verification",
        "DeepSeek-R1-Distill-Qwen-32B",
        "AIME2025",
        "Gemini 2.5 Pro",
        "solution accuracy",
        "automatic knowledge discovery system",
        "ternary system",
        "NuminaMath",
        "data synthesis",
        "data records",
        "flawed data"
      ]
    },
    "publishedAt": "2025-04-14T11:46:33.000Z",
    "title": "Heimdall: test-time scaling on the generative verification",
    "summary": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10337.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6471a24381ded91f253ceb1c",
      "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
      "fullname": "Wesley Shi",
      "name": "WesleyShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11346",
      "authors": [
        {
          "_id": "67ff18961dc5d56fdd6ca724",
          "name": "Yu Gao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca725",
          "name": "Lixue Gong",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca726",
          "name": "Qiushan Guo",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca727",
          "name": "Xiaoxia Hou",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca728",
          "name": "Zhichao Lai",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca729",
          "name": "Fanshi Li",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72a",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72b",
          "name": "Xiaochen Lian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72c",
          "name": "Chao Liao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72d",
          "name": "Liyang Liu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72e",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72f",
          "name": "Yichun Shi",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca730",
          "name": "Shiqi Sun",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca731",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca732",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca733",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca734",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca735",
          "name": "Xuanda Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca736",
          "name": "Xun Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca737",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca738",
          "name": "Guofeng Wu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca739",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73a",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73b",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73c",
          "name": "Zhonghua Zhai",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73d",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73e",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73f",
          "name": "Yuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca740",
          "name": "Shijia Zhao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca741",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca742",
          "name": "Weilin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:19:07.000Z",
      "submittedOnDailyAt": "2025-04-16T01:10:39.295Z",
      "title": "Seedream 3.0 기술보고서",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "종실몽 3.0는 고성능의 중국어・영어의 바이리언 이미지 생성 기반 모델입니다. 종실몽 2.0에서 이미 존재하는 문제점을 해결하기 위해, 프로젝트에서는 데이터 구축부터 모델 배치까지의 전체 프로세스에서 다양한 기술적인 개선을 수행했습니다. 특히, 복잡한 프로ン튤의 대응, 세밀한 타이포그래피의 생성, 화질의 부적절성, 이미지의 해상도의 제한 등 문제점을 해결했습니다. 종실몽 3.0의 발전은 데이터 구축부터 모델 배치까지의 전체 프로세스의 개선에 기반합니다. 데이터 레이어에서, 디фек트에 대한 관심 있는 트레이닝 패러다임과 이중 축 협업 데이터 샘플링 프레임워크를 사용하여 데이터 세트를 두 배로 늘렸습니다. 또한, 사전 학습 시에는 혼합レン지 트레이닝, 크로스 모달리티 RoPE, 표현의 대응 손실, 범위에 대한 시간 스텝 샘플링 등 효과적인 방법을 사용했습니다. 후부 딥러닝 단계에서, 다양한 미술적인 댓글을 활용한 SFT를 사용하며, 확장된 VLM 기반 보상 모델을 사용하여 인간적인 취향에 맞는 출력을 구현했습니다. 또한, 종실몽 3.0은 새로운 가속 패러다임을 개척했습니다. 일관된 노이즈 기대값과 중요성에 대한 시간 스텝 샘플링을 사용하여, 이미지의 품질을 유지하는 동시에 4か 8배의 속도 업을 실현했습니다. 종실몽 3.0은 종실몽 2.0보다 뚜렷한 개선을 보여주며, 특히 복잡한 중국 글자의 텍스트 렌더링 능력을 향상시켰습니다. 이는 프로젝트의 타이포그래피 생성에 중요합니다. 또한, 고해상도 출력을 제공하여 고품질의 이미지를 생성할 수 있도록 하였습니다.",
      "upvotes": 20,
      "discussionId": "67ff189c1dc5d56fdd6ca8e0",
      "projectPage": "https://team.doubao.com/zh/tech/seedream3_0",
      "ai_keywords": [
        "mixed-resolution training",
        "cross-modality RoPE",
        "representation alignment loss",
        "resolution-aware timestep sampling",
        "SFT (Supervised Fine-Tuning)",
        "VLM (Vision Language Model)",
        "consistent noise expectation",
        "importance-aware timestep sampling"
      ]
    },
    "publishedAt": "2025-04-15T12:19:07.000Z",
    "title": "Seedream 3.0 Technical Report",
    "summary": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11346.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 47
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11442",
      "authors": [
        {
          "_id": "67ff1387e1bfbb6bdd79ab72",
          "user": {
            "_id": "628b671f8fb67b90658613f7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653303027789-noauth.jpeg",
            "isPro": false,
            "fullname": "Leon Guertler",
            "user": "LeonGuertler",
            "type": "user"
          },
          "name": "Leon Guertler",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:02.481Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab73",
          "user": {
            "_id": "653879fbf5f5016df355d010",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653879fbf5f5016df355d010/hU3mrTOw3DQ8auUGoQceW.jpeg",
            "isPro": false,
            "fullname": "Bobby Cheng",
            "user": "bobbycxy",
            "type": "user"
          },
          "name": "Bobby Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:06.026Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab74",
          "user": {
            "_id": "636681feaa6a4af6073ba73e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636681feaa6a4af6073ba73e/u_0moYNu6as9Sszp-ej95.png",
            "isPro": true,
            "fullname": "Simon Yu",
            "user": "simonycl",
            "type": "user"
          },
          "name": "Simon Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:08.368Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab75",
          "user": {
            "_id": "635e3a76106f984574c36409",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
            "isPro": false,
            "fullname": "Bo Liu",
            "user": "Benjamin-eecs",
            "type": "user"
          },
          "name": "Bo Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:46:16.265Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab76",
          "name": "Leshem Choshen",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab77",
          "name": "Cheston Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:55:20.000Z",
      "submittedOnDailyAt": "2025-04-16T01:06:25.215Z",
      "title": "TextArena\n\n이 번역은 TextArena의 내용을 한국어로 번역한 것입니다. TextArena는 텍스트를 처리하고 번역하는 플랫폼입니다. 이 플랫폼은 사용자의 요청에 따라 다양한 언어로 텍스트를 번역할 수 있습니다. TextArena는 사용자의 텍스트를 분석하고, 정확한 번역 결과를 제공하며, 다양한 언어와 문법 규칙을 이해하고 적용할 수 있는 고급 번역 기능을 제공합니다. TextArena는 번역 작업에 필요한 다양한 도구와 기능을 제공하여, 사용자의 번역 작업이 효율적이고 정확하게 수행될 수 있도록 지원합니다.",
      "submittedOnDailyBy": {
        "_id": "635e3a76106f984574c36409",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
        "isPro": false,
        "fullname": "Bo Liu",
        "user": "Benjamin-eecs",
        "type": "user"
      },
      "summary": "텍스트アリアン은 대규모 언어 모델(LLMs)의 에이전트 행동의 훈련과 평가를 위해 오픈 소스의 경쟁적인 텍스트 기반 게임의 집합입니다. 57개 이상의 고유한 환경(단일 플레이어, 투어 플레이어, 멀티 플레이어 설정을 포함)를 포함하고 있으며, 온라인 플레이 시스템을 통해 인간이나 다른 제안 모델과 대결하여 모델의 능력을 쉽게 평가할 수 있습니다. 실시간 TrueSkill 점수를 제공합니다. 전통적인 벤치마크는 동적인 사회적 기술(예: ネジジョー, マインドシンク, デジューション)을 평가하는 것을 부족한 것으로 알려져 있으며, 이 격차를 해결할 수 있습니다. 연구, 커뮤니티, 확장성을 기준으로 설계되어 새로운 게임을 추가, 프레임워크의 변경, 모델의 테스트, 모델과의 플레이, 모델의 훈련을 우선시합니다. 환경, 게임, 순위보드, 예시를 포함하여 자세한 설명은 https://github.com/LeonGuertler/TextArena와 https://www.textarena.ai/에서 사용 가능합니다.",
      "upvotes": 18,
      "discussionId": "67ff1388e1bfbb6bdd79abbe",
      "projectPage": "https://textarena.ai/",
      "githubRepo": "https://github.com/LeonGuertler/TextArena",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "TrueSkill scores",
        "negotiation",
        "theory of mind",
        "deception",
        "dynamic social skills"
      ]
    },
    "publishedAt": "2025-04-15T13:55:20.000Z",
    "title": "TextArena",
    "summary": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11442.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "635e3a76106f984574c36409",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
      "fullname": "Bo Liu",
      "name": "Benjamin-eecs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10766",
      "authors": [
        {
          "_id": "67ff114a3026f8abc4bf7e43",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e44",
          "name": "Yanhong Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e45",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e46",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:46:19.086Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T23:53:47.000Z",
      "submittedOnDailyAt": "2025-04-16T00:40:29.697Z",
      "title": "インストラクション 데이터와 이유 데이터가 훈련 후 어떻게 영향을 미칠까： 레이어별로의 경사에서 본 데이터의 품질",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)의 후 학습이 지도를 따르는 복잡한 논리론 문제로 진화하는 가운데, 서로 다른 데이터가 미세 조정의 동적으로 어떤 영향을 미치는지에 대한 이해는 주로 탐색되고 있지 않습니다. 본 논문에서는, LLM의 후 학습에 의한 저/고 품질의 지도 및 논리론 데이터에 의한 레이어별 경사 스펙트럼 분석을 제안합니다. 우리의 분석은, 데이터 평가로 구성된 사전 지표(예: IFD, InsTag, Difficulty, Reward)가 경사의 고유값 분해(SVD)로부터 계산된 스펙트럼 특성으로 해석되어 통일되는 것을 보여줍니다. 특히, 고 품질의 데이터는 일반적으로 核 Norm이 낮고, 효과적인 레ン킹이 높게 나타나며, 효과적인 레ン킹은 核 Norm에 의한 미묘한 품질의 차이를 감지하는 데 더 좋은 강건성과 해상도를 보여주는 것을 보입니다. 예를 들어, 논리론 데이터는 지도 데이터보다 크게 높게 나타나는 효과적인 레ン킹을 보여주며, 복잡한 문제를 다루는 데는 더 풍부한 경사 구조를 가지고 있음을 보여줍니다. 실험도, 같은 family의 모델은 크기에 따라 비슷한 경사 패턴을 공유하고, 모델의 family가 다른 경우 유의하게 다른 것을 보여줍니다. 지도 데이터와 논리론 데이터의 데이터 품질의 영향에 대한 일관된 시각을 제공함으로써, 이 연구는 데이터 품질과 학습의 안정성 사이의 상호작용을 명확히 하고, 후 학습의 데이터 탐색 전략의 개발에 새로운 시각을 제공합니다.",
      "upvotes": 16,
      "discussionId": "67ff11503026f8abc4bf7fed",
      "githubRepo": "https://github.com/MingLiiii/Gradient_Unified",
      "ai_keywords": [
        "spectral analysis",
        "layer-wise gradients",
        "low/high-quality instruction",
        "reasoning data",
        "IFD",
        "InsTag",
        "Difficulty",
        "Reward",
        "singular value decomposition (SVD)",
        "nuclear norms",
        "effective ranks",
        "gradient structures",
        "training stability"
      ]
    },
    "publishedAt": "2025-04-14T19:53:47.000Z",
    "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients",
    "summary": "As the post-training of large language models (LLMs) advances from\ninstruction-following to complex reasoning tasks, understanding how different\ndata affect finetuning dynamics remains largely unexplored. In this paper, we\npresent a spectral analysis of layer-wise gradients induced by low/high-quality\ninstruction and reasoning data for LLM post-training. Our analysis reveals that\nwidely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and\nReward, can be explained and unified by spectral properties computed from\ngradients' singular value decomposition (SVD). Specifically, higher-quality\ndata are usually associated with lower nuclear norms and higher effective\nranks. Notably, effective rank exhibits better robustness and resolution than\nnuclear norm in capturing subtle quality differences. For example, reasoning\ndata achieves substantially higher effective ranks than instruction data,\nimplying richer gradient structures on more complex tasks. Our experiments also\nhighlight that models within the same family share similar gradient patterns\nregardless of their sizes, whereas different model families diverge\nsignificantly. Providing a unified view on the effects of data quality across\ninstruction and reasoning data, this work illuminates the interplay between\ndata quality and training stability, shedding novel insights into developing\nbetter data exploration strategies for post-training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10766.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10462",
      "authors": [
        {
          "_id": "67ff2aa6a0346c2e622afdb2",
          "name": "Weixian Lei",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb3",
          "name": "Jiacong Wang",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb4",
          "name": "Haochen Wang",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb5",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:51.715Z",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb6",
          "name": "Jun Hao Liew",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb7",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb8",
          "name": "Zilong Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:50:20.000Z",
      "submittedOnDailyAt": "2025-04-16T02:27:49.065Z",
      "title": "스칼라빌리티의 간단성: 비젼・런그레이즈 학습의 실험적 분석을 하나의 Transformer로 수행합니다.",
      "submittedOnDailyBy": {
        "_id": "63958b4414513eaf9029ebf1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
        "isPro": false,
        "fullname": "Xiangtai Li",
        "user": "LXT",
        "type": "user"
      },
      "summary": "이 논문에서는, 단일 트랜스포머를 구성하는 통일된 모노모달 대규모 언어 모델(MLLM)인 SAIL을 통해 픽셀 인코딩과 언어 해석을 하나의 아키텍처 내에 통합하고 있습니다. 현재 모듈화된 모델 라인 모델과 달리, SAIL은 사전 학습된 시각 트랜스포머(ViT)의 독립적인 시각 인코더를 필요로 하지 않고, 더 최소한의 아키텍처 설계를 제공하고 있습니다. 새로운 아키텍처 구성 요소를 도입 대신, SAIL은 시각과 문자의 모델의 특성에 맞게, 혼합 어텐션 구조와 모노모달 위치 인코딩을 적용하고 있습니다. SAIL의 scalability, cross-modal information flow pattern, 시각 표현 능력 등 특성을 모듈화된 MLLM과 비교하여 체계적으로 비교하고 있습니다. 훈련 데이터와 모델 크기를 둘 다 스케일링하여, SAIL은 모듈화된 MLLM과 동일한 성능을 달성합니다. 특히, 사전 학습된 ViT 구성 요소의 제거는 SAIL의 scalability를 향상시키고, cross-modal information flow pattern이 상당히 다른 것이 되었습니다. 또한, SAIL은 강력한 시각 표현 능력을 보여주며, ViT-22B과 같은 결과를 Semantic Segmentation 등 시각 태스크에서 얻습니다. 코드와 모델은 https://github.com/bytedance/SAIL에서 사용 가능합니다.",
      "upvotes": 12,
      "discussionId": "67ff2aa7a0346c2e622afe08",
      "ai_keywords": [
        "single transformer",
        "unified multimodal large language model (MLLM)",
        "raw pixel encoding",
        "language decoding",
        "vision transformer (ViT)",
        "mix-attention mechanisms",
        "multimodal positional encodings",
        "scalability",
        "cross-modal information flow patterns",
        "visual representation capabilities",
        "semantic segmentation",
        "ViT-22B"
      ]
    },
    "publishedAt": "2025-04-14T13:50:20.000Z",
    "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer",
    "summary": "This paper introduces SAIL, a single transformer unified multimodal large\nlanguage model (MLLM) that integrates raw pixel encoding and language decoding\nwithin a singular architecture. Unlike existing modular MLLMs, which rely on a\npre-trained vision transformer (ViT), SAIL eliminates the need for a separate\nvision encoder, presenting a more minimalist architecture design. Instead of\nintroducing novel architectural components, SAIL adapts mix-attention\nmechanisms and multimodal positional encodings to better align with the\ndistinct characteristics of visual and textual modalities. We systematically\ncompare SAIL's properties-including scalability, cross-modal information flow\npatterns, and visual representation capabilities-with those of modular MLLMs.\nBy scaling both training data and model size, SAIL achieves performance\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\nenhances SAIL's scalability and results in significantly different cross-modal\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\nrepresentation capabilities, achieving results on par with ViT-22B in vision\ntasks such as semantic segmentation. Code and models are available at\nhttps://github.com/bytedance/SAIL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10462.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63958b4414513eaf9029ebf1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
      "fullname": "Xiangtai Li",
      "name": "LXT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10559",
      "authors": [
        {
          "_id": "67ff1df03b42083b37219456",
          "name": "Keyu Duan",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219457",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219458",
          "name": "Xin Mao",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219459",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945a",
          "name": "Changyu Chen",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945b",
          "name": "Qiguang Chen",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945c",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945d",
          "user": {
            "_id": "6214e4ee1e35c843d42d1f88",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
            "isPro": true,
            "fullname": "Longxu Dou",
            "user": "dreamerdeo",
            "type": "user"
          },
          "name": "Longxu Dou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:00.444Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T14:53:56.000Z",
      "submittedOnDailyAt": "2025-04-16T01:34:03.069Z",
      "title": "활성 화학 학습법을 통해 유효한 프로세스 보상 모형 훈련을 달성하는 방법",
      "submittedOnDailyBy": {
        "_id": "6214e4ee1e35c843d42d1f88",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
        "isPro": true,
        "fullname": "Longxu Dou",
        "user": "dreamerdeo",
        "type": "user"
      },
      "summary": "프로세스 보상 모델 (PRMs)은 대규모 언어 모델 (LLMs)에 단계별 서브 제츰를 제공하지만, 훈련 데이터의 설명의 스케일링은 인간과 LLMs 모두에게 어려움이 있습니다. 이러한 한계를 대처하기 위해, 우리는 능동 학습 접근 방식을 제안하고 있습니다. ActPRM은 가장 불확실한 샘플을 능동적으로 선택하여 라벨링 비용의 절감에 성공합니다. 훈련 중, PRM을 사용하여 전파 후의 불확실성을 평가하고 그 불확실성을 기준으로 높은 위험의 데이터만 남겨두습니다. 이후, 이 데이터를 라벨링하기 위해 가능한 한 비용이 높은 추론 모델을 사용합니다. 라벨에 대한 손실을 계산하고 PRM의 가중치를 업데이트합니다. ActPRM과 베이지안의 미세 조정을 비교하여, POOL 기반의 능동 학습 설정에서 ActPRM은 50%의 설명을 줄이고 상대적으로 나은 성능을 구현합니다. 설명의 효율을 초과하여, 우리는 ActPRM을 사용하여 1M+의 수학 논리 데이터 로더를 필터링하여 60%의 데이터를 남겨둡니다. 이후, 이 선택된 데이터 세트를 기반으로한 훈련은 ProcessBench (75.0%)과 PRMBench (65.5%)에서 새로운 최신 기술 (SOTA)의 PRM을 실현합니다.",
      "upvotes": 8,
      "discussionId": "67ff1df23b42083b372194a8",
      "githubRepo": "https://github.com/sail-sg/ActivePRM",
      "ai_keywords": [
        "active learning",
        "ActPRM",
        "uncertainty estimation",
        "labeling costs",
        "vanilla fine-tuning",
        "pool-based active learning",
        "annotation efficiency",
        "math reasoning trajectories",
        "state-of-the-art (SOTA)",
        "ProcessBench",
        "PRMBench"
      ]
    },
    "publishedAt": "2025-04-14T10:53:56.000Z",
    "title": "Efficient Process Reward Model Training via Active Learning",
    "summary": "Process Reward Models (PRMs) provide step-level supervision to large language\nmodels (LLMs), but scaling up training data annotation remains challenging for\nboth humans and LLMs. To address this limitation, we propose an active learning\napproach, ActPRM, which proactively selects the most uncertain samples for\ntraining, substantially reducing labeling costs. During training, we use the\nPRM to estimate uncertainty after the forward pass, retaining only highly\nuncertain data. A capable yet costly reasoning model then labels this data.\nThen we compute the loss with respect to the labels and update the PRM's\nweights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active\nlearning setting, demonstrating that ActPRM reduces 50% annotation, but\nachieving the comparable or even better performance. Beyond annotation\nefficiency, we further advance the actively trained PRM by filtering over 1M+\nmath reasoning trajectories with ActPRM, retaining 60% of the data. A\nsubsequent training on this selected dataset yields a new state-of-the-art\n(SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same\nsized models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10559.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6214e4ee1e35c843d42d1f88",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
      "fullname": "Longxu Dou",
      "name": "dreamerdeo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11427",
      "authors": [
        {
          "_id": "67ff1cc4372d6790b1b7da90",
          "name": "Yanrui Bin",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da91",
          "name": "Wenbo Hu",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da92",
          "name": "Haoyuan Wang",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da93",
          "name": "Xinya Chen",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da94",
          "name": "Bing Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/dKoipt1ASgjiyDt33avVt.mp4"
      ],
      "publishedAt": "2025-04-15T17:39:07.000Z",
      "submittedOnDailyAt": "2025-04-16T01:28:48.790Z",
      "title": "NormalCrafter: 영화에서 시간계열을 일치시키는 뀜댄스 벡터의 학습\n확산된 앞쪽 프로이징",
      "submittedOnDailyBy": {
        "_id": "657a7458afbb0117ba15c59f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
        "isPro": false,
        "fullname": "Wenbo Hu",
        "user": "wbhu-tc",
        "type": "user"
      },
      "summary": "표면 노르마 추정은 복수의 컴퓨터 비전 애플리케이션의 기초로役立ちます。 정적 이미지 시나리오에 대한 많은 노력을 기록하지만, 영상 기반의 노르마 추정에서 시간적인 일치성을 보장하는 것은 어려운 도전입니다. 기존의 방법에서 시간적인 요소를 추가하는 것이 아니라, 우리는 영상 디퓨저 모델의 고유의 시간적인 예측을 활용하기 위해 NormalCrafter을 소개합니다. 전체 시퀀스에서 고품질의 노르마 추정을 보장하기 위해, 우리는 디퓨저 특성량을 의미적 컷과 일치시켜 모델이 장의 고유의 의미적 특성에 집중하도록 권장하는 Semantic Feature Regularization(SFR)을 제안합니다. 또한, 우리는 잠재 공간과 픽셀 공간의 학습을 동시에 활용하는 2단계 학습 프로토콜을 도입하여 공간적 정확도를 유지하면서 장기적인 시간적인 컨텍스트를 유지하는 것을 목표로 합니다. 확장된 평가는 우리 방법의 효과를 보여주고, 복수의 영상으로부터 복잡한 세부 사항을 가진 시간적으로 일치하는 노르마 시퀀스의 생성에서 상위 성능을 보여주며, 우리 방법의 효과성을 입증합니다.",
      "upvotes": 5,
      "discussionId": "67ff1cc5372d6790b1b7daee",
      "projectPage": "https://normalcrafter.github.io/",
      "githubRepo": "https://github.com/Binyr/NormalCrafter",
      "ai_keywords": [
        "video diffusion models",
        "Semantic Feature Regularization (SFR)",
        "latent space",
        "pixel space",
        "temporal coherence",
        "spatial accuracy",
        "long temporal context",
        "temporally consistent",
        "intricate details"
      ]
    },
    "publishedAt": "2025-04-15T13:39:07.000Z",
    "title": "NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors",
    "summary": "Surface normal estimation serves as a cornerstone for a spectrum of computer\nvision applications. While numerous efforts have been devoted to static image\nscenarios, ensuring temporal coherence in video-based normal estimation remains\na formidable challenge. Instead of merely augmenting existing methods with\ntemporal components, we present NormalCrafter to leverage the inherent temporal\npriors of video diffusion models. To secure high-fidelity normal estimation\nacross sequences, we propose Semantic Feature Regularization (SFR), which\naligns diffusion features with semantic cues, encouraging the model to\nconcentrate on the intrinsic semantics of the scene. Moreover, we introduce a\ntwo-stage training protocol that leverages both latent and pixel space learning\nto preserve spatial accuracy while maintaining long temporal context. Extensive\nevaluations demonstrate the efficacy of our method, showcasing a superior\nperformance in generating temporally consistent normal sequences with intricate\ndetails from diverse videos.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/dKoipt1ASgjiyDt33avVt.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a7458afbb0117ba15c59f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
      "fullname": "Wenbo Hu",
      "name": "wbhu-tc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11343",
      "authors": [
        {
          "_id": "67ff2d4a86e7ad2b4bea1349",
          "name": "Wei Xiong",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134a",
          "name": "Jiarui Yao",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134b",
          "name": "Yuhui Xu",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134c",
          "name": "Bo Pang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134d",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134e",
          "name": "Doyen Sahoo",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134f",
          "name": "Junnan Li",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1350",
          "name": "Nan Jiang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1351",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1352",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1353",
          "name": "Hanze Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:15:02.000Z",
      "submittedOnDailyAt": "2025-04-16T02:39:25.160Z",
      "title": "미니멀리즘적 접근의 LLM 추론: 실험적 부인 샘플링으로부터 재강화\n\n(注意: \"부인 샘플링\"은 \"Rejection Sampling\"의 번역이며, \"재강화\"는 \"Reinforcement\"의 번역입니다.)",
      "submittedOnDailyBy": {
        "_id": "643e59806db6ba8c5ee123f3",
        "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
        "isPro": false,
        "fullname": "Wei Xiong",
        "user": "weqweasdas",
        "type": "user"
      },
      "summary": "강화학습(RL)은 복잡한 이유론 태스크에 대처하기 위해 대규모 언어 모델(LLMs)을 조정하는 주요한 접근법으로 자리잡았습니다. 최근의 방법들 중 GRPO는 DeepSeek-R1 등 모델의 훈련에 있어 실험적 성공을 보였고, 그 효과에 대한 이해가 미숙한 것으로 알려져 있습니다. 본 논문에서는 GRPO를 재평가하고, 정책 그레이디언트 알고리즘과 같은 강화학습 알고리즘의 관점에서 그 핵심적인 구성 요소를 분석합니다. 놀라울 정도로, 간단한 거부 샘플 기반의 RAFT는 긍정적인 보상을 받은 샘플만 훈련되어 있어 GRPO나 PPO와 비교할 수 있는 성능을 보입니다. 정규화 효과는 아닌지, GRPO의 주요한 장점은 완전히 틀린 대답을 가진 프롬프트를 버리는 점입니다. 이러한 관점에서, 완전히 틀린 및 완전히 정확한 샘플을 필터링하는 정책 그레이디언트의 최소한 확장 버전인 Reinforce-Rej을 제안합니다. Reinforce-Rej는 KL의 효율성과 안정성을 향상시키고, 복잡한 RL 알고리즘에 비해 가벼운 효과적인 대체로 활용됩니다. RAFT를 강력한 해석 가능한 베이스라인으로 채택하고, 향후 발전에 있어서, 부정적인 샘플을 무관하게 사용보다 더 원칙적인 설계를 중시하는 것을 주장합니다. 우리의 발견은 보상 기반의 LLM의 후처리에 대한 향후 연구에 대한 가이드라인을 제공합니다.",
      "upvotes": 4,
      "discussionId": "67ff2d4b86e7ad2b4bea1381",
      "ai_keywords": [
        "GRPO",
        "DeepSeek-R1",
        "reinforcement learning (RL)",
        "fine-tuning",
        "large language models (LLMs)",
        "complex reasoning tasks",
        "RAFT",
        "positively rewarded samples",
        "policy gradient",
        "KL efficiency"
      ]
    },
    "publishedAt": "2025-04-15T12:15:02.000Z",
    "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce",
    "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11343.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643e59806db6ba8c5ee123f3",
      "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
      "fullname": "Wei Xiong",
      "name": "weqweasdas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.10188",
      "authors": [
        {
          "_id": "67fe602166c0e8f3c2df22a9",
          "user": {
            "_id": "649d59cec6b4fdd84ebe0d47",
            "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
            "isPro": false,
            "fullname": "Deyuan Liu",
            "user": "SempraETY",
            "type": "user"
          },
          "name": "Deyuan Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T16:46:56.270Z",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22aa",
          "name": "Peng Sun",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22ab",
          "name": "Xufeng Li",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22ac",
          "name": "Tao Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T12:43:17.000Z",
      "submittedOnDailyAt": "2025-04-16T03:03:14.152Z",
      "title": "효율적인 생성 모델 훈련에 의한 내부 표현 WARMUP",
      "submittedOnDailyBy": {
        "_id": "649d59cec6b4fdd84ebe0d47",
        "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
        "isPro": false,
        "fullname": "Deyuan Liu",
        "user": "SempraETY",
        "type": "user"
      },
      "summary": "Diffusion 모델은 고차원 데이터의 생성에 뛰어난 성능을 보입니다が, 자동 인식 방법과 비교하여 훈련 효율과 표현 품질의 향상에 결함이 발견되었습니다. 우리는 주요 결점을 발견했습니다: 훈련 중 고품질, 의미 풍부한 표현의 활용 부족이, 특히 수렴 속도를 늦추고 있습니다. 체계적인 분석에 의해, 생성이 이루어질 때까지 주로 초기 계층에서 의미와 구조의 패턴을 학습하는 중요한 표현 처리 영역을 밝혀냈습니다. 이를 대처하기 위해, Embedded Representation Warmup (ERW)를 제안합니다. ERW는 초기 단계에서 ERW 모듈이 WARMUP와 같이, 미리 학습된 고품질 표현으로 초기화됩니다. 이 WARMUP는 표현 학습을 시작부터 최소한으로 억제하고 수렴을 가속화하고 성능을 향상시키는 데 도움을 줍니다. 이론적 분석에 따르면, ERW의 효과는, 특히 표현 처리 영역에서 특정 신경망 계층의 정밀한 통합에 의해 확인됩니다. 또한, ERW는 표현 품질을 향상시키면 수렴 속도를 실제로 증명합니다: 실험적으로, 현재의 최선 방법 REPA와 비교하여 훈련 속도를 40배 가속화합니다. 코드는 https://github.com/LINs-lab/ERW에 제공됩니다.",
      "upvotes": 4,
      "discussionId": "67fe602266c0e8f3c2df2334",
      "projectPage": "https://lins-lab.github.io/ERW/",
      "githubRepo": "https://github.com/LINs-lab/ERW",
      "ai_keywords": [
        "diffusion models",
        "high-dimensional data",
        "self-supervised methods",
        "high-quality representations",
        "semantic representations",
        "structural pattern learning",
        "Embedded Representation Warmup (ERW)",
        "warmup",
        "early layers",
        "representation processing region",
        "pretrained representations",
        "convergence",
        "training convergence",
        "representation quality",
        "REPA"
      ]
    },
    "publishedAt": "2025-04-14T08:43:17.000Z",
    "title": "Efficient Generative Model Training via Embedded Representation Warmup",
    "summary": "Diffusion models excel at generating high-dimensional data but fall short in\ntraining efficiency and representation quality compared to self-supervised\nmethods. We identify a key bottleneck: the underutilization of high-quality,\nsemantically rich representations during training notably slows down\nconvergence. Our systematic analysis reveals a critical representation\nprocessing region -- primarily in the early layers -- where semantic and\nstructural pattern learning takes place before generation can occur. To address\nthis, we propose Embedded Representation Warmup (ERW), a plug-and-play\nframework where in the first stage we get the ERW module serves as a warmup\nthat initializes the early layers of the diffusion model with high-quality,\npretrained representations. This warmup minimizes the burden of learning\nrepresentations from scratch, thereby accelerating convergence and boosting\nperformance. Our theoretical analysis demonstrates that ERW's efficacy depends\non its precise integration into specific neural network layers -- termed the\nrepresentation processing region -- where the model primarily processes and\ntransforms feature representations for later generation. We further establish\nthat ERW not only accelerates training convergence but also enhances\nrepresentation quality: empirically, our method achieves a 40times\nacceleration in training speed compared to REPA, the current state-of-the-art\nmethods. Code is available at https://github.com/LINs-lab/ERW.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10188.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d59cec6b4fdd84ebe0d47",
      "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
      "fullname": "Deyuan Liu",
      "name": "SempraETY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11447",
      "authors": [
        {
          "_id": "67ff1026f8afab940cc23f88",
          "name": "An Zhaol",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f89",
          "name": "Shengyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8a",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8b",
          "name": "Zejian Li",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8c",
          "name": "Jiale Wu",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8d",
          "name": "Haoran Xu",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8e",
          "name": "AnYang Wei",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8f",
          "name": "Perry Pengyun GU Lingyun Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:57:13.000Z",
      "submittedOnDailyAt": "2025-04-16T00:35:02.754Z",
      "title": "Diffusion Distillation을 이용한 직접적인 선호 최적화에 의한 효율적인 3D LiDAR 스크린의 완성",
      "submittedOnDailyBy": {
        "_id": "63943c882b9483beb473ec25",
        "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
        "isPro": false,
        "fullname": "Shengyuan Zhang",
        "user": "SYZhang0805",
        "type": "user"
      },
      "summary": "3D LiDAR 쉼의 완료 시의 분산 모델의 적용은 분산의 느린 샘플링 속도로 제한되어 있습니다. 점수의 절약으로 샘플링 속도가 향상되지만 성능이 떨어집니다. 직접 정책 최적화(DPO)를 사용한 훈련 후, 선호 데이터로 성능을 향상시킵니다. 본 논문에서는 선호의 어레이먼트를 가진 새로운 분산의 절약 프레임워크 \"Distillation-DPO\"를 제안합니다. 먼저, 학생 모델은 서로 다른 초기 노이즈로 조합된 조합의 완료 쉼을 생성합니다. 다음으로, LiDAR 쉼 평가 지표로 선호를 설정하여 승패의 샘플 페어를 구축합니다. 이 구축은 합리적이며, 많은 LiDAR 쉼 지표가 정보가 있지만 직접 최적화할 수 없기 때문에 합리적입니다. 또한, Distillation-DPO는 교사 모델과 학생 모델의 점수 함수의 차이를 사용하여 학생 모델을 최적화합니다. 이 절차는 수렴까지 반복됩니다. 광범위한 실험은 상태의 최전단 LiDAR 쉼의 완료 분산 모델과 비교하여, Distillation-DPO는 더 높은 품질의 쉼을 실현하고 5배 이상의 완료 속도를 가속시킵니다. 우리 방식은 선호 학습을 절약에 적용하는 것을 지적으로 시도합니다. 우리 코드는 https://github.com/happyw1nd/DistillationDPO에서 공개되어 있습니다.",
      "upvotes": 3,
      "discussionId": "67ff1027f8afab940cc23fd4",
      "ai_keywords": [
        "diffusion models",
        "LiDAR scene completion",
        "score distillation",
        "direct policy optimization (DPO)",
        "preference alignment",
        "student model",
        "paired completion scenes",
        "LiDAR scene evaluation metrics",
        "winning and losing sample pairs",
        "score functions",
        "preference learning"
      ]
    },
    "publishedAt": "2025-04-15T13:57:13.000Z",
    "title": "Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion",
    "summary": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11447.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63943c882b9483beb473ec25",
      "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
      "fullname": "Shengyuan Zhang",
      "name": "SYZhang0805",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11326",
      "authors": [
        {
          "_id": "67ff25b765b52d1b69c1f6c1",
          "user": {
            "_id": "67ff29ecbf6889a333c69c7a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
            "isPro": false,
            "fullname": "Henghui Ding",
            "user": "HenghuiDing",
            "type": "user"
          },
          "name": "Henghui Ding",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T03:55:17.825Z",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c2",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c3",
          "name": "Nikhila Ravi",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c4",
          "name": "Shuting He",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c5",
          "name": "Yunchao Wei",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c6",
          "name": "Song Bai",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c7",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c8",
          "name": "Kehuan Song",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c9",
          "name": "Xinglin Xie",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6ca",
          "name": "Kexin Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cb",
          "name": "Licheng Jiao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cc",
          "name": "Lingling Li",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cd",
          "name": "Shuyuan Yang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6ce",
          "name": "Xuqiang Cao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cf",
          "name": "Linnan Zhao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d0",
          "name": "Jiaxuan Zhao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d1",
          "name": "Fang Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d2",
          "name": "Mengjiao Wang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d3",
          "name": "Junpei Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d4",
          "name": "Xu Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d5",
          "name": "Yuting Yang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d6",
          "name": "Mengru Ma",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d7",
          "name": "Hao Fang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d8",
          "name": "Runmin Cong",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d9",
          "name": "Xiankai Lu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6da",
          "name": "Zhiyang Che",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6db",
          "name": "Wei Zhan",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6dc",
          "name": "Tianming Liang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6dd",
          "name": "Haichao Jiang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6de",
          "name": "Wei-Shi Zheng",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6df",
          "name": "Jian-Fang Hu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e0",
          "name": "Haobo Yuan",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e1",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:58.429Z",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e2",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e3",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e4",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:02:47.000Z",
      "submittedOnDailyAt": "2025-04-16T02:26:49.281Z",
      "title": "PVUW 2025 도전보고서：자연 속의 복잡한 비디오의 픽셀 수준 이해의 발전",
      "submittedOnDailyBy": {
        "_id": "67ff29ecbf6889a333c69c7a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
        "isPro": false,
        "fullname": "Henghui Ding",
        "user": "HenghuiDing",
        "type": "user"
      },
      "summary": "이 보고서는 2025년 CVPR에 함께 개최된 4차례의 Pixel-level Video Understanding in the Wild (PVUW) 챌린지에 대한 상세한 개요를 제공합니다. 이 보고서는 챌린지의 결과를 요약하고 참가 방식, 향후 연구 방향에 대한 정보로 구성되어 있습니다. 챌린지는 두 개의 트랙을 제시하고 있습니다. 하나는 복잡한 장면의 비디오 객체 분할을 중점적으로 다루는 MOSE, 다른 하나는 움직임을 가이드로 하는 언어 기반의 비디오 분할에 대한 연구를 진행하고 있습니다. 두 트랙 모두 실제 세계적인 스캔 라이트에 더 정확한 반영을 위해 새로운, 더 어려운 데이터셋을 소개하고 있습니다. 세부적인 평가와 분석을 통해 챌린지는 복잡한 비디오 분할의 현재의 최상급 기술과新兴의 트렌드에 대한 유익한 지침을 제공하고 있습니다. 더 많은 정보가 있습니다. https://pvuw.github.io/ 에 찾을 수 있습니다.",
      "upvotes": 3,
      "discussionId": "67ff25b865b52d1b69c1f736"
    },
    "publishedAt": "2025-04-15T12:02:47.000Z",
    "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild",
    "summary": "This report provides a comprehensive overview of the 4th Pixel-level Video\nUnderstanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025.\nIt summarizes the challenge outcomes, participating methodologies, and future\nresearch directions. The challenge features two tracks: MOSE, which focuses on\ncomplex scene video object segmentation, and MeViS, which targets\nmotion-guided, language-based video segmentation. Both tracks introduce new,\nmore challenging datasets designed to better reflect real-world scenarios.\nThrough detailed evaluation and analysis, the challenge offers valuable\ninsights into the current state-of-the-art and emerging trends in complex video\nsegmentation. More information can be found on the workshop website:\nhttps://pvuw.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11326.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ff29ecbf6889a333c69c7a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
      "fullname": "Henghui Ding",
      "name": "HenghuiDing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.06949",
      "authors": [
        {
          "_id": "67ff12ea58ed263257af79b5",
          "name": "Zhixuan Lin",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b6",
          "name": "Johan Obando-Ceron",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b7",
          "user": {
            "_id": "66906c4e37eadb9c577984d3",
            "avatarUrl": "/avatars/b81765472942fdf94c0ee885ca62df2d.svg",
            "isPro": false,
            "fullname": "Owen He",
            "user": "littleowen",
            "type": "user"
          },
          "name": "Xu Owen He",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T02:16:11.020Z",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b8",
          "name": "Aaron Courville",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T14:57:55.000Z",
      "submittedOnDailyAt": "2025-04-16T01:28:49.703Z",
      "title": "Adaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdaptive Computation Pruning for the Forgetting Transformer\n\nAdapt",
      "submittedOnDailyBy": {
        "_id": "6694cc1009326cb83f2d11bb",
        "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
        "isPro": false,
        "fullname": "Zhixuan Lin",
        "user": "zhixuan-lin",
        "type": "user"
      },
      "summary": "최근 제안된 Forgetting Transformer (FoX)는 softmax attention에 잊힘 게이트를 삽입하여, 표준적인 RoPE 기반 Transformer와 비교하여, 일관된 성능을 보입니다. 특히, FoX의 많은 注意 헤드는 잊힘이 빠르고, 각 시간 단계에서의 출력은 주로 지역적인 컨텍스트에 의존합니다. 이러한 관찰에 기반하여, 우리는 FoX에 대한 Adaptive Computation Pruning (ACP)를 제안합니다. ACP는 잊힘 게이트를 통해 강하게 감소된 입력-출력 의존관계에 대한 계산을 동적으로 제거하는 방법입니다. ACP는 동적으로 설정되는 축소 임계값을 사용하여, 축소된 注意 가중치는 미시적으로 보이지 않도록 합니다. ACP는 FoX를 사용한 언어 모델의 사전 학습에 적용되어, 다양한 모델 크기와 컨텍스트 길이로 softmax attention의 FLOP 수를 약 70% 감소시킵니다. 훈련 태스크의 트랜스폼 스텝에 약 10%에서 35%의 개선을 얻을 수 있습니다. 또한, 긴 컨텍스트 길이는 더 큰 계산 비용 절감을 얻을 수 있습니다. 이러한 속도 향상은 성능 저하로 인한 영향을 받지 않도록 구현되어 있습니다. 또한, 우리는 축소 패턴의 검토 및 서로 다른 注意 헤드의 FLOP 축소의 분포 분석 등 다양한 분석을 수행하여, 방법론에 깊은 엔진을 제공했습니다. 코드는 https://github.com/zhixuan-lin/arctic-fox에서 사용 가능합니다.",
      "upvotes": 3,
      "discussionId": "67ff12eb58ed263257af79fc",
      "ai_keywords": [
        "Forgetting Transformer (FoX)",
        "forget gate",
        "softmax attention",
        "RoPE-based Transformer",
        "Adaptive Computation Pruning (ACP)",
        "input-output dependencies",
        "pruning threshold",
        "FLOPs",
        "training throughput",
        "pruning patterns"
      ]
    },
    "publishedAt": "2025-04-09T10:57:55.000Z",
    "title": "Adaptive Computation Pruning for the Forgetting Transformer",
    "summary": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate\ninto softmax attention and has shown consistently better or on-par performance\ncompared to the standard RoPE-based Transformer. Notably, many attention heads\nin FoX tend to forget quickly, causing their output at each timestep to rely\nprimarily on the local context. Based on this observation, we propose Adaptive\nComputation Pruning (ACP) for FoX, a method that dynamically prunes\ncomputations involving input-output dependencies that are strongly decayed by\nthe forget gate. This is achieved using a dynamically set pruning threshold\nthat ensures that the pruned attention weights remain negligible. We apply ACP\nto language model pretraining with FoX and show it consistently reduces the\nnumber of FLOPs in softmax attention by around 70% across different model sizes\nand context lengths, resulting in a roughly 10% to 35% improvement in training\nthroughput. Furthermore, longer context lengths yield greater computational\nsavings. All these speed improvements are achieved without any performance\ndegradation. We also perform several analyses to provide deeper insights into\nour method, such as examining the pruning patterns and analyzing the\ndistribution of FLOP savings across different attention heads. Our code is\navailable at https://github.com/zhixuan-lin/arctic-fox.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6694cc1009326cb83f2d11bb",
      "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
      "fullname": "Zhixuan Lin",
      "name": "zhixuan-lin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11456",
      "authors": [
        {
          "_id": "67ff79b3d68757d92e9c168e",
          "name": "Zhiwei He",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c168f",
          "name": "Tian Liang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1690",
          "name": "Jiahao Xu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1691",
          "name": "Qiuzhi Liu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1692",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1693",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1694",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1695",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1696",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1697",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1698",
          "name": "Zhuosheng Zhang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1699",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c169a",
          "name": "Zhaopeng Tu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c169b",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c169c",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:59:51.000Z",
      "submittedOnDailyAt": "2025-04-16T08:09:13.393Z",
      "title": "DeepMath-103K: 대규모, 어려워서, 오염되지 않은, 검증 가능한 수학 데이터 세트로, 논리론을 발전시킴",
      "submittedOnDailyBy": {
        "_id": "60107b385ac3e86b3ea4fc34",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
        "isPro": true,
        "fullname": "Daniel van Strien",
        "user": "davanstrien",
        "type": "user"
      },
      "summary": "복잡한 수학적 논리 능력은 인공지능의 중요한 벤치마크입니다. 강화학습(RL)을 LLM에 적용한 것은 바람직한 결과를 보여주지만, 발전은 대규모 훈련 데이터의 부족, 이 데이터가 충분히 어려워, RL에 적합한 증명 가능한 답의 형식을 갖추지 못하며, 평가 벤치마크에 의한 오염을 제거할 수 없기 때문에 큰 한계로 막혀 있습니다. 이러한 제한을 해결하기 위해 DeepMath-103K를 소개합니다. DeepMath-103K는 약 103K의 수학 문제를 포함하는 새로운 대규모 데이터 세트입니다. 이는 RL을 통해 높은 논리 모델을 훈련하기 위해 특별히 설계되었습니다. DeepMath-103K는 소스 데이터 분석, 여러 벤치마크에 대한 엄격한 오염 제거, 고난도(주로 레벨 5-9)의 필터링으로 미세하게 조정되어 있습니다. 각 문제에는 증명 가능한 최종적인 답이 포함되어 있으며, 규칙 기반의 RL을 가능하게 합니다. 또한 3가지의 다른 R1 생성된 해결책이 포함되어 있으며, 다양한 훈련 패러다임(예: 관찰 학습 피니밍, 스킵)에 적합합니다. 수학적 주제의 폭을 넓히고, DeepMath-103K는 일반화 가능한 논리의 개발을 촉진합니다. DeepMath-103K를 훈련시킨 모델은 어려운 수학 벤치마크에서 상당한 향상을 보여주고, 그 효과를 증명했습니다. DeepMath-103K는 공개적으로 릴리즈되어, 더 강력한 AI 논리 시스템의 구축에 연결되는 커뮤니티의 발전을 촉진하는 것을 목표로 합니다: https://github.com/zwhe99/DeepMath.",
      "upvotes": 2,
      "discussionId": "67ff79b4d68757d92e9c16e1",
      "githubRepo": "https://github.com/zwhe99/DeepMath",
      "ai_keywords": [
        "reinforcement learning",
        "large-scale dataset",
        "mathematical problems",
        "training data",
        "verifiable answer formats",
        "decontamination",
        "benchmark",
        "curriculum learning",
        "rule-based RL",
        "supervised fine-tuning",
        "distillation",
        "generalizable reasoning",
        "AI reasoning systems"
      ]
    },
    "publishedAt": "2025-04-15T13:59:51.000Z",
    "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning",
    "summary": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11456.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60107b385ac3e86b3ea4fc34",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
      "fullname": "Daniel van Strien",
      "name": "davanstrien",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 586
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11001",
      "authors": [
        {
          "_id": "67ff4bdb1dc5d56fdd7a1bc4",
          "user": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "isPro": false,
            "fullname": "Alan Dao",
            "user": "alandao",
            "type": "user"
          },
          "name": "Alan Dao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T06:19:08.903Z",
          "hidden": false
        },
        {
          "_id": "67ff4bdb1dc5d56fdd7a1bc5",
          "name": "Thinh Le",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/s2uxHvFhyqjBXMY5wkDLy.mp4"
      ],
      "publishedAt": "2025-04-15T09:18:21.000Z",
      "submittedOnDailyAt": "2025-04-16T04:49:41.555Z",
      "title": "ReZero: LLM의 검색 능력을 향상시키기 위해 모노마시즘을 시도하는 방법",
      "submittedOnDailyBy": {
        "_id": "62d7b2339b629105a5d6888a",
        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
        "isPro": false,
        "fullname": "Alan Dao",
        "user": "alandao",
        "type": "user"
      },
      "summary": "レタイブレードアウガーデーション(RAG)는 지식밀집 태스크에서 대규모 언어 모델(LLM)의 성능을 향상시키는 데 사용되고 있지만, 초기 검색 키워드의 품질에 크게 의존합니다. 현재의 방법들은 일반적으로 강화학습(RL)을 사용하여, 검색 키워드의 구성이나 검색 결과를 초점을 두고 있습니다만, 검색 실패 후의 재검색을 명시적으로 촉구하는 것은 없습니다. 우리는 새로운 RL 프레임워크인 ReZero(리토레이저로)를 소개합니다. ReZero는 초기 검색 실패 후의 재검색을 직접 보상하며, LLM이 대체 검색 키워드를 탐색하도록 유도합니다. ReZero는 기준치 25%보다 46.88%의 정확도를 달성하고, 초기 검색 키워드가 충분하지 않은 복잡한 정보 탐색 시나리오에서 LLM의 강건성을 향상시킵니다.",
      "upvotes": 2,
      "discussionId": "67ff4bdc1dc5d56fdd7a1c36",
      "githubRepo": "https://github.com/menloresearch/ReZero",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "Large Language Model (LLM)",
        "knowledge-intensive tasks",
        "Reinforcement Learning (RL)",
        "query formulation",
        "reasoning over results",
        "ReZero (Retry-Zero)",
        "persistence",
        "search query",
        "unsuccessful attempt",
        "alternative queries",
        "robustness",
        "information-seeking scenarios"
      ]
    },
    "publishedAt": "2025-04-15T05:18:21.000Z",
    "title": "ReZero: Enhancing LLM search ability by trying one-more-time",
    "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/s2uxHvFhyqjBXMY5wkDLy.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d7b2339b629105a5d6888a",
      "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
      "fullname": "Alan Dao",
      "name": "alandao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10049",
      "authors": [
        {
          "_id": "67ff5b335cf0fe153845d1c9",
          "user": {
            "_id": "60d35154d7b174177faabd55",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
            "isPro": false,
            "fullname": "Théo Gigant",
            "user": "gigant",
            "type": "user"
          },
          "name": "Théo Gigant",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:49.462Z",
          "hidden": false
        },
        {
          "_id": "67ff5b335cf0fe153845d1ca",
          "name": "Camille Guinaudeau",
          "hidden": false
        },
        {
          "_id": "67ff5b335cf0fe153845d1cb",
          "name": "Frédéric Dufaux",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T09:55:01.000Z",
      "submittedOnDailyAt": "2025-04-16T05:55:17.253Z",
      "title": "모달과 구조의 영향에 대한 다 모델 표현의 요약",
      "submittedOnDailyBy": {
        "_id": "60d35154d7b174177faabd55",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
        "isPro": false,
        "fullname": "Théo Gigant",
        "user": "gigant",
        "type": "user"
      },
      "summary": "비지온・런그저드 모델(VLMs)는 시각적이고 텍스트적인 정보의 여러 형식을 처리할 수 있습니다: 텍스트, 이미지, 텍스트와 이미지의 교차, 또는 약 1시간의 비디오. 본 연구에서는, VLMs를 사용하여 여러 모드의 프레젠테이션의 자동 요약에 대한 세부적인 양적 및 질적 분석을 수행합니다. 이러한 실험에서, VLMs를 사용하여, 풍부한 텍스트가 여러 모드의 문서에서 요약을 생성하는 비용 적절한 전략을 제안합니다. 또한, 비디오 스트리밍에서 추출된 슬라이드와 원본 비디오를 비교하여 활용하는 것이 유리함을 보여주고, 교차된 슬라이드와 텍스트로부터 구조화된 표현이 최고의 성능을 제공합니다. 마지막으로, 여러 모드의 프레젠테이션의 크로스 모드 상호작용의 특성에 대한 반성 및 개선을 위해 조언을 제공합니다.",
      "upvotes": 2,
      "discussionId": "67ff5b355cf0fe153845d215",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "automatic summarization",
        "multimodal presentations",
        "text-heavy multimodal documents",
        "input-length budgets",
        "slides",
        "video stream",
        "raw video",
        "structured representation",
        "interleaved slides",
        "transcript",
        "cross-modal interactions"
      ]
    },
    "publishedAt": "2025-04-14T05:55:01.000Z",
    "title": "Summarization of Multimodal Presentations with Vision-Language Models:\n  Study of the Effect of Modalities and Structure",
    "summary": "Vision-Language Models (VLMs) can process visual and textual information in\nmultiple formats: texts, images, interleaved texts and images, or even\nhour-long videos. In this work, we conduct fine-grained quantitative and\nqualitative analyses of automatic summarization of multimodal presentations\nusing VLMs with various representations as input. From these experiments, we\nsuggest cost-effective strategies for generating summaries from text-heavy\nmultimodal documents under different input-length budgets using VLMs. We show\nthat slides extracted from the video stream can be beneficially used as input\nagainst the raw video, and that a structured representation from interleaved\nslides and transcript provides the best performance. Finally, we reflect and\ncomment on the nature of cross-modal interactions in multimodal presentations\nand share suggestions to improve the capabilities of VLMs to understand\ndocuments of this nature.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10049.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d35154d7b174177faabd55",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
      "fullname": "Théo Gigant",
      "name": "gigant",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08846",
      "authors": [
        {
          "_id": "67ff25f33026f8abc4c4a10d",
          "name": "Mostafa Faghih Shojaei",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a10e",
          "name": "Rahul Gulati",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a10f",
          "name": "Benjamin A. Jasperson",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a110",
          "name": "Shangshang Wang",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a111",
          "user": {
            "_id": "6729342804227d5ea3b283c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
            "isPro": false,
            "fullname": "Simone Cimolato",
            "user": "simocimolato",
            "type": "user"
          },
          "name": "Simone Cimolato",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:55.794Z",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a112",
          "user": {
            "_id": "672ad19141a93b8e140e8689",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VRMw7_PIE5QoD_eWP0hOp.png",
            "isPro": false,
            "fullname": "Dangli Cao",
            "user": "Dinzhenzhenzhu",
            "type": "user"
          },
          "name": "Dangli Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:14.535Z",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a113",
          "name": "Willie Neiswanger",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a114",
          "user": {
            "_id": "67859d73e670c62966ba5767",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mx3fE-YPsjsxpYsBh_DDn.png",
            "isPro": false,
            "fullname": "Krishna Garikipati",
            "user": "garikipati",
            "type": "user"
          },
          "name": "Krishna Garikipati",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-16T05:32:49.978Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T01:26:34.000Z",
      "submittedOnDailyAt": "2025-04-16T07:21:37.722Z",
      "title": "AI 대학： 과학 클래스랜드를 위한 인스톰션 어라인먼트 플랫폼",
      "submittedOnDailyBy": {
        "_id": "6729342804227d5ea3b283c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
        "isPro": false,
        "fullname": "Simone Cimolato",
        "user": "simocimolato",
        "type": "user"
      },
      "summary": "AI 대학(AI-U)는 AI를 활용한 코스 콘텐츠의 배달에 유연한 프레임워크를 제공하며, 교사의 교수법을 적응시킵니다. 핵심은 AI-U가 검색 어거지 디지엔레레이션(RAG)를 사용하여 대규모 언어 모델(LLM)을 미세 조정하여, 강의 비디오, 노트, 교과서에 맞는 응답을 생성하는 것입니다. 고급 수준의 유한 요소법(FEM) 코스를 사례 연구로, 체계적으로 훈련 데이터 구축, 오픈 소스 LLM을 Low-Rank Adaptation(LoRA)로 미세 조정하고, RAG를 통해 합성하여 응답을 최적화하는 Scalable Pipeline을 제안합니다. 코사인 유사도, LLM 기반 평가, 전문가의 리뷰가 결합된 평가에 따라, 코스 콘텐츠와 강한 적합성이 입증됩니다. 또한 특정 코스 콘텐츠의 섹션과 시간 스탬프付여의 오픈 액세스 비디오 강의를 연계하여, 추적 기반을 강화하는 프로토 타입의 Web 애플리케이션「https://my-ai-university.com」가 개발되었습니다. 전문가 모델은 86%의 테스트 케이스에서 참조와의 코사인 유사도가 높고, LLM 평가자는 거의 5회 중 4회까지 기본 Llama 3.2 모델을 초과한 성능을 보여주었습니다. AI-U는 AI 디스크리미네이션 교육의 Scalable Approach를 제공하며, 고등교육에서 더 광범위하게 도입을 촉진합니다. 여기에서는 FEM 코스의 프레임워크를 소개하지만, 이 설정은 과학 연구 콘텐츠에 대한 LLM의 미세 조정의 광범위한 컨텍스트의 특정 예시입니다.",
      "upvotes": 2,
      "discussionId": "67ff25f43026f8abc4c4a16c",
      "projectPage": "https://my-ai-university.com",
      "githubRepo": "https://github.com/my-ai-university/finite-element-method",
      "ai_keywords": [
        "large language model (LLM)",
        "retrieval-augmented generation (RAG)",
        "finite-element-method (FEM)",
        "Low-Rank Adaptation (LoRA)",
        "RAG-based synthesis",
        "cosine similarity",
        "LLM-based assessment",
        "traceability",
        "base Llama 3.2 model"
      ]
    },
    "publishedAt": "2025-04-10T21:26:34.000Z",
    "title": "AI-University: An LLM-based platform for instructional alignment to\n  scientific classrooms",
    "summary": "We introduce AI University (AI-U), a flexible framework for AI-driven course\ncontent delivery that adapts to instructors' teaching styles. At its core, AI-U\nfine-tunes a large language model (LLM) with retrieval-augmented generation\n(RAG) to generate instructor-aligned responses from lecture videos, notes, and\ntextbooks. Using a graduate-level finite-element-method (FEM) course as a case\nstudy, we present a scalable pipeline to systematically construct training\ndata, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and\noptimize its responses through RAG-based synthesis. Our evaluation - combining\ncosine similarity, LLM-based assessment, and expert review - demonstrates\nstrong alignment with course materials. We also have developed a prototype web\napplication, available at https://my-ai-university.com, that enhances\ntraceability by linking AI-generated responses to specific sections of the\nrelevant course material and time-stamped instances of the open-access video\nlectures. Our expert model is found to have greater cosine similarity with a\nreference on 86% of test cases. An LLM judge also found our expert model to\noutperform the base Llama 3.2 model approximately four times out of five. AI-U\noffers a scalable approach to AI-assisted education, paving the way for broader\nadoption in higher education. Here, our framework has been presented in the\nsetting of a class on FEM - a subject that is central to training PhD and\nMaster students in engineering science. However, this setting is a particular\ninstance of a broader context: fine-tuning LLMs to research content in science.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6729342804227d5ea3b283c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
      "fullname": "Simone Cimolato",
      "name": "simocimolato",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09454",
      "authors": [
        {
          "_id": "67ff6f8c661b74d0050d2774",
          "name": "Weinan Jia",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2775",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2776",
          "name": "Nan Chen",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2777",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2778",
          "name": "Zhendong Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T06:33:28.000Z",
      "submittedOnDailyAt": "2025-04-16T07:21:45.180Z",
      "title": "D^2iT: 동적 분포 채널 라이브러리 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로워 팔로",
      "submittedOnDailyBy": {
        "_id": "630636bcd37ce67e0e4d1d42",
        "avatarUrl": "/avatars/c3ba695d339eaaf9e810bfa0d9a7689a.svg",
        "isPro": false,
        "fullname": "Mengqi Huang",
        "user": "CoreloneH",
        "type": "user"
      },
      "summary": "디피유션 모달은 고품질의 이미지 생성 능력을 가지고 있어 광범위하게 인정받고 있습니다. 디피유션 트랜스포머(DiT) 아키텍처의 우수한 성능과 scalability를 기반으로 하지만, 디피유션 프로세스 중에는 서로 다른 이미지 영역에 고정된 압축을 적용하여, 이러한 영역에 존재하는 자연스러운 변동적인 정보 밀도를 무시하고 있습니다. 그러나 큰 압축은 지역적인 사진 품질을 제한하고, 작은 압축은 계산 복잡성을 증가시키고, 글로벌 일관성을 파괴하고, 최종적으로 생성된 이미지의 품질을 영향을 미칩니다. 이러한 제한을 해결하기 위해, 우리는 서로 다른 영역의 중요성을 인식하고, 서로 다른 영역을 동적으로 압축하여 이미지 생성의 효율성과 효율성을 향상시키기 위한 새로운 2단계 프레임워크를 제안합니다. 먼저, 동적인 VAE(DVAE)는 특정 정보 밀도에 맞는 다양한 다운 샘플링 레이트를 사용하여, 서로 다른 이미지 영역을 하이라이트 인코더로 압축하여, 디피유션 프로세스에 의해 더 정확한 자연스러운 잠재 코드를 제공합니다. 다음으로, 동적인 디피유션 트랜스포머(D^2iT)는 동적인 그레이드 트랜스포머와 동적인 콘텐츠 트랜스포머의 새로운 조합을 통해, 코어 엘스(평활한 영역에서 적은 잠재 코드)와 피니트 엘스(상세한 영역에서 많은 잠재 코드)를 포함하는 다색 노이즈를 예측하여 이미지를 생성합니다. 노이즈의 거대한 예측과 세부적인 영역의 보정의 조합은 글로벌 일관성과 지역적인 사진 품질의 통합을 실현합니다. 다양한 생성 태스크에 대한 세부적인 실험을 수행하여, 우리의 접근 방식의 효과성을 파악했습니다. 코드는 https://github.com/jiawn-creator/Dynamic-DiT에서 릴리즈됩니다.",
      "upvotes": 1,
      "discussionId": "67ff6f8f661b74d0050d28a9",
      "ai_keywords": [
        "Diffusion models",
        "Diffusion Transformer (DiT)",
        "compression",
        "image regions",
        "information densities",
        "local realism",
        "computational complexity",
        "global consistency",
        "generated images",
        "Dynamic VAE (DVAE)",
        "hierarchical encoder",
        "downsampling rates",
        "latent codes",
        "Dynamic Diffusion Transformer (D$^2$iT)",
        "multi-grained noise",
        "coarse-grained",
        "fine-grained",
        "Dynamic Grain Transformer",
        "Dynamic Content Transformer",
        "rough prediction",
        "detailed regions correction"
      ]
    },
    "publishedAt": "2025-04-13T02:33:28.000Z",
    "title": "D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation",
    "summary": "Diffusion models are widely recognized for their ability to generate\nhigh-fidelity images. Despite the excellent performance and scalability of the\nDiffusion Transformer (DiT) architecture, it applies fixed compression across\ndifferent image regions during the diffusion process, disregarding the\nnaturally varying information densities present in these regions. However,\nlarge compression leads to limited local realism, while small compression\nincreases computational complexity and compromises global consistency,\nultimately impacting the quality of generated images. To address these\nlimitations, we propose dynamically compressing different image regions by\nrecognizing the importance of different regions, and introduce a novel\ntwo-stage framework designed to enhance the effectiveness and efficiency of\nimage generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical\nencoder to encode different image regions at different downsampling rates,\ntailored to their specific information densities, thereby providing more\naccurate and natural latent codes for the diffusion process. (2) Dynamic\nDiffusion Transformer (D^2iT) at second stage generates images by predicting\nmulti-grained noise, consisting of coarse-grained (less latent code in smooth\nregions) and fine-grained (more latent codes in detailed regions), through an\nnovel combination of the Dynamic Grain Transformer and the Dynamic Content\nTransformer. The strategy of combining rough prediction of noise with detailed\nregions correction achieves a unification of global consistency and local\nrealism. Comprehensive experiments on various generation tasks validate the\neffectiveness of our approach. Code will be released at\nhttps://github.com/jiawn-creator/Dynamic-DiT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630636bcd37ce67e0e4d1d42",
      "avatarUrl": "/avatars/c3ba695d339eaaf9e810bfa0d9a7689a.svg",
      "fullname": "Mengqi Huang",
      "name": "CoreloneH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]