[
  {
    "paper": {
      "id": "2507.01949",
      "authors": [
        {
          "_id": "6865e6218c83dab5f72d1e47",
          "name": "Kwai Keye Team",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e48",
          "name": "Biao Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e49",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4a",
          "name": "Changyi Liu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4b",
          "name": "Chenglong Chu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4c",
          "name": "Chengru Song",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4d",
          "name": "Chongling Rao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4e",
          "name": "Chuan Yi",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4f",
          "name": "Da Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e50",
          "name": "Dunju Zang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e51",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e52",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e53",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e54",
          "name": "Haojie Ding",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e55",
          "name": "Jiaming Huang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e56",
          "user": {
            "_id": "65802e81b701ff85a37caba8",
            "avatarUrl": "/avatars/b2e9726893caa7e62aad83b1d02e5b41.svg",
            "isPro": false,
            "fullname": "jiangxia cao",
            "user": "caojiangxia",
            "type": "user"
          },
          "name": "Jiangxia Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:44.694Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e57",
          "name": "Jiankang Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e58",
          "user": {
            "_id": "61540338e5b9ae6774201e58",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61540338e5b9ae6774201e58/p_minqil1sdiqg5wEVxT5.jpeg",
            "isPro": false,
            "fullname": "jingyun",
            "user": "hjy",
            "type": "user"
          },
          "name": "Jingyun Hua",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:15:01.253Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e59",
          "name": "Jin Ouyang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5a",
          "name": "Kaibing Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5b",
          "name": "Kaiyu Jiang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5c",
          "name": "Kaiyu Tang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5d",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5e",
          "name": "Shengnan Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5f",
          "name": "Siyang Mao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e60",
          "name": "Sui Huang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e61",
          "name": "Tianke Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e62",
          "user": {
            "_id": "68652063e29f1407b58da60f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hTgS65zsKRPxELjSMDSNm.png",
            "isPro": false,
            "fullname": "tingting gao",
            "user": "TinaGao",
            "type": "user"
          },
          "name": "Tingting Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:47:43.679Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e63",
          "name": "Wei Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e64",
          "user": {
            "_id": "65423daba385933e812516d5",
            "avatarUrl": "/avatars/d18b85b4206ab5905ef5bc95622dff3e.svg",
            "isPro": false,
            "fullname": "wei yuan",
            "user": "yw95",
            "type": "user"
          },
          "name": "Wei Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:49.219Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e65",
          "name": "Xiangyu Wu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e66",
          "user": {
            "_id": "64a4dba8fe950993d2d89113",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a4dba8fe950993d2d89113/yukb9NNeVspIX7eFTreUq.jpeg",
            "isPro": false,
            "fullname": "Xiao Hu",
            "user": "huxiao09",
            "type": "user"
          },
          "name": "Xiao Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:42.172Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e67",
          "user": {
            "_id": "673597cfc2424474d12ca58c",
            "avatarUrl": "/avatars/f748f19619b07838a66bc419a7a6db9d.svg",
            "isPro": false,
            "fullname": "xingyulu",
            "user": "Xingyulu47",
            "type": "user"
          },
          "name": "Xingyu Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:46.882Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e68",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e69",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yi-Fan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:15:06.461Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6a",
          "name": "Yiping Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6b",
          "name": "Yulong Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6c",
          "name": "Zhenhua Wu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6d",
          "name": "Zhenyu Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6e",
          "user": {
            "_id": "641948b7d13ffa40812eb239",
            "avatarUrl": "/avatars/65a0262fea6907bec48ddc1d966742da.svg",
            "isPro": false,
            "fullname": "Zhixin Ling",
            "user": "NamingIsTroublesome",
            "type": "user"
          },
          "name": "Zhixin Ling",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:15:03.454Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6f",
          "name": "Ziming Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e70",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e71",
          "name": "Di Xu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e72",
          "name": "Haixuan Gao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e73",
          "name": "Hang Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e74",
          "name": "Jiawei Guo",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e75",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e76",
          "name": "Lejian Ren",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e77",
          "name": "Muhao Wei",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e78",
          "name": "Qianqian Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e79",
          "name": "Qigen Hu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7a",
          "name": "Shiyao Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7b",
          "name": "Tao Yu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7c",
          "name": "Xinchen Luo",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7d",
          "name": "Yan Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7e",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7f",
          "name": "Yuhang Hu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e80",
          "name": "Zeyi Lu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e81",
          "name": "Zhuoran Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e82",
          "name": "Zixing Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:57:28.000Z",
      "submittedOnDailyAt": "2025-07-03T00:40:58.759Z",
      "title": "Kwai Keye-VL 기술보고서\n\nKwai Keye-VL 기술보고서는 Kwai Keye-VL 기술에 대한 상세한 정보를 제공하며, 이 기술의 핵심 원리, 기능, 그리고 실제 적용 사례를 자세히 설명합니다. 이 보고서는 Kwai Keye-VL 기술의 발전과 발전의 방향을 파악하는 데 필수적인 정보로, 다양한 산업 분야에서 활용될 수 있는 기술에 대한 심도있는 분석을 제공합니다.",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "마르치 모달 대언어 모뎀(MLLMs)는 정적 이미지에서 놀라운 능력을 보여주지만, 정보 풍부한 짧은 동영상에 대한 이해력이 부족합니다. 이러한 공백을 메우기 위해, 우리는 Kwai Keye-VL를 소개합니다. Kwai Keye-VL는 짧은 동영상의 이해에 가장 先端의 성능을 유지하면서, 강력한 일반적인 시각 언어 능력을 가진 8 바이로이온 파라미터 마르치 모달 베이스 모뎀입니다. Keye-VL의 개발은 큰 규모의 고품질 데이터 세트와 고유의 학습 프로세스의 두 가지 핵심 기둥에 기반합니다. 데이터 세트는 6000억 토큰을 초과하고, 동영상에 특히 중점을 둡니다. 학습 프로세스는 4단계의 사전 학습 프로세스와 2단계의 후 학습 프로세스로 구성되며, 첫 번째 후 학습 단계에서는 지시 따라이와 기초적인 능력을 향상시키고, 두 번째 단계에서는 발전적인 논리론을 촉발시키는 것을 중점으로 합니다. 두 번째 단계에서는, \"생각\", \"비사고\", \"자동 사고\", \"그림과 동시에 사고\", 고품질의 동영상 데이터를 포함하는 5가지 모드의 \"냉정한 시작\" 데이터 세트가 중요한 혁신으로 작용합니다. 이 데이터 세트는 모뎀이 언제, 어떻게 논리론을 수행할 지 학습받습니다. 후속의 강화 학습(RL)과 어레이먼트 스텝은 이러한 논리론 능력을 발전시키고, 반복적인 이상적인 모뎀의 행동을 수정하여 더욱 향상시키는 데 사용됩니다. Keye-VL의 접근 방식의 유효성을 증명하기 위해, 공개된 동영상 벤치마크에서 가장 先端의 결과를 달성하고, 일반적인 이미지 기반의 태스크에서도 높은 경쟁력을 유지하고 있습니다(그림 1). 또한, KC-MMBench라는 새로운 벤치마크를 개발하여, 현실적인 짧은 동영상의 시나리오에 적합한 벤치마크를 제공하고, Keye-VL가 뚜렷한 우위를 보여주는 것을 보여줍니다.",
      "upvotes": 85,
      "discussionId": "6865e6218c83dab5f72d1e83",
      "projectPage": "https://kwai-keye.github.io/",
      "githubRepo": "https://github.com/Kwai-Keye/Keye",
      "githubStars": 365
    },
    "publishedAt": "2025-07-02T13:57:28.000Z",
    "title": "Kwai Keye-VL Technical Report",
    "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce Kwai Keye-VL, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the KC-MMBench, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01945",
      "authors": [
        {
          "_id": "6865e4b88c83dab5f72d1e41",
          "user": {
            "_id": "6629d7c9fa14eaccf07d8633",
            "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
            "isPro": false,
            "fullname": "Nan Chen",
            "user": "CNcreator0331",
            "type": "user"
          },
          "name": "Nan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:51.437Z",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e42",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e43",
          "name": "Yihao Meng",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e44",
          "name": "Zhendong Mao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6629d7c9fa14eaccf07d8633/0WDfsnDIDJ9hzI6iJym9L.mp4"
      ],
      "publishedAt": "2025-07-02T17:55:50.000Z",
      "submittedOnDailyAt": "2025-07-03T00:57:17.831Z",
      "title": "장 애니메이션: 동적 글로벌 로컬 메모리를 이용한 장 애니메이션 생성",
      "submittedOnDailyBy": {
        "_id": "6629d7c9fa14eaccf07d8633",
        "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
        "isPro": false,
        "fullname": "Nan Chen",
        "user": "CNcreator0331",
        "type": "user"
      },
      "summary": "アニメーションの彩色化은 실제의 애니메이션 산업의 생산에서 중요한 부분이 됩니다. 장기적인 애니메이션의彩色化에 많은 노동 비용이 소요됩니다. 따라서, 비디오 생성 모델에 기반한 장기적인 애니메이션의 자동화된彩色化은 큰 연구 가치가 있습니다. 기존의 연구는 단기간의彩色化에 제한되어 있습니다. 이러한 연구들은 지역적인 패러다임을 도입하여, 중복된 특성을 융합하여 지역적인 Segment 사이의mooth한 이동을 실현합니다. 그러나 지역적인 패러다임은 글로벌 정보를 무시하고 장기적인彩色의 일관성을 유지할 수 없다고 지적됩니다. 본 연구에서는, ideale한 장기적인彩色의 일관성은 동적인 글로벌-지역적 패러다임에 의해 실현될 수 있다고 주장합니다. 구체적으로, 우리는 LongAnimation이라는 새로운 프레임워크를 제안합니다. 이는 SketchDiT, Dynamic Global-Local Memory (DGLM), Color Consistency Reward를 주요 요소로 구성됩니다. SketchDiT는 통합을 위해 하이브리드 참조 특성을捉え, DGLM 모듈을 지원합니다. DGLM 모듈은 장기적인 비디오 이해 모델을 사용하여, 글로벌의 역사적 특성을 동적으로 압축하고, 현재의 생성 특성과 적응적으로 융합합니다. Color Consistency Reward는 색상의 일관성을 정밀화하기 위해 도입되었습니다. 추론 시에는 색상의 일관성을mooth화하기 위한 색상의 일관성 융합을 제안합니다. 단기간 (14 프레임)과 장기적인 (평균 500 프레임) 애니메이션에 대한 확장된 실험은 LongAnimation이 오픈 도메인의 애니메이션의彩色化 작업에서 단기적인 및 장기적인 색상의 일관성을 유지하는 효과를 보여줍니다. 코드는 https://cn-makers.github.io/long_animation_web/ 에 찾을 수 있습니다.",
      "upvotes": 52,
      "discussionId": "6865e4b88c83dab5f72d1e45",
      "projectPage": "https://cn-makers.github.io/long_animation_web/",
      "githubRepo": "https://github.com/CN-makers/LongAnimation",
      "githubStars": 65
    },
    "publishedAt": "2025-07-02T13:55:50.000Z",
    "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
    "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6629d7c9fa14eaccf07d8633/0WDfsnDIDJ9hzI6iJym9L.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01945.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6629d7c9fa14eaccf07d8633",
      "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
      "fullname": "Nan Chen",
      "name": "CNcreator0331",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01634",
      "authors": [
        {
          "_id": "6865e04b8c83dab5f72d1e2d",
          "user": {
            "_id": "66ef2611fcc1c455f8dce832",
            "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
            "isPro": false,
            "fullname": "Boyuan Sun",
            "user": "BBBBCHAN",
            "type": "user"
          },
          "name": "Boyuan Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:54.412Z",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e2e",
          "name": "Modi Jin",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e2f",
          "name": "Bowen Yin",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e30",
          "name": "Qibin Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T12:05:57.000Z",
      "submittedOnDailyAt": "2025-07-03T00:16:17.577Z",
      "title": "어떤 조건에서도, 어떤 것도 깊게 다루는 것이 가능한다.",
      "submittedOnDailyBy": {
        "_id": "66ef2611fcc1c455f8dce832",
        "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
        "isPro": false,
        "fullname": "Boyuan Sun",
        "user": "BBBBCHAN",
        "type": "user"
      },
      "summary": "DepthAnything at Any Condition (DepthAnything-AC)는 다양한 환경 조건을 처리하는 기초적인 단일 카메라 깊이 추정 모델입니다. 기존의 기초적인 모델은 일반적인 장소에서 놀라울 만한 성능을 보입니다が, 복잡한 개방된 세계 환경에서 조명의 변화, 불리한 날씨, 센서로 인한 이상적인 이상적인 현상이 있습니다. 데이터의 부족과 손상 이미지로부터 고품질의 가상 라벨을 생성할 수 있는 문제에 해결하기 위해, 무 매뉴얼의 일관성 정규화 미세 조정 패러다임을 제안합니다. 또한, 공간 거리 제약을 제안하고 모델이 패치 수준의 상대적인 관계를 학습하도록 명시하여 의미적인 경계가 명확해지고 세부 사항이 정확하게 되도록 합니다. 실험 결과를 통해, DepthAnything-AC의 0샷 능력이 나타났으며, 실제 세계적인 불리한 날씨 벤치마크, 합성적인 손상 벤치마크, 일반적인 벤치마크를 포함하여 다양한 벤치마크에서 보여주었습니다.\n  \n  프로젝트 페이지: https://ghost233lism.github.io/depthanything-AC-page\n  코드: https://github.com/HVision-NKU/DepthAnythingAC",
      "upvotes": 27,
      "discussionId": "6865e04b8c83dab5f72d1e31",
      "projectPage": "https://ghost233lism.github.io/depthanything-AC-page/",
      "githubRepo": "https://github.com/HVision-NKU/DepthAnythingAC",
      "githubStars": 91
    },
    "publishedAt": "2025-07-02T08:05:57.000Z",
    "title": "Depth Anything at Any Condition",
    "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01634.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ef2611fcc1c455f8dce832",
      "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
      "fullname": "Boyuan Sun",
      "name": "BBBBCHAN",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01925",
      "authors": [
        {
          "_id": "686600cf8c83dab5f72d1ed0",
          "name": "Yifan Zhong",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed1",
          "name": "Fengshuo Bai",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed2",
          "user": {
            "_id": "6578459d62d3ac1817ed79fe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578459d62d3ac1817ed79fe/AXDJuwLUoEOb4Fj3U0Xxo.jpeg",
            "isPro": false,
            "fullname": "Shaofei Cai",
            "user": "phython96",
            "type": "user"
          },
          "name": "Shaofei Cai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:14:49.923Z",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed3",
          "user": {
            "_id": "66ee5cf1801ea45d7a44a542",
            "avatarUrl": "/avatars/04bafbcbf1aea3920a79bddbd1a18f42.svg",
            "isPro": false,
            "fullname": "XUCHUAN HUANG",
            "user": "Feernnn",
            "type": "user"
          },
          "name": "Xuchuan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:14:52.141Z",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed4",
          "name": "Zhang Chen",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed5",
          "name": "Xiaowei Zhang",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed6",
          "name": "Yuanfei Wang",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed7",
          "name": "Shaoyang Guo",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed8",
          "name": "Tianrui Guan",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed9",
          "name": "Ka Nam Lui",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1eda",
          "name": "Zhiquan Qi",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1edb",
          "name": "Yitao Liang",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1edc",
          "name": "Yuanpei Chen",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1edd",
          "name": "Yaodong Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:34:52.000Z",
      "submittedOnDailyAt": "2025-07-03T03:39:33.625Z",
      "title": "비전・런그・액션모듈의 조사: 액션 토크나이션의 관점에서",
      "submittedOnDailyBy": {
        "_id": "655d9f43b5da99edaf3f2f81",
        "avatarUrl": "/avatars/c7225b3ed54d099a4fd87682427fb5bf.svg",
        "isPro": false,
        "fullname": "Yifan Zhong",
        "user": "Yifan-Zhong",
        "type": "user"
      },
      "summary": "시각과 언어의 기초 모델의 놀라운 진보가 다양한 이해, 이유, 생성에 영향을 미치고, 물리 세계로의 이러한 지능의 확장으로 시각-언어-행동(VLA) 모델의 번영을 촉진하고 있습니다. 현재의 VLA 모델은 각각 다른 방법을 보여주고 있지만, 우리는 이들을 하나의 프레임워크 아래 통일할 수 있음을 발견했습니다: 시각과 언어의 입력은 VLA 모듈의 연속적인 처리에 의해, 발전적으로 더욱 구체적인 및 행동 가능한 정보를 포함하는 행동 토큰의 연속을 생성하고, 최종적으로 실행 가능한 행동을 생성합니다. 또한, VLA 모델의 주요 설계 선택은 행동 토큰의 구성 방법에 대한 것을 명확히 밝혔습니다. 이들은 언어의 설명, 코드, AFFORDANCE, 경로, 목표 상태, 잠재적 표현, 단순한 행동, 이유 및 분류로 분류됩니다. 그러나 행동 토큰에 대한 자세한 이해가 부족하여, VLA 개발에서 효과적인 진전을 억제하고 미래의 방향을 숨깁니다. 따라서, 이 조사는 행동 토큰화의 관점에서 기존의 VLA 연구를 분류하고 각 토큰의 강점과 한계를 추출하고 개선의 가능성이 있는 영역을 특정하기 위해 목적입니다. 이 시스템적인 평가와 분석을 통해, VLA 모델의 광범위한 진화를 합성적인 관점으로 바라보며, 조사하지 않았지만 잠재력이 있는 방향을 밝혀, 향후 연구에 대한 지침을 제공하고 일반적인 목적의 지능에 가까워지는 것을 희망합니다.",
      "upvotes": 13,
      "discussionId": "686600cf8c83dab5f72d1ede",
      "githubRepo": "https://github.com/Psi-Robot/Awesome-VLA-Papers",
      "githubStars": 21
    },
    "publishedAt": "2025-07-02T13:34:52.000Z",
    "title": "A Survey on Vision-Language-Action Models: An Action Tokenization\n  Perspective",
    "summary": "The remarkable advancements of vision and language foundation models in\nmultimodal understanding, reasoning, and generation has sparked growing efforts\nto extend such intelligence to the physical world, fueling the flourishing of\nvision-language-action (VLA) models. Despite seemingly diverse approaches, we\nobserve that current VLA models can be unified under a single framework: vision\nand language inputs are processed by a series of VLA modules, producing a chain\nof action tokens that progressively encode more grounded and\nactionable information, ultimately generating executable actions. We further\ndetermine that the primary design choice distinguishing VLA models lies in how\naction tokens are formulated, which can be categorized into language\ndescription, code, affordance, trajectory, goal state, latent representation,\nraw action, and reasoning. However, there remains a lack of comprehensive\nunderstanding regarding action tokens, significantly impeding effective VLA\ndevelopment and obscuring future directions. Therefore, this survey aims to\ncategorize and interpret existing VLA research through the lens of action\ntokenization, distill the strengths and limitations of each token type, and\nidentify areas for improvement. Through this systematic review and analysis, we\noffer a synthesized outlook on the broader evolution of VLA models, highlight\nunderexplored yet promising directions, and contribute guidance for future\nresearch, hoping to bring the field closer to general-purpose intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655d9f43b5da99edaf3f2f81",
      "avatarUrl": "/avatars/c7225b3ed54d099a4fd87682427fb5bf.svg",
      "fullname": "Yifan Zhong",
      "name": "Yifan-Zhong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01953",
      "authors": [
        {
          "_id": "686601648c83dab5f72d1ee0",
          "name": "Yukang Cao",
          "hidden": false
        },
        {
          "_id": "686601648c83dab5f72d1ee1",
          "name": "Chenyang Si",
          "hidden": false
        },
        {
          "_id": "686601648c83dab5f72d1ee2",
          "name": "Jinghao Wang",
          "hidden": false
        },
        {
          "_id": "686601648c83dab5f72d1ee3",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:58:20.000Z",
      "submittedOnDailyAt": "2025-07-03T02:40:23.949Z",
      "title": "FreeMorph: 밴드 폼을 사용한 무조정된 확장 이미지 변형 기술",
      "submittedOnDailyBy": {
        "_id": "63a07c3ab5515dccd40fdb71",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
        "isPro": false,
        "fullname": "Yukang Cao",
        "user": "yukangcao",
        "type": "user"
      },
      "summary": "FreeMorph는 처음의 튜닝 제한 없는 이미지 모orph링 방법 중 하나이며, 서로 다른 세ман틱 및 레이아웃을 처리할 수 있습니다. 현재의 방법들은 미리 학습된 디퓨전 모델을 미세 조정해야 하며, 시간 제약과 세ман틱 및 레이아웃의 차이로 제한되어 있습니다. 그러나 FreeMorph는 이러한 제한을 초월하고, 프로젝트 단위의 훈련이 필요하지 않고, 고품질의 이미지 모orph링을 제공합니다. 튜닝 제한 없는 방법들은 다단계 디퓨전 프로세스의 비선형성과 미리 학습된 모델에서 상속받은 바이어스로부터 고품질 유지에 문제가 있습니다. 이 논문에서는 이러한 문제를 해결하기 위해 2개의 핵심 혁신을 결합하여 FreeMorph를 소개합니다. 1) 입력 이미지로부터 명시적인 가이드를 받아들이는 구사형 인터럽터의 설계를 제안하고, 자기주의 모듈을 개선하여 인식성 손실을 보정하고, 생성된 시퀀스 전체에서 방향적인 기회를 보장합니다. 2) 각 입력 이미지에서 얻은 자기주의 모듈을 융합하여, 두 입력에 대한 제어적이고 일관된 기회를 실현하는 단계별 변화 경향을 도입합니다. 광범위한 평가에 따라, FreeMorph는 현재의 방법을 초월하며, 이미지 모orph링의 새로운 최전단 기술로 10배 ~ 50배 빠르게 혁신을 만들었습니다.",
      "upvotes": 9,
      "discussionId": "686601658c83dab5f72d1ee4",
      "projectPage": "https://yukangcao.github.io/FreeMorph/",
      "githubRepo": "https://github.com/yukangcao/FreeMorph",
      "githubStars": 7
    },
    "publishedAt": "2025-07-02T13:58:20.000Z",
    "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
    "summary": "We present FreeMorph, the first tuning-free method for image morphing that\naccommodates inputs with different semantics or layouts. Unlike existing\nmethods that rely on finetuning pre-trained diffusion models and are limited by\ntime constraints and semantic/layout discrepancies, FreeMorph delivers\nhigh-fidelity image morphing without requiring per-instance training. Despite\ntheir efficiency and potential, tuning-free methods face challenges in\nmaintaining high-quality results due to the non-linear nature of the multi-step\ndenoising process and biases inherited from the pre-trained diffusion model. In\nthis paper, we introduce FreeMorph to address these challenges by integrating\ntwo key innovations. 1) We first propose a guidance-aware spherical\ninterpolation design that incorporates explicit guidance from the input images\nby modifying the self-attention modules, thereby addressing identity loss and\nensuring directional transitions throughout the generated sequence. 2) We\nfurther introduce a step-oriented variation trend that blends self-attention\nmodules derived from each input image to achieve controlled and consistent\ntransitions that respect both inputs. Our extensive evaluations demonstrate\nthat FreeMorph outperforms existing methods, being 10x ~ 50x faster and\nestablishing a new state-of-the-art for image morphing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01953.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a07c3ab5515dccd40fdb71",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
      "fullname": "Yukang Cao",
      "name": "yukangcao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01957",
      "authors": [
        {
          "_id": "686633d28c83dab5f72d1f39",
          "name": "Zhuoyang Zhang",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3a",
          "name": "Luke J. Huang",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3b",
          "name": "Chengyue Wu",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3c",
          "name": "Shang Yang",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3d",
          "name": "Kelly Peng",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3e",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3f",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:59:23.000Z",
      "submittedOnDailyAt": "2025-07-03T06:13:07.255Z",
      "title": "지역정보에 대해 알아둔것으로 효율적인 병렬 결정법을 통한 자동단어연쇄화 이미지 생성\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "650e6ab08f3228d807707735",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650e6ab08f3228d807707735/yFo6aLuyGH06t9yG8AOp7.png",
        "isPro": false,
        "fullname": "Zhuoyang Zhang",
        "user": "zhuoyang20",
        "type": "user"
      },
      "summary": "로카리티에 대한 관심 있는 병렬해석 (LPD)를 소개하고, 자동 리듀스 이미지 생성을 가속화합니다. 전통적인 자동 리듀스 이미지 생성은 다음 패치 예측을 기반으로 메모리 프로세싱으로 되어 있으며, 높은 라틴 시를招致합니다. 기존의 연구는 다음 패치 예측을 병렬화하기 위해 다 패치 예측으로 전환하여 처리를 가속화하려고 했지만, 제한된 병렬화를 달성했습니다. 높은 병렬화를 달성하면서 생성 품질을 유지하기 위해 우리는 두 가지 핵심 기술로 진입합니다: (1) 유연한 병렬화된 자동 리듀스 모델링, 임의의 생성 순서와 병렬화 정도를 가능하게 하는 새로운 아키텍처입니다. 학습 가능한 위치 쿼리 토큰을 사용하여 목표 위치에서의 생성을 가이드하고 동시에 생성되는 토큰의 상호 시각을 보장하며, 일관된 병렬해석을 실현합니다. (2) 로카리티에 대한 관심 있는 생성 순서, 그룹을 형성하고 그룹 내 의존관계를 최소화하고, 컨텍스트 포드를 최대화하여 생성 품질을 향상시킵니다. 이러한 설계에 의해, ImageNet 클래스 조건付き 생성에서 생성 스텝을 256에서 20 (256×256 리지즈)와 1024에서 48 (512×512 리지즈)로 줄이고, 품질을 유지하면서 이전의 병렬화된 자동 리듀스 모델보다 3.4배 낮은 라틴 시를 달성합니다.",
      "upvotes": 7,
      "discussionId": "686633d28c83dab5f72d1f40"
    },
    "publishedAt": "2025-07-02T13:59:23.000Z",
    "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image\n  Generation",
    "summary": "We present Locality-aware Parallel Decoding (LPD) to accelerate\nautoregressive image generation. Traditional autoregressive image generation\nrelies on next-patch prediction, a memory-bound process that leads to high\nlatency. Existing works have tried to parallelize next-patch prediction by\nshifting to multi-patch prediction to accelerate the process, but only achieved\nlimited parallelization. To achieve high parallelization while maintaining\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\nordering and degrees of parallelization. It uses learnable position query\ntokens to guide generation at target positions while ensuring mutual visibility\namong concurrently generated tokens for consistent parallel decoding. (2)\nLocality-aware Generation Ordering, a novel schedule that forms groups to\nminimize intra-group dependencies and maximize contextual support, enhancing\ngeneration quality. With these designs, we reduce the generation steps from 256\nto 20 (256times256 res.) and 1024 to 48 (512times512 res.) without\ncompromising quality on the ImageNet class-conditional generation, and\nachieving at least 3.4times lower latency than previous parallelized\nautoregressive models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01957.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650e6ab08f3228d807707735",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650e6ab08f3228d807707735/yFo6aLuyGH06t9yG8AOp7.png",
      "fullname": "Zhuoyang Zhang",
      "name": "zhuoyang20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23552",
      "authors": [
        {
          "_id": "6865e0148c83dab5f72d1e26",
          "name": "Mingi Kwon",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e27",
          "user": {
            "_id": "631074d895c34b95407945f0",
            "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
            "isPro": false,
            "fullname": "Joonghyuk Shin",
            "user": "alex4727",
            "type": "user"
          },
          "name": "Joonghyuk Shin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:56.605Z",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e28",
          "name": "Jaeseok Jung",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e29",
          "name": "Jaesik Park",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e2a",
          "name": "Youngjung Uh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T06:51:40.000Z",
      "submittedOnDailyAt": "2025-07-03T00:15:16.476Z",
      "title": "JAM-Flow: 흐름 매칭을 이용한 결합 오디오・운동 합성",
      "submittedOnDailyBy": {
        "_id": "631074d895c34b95407945f0",
        "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
        "isPro": false,
        "fullname": "Joonghyuk Shin",
        "user": "alex4727",
        "type": "user"
      },
      "summary": "物理运动와 음성의 고유한 연결관계는 생성 모델링에서 일반적으로 무관한 관계로 간주되어 미시되어 왔습니다. 본 논문에서는 물리적 운동과 음성을 동시에 합성하고 조건에 따라 구성된 일련의 프레임워크를 소개합니다. 우리의 접근법은 flow matching과 새로운 Multi-Modal Diffusion Transformer(MM-DiT) 아키텍처를 활용하여, 특정화된 Motion-DiT와 Audio-DiT 모듈을 조합하고 있습니다. 이들은 선택적인 공통 注意층으로 결합되어, 시간적으로 어레이된 위치 벡터와 지역적인 공통 注意 마스크를 포함하여, 모드 간 상호작용을 효과적으로 촉진하고 동시에 모드 고유의 강점을 유지할 수 있습니다. PENCINTA 타입의 목표로 훈련된 JAM-Flow는 광범위한 조건 입력을 지원하며, 텍스트, 참조 음성, 참조 운동을 포함하여, 텍스트로부터 동기화된 테이블헤드 생성, 음성 구동 애니메이션 등 다양한 태스크를 구현할 수 있습니다. JAM-Flow는 호리스틱な 음성 뷰젼 합성의 실용적인 해결책을 제공하며, 다모드 생성 모델링에 큰 발전을 가집니다. 프로젝트 페이지: https://joonghyuk.com/jamflow-web",
      "upvotes": 3,
      "discussionId": "6865e0148c83dab5f72d1e2b"
    },
    "publishedAt": "2025-06-30T02:51:40.000Z",
    "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
    "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23552.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631074d895c34b95407945f0",
      "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
      "fullname": "Joonghyuk Shin",
      "name": "alex4727",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.22868",
      "authors": [
        {
          "_id": "68637f0d588cea0da970c95e",
          "user": {
            "_id": "6719de3235b6494469ab69f6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
            "isPro": false,
            "fullname": "Junsung Lee",
            "user": "jslee525",
            "type": "user"
          },
          "name": "Junsung Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:32:55.680Z",
          "hidden": false
        },
        {
          "_id": "68637f0d588cea0da970c95f",
          "name": "Junoh Kang",
          "hidden": false
        },
        {
          "_id": "68637f0d588cea0da970c960",
          "name": "Bohyung Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T12:36:19.000Z",
      "submittedOnDailyAt": "2025-07-03T06:08:50.749Z",
      "title": "STR-Match: 공간-시간의 관련성 스코어의 매칭 (비 훈련 필요로 하는 비디오 편집을 위한)",
      "submittedOnDailyBy": {
        "_id": "6719de3235b6494469ab69f6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
        "isPro": false,
        "fullname": "Junsung Lee",
        "user": "jslee525",
        "type": "user"
      },
      "summary": "이전 텍스트 가이드에 따라 수행된 비디오 편집 방법들은 시간적 불연속성, 움직임의 왜곡, 특히 특정 영역 변환으로 인한 제한이 있었다. 이러한 제한으로 인해 편집 프로세스 중 공간-시간적인 픽셀 간의 관계 모델링이 부족한 점을 설명한다. 이러한 문제를 해결하기 위해 비디오 편집 알고리즘 \"STR-Match\"를 제안한다. 이 방법은 새로운 \"STR 스코어\"를 통해 잠재적 계획을 가이드하여 시각적으로 쾌적하고 공간-시간적으로 일관된 비디오를 생성한다. 이 스코어는 2D 공간 注意와 1D 시간 모듈을 활용하여 텍스트로부터 비디오(T2V)의 확산 모델로 인접 프레임 간의 공간-시간적인 픽셀 간의 관계를 파악한다. 3D 注意 구조의 고 계산량을 피하기 위해 이 스코어는 효과적이다.潜在 계획 프레임 워크와 잠재 마스크를 결합한 STR-Match는 시간적으로 일관되고 시각적으로 충실한 비디오를 생성하며, 영역 변환으로 인한 큰 영향을 받지 않도록 강력한 성능을 유지한다. 확산 모델에서 인접 프레임 간의 공간-시간적인 픽셀 간의 관계를 파악함으로써 시각적 품질과 공간-시간적인 일관성에서 현재의 방법보다 뛰어난 성능을 보여주는 것을 상세한 실험에 따라 입증되었다.",
      "upvotes": 3,
      "discussionId": "68637f0e588cea0da970c961",
      "projectPage": "https://jslee525.github.io/str-match",
      "githubRepo": "https://github.com/jslee525/STR-Match_official",
      "ai_summary": "STR-Match uses latent optimization and a novel STR score to produce spatiotemporally coherent and visually appealing edited videos by leveraging 2D spatial and 1D temporal attention in T2V diffusion models.",
      "ai_keywords": [
        "T2V diffusion models",
        "latent optimization",
        "spatiotemporal pixel relevance",
        "latent mask",
        "2D spatial attention",
        "1D temporal modules"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-06-28T08:36:19.000Z",
    "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free\n  Video Editing",
    "summary": "Previous text-guided video editing methods often suffer from temporal\ninconsistency, motion distortion, and-most notably-limited domain\ntransformation. We attribute these limitations to insufficient modeling of\nspatiotemporal pixel relevance during the editing process. To address this, we\npropose STR-Match, a training-free video editing algorithm that produces\nvisually appealing and spatiotemporally coherent videos through latent\noptimization guided by our novel STR score. The score captures spatiotemporal\npixel relevance across adjacent frames by leveraging 2D spatial attention and\n1D temporal modules in text-to-video (T2V) diffusion models, without the\noverhead of computationally expensive 3D attention mechanisms. Integrated into\na latent optimization framework with a latent mask, STR-Match generates\ntemporally consistent and visually faithful videos, maintaining strong\nperformance even under significant domain transformations while preserving key\nvisual attributes of the source. Extensive experiments demonstrate that\nSTR-Match consistently outperforms existing methods in both visual quality and\nspatiotemporal consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22868.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6719de3235b6494469ab69f6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
      "fullname": "Junsung Lee",
      "name": "jslee525",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]