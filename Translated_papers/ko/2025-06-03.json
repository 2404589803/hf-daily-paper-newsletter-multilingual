[
  {
    "paper": {
      "id": "2506.01939",
      "authors": [
        {
          "_id": "683e7a6d97fd742a8edee1ba",
          "user": {
            "_id": "6486dde1f74857df3f1a5828",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
            "isPro": false,
            "fullname": "Shenzhi Wang",
            "user": "shenzhi-wang",
            "type": "user"
          },
          "name": "Shenzhi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:16.481Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1bb",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1bc",
          "name": "Chang Gao",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1bd",
          "user": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "isPro": false,
            "fullname": "Chujie Zheng",
            "user": "chujiezheng",
            "type": "user"
          },
          "name": "Chujie Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:13.605Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1be",
          "name": "Shixuan Liu",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1bf",
          "name": "Rui Lu",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c0",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c1",
          "user": {
            "_id": "63f30b870a16587ea970edfe",
            "avatarUrl": "/avatars/059491b33fecec69032e6d481229ee31.svg",
            "isPro": false,
            "fullname": "Xiong-Hui Chen",
            "user": "xionghuichen",
            "type": "user"
          },
          "name": "Xionghui Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:10.358Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c2",
          "name": "Jianxin Yang",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c3",
          "user": {
            "_id": "64704e973601bb7b06643e98",
            "avatarUrl": "/avatars/52e51f4d1be6769e4397b8be2799cf32.svg",
            "isPro": false,
            "fullname": "Zhenru Zhang",
            "user": "Zhenru",
            "type": "user"
          },
          "name": "Zhenru Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:40:21.027Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c4",
          "user": {
            "_id": "666aacfb918ba11c7c598194",
            "avatarUrl": "/avatars/45bee8f1fdbdd256ee47d25e4bf01a7a.svg",
            "isPro": false,
            "fullname": "Yuqiong Liu",
            "user": "lyq333",
            "type": "user"
          },
          "name": "Yuqiong Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:40:30.240Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c5",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c6",
          "name": "Andrew Zhao",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c7",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c8",
          "name": "Shiji Song",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c9",
          "user": {
            "_id": "63d9d68c1cae35c27bf7a6a7",
            "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
            "isPro": false,
            "fullname": "Bowen Yu",
            "user": "Tigerph",
            "type": "user"
          },
          "name": "Bowen Yu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T04:30:38.648Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1ca",
          "name": "Gao Huang",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1cb",
          "user": {
            "_id": "620760a26e3b7210c2ff1943",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
            "isPro": false,
            "fullname": "Junyang Lin",
            "user": "JustinLin610",
            "type": "user"
          },
          "name": "Junyang Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:39:54.278Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:54:39.000Z",
      "submittedOnDailyAt": "2025-06-03T03:09:30.655Z",
      "title": "80/20 법칙을 초월하여: 고 엔트로피의 소수 토큰이 효과적인 LLM 논리의 강화 학습을 구동합니다.",
      "submittedOnDailyBy": {
        "_id": "6486dde1f74857df3f1a5828",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
        "isPro": false,
        "fullname": "Shenzhi Wang",
        "user": "shenzhi-wang",
        "type": "user"
      },
      "summary": "RLVR（Verifiable Rewards）는 대 언어 모델（LLMs）의 논리 능력을 향상시키기 위한 강력한 강화학습의 방법론으로 등장하지만, 그 구조는 아직 이해되지 않았습니다. 본 연구에서는 토큰 엔트로피 패턴의 새로운 시각으로 RLVR를 선두에 조사하고, 서로 다른 토큰이 논리 성능에 어떻게 영향을 미치는지 상세히 분석합니다. Chain-of-Thought（CoT） 논리의 토큰 엔트로피 패턴을 검토하고, 그 중 고 엔트로피를 나타내는 토큰이 거의 없으며, 이러한 토큰은 다양한 논리 경로를 이끌어내는 중요한 분기점이라는 것을 알아냄니다. 또한 RLVR 훈련 중 엔트로피 패턴의 진화 연구를 통해, RLVR는 주로 기본 모델의 엔트로피 패턴에 따라 이행하고 있으며, 특히 고 엔트로피 토큰의 엔트로피를 조정하는 것을 알 수 있습니다. 이러한 발견은 RLVR에서 고 엔트로피 토큰（즉, 분기 토큰）의 중요성을 밝혀냅니다. 최종적으로, 정책 그레이디언트 업데이트를 분기 토큰에 한정시키고, Qwen3-8B 기본 모델에서 상대적으로 또는 초과하는 성능을 달성하기 위해 20%의 토큰을 사용함으로써, Qwen3-32B（AIME'25에서 +11.04, AIME'24에서 +7.71）과 Qwen3-14B（AIME'25에서 +4.79, AIME'24에서 +5.21）의 기본 모델에서 성능을 크게 초과하는 것을 확인합니다. 이러한 발견은 RLVR의 효과는 논리의 방향을 결정하는 고 엔트로피 토큰의 최적화에 기반한 것이며, 토큰 엔트로피의 시각으로 RLVR를 이해하고, 고 엔트로피의少数 토큰을 활용하여 LLM의 논리 능력을 발전시킬 수 있음을 보여줍니다.",
      "upvotes": 66,
      "discussionId": "683e7a6e97fd742a8edee227",
      "projectPage": "https://shenzhi-wang.github.io/high-entropy-minority-tokens-rlvr/",
      "ai_summary": "Token entropy patterns are crucial in RLVR, with high-entropy tokens significantly impacting reasoning performance and model optimization.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "RLVR",
        "Large Language Models",
        "LLMs",
        "token entropy patterns",
        "Chain-of-Thought",
        "CoT reasoning",
        "high-entropy tokens",
        "policy gradient updates",
        "Qwen3-8B",
        "Qwen3-32B",
        "Qwen3-14B",
        "AIME"
      ]
    },
    "publishedAt": "2025-06-02T13:54:39.000Z",
    "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective\n  Reinforcement Learning for LLM Reasoning",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful approach to enhancing the reasoning capabilities of Large Language\nModels (LLMs), while its mechanisms are not yet well understood. In this work,\nwe undertake a pioneering exploration of RLVR through the novel perspective of\ntoken entropy patterns, comprehensively analyzing how different tokens\ninfluence reasoning performance. By examining token entropy patterns in\nChain-of-Thought (CoT) reasoning, we observe that only a small fraction of\ntokens exhibit high entropy, and these tokens act as critical forks that steer\nthe model toward diverse reasoning pathways. Furthermore, studying how entropy\npatterns evolve during RLVR training reveals that RLVR largely adheres to the\nbase model's entropy patterns, primarily adjusting the entropy of high-entropy\ntokens. These findings highlight the significance of high-entropy tokens (i.e.,\nforking tokens) to RLVR. We ultimately improve RLVR by restricting policy\ngradient updates to forking tokens and uncover a finding even beyond the 80/20\nrule: utilizing only 20% of the tokens while maintaining performance comparable\nto full-gradient updates on the Qwen3-8B base model and significantly\nsurpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71\non AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,\nhighlighting a strong scaling trend. In contrast, training exclusively on the\n80% lowest-entropy tokens leads to a marked decline in performance. These\nfindings indicate that the efficacy of RLVR primarily arises from optimizing\nthe high-entropy tokens that decide reasoning directions. Collectively, our\nresults highlight the potential to understand RLVR through a token-entropy\nperspective and optimize RLVR by leveraging high-entropy minority tokens to\nfurther improve LLM reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01939.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6486dde1f74857df3f1a5828",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
      "fullname": "Shenzhi Wang",
      "name": "shenzhi-wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 326
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01049",
      "authors": [
        {
          "_id": "683e5b9a1167d9630159b27f",
          "user": {
            "_id": "640f7083208821a59b74c757",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678735253848-640f7083208821a59b74c757.jpeg",
            "isPro": false,
            "fullname": "Siyuan Li",
            "user": "Lupin1998",
            "type": "user"
          },
          "name": "Siyuan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:39.296Z",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b280",
          "user": {
            "_id": "670880950e79a8b46f7ff9dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
            "isPro": false,
            "fullname": "Juanxi Tian",
            "user": "Juanxi",
            "type": "user"
          },
          "name": "Juanxi Tian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:40:52.165Z",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b281",
          "user": {
            "_id": "6594d390674349122ce6f368",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/KdWz6lZyGYQpjAgBDeiC1.jpeg",
            "isPro": false,
            "fullname": "Zedong Wang (Jacky)",
            "user": "ZedongWangAI",
            "type": "user"
          },
          "name": "Zedong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:41.911Z",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b282",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b283",
          "user": {
            "_id": "67ee7eef2a8e2fd1445407ab",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/TelMtjU8Ki8ulQU4-b0He.jpeg",
            "isPro": false,
            "fullname": "Zicheng Liu",
            "user": "MarcusB3n",
            "type": "user"
          },
          "name": "Zicheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:41:29.426Z",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b284",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b285",
          "user": {
            "_id": "67cd1d6c96e0a33b99c78b26",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wsUk7e5BPHa6L7GroWtat.png",
            "isPro": false,
            "fullname": "Dan Xu",
            "user": "danxu",
            "type": "user"
          },
          "name": "Dan Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:41:02.065Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T15:30:37.000Z",
      "submittedOnDailyAt": "2025-06-03T00:52:33.852Z",
      "title": "에로틱스 LLM의 제어를 위한 학습 속도 스케일링에 의한 경사 그룹핑",
      "submittedOnDailyBy": {
        "_id": "6594d390674349122ce6f368",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/KdWz6lZyGYQpjAgBDeiC1.jpeg",
        "isPro": false,
        "fullname": "Zedong Wang (Jacky)",
        "user": "ZedongWangAI",
        "type": "user"
      },
      "summary": "대규모 언어 모델(LLMs)의 훈련은 그 크기와 다른 아키텍처에 따라 여러 문제를 겪고 있습니다. AdamW와 같은 적응 가능한 최적화 알고리즘은 경사 변화 문제를 해결할 수 있지만, 각 파라미터에 대해 적절한 학습률을 평가하는 것은 어려워, 훈련의 불안정성, 수렴 속도의 느린성, 파라미터 효율적인 미세 조정(PEFT) 방법과의 불합성 등 여러 문제를 남깁니다. 본 논문에서는 적응 가능한 학습률을 개선하기 위한 그래피ン거스레이어 스탬프(SGG)를 도입합니다. SGG는 동적인 그룹 분리와 그룹 고유의 스케일링을 사용하여 각 층의 경사 통계를 클러스터로 묶고, 각 파라미터의 학습률을 조정합니다. SGG는 전체 그룹의 제약을 가치지 않고, 각 파라미터의 적절한 반응을 유지하는 것을 목표로 합니다. 다양한(M) LLM 벤치마크에서 수행한 실험에 따라, SGG는 현재의 최적화 알고리즘과 통합하고, 베이스라인보다 일관된 효과와 빠른 수렴을 보여주며, 모델 크기의 종류에 따라 구현할 수 있습니다. SGG는 변하는 배치 크기와 학습률에 의한 불안정성을 극복하고, LLM의 최적화에 강력한 선택지로 자리잡고 있습니다.",
      "upvotes": 22,
      "discussionId": "683e5b9b1167d9630159b2ef",
      "ai_summary": "SGG, an optimizer wrapper, enhances adaptive learning rates for large language models by grouping gradients and applying cluster-specific scaling, improving convergence and stability.",
      "ai_keywords": [
        "large language models",
        "adaptive optimizers",
        "AdamW",
        "parameter-wise learning rate estimation",
        "training instability",
        "parameter-efficient fine-tuning",
        "Scaling with Gradient Grouping",
        "gradient grouping",
        "cluster-specific scaling",
        "LLM benchmarks",
        "robust choice for LLM optimization"
      ]
    },
    "publishedAt": "2025-06-01T11:30:37.000Z",
    "title": "Taming LLMs by Scaling Learning Rates with Gradient Grouping",
    "summary": "Training large language models (LLMs) poses challenges due to their massive\nscale and heterogeneous architectures. While adaptive optimizers like AdamW\nhelp address gradient variations, they still struggle with efficient and\neffective parameter-wise learning rate estimation, resulting in training\ninstability, slow convergence, and poor compatibility with parameter-efficient\nfine-tuning (PEFT) techniques. This work introduces Scaling with Gradient\nGrouping (SGG), an optimizer wrapper that improves adaptive learning rate\nestimation by dynamic grouping and group-specific scaling. SGG first groups\ngradient statistics in each layer into clusters and then applies\ncluster-specific scaling to calibrate learning rates for each parameter, thus\nimposing collective group-wise constraints while maintaining precise\nper-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that\nSGG integrates seamlessly with existing optimizers, and offers consistent gains\nand faster convergence over baselines, with various model sizes. Its stability\nacross varying batch sizes and learning rates establishes SGG as a robust\nchoice for LLM optimization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01049.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6594d390674349122ce6f368",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/KdWz6lZyGYQpjAgBDeiC1.jpeg",
      "fullname": "Zedong Wang (Jacky)",
      "name": "ZedongWangAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23590",
      "authors": [
        {
          "_id": "683e6b2d97fd742a8edb8a8e",
          "user": {
            "_id": "64f5c7cb65a4b1acb20ffc15",
            "avatarUrl": "/avatars/67c42a6e26bbeddc4bf706eb8f1a75b1.svg",
            "isPro": false,
            "fullname": "Zifu Wang",
            "user": "wangzifu",
            "type": "user"
          },
          "name": "Zifu Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:41:52.137Z",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a8f",
          "user": {
            "_id": "6477b2038ab7e732b6d8a9b5",
            "avatarUrl": "/avatars/c859a35b8965a904f103bbc34f36ab2a.svg",
            "isPro": false,
            "fullname": "Junyi Zhu",
            "user": "RyanZhu",
            "type": "user"
          },
          "name": "Junyi Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:42:00.703Z",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a90",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a91",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a92",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a93",
          "name": "Jiaqian Yu",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a94",
          "name": "Matthew B. Blaschko",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:01:22.000Z",
      "submittedOnDailyAt": "2025-06-03T01:56:17.990Z",
      "title": "Jigsaw-R1: ジグサーパズル에 의한 규칙 기반 시각화 재학습 연구\n\n(Note: The original text \"Jigsaw-R1: ジグサーパズルによるルールベース可視化再強化学習の研究\" is a Japanese title. The translation provided above is in Korean. If you intended to have the text translated into English, please let me know, and I will provide the English translation instead.)",
      "submittedOnDailyBy": {
        "_id": "64f5c7cb65a4b1acb20ffc15",
        "avatarUrl": "/avatars/67c42a6e26bbeddc4bf706eb8f1a75b1.svg",
        "isPro": false,
        "fullname": "Zifu Wang",
        "user": "wangzifu",
        "type": "user"
      },
      "summary": "ルールベース의 강화학습(RL)을 여러 모델의 대 언어 모델(MLLMs)에 적용하여 문서만 있는 분야에서 검색 방식과 특징을 특정화하는 가능성을 가졌으며, 시각적인 작업에 특히 중요한 도전이 발생합니다. 본 논문에서는 구조화된 실험 프레임워크로 활용되는 캄캄 퍼즐을 사용하여 RL에 대한 구체적인 연구를 수행합니다. 캄캄 퍼즐은 내적 정답, 난이도 조정, 복잡한 판단을 요구함으로써 이 연구에 가장 적합한 것으로 선정되었습니다. 본 연구에서 다음 몇 가지 노이드 포인트가 밝혀졌습니다.\n\n1. MLLM은 가장 간단한 캄캄 퍼즐에서 근사한 랜덤 예측과 유사한 성능을 보였지만, 최종 훈련에서 근사한 완전한 정확성을 달성하고, 복잡한, 새로운 배치에 일반화할 수 있습니다.\n2. 캄캄 퍼즐에서의 학습은 다른 시각적인 작업에서도 일반화할 수 있으며, 이 효과는 특정 작업의 설정에 따라 달라집니다.\n3. MLLM은 명시적인 이유를 가지고 학습하여 일반화할 수 있으며, 대부분의 오픈 소스 모델은 직접적인 답변을 선호합니다. 따라서, 단계별로 이유를 학습한 경우, 최종 답변을 구하는 과정에서 이유를 무시할 수도 있습니다.\n4. 복잡한 이유의 패턴은 기존의 것이며, 그 빈도는 학습과 작업의 난이도에 따라 증가합니다.\n5. 결과적으로, RL은 시각적인 작업에서 서브바이더 피드백(SFT)보다 효과적인 일반화에 나타났으며, SFT의 초기 단계는 후속의 RL 최적화에 영향을 미칠 수 있습니다.\n\n이러한 발견은 캄캄 퍼즐에 기반하므로 다른 시각적인 작업에서는 다른 결과를 보일 가능성이 있지만, 이 연구는 RL에 대한 이해와 다 모델 학습의 가능성에 기여합니다. 코드는 https://github.com/zifuwanggg/Jigsaw-R1에 공개되어 있습니다.",
      "upvotes": 20,
      "discussionId": "683e6b2e97fd742a8edb8ac1",
      "githubRepo": "https://github.com/zifuwanggg/Jigsaw-R1",
      "ai_summary": "Rule-based reinforcement learning applied to multimodal large language models demonstrates effective generalization in visual tasks, particularly using jigsaw puzzles, outperforming supervised fine-tuning.",
      "ai_keywords": [
        "rule-based reinforcement learning",
        "multimodal large language models",
        "visual RL",
        "jigsaw puzzles",
        "fine-tuning",
        "supervised fine-tuning",
        "complex decision-making",
        "visual tasks",
        "step-by-step reasoning",
        "generalization"
      ]
    },
    "publishedAt": "2025-05-29T12:01:22.000Z",
    "title": "Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with\n  Jigsaw Puzzles",
    "summary": "The application of rule-based reinforcement learning (RL) to multimodal large\nlanguage models (MLLMs) introduces unique challenges and potential deviations\nfrom findings in text-only domains, particularly for perception-heavy tasks.\nThis paper provides a comprehensive study of rule-based visual RL, using jigsaw\npuzzles as a structured experimental framework. Jigsaw puzzles offer inherent\nground truth, adjustable difficulty, and demand complex decision-making, making\nthem ideal for this study. Our research reveals several key findings:\nFirstly, we find that MLLMs, initially performing near to random\nguessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and\ngeneralize to complex, unseen configurations through fine-tuning.\nSecondly, training on jigsaw puzzles can induce generalization to\nother visual tasks, with effectiveness tied to specific task configurations.\nThirdly, MLLMs can learn and generalize with or without explicit\nreasoning, though open-source models often favor direct answering.\nConsequently, even when trained for step-by-step reasoning, they can ignore the\nthinking process in deriving the final answer. Fourthly, we observe\nthat complex reasoning patterns appear to be pre-existing rather than emergent,\nwith their frequency increasing alongside training and task difficulty.\nFinally, our results demonstrate that RL exhibits more effective\ngeneralization than Supervised Fine-Tuning (SFT), and an initial SFT cold start\nphase can hinder subsequent RL optimization. Although these observations are\nbased on jigsaw puzzles and may vary across other visual tasks, this research\ncontributes a valuable piece of jigsaw to the larger puzzle of collective\nunderstanding rule-based visual RL and its potential in multimodal learning.\nThe code is available at: https://github.com/zifuwanggg/Jigsaw-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f5c7cb65a4b1acb20ffc15",
      "avatarUrl": "/avatars/67c42a6e26bbeddc4bf706eb8f1a75b1.svg",
      "fullname": "Zifu Wang",
      "name": "wangzifu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00539",
      "authors": [
        {
          "_id": "683e76c17a0996f979e72700",
          "user": {
            "_id": "671a4abbef737c0abe21b3f8",
            "avatarUrl": "/avatars/da826af5472a3b9f1969f0c766672731.svg",
            "isPro": false,
            "fullname": "Ruihan Yang",
            "user": "rhyang2021",
            "type": "user"
          },
          "name": "Ruihan Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:32.053Z",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72701",
          "name": "Yikai Zhang",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72702",
          "user": {
            "_id": "63f86b099f87cc3e645b51d9",
            "avatarUrl": "/avatars/27ca5ba425640bf67474cee871e8e53a.svg",
            "isPro": false,
            "fullname": "Ellie Chen",
            "user": "sheep33333",
            "type": "user"
          },
          "name": "Aili Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:28.422Z",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72703",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72704",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72705",
          "name": "Jiangjie Chen",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72706",
          "name": "Deqing Yang",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72707",
          "name": "Yanghua Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T12:54:49.000Z",
      "submittedOnDailyAt": "2025-06-03T02:50:22.439Z",
      "title": "ARIA: 목적에 따라 보상을 집중하는 언어 어휘 어러바러의 훈련",
      "submittedOnDailyBy": {
        "_id": "671a4abbef737c0abe21b3f8",
        "avatarUrl": "/avatars/da826af5472a3b9f1969f0c766672731.svg",
        "isPro": false,
        "fullname": "Ruihan Yang",
        "user": "rhyang2021",
        "type": "user"
      },
      "summary": "대 언어 모뎀(LLMs)는 단어의 조합의 공분포를 표현하는 개방적인 언어 행동 환경에서 행동 공간의 지수적으로 큰 크기를 가짐으로써 복잡한 이유론과 결정을 수행할 수 있습니다. 그러나 예를 들어 대화나 질문 게임처럼 개방적인 언어 행동 환경에서 행동 공간은 단어의 조합의 공분포를 표현함으로써 지수적으로 커집니다. 이러한 공간에서 행동을 샘플링하는 것은 보상의 극단적인 희박성을招く 것으로 보상의 분산이 크게 되고 효과적인 재보상 학습(RL)을 방해합니다. 이에 대해 우리는 보상의 집중을 수행하는 방법을 제안합니다. 이 방법은 효율적이고 효과적인 언어 아웃풋의 훈련을 가능하게 합니다. ARIA는 고차원 단어의 조합의 공분포 공간에서 자연어 행동을 저차원 의도 공간에 투영하고 의미적으로 유사한 행동을 클러스터링하고 공유 보상을 할당합니다. 의도에 대한 보상의 집중은 보상의 분산을 줄이고 보상의 신호를 밀어넣는 것으로 더 좋은 정책의 최적화를 촉진합니다. 엄격한 실험에 의하면 ARIA는 4개의 하류 태스크의 평균 9.95%의 효과적인 성능 향상을 얻으며 온라인과 오프라인 RL 기반 라인을 일치하여 우수함을 입증했습니다.",
      "upvotes": 19,
      "discussionId": "683e76ca7a0996f979e728e0",
      "projectPage": "https://aria-agent.github.io/",
      "githubRepo": "https://github.com/rhyang2021/ARIA",
      "ai_summary": "ARIA aggregates rewards in an intention space to mitigate reward sparsity and improve policy optimization in language-based reinforcement learning tasks.",
      "ai_keywords": [
        "large language models",
        "reinforcement learning",
        "action space",
        "token distribution",
        "extreme reward sparsity",
        "reward variance",
        "policy optimization",
        "intention space",
        "semantically similar actions",
        "shared rewards",
        "policy gradient variance",
        "performance gains",
        "offline RL",
        "online RL"
      ]
    },
    "publishedAt": "2025-05-31T08:54:49.000Z",
    "title": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation",
    "summary": "Large language models (LLMs) have enabled agents to perform complex reasoning\nand decision-making through free-form language interactions. However, in\nopen-ended language action environments (e.g., negotiation or question-asking\ngames), the action space can be formulated as a joint distribution over tokens,\nresulting in an exponentially large action space. Sampling actions in such a\nspace can lead to extreme reward sparsity, which brings large reward variance,\nhindering effective reinforcement learning (RL). To address this, we propose\nARIA, a method that Aggregates Rewards in Intention space to enable efficient\nand effective language Agents training. ARIA aims to project natural language\nactions from the high-dimensional joint token distribution space into a\nlow-dimensional intention space, where semantically similar actions are\nclustered and assigned shared rewards. This intention-aware reward aggregation\nreduces reward variance by densifying reward signals, fostering better policy\noptimization. Extensive experiments demonstrate that ARIA not only\nsignificantly reduces policy gradient variance, but also delivers substantial\nperformance gains of an average of 9.95% across four downstream tasks,\nconsistently outperforming offline and online RL baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00539.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671a4abbef737c0abe21b3f8",
      "avatarUrl": "/avatars/da826af5472a3b9f1969f0c766672731.svg",
      "fullname": "Ruihan Yang",
      "name": "rhyang2021",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00411",
      "authors": [
        {
          "_id": "683e86e31bca54bb6d169fc4",
          "user": {
            "_id": "6707eaaf5f50b17754ff9cbc",
            "avatarUrl": "/avatars/f08ca6228b124a8955787d0662a52bbd.svg",
            "isPro": false,
            "fullname": "Yang",
            "user": "Yysrc",
            "type": "user"
          },
          "name": "Yi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:40:09.713Z",
          "hidden": false
        },
        {
          "_id": "683e86e31bca54bb6d169fc5",
          "name": "Jiaxuan Sun",
          "hidden": false
        },
        {
          "_id": "683e86e31bca54bb6d169fc6",
          "name": "Siqi Kou",
          "hidden": false
        },
        {
          "_id": "683e86e31bca54bb6d169fc7",
          "name": "Yihan Wang",
          "hidden": false
        },
        {
          "_id": "683e86e31bca54bb6d169fc8",
          "name": "Zhijie Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T06:01:03.000Z",
      "submittedOnDailyAt": "2025-06-03T03:54:29.259Z",
      "title": "LoHoVLA: 장기 시각 언어 행동 모델",
      "submittedOnDailyBy": {
        "_id": "654e330f350abceb30a1390b",
        "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg",
        "isPro": false,
        "fullname": "KouSiqi",
        "user": "karrykkk",
        "type": "user"
      },
      "summary": "실세계의 시각화 에이전트는 장기간의 태스크를 직면하여, 높은 수준의 목표를 추구하는 다단계적인 해결책을 수행하는 것을 요구한다. 이러한 태스크를 성공적으로 수행하기 위해서는, 높은 수준의 태스크 계획(즉, 목표를 단계별로 분할하는 것)과 낮은 수준의 동작 제어(즉, 로봇의 동작을 정밀하게 생성하는 것)이 모두 필요하며, 이 두 가지는 함께 필요로 한다. 현재의 시각 언어 행동(VLA) 모델과 계층 구조는 시각화 태스크에 잠재적인 가능성을 제공하지만, 첫 번째 모델은 계획에 차이가 있고, 두 번째는 협조 문제에 직면하여, 이러한 문제를 해결하지 못하여 성능을 저해하고 있다. 우리는 장기간 태스크에 대한 새로운 통합 프레임워크를 소개하고, 이를 LoHoVLA(Long-Horizon Vision Language Action)라고 한다. LoHoVLA는 큰 규모의 사전 학습 시각 언어 모델(VLM)을 백드롭으로 활용하고, 단계별로 태스크 생성과 로봇 행동 예측에 대한 언어와 행동 토큰을 함께 생성한다. 이 공유 표현은 태스크 간의 더 좋은 일반화에 촉진한다. 또한, LoHoVLA는 계층 구조의 폐루프 제어 구조를 도입하여, 높은 수준의 계획과 낮은 수준의 제어에서 발생하는 오류를 줄이는 데 사용한다. LoHoVLA의 훈련에는 Ravens 시뮬레이터를 기반으로 한 LoHoSet 데이터 세트를 도입하여, 20개의 장기간 태스크를 포함하고, 각각 1,000명의 전문가의 데모레이션을 구성하는 시각적인 관찰, 언어적 목표, 단계별로 태스크, 로봇 행동을 포함하는 데이터 세트를 구축하였다. 실험 결과를 통해, Ravens 시뮬레이터에서 장기간 시각화 태스크에 대해, LoHoVLA는 계층적 및 표준적인 VLA 접근 방식을 크게 초과하는 것을 보여주며, 이러한 발견은 통합 프레임워크가 광범위한 시각화 지능의 발전에 대한 가능성을 강조한다.",
      "upvotes": 19,
      "discussionId": "683e86e31bca54bb6d169ff5",
      "ai_summary": "A unified vision language action framework, LoHoVLA, combines a large pretrained vision language model with hierarchical closed-loop control to improve performance on long-horizon embodied tasks.",
      "ai_keywords": [
        "vision language action models",
        "hierarchical architectures",
        "high-level task planning",
        "low-level motion control",
        "LoHoVLA",
        "large pretrained vision language model",
        "shared representation",
        "hierarchical closed-loop control",
        "LoHoSet",
        "Ravens simulator",
        "long-horizon tasks",
        "sub-task generation",
        "robot action prediction",
        "embodied intelligence"
      ]
    },
    "publishedAt": "2025-05-31T02:01:03.000Z",
    "title": "LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon\n  Embodied Tasks",
    "summary": "Real-world embodied agents face long-horizon tasks, characterized by\nhigh-level goals demanding multi-step solutions beyond single actions.\nSuccessfully navigating these requires both high-level task planning (i.e.,\ndecomposing goals into sub-tasks) and low-level motion control (i.e.,\ngenerating precise robot actions). While existing vision language action (VLA)\nmodels and hierarchical architectures offer potential in embodied tasks, the\nformer often falter in planning, and the latter can suffer from coordination\nissues, both hampering performance. We introduce a new unified VLA framework\nfor long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA\nleverages a large pretrained vision language model (VLM) as the backbone to\njointly generate language and action tokens for sub-task generation and robot\naction prediction, respectively. This shared representation promotes better\ngeneralization across tasks. Additionally, LoHoVLA embraces a hierarchical\nclosed-loop control mechanism to mitigate errors originating from both\nhigh-level planning and low-level control. To train LoHoVLA, we introduce\nLoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon\ntasks, each with 1,000 expert demonstrations composed of visual observations,\nlinguistic goals, sub-tasks, and robot actions. Experimental results show that\nLoHoVLA significantly surpasses both hierarchical and standard VLA approaches\non long-horizon embodied tasks in the Ravens simulator. These findings\nunderscore the promise of unified architectures for advancing generalizable\nembodied intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654e330f350abceb30a1390b",
      "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg",
      "fullname": "KouSiqi",
      "name": "karrykkk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01844",
      "authors": [
        {
          "_id": "683eb85825fcc99d2a7fc26d",
          "user": {
            "_id": "62bdeedd01dc22b4d22a371e",
            "avatarUrl": "/avatars/6adc904fb1e08661d293a966270afabb.svg",
            "isPro": false,
            "fullname": "Mustafa Shukor",
            "user": "mshukor",
            "type": "user"
          },
          "name": "Mustafa Shukor",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:09.058Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc26e",
          "user": {
            "_id": "640e21ef3c82bd463ee5a76d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e21ef3c82bd463ee5a76d/nVR1DFPAsiLw6Boys28Rb.jpeg",
            "isPro": false,
            "fullname": "Dana Aubakirova",
            "user": "danaaubakirova",
            "type": "user"
          },
          "name": "Dana Aubakirova",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:18.586Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc26f",
          "user": {
            "_id": "63d67eac6f49aa8230601996",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d67eac6f49aa8230601996/djvtWdy718whUgh7tu1Ko.jpeg",
            "isPro": false,
            "fullname": "Francesco Capuano",
            "user": "fracapuano",
            "type": "user"
          },
          "name": "Francesco Capuano",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:29.639Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc270",
          "user": {
            "_id": "65f9d37113336392bad1e49c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f9d37113336392bad1e49c/B0Fxwconnu7lvtjBz4Ruq.jpeg",
            "isPro": false,
            "fullname": "Pepijn Kooijmans",
            "user": "pepijn223",
            "type": "user"
          },
          "name": "Pepijn Kooijmans",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:38.693Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc271",
          "user": {
            "_id": "67b124b081d4eae18b957606",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/CXvSv2l15uPkMQL_HBRDF.png",
            "isPro": false,
            "fullname": "Steven Palma",
            "user": "imstevenpmwork",
            "type": "user"
          },
          "name": "Steven Palma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:47.379Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc272",
          "user": {
            "_id": "64c255b2254239173af0570a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c255b2254239173af0570a/JvYNX4gpk0hVQJeJif8Mo.jpeg",
            "isPro": false,
            "fullname": "Adil Zouitine",
            "user": "AdilZtn",
            "type": "user"
          },
          "name": "Adil Zouitine",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:56.957Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc273",
          "user": {
            "_id": "668bd06dd58b51a628566d80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668bd06dd58b51a628566d80/II7Yr5dT5ItMrpoMkQEy3.jpeg",
            "isPro": false,
            "fullname": "Michel Aractingi",
            "user": "aractingi",
            "type": "user"
          },
          "name": "Michel Aractingi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:05.798Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc274",
          "user": {
            "_id": "67d7dea1786ddcb3af5a44b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d7dea1786ddcb3af5a44b3/gEgXTH4oO91GIzjHR-yrb.png",
            "isPro": false,
            "fullname": "Caroline Pascal",
            "user": "CarolinePascal",
            "type": "user"
          },
          "name": "Caroline Pascal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:15.340Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc275",
          "user": {
            "_id": "631365ad289cf15634c6f600",
            "avatarUrl": "/avatars/a464d228328719274a20121e2a82f703.svg",
            "isPro": false,
            "fullname": "Martino Russi",
            "user": "nepyope",
            "type": "user"
          },
          "name": "Martino Russi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:23.474Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc276",
          "user": {
            "_id": "65d66b494bbd0d92b641cdbb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
            "isPro": false,
            "fullname": "Andres Marafioti",
            "user": "andito",
            "type": "user"
          },
          "name": "Andres Marafioti",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:31.740Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc277",
          "user": {
            "_id": "65fcb7f133a3d6f126772121",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fcb7f133a3d6f126772121/BvVbNqnlQgDr2f_9dm5Es.jpeg",
            "isPro": false,
            "fullname": "Simon  Alibert",
            "user": "aliberts",
            "type": "user"
          },
          "name": "Simon Alibert",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:39.999Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc278",
          "name": "Matthieu Cord",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc279",
          "user": {
            "_id": "5df7e9e5da6d0311fd3d53f9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg",
            "isPro": false,
            "fullname": "Thomas Wolf",
            "user": "thomwolf",
            "type": "user"
          },
          "name": "Thomas Wolf",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:13:02.139Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc27a",
          "user": {
            "_id": "62f857fbb9fda55613ce80d9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f857fbb9fda55613ce80d9/d7bRniKLmOt-iFN07k1Su.png",
            "isPro": false,
            "fullname": "Remi Cadene",
            "user": "cadene",
            "type": "user"
          },
          "name": "Remi Cadene",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:13:11.882Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T16:30:19.000Z",
      "submittedOnDailyAt": "2025-06-03T07:31:41.152Z",
      "title": "SmolVLA: 저비용적이고 효율적인 로봇공학에 대한 시각・언어・행동 모델",
      "submittedOnDailyBy": {
        "_id": "65d66b494bbd0d92b641cdbb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
        "isPro": false,
        "fullname": "Andres Marafioti",
        "user": "andito",
        "type": "user"
      },
      "summary": "시각 언어 모델(VLMs)은 대규모의 다 타입 데이터 세트를 사전 학습하여 풍부한 시각적 및 언어적 지식을 기록하며, 로봇 시스템의 강력한 기초를 이루는 데 사용됩니다. 로봇 정책을 설정하는 데에는 이러한 모델을 더 자연어로 구동하여 시각 언어 행동(VLA) 모델로 변환하는 것이 최근의 접근 방식입니다. 그러나 현재의 VLA는 일반적으로 수 백억 파라미터를 가지고 있으며, 높은 학습 비용과 실세계에서의 기능성을 동반합니다. 또한, 데이터 세트는 학술과 산업 중심이며, 가격 낮은 로봇 플랫폼에서 구축된 커뮤니티 데이터의 증가를 무시하고 있습니다. 본 연구에서는 SmolVLA를 소개하며, 작은 효율적인 커뮤니티 기반의 VLA이며, 학습 비용과 추론 비용을 크게 줄이고, 대립적인 성능을 유지하는 데 사용됩니다. SmolVLA는 단일 GPU에서 학습하며, 소비자 수준의 GPU나 CPU로 작동하도록 설계되었습니다. 또한, 더 높은 응답성을 향상시키기 위해 비동기 추론 스택을 도입하고, 시각과 행동 예측을 행동 실행과 분리하여 쉼핑된 행동 생성으로 높은 제어 속도를 실현합니다. 작은 크기에도 불구하고, SmolVLA는 10배 큰 VLA와 같은 성능을 달성합니다. SmolVLA는 여러 시뮬레이션 및 실세계 로봇 벤치마크에서 평가되었으며, 모든 코드, 사전 학습 모델 및 학습 데이터가 공개되어 있습니다.",
      "upvotes": 17,
      "discussionId": "683eb85925fcc99d2a7fc2dc",
      "ai_summary": "SmolVLA is a compact, efficient vision-language-action model that achieves competitive performance at reduced computational costs and can be deployed on consumer-grade hardware.",
      "ai_keywords": [
        "vision-language models",
        "multimodal datasets",
        "robotic policies",
        "vision-language-action models",
        "natural language-driven perception",
        "asynchronous inference",
        "action prediction",
        "action execution",
        "chunked action generation",
        "performance benchmarks"
      ]
    },
    "publishedAt": "2025-06-02T12:30:19.000Z",
    "title": "SmolVLA: A Vision-Language-Action Model for Affordable and Efficient\n  Robotics",
    "summary": "Vision-language models (VLMs) pretrained on large-scale multimodal datasets\nencode rich visual and linguistic knowledge, making them a strong foundation\nfor robotics. Rather than training robotic policies from scratch, recent\napproaches adapt VLMs into vision-language-action (VLA) models that enable\nnatural language-driven perception and control. However, existing VLAs are\ntypically massive--often with billions of parameters--leading to high training\ncosts and limited real-world deployability. Moreover, they rely on academic and\nindustrial datasets, overlooking the growing availability of\ncommunity-collected data from affordable robotic platforms. In this work, we\npresent SmolVLA, a small, efficient, and community-driven VLA that drastically\nreduces both training and inference costs, while retaining competitive\nperformance. SmolVLA is designed to be trained on a single GPU and deployed on\nconsumer-grade GPUs or even CPUs. To further improve responsiveness, we\nintroduce an asynchronous inference stack decoupling perception and action\nprediction from action execution, allowing higher control rates with chunked\naction generation. Despite its compact size, SmolVLA achieves performance\ncomparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both\nsimulated as well as real-world robotic benchmarks and release all code,\npretrained models, and training data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01844.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65d66b494bbd0d92b641cdbb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
      "fullname": "Andres Marafioti",
      "name": "andito",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 185
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01943",
      "authors": [
        {
          "_id": "683e6b6424742a21489ec9f8",
          "name": "Xiao Fu",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9f9",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fa",
          "name": "Xian Liu",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fb",
          "name": "Jianhong Bai",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fc",
          "name": "Runsen Xu",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fd",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fe",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9ff",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63aef2cafcca84593e6682db/9mFDJaCOc6KLHlhboYA59.mp4"
      ],
      "publishedAt": "2025-06-02T17:57:06.000Z",
      "submittedOnDailyAt": "2025-06-03T01:57:30.514Z",
      "title": "학습 비디오 생성에 의한 로봇 조작의 협력적궤도제어",
      "submittedOnDailyBy": {
        "_id": "63aef2cafcca84593e6682db",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672409763337-noauth.jpeg",
        "isPro": false,
        "fullname": "Xiao Fu",
        "user": "lemonaddie",
        "type": "user"
      },
      "summary": "최근의 이미지 디퓨저 모델의 발전은 로봇의 의사결정 데이터의 생성에 강력한 가능성을 보여주고 있습니다. 프로젝트 조건은 더 세밀한 제어를 가능하게 합니다. 그러나 현재의 프로젝트에 기반한 방법은 주로 개별 물체의 움직임을 초점을 맞추며, 복잡한 로봇 조작에 필요한 다물체 상호작용을 이해하지 못합니다. 이 한계는 중복된 영역에서 다양한 특성의 결합으로 원인입니다. 이를 해결하기 위해, 우리는 RoboMaster를 소개합니다. RoboMaster는 협동적인 프로젝트 형성에 의한 물체 간 역학을 모델링합니다. 기존의 방법에 비해 물체를 분해하는 것이 아니라, 상호작용 전, 중, 후의 세 단계로 상호작용 과정을 분할합니다. 각 단계는 주인공 물체의 특성을 사용하여 모델링됩니다. 상호작용 전과 후의 단계에서 로봇 팔을, 상호작용 중의 단계에서는 조작 대상 물체를 특성으로 사용하며, 기존의 연구에서 볼 수 있는 다물체 특성 융합의 단점을 줄입니다. 또한,物体의 외관과 형상에 대한 잠재적 표현을 포함하여, 비디오 전체에서 주제의 의미적 일관성을 보장합니다. Bridge V2 데이터 세트에 대한 자세한 실험 및 자연스러운 상태의 평가에 따라, 우리의 방법은 기존의 방법보다 뛰어나며, 로봇 조작에서 프로젝트 제어 비디오 생성의 새로운 최상급 성능을 도모합니다.",
      "upvotes": 15,
      "discussionId": "683e6b6724742a21489eca8d",
      "ai_summary": "A novel framework, RoboMaster, enhances trajectory-controlled video generation for robotic manipulation by modeling inter-object dynamics through a collaborative trajectory formulation, achieving state-of-the-art performance on the Bridge V2 dataset.",
      "ai_keywords": [
        "video diffusion models",
        "trajectory conditions",
        "multi-object interaction",
        "multi-feature entanglement",
        "visual fidelity",
        "collaborative trajectory formulation",
        "pre-interaction",
        "interaction",
        "post-interaction",
        "appearance-aware latent representations",
        "shape-aware latent representations",
        "trajectory-controlled video generation",
        "robotic manipulation",
        "Bridge V2 dataset"
      ]
    },
    "publishedAt": "2025-06-02T13:57:06.000Z",
    "title": "Learning Video Generation for Robotic Manipulation with Collaborative\n  Trajectory Control",
    "summary": "Recent advances in video diffusion models have demonstrated strong potential\nfor generating robotic decision-making data, with trajectory conditions further\nenabling fine-grained control. However, existing trajectory-based methods\nprimarily focus on individual object motion and struggle to capture\nmulti-object interaction crucial in complex robotic manipulation. This\nlimitation arises from multi-feature entanglement in overlapping regions, which\nleads to degraded visual fidelity. To address this, we present RoboMaster, a\nnovel framework that models inter-object dynamics through a collaborative\ntrajectory formulation. Unlike prior methods that decompose objects, our core\nis to decompose the interaction process into three sub-stages: pre-interaction,\ninteraction, and post-interaction. Each stage is modeled using the feature of\nthe dominant object, specifically the robotic arm in the pre- and\npost-interaction phases and the manipulated object during interaction, thereby\nmitigating the drawback of multi-object feature fusion present during\ninteraction in prior work. To further ensure subject semantic consistency\nthroughout the video, we incorporate appearance- and shape-aware latent\nrepresentations for objects. Extensive experiments on the challenging Bridge V2\ndataset, as well as in-the-wild evaluation, demonstrate that our method\noutperforms existing approaches, establishing new state-of-the-art performance\nin trajectory-controlled video generation for robotic manipulation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63aef2cafcca84593e6682db/9mFDJaCOc6KLHlhboYA59.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01943.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63aef2cafcca84593e6682db",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672409763337-noauth.jpeg",
      "fullname": "Xiao Fu",
      "name": "lemonaddie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01853",
      "authors": [
        {
          "_id": "683e671483a130f817c4937a",
          "name": "Junliang Ye",
          "hidden": false
        },
        {
          "_id": "683e671483a130f817c4937b",
          "name": "Zhengyi Wang",
          "hidden": false
        },
        {
          "_id": "683e671483a130f817c4937c",
          "user": {
            "_id": "6522e4fbd89bc7773ddc4b58",
            "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
            "isPro": false,
            "fullname": "Ruowen Zhao",
            "user": "zzzrw",
            "type": "user"
          },
          "name": "Ruowen Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:03.565Z",
          "hidden": false
        },
        {
          "_id": "683e671483a130f817c4937d",
          "name": "Shenghao Xie",
          "hidden": false
        },
        {
          "_id": "683e671483a130f817c4937e",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/84A40qBWJeZaBv2qYO6Pj.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/Xl6IajTG3VravdQABgC6l.mp4"
      ],
      "publishedAt": "2025-06-02T16:40:50.000Z",
      "submittedOnDailyAt": "2025-06-03T03:57:42.122Z",
      "title": "ShapeLLM-Omni: 3D 생성 및 이해를 위한 원생모노모달 LLM",
      "submittedOnDailyBy": {
        "_id": "65a420cd90e65dc39a6abe9e",
        "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
        "isPro": false,
        "fullname": "yejunliang",
        "user": "yejunliang23",
        "type": "user"
      },
      "summary": "최근, ChatGPT-4o의 강력한 텍스트에서 이미지로 변환하는 능력으로, 원생의 다모델 대언어 모델의 즐거움을 증가시켰지만, 그 다모델 능력은 아직 이미지와 텍스트만 제한되어 있습니다. 그러나 이미지보다 3D 콘텐츠의 이해와 생성 능력은 동일하게 중요합니다. 이를 채워주기 위해, ShapeLLM-Omni, 즉 원생의 3D 대언어 모델을 제안합니다. 이는 3D 자산과 텍스트를 임의의 순서로 이해하고 생성할 수 있습니다. 먼저, 3D 오브젝트를 분산한 잠재 공간에 매핑하여 효율적이고 정확한 형태 표현과 재구성을 실현하기 위해 3D 벡터 캐리어 가시화 분산 자동 인코더 (VQVAE)를 훈련합니다. 3D-Alpaca라는 큰 연속적인 훈련 데이터 세트를 구축하여, 생성, 이해, 편집의 모든 것을 포함하며, 향후 연구와 훈련에 풍부한 리소스를 제공합니다. 마지막으로, Qwen-2.5-vl-7B-Instruct 모델을 3D-Alpaca 데이터 세트에 대해 인스톰스 기반 훈련을 수행합니다. 우리의 연구는 기본적인 3D 능력을 가진 다모델 모델의 확장에 대한 효과적인 시도를 제공하며, 미래의 3D 원생 AI 연구에 기여합니다. 프로젝트 페이지: https://github.com/JAMESYJL/ShapeLLM-Omni",
      "upvotes": 15,
      "discussionId": "683e671683a130f817c493cd",
      "ai_summary": "A native 3D large language model, ShapeLLM-Omni, is proposed to understand and generate 3D assets and text, trained using a 3D vector-quantized variational autoencoder and a new 3D-Alpaca dataset.",
      "ai_keywords": [
        "3D vector-quantized variational autoencoder (VQVAE)",
        "discrete latent space",
        "instruction-based training",
        "3D-Alpaca dataset",
        "3D-native AI"
      ]
    },
    "publishedAt": "2025-06-02T12:40:50.000Z",
    "title": "ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and\n  Understanding",
    "summary": "Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to\ngrowing appreciation for native multimodal large language models. However, its\nmultimodal capabilities remain confined to images and text. Yet beyond images,\nthe ability to understand and generate 3D content is equally crucial. To\naddress this gap, we propose ShapeLLM-Omni-a native 3D large language model\ncapable of understanding and generating 3D assets and text in any sequence.\nFirst, we train a 3D vector-quantized variational autoencoder (VQVAE), which\nmaps 3D objects into a discrete latent space to achieve efficient and accurate\nshape representation and reconstruction. Building upon the 3D-aware discrete\ntokens, we innovatively construct a large-scale continuous training dataset\nnamed 3D-Alpaca, encompassing generation, comprehension, and editing, thus\nproviding rich resources for future research and training. Finally, by\nperforming instruction-based training of the Qwen-2.5-vl-7B-Instruct model on\nthe 3D-Alpaca dataset. Our work provides an effective attempt at extending\nmultimodal models with basic 3D capabilities, which contributes to future\nresearch in 3D-native AI. Project page:\nhttps://github.com/JAMESYJL/ShapeLLM-Omni",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/84A40qBWJeZaBv2qYO6Pj.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/Xl6IajTG3VravdQABgC6l.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65a420cd90e65dc39a6abe9e",
      "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
      "fullname": "yejunliang",
      "name": "yejunliang23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24760",
      "authors": [
        {
          "_id": "683e6af92139ea008faa74ba",
          "user": {
            "_id": "65144e46004a986ccc9d21d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65144e46004a986ccc9d21d6/oImRY64V-UeYGdrbd_ozU.jpeg",
            "isPro": false,
            "fullname": "Zafir Stojanovski",
            "user": "zafstojano",
            "type": "user"
          },
          "name": "Zafir Stojanovski",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-03T06:28:46.378Z",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74bb",
          "user": {
            "_id": "6303f5f37b50dd9d0a371b28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6303f5f37b50dd9d0a371b28/H25eCzAYVwBtpSpD8tnUV.jpeg",
            "isPro": false,
            "fullname": "Oliver Stanley",
            "user": "OllieStanley",
            "type": "user"
          },
          "name": "Oliver Stanley",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:56.277Z",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74bc",
          "name": "Joe Sharratt",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74bd",
          "name": "Richard Jones",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74be",
          "name": "Abdulhakeem Adefioye",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74bf",
          "user": {
            "_id": "6304061c0547362a22a76a17",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661339692442-6304061c0547362a22a76a17.jpeg",
            "isPro": false,
            "fullname": "Jean Kaddour",
            "user": "JeanKaddour",
            "type": "user"
          },
          "name": "Jean Kaddour",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T09:13:54.739Z",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74c0",
          "name": "Andreas Köpf",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T16:20:18.000Z",
      "submittedOnDailyAt": "2025-06-03T02:02:10.153Z",
      "title": "레이싱 기م：증거 가능한 보상을 갖는 강화학습의 논리 환경",
      "submittedOnDailyBy": {
        "_id": "65144e46004a986ccc9d21d6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65144e46004a986ccc9d21d6/oImRY64V-UeYGdrbd_ozU.jpeg",
        "isPro": false,
        "fullname": "Zafir Stojanovski",
        "user": "zafstojano",
        "type": "user"
      },
      "summary": "Reasoning Gym (RG)는 실험적인 보상을 증명할 수 있는 강화학습의 논리론 환경의 라이브러리입니다. 100 이상의 데이터 생성자와 증명기가 있으며, 알기, 산술, 계산, 감각, 기하학, 그래프 이론, 로지키, 그리고 많은 일반적인 게임 분야를 확장하고 있습니다. 주요 혁신점은 이전 논리론 데이터 세트와 달리, 가변적인 복잡도를 가지고 무한히 데이터를 생성하는 것입니다. 이 프로세스 생성 접근 방식에 의해, 변화하는 난이도 수준에서 지속적인 평가가 가능합니다. 실험 결과를 통해, RG가 논리론 모델의 평가와 강화학습에서의 효과성을 보여주고 있습니다.",
      "upvotes": 15,
      "discussionId": "683e6afa2139ea008faa7531",
      "githubRepo": "https://github.com/open-thought/reasoning-gym",
      "ai_summary": "Reasoning Gym provides a library of reasoning environments with verifiable rewards and procedural data generation for reinforcement learning, enabling the evaluation and training of reasoning models at varying difficulty levels.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "data generators",
        "verifiers",
        "procedural generation",
        "reasoning models"
      ]
    },
    "publishedAt": "2025-05-30T12:20:18.000Z",
    "title": "REASONING GYM: Reasoning Environments for Reinforcement Learning with\n  Verifiable Rewards",
    "summary": "We introduce Reasoning Gym (RG), a library of reasoning environments for\nreinforcement learning with verifiable rewards. It provides over 100 data\ngenerators and verifiers spanning multiple domains including algebra,\narithmetic, computation, cognition, geometry, graph theory, logic, and various\ncommon games. Its key innovation is the ability to generate virtually infinite\ntraining data with adjustable complexity, unlike most previous reasoning\ndatasets, which are typically fixed. This procedural generation approach allows\nfor continuous evaluation across varying difficulty levels. Our experimental\nresults demonstrate the efficacy of RG in both evaluating and reinforcement\nlearning of reasoning models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24760.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65144e46004a986ccc9d21d6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65144e46004a986ccc9d21d6/oImRY64V-UeYGdrbd_ozU.jpeg",
      "fullname": "Zafir Stojanovski",
      "name": "zafstojano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00996",
      "authors": [
        {
          "_id": "683e7cbf402acb186580d5ec",
          "name": "Kinam Kim",
          "hidden": false
        },
        {
          "_id": "683e7cbf402acb186580d5ed",
          "name": "Junha Hyung",
          "hidden": false
        },
        {
          "_id": "683e7cbf402acb186580d5ee",
          "name": "Jaegul Choo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64797735a68454566356b708/a-8jl8yq3KdDLnuZEaB3K.mp4"
      ],
      "publishedAt": "2025-06-01T12:57:43.000Z",
      "submittedOnDailyAt": "2025-06-03T03:25:10.796Z",
      "title": "시계열 다이버지언스 조정에 의한 동영상 다이버지언스 모델의 다양한 제어",
      "submittedOnDailyBy": {
        "_id": "64797735a68454566356b708",
        "avatarUrl": "/avatars/3424d022dd8ad29b56eb41814c5c3dee.svg",
        "isPro": false,
        "fullname": "Kinam Kim",
        "user": "kinam0252",
        "type": "user"
      },
      "summary": "최근의 맥락에서 동영상에 대한 확산 모델의 발전은 고품질의 동영상 합성이 가능하게 되었지만, 제어 가능한 생성은 특히 데이터와 계산량의 제한 아래 어려워졌습니다. 현재의 조건付き 생성에 대한 조정 방법들은 외부 인코더나 구조적 변경에 의존하지만, 이들은 큰 데이터 세트를 필요로 하며, 일반적으로 공간적으로 일치하는 조건付き 생성에 제한되어, 유연성과 스케일러빌리티를 제한합니다. 본 논문에서는 시간적인 조건付き 조정(Temporal In-Context Fine-Tuning, TIC-FT)을 도입하여, 미리 제공된 동영상 확산 모델을 다양한 조건付き 생성 태스크에 효율적이고 광범위한 접근 방식을 제안합니다. 우리의 주요 아이디어는 조건과 목표 프레임을 시간적 축에 결합하고, 점차 증가하는 노이즈 레벨을 가진 중간 버퍼 프레임을 삽입하는 것입니다. 이러한 버퍼 프레임은mooth한 이동을 가능하게 하며, 조정 프로세스가 제공된 모델의 시간적 동적으로 일치시킵니다. TIC-FT는 구조적 변경을 필요로 하지 않으며, 10-30개의 훈련 샘플로 강력한 성능을 달성합니다. 우리의 방법은 CogVideoX-5B나 Wan-14B와 같은 규모의 기초 모델을 사용하여 이미지에서 동영상, 동영상에서 동영상으로 생성 등 다양한 태스크에서 검증되었습니다. 확장된 실험은 조건 충실성과 시각적 품질의 양면에서 기존의 baseline을 초과함을 보여주며, 훈련 및 추론 모두에서 고효율적입니다. 추가적인 결과를 참조하려면 https://kinam0252.github.io/TIC-FT/를 참고해주세요.",
      "upvotes": 14,
      "discussionId": "683e7cc1402acb186580d663",
      "ai_summary": "Temporal In-Context Fine-Tuning (TIC-FT) enhances pretrained video diffusion models for diverse conditional generation tasks with minimal data and without architectural changes.",
      "ai_keywords": [
        "text-to-video diffusion models",
        "fine-tuning",
        "external encoders",
        "architectural modifications",
        "Temporal In-Context Fine-Tuning",
        "condition and target frames",
        "buffer frames",
        "noise levels",
        "smooth transitions",
        "pretrained video diffusion models",
        "image-to-video generation",
        "video-to-video generation",
        "CogVideoX-5B",
        "Wan-14B"
      ]
    },
    "publishedAt": "2025-06-01T08:57:43.000Z",
    "title": "Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion\n  Models",
    "summary": "Recent advances in text-to-video diffusion models have enabled high-quality\nvideo synthesis, but controllable generation remains challenging, particularly\nunder limited data and compute. Existing fine-tuning methods for conditional\ngeneration often rely on external encoders or architectural modifications,\nwhich demand large datasets and are typically restricted to spatially aligned\nconditioning, limiting flexibility and scalability. In this work, we introduce\nTemporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach\nfor adapting pretrained video diffusion models to diverse conditional\ngeneration tasks. Our key idea is to concatenate condition and target frames\nalong the temporal axis and insert intermediate buffer frames with\nprogressively increasing noise levels. These buffer frames enable smooth\ntransitions, aligning the fine-tuning process with the pretrained model's\ntemporal dynamics. TIC-FT requires no architectural changes and achieves strong\nperformance with as few as 10-30 training samples. We validate our method\nacross a range of tasks, including image-to-video and video-to-video\ngeneration, using large-scale base models such as CogVideoX-5B and Wan-14B.\nExtensive experiments show that TIC-FT outperforms existing baselines in both\ncondition fidelity and visual quality, while remaining highly efficient in both\ntraining and inference. For additional results, visit\nhttps://kinam0252.github.io/TIC-FT/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64797735a68454566356b708/a-8jl8yq3KdDLnuZEaB3K.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64797735a68454566356b708",
      "avatarUrl": "/avatars/3424d022dd8ad29b56eb41814c5c3dee.svg",
      "fullname": "Kinam Kim",
      "name": "kinam0252",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24846",
      "authors": [
        {
          "_id": "683e82f2fa7ede4842f95214",
          "name": "Jingyan Shen",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f95215",
          "user": {
            "_id": "66f8689725464a7989b75845",
            "avatarUrl": "/avatars/43a61a528c5779103eaf5687ba44ee14.svg",
            "isPro": false,
            "fullname": "Jiarui Yao",
            "user": "FlippyDora",
            "type": "user"
          },
          "name": "Jiarui Yao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:40:58.100Z",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f95216",
          "user": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "isPro": false,
            "fullname": "Rui Yang",
            "user": "Ray2333",
            "type": "user"
          },
          "name": "Rui Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:40:16.572Z",
          "hidden": true
        },
        {
          "_id": "683e82f2fa7ede4842f95217",
          "name": "Yifan Sun",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f95218",
          "name": "Feng Luo",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f95219",
          "name": "Rui Pan",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f9521a",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f9521b",
          "name": "Han Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:44:28.000Z",
      "submittedOnDailyAt": "2025-06-03T03:51:59.115Z",
      "title": "MiCRO: 개인화 선호 학습을 위한 ミクスモデリング 및 맥락에 대한 루팅\n\n(注：原文中的“ミクスモデリング”在韩语中通常翻译为“믹스 모델링”，但在这里为了保持原文的准确性，保留了原文的翻译。如果需要更符合韩语习惯的翻译，可以考虑使用“믹스 모델링”。)",
      "submittedOnDailyBy": {
        "_id": "64d45451c34a346181b130dd",
        "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
        "isPro": false,
        "fullname": "Rui Yang",
        "user": "Ray2333",
        "type": "user"
      },
      "summary": "レベルモデリング은, 인간의 피드백을 기반으로 하여, 큰 규모의 언어 모델(LLMs)을 안전하게 구축하는 과정에서 중요한 단계입니다. 그러나 Bradley-Terry(BT) 모델을 기반으로 한 레벨 모델링은, 독특한 보상 함수를 가정하고, 고유의 다양성과 차별성을 감지하지 못합니다. 이러한 단순화는 LLMs가 개인화 및 다양한 대립을 지원할 수 있는 한계를 초래합니다.\n\n이론적으로, 인간의 취향이 다양한 그룹의 혼합 분포에 따라 할 때, 하나의 BT 모델은 불해상한 오류를 가지는 것을 보여줍니다. 현재의 해결책으로는, 다목적 학습과 미세한 注針를 사용하며, 이러한 문제를 해결할 수 있지만, 이들은 고비율적이고, 사전 정의된 특성에 제한되어, 인간 가치의 풍부함을 완전히 감지하는 것이 불가능합니다.\n\n본 논문에서는, 2단계 프레임워크 MiCRo를 통해, 明確한 미세한 注針를 필요로 하지 않는 개인화된 취향 학습을 강화하기 위해, 큰 규모의 이진 취향 데이터 세트를 활용합니다. 첫 번째 단계에서, MiCRo는上下文中 관련된 혼합 모델링 접근 방식을 도입하여, 다양한 인간 취향을 감지하게 됩니다. 두 번째 단계에서는, MiCRo는 특정한上下文中에 기반하여 동적으로 혼합 가중치를 조정하는 온라인 루팅 전략을 통합하여, 불확실성을 해결하고, 최소한의 추가 스テラチン를 필요로 하지 않도록, 효율적이고 scalable한 취향의 적응을 가능하게 합니다. 여러 취향 데이터 세트에 대한 실험은, MiCRo가 효과적으로 다양한 인간 취향을 감지하고, 하류의 개인화를 크게 향상시키는 것을 보여주었습니다.",
      "upvotes": 11,
      "discussionId": "683e82f3fa7ede4842f95246",
      "ai_summary": "MiCRo, a two-stage framework, improves personalized preference learning for large language models by leveraging binary preference datasets and dynamically adapting mixture weights based on context, effectively capturing diverse human preferences.",
      "ai_keywords": [
        "Reward modeling",
        "reinforcement learning from human feedback (RLHF)",
        "Large Language Models (LLMs)",
        "Bradley-Terry (BT) model",
        "mixture distribution",
        "personalization",
        "pluralistic alignment",
        "multi-objective learning",
        "context-aware mixture modeling",
        "online routing strategy"
      ]
    },
    "publishedAt": "2025-05-30T13:44:28.000Z",
    "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning",
    "summary": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d45451c34a346181b130dd",
      "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
      "fullname": "Rui Yang",
      "name": "Ray2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24298",
      "authors": [
        {
          "_id": "683d12963aa5ac98190e1eda",
          "name": "Wei Fu",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1edb",
          "name": "Jiaxuan Gao",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1edc",
          "name": "Xujie Shen",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1edd",
          "name": "Chen Zhu",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ede",
          "name": "Zhiyu Mei",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1edf",
          "name": "Chuyi He",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee0",
          "name": "Shusheng Xu",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee1",
          "name": "Guo Wei",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee2",
          "name": "Jun Mei",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee3",
          "name": "Jiashu Wang",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee4",
          "name": "Tongkai Yang",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee5",
          "name": "Binhang Yuan",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee6",
          "name": "Yi Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T07:18:25.000Z",
      "submittedOnDailyAt": "2025-06-03T05:39:21.066Z",
      "title": "AReaL: シェイプ의 규모가 큰 비동기 강화학습 시스템에서 언어논리가 대상입니다.",
      "submittedOnDailyBy": {
        "_id": "63159678915d0b80682fe9f9",
        "avatarUrl": "/avatars/a19e44ffe681099a155b8f8ecb53f0ce.svg",
        "isPro": false,
        "fullname": "Shusheng Xu",
        "user": "xssstory",
        "type": "user"
      },
      "summary": "강화학습(RL)은 특히 명리론으로 간주되는 태스크에 대해, 대규모 언어 모델(LLMs)의 훈련에서 급격히 발전하고 있는 패러다임으로 자리잡고 있습니다. LLMs의 효과적인 RL은 큰 병렬화가 필요하며, 효율적인 훈련 시스템의 급한 필요성을 강조하고 있습니다. 현재 많은 대규모 LLMs의 RL 시스템은 동시에 훈련과 생성을 교환하는 배치 설정으로 이루어져 있으며, 각 훈련 배치 내에서 동일한 내용(또는 최신 모델)으로 출력을 생성하고 있습니다. 이는 RL의 훈련을 안정화하지만, 엄격한 시스템 수준의 무용비가 발생할 수 있는 문제를 제기하고 있습니다. 생성은 배치 내에서 가장 긴 출력이 완성될 때까지 대기하는 것이 필요하며, 이로 인해 GPU의 사용율이 떨어집니다. 우리는 완전한 비동기 RL 시스템인 AReaL을 소개합니다. AReaL은 생성과 훈련을 완전히 분리하고, 출력을 계속해서 생성하면서, 훈련 워크러는 데이터가 모이면 모델을 업데이트합니다. AReaL은 GPU의 사용율을 크게 높이기 위해 시스템 수준의 최적화를 도입하고 있습니다. RL의 훈련을 안정화하기 위해, AReaL은 출력과 훈련 워크러의 작업량을 균형을 맞추고, 데이터의 만료를 제어하고, 만료도를 강화된 PPO의 변형을 도입하여 만료된 훈련 샘플을 더 잘 처리하는 것을 목표로 합니다. 수학과 코드의 명리론 벤치마크에서 분산된 실험에 따라, AReaL은 동일한 GPU를 사용하며, 가장 좋은 동기 시스템과 같은 또는 개선된 최종 성능을 보여주는 경우, 2.57배의 훈련 속도 업그레이드를 달성합니다. AReaL의 코드는 https://github.com/inclusionAI/AReaL/ 에서 공개되어 있습니다.",
      "upvotes": 10,
      "discussionId": "683d12973aa5ac98190e1f19",
      "ai_summary": "AReaL, a fully asynchronous reinforcement learning system, decouples generation and training to achieve higher GPU utilization and up to 2.57x training speedup for large language models on reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "asynchronous system",
        "rollouts",
        "model update",
        "GPU utilization",
        "PPO",
        "data staleness",
        "training speedup"
      ]
    },
    "publishedAt": "2025-05-30T03:18:25.000Z",
    "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning",
    "summary": "Reinforcement learning (RL) has become a trending paradigm for training large\nlanguage models (LLMs), particularly for reasoning tasks. Effective RL for LLMs\nrequires massive parallelization and poses an urgent need for efficient\ntraining systems. Most existing large-scale RL systems for LLMs are synchronous\nby alternating generation and training in a batch setting, where the rollouts\nin each training batch are generated by the same (or latest) model. This\nstabilizes RL training but suffers from severe system-level inefficiency.\nGeneration must wait until the longest output in the batch is completed before\nmodel update, resulting in GPU underutilization. We present AReaL, a\nfully asynchronous RL system that completely decouples generation from\ntraining. Rollout workers in AReaL continuously generate new outputs without\nwaiting, while training workers update the model whenever a batch of data is\ncollected. AReaL also incorporates a collection of system-level optimizations,\nleading to substantially higher GPU utilization. To stabilize RL training,\nAReaL balances the workload of rollout and training workers to control data\nstaleness, and adopts a staleness-enhanced PPO variant to better handle\noutdated training samples. Extensive experiments on math and code reasoning\nbenchmarks show that AReaL achieves up to 2.57times training\nspeedup compared to the best synchronous systems with the same number of GPUs\nand matched or even improved final performance. The code of AReaL is available\nat https://github.com/inclusionAI/AReaL/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24298.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63159678915d0b80682fe9f9",
      "avatarUrl": "/avatars/a19e44ffe681099a155b8f8ecb53f0ce.svg",
      "fullname": "Shusheng Xu",
      "name": "xssstory",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23907",
      "authors": [
        {
          "_id": "683e6838a6815c77acb18ea3",
          "user": {
            "_id": "650e3abcae507a2c7c847baa",
            "avatarUrl": "/avatars/a138bc6c9ba80a486fb36f4e1aa16b33.svg",
            "isPro": false,
            "fullname": "Amirhossein Alimohammadi",
            "user": "Amirhossein-Alimohammadi",
            "type": "user"
          },
          "name": "Amirhossein Almohammadi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:59.757Z",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea4",
          "name": "Aryan Mikaeili",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea5",
          "name": "Sauradip Nag",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea6",
          "name": "Negar Hassanpour",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea7",
          "name": "Andrea Tagliasacchi",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea8",
          "name": "Ali Mahdavi-Amiri",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/650e3abcae507a2c7c847baa/dRNPZqV4HrW79gs506jk7.mp4"
      ],
      "publishedAt": "2025-05-29T18:00:56.000Z",
      "submittedOnDailyAt": "2025-06-03T01:51:49.601Z",
      "title": "코라: 적은 단계로 인한 분화에 의한 통신에 대한 이미지 편집",
      "submittedOnDailyBy": {
        "_id": "650e3abcae507a2c7c847baa",
        "avatarUrl": "/avatars/a138bc6c9ba80a486fb36f4e1aa16b33.svg",
        "isPro": false,
        "fullname": "Amirhossein Alimohammadi",
        "user": "Amirhossein-Alimohammadi",
        "type": "user"
      },
      "summary": "화상 편집은 컴퓨터 그래픽, 비전, VFX 등 중요한 작업 중 하나이며, 최근 확산 기반의 방법들은 고속으로 고품질의 결과를 구현하고 있습니다. 그러나 비정형 변형, 객체의 수정, 콘텐츠의 생성 등 구조적인 변경이 필요한 편집은 어려워 할 수 있습니다. 현재 적은 단계의 편집 접근 방식은 관련없는 테크스, 원화상의 키 속성(예: 자세)의 보존에 어려움을 겪습니다. 우리는 대응 관계를 기반으로 노이즈 보정과 interpolation된 어텐션 맵을 도입하여 이러한 제한을 해결하는 새로운 편집 프레임워크 Cora를 소개합니다. 우리 방식은 소스와 타겟 화상의 테크스 및 구조를 대응 관계를 통해 어레이링하고, 필요에 따라 새로운 콘텐츠를 생성함으로써 정확한 테크스 전달을 가능하게 합니다. Cora는 콘텐츠의 생성과 저장의 균형을 제어합니다. 확장된 실험은 양적으로 및 질적으로 Cora는 구조, 테크스, 그리고 다양한 편집 중의 동일성을 유지하며, 자세 변화, 객체 추가, 테크스의 보완 등 편집 중 뛰어난 성능을 보여주었습니다. 사용자 스테이지는 Cora가 최상위의 결과를 제공하고 대체 방법을 초월하고 있음을 확인했습니다.",
      "upvotes": 8,
      "discussionId": "683e683aa6815c77acb18f03",
      "projectPage": "https://cora-edit.github.io/",
      "githubRepo": "https://github.com/alimohammadiamirhossein/cora",
      "ai_summary": "Cora framework enhances image editing through correspondence-aware noise correction and interpolated attention maps, excelling in structure and texture preservation and generation.",
      "ai_keywords": [
        "diffusion-based methods",
        "non-rigid deformations",
        "object modifications",
        "content generation",
        "correspondence-aware noise correction",
        "interpolated attention maps",
        "semantic correspondence",
        "texture transfer",
        "content generation",
        "preservation",
        "pose changes",
        "object addition",
        "texture refinements"
      ]
    },
    "publishedAt": "2025-05-29T14:00:56.000Z",
    "title": "Cora: Correspondence-aware image editing using few step diffusion",
    "summary": "Image editing is an important task in computer graphics, vision, and VFX,\nwith recent diffusion-based methods achieving fast and high-quality results.\nHowever, edits requiring significant structural changes, such as non-rigid\ndeformations, object modifications, or content generation, remain challenging.\nExisting few step editing approaches produce artifacts such as irrelevant\ntexture or struggle to preserve key attributes of the source image (e.g.,\npose). We introduce Cora, a novel editing framework that addresses these\nlimitations by introducing correspondence-aware noise correction and\ninterpolated attention maps. Our method aligns textures and structures between\nthe source and target images through semantic correspondence, enabling accurate\ntexture transfer while generating new content when necessary. Cora offers\ncontrol over the balance between content generation and preservation. Extensive\nexperiments demonstrate that, quantitatively and qualitatively, Cora excels in\nmaintaining structure, textures, and identity across diverse edits, including\npose changes, object addition, and texture refinements. User studies confirm\nthat Cora delivers superior results, outperforming alternatives.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650e3abcae507a2c7c847baa/dRNPZqV4HrW79gs506jk7.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23907.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650e3abcae507a2c7c847baa",
      "avatarUrl": "/avatars/a138bc6c9ba80a486fb36f4e1aa16b33.svg",
      "fullname": "Amirhossein Alimohammadi",
      "name": "Amirhossein-Alimohammadi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23001",
      "authors": [
        {
          "_id": "6839f87c49d173e7b23f220b",
          "user": {
            "_id": "66fa2c61c25c3fcb32f9f131",
            "avatarUrl": "/avatars/05387c30f2d1803fa0a5b176c3706772.svg",
            "isPro": false,
            "fullname": "Yize Cheng",
            "user": "yizecheng",
            "type": "user"
          },
          "name": "Yize Cheng",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T18:27:47.059Z",
          "hidden": false
        },
        {
          "_id": "6839f87c49d173e7b23f220c",
          "user": {
            "_id": "659dc02d72238596c24d49f5",
            "avatarUrl": "/avatars/d4600d23ccc72f296fab7f626d5895e7.svg",
            "isPro": false,
            "fullname": "Wenxiao Wang",
            "user": "wangwenxiao",
            "type": "user"
          },
          "name": "Wenxiao Wang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T18:28:18.975Z",
          "hidden": false
        },
        {
          "_id": "6839f87c49d173e7b23f220d",
          "user": {
            "_id": "63449874ee1504dbcd59af3d",
            "avatarUrl": "/avatars/57a805d82d16de9544c98585bd7a3e55.svg",
            "isPro": false,
            "fullname": "MazdaM",
            "user": "mmoayeri",
            "type": "user"
          },
          "name": "Mazda Moayeri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T20:01:34.913Z",
          "hidden": false
        },
        {
          "_id": "6839f87c49d173e7b23f220e",
          "name": "Soheil Feizi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T02:22:14.000Z",
      "submittedOnDailyAt": "2025-06-03T00:16:12.678Z",
      "title": "다이파크：LLMs에서 백도어 사용으로 테스트 세트의 오염을 확실하게 플래그하는 방법",
      "submittedOnDailyBy": {
        "_id": "66fa2c61c25c3fcb32f9f131",
        "avatarUrl": "/avatars/05387c30f2d1803fa0a5b176c3706772.svg",
        "isPro": false,
        "fullname": "Yize Cheng",
        "user": "yizecheng",
        "type": "user"
      },
      "summary": "오픈 벤치마크는 대규모 언어 모델의 평가와 발전에 필수적이며 재현성과 투명성을 제공하지만, 그 접근성은 테스트 세트 오염 시 약해지는 가능성이 있습니다. 본 논문에서는, 모델이 벤치마크 테스트 세트를 사용했는지 특정하기 위해 'DyePack' 프레임워크를 활용합니다. 백도 샘플을 테스트 데이터와 섞어 모델이 이를 학습한 것을 플래그로 표시하는 것을 목표로 합니다. 은행이 드라이버백 패크를 돈과 섞어 도둑을 식별하는 것처럼, DyePack은 백도 샘플을 테스트 데이터와 섞어 모델이 이를 학습한 것을 플래그로 표시하는 것을 목표로 합니다. 또한 다수의 백도를 포함하는 원칙적인 설계를 제안하고, 플래그된 모델의 모든 것을 대상으로 확률적인 Falsi Positive Rate(FPR)를 계산할 수 있습니다. 이로써 거짓사건을 방지하면서 오염의 이러한 현상이 발견된 경우 강력한 증거를 제공합니다. DyePack은 5개의 모델을 3개의 데이터 세트로 평가하며, 다수 선택과 개방적 생성 태스크를 커버합니다. 다수 선택 문제에서, MMLU-Pro와 Big-Bench-Hard에서 8개의 백도를 사용하며 확률적인 FPR가 낮으며, 0.000073%까지 확인하여 모든 오염 모델을 성공적으로 감지합니다. 개방적 생성 태스크에서, 모델을 모두 오염 모델로 감지하고 6개의 백도를 사용하며 확률적인 FPR가 0.127%로, 모든 오염 모델을 감지합니다.",
      "upvotes": 8,
      "discussionId": "6839f87c49d173e7b23f222c",
      "githubRepo": "https://github.com/chengez/DyePack",
      "ai_summary": "DyePack, a framework using backdoor attacks, identifies models that leveraged benchmark test sets during training by introducing benign backdoor samples, ensuring precise false positive rates while preventing false accusations.",
      "ai_keywords": [
        "backdoor attacks",
        "test set contamination",
        "false positive rate",
        "FPR",
        "DyePack",
        "multiple backdoors",
        "stochastic targets",
        "MMLU-Pro",
        "Big-Bench-Hard",
        "Alpaca",
        "multiple-choice questions",
        "open-ended generation tasks"
      ]
    },
    "publishedAt": "2025-05-28T22:22:14.000Z",
    "title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using\n  Backdoors",
    "summary": "Open benchmarks are essential for evaluating and advancing large language\nmodels, offering reproducibility and transparency. However, their accessibility\nmakes them likely targets of test set contamination. In this work, we introduce\nDyePack, a framework that leverages backdoor attacks to identify models that\nused benchmark test sets during training, without requiring access to the loss,\nlogits, or any internal details of the model. Like how banks mix dye packs with\ntheir money to mark robbers, DyePack mixes backdoor samples with the test data\nto flag models that trained on it. We propose a principled design incorporating\nmultiple backdoors with stochastic targets, enabling exact false positive rate\n(FPR) computation when flagging every model. This provably prevents false\naccusations while providing strong evidence for every detected case of\ncontamination. We evaluate DyePack on five models across three datasets,\ncovering both multiple-choice and open-ended generation tasks. For\nmultiple-choice questions, it successfully detects all contaminated models with\nguaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard\nusing eight backdoors. For open-ended generation tasks, it generalizes well and\nidentifies all contaminated models on Alpaca with a guaranteed false positive\nrate of just 0.127% using six backdoors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66fa2c61c25c3fcb32f9f131",
      "avatarUrl": "/avatars/05387c30f2d1803fa0a5b176c3706772.svg",
      "fullname": "Yize Cheng",
      "name": "yizecheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00577",
      "authors": [
        {
          "_id": "683e646de9f216ff5a3e5dea",
          "user": {
            "_id": "658ab894c4b2004663dff3ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658ab894c4b2004663dff3ae/oPRnFuW2Imaa2KkYWNbSf.jpeg",
            "isPro": false,
            "fullname": "YUFA ZHOU",
            "user": "MasterZhou",
            "type": "user"
          },
          "name": "Yufa Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:13.058Z",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5deb",
          "user": {
            "_id": "66968099c952e09a4cb29f78",
            "avatarUrl": "/avatars/bd3a361fe5315e26e9ae328071704eed.svg",
            "isPro": false,
            "fullname": "Wang",
            "user": "Steven-Shaobo",
            "type": "user"
          },
          "name": "Shaobo Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:16.609Z",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5dec",
          "name": "Xingyu Dong",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5ded",
          "name": "Xiangqi Jin",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5dee",
          "name": "Yifang Chen",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5def",
          "name": "Yue Min",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5df0",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5df1",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5df2",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5df3",
          "name": "Linfeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T14:22:40.000Z",
      "submittedOnDailyAt": "2025-06-03T01:32:07.834Z",
      "title": "「경제학자처럼 이유를 찾아봅시다：경제 문제에서 훈련 후 전략적인 일반적인 일반화에 따른 영향」\n\n(Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs)",
      "submittedOnDailyBy": {
        "_id": "658ab894c4b2004663dff3ae",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658ab894c4b2004663dff3ae/oPRnFuW2Imaa2KkYWNbSf.jpeg",
        "isPro": false,
        "fullname": "YUFA ZHOU",
        "user": "MasterZhou",
        "type": "user"
      },
      "summary": "直接훈련하는 대 언어 모델(LLMs) 다중 에이전트 시스템(MAS)는 복잡한 보상 모델링, 동적인 에이전트 상호작용, 그리고 강력한 일반화 요구에 의해 어려워질 수 있습니다. 본 논문에서는 후 훈련 기술, 특히 관찰 가능한 보상을 포함한 학습 검증(SFT)과 강화 학습(RLVR)에 대해 다중 에이전트 시나리오에 적용할 수 있는지 조사합니다. 우리는 경제적 논리 테스트 벤더로 테스트를 진행하며, 수학과 게임 이론의 강력한 기초, 구조화된 분석 논리의 필요성, 시장 설계, 리소스 분배, 정책 분석 등 현실 세계의 응용과 관련된 것을 활용합니다. 우리는 Recon(ECONomist처럼 논리적으로 생각)를 소개합니다. Recon은 2,100건의 고품질 경제적 논리 문제의 손동작 선택 데이터 세트를 사용하여 7B 파라미터의 오픈 소스 LLM을 후 훈련합니다. 경제적 논리 벤치마크와 다중 에이전트 게임의 세부 평가에 따라, 구조화된 논리와 경제적 합리성을 명확히 향상시켰습니다. 이러한 결과는 영역에 맞는 후 훈련이 논리와 에이전트의 대응을 향상시키는 가능성을 강조하며, SFT와 RL이 모델의 행동을 형성하는 역할을 강조합니다. 코드는 https://github.com/MasterZhou1/Recon 에 접근할 수 있습니다.",
      "upvotes": 7,
      "discussionId": "683e646ee9f216ff5a3e5e2c",
      "githubRepo": "https://github.com/MasterZhou1/Recon",
      "ai_summary": "Post-training techniques such as Supervised Fine-Tuning and Reinforcement Learning with Verifiable Rewards improve the reasoning and economic rationality of Large Language Models in multi-agent scenarios through domain-aligned training.",
      "ai_keywords": [
        "Large Language Models",
        "Multi-Agent Systems",
        "Supervised Fine-Tuning",
        "Reinforcement Learning with Verifiable Rewards",
        "economic reasoning",
        "domain-aligned post-training"
      ]
    },
    "publishedAt": "2025-05-31T10:22:40.000Z",
    "title": "Reasoning Like an Economist: Post-Training on Economic Problems Induces\n  Strategic Generalization in LLMs",
    "summary": "Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS)\nremains challenging due to intricate reward modeling, dynamic agent\ninteractions, and demanding generalization requirements. This paper explores\nwhether post-training techniques, specifically Supervised Fine-Tuning (SFT) and\nReinforcement Learning with Verifiable Rewards (RLVR), can effectively\ngeneralize to multi-agent scenarios. We use economic reasoning as a\ntestbed, leveraging its strong foundations in mathematics and game theory, its\ndemand for structured analytical reasoning, and its relevance to real-world\napplications such as market design, resource allocation, and policy analysis.\nWe introduce Recon (Reasoning like an\nECONomist), a 7B-parameter open-source LLM post-trained on a\nhand-curated dataset of 2,100 high-quality economic reasoning problems.\nComprehensive evaluation on economic reasoning benchmarks and multi-agent games\nreveals clear improvements in structured reasoning and economic rationality.\nThese results underscore the promise of domain-aligned post-training for\nenhancing reasoning and agent alignment, shedding light on the roles of SFT and\nRL in shaping model behavior. Code is available at\nhttps://github.com/MasterZhou1/Recon .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00577.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658ab894c4b2004663dff3ae",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658ab894c4b2004663dff3ae/oPRnFuW2Imaa2KkYWNbSf.jpeg",
      "fullname": "YUFA ZHOU",
      "name": "MasterZhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23977",
      "authors": [
        {
          "_id": "683e380cb028cae60270adcf",
          "user": {
            "_id": "6702d9e2db3b7a57f9420e8d",
            "avatarUrl": "/avatars/2e65e83e8d13ca129f6382deb6e8bdfc.svg",
            "isPro": false,
            "fullname": "Yichen Feng",
            "user": "EthanSta",
            "type": "user"
          },
          "name": "Yichen Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:47:01.523Z",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add0",
          "user": {
            "_id": "653df1323479e9ebbe3eb6cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
            "isPro": true,
            "fullname": "Zhangchen Xu",
            "user": "zhangchenxu",
            "type": "user"
          },
          "name": "Zhangchen Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:58.968Z",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add1",
          "name": "Fengqing Jiang",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add2",
          "name": "Yuetai Li",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add3",
          "name": "Bhaskar Ramasubramanian",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add4",
          "name": "Luyao Niu",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add5",
          "name": "Bill Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add6",
          "name": "Radha Poovendran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T20:08:36.000Z",
      "submittedOnDailyAt": "2025-06-03T01:08:43.749Z",
      "title": "VisualSphinx: 대규모 합성 시각 로직 퍼즐의 RL용\n\n(注意：虽然要求不添加解释或额外文本，但为了确保翻译的准确性和专业性，这里提供了一个更符合韩国语习惯的表达方式。)",
      "submittedOnDailyBy": {
        "_id": "653df1323479e9ebbe3eb6cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
        "isPro": true,
        "fullname": "Zhangchen Xu",
        "user": "zhangchenxu",
        "type": "user"
      },
      "summary": "비지언 언어 모델(VLMs)은 효과적인 다모우드 논리와 로지컬적으로 일관된 판단을 수행하는 것을 기대하고 있습니다. 이는 그림 해석이나 공간 문제 해결 등의 태스크에서 중요합니다. 그러나 현재의 VLM의 논리는 대규모적이고 구조적으로 좋은 훈련 데이터 세트에 부족합니다. 이러한 빈점을 메우기 위해 우리는 VisualSphinx를 제안합니다. 이것은 최초의 대규모 합성적인 시각적인 로지컬 논리 훈련 데이터 세트입니다. 이미지 합성에서의 답의 근본화의 도전을 해결하기 위해 우리는 규칙으로부터 이미지 합성의 파이프라인을 제안합니다. 이는 다양한 질문으로부터 미지의 규칙을 추출하고 확장하여 미지의 샘플의 구성에 대한 근본화 합성 이미지의 코드를 생성합니다. 실험은 VisualSphinx에서 GRPO를 사용하여 훈련된 VLM은 우리 데이터 세트의 로지컬 일관성과 해석성을 통해 베타를 받습니다. 논리논리 태스크에서 성능 향상을 보여줍니다. VisualSphinx에서 개발된 확장된 논리 능력은 대수논리, 산술논리, 기하논리 등 다른 논리 태스크에도 이점을 제공합니다.",
      "upvotes": 7,
      "discussionId": "683e380eb028cae60270ae82",
      "ai_summary": "VisualSphinx provides a large-scale synthetic dataset to improve multimodal reasoning in vision language models, enhancing performance on various logical reasoning tasks.",
      "ai_keywords": [
        "vision language models",
        "multimodal reasoning",
        "logical reasoning",
        "large-scale synthetic visual logical reasoning",
        "image synthesis",
        "rule-to-image synthesis",
        "GRPO"
      ]
    },
    "publishedAt": "2025-05-29T16:08:36.000Z",
    "title": "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL",
    "summary": "Vision language models (VLMs) are expected to perform effective multimodal\nreasoning and make logically coherent decisions, which is critical to tasks\nsuch as diagram understanding and spatial problem solving. However, current VLM\nreasoning lacks large-scale and well-structured training datasets. To bridge\nthis gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic\nvisual logical reasoning training data. To tackle the challenge of image\nsynthesis with grounding answers, we propose a rule-to-image synthesis\npipeline, which extracts and expands puzzle rules from seed questions and\ngenerates the code of grounding synthesis image synthesis for puzzle sample\nassembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx\nbenefit from logical coherence and readability of our dataset and exhibit\nimproved performance on logical reasoning tasks. The enhanced reasoning\ncapabilities developed from VisualSphinx also benefit other reasoning tasks\nsuch as algebraic reasoning, arithmetic reasoning and geometry reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "zhangchenxu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23059",
      "authors": [
        {
          "_id": "683b4b86c4b9677f3f7125e5",
          "user": {
            "_id": "6683c3b04905815dcffe7a21",
            "avatarUrl": "/avatars/73c50843d99b6fb8b1ee8fe11106c4ce.svg",
            "isPro": false,
            "fullname": "Dohyeon Lee",
            "user": "waylight3",
            "type": "user"
          },
          "name": "Dohyeon Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:50:07.024Z",
          "hidden": false
        },
        {
          "_id": "683b4b86c4b9677f3f7125e6",
          "user": {
            "_id": "645aedd221ab438e732bff43",
            "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
            "isPro": false,
            "fullname": "Yeonseok Jeong",
            "user": "yeonseokjeong",
            "type": "user"
          },
          "name": "Yeonseok Jeong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:42:20.758Z",
          "hidden": false
        },
        {
          "_id": "683b4b86c4b9677f3f7125e7",
          "name": "Seung-won Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T04:04:25.000Z",
      "submittedOnDailyAt": "2025-06-03T01:42:51.938Z",
      "title": "토큰으로부터 행동으로: 상태기의 이유론을 활용하여 정보 검색에서의 과도한 고려를 줄임",
      "submittedOnDailyBy": {
        "_id": "645aedd221ab438e732bff43",
        "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
        "isPro": false,
        "fullname": "Yeonseok Jeong",
        "user": "yeonseokjeong",
        "type": "user"
      },
      "summary": "Chain-of-Thought (CoT) prompting은 복잡한 논리론을 대규모 언어 모델(LLMs)에서 가능하게 합니다. 이 기술은 정보 검색(IR)에도 적용이 가능합니다. 그러나, 과도한 논리론으로 모델이 과도한 긴 의미적인冗長한 문장을 생성하고, 거의 무용무용한 것을 생성하면서 모델의 성능이 떨어지는 경우가 있습니다. 우리는 IR에서 두 가지 중요한 문제점을 식별했습니다. 첫째는 유사한 상태를 재방문하는冗長한 경로, 둘째는 사용자의 의도에서 벗어난 논리론입니다. 이러한 문제를 해결하기 위해, 우리는 State Machine Reasoning(SMR)을 제안했습니다. SMR은 빠른 종료와 미세한 제어를 지원하는 이산적인 행동(Refine, Rerank, Stop)으로 이루어져 있습니다. BEIR와 BRIGHT 벤치마크에서의 실험은, SMR은 토큰 사용량을 74.4% 줄일 수 있으며, nDCG@10의 검색 성능을 3.4% 향상시켰음을 보여주었습니다. SMR은 LLMs와 검색 모델에 대한 일반화가 가능하며, 특정 태스크에专用한 튜닝이 필요하지 않고, 단순한 CoT 논리론의 실용적인 대체로 제공됩니다. 코드와 세부 사항은 https://github.com/ldilab/SMR에서 접근할 수 있습니다.",
      "upvotes": 7,
      "discussionId": "683b4b87c4b9677f3f712609",
      "githubRepo": "https://github.com/ldilab/SMR",
      "ai_summary": "State Machine Reasoning (SMR) improves information retrieval performance and reduces token usage in large language models by addressing overthinking through a discrete action framework.",
      "ai_keywords": [
        "Chain-of-Thought",
        "State Machine Reasoning",
        "IR",
        "redundant trajectories",
        "misguided reasoning",
        "early stopping",
        "nDCG@10"
      ]
    },
    "publishedAt": "2025-05-29T00:04:25.000Z",
    "title": "From Token to Action: State Machine Reasoning to Mitigate Overthinking\n  in Information Retrieval",
    "summary": "Chain-of-Thought (CoT) prompting enables complex reasoning in large language\nmodels (LLMs), including applications in information retrieval (IR). However,\nit often leads to overthinking, where models produce excessively long and\nsemantically redundant traces with little or no benefit. We identify two key\nchallenges in IR: redundant trajectories that revisit similar states and\nmisguided reasoning that diverges from user intent. To address these, we\npropose State Machine Reasoning (SMR), a transition-based reasoning framework\ncomposed of discrete actions (Refine, Rerank, Stop) that support early stopping\nand fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show\nthat SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token\nusage by 74.4%. It generalizes across LLMs and retrievers without requiring\ntask-specific tuning, offering a practical alternative to conventional CoT\nreasoning. The code and details are available at https://github.com/ldilab/SMR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23059.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645aedd221ab438e732bff43",
      "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
      "fullname": "Yeonseok Jeong",
      "name": "yeonseokjeong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01881",
      "authors": [
        {
          "_id": "683e6708ef9c250c6642783c",
          "user": {
            "_id": "65c431a609672feb8cac22e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c431a609672feb8cac22e7/obXCXUOgEoCuJbGuzYOGA.jpeg",
            "isPro": false,
            "fullname": "Yaoyao Qian",
            "user": "FreaxRuby",
            "type": "user"
          },
          "name": "Yaoyao Qian",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T03:07:55.037Z",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c6642783d",
          "name": "Jindan Huang",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c6642783e",
          "name": "Yuanli Wang",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c6642783f",
          "user": {
            "_id": "636681feaa6a4af6073ba73e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636681feaa6a4af6073ba73e/u_0moYNu6as9Sszp-ej95.png",
            "isPro": true,
            "fullname": "Simon Yu",
            "user": "simonycl",
            "type": "user"
          },
          "name": "Simon Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:06.559Z",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c66427840",
          "name": "Kyrie Zhixuan Zhou",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c66427841",
          "name": "Jiayuan Mao",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c66427842",
          "name": "Mingfu Liang",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c66427843",
          "name": "Hanhan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:11:10.000Z",
      "submittedOnDailyAt": "2025-06-03T01:38:44.220Z",
      "title": "태스크를 위한 다이어로지에서 의도의 트라이거 가능성의 모델화에 필요한 구조적인 트래지셈의 시간 선택: 행동하는 때, 기다리는 때",
      "submittedOnDailyBy": {
        "_id": "65c431a609672feb8cac22e7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c431a609672feb8cac22e7/obXCXUOgEoCuJbGuzYOGA.jpeg",
        "isPro": false,
        "fullname": "Yaoyao Qian",
        "user": "FreaxRuby",
        "type": "user"
      },
      "summary": "TASK-oriented Dialogue Systems은 사용자의 요구가 문법적으로 완전하지만, 적절한 시스템 행동에 필요한 구조적 정보를 부족할 때 어려움을 겪습니다. 이는 사용자가 자신의 필요를 완전히 이해하지 않는 반면, 시스템이 구체적인 의도를 정의하는 것이 필요하기 때문입니다. 현재의 LLM 기반의 Agent는 언어적으로 완전한지, 혹은 кон텍스트에 따라 텍스트를 불러올 수 있는지 효과적으로 구분할 수 없기 때문에, 협업적인 의도 형성의 프레임워크를 미欠하여 문제를 드러냅니다. 우리는 STORM이라는 프레임워크를 소개합니다. 이 프레임워크는 UserLLM(완전한 내부 액세스)와 AgentLLM(관찰 가능한 행동만 가능)의 대화에서 대칭적인 정보역학을 모델링합니다. STORM은 표현의 추적과 잠재적인 인지적 트랜지션을 감지하는 코퍼스를 생성하고, 협업적인 이해의 체계적인 분석을 통해 개발할 수 있습니다. 우리의 기여는 다음과 같습니다: (1) 대화 시스템의 대칭적인 정보 처리의 형식화, (2) 의도 형성의 추적과 협업적인 이해의 발전, (3) 태스크 성능과 함께 내부적인 인지의 향상을 측정하는 평가 지표입니다. 4개의 언어 모델을 경험적으로 검증한 결과, 중度的 불확실성(40-60%)은 특정 스캔더에서 완전한 투명성을 초과할 수 있다는 것을 알게 되었으며, 모델 고유의 패턴은 인간과 AI의 협업에서 가장 적절한 정보의 완전성을 재평가하는 데 기여합니다. 이러한 발견은 대칭적인 이유의 동적으로 기여하고, 불확실성 조정된 대화 시스템의 설계에 정보를 제공합니다.",
      "upvotes": 6,
      "discussionId": "683e670bef9c250c664278be",
      "projectPage": "https://nanostorm.netlify.app/",
      "githubRepo": "https://github.com/H-Freax/Storm",
      "ai_summary": "STORM frameworks facilitates collaborative intent formation in task-oriented dialogue systems by modeling asymmetric information dynamics between UserLLM and AgentLLM.",
      "ai_keywords": [
        "UserLLM",
        "AgentLLM",
        "asymmetric information dynamics",
        "collaborative understanding",
        "intent formation",
        "expression trajectories",
        "latent cognitive transitions",
        "uncertainty-calibrated dialogue systems"
      ]
    },
    "publishedAt": "2025-06-02T13:11:10.000Z",
    "title": "WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent\n  Triggerability in Task-Oriented Dialogue",
    "summary": "Task-oriented dialogue systems often face difficulties when user utterances\nseem semantically complete but lack necessary structural information for\nappropriate system action. This arises because users frequently do not fully\nunderstand their own needs, while systems require precise intent definitions.\nCurrent LLM-based agents cannot effectively distinguish between linguistically\ncomplete and contextually triggerable expressions, lacking frameworks for\ncollaborative intent formation. We present STORM, a framework modeling\nasymmetric information dynamics through conversations between UserLLM (full\ninternal access) and AgentLLM (observable behavior only). STORM produces\nannotated corpora capturing expression trajectories and latent cognitive\ntransitions, enabling systematic analysis of collaborative understanding\ndevelopment. Our contributions include: (1) formalizing asymmetric information\nprocessing in dialogue systems; (2) modeling intent formation tracking\ncollaborative understanding evolution; and (3) evaluation metrics measuring\ninternal cognitive improvements alongside task performance. Experiments across\nfour language models reveal that moderate uncertainty (40-60%) can outperform\ncomplete transparency in certain scenarios, with model-specific patterns\nsuggesting reconsideration of optimal information completeness in human-AI\ncollaboration. These findings contribute to understanding asymmetric reasoning\ndynamics and inform uncertainty-calibrated dialogue system design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01881.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c431a609672feb8cac22e7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c431a609672feb8cac22e7/obXCXUOgEoCuJbGuzYOGA.jpeg",
      "fullname": "Yaoyao Qian",
      "name": "FreaxRuby",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01667",
      "authors": [
        {
          "_id": "683eaa8ed8d42fc832445ebd",
          "name": "Yan Shu",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ebe",
          "name": "Bin Ren",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ebf",
          "name": "Zhitong Xiong",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec0",
          "name": "Danda Pani Paudel",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec1",
          "name": "Luc Van Gool",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec2",
          "name": "Begum Demir",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec3",
          "name": "Nicu Sebe",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec4",
          "name": "Paolo Rota",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T13:36:05.000Z",
      "submittedOnDailyAt": "2025-06-03T06:26:41.885Z",
      "title": "EarthMind: エアーミンド へ의向けて 다粒度と 다센서의 지구관측\n\nwith Large Multimodal Models\n\nエアーミンド へ의向けて、 대규모な 다모달 모델을 활용한 다粒度 및 다센서의 지구관측에 대한 연구\n\n(Note: The translation maintains the original structure and professional tone while ensuring accuracy and readability in Korean.)",
      "submittedOnDailyBy": {
        "_id": "65c4f99b27736b5b86c2cbda",
        "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
        "isPro": false,
        "fullname": "Yan Shu",
        "user": "sy1998",
        "type": "user"
      },
      "summary": "대규모 다모달 모댈(LMMs)은 다양한 시각 언어 태스크에서 강력한 성능을 보여주고 있습니다. 그러나 지구관측(EO) 데이터의 전반적인 이해는 어려움이 있으며, 환경의 관측 및 인간 활동의 영향을 파악하기 위해 중요합니다. 본 논문에서는 새로운 시각 언어 프레임워크 \"EarthMind\"를 소개합니다. EarthMind는 다粒도 및 다센서의 EO 데이터의 이해를 목표로 합니다. EarthMind에는 두 가지 핵심 요소가 포함되어 있습니다: (1) 공간 어텐션 프로노팅(SAP), LLM의 어텐션을 재배치하여 픽셀 수준의 이해를 강화합니다; (2) 크로스 모우드 융합, 부적절한 모우드 모델을 공유 공간에 조정하여 정보 밀도에 따라 토큰을 적응적으로 가중치付け하여 효과적인 융합을 구현합니다. 다센서 융합 평가를 촉진하기 위해, EarthMind-Bench라는 상세한 벤치마크를 제안합니다. EarthMind-Bench는 2,000개 이상의 인간 Annotation된 다센서 이미지-질문 쌍을 포함하며, 광범위한 관측 및 논리적 태스크를 기록하고 있습니다. 확장된 실험은 EarthMind의 효과성을 보여주고 있습니다. EarthMind는 EarthMind-Bench에서 가장 先端의 성능을 달성하고, GPT-4o를 초월할 수 있습니다. 또한, EarthMind는 현재의 방법보다 다수의 공개 EO 벤치마크에서 뛰어납니다. 이러한 결과는 EarthMind가 한 개의 통합 프레임워크로 다粒도 및 다센서의 도전을 쉽게 처리할 수 있음을 보여줍니다.",
      "upvotes": 6,
      "discussionId": "683eaa94d8d42fc832446013",
      "githubRepo": "https://github.com/shuyansy/EarthMind",
      "ai_summary": "EarthMind is a vision-language framework that uses spatial attention prompting and cross-modal fusion for efficient multi-granular and multi-sensor Earth Observation data understanding, outperforming larger models on specialized benchmarks.",
      "ai_keywords": [
        "Spatial Attention Prompting",
        "Cross-modal Fusion",
        "Earth Observation",
        "multi-granular",
        "multi-sensor",
        "EarthMind-Bench"
      ]
    },
    "publishedAt": "2025-06-02T09:36:05.000Z",
    "title": "EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation\n  with Large Multimodal Models",
    "summary": "Large Multimodal Models (LMMs) have demonstrated strong performance in\nvarious vision-language tasks. However, they often struggle to comprehensively\nunderstand Earth Observation (EO) data, which is critical for monitoring the\nenvironment and the effects of human activity on it. In this work, we present\nEarthMind, a novel vision-language framework for multi-granular and\nmulti-sensor EO data understanding. EarthMind features two core components: (1)\nSpatial Attention Prompting (SAP), which reallocates attention within the LLM\nto enhance pixel-level understanding; and (2) Cross-modal Fusion, which aligns\nheterogeneous modalities into a shared space and adaptively reweighs tokens\nbased on their information density for effective fusion. To facilitate\nmulti-sensor fusion evaluation, we propose EarthMind-Bench, a comprehensive\nbenchmark with over 2,000 human-annotated multi-sensor image-question pairs,\ncovering a wide range of perception and reasoning tasks. Extensive experiments\ndemonstrate the effectiveness of EarthMind. It achieves state-of-the-art\nperformance on EarthMind-Bench, surpassing GPT-4o despite being only 4B in\nscale. Moreover, EarthMind outperforms existing methods on multiple public EO\nbenchmarks, showcasing its potential to handle both multi-granular and\nmulti-sensor challenges in a unified framework.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01667.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c4f99b27736b5b86c2cbda",
      "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
      "fullname": "Yan Shu",
      "name": "sy1998",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24625",
      "authors": [
        {
          "_id": "683e5569fce31842c60675d7",
          "user": {
            "_id": "646e2fcaf813cfe153f1af6c",
            "avatarUrl": "/avatars/2f87d0e5c071000990da29cd744bc03d.svg",
            "isPro": false,
            "fullname": "Duo Zheng",
            "user": "zd11024",
            "type": "user"
          },
          "name": "Duo Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:45.196Z",
          "hidden": false
        },
        {
          "_id": "683e5569fce31842c60675d8",
          "name": "Shijia Huang",
          "hidden": false
        },
        {
          "_id": "683e5569fce31842c60675d9",
          "name": "Yanyang Li",
          "hidden": false
        },
        {
          "_id": "683e5569fce31842c60675da",
          "name": "Liwei Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T14:16:41.000Z",
      "submittedOnDailyAt": "2025-06-03T00:24:09.506Z",
      "title": "비디오에서 배우는 3차원 세계: 3차원 시각으로 MLLM을 향상시키기\n기오메트리 프로피드",
      "submittedOnDailyBy": {
        "_id": "646e2fcaf813cfe153f1af6c",
        "avatarUrl": "/avatars/2f87d0e5c071000990da29cd744bc03d.svg",
        "isPro": false,
        "fullname": "Duo Zheng",
        "user": "zd11024",
        "type": "user"
      },
      "summary": "이전의 연구는 3D 스케네를 이해하기 위해 비디오를 읽는 Multimodal Large Language Models（MLLMs）의 적용에 대해 조사했습니다. 이러한 접근 방식은 일반적으로 3D 데이터의 입력을 필요로 합니다. 예를 들어, 포인트 클러스터 또는 재구성된 Bird's-Eye View（BEV）맵 등이 있습니다. 우리 연구에서는 이러한 연구를 발전시키고, 비디오 데이터로부터 직접 3D 공간에서의 이해와 이유를 수행하는 MLLM의 능력을 향상시키며 이 분야를 발전시켰습니다. 우리 연구팀은 Video-3D Geometry Large Language Model（VG LLM）라는 새로운 효과적인 방법을 제안했습니다. 우리 접근 방식은 3D 시각 기하학 인코더를 사용하며, 비디오 시퀀스으로부터 3D 정보에서 정보를 추출합니다. 이 정보는 시각 토큰과 통합되어 MLLM에 입력됩니다. 확장된 실험은 3D 스케네 이해와 공간 이유에 대한 다양한 태스크에서 우리의 방법의 상당한 개선을 보여주었습니다. 놀라운 점은, 우리 4B 모델은 명시적인 3D 데이터의 입력을 필요로 하지 않아, 현재의 최상위 방법과 비교하여 경쟁적인 결과를 얻으며, VSI-Bench 평가에서 Gemini-1.5-Pro를 초월한 것을 보여주었습니다.",
      "upvotes": 6,
      "discussionId": "683e556efce31842c6067737",
      "projectPage": "https://lavi-lab.github.io/VG-LLM/",
      "githubRepo": "https://github.com/LaVi-Lab/VG-LLM",
      "ai_summary": "A novel Video-3D Geometry Large Language Model (VG LLM) extracts 3D information directly from video sequences to enhance 3D scene understanding without additional 3D data, achieving competitive results in various tasks.",
      "ai_keywords": [
        "MLLMs",
        "Video-3D Geometry Large Language Model",
        "VG LLM",
        "3D visual geometry encoder",
        "VSI-Bench"
      ]
    },
    "publishedAt": "2025-05-30T10:16:41.000Z",
    "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors",
    "summary": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method, the Video-3D Geometry Large\nLanguage Model (VG LLM). Our approach employs a 3D visual geometry encoder that\nextracts 3D prior information from video sequences. This information is\nintegrated with visual tokens and fed into the MLLM. Extensive experiments have\nshown that our method has achieved substantial improvements in various tasks\nrelated to 3D scene understanding and spatial reasoning, all directly learned\nfrom video sources. Impressively, our 4B model, which does not rely on explicit\n3D data inputs, achieves competitive results compared to existing\nstate-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646e2fcaf813cfe153f1af6c",
      "avatarUrl": "/avatars/2f87d0e5c071000990da29cd744bc03d.svg",
      "fullname": "Duo Zheng",
      "name": "zd11024",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01413",
      "authors": [
        {
          "_id": "683e6aa057738c5cc3616d70",
          "user": {
            "_id": "6390525c00fb8ec4a424e0ff",
            "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
            "isPro": false,
            "fullname": "Yulei Qin",
            "user": "yolay",
            "type": "user"
          },
          "name": "Yulei Qin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T03:23:18.288Z",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d71",
          "name": "Gang Li",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d72",
          "name": "Zongyi Li",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d73",
          "name": "Zihan Xu",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d74",
          "name": "Yuchen Shi",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d75",
          "name": "Zhekai Lin",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d76",
          "name": "Xiao Cui",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d77",
          "name": "Ke Li",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d78",
          "name": "Xing Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T08:11:44.000Z",
      "submittedOnDailyAt": "2025-06-03T02:06:35.145Z",
      "title": "インセンティブズ・라이징・리ジュース・프로그램・리드・インストラクション・프로그램・대규모・언어・모델",
      "submittedOnDailyBy": {
        "_id": "6390525c00fb8ec4a424e0ff",
        "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
        "isPro": false,
        "fullname": "Yulei Qin",
        "user": "yolay",
        "type": "user"
      },
      "summary": "현재의 대규모 언어 모델(LLMs)은, 복잡한 지시를 따라갈 때, 특히 여러 제약이 병행적이고, 연속적이고, 분지의 구조로 배치되어 있을 때, 문제점을 드러내고 있습니다. 직감적인 해결책으로, chain-of-thought(CoT)는 LLMs의 능력을 일반적인 수준으로 향상시킬 것으로 기대됩니다. 그러나, baseline의 CoT은 명령의 단순한 재언어화의 표면적인 이유패턴에 의해, 성능에 부정적인 영향을 미칩니다. 이는 종류와 차원의 계층 구조 사이에 제약의 관계를 특정하기 위해, 제약의 구성을 뺄 수 없기 때문입니다.\n\n이점에대해서, 우리는 테스트 시의 계산 스케일링에 의한 이유를 촉발시키는 방법으로, 복잡한 지시를 처리하기 위한 LLMs의 능력을 향상시키는 방법을 제안합니다. 우선, 현재의 기술의 분해에 기반하여, 복잡한 지시의 재현 가능한 데이터의 획득 방법을 제안합니다. 다음으로, 증명 가능한 규칙 중심의 리바イ와링(RL)을 활용하여, 명령 따라하는 이유를 특히 육성하는 것을 시도합니다. 복잡한 지시의 이유의 얕은, 비중요한 특성을 해결하기 위해, 샘플 단위의 대비를 사용하여, 상위의 CoT의 강제를 실현합니다. 또한, 전문가의 행동 크로닝을 활용하여, 빠른 사고의 LLMs로부터 능숙한 이유자로의 안정적인 분포의 이동을 촉진합니다.\n\n7개의 세부적인 벤치마크에서의 확장된 평가는 제안된 방법의 유효성을 확인하고, 1.5B LLM은 8B LLM과 같은 성능을 달성하며, 11.74%의 효과를 얻었습니다. 코드와 데이터는 https://github.com/yuleiqin/RAIF에 액세스할 수 있습니다.",
      "upvotes": 5,
      "discussionId": "683e6aa657738c5cc3616ecc",
      "projectPage": "https://huggingface.co/collections/yolay/raif-arxivorg-pdf-250601413-682b16e5c0c2fa9b73811369",
      "githubRepo": "https://github.com/yuleiqin/RAIF",
      "ai_summary": "A method is proposed to enhance large language models in handling complex instructions through incentivized reasoning and reinforcement learning, improving performance and reducing computational load.",
      "ai_keywords": [
        "chain-of-thought (CoT)",
        "reinforcement learning (RL)",
        "rule-centric reward signals",
        "sample-wise contrast",
        "behavior cloning",
        "instruction following",
        "decomposition of complex instructions"
      ]
    },
    "publishedAt": "2025-06-02T04:11:44.000Z",
    "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models",
    "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nare available at https://github.com/yuleiqin/RAIF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01413.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6390525c00fb8ec4a424e0ff",
      "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
      "fullname": "Yulei Qin",
      "name": "yolay",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24452",
      "authors": [
        {
          "_id": "683e8abd8e6e97efe0bf20d9",
          "name": "Anda Tang",
          "hidden": false
        },
        {
          "_id": "683e8abd8e6e97efe0bf20da",
          "name": "Yiming Dong",
          "hidden": false
        },
        {
          "_id": "683e8abd8e6e97efe0bf20db",
          "user": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "isPro": false,
            "fullname": "Yutao Zeng",
            "user": "Taoer",
            "type": "user"
          },
          "name": "Yutao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:39:15.864Z",
          "hidden": false
        },
        {
          "_id": "683e8abd8e6e97efe0bf20dc",
          "name": "zhou Xun",
          "hidden": false
        },
        {
          "_id": "683e8abd8e6e97efe0bf20dd",
          "name": "Zhouchen Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/GfsrISs7G7K3rMBB1uws5.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/eQqbJxhzxMNKVlyoqQPRA.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/q1H4zp_jhjZw57UYMnECl.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/TpjaRwRNbGOzWfL7y5C4P.png"
      ],
      "publishedAt": "2025-05-30T10:38:03.000Z",
      "submittedOnDailyAt": "2025-06-03T04:32:00.766Z",
      "title": "스텝 사이즈 중 하나: 멀티 프로세싱 학습률 스케줄링의 통합",
      "submittedOnDailyBy": {
        "_id": "6371128eafbe42caa5a5222b",
        "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
        "isPro": false,
        "fullname": "Yutao Zeng",
        "user": "Taoer",
        "type": "user"
      },
      "summary": "확대되는 계산 비용과 제한된 리소스로 인해 예산 배치 훈련의 중요성이 강조되어 있습니다. 이는 예약된 훈련 배치 내에서 최적의 학습을 달성하기 위해 설계되었습니다. 학습률 스케줄은 서로 다른 네트워크와 태스크의 성능을 지배하며, 특히 예산 배치 훈련의 경우, 설계는 주로 휴리스틱으로 이루어져 있으며, 이론적인 기반이 부족합니다. 또한 최적의 학습률 스케줄의 선택은 많은 테스트를 통해 선택되어 훈련 프로세스가 낮은 효율성을 가집니다. 본 논문에서는 서로 다른 아키텍처와 태스크 사이에서, 제한된 훈련 예산 아래도 일관된 우수한 성능을 보여주는 이론적으로 기반된 학습률 스케줄을 제안합니다. 먼저, 새로운 훈련 예산에 관련된 최적화 프레임워크를 구축하고, 곡률의 변화에 대한 강건성을 명확히 고려합니다. 이 프레임워크에서 이론적으로 기반된 Unified Budget-Aware (UBA) 스케줄을 얻을 수 있습니다. UBA 스케줄은 하나의 초 파라미터 φ로 제어되어, 유연성과 간단성의 균형을 제공하며, 네트워크별로 숫자 최적화의 필요성을 줄입니다. 또한 φ와 조건수 사이의 이론적인 관계를 확립하고, 우리의 접근 방식을 설명하고 정당화합니다. 또한 φ의 다양한 값에 대한 수렴을 증명합니다. 이론적 분석과 실험 결과를 바탕으로 φ의 선택의 실용적인 가이드라인을 제공합니다. 확장되는 실험 결과를 통해, UBA는 다양한 시각과 언어 태스크의 광범위한 범위에서, 서로 다른 네트워크 아키텍처 (예: ResNet, OLMo)와 규모의 범위에서, 서로 다른 훈련 이터레이션 예산 아래에서 일반적인 스케줄을 초과하는 것을 보여줍니다.",
      "upvotes": 5,
      "discussionId": "683e8abf8e6e97efe0bf2155",
      "ai_summary": "A unified budget-aware learning rate schedule is proposed to optimize training within limited iteration budgets, outperforming traditional schedules across various tasks and network architectures.",
      "ai_keywords": [
        "budgeted-iteration training",
        "learning rate schedules",
        "Unified Budget-Aware (UBA) schedule",
        "training budget-aware optimization framework",
        "robustness to landscape curvature variations",
        "condition number",
        "convergence",
        "ResNet",
        "OLMo"
      ]
    },
    "publishedAt": "2025-05-30T06:38:03.000Z",
    "title": "Stepsize anything: A unified learning rate schedule for\n  budgeted-iteration training",
    "summary": "The expanding computational costs and limited resources underscore the\ncritical need for budgeted-iteration training, which aims to achieve optimal\nlearning within predetermined iteration budgets.While learning rate schedules\nfundamentally govern the performance of different networks and tasks,\nparticularly in budgeted-iteration scenarios, their design remains largely\nheuristic, lacking theoretical foundations.In addition, the optimal learning\nrate schedule requires extensive trial-and-error selection, making the training\nprocess inefficient.In this work, we propose the Unified Budget-Aware (UBA)\nschedule, a theoretically grounded learning rate schedule that consistently\noutperforms commonly-used schedules among diverse architectures and tasks under\ndifferent constrained training budgets.First, we bridge the gap by constructing\na novel training budget-aware optimization framework, which explicitly accounts\nfor the robustness to landscape curvature variations.From this framework, we\nderive the UBA schedule, controlled by a single hyper-parameter varphi that\nprovides a trade-off between flexibility and simplicity, eliminating the need\nfor per-network numerical optimization. Moreover, we establish a theoretical\nconnection between varphi and the condition number, adding interpretation\nand justification to our approach. Besides, we prove the convergence for\ndifferent values of varphi.We offer practical guidelines for its selection\nvia theoretical analysis and empirical results.xtensive experimental results\nshow that UBA consistently surpasses the commonly-used schedules\nacross diverse vision and language tasks, spanning network architectures (e.g.,\nResNet, OLMo) and scales, under different training-iteration budgets.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/GfsrISs7G7K3rMBB1uws5.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/eQqbJxhzxMNKVlyoqQPRA.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/q1H4zp_jhjZw57UYMnECl.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/TpjaRwRNbGOzWfL7y5C4P.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24452.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6371128eafbe42caa5a5222b",
      "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
      "fullname": "Yutao Zeng",
      "name": "Taoer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00338",
      "authors": [
        {
          "_id": "683e64eb3e5a54d05365ddc6",
          "user": {
            "_id": "61809f31a367a8f5351ef353",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61809f31a367a8f5351ef353/s5eQ00YeoirakzE_rJ0cy.jpeg",
            "isPro": false,
            "fullname": "Yifan Peng",
            "user": "pyf98",
            "type": "user"
          },
          "name": "Yifan Peng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:09.029Z",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddc7",
          "name": "Shakeel Muhammad",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddc8",
          "name": "Yui Sudo",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddc9",
          "name": "William Chen",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddca",
          "name": "Jinchuan Tian",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddcb",
          "name": "Chyi-Jiunn Lin",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddcc",
          "name": "Shinji Watanabe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T01:44:44.000Z",
      "submittedOnDailyAt": "2025-06-03T01:39:05.927Z",
      "title": "OWSM v4: 데이터 스케일링과 정제로 Open Whisper 스타일의 음성 모델의 개선",
      "submittedOnDailyBy": {
        "_id": "61809f31a367a8f5351ef353",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61809f31a367a8f5351ef353/s5eQ00YeoirakzE_rJ0cy.jpeg",
        "isPro": false,
        "fullname": "Yifan Peng",
        "user": "pyf98",
        "type": "user"
      },
      "summary": "Open Whisper-style Speech Models (OWSM) 프로젝트는 학술용 레지즈럴 자원을 사용하여 완전히 개방된 음성 기반 모델을 개발했지만, 훈련 데이터는 충분하지 않습니다. 본 논문은 Creative Commons 许可의 큰 규모의 웹 크롤링 데이터 세트 YODAS를 통합하여 OWSM을 강화합니다. 그러나 YODAS의 통합은 부정적인 언어 라벨과 음성-텍스트의 비대응 등 문제로 복잡합니다. 이를 대처하기 위해 공개 도구 패키지를 사용하여 scalable한 데이터 정제 파이프라인을 개발하고, 75언어로 166,000시간의 음성을 구성하는 데이터 세트를 수집했습니다. 이러한 캄보디아 데이터 세트와 기존 OWSM 데이터를 사용하여 훈련된 새로운 OWSM v4 모델은 이전 버전보다 크게 향상되어, 다언어 벤치마크에서도 대단히 뛰어납니다. 또한 모델은 Whisper나 MMS와 같은 첨단 산업 모델과 비교하여, 여러 시나리오에서 프론티어에 도달합니다. 공개로 인해, cleaned data, pretrained model과 모든 관련 암호를 ESPnet 도구 패키지를 통해 공개됩니다.",
      "upvotes": 4,
      "discussionId": "683e64ec3e5a54d05365ddee",
      "projectPage": "https://www.wavlab.org/activities/2024/owsm/",
      "githubRepo": "https://github.com/espnet/espnet",
      "ai_summary": "The OWSM project is enhanced with a large-scale, cleaned web dataset, leading to improved multilingual speech models comparable to leading industrial models.",
      "ai_keywords": [
        "speach foundation models",
        "YODAS",
        "data-cleaning pipeline",
        "multilingual benchmarks",
        "Whisper",
        "MMS",
        "ESPnet toolkit"
      ]
    },
    "publishedAt": "2025-05-30T21:44:44.000Z",
    "title": "OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and\n  Cleaning",
    "summary": "The Open Whisper-style Speech Models (OWSM) project has developed a series of\nfully open speech foundation models using academic-scale resources, but their\ntraining data remains insufficient. This work enhances OWSM by integrating\nYODAS, a large-scale web-crawled dataset with a Creative Commons license.\nHowever, incorporating YODAS is nontrivial due to its wild nature, which\nintroduces challenges such as incorrect language labels and audio-text\nmisalignments. To address this, we develop a scalable data-cleaning pipeline\nusing public toolkits, yielding a dataset with 166,000 hours of speech across\n75 languages. Our new series of OWSM v4 models, trained on this curated dataset\nalongside existing OWSM data, significantly outperform previous versions on\nmultilingual benchmarks. Our models even match or surpass frontier industrial\nmodels like Whisper and MMS in multiple scenarios. We will publicly release the\ncleaned YODAS data, pre-trained models, and all associated scripts via the\nESPnet toolkit.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61809f31a367a8f5351ef353",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61809f31a367a8f5351ef353/s5eQ00YeoirakzE_rJ0cy.jpeg",
      "fullname": "Yifan Peng",
      "name": "pyf98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24183",
      "authors": [
        {
          "_id": "683e9a174de2ca71b8bc915b",
          "user": {
            "_id": "67de68f4f38795c545310088",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8604Li6_OlATOsTdY9oHL.png",
            "isPro": false,
            "fullname": "Yaoyu Zhu",
            "user": "zhuyaoyu",
            "type": "user"
          },
          "name": "Yaoyu Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:39:09.237Z",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc915c",
          "user": {
            "_id": "62c581177b48ba0bb8cdb737",
            "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
            "isPro": false,
            "fullname": "di huang",
            "user": "dihuang",
            "type": "user"
          },
          "name": "Di Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:37:07.655Z",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc915d",
          "name": "Hanqi Lyu",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc915e",
          "name": "Xiaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc915f",
          "name": "Chongxiao Li",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9160",
          "name": "Wenxuan Shi",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9161",
          "name": "Yutong Wu",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9162",
          "name": "Jianan Mu",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9163",
          "name": "Jinghua Wang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9164",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9165",
          "name": "Pengwei Jin",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9166",
          "name": "Shuyao Cheng",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9167",
          "name": "Shengwen Liang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9168",
          "name": "Xishan Zhang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9169",
          "name": "Rui Zhang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc916a",
          "name": "Zidong Du",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc916b",
          "name": "Qi Guo",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc916c",
          "name": "Xing Hu",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc916d",
          "name": "Yunji Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T03:51:06.000Z",
      "submittedOnDailyAt": "2025-06-03T05:58:04.258Z",
      "title": "CodeV-R1: 사유付 Verilog 생성",
      "submittedOnDailyBy": {
        "_id": "62c581177b48ba0bb8cdb737",
        "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
        "isPro": false,
        "fullname": "di huang",
        "user": "dihuang",
        "type": "user"
      },
      "summary": "大型 언어 모델 (LLMs)은 증명 가능한 보상 (RLVR)를 기반으로 하는 강화 학습을 사용하여, 소프트웨어 프로그래밍이나 수학 문제와 같은 명확하고 자동 가능한 확인이 가능한 작업에서 발전을 이루었습니다. 이러한 효과에 대해 전자 설계 자동화 (EDA)에 확장하여, 특히 자연어 (NL) 규칙으로부터 하드웨어 표현 언어 (HDL)처럼 Verilog를 자동적으로 생성하는 데에 있어 3가지의 주요한 문제를 제기되었습니다: 자동화된 정확한 확인 환경의 부족, 고품질의 NL-코드 페어의 부족, RLVR의 계산 비용의 높음. 이에 CodeV-R1이라는 RLVR 프레임워크를 소개합니다. 먼저, 로브베이스 테스트 벤치 생성기를 개발하여, 금전 레퍼런스와 강한 등호성 체크를 수행합니다. 다음으로, 오픈 소스 Verilog 스냅샷과 LLM으로 생성된 NL 설명을 조합하여, 생성된 테스트 벤치에 의한 코드-NL-코드의 일치성을 확인하고, 불등호한 예를 제거하여 고품질의 데이터 세트를 생성합니다. 마지막으로, 두 단계의 「디스틸 후 RL」의 훈련 파이프라인을 사용합니다: 이유 능력의 초기 단계의 디스틸을 수행하고, 새로운 RLVR 알고리즘 (DAPO)을 사용하여 자동으로 샘플링 레이트를 조정하여 훈련 비용의 절감이 가능한 새로운 학습을 수행합니다. 이 기반으로, CodeV-R1-7B는 VerilogEval v2와 RTLLM v1.1 각각 68.6%와 72.9%의 pass@1을 달성하여, 이전의 최선보다 12~20% 이상 뛰어넘거나 671B DeepSeek-R1의 성능을 초월할 수 있습니다. 모델, 훈련 파이프라인, 데이터 세트를 공개하여 EDA와 LLM 커뮤니티의 연구에 도움을 줄 것입니다.",
      "upvotes": 4,
      "discussionId": "683e9a184de2ca71b8bc91b3",
      "projectPage": "https://iprc-dip.github.io/CodeV-R1",
      "ai_summary": "CodeV-R1, an RLVR framework for Verilog generation, achieves state-of-the-art performance in EDA using a rule-based testbench, round-trip data synthesis, and adaptive RLVR training.",
      "ai_keywords": [
        "reinforcement learning with verifiable reward",
        "RLVR",
        "electronic design automation",
        "EDA",
        "hardware description languages",
        "HDLs",
        "Verilog",
        "natural-language",
        "NL",
        "testbench generator",
        "equivalence checking",
        "round-trip data synthesis",
        "dataset",
        "two-stage training pipeline",
        "distillation",
        "adaptive DAPO",
        "VerilogEval",
        "RTLLM"
      ]
    },
    "publishedAt": "2025-05-29T23:51:06.000Z",
    "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
    "summary": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24183.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c581177b48ba0bb8cdb737",
      "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
      "fullname": "di huang",
      "name": "dihuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23504",
      "authors": [
        {
          "_id": "683e501c89dc42ba0515a4d8",
          "name": "Liyun Zhu",
          "hidden": false
        },
        {
          "_id": "683e501c89dc42ba0515a4d9",
          "name": "Qixiang Chen",
          "hidden": false
        },
        {
          "_id": "683e501c89dc42ba0515a4da",
          "name": "Xi Shen",
          "hidden": false
        },
        {
          "_id": "683e501c89dc42ba0515a4db",
          "name": "Xiaodong Cun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T14:48:10.000Z",
      "submittedOnDailyAt": "2025-06-03T00:01:06.695Z",
      "title": "VAU-R1: 강화학습을 이용한 이상 감지 이해를 향상시키기",
      "submittedOnDailyBy": {
        "_id": "63184c517ca1b876d99b7e0e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
        "isPro": false,
        "fullname": "Xiaodong Cun",
        "user": "vinthony",
        "type": "user"
      },
      "summary": "Video Anomaly Understanding (VAU)는 스마트 도시, 안전 감시, 그리고 재난 경보 시스템 등 다양한 응용 분야에서 중요하지만, 공간적 시간적 세부적인 인식과 복잡한 상황에서의 견고한 논리론을 요구하기 때문에 어려움을 겪습니다. 이상 검출의 발전과 함께, 현재의 방법들은 해석성 부족과 이상 이벤트의 원인 및 맥락적 모습을 쉽게 파악하는 데의 단점이 있습니다. 이러한 제한은 이상 검출의 논리론 능력 평가에 대한 상세한 벤치마크의 부족으로 더욱 심각해지게 됩니다. 이러한 두 가지 도전에 대처하기 위해, 우리는 VAU-R1을 소개합니다. VAU-R1은 Multimodal Large Language Models (MLLMs)에 기반한 데이터 효율적인 프레임워크로, Reinforcement Fine-Tuning (RFT)을 통해 이상 논리론을 강화합니다. 또한, 우리는 VAU-Bench를 제안합니다. VAU-Bench는 이상 논리론에 적합한 Chain-of-Thought 벤치마크로, 복수 선택 QA, 세부적인 이유, 시간적 注記, 설명적인 캡처션을 특징으로 합니다. 실험 결과를 통해, VAU-R1은 다양한 맥락에서 질문의 정확성, 시간적 성장, 이유의 일관성을 크게 향상시킵니다. 이러한 방법과 벤치마크는 설명 가능한 논리론을 포함한 영상 이상 이해의 강력한 기초를 구축합니다. 코드는 https://github.com/GVCLab/VAU-R1에 공개되어 있습니다.",
      "upvotes": 4,
      "discussionId": "683e502189dc42ba0515a5e1",
      "ai_summary": "VAU-R1 uses Multimodal Large Language Models with Reinforcement Fine-Tuning to enhance video anomaly reasoning, complemented by VAU-Bench, a Chain-of-Thought benchmark for evaluating anomaly understanding.",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Reinforcement Fine-Tuning (RFT)",
        "Chain-of-Thought",
        "benchmark",
        "question answering",
        "temporal grounding",
        "reasoning coherence"
      ]
    },
    "publishedAt": "2025-05-29T10:48:10.000Z",
    "title": "VAU-R1: Advancing Video Anomaly Understanding via Reinforcement\n  Fine-Tuning",
    "summary": "Video Anomaly Understanding (VAU) is essential for applications such as smart\ncities, security surveillance, and disaster alert systems, yet remains\nchallenging due to its demand for fine-grained spatio-temporal perception and\nrobust reasoning under ambiguity. Despite advances in anomaly detection,\nexisting methods often lack interpretability and struggle to capture the causal\nand contextual aspects of abnormal events. This limitation is further\ncompounded by the absence of comprehensive benchmarks for evaluating reasoning\nability in anomaly scenarios. To address both challenges, we introduce VAU-R1,\na data-efficient framework built upon Multimodal Large Language Models (MLLMs),\nwhich enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT).\nBesides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored\nfor video anomaly reasoning, featuring multiple-choice QA, detailed rationales,\ntemporal annotations, and descriptive captions. Empirical results show that\nVAU-R1 significantly improves question answering accuracy, temporal grounding,\nand reasoning coherence across diverse contexts. Together, our method and\nbenchmark establish a strong foundation for interpretable and reasoning-aware\nvideo anomaly understanding. Our code is available at\nhttps://github.com/GVCLab/VAU-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23504.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63184c517ca1b876d99b7e0e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
      "fullname": "Xiaodong Cun",
      "name": "vinthony",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 323
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21179",
      "authors": [
        {
          "_id": "6838c66dc60fb2fc462cec9f",
          "user": {
            "_id": "64d0eb731ed6649d70afb136",
            "avatarUrl": "/avatars/4c591c86c575c82760126c39af5a02b4.svg",
            "isPro": true,
            "fullname": "Chen Dar-Yen",
            "user": "ChenDY",
            "type": "user"
          },
          "name": "Dar-Yen Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:47:40.163Z",
          "hidden": false
        },
        {
          "_id": "6838c66dc60fb2fc462ceca0",
          "user": {
            "_id": "638c81fa61eb51017518fa31",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/f0eCrzBrxz7Y9n25WkZ2v.png",
            "isPro": false,
            "fullname": "Hmrishav Bandyopadhyay",
            "user": "Hmrishav",
            "type": "user"
          },
          "name": "Hmrishav Bandyopadhyay",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T13:40:02.354Z",
          "hidden": false
        },
        {
          "_id": "6838c66dc60fb2fc462ceca1",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "6838c66dc60fb2fc462ceca2",
          "name": "Yi-Zhe Song",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64d0eb731ed6649d70afb136/BtItSeWg7RJJDSWqnZWIB.mp4"
      ],
      "publishedAt": "2025-05-27T13:30:46.000Z",
      "submittedOnDailyAt": "2025-06-03T07:58:27.904Z",
      "title": "정규화 어텐션 가이드라인: 일반적인 부정적인 가이드라인의 Diffusion 모델",
      "submittedOnDailyBy": {
        "_id": "64d0eb731ed6649d70afb136",
        "avatarUrl": "/avatars/4c591c86c575c82760126c39af5a02b4.svg",
        "isPro": true,
        "fullname": "Chen Dar-Yen",
        "user": "ChenDY",
        "type": "user"
      },
      "summary": "음의 가이드라인 - 명시적으로 불만족의 특성을 억제하는 것 -는 확산 모델에서 특히 적은 스텝 샘플링 리지민에서 기본적인 문제입니다. 클래스 피사저 자유 가이드라인 (CFG)은 일반적인 설정에서 효과적이지만, 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다. 우리는 L1 기준의 양분별 예측이 떨어질 때 엄격한 샘플링 스텝의 압축으로 실패합니다.",
      "upvotes": 4,
      "discussionId": "6838c673c60fb2fc462cee10",
      "projectPage": "https://chendaryen.github.io/NAG.github.io/",
      "githubRepo": "https://github.com/ChenDarYen/Normalized-Attention-Guidance",
      "ai_summary": "Normalized Attention Guidance (NAG) enhances diffusion models by providing effective negative guidance across regimes and modalities without retraining.",
      "ai_keywords": [
        "negative guidance",
        "diffusion models",
        "few-step sampling",
        "Classifier-Free Guidance (CFG)",
        "Normalized Attention Guidance (NAG)",
        "attention space",
        "L1-based normalization",
        "extrapolation",
        "fidelity",
        "CLIP Score",
        "FID",
        "PFID",
        "ImageReward",
        "UNet",
        "DiT",
        "image",
        "video",
        "model-agnostic inference-time approach"
      ]
    },
    "publishedAt": "2025-05-27T09:30:46.000Z",
    "title": "Normalized Attention Guidance: Universal Negative Guidance for Diffusion\n  Model",
    "summary": "Negative guidance -- explicitly suppressing unwanted attributes -- remains a\nfundamental challenge in diffusion models, particularly in few-step sampling\nregimes. While Classifier-Free Guidance (CFG) works well in standard settings,\nit fails under aggressive sampling step compression due to divergent\npredictions between positive and negative branches. We present Normalized\nAttention Guidance (NAG), an efficient, training-free mechanism that applies\nextrapolation in attention space with L1-based normalization and refinement.\nNAG restores effective negative guidance where CFG collapses while maintaining\nfidelity. Unlike existing approaches, NAG generalizes across architectures\n(UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image,\nvideo), functioning as a universal plug-in with minimal computational\noverhead. Through extensive experimentation, we demonstrate consistent\nimprovements in text alignment (CLIP Score), fidelity (FID, PFID), and\nhuman-perceived quality (ImageReward). Our ablation studies validate each\ndesign component, while user studies confirm significant preference for\nNAG-guided outputs. As a model-agnostic inference-time approach requiring no\nretraining, NAG provides effortless negative guidance for all modern diffusion\nframeworks -- pseudocode in the Appendix!",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64d0eb731ed6649d70afb136/BtItSeWg7RJJDSWqnZWIB.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21179.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64d0eb731ed6649d70afb136",
      "avatarUrl": "/avatars/4c591c86c575c82760126c39af5a02b4.svg",
      "fullname": "Chen Dar-Yen",
      "name": "ChenDY",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01084",
      "authors": [
        {
          "_id": "683ea4388be2e40086ea9056",
          "name": "Saibo Geng",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea9057",
          "user": {
            "_id": "6420afc71ccd411979dc12dc",
            "avatarUrl": "/avatars/ee4a89ebc7a0716e21deaebc86e062e6.svg",
            "isPro": false,
            "fullname": "nathan ranchin",
            "user": "nathanrchn",
            "type": "user"
          },
          "name": "Nathan Ranchin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T09:39:27.313Z",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea9058",
          "name": "Yunzhen yao",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea9059",
          "name": "Maxime Peyrard",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea905a",
          "name": "Chris Wendler",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea905b",
          "name": "Michael Gastpar",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea905c",
          "name": "Robert West",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/VnqCCx4RG5-62le1tQcaW.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/EGD7DC-lDXkTXOLD6r0IP.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/YDPF1VlfQ0Yfynq9peK5s.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/QY0t5DXqhIVBWrLpJvZm8.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/af0uwzA8bV-7byretzGwW.png"
      ],
      "publishedAt": "2025-06-01T17:03:02.000Z",
      "submittedOnDailyAt": "2025-06-03T06:05:31.112Z",
      "title": "zip2zip: 언어 모델의 추론 시에 적용 가능한 단어 집합을 토큰 압축에 의해 제공됨.",
      "submittedOnDailyBy": {
        "_id": "5fce0cfeb3dbf216ad31836a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625748458774-5fce0cfeb3dbf216ad31836a.png",
        "isPro": false,
        "fullname": "Saibo-creator",
        "user": "Saibo-creator",
        "type": "user"
      },
      "summary": "トークナイゼーション의 효율성은 대규모 언어 모델(LLMs)의 성능과 비용에 중대한 역할을 수행하지만, 대부분의 모델은 일반적인 코퍼ス에 최적화된 고정된 토큰나이저를 사용합니다. 이러한 토큰나이저的固定 vocabulary는 주로 특정 분야나 언어에 특화된 입력에 적응하지 못하며, 이는 토큰 시퀀스의 길이를 늘려 계산 비용이 증가하는 데 기여합니다. 우리는 zip2zip 프레임워크를 소개합니다. 이 프레임워크는 LLMs가 추론 시 토큰 vocabulary를 동적으로 조정할 수 있도록 하여, 생성되는 토큰을 줄이고, 이로 인해 추론 속도를 빠르게 하는 것입니다. zip2zip는 3개의 중요한 구성 요소로 구성되어 있습니다: 1) Lempel-Ziv-Welch(LZW) 압축에 기반한 토큰나이저, 토큰을 동적으로 압축하여 재사용 가능한 '하이퍼 토큰'으로 변환하는 것입니다. 2) 새로운 하이퍼 토큰에 대한 임베딩을 계산하는 임베딩 레이어입니다. 3)因果 언어 모델링의 변형, 모델을 하이퍼 토큰화된 압축 시퀀스로 동작시킵니다. 우리는 zip2zip로 기존 LLM을 처리하는 데 10 GPU 시간이 소요되는 것을 보여줍니다. 그 결과, zip2zip LLMs은 추론 시 하이퍼 토큰을 사용함으로써, 입력과 출력 시퀀스의 길이를 20-60% 줄일 수 있으며, 추론 지연에 상당한 개선을 입혔습니다.",
      "upvotes": 3,
      "discussionId": "683ea4398be2e40086ea90b7",
      "githubRepo": "https://github.com/epfl-dlab/zip2zip",
      "ai_summary": "A framework called zip2zip dynamically adjusts token vocabulary in LLMs at inference time using LZW compression, reducing token sequence length and improving inference speed.",
      "ai_keywords": [
        "LZW compression",
        "hypertokens",
        "embedding layer",
        "causal language modeling",
        "parameter-efficient finetuning"
      ]
    },
    "publishedAt": "2025-06-01T13:03:02.000Z",
    "title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via\n  Token Compression",
    "summary": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized for general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a\nframework that enables LLMs to dynamically adjust token vocabulary at inference\ntime, allowing for fewer generated tokens and thus faster inference. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\n(LZW) compression that incrementally compresses tokens into reusable\n\"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for\nnewly formed hypertokens at runtime; and (3) a causal language modeling variant\nthat trains the model to operate on hypertokenized, compressed sequences. We\nshow that an existing LLM can be zip2zip-fied in 10 GPU-hours via\nparameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to\nuse hypertokens at inference time, reducing input and output sequence length by\n20-60\\%, with significant improvements in inference latency.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/VnqCCx4RG5-62le1tQcaW.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/EGD7DC-lDXkTXOLD6r0IP.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/YDPF1VlfQ0Yfynq9peK5s.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/QY0t5DXqhIVBWrLpJvZm8.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/af0uwzA8bV-7byretzGwW.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fce0cfeb3dbf216ad31836a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625748458774-5fce0cfeb3dbf216ad31836a.png",
      "fullname": "Saibo-creator",
      "name": "Saibo-creator",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00643",
      "authors": [
        {
          "_id": "683e61338ebad8b7519bc7f3",
          "user": {
            "_id": "63e3f57754f51ea342ce26be",
            "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
            "isPro": false,
            "fullname": "Weijie Xu",
            "user": "xwjzds",
            "type": "user"
          },
          "name": "Weijie Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T02:43:01.430Z",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f4",
          "name": "Shixian Cui",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f5",
          "name": "Xi Fang",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f6",
          "name": "Chi Xue",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f7",
          "name": "Stephanie Eckman",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f8",
          "name": "Chandan Reddy",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/lDZvyKeHC66Fk1-x6VgbB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/l-YQ7eiVk7rb7n3Hi51p5.png"
      ],
      "publishedAt": "2025-05-31T17:14:21.000Z",
      "submittedOnDailyAt": "2025-06-03T01:22:58.893Z",
      "title": "SATA-BENCH: 멀티 선택 벤치마크\n질문",
      "submittedOnDailyBy": {
        "_id": "63e3f57754f51ea342ce26be",
        "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
        "isPro": false,
        "fullname": "Weijie Xu",
        "user": "xwjzds",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)은 단일 답변의 다수 선택 태스크에서 평가가 증가하지만, 많은 실세계의 문제를 해결하기 위해 모든 올바른 답변을 특정해야 합니다. 이 능력은 아직 조사가 부족합니다. 우리는, 모든 것을 선택할 수 있는(SATA) 질문을 평가하기 위한 첫 번째 전문적인 벤치마크인 SATA-BENCH를 소개합니다. 이 벤치마크는 읽기, 법, 바이오미크스 등 다양한 분야에서 LLMs를 평가합니다. 27개의 오픈 소스 및 저작권 모델의 평가에 따라, 상당한 오류가 밝혀졌습니다: 가장 강력한 모델도, 정확한 매칭이 상당히 높지 않습니다, 즉 LLMs는 모든 올바른 답변을 신뢰적으로 특정할 수 없습니다. 이 약점은 두 가지 핵심적인 문제를 초래합니다: 선택 편향 - 모델은, 내용은 상관없이 특정 선택지를 선호합니다, 카운트 편향 - 모델은 올바른 답변의 수를 예측할 수 없습니다. 이러한 문제를 해결하기 위해, 우리는 토큰 편향과 적응적 디스크리미네이션을 결합한 해석 모델에 의한 선택 플런널(Choice Funnel)을 제안합니다. Choice Funnel은 상대적인 기준과 비교하여, 정확한 매칭을 최대 29% 높일 수 있으며, 추론 비용을 64% 이상 줄일 수 있습니다. 이 발견은, 현재의 LLMs의 기본적인 한계를 명확히 하고, 여러 답변의 논리론을 진단 및 개선하는 새로운 프레임워크를 도입합니다. SATA-BENCH와 Choice Funnel을 공개하여, 다양한 답변의 응용 프로그램에서 강력한 결정을 위한 LLMs의 개발을 촉진합니다.",
      "upvotes": 3,
      "discussionId": "683e61358ebad8b7519bc8cf",
      "githubRepo": "https://github.com/sata-bench/sata-bench",
      "ai_summary": "SATA-BENCH evaluates LLMs on multi-answer questions, revealing selections biases and proposing Choice Funnel to improve accuracy and reduce costs in multi-answer reasoning tasks.",
      "ai_keywords": [
        "Select All That Apply (SATA) questions",
        "SATA-BENCH",
        "token debiasing",
        "adaptive thresholding",
        "Choice Funnel"
      ]
    },
    "publishedAt": "2025-05-31T13:14:21.000Z",
    "title": "SATA-BENCH: Select All That Apply Benchmark for Multiple Choice\n  Questions",
    "summary": "Large language models (LLMs) are increasingly evaluated on single-answer\nmultiple-choice tasks, yet many real-world problems require identifying all\ncorrect answers from a set of options. This capability remains underexplored.\nWe introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on\nSelect All That Apply (SATA) questions across diverse domains, including\nreading comprehension, law, and biomedicine. Our evaluation of 27 open-source\nand proprietary models reveals a significant gap: even the strongest model\nachieves only 41.8% exact match, exposing LLMs' inability to reliably identify\nall correct answers. We find that this weakness stems from two core challenges:\nselection bias - models favor certain choices regardless of content, and count\nbias - models fail to predict the correct number of answers. To address these\nissues, we propose Choice Funnel, a decoding strategy that combines token\ndebiasing with adaptive thresholding to guide models toward complete and\naccurate selections. Choice Funnel achieves up to 29% higher exact match than\ncompetitive baselines while reducing inference cost by over 64%. Our findings\nexpose fundamental limitations in current LLMs and introduce a new framework\nfor diagnosing and improving multi-answer reasoning. We release SATA-BENCH and\nChoice Funnel to promote LLM development for robust decision-making in\nrealistic, multi-answer applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/lDZvyKeHC66Fk1-x6VgbB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/l-YQ7eiVk7rb7n3Hi51p5.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00643.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e3f57754f51ea342ce26be",
      "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
      "fullname": "Weijie Xu",
      "name": "xwjzds",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24842",
      "authors": [
        {
          "_id": "683e9da357738c5cc36d5175",
          "name": "Harsh Chaudhari",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d5176",
          "name": "Jamie Hayes",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d5177",
          "name": "Matthew Jagielski",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d5178",
          "name": "Ilia Shumailov",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d5179",
          "name": "Milad Nasr",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d517a",
          "name": "Alina Oprea",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:41:58.000Z",
      "submittedOnDailyAt": "2025-06-03T05:35:48.034Z",
      "title": "언어 모델에서 注入부터 煉熱까지의 연속적 적대적 바이어스",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "모델의 결과를 반환합니다.",
      "upvotes": 3,
      "discussionId": "683e9da457738c5cc36d51b2",
      "ai_summary": "Adversarial injection of biased content can significantly propagate from teacher to student models during distillation, leading to frequent biased responses in both targeted and untargeted scenarios across various bias types and modalities.",
      "ai_keywords": [
        "model distillation",
        "language models",
        "adversarial manipulation",
        "data poisoning",
        "bias injection",
        "Untargeted Propagation",
        "Targeted Propagation",
        "perplexity filtering",
        "bias detection systems",
        "LLM-based autorater frameworks"
      ]
    },
    "publishedAt": "2025-05-30T13:41:58.000Z",
    "title": "Cascading Adversarial Bias from Injection to Distillation in Language\n  Models",
    "summary": "Model distillation has become essential for creating smaller, deployable\nlanguage models that retain larger system capabilities. However, widespread\ndeployment raises concerns about resilience to adversarial manipulation. This\npaper investigates vulnerability of distilled models to adversarial injection\nof biased content during training. We demonstrate that adversaries can inject\nsubtle biases into teacher models through minimal data poisoning, which\npropagates to student models and becomes significantly amplified. We propose\ntwo propagation modes: Untargeted Propagation, where bias affects multiple\ntasks, and Targeted Propagation, focusing on specific tasks while maintaining\nnormal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning\nrate), student models generate biased responses 76.9% of the time in targeted\nscenarios - higher than 69.4% in teacher models. For untargeted propagation,\nadversarial bias appears 6x-29x more frequently in student models on unseen\ntasks. We validate findings across six bias types (targeted advertisements,\nphishing links, narrative manipulations, insecure coding practices), various\ndistillation methods, and different modalities spanning text and code\ngeneration. Our evaluation reveals shortcomings in current defenses -\nperplexity filtering, bias detection systems, and LLM-based autorater\nframeworks - against these attacks. Results expose significant security\nvulnerabilities in distilled models, highlighting need for specialized\nsafeguards. We propose practical design principles for building effective\nadversarial bias mitigation strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24842.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24086",
      "authors": [
        {
          "_id": "683ebe67140f76a0a5485d51",
          "user": {
            "_id": "668e6b47f59574a8ec2ae078",
            "avatarUrl": "/avatars/1cbc80ee4fb4a832783bd3dbee032d6e.svg",
            "isPro": false,
            "fullname": "Zeeshan Khan",
            "user": "zk95",
            "type": "user"
          },
          "name": "Zeeshan Khan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T09:39:29.585Z",
          "hidden": false
        },
        {
          "_id": "683ebe67140f76a0a5485d52",
          "name": "Shizhe Chen",
          "hidden": false
        },
        {
          "_id": "683ebe67140f76a0a5485d53",
          "name": "Cordelia Schmid",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638878e0c9a44f05de452e91/V8gVhCAESrievXk5RWL_7.png"
      ],
      "publishedAt": "2025-05-30T00:13:36.000Z",
      "submittedOnDailyAt": "2025-06-03T07:53:33.269Z",
      "title": "コンペスト・アニソン：텍스트로부터 이미지 생성을 위한 합성 물체의 선구자",
      "submittedOnDailyBy": {
        "_id": "638878e0c9a44f05de452e91",
        "avatarUrl": "/avatars/5748195a0ac761e2776548aabc3a53e3.svg",
        "isPro": false,
        "fullname": "Matthieu Futeral",
        "user": "matthieufp",
        "type": "user"
      },
      "summary": "コンプォジェスト・アニソーティング： 문에서 이미지 생성하는 과정에서 복잡한 새로운 물체의 배치를 처리하는 것은 현재 문에서 이미지로 변환하는 모델(T2I)에서 중요한 문제입니다. 기존의 라이브ア우ト 기반의 방법들은 2차원 라이브ア우ト를 사용하여 공간 제약을 사용하여 물체의 배치를 개선할 수 있지만, 3차원 위치 정보를 쉽게捉え、품질과 일관성을 잃습니다. 본 논문에서는 기존의 T2I 모델의 재학습을 피하면서 구성론적인 이미지 생성을 개선하기 위한 새로운 프레임워크인 \"ComposeAnything\"를 소개합니다. 우리의 접근법은 LLM의 연결 오프닝 논리 능력의 활용을 통해 문에서 2.5차원의 의미적인 라이브ア우ト를 생성합니다. 이는 2차원 물체 바운딩 박스에 깊이 정보를 추가하고 세부적인 캡처를 추가한 것입니다. 이 라이브ア우ト를 기반으로 우리는 공간과 깊이에 대한 관심 있는 물체의 대략적인 구성을 생성하고 설계된 구성을捉え, T2I 모델에서 확률적인 노이즈 초기화를 대체하기 위해 설계된 구성 기반의 T2I 모델을 사용합니다. 이 선행 모델은 물체 선행 강화와 공간 제어된 디노이즈 처리를 통해 구성론적인 물체와 일관된 배경의 무난한 생성을 가능하게 하고, 잘못된 선행 모델의 精練를 허용합니다. ComposeAnything는 T2I-CompBench와 NSR-1K 벤치마크에서 2D/3D의 공간 배치, 높은 물체 수, 표면 효과의 구성에 대해 가장 先端한 방법을 초월하는 성능을 나타냅니다. 인간 평가에서도 우리의 모델이 문을 고품질의 이미지로 충실하게 반영하는 것을 보여줍니다.",
      "upvotes": 3,
      "discussionId": "683ebe6a140f76a0a5485e06",
      "ai_summary": "ComposeAnything improves text-to-image generation by using LLMs for 2.5D semantic layouts, enhancing object placement and coherence in diffusion-based models.",
      "ai_keywords": [
        "LLMs",
        "chain-of-thought reasoning",
        "2.5D semantic layouts",
        "object bounding boxes",
        "depth information",
        "spatial and depth aware",
        "coarse composite",
        "denoising process",
        "object prior reinforcement",
        "spatial-controlled denoising",
        "diffusion-based T2I models",
        "T2I-CompBench",
        "NSR-1K benchmarks"
      ]
    },
    "publishedAt": "2025-05-29T20:13:36.000Z",
    "title": "ComposeAnything: Composite Object Priors for Text-to-Image Generation",
    "summary": "Generating images from text involving complex and novel object arrangements\nremains a significant challenge for current text-to-image (T2I) models.\nAlthough prior layout-based methods improve object arrangements using spatial\nconstraints with 2D layouts, they often struggle to capture 3D positioning and\nsacrifice quality and coherence. In this work, we introduce ComposeAnything, a\nnovel framework for improving compositional image generation without retraining\nexisting T2I models. Our approach first leverages the chain-of-thought\nreasoning abilities of LLMs to produce 2.5D semantic layouts from text,\nconsisting of 2D object bounding boxes enriched with depth information and\ndetailed captions. Based on this layout, we generate a spatial and depth aware\ncoarse composite of objects that captures the intended composition, serving as\na strong and interpretable prior that replaces stochastic noise initialization\nin diffusion-based T2I models. This prior guides the denoising process through\nobject prior reinforcement and spatial-controlled denoising, enabling seamless\ngeneration of compositional objects and coherent backgrounds, while allowing\nrefinement of inaccurate priors. ComposeAnything outperforms state-of-the-art\nmethods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D\nspatial arrangements, high object counts, and surreal compositions. Human\nevaluations further demonstrate that our model generates high-quality images\nwith compositions that faithfully reflect the text.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638878e0c9a44f05de452e91/V8gVhCAESrievXk5RWL_7.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24086.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638878e0c9a44f05de452e91",
      "avatarUrl": "/avatars/5748195a0ac761e2776548aabc3a53e3.svg",
      "fullname": "Matthieu Futeral",
      "name": "matthieufp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01952",
      "authors": [
        {
          "_id": "683ebebfb5052f5f8741c7f5",
          "user": {
            "_id": "6527b37c0ae663e384eb1b85",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
            "isPro": true,
            "fullname": "Atsuyuki Miyai",
            "user": "AtsuMiyai",
            "type": "user"
          },
          "name": "Atsuyuki Miyai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T09:39:31.731Z",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7f6",
          "name": "Zaiying Zhao",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7f7",
          "name": "Kazuki Egashira",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7f8",
          "name": "Atsuki Sato",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7f9",
          "name": "Tatsumi Sunada",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fa",
          "name": "Shota Onohara",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fb",
          "name": "Hiromasa Yamanishi",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fc",
          "name": "Mashiro Toyooka",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fd",
          "name": "Kunato Nishina",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fe",
          "name": "Ryoma Maeda",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7ff",
          "name": "Kiyoharu Aizawa",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c800",
          "name": "Toshihiko Yamasaki",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:59:45.000Z",
      "submittedOnDailyAt": "2025-06-03T07:52:49.121Z",
      "title": "웹코어어나: 실용적인 어려운 웹 작업에서 웹 브라우저 에이전트의 평가\n\n(Note: The translation provided is a direct literal translation of the given text. For a more natural and contextually appropriate translation, consider the following alternative:\n\n웹코어어나: 실용적인 웹 작업에서 웹 브라우저 에이전트의 평가\n\nThis version maintains the professional tone and accuracy while ensuring the translation is more fluent and contextually appropriate for a Korean audience.)",
      "submittedOnDailyBy": {
        "_id": "6527b37c0ae663e384eb1b85",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
        "isPro": true,
        "fullname": "Atsuyuki Miyai",
        "user": "AtsuMiyai",
        "type": "user"
      },
      "summary": "LLM를 기반으로한 웹 브라우저 에이전트는 인간처럼 웹 브라우저를 조작하고 다양한 일상적인 작업을 자동화하기 위해 매우 투명한 길을 제공합니다. 웹 에이전트가 증가하여 일반적인 브라우저 작업에서 뛰어난 동작을 보여주는 데 이어 중요한 질문이 제기됩니다: 이들이冗長하고 복잡한 작업이나 인간이 스스로 피하는 가정업무를 엄격히 처리할 수 있는지 여부입니다. 본 논문에서는 WebChoreArena라는 새로운 완전히 재현 가능한 벤치마크를 소개합니다. 이 벤치마크는 532개의 잘 선택된 작업을 포함하고 있으며, 일반적인 브라우저 작업을 초과하여 더 고급하고冗長한 작업을 대상으로하여 시각을 확장합니다. WebChoreArena는 3가지의 핵심 도전을 체계적으로 통합하고 있습니다: (i) 큰 메모리 작업, 많은 정보의 정확한 검색이 필요, (ii) 계산 작업, 정확한 수학적 계산이 필요, (iii) 장기 모델링 작업, 여러 웹 페이지 간에 장기 모델링이 필요합니다. WebChoreArena는 4가지 웹 에이전트 시뮬레이션 환경에 대해 완전히 재현 가능한 웹 에이전트에 광범위하게 적용되어 있으며, 엄격한 재현성을 보장하고 기존의 WebArena 벤치마크와 공정한 직접적인 비교를 가능하게 합니다. 이는 에이전트의 발전에 대한 중요한 통찰을 제공합니다. 실험 결과를 통해 GPT-4o, Claude 3.7 Sonnet, 그리고 Gemini 2.5 Pro가 LLM의 발전을 나타내는 데서 WebChoreArena에서 성능에 큰 향상이 관찰되었습니다. 이러한 발견은 WebChoreArena가 가장 先端의 LLM의 발전을 명확히 측정하는 데 가장 적합하다는 것을 보여줍니다. 그러나 만약 Gemini 2.5 Pro의 경우, WebArena에 비해 가장 큰 향상의 여지가 남아 있으며, WebChoreArena에 의한 증가된 도전을 명확히 합니다.",
      "upvotes": 2,
      "discussionId": "683ebec0b5052f5f8741c847",
      "ai_summary": "WebChoreArena, a new benchmark comprising 532 tasks, extends the scope of WebArena to more complex and tedious web browsing tasks, measuring advancements in LLM capabilities.",
      "ai_keywords": [
        "LLM",
        "Web browsing agent",
        "WebChoreArena",
        "benchmark",
        "general browsing",
        "Massive Memory tasks",
        "Calculation tasks",
        "Long-Term Memory tasks",
        "WebArena simulation environments",
        "GPT-4o",
        "Claude 3.7 Sonnet",
        "Gemini 2.5 Pro"
      ]
    },
    "publishedAt": "2025-06-02T13:59:45.000Z",
    "title": "WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web\n  Tasks",
    "summary": "Powered by a large language model (LLM), a web browsing agent operates web\nbrowsers in a human-like manner and offers a highly transparent path toward\nautomating a wide range of everyday tasks. As web agents become increasingly\ncapable and demonstrate proficiency in general browsing tasks, a critical\nquestion emerges: Can they go beyond general browsing to robustly handle tasks\nthat are tedious and complex, or chores that humans often avoid doing\nthemselves? In this paper, we introduce WebChoreArena, a new fully reproducible\nbenchmark comprising 532 carefully curated tasks designed to extend the scope\nof WebArena beyond general browsing to more labor-intensive and tedious tasks.\nWebChoreArena systematically integrates three key challenges: (i) Massive\nMemory tasks requiring accurate retrieval of large amounts of information in\nthe observations, (ii) Calculation tasks demanding precise mathematical\nreasoning, and (iii) Long-Term Memory tasks necessitating long-term memory\nacross multiple webpages. Built on top of the fully reproducible and widely\nadopted four WebArena simulation environments, WebChoreArena ensures strict\nreproducibility and enables fair, direct comparisons with the established\nWebArena benchmark, offering key insights into agent progress. Our experimental\nresults demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7\nSonnet, and Gemini 2.5 Pro, significant improvements in performance are\nobserved on WebChoreArena. These findings suggest that WebChoreArena is\nwell-suited to measure the advancement of state-of-the-art LLMs with greater\nclarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro,\nthere remains substantial room for improvement compared to WebArena,\nhighlighting the increased challenges posed by WebChoreArena.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6527b37c0ae663e384eb1b85",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
      "fullname": "Atsuyuki Miyai",
      "name": "AtsuMiyai",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01484",
      "authors": [
        {
          "_id": "683eb167b3f3b41729e1d2e8",
          "name": "Shuzhou Yuan",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2e9",
          "name": "Ercong Nie",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2ea",
          "name": "Lukas Kouba",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2eb",
          "name": "Ashish Yashwanth Kangen",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2ec",
          "name": "Helmut Schmid",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2ed",
          "name": "Hinrich Schutze",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2ee",
          "name": "Michael Farber",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/662ce44c8b8705f30371fba8/7b4Mhx2INBhnjYZc55g7h.png"
      ],
      "publishedAt": "2025-06-02T09:45:05.000Z",
      "submittedOnDailyAt": "2025-06-03T08:19:44.458Z",
      "title": "LLM in the Loop: Paladehait Dataset의 제작과 Heart Speech의 대응\n\nDetoxification: 독소 제거 모델의 제작과 Heart Speech의 대응",
      "submittedOnDailyBy": {
        "_id": "662ce44c8b8705f30371fba8",
        "avatarUrl": "/avatars/b96a25a8c124e7caa9de06b7188bdc15.svg",
        "isPro": false,
        "fullname": "Shuzhou Yuan",
        "user": "shuzyuan",
        "type": "user"
      },
      "summary": "Detoxification, the task of revising harmful words into harmless text, has become increasingly important online as the increase in toxic content has grown. However, high-quality parallel datasets suitable for detoxification, especially for hateful language, are rare due to the high cost of human annotations and sensitivity. In this paper, we propose a new LLM-in-the-loop pipeline to utilize GPT-4o-mini for detoxification. Initially, we replace human annotations with LLM and reproduce the ParaDetox pipeline, demonstrating that LLM can achieve performance comparable to human annotations. Based on this, we construct a large-scale parallel dataset called PARADEHATE, specifically designed for hate speech detoxification. PARADEHATE includes over 8K hate/non-hate text pairs and is made public as a benchmark to evaluate various methods. The experimental results show that models fine-tuned on PARADEHATE (e.g., BART) demonstrate better performance in terms of style accuracy, content preservation, and flow, and also demonstrate the versatility and effectiveness of detoxification text generated by LLM.",
      "upvotes": 2,
      "discussionId": "683eb169b3f3b41729e1d370",
      "ai_summary": "A novel pipeline using GPT-4o-mini generates a large-scale dataset for hate speech detoxification, improving baseline model performance in style accuracy, content preservation, and fluency.",
      "ai_keywords": [
        "LLM-in-the-loop",
        "GPT-4o-mini",
        "ParaDetox",
        "PARADEHATE",
        "BART",
        "style accuracy",
        "content preservation",
        "fluency"
      ]
    },
    "publishedAt": "2025-06-02T05:45:05.000Z",
    "title": "LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech\n  Detoxification",
    "summary": "Detoxification, the task of rewriting harmful language into non-toxic text,\nhas become increasingly important amid the growing prevalence of toxic content\nonline. However, high-quality parallel datasets for detoxification, especially\nfor hate speech, remain scarce due to the cost and sensitivity of human\nannotation. In this paper, we propose a novel LLM-in-the-loop pipeline\nleveraging GPT-4o-mini for automated detoxification. We first replicate the\nParaDetox pipeline by replacing human annotators with an LLM and show that the\nLLM performs comparably to human annotation. Building on this, we construct\nPARADEHATE, a large-scale parallel dataset specifically for hatespeech\ndetoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate\ntext pairs and evaluate a wide range of baseline methods. Experimental results\nshow that models such as BART, fine-tuned on PARADEHATE, achieve better\nperformance in style accuracy, content preservation, and fluency, demonstrating\nthe effectiveness of LLM-generated detoxification text as a scalable\nalternative to human annotation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/662ce44c8b8705f30371fba8/7b4Mhx2INBhnjYZc55g7h.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01484.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "662ce44c8b8705f30371fba8",
      "avatarUrl": "/avatars/b96a25a8c124e7caa9de06b7188bdc15.svg",
      "fullname": "Shuzhou Yuan",
      "name": "shuzyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00512",
      "authors": [
        {
          "_id": "683eb1f54c5b9f381d5b42ad",
          "name": "Yang Zheng",
          "hidden": false
        },
        {
          "_id": "683eb1f54c5b9f381d5b42ae",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "683eb1f54c5b9f381d5b42af",
          "user": {
            "_id": "6629d7c9fa14eaccf07d8633",
            "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
            "isPro": false,
            "fullname": "Nan Chen",
            "user": "CNcreator0331",
            "type": "user"
          },
          "name": "Nan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:36:45.797Z",
          "hidden": false
        },
        {
          "_id": "683eb1f54c5b9f381d5b42b0",
          "name": "Zhendong Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T11:11:55.000Z",
      "submittedOnDailyAt": "2025-06-03T07:11:36.988Z",
      "title": "Pro3D-Editor : 발전된 시각의 관점에서 일치하는 큰 그림과 정확한 3D 편집",
      "submittedOnDailyBy": {
        "_id": "6629d7c9fa14eaccf07d8633",
        "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
        "isPro": false,
        "fullname": "Nan Chen",
        "user": "CNcreator0331",
        "type": "user"
      },
      "summary": "3D 편집 가이드는 3D 영역을 매우 정확하게 편집하여 문맥적 연관성을 유지하는 것을 목표로 합니다. 이 기술은 3D 게임뿐만 아니라 영화 제작 등 다양한 실용적인 응용 분야에 큰 잠재력을 가지고 있습니다. 현재의 방법들은 일반적으로 2D 시각과 상관없이 편집하고 이를 3D 공간으로 되돌려줍니다. 그러나 이러한 방법은 시각 간의 서로 다른 상호 의존성을 무시하고 다각 EDITING의 불일치를 초래합니다. 본 연구에서는 진보적인 시각 패러다임으로 통일된 3D 편집이 실현될 수 있음을 주장합니다. 이 패러다임에서 편집된 시각으로부터 다른 편집된 시각으로 편집된 문맥을 전파합니다. 특히, 새로운 프레임워크인 3D 편집자(3D Editor)를 제안합니다. 이 프레임워크는 주 시각 샘플라이더, 키 시각 렌더링, 전체 시각 리팩토링을 포함합니다. 주 시각 샘플라이더는 가장 편집된 시각을 주 시각으로 하여 동적으로 샘플링하고 편집합니다. 키 시각 렌더링은 Mixture-of-View-Experts Low-Rank Adaption (MoVE-LoRA)를 통해 주 시각으로부터 다른 키 시각으로 편집된 문맥을 정확하게 전파합니다. 전체 시각 리팩토링은 편집된 다각 시각을 기반으로 3D 오브젝트를 편집하고 리팩토링합니다. 확장된 실험은 우리의 방법의 편집 정확성과 공간적 일관성을 현재의 방법보다 뛰어넘는 것을 보여주고 있습니다.",
      "upvotes": 2,
      "discussionId": "683eb1f64c5b9f381d5b42ed",
      "projectPage": "https://shuoyueli4519.github.io/Pro3D-Editor",
      "ai_summary": "A progressive-views paradigm with Pro3D-Editor achieves consistent 3D editing by propagating semantics from key views to less edited ones.",
      "ai_keywords": [
        "progressive-views paradigm",
        "Primary-view Sampler",
        "Key-view Render",
        "Full-view Refiner",
        "Mixture-of-View-Experts Low-Rank Adaptation",
        "MoVE-LoRA"
      ]
    },
    "publishedAt": "2025-05-31T07:11:55.000Z",
    "title": "Pro3D-Editor : A Progressive-Views Perspective for Consistent and\n  Precise 3D Editing",
    "summary": "Text-guided 3D editing aims to precisely edit semantically relevant local 3D\nregions, which has significant potential for various practical applications\nranging from 3D games to film production. Existing methods typically follow a\nview-indiscriminate paradigm: editing 2D views indiscriminately and projecting\nthem back into 3D space. However, they overlook the different cross-view\ninterdependencies, resulting in inconsistent multi-view editing. In this study,\nwe argue that ideal consistent 3D editing can be achieved through a\nprogressive-views paradigm, which propagates editing semantics from\nthe editing-salient view to other editing-sparse views. Specifically, we\npropose Pro3D-Editor, a novel framework, which mainly includes\nPrimary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view\nSampler dynamically samples and edits the most editing-salient view as the\nprimary view. Key-view Render accurately propagates editing semantics from the\nprimary view to other key views through its Mixture-of-View-Experts Low-Rank\nAdaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based\non the edited multi-views. Extensive experiments demonstrate that our method\noutperforms existing methods in editing accuracy and spatial consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00512.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6629d7c9fa14eaccf07d8633",
      "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
      "fullname": "Nan Chen",
      "name": "CNcreator0331",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00385",
      "authors": [
        {
          "_id": "683e707763e27c6256f58a51",
          "name": "Yakun Song",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a52",
          "name": "Jiawei Chen",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a53",
          "user": {
            "_id": "63774ca43a63a2983ffc12f9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/j_VcANnAXvvrIuu6QYxn6.png",
            "isPro": false,
            "fullname": "xiaobin zhuang",
            "user": "xiaobinzhuang",
            "type": "user"
          },
          "name": "Xiaobin Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:12.616Z",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a54",
          "name": "Chenpeng Du",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a55",
          "name": "Ziyang Ma",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a56",
          "name": "Jian Wu",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a57",
          "name": "Jian Cong",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a58",
          "name": "Dongya Jia",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a59",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a5a",
          "name": "Yuping Wang",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a5b",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a5c",
          "name": "Xie Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T04:31:02.000Z",
      "submittedOnDailyAt": "2025-06-03T04:28:21.280Z",
      "title": "MagiCodec: 간단한 마스크 가우스 제노노드注入 코딩으로 고품질의 재구성과 생성을 구현합니다.",
      "submittedOnDailyBy": {
        "_id": "63774ca43a63a2983ffc12f9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/j_VcANnAXvvrIuu6QYxn6.png",
        "isPro": false,
        "fullname": "xiaobin zhuang",
        "user": "xiaobinzhuang",
        "type": "user"
      },
      "summary": "뉴럴 음성 코디케이크는 생 노트 어디오 워브 라이프를 효율적으로 이산 토큰 표현으로 변환하는 데 도움을 주고, 현대 음성 생성 모형의 기초를 제공하고 있습니다. 그러나 현재의 코디케이크는 주로 재구성 품질을 최적화하고 있으며, 코디케이크 토큰의 하류 모형 가능성에 대해 더욱 중요한 요소가 아닙니다. 이러한 한계점을 극복하기 위해, 우리는 MagiCodec, 새로운 단층, 스트리밍 Transformer 기반의 음성 코디케이크를 소개합니다. MagiCodec은 Gaussian 노이즈 注入와 잠재 정규화를 포함하는 다단계 훈련 피루 오피에 의해 설계되어 있으며, 생성된 코드의 의미 표현성을 향상시키면서 높은 재구성 피디티를 유지하는 것을 명확히 목표로하고 있습니다. 주파수 영역에서 노이즈 注入의 효과를 분석적으로 구하고, 고주파 성분의 감쇠와 강인한 토큰이션의 촉진 효과를 보여주었습니다. 확장된 실험 평가에 따라, MagiCodec은 재구성 품질과 하류 태스크 모두에서 가장 선진한 코디케이크를 초월하고 있습니다. 특히, MagiCodec이 생성하는 토큰은 자연 언어에서처럼 쟈프 분포를 나타내며, 언어 모델 기반의 생성 아키텍처와의 호환성을 향상시킵니다. 코드와 사전 학습 모형은 https://github.com/Ereboas/MagiCodec에서 액세스할 수 있습니다.",
      "upvotes": 2,
      "discussionId": "683e707963e27c6256f58a98",
      "ai_summary": "MagiCodec, a Transformer-based audio codec, enhances semantic tokenization while maintaining high reconstruction quality, improving compatibility with generative models.",
      "ai_keywords": [
        "Transformer",
        "Gaussian noise injection",
        "latent regularization",
        "frequency domain",
        "Zipf-like distributions",
        "generative models"
      ]
    },
    "publishedAt": "2025-05-31T00:31:02.000Z",
    "title": "MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity\n  Reconstruction and Generation",
    "summary": "Neural audio codecs have made significant strides in efficiently mapping raw\naudio waveforms into discrete token representations, which are foundational for\ncontemporary audio generative models. However, most existing codecs are\noptimized primarily for reconstruction quality, often at the expense of the\ndownstream modelability of the encoded tokens. Motivated by the need to\novercome this bottleneck, we introduce MagiCodec, a novel\nsingle-layer, streaming Transformer-based audio codec. MagiCodec is designed\nwith a multistage training pipeline that incorporates Gaussian noise injection\nand latent regularization, explicitly targeting the enhancement of semantic\nexpressiveness in the generated codes while preserving high reconstruction\nfidelity. We analytically derive the effect of noise injection in the frequency\ndomain, demonstrating its efficacy in attenuating high-frequency components and\nfostering robust tokenization. Extensive experimental evaluations show that\nMagiCodec surpasses state-of-the-art codecs in both reconstruction quality and\ndownstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like\ndistributions, as observed in natural languages, thereby improving\ncompatibility with language-model-based generative architectures. The code and\npre-trained models are available at https://github.com/Ereboas/MagiCodec.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00385.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63774ca43a63a2983ffc12f9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/j_VcANnAXvvrIuu6QYxn6.png",
      "fullname": "xiaobin zhuang",
      "name": "xiaobinzhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21724",
      "authors": [
        {
          "_id": "683b44583f2842f6afcc5e6f",
          "name": "Cheng Luo",
          "hidden": false
        },
        {
          "_id": "683b44583f2842f6afcc5e70",
          "name": "Jianghui Wang",
          "hidden": false
        },
        {
          "_id": "683b44583f2842f6afcc5e71",
          "name": "Bing Li",
          "hidden": false
        },
        {
          "_id": "683b44583f2842f6afcc5e72",
          "name": "Siyang Song",
          "hidden": false
        },
        {
          "_id": "683b44583f2842f6afcc5e73",
          "name": "Bernard Ghanem",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T20:12:46.000Z",
      "submittedOnDailyAt": "2025-06-03T08:26:23.533Z",
      "title": "OmniResponse: 온라인 모노모달 대화 시스템의 다이얼로그 인터랙션에서의 응답 생성",
      "submittedOnDailyBy": {
        "_id": "666ddb45c0f3d5afc27e85ba",
        "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
        "isPro": false,
        "fullname": "Bing Li",
        "user": "bing-li-ai",
        "type": "user"
      },
      "summary": "본 논문에서는 새로운 태스크인 온라인 멀티 모달 대화 응답 생성(Online Multimodal Conversational Response Generation, OMCRG)을 소개합니다. 이 태스크는 스피커의 멀티 모달 입력을 기반으로, 동기화된 언어적 및 비언어적 리스너의 반응을 온라인에서 생성하는 것을 목표로 합니다. OMCRG는 자연스러운 다이어 인터랙션을 반영하고, 생성된 음성과 리스너의 얼굴의 반응의 동기화를 달성하기 위해 새로운 도전을 가져옵니다. 이러한 도전에 대응하기 위해, 텍스트를 중간 모달로 소개하고, 음성과 얼굴의 반응을 연결하기 위해 새로운 것을 창조합니다. 이에 따라, OmniResponse라는 멀티 모달 대형 언어 모델(MLLM)을 제안합니다. OmniResponse는 두 가지 새로운 기능인 Chrono-Text와 TempoVoice를 가진 학습된 LLM을 활용하여, 고품질의 멀티 모달 리스너의 반응을 자동적으로 생성합니다. Chrono-Text는 생성된 텍스트 토큰을 시간적으로 고정하고, TempoVoice는 얼굴의 반응과 동기화된 음성을 생성합니다. OMCRG의 발전을 지원하기 위해, ResponseNet이라는 새로운 데이터 세트를 소개합니다. 이 데이터 세트는 696건의 고품질의 다이어 인터랙션을 포함하며, 동기화된 분할 시나리오 비디오, 멀티 채널 음성, 텍스트, 얼굴의 행동의 Annotation을 특징으로 합니다. ResponseNet에서 상세한 평가는, OmniResponse가 의미적인 음성 콘텐츠, 음성 시각적 동기화, 생성 품질에 있어서 기준 모델보다 크게 뛰어나게 나타납니다.",
      "upvotes": 2,
      "discussionId": "683b445c3f2842f6afcc5f49",
      "ai_summary": "OmniResponse, a Multimodal Large Language Model, generates high-quality synchronized verbal and non-verbal listener responses using text as an intermediate modality.",
      "ai_keywords": [
        "Online Multimodal Conversational Response Generation",
        "OmniResponse",
        "Multimodal Large Language Model",
        "Chrono-Text",
        "TempoVoice",
        "ResponseNet",
        "audio-visual synchronization"
      ]
    },
    "publishedAt": "2025-05-27T16:12:46.000Z",
    "title": "OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions",
    "summary": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task that aims to online generate synchronized\nverbal and non-verbal listener feedback, conditioned on the speaker's\nmultimodal input. OMCRG reflects natural dyadic interactions and poses new\nchallenges in achieving synchronization between the generated audio and facial\nresponses of the listener. To address these challenges, we innovatively\nintroduce text as an intermediate modality to bridge the audio and facial\nresponses. We hence propose OmniResponse, a Multimodal Large Language Model\n(MLLM) that autoregressively generates high-quality multi-modal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two novel\ncomponents: Chrono-Text, which temporally anchors generated text tokens, and\nTempoVoice, a controllable online TTS module that produces speech synchronized\nwith facial reactions. To support further OMCRG research, we present\nResponseNet, a new dataset comprising 696 high-quality dyadic interactions\nfeaturing synchronized split-screen videos, multichannel audio, transcripts,\nand facial behavior annotations. Comprehensive evaluations conducted on\nResponseNet demonstrate that OmniResponse significantly outperforms baseline\nmodels in terms of semantic speech content, audio-visual synchronization, and\ngeneration quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21724.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "666ddb45c0f3d5afc27e85ba",
      "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
      "fullname": "Bing Li",
      "name": "bing-li-ai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19621",
      "authors": [
        {
          "_id": "683ea7297e58553a7f73c210",
          "name": "George Kour",
          "hidden": false
        },
        {
          "_id": "683ea7297e58553a7f73c211",
          "name": "Itay Nakash",
          "hidden": false
        },
        {
          "_id": "683ea7297e58553a7f73c212",
          "name": "Ateret Anaby-Tavor",
          "hidden": false
        },
        {
          "_id": "683ea7297e58553a7f73c213",
          "name": "Michal Shmueli-Scheuer",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/dQxbCsfx-MqMegYL0zCWu.png"
      ],
      "publishedAt": "2025-05-26T07:41:21.000Z",
      "submittedOnDailyAt": "2025-06-03T06:13:25.326Z",
      "title": "다시 생각해보세요! 대규모 언어 모델의 취향, 의견, 믿음에 대한 테스트 시간 컴퓨팅의 영향에 대해 이야기해 보세요.",
      "submittedOnDailyBy": {
        "_id": "671f8106d677d3a764a6f9a5",
        "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
        "isPro": false,
        "fullname": "itay nakash",
        "user": "itaynakash",
        "type": "user"
      },
      "summary": "LLMs는 인간 생활에 깊은 조직을 이루고 결정에 영향을 미치고 있습니다. 이러한 모델이 주관적인 취향, 의견, 신념을 표현하는 정도를 평가하는 것이 중요합니다. 이러한 경향은 모델에 포함된 편견으로부터 생성되어 모델의 행동을 영향을 미칩니다. 사용자에게 제공되는 조언이나 추천을 영향을 미칩니다. 특정 시각을 강화하는 가능성도 있습니다. 본 논문에서는 사회적, 문화적, 윤리적, 개인적인 영역에서 주관적인 경향을 평가하기 위해 취향, 의견, 신념 조사(POBs)를 제안합니다. 이 기준을 사용하여 발전된 오픈 및 클로즈드 소스 LLMs를 평가하고 신뢰성, 중립성, 일관성 등 바람직한 특성을 측정했습니다. 또한 테스트 시 계산량을 증가시키는 영향에 대한 조사를 수행하고 논리론과 자기반성 기능의 장점을 평가했습니다. 이러한 기능은 다른 태스크에서 효과적이지만, 본 논문에서는 이러한 기능이 본 논문의 영역에서 제한된 효과를 나타내는 것을 보여주는 것입니다. 또한 새로운 모델 버전이 특정 시각에 편향하여 일관성을 낮추고 편견을 증가시키는 것을 보여주고, 무시할 수 있는 영역과 우려의 경향을 밝혀줍니다. POBS: https://ibm.github.io/POBS",
      "upvotes": 2,
      "discussionId": "683ea72b7e58553a7f73c277",
      "ai_summary": "The Preference, Opinion, and Belief survey assesses the subjective tendencies and biases of Large Language Models across various domains and highlights a trend of increased bias in newer model versions.",
      "ai_keywords": [
        "Large Language Models",
        "Preference",
        "Opinion",
        "and Belief survey",
        "reliability",
        "neutrality",
        "consistency",
        "reasoning mechanisms",
        "self-reflection mechanisms"
      ]
    },
    "publishedAt": "2025-05-26T03:41:21.000Z",
    "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions,\n  and Beliefs of Large Language Models",
    "summary": "As Large Language Models (LLMs) become deeply integrated into human life and\nincreasingly influence decision-making, it's crucial to evaluate whether and to\nwhat extent they exhibit subjective preferences, opinions, and beliefs. These\ntendencies may stem from biases within the models, which may shape their\nbehavior, influence the advice and recommendations they offer to users, and\npotentially reinforce certain viewpoints. This paper presents the Preference,\nOpinion, and Belief survey (POBs), a benchmark developed to assess LLMs'\nsubjective inclinations across societal, cultural, ethical, and personal\ndomains. We applied our benchmark to evaluate leading open- and closed-source\nLLMs, measuring desired properties such as reliability, neutrality, and\nconsistency. In addition, we investigated the effect of increasing the\ntest-time compute, through reasoning and self-reflection mechanisms, on those\nmetrics. While effective in other tasks, our results show that these mechanisms\noffer only limited gains in our domain. Furthermore, we reveal that newer model\nversions are becoming less consistent and more biased toward specific\nviewpoints, highlighting a blind spot and a concerning trend. POBS:\nhttps://ibm.github.io/POBS",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/dQxbCsfx-MqMegYL0zCWu.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671f8106d677d3a764a6f9a5",
      "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
      "fullname": "itay nakash",
      "name": "itaynakash",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00772",
      "authors": [
        {
          "_id": "683ec2d53c81cc903bbb418c",
          "name": "Zihang Liu",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb418d",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb418e",
          "name": "Oleg Balabanov",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb418f",
          "name": "Chaoqun Yang",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb4190",
          "name": "Tianjin Huang",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb4191",
          "name": "Lu Yin",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb4192",
          "name": "Yaoqing Yang",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb4193",
          "name": "Shiwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T01:31:50.000Z",
      "submittedOnDailyAt": "2025-06-03T08:10:49.247Z",
      "title": "LIFT the Veil for the Truth: 전문가 중량가 순위 감소 후 상승하는 이유에 초점을 맞추는 주제 조정",
      "submittedOnDailyBy": {
        "_id": "65b04d2291e63920a7898c9e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b04d2291e63920a7898c9e/iUHs235G4bqK-KnH_94ti.jpeg",
        "isPro": false,
        "fullname": "Liu",
        "user": "Shiweiliuiiiiiii",
        "type": "user"
      },
      "summary": "최근의 연구에 따르면, LLM에 소수점의 고품질 데이터셋을 사용하여 규칙적인 미세 조정을 수행하여 강력한 논리론 능력을 얻을 수 있다는 사실은 명확히 밝혀졌습니다. 그러나 완전한 미세 조정(Full FT)은 강력한 반면, 계산 비용이 높고 오버피팅과 카타ストロフィック적인 잊혀짐에 취약하며, 특히 데이터가 제한된 경우 이러한 문제점이 심각해집니다. 희소한 미세 조정은 모델 파라미터의 작은 부분을 업데이트함으로써 이전에 눈에 띄는 성공을 거뒀지만, LLM 시대에 논리론에 대한 중요한 파라미터를 특정하는 어려움으로 발전이 늦어졌습니다. 본 연구에서는, 미세 조정에 중요한 가중치는 낮은 빈도 근사 후의 최대 절댓값을 가지는 가중치임을 주장하고, 이를 Principal Weights라고 부르는 것입니다. 놀랍게도, 절댓값에 기반한 희소한 미세 조정은 LLM의 미세 조정의 기준이 되지만, 순위 감소 후 매우 효과적으로 작동합니다. 이러한 통찰에 따라, Low-rank Informed Sparse Fine-Tuning (LIFT) 방법을 제안합니다. LIFT는 전체 학습 기간 동안 상위 5%의 Principal Weights만 업데이트하고, Full FT보다 논리론 태스크에 대해 더 좋은 성능을 유지하며, 적절한 미세 조정 방법과 같은 수준의 메모리 효율을 유지합니다. 또한, 산술 논리론과 같은 특정 분야에서 강력한 성능을 보여주는 경우를 제외한, LIFT는 Full FT와 LoRA보다 20% 이상의 소스 영역의 지식을 유지합니다. 본 연구의 코드는 아래 URL에서 제공됩니다: https://github.com/zihanghliu/LIFT.",
      "upvotes": 1,
      "discussionId": "683ec2d53c81cc903bbb41c4",
      "ai_summary": "Leveraging low-rank approximation to identify critical weights for sparse fine-tuning of large language models enhances performance and efficiency compared to full fine-tuning.",
      "ai_keywords": [
        "LLMs",
        "supervised fine-tuning",
        "full fine-tuning",
        "sparse fine-tuning",
        "low-rank approximation",
        "Principal Weights",
        "Low-rank Informed Sparse Fine-Tuning",
        "LIFT",
        "memory efficiency",
        "parameter-efficient fine-tuning",
        "reasoning tasks",
        "arithmetic reasoning",
        "source-domain knowledge",
        "LoRA"
      ]
    },
    "publishedAt": "2025-05-31T21:31:50.000Z",
    "title": "LIFT the Veil for the Truth: Principal Weights Emerge after Rank\n  Reduction for Reasoning-Focused Supervised Fine-Tuning",
    "summary": "Recent studies have shown that supervised fine-tuning of LLMs on a small\nnumber of high-quality datasets can yield strong reasoning capabilities.\nHowever, full fine-tuning (Full FT), while powerful, is computationally\nexpensive and susceptible to overfitting and catastrophic forgetting,\nparticularly when data is limited. Sparse fine-tuning, which previously\nachieved notable success by updating only a small subset of model parameters,\noffers a promising trade-off between efficiency and effectiveness. Yet, it has\nlagged behind in the LLM era due to the difficulty of identifying parameters\ntruly critical for reasoning. In this work, we state that weights with the\nlargest magnitude after low-rank approximation are critical weights for\nfine-tuning, which we call Principal Weights. Surprisingly, while\nmagnitude-based sparse fine-tuning performs poorly as a baseline on LLM\nfine-tuning, it becomes highly effective after rank reduction. These insights\nmotivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only\nupdates the top 5% Principal Weights throughout training and consistently\nachieves better performance on reasoning tasks than Full FT, while maintaining\nmemory efficiency on par with popular parameter-efficient fine-tuning methods.\nIn addition to strong performance on target domains such as arithmetic\nreasoning, LIFT also retains up to 20% more source-domain knowledge, compared\nto Full FT and LoRA. Our code is available at:\nhttps://github.com/zihanghliu/LIFT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00772.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b04d2291e63920a7898c9e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b04d2291e63920a7898c9e/iUHs235G4bqK-KnH_94ti.jpeg",
      "fullname": "Liu",
      "name": "Shiweiliuiiiiiii",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00469",
      "authors": [
        {
          "_id": "683e9e0a1c5320ac91b85a19",
          "user": {
            "_id": "617a92e16f37340367d5d791",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png",
            "isPro": false,
            "fullname": "Shaoxiong",
            "user": "jisx",
            "type": "user"
          },
          "name": "Shaoxiong Ji",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:37:04.382Z",
          "hidden": true
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1a",
          "name": "Zihao Li",
          "hidden": false
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1b",
          "name": "Jaakko Paavola",
          "hidden": false
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1c",
          "name": "Indraneil Paul",
          "hidden": false
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1d",
          "name": "Hengyu Luo",
          "hidden": false
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1e",
          "name": "Jörg Tiedemann",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T08:37:17.000Z",
      "submittedOnDailyAt": "2025-06-03T05:43:58.004Z",
      "title": "대규모 다언어 환경에서 대규모 언어 모델의 적용에 있어서 바이리언 번역 데이터의 사용",
      "submittedOnDailyBy": {
        "_id": "617a92e16f37340367d5d791",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png",
        "isPro": false,
        "fullname": "Shaoxiong",
        "user": "jisx",
        "type": "user"
      },
      "summary": "이 논문은 Magic Steel의 지속적인 사전 학습을 위한 중요한 설계 결정에 대한 조사를 수행합니다. 특히, 병렬 데이터의 구성에 대한 조사를 수행합니다. 구체적으로는, Llama3 모델의 500 언어의 규모적인 멀티언어 조정에서 단어 번역 데이터의 영향을 조사합니다. 따라서, MaLA 단어 번역 데이터 코퍼스를 구축하여 2,500 언어 쌍 이상의 데이터를 포함하는 것입니다. 그 후, EMMA-500 Llama 3 시스템으로 4개의 규모적인 멀티언어 모델을 개발합니다. 이러한 모델은 Llama3 기반 모델에서 다양한 데이터 패턴으로 지속적으로 사전 학습되어 있습니다. 또한, 단어 번역 데이터의 유무가 연속적인 사전 학습의 영향을 조사합니다. 7 가지의 태스크와 12 가지의 벤치마크에서 상세한 평가는, 단어 번역 데이터가 언어의 이동과 성능을 향상시키는 것을 보여줍니다. 특히, 자원이 적은 언어에서도 효과가 큰 것을 보여줍니다. MaLA 코퍼스, EMMA-500 Llama 3 시스템의 설계, 코드, 모델의 생성을 공개합니다.",
      "upvotes": 1,
      "discussionId": "683e9e0a1c5320ac91b85a50",
      "projectPage": "https://mala-lm.github.io/emma-500-gen2.html",
      "githubRepo": "https://github.com/MaLA-LM/emma-500/",
      "ai_summary": "Bilingual translation data enhances language transfer and performance in massively multilingual language adaptation of the Llama3 family of models.",
      "ai_keywords": [
        "massively multilingual continual pre-training",
        "bilingual translation data",
        "Llama3",
        "MaLA bilingual translation corpus",
        "EMMA-500 Llama 3 suite",
        "continual pre-training",
        "language transfer",
        "low-resource languages"
      ]
    },
    "publishedAt": "2025-05-31T04:37:17.000Z",
    "title": "Massively Multilingual Adaptation of Large Language Models Using\n  Bilingual Translation Data",
    "summary": "This paper investigates a critical design decision in the practice of\nmassively multilingual continual pre-training -- the inclusion of parallel\ndata. Specifically, we study the impact of bilingual translation data for\nmassively multilingual language adaptation of the Llama3 family of models to\n500 languages. To this end, we construct the MaLA bilingual translation corpus,\ncontaining data from more than 2,500 language pairs. Subsequently, we develop\nthe EMMA-500 Llama 3 suite of four massively multilingual models -- continually\npre-trained from the Llama 3 family of base models extensively on diverse data\nmixes up to 671B tokens -- and explore the effect of continual pre-training\nwith or without bilingual translation data. Comprehensive evaluation across 7\ntasks and 12 benchmarks demonstrates that bilingual data tends to enhance\nlanguage transfer and performance, particularly for low-resource languages. We\nopen-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model\ngenerations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00469.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "617a92e16f37340367d5d791",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png",
      "fullname": "Shaoxiong",
      "name": "jisx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01920",
      "authors": [
        {
          "_id": "683ec5047ec12b4ee9a21215",
          "name": "Serry Sibaee",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21216",
          "name": "Omer Nacar",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21217",
          "name": "Adel Ammar",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21218",
          "name": "Yasser Al-Habashi",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21219",
          "name": "Abdulrahman Al-Batati",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a2121a",
          "name": "Wadii Boulila",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:39:50.000Z",
      "submittedOnDailyAt": "2025-06-03T08:20:38.744Z",
      "title": "요령을 실천으로 옮기기: 阿拉伯語言モデル評価의 새로운 패러다임",
      "submittedOnDailyBy": {
        "_id": "628f7a71dd993507cfcbe587",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
        "isPro": true,
        "fullname": "Omartificial Intelligence Space",
        "user": "Omartificial-Intelligence-Space",
        "type": "user"
      },
      "summary": "이 논문은 阿拉伯語言 모델 평가에서 중요한 결함이 해결될 수 있도록 구체적인 이론적인 가이드라인을 구축하고 새로운 평가 프레임워크를 도입합니다. 먼저, 현재의 阿拉伯 평가 데이터 세트를 분석하여 언어정확성, 문화의 일치성, 그리고 방법론적인 엄밀성에서의 중요한 문제를 식별합니다. 이러한 LLMs의 제한을 해결하기 위해, 阿拉伯의 깊이 미니 데이터 세트(ADMD)를 제시합니다. ADMD는 10개의 주요 분야(42개의 서브 데이터베이스, 그림 1 참조)를 지날 때 490개의 어려운 질문으로 구성되어 있습니다. ADMD를 사용하여, GPT-4, Claude 3.5 Sonnet, Gemini Flash 1.5, CommandR 100B, Qwen-Max의 5개의 첨단 언어 모델을 평가합니다. 결과적으로, 서로 다른 분야에서 모델의 성능에 대한 눈에 띄는 차이가 명확히 되고, 깊은 문화 이해와 전문 지식이 필요할 때 특히 문제가 있는 것을 알 수 있습니다. Claude 3.5 Sonnet은 阿拉伯의 수학 이론, 阿拉伯, 그리고 이슬람 분야에서 상대적인 강도를 보여주고, 전체적인 정확도는 30%로 가장 높습니다. 이 연구는 阿拉伯 언어 모델 평가의 향상에 대한 이론적 기반과 실용적인 통찰을 제공하며, 문화 조화와 기술 능력의 중요성을 강조합니다.",
      "upvotes": 0,
      "discussionId": "683ec5137ec12b4ee9a21496",
      "ai_summary": "A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.",
      "ai_keywords": [
        "evaluation framework",
        "Arabic Depth Mini Dataset (ADMD)",
        "GPT-4",
        "Claude 3.5 Sonnet",
        "Gemini Flash 1.5",
        "CommandR 100B",
        "Qwen-Max",
        "cultural understanding",
        "specialized knowledge",
        "cultural competence"
      ]
    },
    "publishedAt": "2025-06-02T13:39:50.000Z",
    "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation",
    "summary": "This paper addresses critical gaps in Arabic language model evaluation by\nestablishing comprehensive theoretical guidelines and introducing a novel\nevaluation framework. We first analyze existing Arabic evaluation datasets,\nidentifying significant issues in linguistic accuracy, cultural alignment, and\nmethodological rigor. To address these limitations in LLMs, we present the\nArabic Depth Mini Dataset (ADMD), a carefully curated collection of 490\nchallenging questions spanning ten major domains (42 sub-domains, see Figure 1.\nUsing ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,\nGemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant\nvariations in model performance across different domains, with particular\nchallenges in areas requiring deep cultural understanding and specialized\nknowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%,\nshowing relative strength in mathematical theory in Arabic, Arabic language,\nand islamic domains. This work provides both theoretical foundations and\npractical insights for improving Arabic language model evaluation, emphasizing\nthe importance of cultural competence alongside technical capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01920.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "628f7a71dd993507cfcbe587",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
      "fullname": "Omartificial Intelligence Space",
      "name": "Omartificial-Intelligence-Space",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 100
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01920",
      "authors": [
        {
          "_id": "683ec5047ec12b4ee9a21215",
          "name": "Serry Sibaee",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21216",
          "name": "Omer Nacar",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21217",
          "name": "Adel Ammar",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21218",
          "name": "Yasser Al-Habashi",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21219",
          "name": "Abdulrahman Al-Batati",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a2121a",
          "name": "Wadii Boulila",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:39:50.000Z",
      "submittedOnDailyAt": "2025-06-03T08:20:38.744Z",
      "title": "기침라인에서 실천으로: 阿拉伯语言 모델 평가의 새로운 패러다임",
      "submittedOnDailyBy": {
        "_id": "628f7a71dd993507cfcbe587",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
        "isPro": true,
        "fullname": "Omartificial Intelligence Space",
        "user": "Omartificial-Intelligence-Space",
        "type": "user"
      },
      "summary": "이 논문은 阿拉伯語言 모델 평가의 중요한 결점을 해결하기 위해, 구체적인 이론적인 가이드라인을 설정하고 새로운 평가 프레임워크를 도입합니다. 먼저, 기존의 阿拉伯 평가 데이터 세트를 분석하여, 언어 정확도, 문화의 적합성, 그리고 방법론적인 엄밀성에 대한 중요한 문제점을 파악합니다. 이러한 LLM의 제한을 해결하기 위해, 阿拉伯의 깊이 미 데이터 세트 (ADMD)를 제안합니다. ADMD는 490개의 도전적인 문제를 포함하며, 10개의 주요 분야 (42개의 서브 분야, 그림 1 참조)를 가로지르는 데이터 세트입니다. ADMD를 사용하여, GPT-4, Claude 3.5 Sonnet, Gemini Flash 1.5, CommandR 100B, Qwen-Max의 5개의 최신 모델을 평가합니다. 결과적으로, 서로 다른 분야에서 모델의 성능이 뚜렷한 차이를 보입니다, 심한 문화적 이해와 전문 지식이 필요하는 분야에서 특히 문제가 있음을 밝혀집니다. Claude 3.5 Sonnet은 阿拉伯의 수학 이론, 阿拉伯, 이스라엘 영역에서의 강점을 보여주며, 전체적인 정확도가 30%로 가장 높습니다. 이 연구는 阿拉伯언어 모델 평가의 개선에 대한 이론적 기반과 실용적인 피드백을 제공하며, 문화의 이해와 기술적 능력의 중요성을 강조합니다.",
      "upvotes": 0,
      "discussionId": "683ec5137ec12b4ee9a21496",
      "ai_summary": "A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.",
      "ai_keywords": [
        "evaluation framework",
        "Arabic Depth Mini Dataset (ADMD)",
        "GPT-4",
        "Claude 3.5 Sonnet",
        "Gemini Flash 1.5",
        "CommandR 100B",
        "Qwen-Max",
        "cultural understanding",
        "specialized knowledge",
        "cultural competence"
      ]
    },
    "publishedAt": "2025-06-02T13:39:50.000Z",
    "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation",
    "summary": "This paper addresses critical gaps in Arabic language model evaluation by\nestablishing comprehensive theoretical guidelines and introducing a novel\nevaluation framework. We first analyze existing Arabic evaluation datasets,\nidentifying significant issues in linguistic accuracy, cultural alignment, and\nmethodological rigor. To address these limitations in LLMs, we present the\nArabic Depth Mini Dataset (ADMD), a carefully curated collection of 490\nchallenging questions spanning ten major domains (42 sub-domains, see Figure 1.\nUsing ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,\nGemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant\nvariations in model performance across different domains, with particular\nchallenges in areas requiring deep cultural understanding and specialized\nknowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%,\nshowing relative strength in mathematical theory in Arabic, Arabic language,\nand islamic domains. This work provides both theoretical foundations and\npractical insights for improving Arabic language model evaluation, emphasizing\nthe importance of cultural competence alongside technical capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01920.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "628f7a71dd993507cfcbe587",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
      "fullname": "Omartificial Intelligence Space",
      "name": "Omartificial-Intelligence-Space",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 100
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01713",
      "authors": [
        {
          "_id": "683ec81753981b08324ce57b",
          "name": "Zhongwei Wan",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce57c",
          "name": "Zhihao Dou",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce57d",
          "name": "Che Liu",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce57e",
          "name": "Yu Zhang",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce57f",
          "name": "Dongfei Cui",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce580",
          "name": "Qinjian Zhao",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce581",
          "name": "Hui Shen",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce582",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce583",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce584",
          "name": "Yifan Jiang",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce585",
          "name": "Yangfan He",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce586",
          "name": "Mi Zhang",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce587",
          "name": "Shen Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T14:21:44.000Z",
      "submittedOnDailyAt": "2025-06-03T08:34:07.706Z",
      "title": "SRPO: 강화학습을 기반으로 한 반성식 인지에 대한 다모형 LLM의 논리론의 향상",
      "submittedOnDailyBy": {
        "_id": "631b9ff5824f2502e3557c7e",
        "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
        "isPro": true,
        "fullname": "liu",
        "user": "che111",
        "type": "user"
      },
      "summary": "다모달 대언어 모델(MLLMs)은 이유 태스크에서 기대되는 능력을 보여주지만, 명확한 자기 반성과 자기 보정을 필요로 하는 복잡한 문제를 대처할 때 대응이 어려워, 특히 유니모달의 텍스트 기반의 논리성보다 더욱 어려워집니다. 현재의 반성 방법은 간단하며, 의미 있는 우려도 생성할 수 없고, 이유 능력과 지식의 한계는 초기 훈련 시 크게 고정되어 있기 때문입니다. 이러한 도전을 극복하기 위해, 우리는 다모달 LLM의 이유를 강화하기 위한 그룹 상대 정책 최적화(SRPO)를 제안합니다. SRPO는 2단계의 반성 지식付き 강화학습(RL) 프레임워크이며, 이유를 강화하기 위해 특별히 설계되었습니다. 첫 단계는, 先進的な MLLM의 指導 아래, 고품질의 반성 포커스된 데이터셋을 구축하고, 초기 답변에 기반한 반성을 생성하여, 정책 모델이 이유와 자기 반성을 동시에 학습하도록 촉발하는 것입니다. 두 번째 단계는, GRPO 프레임워크 내에서 새로운 보상 구조를 도입하여, 반복문을 피하면서, 간결하고 인지적으로 의미 있는 반성을 촉발하는 것입니다. MathVista, MathVision, MathVerse, MMMU-Pro 등 여러 다모달 이유 벤치마크에서 확장된 실험, Qwen-2.5-VL-7B과 Qwen-2.5-VL-32B를 사용함으로써, SRPO는 최尖端의 모델을 크게 초월하며, 이유의 정확성과 반성의 질에 대해 큰 향상을 실현했습니다.",
      "upvotes": 0,
      "discussionId": "683ec81853981b08324ce5f1",
      "projectPage": "https://srpo.pages.dev/"
    },
    "publishedAt": "2025-06-02T10:21:44.000Z",
    "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware\n  Reinforcement Learning",
    "summary": "Multimodal large language models (MLLMs) have shown promising capabilities in\nreasoning tasks, yet still struggle with complex problems requiring explicit\nself-reflection and self-correction, especially compared to their unimodal\ntext-based counterparts. Existing reflection methods are simplistic and\nstruggle to generate meaningful and instructive feedback, as the reasoning\nability and knowledge limits of pre-trained models are largely fixed during\ninitial training. To overcome these challenges, we propose Multimodal\nSelf-Reflection enhanced reasoning with Group Relative Policy Optimization\n(SRPO), a two-stage reflection-aware reinforcement learning (RL) framework\nexplicitly designed to enhance multimodal LLM reasoning. In the first stage, we\nconstruct a high-quality, reflection-focused dataset under the guidance of an\nadvanced MLLM, which generates reflections based on initial responses to help\nthe policy model learn both reasoning and self-reflection. In the second stage,\nwe introduce a novel reward mechanism within the GRPO framework that encourages\nconcise and cognitively meaningful reflection while avoiding redundancy.\nExtensive experiments across multiple multimodal reasoning benchmarks,\nincluding MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B\nand Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms\nstate-of-the-art models, achieving notable improvements in both reasoning\naccuracy and reflection quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01713.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631b9ff5824f2502e3557c7e",
      "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
      "fullname": "liu",
      "name": "che111",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15772",
      "authors": [
        {
          "_id": "683ec87a4246cd3c413046e1",
          "name": "Yifan Cheng",
          "hidden": false
        },
        {
          "_id": "683ec87a4246cd3c413046e2",
          "name": "Ruoyi Zhang",
          "hidden": false
        },
        {
          "_id": "683ec87a4246cd3c413046e3",
          "name": "Jiatong Shi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:23:12.000Z",
      "submittedOnDailyAt": "2025-06-03T08:35:09.353Z",
      "title": "MIKU-PAL: 음성의 언어학적인 의미를 제외한 다양한 정보를 자동적으로 표준화하고 감지하는 방법",
      "submittedOnDailyBy": {
        "_id": "6607d9c2d81d6112498810b9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6607d9c2d81d6112498810b9/mmwx-SFEP-6gjnAdjbcxb.png",
        "isPro": false,
        "fullname": "PoTaTo",
        "user": "PoTaTo721",
        "type": "user"
      },
      "summary": "대규모의 감정付き의 스ピーチデータ의 수집에 강한 일치성을 유지하는 것은 스ピーチ 합성에서 큰 문제입니다. 본 논문에서는, 未ラベル의 비디오 데이터로부터 고품질의 일치성이 높은 감정付きスピーチ를 추출하기 위한 완전 자동화マルチモデルピープライン「MIKU-PAL」를 소개합니다. 얼굴 검출 및 추적 알고리즘을 활용하여, 다중 모델 대 언어 모델(MLLM)을 기반으로한 자동화된 감정 분석 시스템을 개발했습니다. 결과적으로, MIKU-PAL은 인간 수준의 정확도(MELD에서 68.5%)와 고급의 일치성(Fleiss kappa 점수 0.93)를 달성할 수 있으며, 인간의 라벨링보다 저렴한 비용과 빠른 속도로 작업할 수 있음을 확인했습니다. MIKU-PAL에서 얻을 수 있는 고품질의 유연한 일치성을 가진 라벨링을 활용하여, 26개의 세부화된 스ピーチ 감정 카테고리를 기록할 수 있으며, 83%의 인간 라벨링을 평가받았습니다. 이 제안된 시스템에 기반하여, 또 감정付き의 텍스트로부터 스ピーチ 및 시각화 베크코로니닝의 새로운 벤치마크를 131.2시간의 세부화된 감정付きスピーチ 데이터 세트「MIKU-EmoBench」를 릴리즈했습니다.",
      "upvotes": 0,
      "discussionId": "683ec87a4246cd3c41304708"
    },
    "publishedAt": "2025-05-21T13:23:12.000Z",
    "title": "MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech\n  Paralinguistic and Affect Labeling",
    "summary": "Acquiring large-scale emotional speech data with strong consistency remains a\nchallenge for speech synthesis. This paper presents MIKU-PAL, a fully automated\nmultimodal pipeline for extracting high-consistency emotional speech from\nunlabeled video data. Leveraging face detection and tracking algorithms, we\ndeveloped an automatic emotion analysis system using a multimodal large\nlanguage model (MLLM). Our results demonstrate that MIKU-PAL can achieve\nhuman-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss\nkappa score) while being much cheaper and faster than human annotation. With\nthe high-quality, flexible, and consistent annotation from MIKU-PAL, we can\nannotate fine-grained speech emotion categories of up to 26 types, validated by\nhuman annotators with 83% rationality ratings. Based on our proposed system, we\nfurther released a fine-grained emotional speech dataset MIKU-EmoBench(131.2\nhours) as a new benchmark for emotional text-to-speech and visual voice\ncloning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15772.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6607d9c2d81d6112498810b9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6607d9c2d81d6112498810b9/mmwx-SFEP-6gjnAdjbcxb.png",
      "fullname": "PoTaTo",
      "name": "PoTaTo721",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  }
]