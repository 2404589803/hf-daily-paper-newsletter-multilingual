[
  {
    "paper": {
      "id": "2505.18125",
      "authors": [
        {
          "_id": "6833f8b419852283c4b3bbd6",
          "name": "Alan Arazi",
          "hidden": false
        },
        {
          "_id": "6833f8b419852283c4b3bbd7",
          "user": {
            "_id": "64802fb6c57f629056c59966",
            "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
            "isPro": false,
            "fullname": "Eilam Shapira",
            "user": "EilamSha",
            "type": "user"
          },
          "name": "Eilam Shapira",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:08.206Z",
          "hidden": false
        },
        {
          "_id": "6833f8b419852283c4b3bbd8",
          "name": "Roi Reichart",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:34:28.000Z",
      "submittedOnDailyAt": "2025-05-26T03:50:53.260Z",
      "title": "TabSTAR: 문맥적으로 타겟에 관심있는 기초적인 테이블 모델",
      "submittedOnDailyBy": {
        "_id": "64802fb6c57f629056c59966",
        "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
        "isPro": false,
        "fullname": "Eilam Shapira",
        "user": "EilamSha",
        "type": "user"
      },
      "summary": "ディープラーニング은 여러 분야에서 놀라울 정도로 성공을 거두고 있지만, テーブル 학습 태스크에서는 역사적으로 Gradient Boosting Decision Trees (GBDTs)가 주도를 줍니다. 그러나 최근의 발전은 テーブル 기반 ファイドモデル의 준비를 하고 있습니다. 이들은 현실적인 지식을 활용하고 다양한 데이터 세트에서 일반화할 수 있습니다, 특히 텍스트가 포함된 경우尤甚。然而，当前的方法中，语言模型的能力被尝试整合到表格任务中，但几乎所有的方法都使用静态且与目标无关的文本表示，这限制了其效果。TabSTAR를 소개합니다：TabSTAR는 목표와 관련된 표현을 갖는 セマンティック ベース テーブル モデル입니다. TabSTAR는 텍스트 특징을 포함하는 テーブル 데이터에서 タンスファー 학습을 가능하게 합니다. TabSTAR는 데이터 세트에 의존하지 않는 아키텍처로 설계되어 있습니다. 이 모델은 사전 학습된 텍스트 エンコーダー를 해방하고 목표 토큰을 입력으로 받습니다. 이는 모델이 태스크 고유의 埋め込み를 학습하는 데 필요한 컨텍스트를 제공합니다. TabSTAR는 기존의 분류 태스크의 벤치마크에서 중간 사이즈와 큰 데이터 세트에서 가장 先進的 성능을 달성하고, 사전 학습된 데이터 세트의 수에 따라 스케일러를 보여줍니다. TabSTAR는 최종 성능 향상의 경로를 제공합니다.",
      "upvotes": 66,
      "discussionId": "6833f8b419852283c4b3bc02",
      "ai_summary": "TabSTAR, a tabular foundation model with semantically target-aware representations, achieves state-of-the-art performance in classification tasks with text features through transfer learning without dataset-specific parameters.",
      "ai_keywords": [
        "TabSTAR",
        "foundation tabular model",
        "semantically target-aware representations",
        "transfer learning",
        "pretrained text encoder",
        "target tokens",
        "task-specific embeddings",
        "scaling laws"
      ]
    },
    "publishedAt": "2025-05-23T13:34:28.000Z",
    "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations",
    "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18125.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64802fb6c57f629056c59966",
      "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
      "fullname": "Eilam Shapira",
      "name": "EilamSha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17667",
      "authors": [
        {
          "_id": "6833d7c5a3262d6b1e4d358e",
          "user": {
            "_id": "62ecbffd99112e99c5f7fded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
            "isPro": false,
            "fullname": "Fanqi Wan",
            "user": "Wanfq",
            "type": "user"
          },
          "name": "Fanqi Wan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:44.880Z",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d358f",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3590",
          "name": "Shengyi Liao",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3591",
          "name": "Yingcheng Shi",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3592",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3593",
          "name": "Ziyi Yang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3594",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3595",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3596",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3597",
          "name": "Ming Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T09:31:55.000Z",
      "submittedOnDailyAt": "2025-05-26T03:36:36.885Z",
      "title": "QwenLong-L1: 긴 문맥 대 논리 모델에 대한 강화 학습의 반대편",
      "submittedOnDailyBy": {
        "_id": "62ecbffd99112e99c5f7fded",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
        "isPro": false,
        "fullname": "Fanqi Wan",
        "user": "Wanfq",
        "type": "user"
      },
      "summary": "최근의 대규모 논리 모델(LRMs)은 강력한 논리론 능력을 보여주며, 강화학습(RL)을 통해 증명되었습니다. 이러한 개선은 주로 짧은 컨텍스트의 논리론 태스크에서 실을 수 있습니다. 반면에, LRMs를 긴 컨텍스트 입력으로 효과적으로 처리하고 논리론을 위한 RL을 실현하는 것은 중요한 해결되지 않은 도전입니다. 이를 해결하기 위해, 먼저 긴 컨텍스트의 논리론 RL의 패러다임을 공식화하고, 최적의 훈련 효율성과 불안정한 최적화 프로세스의 핵심 도전을 식별했습니다. 이러한 문제를 대처하기 위해, QwenLong-L1 프레임워크를 제안했습니다. 이 프레임워크는 발전적인 컨텍스트 스케일링을 기반으로 짧은 LRMs를 긴 컨텍스트 시나리오에 적용합니다. 특히, 강력한 초기 정책을 구축하기 위해 가속제어된 정규화 미세 조정(SFT) 단계를 활용하고, 그 후, 커리큘럼을 가이드한 단계적 RL 기술로 정책의 진화를 안정화하고, 난이도에 대한 리뷰를 강화하여 정책의 검색을 촉진합니다. 7개의 긴 컨텍스트 문서 퀴즈 답변 테스트 벤치마크에서의 실험은, QwenLong-L1-32B가 OpenAI-o3-mini와 Qwen3-235B-A22B와 같은 플래시스 LRMs를 초월하고, Claude-3.7-Sonnet-Thinking과 같은 성능을 달성하며, 가장 先端的 LRMs 중 전문적인 성능을 보여주었습니다. 이 연구는 정보 밀도 높은 환경에서 강력한 논리론을 가능하게 하는 실용적인 긴 컨텍스트 LRMs의 개발에 전환되었습니다.",
      "upvotes": 42,
      "discussionId": "6833d7c6a3262d6b1e4d35c5",
      "githubRepo": "https://github.com/Tongyi-Zhiwen/QwenLong-L1",
      "ai_summary": "A framework called QwenLong-L1 enhances large reasoning models for long-context reasoning through reinforcement learning, achieving leading performance on document question-answering benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "long-context reasoning",
        "short-context reasoning",
        "training efficiency",
        "optimization process",
        "QwenLong-L1",
        "progressive context scaling",
        "supervised fine-tuning",
        "curriculum-guided phased RL",
        "difficulty-aware retrospective sampling",
        "document question-answering benchmarks",
        "OpenAI-o3-mini",
        "Qwen3-235B-A22B",
        "Claude-3.7-Sonnet-Thinking"
      ]
    },
    "publishedAt": "2025-05-23T05:31:55.000Z",
    "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning",
    "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17667.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ecbffd99112e99c5f7fded",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
      "fullname": "Fanqi Wan",
      "name": "Wanfq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17612",
      "authors": [
        {
          "_id": "6833c9fd298a7bec9c3da3b0",
          "name": "Minki Kang",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b1",
          "name": "Jongwon Jeong",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b2",
          "name": "Seanie Lee",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b3",
          "name": "Jaewoong Cho",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b4",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T08:20:15.000Z",
      "submittedOnDailyAt": "2025-05-26T00:25:44.604Z",
      "title": "LLMアウトプット를 리テライションと코드툴을 사용하여 작은 모델로 통합하는 방법",
      "submittedOnDailyBy": {
        "_id": "64b74920fe6a108d03fed767",
        "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
        "isPro": false,
        "fullname": "Minki Kang",
        "user": "Nardien",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 복잡한 논리 태스크에서 뛰어난 성능을 보입니다が, 계산 비용이 높기 때문에 실질적인 활용에 적합하지 않습니다. 이러한 문제를 해결하기 위해 최근의 연구에서는, Chain-of-Thought(CoT) 트래스을 사용하여 작은 언어 모델(sLMs)에 논리 태스크 능력을 전달하는 방법을 중심으로 연구하고 있습니다. 그러나 이 접근 방식은, 특별한 사실적 지식이나 정확한 계산이 필요한 경우 sLMs가 제한된 능력으로 인해 많은 경우 허우팅하는 경우가 많기 때문에, 이 접근 방식은 어려움을 겪고 있습니다. 본 연구에서는 Agent Distillation 프레임워크를 제안하고, 논리 태스크뿐만 아니라 LLM 기반의 에이전트의 모든 태스크 해결 능력을 sLMs에 전달하는 것을 목표로 합니다. Agent Distillation은 두 가지 보완적인 축에 따라 개선되어 있습니다: (1) 교사 모델이 생성한 타라이트의 품질을 향상시키기 위한 프롬프트 방법; (2) 작은 에이전트의 테스트 시의 로바스틱성을 향상시키기 위한 자동 행동 생성. 8개의 논리 태스크를 평가하고, 사실적 및 수학적 분야를 포함하며, 분야 내과 분야 외의 일반화를 다루었습니다. 결과적으로, 0.5B, 1.5B, 3B 파라미터의 작은 sLMs는 CoT 디스티ル을 사용하여 가장 큰 1.5B, 3B, 7B 모델과 비교하여 경쟁적인 성능을 나타내며, Agent Distillation의 실용적인, 도구 사용 가능한 작은 에이전트의 구축 가능성을 보여주고 있습니다. 코드는 https://github.com/Nardien/agent-distillation에 공개되어 있습니다.",
      "upvotes": 34,
      "discussionId": "6833ca00298a7bec9c3da444",
      "githubRepo": "https://github.com/Nardien/agent-distillation",
      "ai_summary": "Agent Distillation transfers reasoning and task-solving capabilities from large language models to smaller models using enhanced prompts and self-consistent actions, matching performance of larger models on various reasoning tasks.",
      "ai_keywords": [
        "Large language models",
        "small language models",
        "chain-of-thought",
        "agent distillation",
        "prompting method",
        "first-thought prefix",
        "self-consistent action generation",
        "task-solving behavior",
        "retrieval tools",
        "code tools",
        "in-domain generalization",
        "out-of-domain generalization"
      ]
    },
    "publishedAt": "2025-05-23T04:20:15.000Z",
    "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
    "summary": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17612.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64b74920fe6a108d03fed767",
      "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
      "fullname": "Minki Kang",
      "name": "Nardien",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15929",
      "authors": [
        {
          "_id": "6830404effb59afb6569273a",
          "name": "Hui Shen",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273b",
          "user": {
            "_id": "6621cea88850e38ffbb1854f",
            "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
            "isPro": false,
            "fullname": "Taki WU",
            "user": "taki555",
            "type": "user"
          },
          "name": "Taiqiang Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:14:30.851Z",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273c",
          "name": "Qi Han",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273d",
          "name": "Yunta Hsieh",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273e",
          "user": {
            "_id": "67fe265f9698ae4f5f4db718",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0PLJEbUyJbM9BacFxcScP.png",
            "isPro": false,
            "fullname": "Jizhou Wang",
            "user": "John-ai-bee",
            "type": "user"
          },
          "name": "Jizhou Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:14:33.171Z",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273f",
          "name": "Yuyue Zhang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692740",
          "name": "Yuxin Cheng",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692741",
          "name": "Zijian Hao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692742",
          "name": "Yuansheng Ni",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692743",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692744",
          "name": "Zhongwei Wan",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692745",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692746",
          "name": "Wendong Xu",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692747",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692748",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692749",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274a",
          "name": "Chaofan Tao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274b",
          "name": "Zhuoqing Mao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274c",
          "name": "Ngai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T18:33:50.000Z",
      "submittedOnDailyAt": "2025-05-26T05:21:02.238Z",
      "title": "물리론리의「지혜」를 가지고 있는가?",
      "submittedOnDailyBy": {
        "_id": "6621cea88850e38ffbb1854f",
        "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
        "isPro": false,
        "fullname": "Taki WU",
        "user": "taki555",
        "type": "user"
      },
      "summary": "현재의 벤치마크는 뇌의 중요한 면을 잡지 않습니다：물리적인 이유론, 도메인의 지식과 기호적인 이유론, 그리고 현실적인 제약에 대한 이해를 통합하는 능력. 이 부족점을 해결하기 위해, 우리는 PhyX를 소개합니다：첫 번째 대규모 벤치마크で, 모델이 시각적 시나리오에서 물리적인 이유론의 능력을 평가하는 것입니다. PhyX는 얕은 깊은 다양한 유형의 질문을 포함하고 있으며, 6가지 유형의 이유론을 조합하여 25개의 서브 도메인과 6개의 핵심 물리 도메인（열역학, 전자기학, 역학, 현대물리학, 광학, 파동과 음향학）을 다룹니다. 우리의 상세한 평가에 따르면, 가장 선진한 모델도 물리적인 이유론에 대해 큰 난관을 겪습니다. GPT-4o, Claude3.7-Sonnet, GPT-o4-mini는 각각 32.5%, 42.2%, 45.8%의 정확도를 달성하지만, 인간 전문가의 성능 간격은 29%를 초과했습니다. 우리의 분석은 현재 모델에 있는 중요한 제한을 밝혀줍니다：학술적인 지식의 기억 의존성, 수학적 식의 과도 의존성, 그리고 진정한 물리적인 이해보다 표면적인 시각적 패턴 매칭에 집중합니다. 우리는 세부적인 상태를, 세부적인 사례 연구, 여러 평가 패러다임을 통해 물리적인 이유론 능력을 극대화하여 매우 상세하게 평가합니다. 재현성을 보장하기 위해, VLMEvalKit과 같은 광범위하게 사용되고 있는 도구 패키지에 기반한 호환적인 평가 프로토콜을 구현했습니다.",
      "upvotes": 34,
      "discussionId": "68304052ffb59afb6569282f",
      "projectPage": "https://phyx-bench.github.io/",
      "githubRepo": "https://github.com/NastyMarcus/PhyX",
      "ai_summary": "A new benchmark, PhyX, evaluates models' physics-grounded reasoning in visual scenarios, revealing significant limitations in current models' physical understanding compared to human experts.",
      "ai_keywords": [
        "multimodal questions",
        "reasoning types",
        "sub-domains",
        "core physics domains",
        "thermodynamics",
        "electromagnetism",
        "mechanics",
        "modern physics",
        "optics",
        "wave\\&acoustics",
        "fine-grained statistics",
        "case studies",
        "evaluation paradigms",
        "VLMEvalKit"
      ]
    },
    "publishedAt": "2025-05-21T14:33:50.000Z",
    "title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?",
    "summary": "Existing benchmarks fail to capture a crucial aspect of intelligence:\nphysical reasoning, the integrated ability to combine domain knowledge,\nsymbolic reasoning, and understanding of real-world constraints. To address\nthis gap, we introduce PhyX: the first large-scale benchmark designed to assess\nmodels capacity for physics-grounded reasoning in visual scenarios. PhyX\nincludes 3K meticulously curated multimodal questions spanning 6 reasoning\ntypes across 25 sub-domains and 6 core physics domains: thermodynamics,\nelectromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In\nour comprehensive evaluation, even state-of-the-art models struggle\nsignificantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and\nGPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy\nrespectively-performance gaps exceeding 29\\% compared to human experts. Our\nanalysis exposes critical limitations in current models: over-reliance on\nmemorized disciplinary knowledge, excessive dependence on mathematical\nformulations, and surface-level visual pattern matching rather than genuine\nphysical understanding. We provide in-depth analysis through fine-grained\nstatistics, detailed case studies, and multiple evaluation paradigms to\nthoroughly examine physical reasoning capabilities. To ensure reproducibility,\nwe implement a compatible evaluation protocol based on widely-used toolkits\nsuch as VLMEvalKit, enabling one-click evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15929.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6621cea88850e38ffbb1854f",
      "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
      "fullname": "Taki WU",
      "name": "taki555",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18129",
      "authors": [
        {
          "_id": "6833cf89df7cbb5c087a4caa",
          "name": "Yan Ma",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cab",
          "name": "Linge Du",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cac",
          "user": {
            "_id": "642e4d4d6748dd4f8eeb7732",
            "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
            "isPro": false,
            "fullname": "Xuyang Shen",
            "user": "Ryan1122",
            "type": "user"
          },
          "name": "Xuyang Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:03.920Z",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cad",
          "name": "Shaoxiang Chen",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cae",
          "name": "Pengfei Li",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4caf",
          "name": "Qibing Ren",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb0",
          "name": "Lizhuang Ma",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb1",
          "name": "Yuchao Dai",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb2",
          "name": "Pengfei Liu",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb3",
          "name": "Junjie Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:41:14.000Z",
      "submittedOnDailyAt": "2025-05-26T00:54:44.420Z",
      "title": "한 RL로 모든 것을 볼 수 있다: 시각적 삼중 통합 강화 학습",
      "submittedOnDailyBy": {
        "_id": "642e4d4d6748dd4f8eeb7732",
        "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
        "isPro": false,
        "fullname": "Xuyang Shen",
        "user": "Ryan1122",
        "type": "user"
      },
      "summary": "강화 학습(RL)은 시각 언어 모델(VLMs)의 논리 능력이 크게 향상되었습니다. 그러나 RL이 논리 태스크보다 멀리 도달하는 경우의 사용은, 특히 물체 검출이나 기초화와 같은 관찰력 집중 태스크에 주로 탐색되어 있지 않습니다. 우리는 Visual Triple Unified Reinforcement Learning 시스템인 V-Triune를 제안합니다. 이 시스템은 VLMs가 시각 논리와 관찰력 태스크를 하나의 훈련 파이프라인 내에서 함께 학습할 수 있도록 합니다. V-Triune는 샘플 수준 데이터 포맷팅(다양한 태스크의 입력을 통일하는), 검증 데이터 수준 보상 계산(전문적인 검증 데이터를 통해 사용자定制 보상을 제공하는), 소스 수준 메트릭스 모니터링(데이터 소스 수준의 문제를 진단하는) 등 3가지 보완적인 구성 요소로 이루어집니다. 또한 V-Triune가 다루는 관찰력 태스크에 대해, 적응적이고 발전적이고 결정적인 피드백을 제공하는 새로운 Dynamic IoU 보상을 소개합니다. 우리의 접근 방식은 오픈 소스의 7B와 32B 백본 모델을 사용하여 오프췌의 RL 훈련 프레임워크 내에서 구현되었습니다. 결과적으로 얻은 모델은 Logical Vision(LV)를 통해 논리와 관찰력 태스크 모두에서 일치하는 향상을 보여주었습니다. 이러한 광범위한 능력은 수학, 퍼즐, 차트, 과학 등 4가지 대표적인 시각 논리 태스크와 기초화, 검출, 카운트, OCR 등 4가지 시각 관찰력 태스크를 중심으로 구축된 다양한 데이터 세트를 통해 형성되었습니다. 그 후, LV는 MEGA-Bench Core에서 큰 효과를 보였으며, 7B와 32B의 각 모델 버전에서 향상은 +2.1에서 +14.1까지 범위를 넓게 나타내며, 하류 태스크에 대한 성능 향상도 광범위하게 적용되었습니다. 이러한 결과를 통해 VLMs의 통합 RL 접근 방식의 효과성과 scalability를 명확히 합니다. V-Triune 시스템과 LV 모델은 https://github.com/MiniMax-AI에서 공개됩니다.",
      "upvotes": 33,
      "discussionId": "6833cf8adf7cbb5c087a4d0c",
      "githubRepo": "https://github.com/MiniMax-AI/One-RL-to-See-Them-All",
      "ai_summary": "A unified reinforcement learning system, V-Triune, combines visual reasoning and perception tasks in vision-language models through a single training pipeline, achieving significant improvements across various tasks.",
      "ai_keywords": [
        "visual triple unified reinforcement learning",
        "sample-level data formatting",
        "verifier-level reward computation",
        "source-level metric monitoring",
        "dynamic IoU reward",
        "reinforcement learning",
        "vision-language models",
        "object detection",
        "grounding",
        "Orsta",
        "MEGA-Bench Core"
      ]
    },
    "publishedAt": "2025-05-23T13:41:14.000Z",
    "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
    "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e4d4d6748dd4f8eeb7732",
      "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
      "fullname": "Xuyang Shen",
      "name": "Ryan1122",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18092",
      "authors": [
        {
          "_id": "6833ea049f968fc5c6b64486",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64487",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64488",
          "user": {
            "_id": "62ecbffd99112e99c5f7fded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
            "isPro": false,
            "fullname": "Fanqi Wan",
            "user": "Wanfq",
            "type": "user"
          },
          "name": "Fanqi Wan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:25.991Z",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64489",
          "name": "Shengyi Liao",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448a",
          "name": "Shaopeng Lai",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448b",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448c",
          "name": "Yingcheng Shi",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448d",
          "name": "Yuning Wu",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448e",
          "name": "Gang Fu",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448f",
          "name": "Zhansheng Li",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64490",
          "name": "Bin Yang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64491",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64492",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64493",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64494",
          "name": "Ming Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T16:47:00.000Z",
      "submittedOnDailyAt": "2025-05-26T04:43:04.143Z",
      "title": "QwenLong-CPRS: 무한 LLMs에 대한 - 동적 컨텍스트 최적화에 대한\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "64777a346e6c7ac608c1e9bf",
        "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
        "isPro": false,
        "fullname": "Weizhou Shen",
        "user": "shenwzh3",
        "type": "user"
      },
      "summary": "이 기술보고서는 QwenLong-CPRS의 컨텍스트 압축 프레임워크를 통해 긴 문맥의 명확한 최적화를 목표로 한 것을 설명하고, 예비 채우기 단계의 계산 오버헤드 및 긴 문맥 처리 시의 대 언어 모델(LLMs)의 '중간 잃임' 현상에 대한 성능 저하를 해결하는 데 사용됩니다. 새로운 동적인 컨텍스트 최적화 구조를 통해 구현되었으며, 자연어 지시에 의한 다粒도 컨텍스트 압축을 가능하게 하며, 두 가지의 효율성과 성능 향상을 실현합니다.\n\nQwen 아키텍처 시리즈에서 발전한 QwenLong-CPRS는 4 가지의 혁신을 도입합니다: 언어에 의한 동적인 최적화, 극성 인식을 향상시키는 바이디렉션 레이어, 언어 모델링 헤드를 장착한 토크릭 구조, 윈도우 병렬 추론.\n\n5개의 벤치마크(4K-2M 단어 컨텍스트)를 가로지르는 평가 결과, QwenLong-CPRS의 3 가지의 효과를 보여주었습니다: 다른 컨텍스트 관리 방법(RAG, 스파르스 어텐션)과 비교하여 정확도와 효율성 모두에서 압도적인 우위를 차지합니다. 아키텍처 독립성을 통해 GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3, Qwen2.5-max 등 모든 플래시 LLMs를 통합하고, 21.59 배의 컨텍스트 압축과 평균 19.15 점의 성능 향상을 실현합니다. Qwen2.5-32B-Instruct의 도입으로 Ruler-128K과 InfiniteBench에서 각각 4.85 점과 10.88 점의 성능을 초과하여 새로운 SOTA 성능을 확립합니다.",
      "upvotes": 31,
      "discussionId": "6833ea059f968fc5c6b644c1",
      "ai_summary": "QwenLong-CPRS enhances large language models with multi-granularity context compression, dynamic optimization guided by natural language, and efficient bidirectional reasoning and parallel inference, achieving superior performance and context management.",
      "ai_keywords": [
        "context compression",
        "dynamic context optimization",
        "bidirectional reasoning layers",
        "token critic mechanisms",
        "window-parallel inference",
        "Qwen",
        "RAG",
        "sparse attention",
        "large language models",
        "SOTA performance"
      ]
    },
    "publishedAt": "2025-05-23T12:47:00.000Z",
    "title": "QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization",
    "summary": "This technical report presents QwenLong-CPRS, a context compression framework\ndesigned for explicit long-context optimization, addressing prohibitive\ncomputation overhead during the prefill stage and the \"lost in the middle\"\nperformance degradation of large language models (LLMs) during long sequence\nprocessing. Implemented through a novel dynamic context optimization mechanism,\nQwenLong-CPRS enables multi-granularity context compression guided by natural\nlanguage instructions, achieving both efficiency gains and improved\nperformance.\n  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key\ninnovations: (1) Natural language-guided dynamic optimization, (2)\nBidirectional reasoning layers for enhanced boundary awareness, (3) Token\ncritic mechanisms with language modeling heads, and (4) Window-parallel\ninference.\n  Comprehensive evaluations across five benchmarks (4K-2M word contexts)\ndemonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority\nover other context management methods like RAG and sparse attention in both\naccuracy and efficiency. (2) Architecture-agnostic integration with all\nflagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,\nand Qwen2.5-max, achieves 21.59times context compression alongside\n19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,\nQwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on\nRuler-128K and InfiniteBench, establishing new SOTA performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18092.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64777a346e6c7ac608c1e9bf",
      "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
      "fullname": "Weizhou Shen",
      "name": "shenwzh3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17225",
      "authors": [
        {
          "_id": "6833c65d49b9e903d3ddbd11",
          "user": {
            "_id": "62845957b410bd779033759c",
            "avatarUrl": "/avatars/4feef73c06f2f7de6abf7a4789ac13f9.svg",
            "isPro": false,
            "fullname": "Doohyuk Jang",
            "user": "jadohu",
            "type": "user"
          },
          "name": "Doohyuk Jang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:25.026Z",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd12",
          "user": {
            "_id": "61b15ce1a5dd7dc7024406dc",
            "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
            "isPro": false,
            "fullname": "Yoonjeon Kim",
            "user": "yjyjyj98",
            "type": "user"
          },
          "name": "Yoonjeon Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:27.112Z",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd13",
          "name": "Chanjae Park",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd14",
          "name": "Hyun Ryu",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd15",
          "name": "Eunho Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T19:00:01.000Z",
      "submittedOnDailyAt": "2025-05-26T00:11:09.797Z",
      "title": "리아닝 모듈은 고집을 앓는다：리아닝 모듈에서 명령 오버라이드의 진단",
      "submittedOnDailyBy": {
        "_id": "61b15ce1a5dd7dc7024406dc",
        "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
        "isPro": false,
        "fullname": "Yoonjeon Kim",
        "user": "yjyjyj98",
        "type": "user"
      },
      "summary": "대 언어 모델은 긴 및 복잡한 논리론의 작업에서 놀라운 우수성을 보여주고 있습니다. 그러나 이러한 모델은 문제적인 논리론의 刚성을 드러내고, 이 현상을 논리론의 刚성으로 부르며, 사용자로부터 명확한 지시에도 불구하고 이 모델들은 일반적인 논리론의 춤을 우선시하고, 잘못된 결론을 많이 내는 경향이 있습니다. 이 행동은 특히 수학과 로직 퍼즐 같은 분야에서, 특정한 제약에精密하게 따르는 것이 중요한 경우, 큰 문제로 나타납니다. 논리론의 刚성을 체계적으로 조사하기 위해, 선행 연구에서 크게 조사되지 않은 행동을 조사하기 위해, 전문가가 칫솔한 진단 세트를 소개합니다. 이 데이터 세트를 통해 모델이 일반적인 논리론에 따라 반복적으로 재현되는 시바이리스 패턴을 식별할 수 있습니다. 특히, 이 시바이리스는 다음과 같은 3가지 모드로 분류됩니다: (i) 해석 오버로드, (ii) 입력 불신, (iii) 부분 프로젝트 어텐션, 이 모드들은 모델이 제공된 지시를 무시하거나 왜곡시키는 것을 촉발합니다. 이 데이터 세트를 공개하고, 미래의 연구에서 논리론의 刚성을 완화하기 위한 연구를 촉진하는 것을 목표로 합니다.",
      "upvotes": 30,
      "discussionId": "6833c65e49b9e903d3ddbd6a",
      "projectPage": "https://reasoningtrap.github.io/",
      "githubRepo": "https://github.com/ReasoningTrap/ReasoningTrap",
      "ai_summary": "A diagnostic set examines and categorizes reasoning rigidity in large language models, identifying patterns where models ignore instructions and default to familiar reasoning.",
      "ai_keywords": [
        "reasoning rigidity",
        "large language models",
        "long and complex reasoning tasks",
        "reasoning trajectories",
        "diagnostic set",
        "AIME",
        "MATH500",
        "Interpretation Overload",
        "Input Distrust",
        "Partial Instruction Attention"
      ]
    },
    "publishedAt": "2025-05-22T15:00:01.000Z",
    "title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in\n  Reasoning Models",
    "summary": "Large language models have demonstrated remarkable proficiency in long and\ncomplex reasoning tasks. However, they frequently exhibit a problematic\nreliance on familiar reasoning patterns, a phenomenon we term reasoning\nrigidity. Despite explicit instructions from users, these models often\noverride clearly stated conditions and default to habitual reasoning\ntrajectories, leading to incorrect conclusions. This behavior presents\nsignificant challenges, particularly in domains such as mathematics and logic\npuzzle, where precise adherence to specified constraints is critical. To\nsystematically investigate reasoning rigidity, a behavior largely unexplored in\nprior work, we introduce a expert-curated diagnostic set, . Our\ndataset includes specially modified variants of existing mathematical\nbenchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately\nredesigned to require deviation from familiar reasoning strategies. Using this\ndataset, we identify recurring contamination patterns that occur when models\ndefault to ingrained reasoning. Specifically, we categorize this contamination\ninto three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,\nand (iii) Partial Instruction Attention, each causing models to ignore or\ndistort provided instructions. We publicly release our diagnostic set to\nfacilitate future research on mitigating reasoning rigidity in language models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b15ce1a5dd7dc7024406dc",
      "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
      "fullname": "Yoonjeon Kim",
      "name": "yjyjyj98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17941",
      "authors": [
        {
          "_id": "6833cc35015eb19058ed83d9",
          "user": {
            "_id": "65811eeaa2284a018e51f1ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
            "isPro": true,
            "fullname": "Zigeng Chen",
            "user": "Zigeng",
            "type": "user"
          },
          "name": "Zigeng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:12.285Z",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83da",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83db",
          "name": "Gongfan Fang",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83dc",
          "name": "Ruonan Yu",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83dd",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T14:17:56.000Z",
      "submittedOnDailyAt": "2025-05-26T00:36:09.618Z",
      "title": "VeriThinker: 검증 학습에 의한 추론 모델의 효율화",
      "submittedOnDailyBy": {
        "_id": "65811eeaa2284a018e51f1ba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
        "isPro": true,
        "fullname": "Zigeng Chen",
        "user": "Zigeng",
        "type": "user"
      },
      "summary": "대논리 모델(LRMs)는 Chain-of-Thought(CoT) 논리를 사용하여 복잡한 태스크를 뛰어넘습니다. 그러나 과도한 생각방식으로 인해 불필요하게 긴 논리연쇄가 발생하여 추론 비용이 크게 증가합니다. 이러한 문제를 완화하기 위해 우리는 VeriThinker라는 새로운 접근 방식을 소개합니다. 이것은 합성적인 간결한 CoT 데이터로 LRMs를 직접 조정하는 전통적인 방법과 달리, 보조적인 증명 태스크만 사용하여 모델을 조정합니다. LRMs를 CoT 해결책의 정확성을 정확히 증명함으로써 LRMs는 후속의 자각반성 단계의 필요성을 더 명확하게 판단하며 과도한 생각방식의 억제에 성공합니다. 확장된 실험은 VeriThinker가 논리연쇄의 길이를 크게 줄이고 정확도를 유지하면서 약간 증가시키는 것을 증명했습니다. DeepSeek-R1-Distill-Qwen-7B에 적용된 경우, MATH500에서는 이유 토큰이 3790에서 2125로 감소하여 정확도가 0.8% 상승(94.0%→94.8%)하고, AIME25에서 토큰이 14321에서 10287로 감소하여 정확도가 2.1% 상승(38.7%→40.8%)했습니다. 또한 실험은 VeriThinker가 추측논리에서도 0-shot 일반화 가능한 것을 보여주었습니다. 코드는 https://github.com/czg1225/VeriThinker에서 사용 가능합니다.",
      "upvotes": 20,
      "discussionId": "6833cc36015eb19058ed8419",
      "githubRepo": "https://github.com/czg1225/VeriThinker",
      "ai_summary": "VeriThinker reduces the length of complex reasoning chains in Large Reasoning Models (LRMs) by fine-tuning them on a verification task, thereby decreasing inference costs without significantly sacrificing accuracy.",
      "ai_keywords": [
        "Large Reasoning Models (LRMs)",
        "Chain-of-Thought (CoT) reasoning",
        "CoT compression",
        "verification task",
        "reasoning chain lengths",
        "reasoning tokens",
        "accuracy",
        "DeepSeek-R1-Distill-Qwen-7B",
        "MATH500",
        "AIME25",
        "speculative reasoning"
      ]
    },
    "publishedAt": "2025-05-23T10:17:56.000Z",
    "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
    "summary": "Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought\n(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily\nlengthy reasoning chains, dramatically increasing inference costs. To mitigate\nthis issue, we introduce VeriThinker, a novel approach for CoT compression.\nUnlike conventional methods that fine-tune LRMs directly on the original\nreasoning task using synthetic concise CoT data, we innovatively fine-tune the\nmodel solely through an auxiliary verification task. By training LRMs to\naccurately verify the correctness of CoT solutions, the LRMs inherently become\nmore discerning about the necessity of subsequent self-reflection steps,\nthereby effectively suppressing overthinking. Extensive experiments validate\nthat VeriThinker substantially reduces reasoning chain lengths while\nmaintaining or even slightly improving accuracy. When applied to\nDeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500\nfrom 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on\nAIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to\n40.8%). Additionally, our experiments demonstrate that VeriThinker can also be\nzero-shot generalized to speculative reasoning. Code is available at\nhttps://github.com/czg1225/VeriThinker",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65811eeaa2284a018e51f1ba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
      "fullname": "Zigeng Chen",
      "name": "Zigeng",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17561",
      "authors": [
        {
          "_id": "6833cb9030cd9df52a117557",
          "name": "Kwanyoung Kim",
          "hidden": false
        },
        {
          "_id": "6833cb9030cd9df52a117558",
          "name": "Sanghyun Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T07:09:10.000Z",
      "submittedOnDailyAt": "2025-05-26T00:33:24.403Z",
      "title": "모델은 이미 최적의 노이즈를 알고 있습니다: 비디오 디피루션 모델에서 attention을 통해 베이즈 활성 노이즈 선택",
      "submittedOnDailyBy": {
        "_id": "63973ee44e7b4959dc98028f",
        "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
        "isPro": false,
        "fullname": "Kwanyoung",
        "user": "kwanyoung",
        "type": "user"
      },
      "summary": "초기 노이즈의 선택은 비디오 디퓨저 모델의 품질과 프로ン퓰트의 어레이먼트에 중대한 영향을 미칩니다. 같은 프로ン퓰트에 대해 다른 노이즈 시드가 사용되면, 생성되는 비디오가 크게 달라질 수 있습니다. 최근의 방법은 주변 설계된 선두(예: 주파수 필터, 인접한 프레임의 평활화)을 의존하지만, 모델 내부의 신호를 밖으로 뜯어내지 않습니다. 이를 해결하기 위해, ANSE(Generation을 위한 능동 노이즈 선택)를 제안합니다. ANSE는 어텐션 기반의 불확실성을 정량화하여 고품질의 노이즈 시드를 선택하는 모델에 의존하는 프레임워크입니다. 그 핵심은 BANSA(Bayesian Active Noise Selection via Attention)입니다. BANSA는 여러 스토르스 어텐션 샘플의 엔트로피 불일치를 측정하여 모델의 신뢰성과 일관성을 추정합니다. 추론 시 효율적인 처리를 위해, BANSA의 베르누이 정규스케일 마스크 근사화를 도입하고, 1 단계의 디퓨저와 일부 어텐션 레이어를 사용하여 점수 추정을 수행할 수 있습니다. CogVideoX-2B와 5B의 실험에서, ANSE는 추론 시간 8%와 13%의 증가로 비디오의 품질과 시간적 일관성을 향상시켰으며, 비디오 디퓨저에서 노이즈 선택에 대한 원리적이고 일반화 가능한 접근을 제공합니다. 프로젝트 페이지를 참조하세요: https://anse-project.github.io/anse-project/",
      "upvotes": 18,
      "discussionId": "6833cb9430cd9df52a11765d",
      "projectPage": "https://anse-project.github.io/anse-project/",
      "ai_summary": "ANSE enhances video diffusion models by selecting noise seeds based on model confidence, improving video quality and temporal coherence with minimal increase in inference time.",
      "ai_keywords": [
        "video diffusion models",
        "noise seeds",
        "prompt alignment",
        "external priors",
        "frequency filters",
        "inter-frame smoothing",
        "ANSE",
        "Active Noise Selection for Generation",
        "BANSA",
        "Bayesian Active Noise Selection via Attention",
        "acquisition function",
        "entropy disagreement",
        "stochastic attention samples",
        "score estimation",
        "diffusion step",
        "temporal coherence"
      ]
    },
    "publishedAt": "2025-05-23T03:09:10.000Z",
    "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via\n  Attention in Video Diffusion Model",
    "summary": "The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17561.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63973ee44e7b4959dc98028f",
      "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
      "fullname": "Kwanyoung",
      "name": "kwanyoung",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17873",
      "authors": [
        {
          "_id": "68341f661d53989a8ecb685d",
          "user": {
            "_id": "6684b284dc7b0ae2cc67660c",
            "avatarUrl": "/avatars/54b3c0c4d808f293d78085d4d504570a.svg",
            "isPro": false,
            "fullname": "liuwanhao",
            "user": "wanhaoliu",
            "type": "user"
          },
          "name": "Wanhao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:54.880Z",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb685e",
          "user": {
            "_id": "646a11791556443f24b582e9",
            "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
            "isPro": false,
            "fullname": "Zonglin Yang",
            "user": "ZonglinY",
            "type": "user"
          },
          "name": "Zonglin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:08:35.046Z",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb685f",
          "name": "Jue Wang",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6860",
          "name": "Lidong Bing",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6861",
          "user": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:40:05.397Z",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6862",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6863",
          "name": "Yuqiang Li",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6864",
          "name": "Houqiang Li",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6865",
          "name": "Erik Cambria",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6866",
          "name": "Wanli Ouyang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T13:24:50.000Z",
      "submittedOnDailyAt": "2025-05-26T06:33:21.775Z",
      "title": "MOOSE-Chem3: 실험에 대한 피드백을 통해 가설의 순위를 정하는 방법론\n\n(Note: The translation provided is a direct translation of the given text. If \"MOOSE-Chem3\" is a specific term or acronym, it is kept as is in the translation.)",
      "submittedOnDailyBy": {
        "_id": "646a11791556443f24b582e9",
        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
        "isPro": false,
        "fullname": "Zonglin Yang",
        "user": "ZonglinY",
        "type": "user"
      },
      "summary": "假説順位付け은 자동화 과학발견의 중요한 구성 요소 중 하나이며, 특히 습식실험이 비용 높고 실험 형식에 제한이 많은 자연과학 분야에서 특히 중요합니다. 현재의 접근 방식은 실험 전의 순위付け을 중점으로 하고, 대 언어 모델의 내부적인 이유에만 의존하며, 실험의 실제 결과를 포함하지 않습니다. 우리는 실험을 지도한 순위付け의 임무를 소개하고, 미리 측정된 결과를 기반으로 후보의 가설을 우선순위付け하는 것을 목표로 합니다. 그러나 이러한 전략의 개발은 자연과학 분야에서 실제 실험을 반복적으로 수행하는 비효율성으로 인해 어려워 합니다. 이에 대처하여, 우리는 3개 영역 정보를 기반으로 한 가상기구를 제안하고, 가설의 성능을 이미 알고 있는 사실 가설과의 유사성에 의해 결정하고, 노이즈에 의해 피크되는 것을 모델링합니다. 124개의 화학 가설의 실험보고를 바탕으로 데이터셋을 구축하고, 이 가상기구를 통해 실험을 지도한 순위付け의 가짜적인 방법을 개발합니다. 기능적인 특징을 공유하는 가설을 클러스터링하고, 시뮬레이션된 실험의 피드백으로부터 얻은 통찰에 기반하여 후보를 우선순위付け합니다. 실험은 우리의 방법이 실험 전의 기준과 강력한 제한에 뛰어넘는 것을 보여줍니다.",
      "upvotes": 14,
      "discussionId": "68341f671d53989a8ecb68b8",
      "ai_summary": "A novel simulator and experiment-guided ranking method improve hypothesis prioritization in scientific discovery by incorporating simulated experimental outcomes.",
      "ai_keywords": [
        "hypothesis ranking",
        "automated scientific discovery",
        "natural sciences",
        "wet-lab experiments",
        "large language model",
        "pre-experiment ranking",
        "experiment-guided ranking",
        "hypothesis performance",
        "similarity",
        "noise",
        "dataset",
        "pseudo experiment-guided ranking",
        "clustering",
        "functional characteristics",
        "simulated experimental feedback"
      ]
    },
    "publishedAt": "2025-05-23T09:24:50.000Z",
    "title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated\n  Experimental Feedback",
    "summary": "Hypothesis ranking is a crucial component of automated scientific discovery,\nparticularly in natural sciences where wet-lab experiments are costly and\nthroughput-limited. Existing approaches focus on pre-experiment ranking,\nrelying solely on large language model's internal reasoning without\nincorporating empirical outcomes from experiments. We introduce the task of\nexperiment-guided ranking, which aims to prioritize candidate hypotheses based\non the results of previously tested ones. However, developing such strategies\nis challenging due to the impracticality of repeatedly conducting real\nexperiments in natural science domains. To address this, we propose a simulator\ngrounded in three domain-informed assumptions, modeling hypothesis performance\nas a function of similarity to a known ground truth hypothesis, perturbed by\nnoise. We curate a dataset of 124 chemistry hypotheses with experimentally\nreported outcomes to validate the simulator. Building on this simulator, we\ndevelop a pseudo experiment-guided ranking method that clusters hypotheses by\nshared functional characteristics and prioritizes candidates based on insights\nderived from simulated experimental feedback. Experiments show that our method\noutperforms pre-experiment baselines and strong ablations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17873.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "646a11791556443f24b582e9",
      "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
      "fullname": "Zonglin Yang",
      "name": "ZonglinY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16211",
      "authors": [
        {
          "_id": "6833d9cfdf7cbb5c087cb9cd",
          "name": "Kai Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ce",
          "name": "Can Shen",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9cf",
          "name": "Yile Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d0",
          "name": "Jirui Han",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d1",
          "name": "Kelong Zheng",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d2",
          "name": "Xuechao Zou",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d3",
          "name": "Zhe Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d4",
          "name": "Xingjian Du",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d5",
          "name": "Shun Zhang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d6",
          "name": "Hanjun Luo",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d7",
          "name": "Yingbin Jin",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d8",
          "name": "Xinxin Xing",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d9",
          "name": "Ziyang Ma",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9da",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9db",
          "user": {
            "_id": "64c6627d5671d42e0adfad56",
            "avatarUrl": "/avatars/8b98054b2911b86dcc4856a15306e60f.svg",
            "isPro": false,
            "fullname": "jiaxiaojunQAQ",
            "user": "jiaxiaojunQAQ",
            "type": "user"
          },
          "name": "Xiaojun Jia",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:37.847Z",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9dc",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9dd",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9de",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9df",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e0",
          "name": "Haoyang Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e1",
          "name": "Yiming Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e2",
          "name": "Xiaobin Zhuang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e3",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e4",
          "name": "Haibo Hu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e5",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e6",
          "name": "Zhizheng Wu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e7",
          "name": "Xiaolin Hu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e8",
          "name": "Eng-Siong Chng",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e9",
          "name": "XiaoFeng Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ea",
          "name": "Wenyuan Xu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9eb",
          "name": "Wei Dong",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ec",
          "name": "Xinfeng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T04:27:46.000Z",
      "submittedOnDailyAt": "2025-05-26T01:33:43.107Z",
      "title": "AudioTrust: 음성 대 언어 모델의 다면성 신뢰도 벤치마크",
      "submittedOnDailyBy": {
        "_id": "6387676c23da90491eb9fb16",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
        "isPro": false,
        "fullname": "Kai Li",
        "user": "JusperLee",
        "type": "user"
      },
      "summary": "오디오 라ー징 대 언어 모델(ALLMs)의 급격한 발전과 확장된 응용 분야에 따라, 그 신뢰성의 이해가 엄격히 요구되어 있습니다. 그러나 이러한 모델의 평가에 관한 체계적인 연구, 특히 오디오 모델에 특유의 위험에 대한 연구는 주로 탐색되지 않았습니다. 현재의 평가 프레임워크는 주로 텍스트 모델을 중심으로하거나, 한정된 안전성 차원을 다루며, 오디오 모델의 고유한 특성과 응용 시나리오에 적절한 대응하지 않습니다. 여기, 오디오 트러스트(AudioTrust)를 소개합니다. 이것은 처음으로 다면적인 신뢰성 평가 프레임워크와 벤치마크입니다. 이것은 ALLMs에 특별히 설계되었습니다. 오디오 트러스트는 공정성, 해킹, 안전성, 프라이버시, 로바스틱, 어우텐티피케이션의 6가지 핵심 차원의 평가를 촉진합니다. 이러한 차원을 자세히 평가하기 위해, 오디오 트러스트는 18가지 다른 실험 설정을 구성하고 있습니다. 그 핵심은 4,420 이상의 오디오/텍스트 샘플에서 구축된 섬세한 데이터 세트입니다. 이 데이터 세트는 일상의 대화, 긴급 호출, 음성 보조의 상호작용 등 현실적인 시나리오를 추출하여 있으며, ALLMs의 다면적인 신뢰성을 조사하기 위해 특별히 설계되었습니다. 평가의ため, 벤치마크는 9가지 고유의 오디오 평가 지표를 엄밀히 설정하고, 대규모의 자동 프로세스를 사용하여 모델의 출력을 객관적이고 스케일러블한 점수를 제공합니다. 실험 결과를 통해, 현재의 가장 선진한 오픈 소스 및 클로즈드 소스의 ALLMs가, 다양한 고위험의 오디오 시나리오에 직면할 때의 신뢰성의 경계와 한계를 명확히 하고, 미래의 오디오 모델의 안전한 신뢰성 도입에 있어 유익한 엔드 투 엔드의 프라이버시를 제공합니다. 플랫폼과 벤치마크는 https://github.com/JusperLee/AudioTrust에서 사용 가능합니다.",
      "upvotes": 14,
      "discussionId": "6833d9d1df7cbb5c087cba85",
      "githubRepo": "https://github.com/JusperLee/AudioTrust",
      "ai_summary": "AudioTrust evaluates the trustworthiness of Audio Large Language Models across multifaceted dimensions, using a comprehensive dataset and specific metrics to assess their performance in real-world audio scenarios.",
      "ai_keywords": [
        "Audio Large Language Models",
        "ALLMs",
        "trustworthiness",
        "fairness",
        "hallucination",
        "safety",
        "privacy",
        "robustness",
        "authentication",
        "AudioTrust",
        "experimental setups",
        "audio-specific evaluation metrics",
        "automated pipeline"
      ]
    },
    "publishedAt": "2025-05-22T00:27:46.000Z",
    "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models",
    "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16211.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6387676c23da90491eb9fb16",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
      "fullname": "Kai Li",
      "name": "JusperLee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17618",
      "authors": [
        {
          "_id": "6833eeaf98515618764fc204",
          "user": {
            "_id": "6672937ceac0fb1b9e516595",
            "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg",
            "isPro": false,
            "fullname": "haoran he",
            "user": "haoranhe",
            "type": "user"
          },
          "name": "Haoran He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:19.593Z",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc205",
          "name": "Jiajun Liang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc206",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc207",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc208",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc209",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc20a",
          "name": "Ling Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T08:25:46.000Z",
      "submittedOnDailyAt": "2025-05-26T04:32:07.257Z",
      "title": "进化 계산을 이용한 테스트 시간 동안의 이미지와 비디오의 생성 스케일링",
      "submittedOnDailyBy": {
        "_id": "667187ba9ab144eb3ac43a1b",
        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
        "isPro": false,
        "fullname": "Runze Liu",
        "user": "RyanLiu112",
        "type": "user"
      },
      "summary": "모델 예약 훈련 시 확장 계산량(데이터와 파라미터)의 비용이 크게 증가하는 가운데, 추론 시의 확장 계산(TTS)은 추론 시 추가 계산량을 할당하여 생성 모델의 성능을 향상시키는 잠재적인 방향로 등장했습니다. TTS는 여러 언어 태스크에서 상당한 성공을 보였지만, 이미지나 영상 생성 모델(확산 기반이나 플로우 기반 모델)의 검증 시의 확장 동작에 대한 이해가 늦어져 있습니다. 최근의 연구는 시각 태스크의 추론 시의 전략에 대해 시도하고 있지만, 이러한 접근法是 중요한 제한을 가지고 있으며, 태스크 고유 영역에 제한되어 확장성이 떨어지거나, 과도 최적화에 의해 샘플의 다양성을 잃습니다. 본 논문에서는, 새로운 일반적인 효율적인 TTS 방법인 \"진화 탐색(EvoSearch)\"를 제안합니다. 이 방법은 확산 모델이나 플로우 모델의 이미지나 영상 생성의 확장성을 효과적으로 향상시키고, 추가 훈련이나 모델 확장이 필요하지 않습니다. EvoSearch는 생물의 진화 원칙을 활용하여 확산 모델이나 플로우 모델의 검증 시의 확장을 진화 탐색 문제로 재설계하고, 일반적이며 효율적인 계산 경로를 탐색하고 개선합니다. 확산 방정식의 디지션 프로세스에 맞는 적절한 설계된 선택과 갑작스러운 변이 구조를 포함하여, EvoSearch는 다양한 population의 다양성을 유지하면서 고품질의 후손을 연속적으로 생성합니다. 이미지나 영상 생성 태스크의 확산 모델이나 플로우 모델의 광범위한 평가에서, 우리의 방법은 기존의 접근법을 경험적으로 초과하며, 높은 다양성을 갖으며, 새로운 평가 기준에 강한 일반화 성능을 나타냅니다. 본 프로젝트는 https://tinnerhrhe.github.io/evosearch에 접근할 수 있습니다.",
      "upvotes": 11,
      "discussionId": "6833eeb198515618764fc277",
      "projectPage": "https://tinnerhrhe.github.io/evosearch/",
      "githubRepo": "https://github.com/tinnerhrhe/EvoSearch-codes",
      "ai_summary": "EvoSearch, an evolutionary search method, enhances test-time scaling for diffusion and flow-based generative models, improving image and video generation quality, diversity, and generalizability.",
      "ai_keywords": [
        "test-time scaling",
        "TTS",
        "image generation",
        "video generation",
        "diffusion models",
        "flow-based models",
        "denoising trajectory",
        "stochastic differential equation",
        "selection",
        "mutation",
        "EvoSearch"
      ]
    },
    "publishedAt": "2025-05-23T04:25:46.000Z",
    "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
    "summary": "As the marginal cost of scaling computation (data and parameters) during\nmodel pre-training continues to increase substantially, test-time scaling (TTS)\nhas emerged as a promising direction for improving generative model performance\nby allocating additional computation at inference time. While TTS has\ndemonstrated significant success across multiple language tasks, there remains\na notable gap in understanding the test-time scaling behaviors of image and\nvideo generative models (diffusion-based or flow-based models). Although recent\nworks have initiated exploration into inference-time strategies for vision\ntasks, these approaches face critical limitations: being constrained to\ntask-specific domains, exhibiting poor scalability, or falling into reward\nover-optimization that sacrifices sample diversity. In this paper, we propose\nEvolutionary Search (EvoSearch), a novel, generalist, and\nefficient TTS method that effectively enhances the scalability of both image\nand video generation across diffusion and flow models, without requiring\nadditional training or model expansion. EvoSearch reformulates test-time\nscaling for diffusion and flow models as an evolutionary search problem,\nleveraging principles from biological evolution to efficiently explore and\nrefine the denoising trajectory. By incorporating carefully designed selection\nand mutation mechanisms tailored to the stochastic differential equation\ndenoising process, EvoSearch iteratively generates higher-quality offspring\nwhile preserving population diversity. Through extensive evaluation across both\ndiffusion and flow architectures for image and video generation tasks, we\ndemonstrate that our method consistently outperforms existing approaches,\nachieves higher diversity, and shows strong generalizability to unseen\nevaluation metrics. Our project is available at the website\nhttps://tinnerhrhe.github.io/evosearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667187ba9ab144eb3ac43a1b",
      "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
      "fullname": "Runze Liu",
      "name": "RyanLiu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15692",
      "authors": [
        {
          "_id": "68306ffdff038ca6400a153a",
          "user": {
            "_id": "6747de57f8cab58c22ec94a2",
            "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
            "isPro": false,
            "fullname": "Jinyang Wu",
            "user": "Jinyang23",
            "type": "user"
          },
          "name": "Jinyang Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:13:57.397Z",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153b",
          "user": {
            "_id": "667fdaee20ee9ac417c7708c",
            "avatarUrl": "/avatars/69dfba6ff392643af1dcfe8af0a42ae9.svg",
            "isPro": false,
            "fullname": "Chonghua Liao",
            "user": "ChonghuaLiao",
            "type": "user"
          },
          "name": "Chonghua Liao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:16:00.056Z",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153c",
          "name": "Mingkuan Feng",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153d",
          "name": "Shuai Zhang",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153e",
          "name": "Zhengqi Wen",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153f",
          "name": "Pengpeng Shao",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a1540",
          "name": "Huazhe Xu",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a1541",
          "name": "Jianhua Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T16:06:10.000Z",
      "submittedOnDailyAt": "2025-05-26T01:19:22.736Z",
      "title": "신드레라 정책 최적화: 외부 가이드와 내부 역량의 풍부한 연결",
      "submittedOnDailyBy": {
        "_id": "6747de57f8cab58c22ec94a2",
        "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
        "isPro": false,
        "fullname": "Jinyang Wu",
        "user": "Jinyang23",
        "type": "user"
      },
      "summary": "강화학습(RL)은 논리론 모델의 훈련에 효과적인 방법으로 등장했지만, 현재의 RL 접근 방식은 일반적으로 모델의 출력 분포를 보상 최대화의 경로로 편향시키고 외부 지식을 도입하지 않습니다. 이는 탐험 능력을 제한하고 기본 모델보다 논리론 능력을 제한적으로 발휘하는 것을 실현하고 있습니다. 이러한 제한을 해결하기 위해, 우리는 TAPO(Template Addition Policy Optimization)라는 새로운 프레임워크를 제안하고 있습니다. TAPO는 외부의 고수준의 가이드(「생각의 패턴」)을 통합하여 RL을 강화시키고 있습니다. 훈련 과정에서 구조화된 생각들을 적응적으로 통합함으로써, TAPO는 모델 내부의 탐험과 외부 가이드의 활용을 더 잘 균형을 이루고 있습니다. 확장된 실험에 따라, 우리의 접근 방식은 AIME에서 99%, AMC에서 41%, Minerva Math에서 17% 이상의 성능을 향상시키고, 이러한 고수준의 생각의 패턴은 500건의 샘플에서 추상화되었지만, 다양한 태스크와 모델에 광범위하게 일반화되고 있습니다. 이는 TAPO가 다양한 태스크와 도메인에서 광범위하게 적용 가능한 것을 보여줍니다. 우리의 진행 중인 분석에 따르면, 외부 가이드의 도입은 추론행위의 설명성 향상과 출력의 이해성을 향상시켜 강력한 논리론 모델을 만들 수 있다는 것을 명확히 알 수 있습니다.",
      "upvotes": 11,
      "discussionId": "68306ffeff038ca6400a1569",
      "ai_summary": "A novel RL framework, TAPO, integrates external guidance to enhance model performance and exploration compared to existing methods.",
      "ai_keywords": [
        "reinforcement learning",
        "TAPO",
        "Thought-Augmented Policy Optimization",
        "high-level guidance",
        "thought patterns",
        "model exploration",
        "AIME",
        "AMC",
        "Minerva Math",
        "reasoning models",
        "explainability",
        "output readability"
      ]
    },
    "publishedAt": "2025-05-21T12:06:10.000Z",
    "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities",
    "summary": "Reinforcement learning (RL) has emerged as an effective method for training\nreasoning models. However, existing RL approaches typically bias the model's\noutput distribution toward reward-maximizing paths without introducing external\nknowledge. This limits their exploration capacity and results in a narrower\nreasoning capability boundary compared to base models. To address this\nlimitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel\nframework that augments RL by incorporating external high-level guidance\n(\"thought patterns\"). By adaptively integrating structured thoughts during\ntraining, TAPO effectively balances model-internal exploration and external\nguidance exploitation. Extensive experiments show that our approach\nsignificantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva\nMath. Notably, these high-level thought patterns, abstracted from only 500\nprior samples, generalize effectively across various tasks and models. This\nhighlights TAPO's potential for broader applications across multiple tasks and\ndomains. Our further analysis reveals that introducing external guidance\nproduces powerful reasoning models with superior explainability of inference\nbehavior and enhanced output readability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15692.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6747de57f8cab58c22ec94a2",
      "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
      "fullname": "Jinyang Wu",
      "name": "Jinyang23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17399",
      "authors": [
        {
          "_id": "6833fd69fe87d9433d098068",
          "name": "Haoyu Sun",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d098069",
          "name": "Huichen Will Wang",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d09806a",
          "user": {
            "_id": "645b4819f9d4ec91fdd54852",
            "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
            "isPro": false,
            "fullname": "Kuvvi Gu",
            "user": "Kuvvi",
            "type": "user"
          },
          "name": "Jiawei Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:01.542Z",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d09806b",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d09806c",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T02:16:11.000Z",
      "submittedOnDailyAt": "2025-05-26T08:05:22.618Z",
      "title": "FullFront: フルフォーナント: 전전단학 워크플로우의 MLLM 벤치마크\n\n（注：「FullFront」는 그대로 영어로 유지되었습니다. 나머지 부분을 일본어로 번역되었습니다.）",
      "submittedOnDailyBy": {
        "_id": "645b4819f9d4ec91fdd54852",
        "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
        "isPro": false,
        "fullname": "Kuvvi Gu",
        "user": "Kuvvi",
        "type": "user"
      },
      "summary": "전면 엔지니어링은 엔지니어가 디자인을 개념화하고 코드로 번역하고 반복적으로 구현을 개선하는 복잡한 작업 흐름을 포함하는 것입니다. 최근의 벤치마크는 주로 시각적인 디자인을 코드로 변환하는 것을 중점으로 하지만, 우리는 전면 개발의 전체 프로세스를 평가하기 위한 벤치마크인 \"FullFront\"을 소개합니다. FullFront는 웹 페이지 디자인(개념화 단계), 웹 페이지 인식 QA(시각적인 조직과 요소의 이해), 웹 페이지 코드 생성(실행 단계)의 3가지 기본적인 태스크를 평가합니다. 현재의 벤치마크는 코드가 많아져서 스크레이핑된 웹 사이트나 단순화된 LLM 생성 햄헿 HTML을 사용하지만, FullFront는 실제적인 웹 페이지를 清晰한, 표준화된 HTML로 변환하기 위해 새로운 2단계 프로세스를 사용하며 다양한 시각적인 디자인을 유지하고 저작권 문제를 피하기 위해 개발되었습니다. 최신의 MLLM의 확장 실험에 따라, 페이지 인식, 코드 생성(특히 이미지 처리와 레이아웃), 상호작용 구현에 대한 상당한 한계가 관찰되었습니다. 우리의 결과를 통해 모델과 태스크의 성능 차이를定量적으로 보여주고, 현재의 MLLM의 능력과 전면 엔지니어의 전문가의 성능 사이의 큰 차이를 명확히 합니다. FullFront 벤치마크와 코드는 https://github.com/Mikivishy/FullFront에 접근할 수 있습니다.",
      "upvotes": 10,
      "discussionId": "6833fd6bfe87d9433d0980c2",
      "githubRepo": "https://github.com/Mikivishy/FullFront",
      "ai_summary": "FullFront is a benchmark evaluating Multimodal Large Language Models across conceptualization, comprehension, and implementation phases in front-end engineering.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "Webpage Design",
        "Webpage Perception QA",
        "Webpage Code Generation",
        "front-end engineering"
      ]
    },
    "publishedAt": "2025-05-22T22:16:11.000Z",
    "title": "FullFront: Benchmarking MLLMs Across the Full Front-End Engineering\n  Workflow",
    "summary": "Front-end engineering involves a complex workflow where engineers\nconceptualize designs, translate them into code, and iteratively refine the\nimplementation. While recent benchmarks primarily focus on converting visual\ndesigns to code, we present FullFront, a benchmark designed to evaluate\nMultimodal Large Language Models (MLLMs) across the full front-end\ndevelopment pipeline. FullFront assesses three fundamental tasks that map\ndirectly to the front-end engineering pipeline: Webpage Design\n(conceptualization phase), Webpage Perception QA (comprehension of visual\norganization and elements), and Webpage Code Generation (implementation phase).\nUnlike existing benchmarks that use either scraped websites with bloated code\nor oversimplified LLM-generated HTML, FullFront employs a novel, two-stage\nprocess to transform real-world webpages into clean, standardized HTML while\nmaintaining diverse visual designs and avoiding copyright issues. Extensive\ntesting of state-of-the-art MLLMs reveals significant limitations in page\nperception, code generation (particularly for image handling and layout), and\ninteraction implementation. Our results quantitatively demonstrate performance\ndisparities across models and tasks, and highlight a substantial gap between\ncurrent MLLM capabilities and human expert performance in front-end\nengineering. The FullFront benchmark and code are available in\nhttps://github.com/Mikivishy/FullFront.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17399.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b4819f9d4ec91fdd54852",
      "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
      "fullname": "Kuvvi Gu",
      "name": "Kuvvi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14669",
      "authors": [
        {
          "_id": "682da9d3781210358218a950",
          "name": "Roberto L. Castro",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a951",
          "name": "Andrei Panferov",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a952",
          "name": "Soroush Tabesh",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a953",
          "name": "Oliver Sieberling",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a954",
          "name": "Jiale Chen",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a955",
          "name": "Mahdi Nikdan",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a956",
          "name": "Saleh Ashkboos",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a957",
          "name": "Dan Alistarh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/N9QxR8-CzRd2UcOj9uwRR.png"
      ],
      "publishedAt": "2025-05-20T17:55:50.000Z",
      "submittedOnDailyAt": "2025-05-26T08:34:57.302Z",
      "title": "キャプテッド： Native FP4 Training is the most suitable for large-scale language models.",
      "submittedOnDailyBy": {
        "_id": "623753b5eddd7763adc9346a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
        "isPro": false,
        "fullname": "Andrei Panferov",
        "user": "BlackSamorez",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)의 급격한 발전은 계산 요구의 전례없는 증가와 함께 진행되고 있습니다. 가장 선진 모델의 훈련 비용은 몇 달 만에 두 배가 됩니다. 낮은 정밀도 계산을 직접 사용하여 모델을 훈련하는 것은 계산 속도와 에너지 효율을 모두 향상시키는 해결책으로 됩니다. 특히, NVIDIA의 최근의 Blackwell 아키텍처는 특히 FP4 버전의 매우 낮은 정밀도 연산을 촉진하고, 큰 효율 향상을 목표로 합니다. 그러나 현재의 LLMs의 FP4 정밀도 훈련에 사용된 알고리즘은 큰 문제가 되고, 일반적으로 혼합 정밀도의 후퇴를 의존합니다. 이 논문에서는 하드웨어 지원된 FP4 훈련을 체계적으로 조사하고, Quartet라는 새로운 접근법을 소개합니다. 이 접근법은 모든 주요 계산(예: 선형 레이어)이 낮은 정밀도로 수행되는, 정확한 끝에서부터의 FP4 훈련을 가능하게 합니다. Llama 타입의 모델에 대한 확장 평가를 통해 비트 폭의 변화에 따른 성능의 트레이드 오프를定量화하고, 정확도와 계산과의 「근사 최적」의 낮은 정밀도 훈련 방법을 특정한 새로운 스케일러를 밝혀줍니다. 이를 Quartet이라고 부르고, NVIDIA Blackwell GPU에 타일러링된 최적화된 CUDA 캐너를 사용하여 구현하고, FP4 정밀도로 가장 선진한 정확도를 달성하고, 빌리언스케일의 모델을 성공적으로 훈련할 수 있음을 보여줍니다. 우리 방법은 표준 정밀도와 FP8 훈련과 비교하여 경쟁적인 선택지이며, 완전한 FP4 기반의 훈련을 보여줍니다. 우리 코드는 https://github.com/IST-DASLab/Quartet에 공개되어 있습니다.",
      "upvotes": 10,
      "discussionId": "682da9d4781210358218a982",
      "ai_summary": "Quartet, a hardware-supported FP4 training approach for large language models, demonstrates state-of-the-art accuracy while significantly reducing computational costs compared to standard or FP8 precision.",
      "ai_keywords": [
        "large language models",
        "low-precision arithmetic",
        "Blackwell architecture",
        "FP4",
        "mixed-precision",
        "linear layers",
        "low-precision scaling law",
        "CUDA kernels"
      ]
    },
    "publishedAt": "2025-05-20T13:55:50.000Z",
    "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
    "summary": "The rapid advancement of large language models (LLMs) has been paralleled by\nunprecedented increases in computational demands, with training costs for\nstate-of-the-art models doubling every few months. Training models directly in\nlow-precision arithmetic offers a solution, by improving both computational\nthroughput and energy efficiency. Specifically, NVIDIA's recent Blackwell\narchitecture facilitates extremely low-precision operations, specifically FP4\nvariants, promising substantial efficiency gains. Yet, current algorithms for\ntraining LLMs in FP4 precision face significant accuracy degradation and often\nrely on mixed-precision fallbacks. In this paper, we systematically investigate\nhardware-supported FP4 training and introduce Quartet, a new approach enabling\naccurate, end-to-end FP4 training with all the major computations (in e.g.\nlinear layers) being performed in low precision. Through extensive evaluations\non Llama-type models, we reveal a new low-precision scaling law that quantifies\nperformance trade-offs across varying bit-widths and allows us to identify a\n\"near-optimal\" low-precision training technique in terms of\naccuracy-vs-computation, called Quartet. We implement Quartet using optimized\nCUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve\nstate-of-the-art accuracy for FP4 precision, successfully training\nbillion-scale models. Our method demonstrates that fully FP4-based training is\na competitive alternative to standard-precision and FP8 training. Our code is\navailable at https://github.com/IST-DASLab/Quartet.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/N9QxR8-CzRd2UcOj9uwRR.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623753b5eddd7763adc9346a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
      "fullname": "Andrei Panferov",
      "name": "BlackSamorez",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17558",
      "authors": [
        {
          "_id": "6833c8af029c4a53a60a5dfa",
          "user": {
            "_id": "648749094dea003c6dae810f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
            "isPro": false,
            "fullname": "Shrey Pandit",
            "user": "SP2001",
            "type": "user"
          },
          "name": "Shrey Pandit",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-26T01:49:36.568Z",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfb",
          "user": {
            "_id": "62fa7294363251ee40a41dba",
            "avatarUrl": "/avatars/869c6de9a1cb2ded690ae56559916cae.svg",
            "isPro": false,
            "fullname": "Ashwin V",
            "user": "ashwinnv",
            "type": "user"
          },
          "name": "Ashwin Vinod",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:22.347Z",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfc",
          "name": "Liu Leqi",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfd",
          "name": "Ying Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T07:05:09.000Z",
      "submittedOnDailyAt": "2025-05-26T00:20:35.511Z",
      "title": "리브러리로 가르침: 합성된 음의 데이터에 기반한 커리큘럼 DPO에서의 상상 검출",
      "submittedOnDailyBy": {
        "_id": "648749094dea003c6dae810f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
        "isPro": false,
        "fullname": "Shrey Pandit",
        "user": "SP2001",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)의 해우신 경보 감지 대응은 해우신 텍스트의 복잡한 특성에 따라 중대한 문제로 남아 있습니다. 해우신 샘플은 전통적인 부정 샘플보다 높은 가짜 품질을 나타내며, 이러한 잘 계획된 해우신은 DPO 대응 절차의 부정 샘플로 사용되고 있습니다. 우리 방법은 클레클 미드 학습 스텝을 도입하여, 독립된 사실 검증 모델로부터 얻은 확률 스코어의 최대 감소를 기반으로, 더 간단한 샘플에서부터 단계적으로 어려운 샘플로 적응됩니다. 이 구조화된 난이도 스케일링은 안정적이고 단계적인 학습을 보장합니다. 실험적 평가에 따르면, 우리 HaluCheck 모델은 클레클 DPO 접근법과 고품질의 부정 샘플을 사용하여 훈련되어 있으며, 다양한 메트릭에서 모델 성능을 크게 향상시켰으며, 예를 들어 MedHallu와 HaluEval의 어려운 벤치마크에서 약 24% 정도의 향상을 보였습니다. 또한, HaluCheck 모델은 0샷 설정에서 강건성을 보여주며, 다양한 벤치마크에서 큰 수준의 최신 모델을 크게 초월했습니다.",
      "upvotes": 9,
      "discussionId": "6833c8b0029c4a53a60a5e3a",
      "ai_summary": "The use of carefully crafted hallucinations in a curriculum learning approach within the DPO alignment procedure significantly enhances LLMs' hallucination detection abilities.",
      "ai_keywords": [
        "LLMs",
        "hallucinations",
        "DPO alignment procedure",
        "curriculum learning",
        "probability scores",
        "fact checking models",
        "HaluCheck models",
        "MedHallu",
        "HaluEval",
        "zero-shot settings"
      ]
    },
    "publishedAt": "2025-05-23T03:05:09.000Z",
    "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for\n  Hallucination Detection",
    "summary": "Aligning large language models (LLMs) to accurately detect hallucinations\nremains a significant challenge due to the sophisticated nature of hallucinated\ntext. Recognizing that hallucinated samples typically exhibit higher deceptive\nquality than traditional negative samples, we use these carefully engineered\nhallucinations as negative examples in the DPO alignment procedure. Our method\nincorporates a curriculum learning strategy, gradually transitioning the\ntraining from easier samples, identified based on the greatest reduction in\nprobability scores from independent fact checking models, to progressively\nharder ones. This structured difficulty scaling ensures stable and incremental\nlearning. Experimental evaluation demonstrates that our HaluCheck models,\ntrained with curriculum DPO approach and high quality negative samples,\nsignificantly improves model performance across various metrics, achieving\nimprovements of upto 24% on difficult benchmarks like MedHallu and HaluEval.\nAdditionally, HaluCheck models demonstrate robustness in zero-shot settings,\nsignificantly outperforming larger state-of-the-art models across various\nbenchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17558.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648749094dea003c6dae810f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
      "fullname": "Shrey Pandit",
      "name": "SP2001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16479",
      "authors": [
        {
          "_id": "682fdc63bf762029ddcad451",
          "user": {
            "_id": "6640c647acae6bb179eedff5",
            "avatarUrl": "/avatars/bcaafaaa1d4b4c241d72a886401772e3.svg",
            "isPro": false,
            "fullname": "Yuetong Liu",
            "user": "YuetongLiu",
            "type": "user"
          },
          "name": "Yuetong Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:16:39.979Z",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad452",
          "user": {
            "_id": "646c77911ee398a4e9404b8b",
            "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
            "isPro": false,
            "fullname": "Yunqiu Xu",
            "user": "Yunqiu",
            "type": "user"
          },
          "name": "Yunqiu Xu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T04:01:10.107Z",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad453",
          "name": "Yang Wei",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad454",
          "name": "Xiuli Bi",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad455",
          "name": "Bin Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T10:06:35.000Z",
      "submittedOnDailyAt": "2025-05-26T05:13:10.671Z",
      "title": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration\n\n이제부터는 맑은 밤이 다가오며, 다중 기상 조건을 고려한 야간 이미지 복원에 대한 연구를 진행합니다.",
      "submittedOnDailyBy": {
        "_id": "646c77911ee398a4e9404b8b",
        "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
        "isPro": false,
        "fullname": "Yunqiu Xu",
        "user": "Yunqiu",
        "type": "user"
      },
      "summary": "夜景画像의 악천候 효과로 인한 손상을 고안하는 연구 문제는 실용적이지만 조사가 부족합니다. 악천候 상태는 야간에는 많은 조명 효과를 동반하여 존재합니다. 본 연구에서는 악천候의 영향과 화염 효과를 섞인 야간 사진의 고안 작업에 대한 문제를 검토합니다. 또한 이 연구를 지원하기 위해, 모든 웨이저너의 데이터 세트를 제공하겠습니다. 이 데이터 세트는 고품질의 야간 사진 데이터와 다양한 구조적인 손상을 포함합니다. 이러한 사진은 우리가 제안한 조명에 대한 손상 생성에 의해 합성되었습니다. 또한, 우리가 ClearNight라는 통일된 야간 사진 고안 프레임워크를 소개하겠습니다. 이 프레임워크는 복잡한 손상을 한꺼번에 제거할 수 있습니다. 특히, ClearNight는 리티克斯 기반의 이중 선두를 추출하고 불균등한 조명 영역과 고유의 테크스처 내용을 중점으로, 야간의 경우 고효과로 고안합니다. 또한, 다양한 악천候의 공통점과 차이를 더 잘 표현하기 위해, 기후에 대한 동적인 특정-공통성 협조 방법을 제안하겠습니다. 이는 악천候의 손상을 식별하고 특정 기후에 관련된 최적의 단위를 선택할 수 있게 합니다. 우리 ClearNight는 합성 사진과 실현 사진에서 가장 선진적인 성능을 갖습니다. 자세한 제거 실험은, 모든 웨이저너 데이터 세트의 필요성과 ClearNight의 효과성을 증명합니다. 프로젝트 페이지는, https://henlyta.github.io/ClearNight/mainpage.html 입니다.",
      "upvotes": 9,
      "discussionId": "682fdc67bf762029ddcad58c",
      "projectPage": "https://henlyta.github.io/ClearNight/mainpage.html",
      "githubRepo": "https://github.com/henlyta/ClearNight",
      "ai_summary": "A unified framework for restoring nighttime images under diverse weather conditions using dual priors and adaptive collaboration.",
      "ai_keywords": [
        "Retinex-based dual priors",
        "illumination-aware degradation generation",
        "weather-aware dynamic specific-commonality collaboration",
        "nighttime image restoration"
      ]
    },
    "publishedAt": "2025-05-22T06:06:35.000Z",
    "title": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration",
    "summary": "Restoring nighttime images affected by multiple adverse weather conditions is\na practical yet under-explored research problem, as multiple weather conditions\noften coexist in the real world alongside various lighting effects at night.\nThis paper first explores the challenging multi-weather nighttime image\nrestoration task, where various types of weather degradations are intertwined\nwith flare effects. To support the research, we contribute the AllWeatherNight\ndataset, featuring large-scale high-quality nighttime images with diverse\ncompositional degradations, synthesized using our introduced illumination-aware\ndegradation generation. Moreover, we present ClearNight, a unified nighttime\nimage restoration framework, which effectively removes complex degradations in\none go. Specifically, ClearNight extracts Retinex-based dual priors and\nexplicitly guides the network to focus on uneven illumination regions and\nintrinsic texture contents respectively, thereby enhancing restoration\neffectiveness in nighttime scenarios. In order to better represent the common\nand unique characters of multiple weather degradations, we introduce a\nweather-aware dynamic specific-commonality collaboration method, which\nidentifies weather degradations and adaptively selects optimal candidate units\nassociated with specific weather types. Our ClearNight achieves\nstate-of-the-art performance on both synthetic and real-world images.\nComprehensive ablation experiments validate the necessity of AllWeatherNight\ndataset as well as the effectiveness of ClearNight. Project page:\nhttps://henlyta.github.io/ClearNight/mainpage.html",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16479.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646c77911ee398a4e9404b8b",
      "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
      "fullname": "Yunqiu Xu",
      "name": "Yunqiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16483",
      "authors": [
        {
          "_id": "6833cb27e10e89e250a6d9ae",
          "name": "Shuzheng Si",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9af",
          "user": {
            "_id": "63ff09a02098b9ad105a09f6",
            "avatarUrl": "/avatars/4409ca5d320050cf4c3df05962c7ff58.svg",
            "isPro": false,
            "fullname": "Hans Zhao",
            "user": "BleachNick",
            "type": "user"
          },
          "name": "Haozhe Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:14.872Z",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b0",
          "name": "Cheng Gao",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b1",
          "name": "Yuzhuo Bai",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b2",
          "name": "Zhitong Wang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b3",
          "name": "Bofei Gao",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b4",
          "name": "Kangyang Luo",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b5",
          "name": "Wenhao Li",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b6",
          "name": "Yufei Huang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b7",
          "name": "Gang Chen",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b8",
          "name": "Fanchao Qi",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b9",
          "name": "Minjia Zhang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9ba",
          "name": "Baobao Chang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9bb",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T10:10:07.000Z",
      "submittedOnDailyAt": "2025-05-26T00:36:30.930Z",
      "title": "합성 태스크와 강화 학습을 활용하여 대규모 언어 모델의 컨텍스트 의존성을 유지하는 방법",
      "submittedOnDailyBy": {
        "_id": "637c99bbfe115289cfedfb44",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
        "isPro": false,
        "fullname": "ssz",
        "user": "ssz1111",
        "type": "user"
      },
      "summary": "テコードーラージャングルモデル(LLMs)를 제공받는 맥락에서 정확한 응답을 제공하기 위해 신뢰할 수 있는 정보 탐색 시스템의 구축이 중요합니다. 따라서, 우리는 인간 Annotation을 제외한 짧은 및 긴 생성 태스크에서의 정확도를 향상시키기 위한 체계적인 프레임워크인 CANOE를 제안합니다. 구체적으로, 우리는 네 가지 다른 태스크를 포함하는 짧은 질문 대답 데이터를 합성하여 인간 Annotation을 제외한 고품질, 간단하게 검증할 수 있는 훈련 데이터를 구축합니다. 또한, 우리는 합성된 짧은 질문 대답 데이터로부터 얻은 세 가지의 튜닝 리즈룰 기반의 보상을 포함하는 튜닝 리즈룰 기반의 강화 학습 방법인 Dual-GRPO를 제안합니다. 이 방법은 짧은 및 긴 답변 생성을 동시에 최적화합니다. 특히, Dual-GRPO는 보상 모델 학습에 대한 매뉴얼 레이블 데이터의 직접적인 사용자 Annotation을 필요로 하지 않고, 합성된 짧은 질문 대답 데이터에만 의존하여 짧은 생성을 과도하게 최적화하지 않도록 합니다. 실험 결과를 통해 CANOE가 11가지의 다양한 다음 세대 태스크에서 LLMs의 정확도를 크게 향상시키고, 예를 들어 GPT-4o와 OpenAI o1의 가장 최신의 LLMs을 초과하는 것을 보여주었습니다.",
      "upvotes": 8,
      "discussionId": "6833cb28e10e89e250a6da0a",
      "projectPage": "https://github.com/S1s-Z/CANOE",
      "githubRepo": "https://github.com/S1s-Z/CANOE",
      "ai_summary": "CANOE improves LLM faithfulness in generation tasks using synthetic QA data and Dual-GRPO reinforcement learning without human annotations.",
      "ai_keywords": [
        "teaching large language models",
        "faithfulness",
        "context",
        "CANOE",
        "short-form generation",
        "long-form generation",
        "question-answering",
        "Dual-GRPO",
        "rule-based reinforcement learning",
        "preference data",
        "reward models"
      ]
    },
    "publishedAt": "2025-05-22T06:10:07.000Z",
    "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via\n  Synthetic Tasks and Reinforcement Learning",
    "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16483.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "637c99bbfe115289cfedfb44",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
      "fullname": "ssz",
      "name": "ssz1111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13508",
      "authors": [
        {
          "_id": "683148c0018bba5b656c94e3",
          "user": {
            "_id": "65d188a4aa309d842e438ef1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
            "isPro": false,
            "fullname": "Zijia Liu",
            "user": "m-serious",
            "type": "user"
          },
          "name": "Zijia Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:18.318Z",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e4",
          "name": "Peixuan Han",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e5",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e6",
          "name": "Haoru Li",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e7",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T13:46:28.000Z",
      "submittedOnDailyAt": "2025-05-26T06:47:47.579Z",
      "title": "시간 R1: 시간계열논리의 전방위적인 해결책에 대한 논의",
      "submittedOnDailyBy": {
        "_id": "65d188a4aa309d842e438ef1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
        "isPro": false,
        "fullname": "Zijia Liu",
        "user": "m-serious",
        "type": "user"
      },
      "summary": "대언어 모델(LLMs)는 놀라운 능력을 보여주지만, 강력한 시간 지식이 부족하고 과거의 이유를 생각하고 미래의 예측이나 적절하게 생성을 통합하는 것이 어려워진다. 동시에, 현재의 방법들은 일반적으로 분리된 시간 스킬을 목표로 하고 과거의 사실에 대한 질문에 대한 답변이나 기초적인 예측을 많이 포함하지만, 특히 지식의 전환보다 먼 사건이나 창의적인 전망이 필요할 때 일반화 능력이 좋지 않다. 이러한 한계를 극복하기 위해, 우리는 Time-R1을 소개합니다. Time-R1은 첫 번째 프레임워크이며, 중간 크기(3B 파라미터)의 LLM에 시간적인 능력(인식, 예측, 창의적인 생성)을 제공합니다. 우리의 접근 방식은 새로운 세 단계 개발 패스를 특징으로 하고, 처음 두 단계는 설계된 동적인 규칙 기반의 보상 시스템에 의한 강화학습(RL) 커리큘럼으로 구성되어 있습니다. 이 프레임워크는 (1) 역사 데이터에서 기초적인 시간 이해와 사건의 시간 관계 매핑, (2) 지식의 전환보다 먼 사건의 예측 능력, 최종적으로 (3) 창의적인 미래의 시나리오 생성의 우수한 일반화에 가능하게 합니다. 특히, 실험은 Time-R1이 200배 이상의 모델을 초월하며, 특히 가장 先端의 671B DeepSeek-R1을 포함하여 매우 어려운 미래의 사건 예측과 창의적인 시나리오 생성 벤치마크에서 상위를 차지합니다. 이 연구는 정밀한 엔지니어링과 발전적인 RL 미세 조정이 작은, 효율적인 모델이 우수한 시간 성능을 달성할 수 있음을 강력한 증거로 합니다. 또한, 이러한 연구를 진행하기 위해, 우리는 10년의 뉴스 데이터로부터의 대형 다 태스크 시간적 추론 데이터 세트인 Time-Bench를 공개하고, Time-R1의 체크포인트 시리즈를 제공합니다.",
      "upvotes": 8,
      "discussionId": "683148c1018bba5b656c9511",
      "githubRepo": "https://github.com/ulab-uiuc/Time-R1",
      "ai_summary": "A novel framework, Time-R1, enhances moderate-sized LLMs with comprehensive temporal abilities through a reinforcement learning curriculum, outperforming larger models on future event prediction and creative scenario generation benchmarks.",
      "ai_keywords": [
        "Large Language Models",
        "reinforcement learning",
        "RL curriculum",
        "rule-based reward system",
        "temporal understanding",
        "event-time mappings",
        "future event prediction",
        "creative scenario generation",
        "Time-Bench",
        "Time-R1 checkpoints"
      ]
    },
    "publishedAt": "2025-05-16T09:46:28.000Z",
    "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs",
    "summary": "Large Language Models (LLMs) demonstrate impressive capabilities but lack\nrobust temporal intelligence, struggling to integrate reasoning about the past\nwith predictions and plausible generations of the future. Meanwhile, existing\nmethods typically target isolated temporal skills, such as question answering\nabout past events or basic forecasting, and exhibit poor generalization,\nparticularly when dealing with events beyond their knowledge cutoff or\nrequiring creative foresight. To address these limitations, we introduce\nTime-R1, the first framework to endow a moderate-sized (3B-parameter)\nLLM with comprehensive temporal abilities: understanding, prediction, and\ncreative generation. Our approach features a novel three-stage development\npath; the first two constitute a reinforcement learning (RL)\ncurriculum driven by a meticulously designed dynamic rule-based reward system.\nThis framework progressively builds (1) foundational temporal understanding and\nlogical event-time mappings from historical data, (2) future event prediction\nskills for events beyond its knowledge cutoff, and finally (3) enables\nremarkable generalization to creative future scenario generation without any\nfine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms\nmodels over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,\non highly challenging future event prediction and creative scenario generation\nbenchmarks. This work provides strong evidence that thoughtfully engineered,\nprogressive RL fine-tuning allows smaller, efficient models to achieve superior\ntemporal performance, offering a practical and scalable path towards truly\ntime-aware AI. To foster further research, we also release Time-Bench,\na large-scale multi-task temporal reasoning dataset derived from 10 years of\nnews data, and our series of Time-R1 checkpoints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13508.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65d188a4aa309d842e438ef1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
      "fullname": "Zijia Liu",
      "name": "m-serious",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17417",
      "authors": [
        {
          "_id": "6833d2df73bebebe5cd6604e",
          "user": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "isPro": false,
            "fullname": "Alan Dao",
            "user": "alandao",
            "type": "user"
          },
          "name": "Alan Dao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-26T02:38:31.379Z",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd6604f",
          "name": "Dinh Bach Vu",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66050",
          "name": "Huy Hoang Ha",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66051",
          "name": "Tuan Le Duc Anh",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66052",
          "name": "Shreyas Gopal",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66053",
          "name": "Yue Heng Yeo",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66054",
          "name": "Warren Keng Hoong Low",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66055",
          "name": "Eng Siong Chng",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66056",
          "name": "Jia Qi Yip",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T03:05:47.000Z",
      "submittedOnDailyAt": "2025-05-26T01:03:18.385Z",
      "title": "무성언어 교실 훈련: 저자원 언어의 언어 교실 훈련에서 언어 없는 언어 지시 훈련",
      "submittedOnDailyBy": {
        "_id": "62d7b2339b629105a5d6888a",
        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
        "isPro": false,
        "fullname": "Alan Dao",
        "user": "alandao",
        "type": "user"
      },
      "summary": "대 언어 모뎀(LLM)를 가진 음성 보조 시스템의 급격한 성장은 이러한 시스템의 훈련에 필요한 음성 지시 데이터의 필요성을 명확히 보여주었습니다. 언어 인식 데이터의 풍부함에 반해, 언어 지시 데이터의 부족성이 특히 눈에 띄며, 언어를 이해하고 실행할 수 있는 모델 조정에 있어서 중요합니다. 고품질의 합성 언어를 생성하기 위해서는 좋은 문자를 음성으로 변환(TTS) 모뎀이 필요하지만, 자원이 적은 언어에는 이러한 모뎀이 보이지 않습니다. 우리 새로운 접근법은 이 문제를 해결하기 위해, 합성을 의미적 표현 수준에서 멈추고 TTS의 필요성을 회피하는 것입니다. 이를 실현하기 위해, 합성된 의미적 표현을 사전 학습된 Whisper 인코더와 어레이링하고, LLM은 문장의 지시에 의한 훈련을 수행할 수 있는 반면, 추론 시 언어 지시를 이해하는 능력을 유지할 수 있습니다. 이 단순화된 훈련 프로세스는 자원이 적은 언어의 음성 보조 시스템을 구축하기 위한 유망한 접근법이 됩니다.",
      "upvotes": 6,
      "discussionId": "6833d2df73bebebe5cd66074",
      "githubRepo": "https://github.com/menloresearch/ichigo",
      "ai_summary": "A method bypasses the need for TTS models by aligning semantic representations with a Whisper encoder, enabling LLMs to understand both text and spoken instructions for low-resource languages.",
      "ai_keywords": [
        "large language models",
        "LLM",
        "speech instruction data",
        "TTS",
        "semantic representation",
        "Whisper encoder",
        "fine-tuning",
        "low-resource languages"
      ]
    },
    "publishedAt": "2025-05-22T23:05:47.000Z",
    "title": "Speechless: Speech Instruction Training Without Speech for Low Resource\n  Languages",
    "summary": "The rapid growth of voice assistants powered by large language models (LLM)\nhas highlighted a need for speech instruction data to train these systems.\nDespite the abundance of speech recognition data, there is a notable scarcity\nof speech instruction data, which is essential for fine-tuning models to\nunderstand and execute spoken commands. Generating high-quality synthetic\nspeech requires a good text-to-speech (TTS) model, which may not be available\nto low resource languages. Our novel approach addresses this challenge by\nhalting synthesis at the semantic representation level, bypassing the need for\nTTS. We achieve this by aligning synthetic semantic representations with the\npre-trained Whisper encoder, enabling an LLM to be fine-tuned on text\ninstructions while maintaining the ability to understand spoken instructions\nduring inference. This simplified training process is a promising approach to\nbuilding voice assistant for low-resource languages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d7b2339b629105a5d6888a",
      "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
      "fullname": "Alan Dao",
      "name": "alandao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16770",
      "authors": [
        {
          "_id": "68308e2697d9a81c8521bc6a",
          "user": {
            "_id": "62145614b670cb63a38075ba",
            "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
            "isPro": false,
            "fullname": "MenghaoGuo",
            "user": "MenghaoGuo",
            "type": "user"
          },
          "name": "Meng-Hao Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:57.462Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6b",
          "user": {
            "_id": "66b711f9512dac2ac08bc5e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b711f9512dac2ac08bc5e5/n2kSqNhg-TE56iN_V0xHm.png",
            "isPro": false,
            "fullname": "Xuanyu Chu",
            "user": "CXY07",
            "type": "user"
          },
          "name": "Xuanyu Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:48.661Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6c",
          "name": "Qianrui Yang",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6d",
          "user": {
            "_id": "6816bd8e0499f6c7c7b89601",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6816bd8e0499f6c7c7b89601/-dkIPxjOGbdwDZFxhkBMC.jpeg",
            "isPro": false,
            "fullname": "Zhe-Han Mo",
            "user": "Mo-ZheHan",
            "type": "user"
          },
          "name": "Zhe-Han Mo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:52.048Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6e",
          "name": "Yiqing Shen",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6f",
          "name": "Pei-lin Li",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc70",
          "name": "Xinjie Lin",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc71",
          "name": "Jinnian Zhang",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc72",
          "name": "Xin-Sheng Chen",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc73",
          "user": {
            "_id": "63b2efb5922f26a27e76381c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b2efb5922f26a27e76381c/zOQAt_xywiY8eTvvQOrmQ.png",
            "isPro": false,
            "fullname": "Yi Zhang",
            "user": "uyzhang",
            "type": "user"
          },
          "name": "Yi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:54.379Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc74",
          "name": "Kiyohiro Nakayama",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc75",
          "name": "Zhengyang Geng",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc76",
          "name": "Houwen Peng",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc77",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc78",
          "name": "Shi-Nin Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T15:11:57.000Z",
      "submittedOnDailyAt": "2025-05-26T06:55:20.321Z",
      "title": "RBench-V: 시각적 이유 모델의 기초적인 평가에 대한 다모달 출력",
      "submittedOnDailyBy": {
        "_id": "62145614b670cb63a38075ba",
        "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
        "isPro": false,
        "fullname": "MenghaoGuo",
        "user": "MenghaoGuo",
        "type": "user"
      },
      "summary": "노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트 인덱스\n\n노트",
      "upvotes": 6,
      "discussionId": "68308e2797d9a81c8521bca5",
      "ai_summary": "A benchmark called RBench-V evaluates multi-modal models' vision-indispensable reasoning through image manipulation and auxiliary line construction, demonstrating that current models struggle with multi-modal outputs.",
      "ai_keywords": [
        "multi-modal models",
        "omni-models",
        "GPT-4o",
        "Gemini",
        "o3",
        "multi-modal chain of thought",
        "M-CoT",
        "RBench-V",
        "image manipulation",
        "auxiliary lines"
      ]
    },
    "publishedAt": "2025-05-22T11:11:57.000Z",
    "title": "RBench-V: A Primary Assessment for Visual Reasoning Models with\n  Multi-modal Outputs",
    "summary": "The rapid advancement of native multi-modal models and omni-models,\nexemplified by GPT-4o, Gemini, and o3, with their capability to process and\ngenerate content across modalities such as text and images, marks a significant\nmilestone in the evolution of intelligence. Systematic evaluation of their\nmulti-modal output capabilities in visual thinking processes (also known as\nmulti-modal chain of thought, M-CoT) becomes critically important. However,\nexisting benchmarks for evaluating multi-modal models primarily focus on\nassessing multi-modal inputs and text-only reasoning while neglecting the\nimportance of reasoning through multi-modal outputs. In this paper, we present\na benchmark, dubbed RBench-V, designed to assess models' vision-indispensable\nreasoning abilities. To construct RBench-V, we carefully hand-pick 803\nquestions covering math, physics, counting, and games. Unlike previous\nbenchmarks that typically specify certain input modalities, RBench-V presents\nproblems centered on multi-modal outputs, which require image manipulation such\nas generating novel images and constructing auxiliary lines to support the\nreasoning process. We evaluate numerous open- and closed-source models on\nRBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the\nbest-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below\nthe human score of 82.3%, highlighting that current models struggle to leverage\nmulti-modal reasoning. Data and code are available at\nhttps://evalmodels.github.io/rbenchv",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16770.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62145614b670cb63a38075ba",
      "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
      "fullname": "MenghaoGuo",
      "name": "MenghaoGuo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15389",
      "authors": [
        {
          "_id": "682f518184a99219c4b3090c",
          "user": {
            "_id": "6540fbf9cb7fffd683942b43",
            "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
            "isPro": false,
            "fullname": "DongGeon Lee",
            "user": "oneonlee",
            "type": "user"
          },
          "name": "DongGeon Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:17:11.519Z",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090d",
          "name": "Joonwon Jang",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090e",
          "name": "Jihae Jeong",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090f",
          "name": "Hwanjo Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T11:26:40.000Z",
      "submittedOnDailyAt": "2025-05-26T00:35:59.051Z",
      "title": "비전-런그라우지 모델은 야생 상태에서 안전할까? 메모리 기반 벤치마크 스튜디오",
      "submittedOnDailyBy": {
        "_id": "6540fbf9cb7fffd683942b43",
        "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
        "isPro": false,
        "fullname": "DongGeon Lee",
        "user": "oneonlee",
        "type": "user"
      },
      "summary": "급속으로 배포되는 시각 언어 모델(VLMs)는 보안 위험을 확대하고 있지만 대부분의 평가는 인공 이미지에 의존합니다. 본 연구는 일반 사용자가 공유하는 meme 이미지에 직면한 VLMs의 안전성을 조사합니다. 이를 위해 우리는 MemeSafetyBench를 도입하였습니다. 이는 실제 meme 이미지와 유해, 무害 명령을 짝지어 50,430개의 예시로 구성된 기준 테스트입니다. 전체적인 안전 분류법과 LLM 기반의 명령 생성을 활용하여 여러 VLMs에 대해 단일 루프와 다중 루프 상호작용을 평가하였습니다. 우리는 실제 세계의 meme가 유해 출력에 어떻게 영향을 미칠지, 대화 맥락의 완화 효과, 그리고 모델 규모와 안전 지표 사이의 관계에 대해 연구하였습니다. 우리의 연구 결과는 VLMs이 memebased有害 명령에 대해 합성 또는 인쇄 이미지에 비해 더 쉽게 공격당할 수 있음을 나타냅니다. 텍스트 입력에만 대한 경우, meme는 유해 응답을 증가시키고 거부를 줄였습니다. 다중 루프 상호작용은 일부 완화를 제공하지만, 고위험은 여전히 존재합니다. 이러한 결과를 통해 생태학적으로 효과적인 평가와 보안 메커니즘의 강화가 필요함을 강조합니다.",
      "upvotes": 6,
      "discussionId": "682f518184a99219c4b30956",
      "ai_summary": "VLMs are more vulnerable to harmful meme-based prompts than to synthetic images, and while multi-turn interactions offer some protection, significant vulnerabilities remain.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "MemeSafetyBench",
        "safety taxonomy",
        "LLM-based instruction generation",
        "single and multi-turn interactions"
      ]
    },
    "publishedAt": "2025-05-21T07:26:40.000Z",
    "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
    "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs show greater vulnerability to\nmeme-based harmful prompts than to synthetic or typographic images. Memes\nsignificantly increase harmful responses and decrease refusals compared to\ntext-only inputs. Though multi-turn interactions provide partial mitigation,\nelevated vulnerability persists. These results highlight the need for\necologically valid evaluations and stronger safety mechanisms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15389.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6540fbf9cb7fffd683942b43",
      "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
      "fullname": "DongGeon Lee",
      "name": "oneonlee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17826",
      "authors": [
        {
          "_id": "6833ce1bd5c438959f750d57",
          "name": "Xuchen Pan",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d58",
          "name": "Yanxi Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d59",
          "name": "Yushuo Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5a",
          "name": "Yuchang Sun",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5b",
          "name": "Daoyuan Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5c",
          "name": "Wenhao Zhang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5d",
          "name": "Yuexiang Xie",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5e",
          "name": "Yilun Huang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5f",
          "name": "Yilei Zhang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d60",
          "name": "Dawei Gao",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d61",
          "name": "Yaliang Li",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d62",
          "name": "Bolin Ding",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d63",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6576f9f4654561a1b345610b/kDRshLvxO0EynfeH0BOZK.png"
      ],
      "publishedAt": "2025-05-23T12:41:09.000Z",
      "submittedOnDailyAt": "2025-05-26T00:50:50.296Z",
      "title": "トリニティー-RFT: 강화학습의 일반적인 프레임워크로서의 대규모 언어 모델의 미세 조정의 통일된 프레임워크",
      "submittedOnDailyBy": {
        "_id": "6576f9f4654561a1b345610b",
        "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
        "isPro": false,
        "fullname": "Yanxi Chen",
        "user": "yanxi-chen",
        "type": "user"
      },
      "summary": "トリニティー-RFT은 강화학습의 미세조정(RFT)에 적합한 일반적인, 유연하고 scalable한 프레임워크입니다. 이 프레임워크는 解結設計를 사용합니다. 구성 요소로는 다음과 같습니다. (1) RFT-core로, 동기/비동기, 온라인/오프라인, 온라인/오프라인의 RFT의 표준화와 일반화를 실현합니다. (2) 효율성과 강건성을 중시하는 에이전트-환경 상호작용의 무한한 통합. (3) RFT에 최적화된 시스템적인 데이터 파이프라인을 구성합니다.トリニティー-RFT은 다양한 애플리케이션 환경에 적용할 수 있으며, 先進的な 강화학습 패러다임의 탐색을 위해 통합된 플랫폼으로役立ちます. 이 기술보고서에서는,トリニティー-RFT의 관점, 기능, 설계 및 구현을 자세히 설명하고 다양한 예를 포함하여 제안된 프레임워크의 유용성과 사용자 친화성을 보여줍니다.",
      "upvotes": 5,
      "discussionId": "6833ce1cd5c438959f750dab",
      "projectPage": "https://github.com/modelscope/Trinity-RFT",
      "githubRepo": "https://github.com/modelscope/Trinity-RFT",
      "ai_summary": "Trinity-RFT is a flexible and scalable framework for reinforcement fine-tuning of large language models, supporting various interaction modes and data pipelines.",
      "ai_keywords": [
        "reinforcement fine-tuning",
        "RFT-core",
        "synchronous/asynchronous",
        "on-policy/off-policy",
        "online/offline",
        "agent-environment interaction",
        "data pipelines",
        "reinforcement learning paradigms"
      ]
    },
    "publishedAt": "2025-05-23T08:41:09.000Z",
    "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models",
    "summary": "Trinity-RFT is a general-purpose, flexible and scalable framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na decoupled design, consisting of (1) an RFT-core that unifies and generalizes\nsynchronous/asynchronous, on-policy/off-policy, and online/offline modes of\nRFT, (2) seamless integration for agent-environment interaction with high\nefficiency and robustness, and (3) systematic data pipelines optimized for RFT.\nTrinity-RFT can be easily adapted for diverse application scenarios, and serves\nas a unified platform for exploring advanced reinforcement learning paradigms.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples demonstrating the utility and\nuser-friendliness of the proposed framework.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6576f9f4654561a1b345610b/kDRshLvxO0EynfeH0BOZK.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17826.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576f9f4654561a1b345610b",
      "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
      "fullname": "Yanxi Chen",
      "name": "yanxi-chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17508",
      "authors": [
        {
          "_id": "6833cf5a2d728e2330d572e3",
          "user": {
            "_id": "647bf082aba7062fe5c51ca9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
            "isPro": false,
            "fullname": "Yifan Zhang",
            "user": "yifAI",
            "type": "user"
          },
          "name": "Yifan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:09.930Z",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e4",
          "user": {
            "_id": "653d276681f52ceb4d12bd85",
            "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
            "isPro": false,
            "fullname": "Yifeng Liu",
            "user": "Lewis-Lau",
            "type": "user"
          },
          "name": "Yifeng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:06.913Z",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e5",
          "name": "Huizhuo Yuan",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e6",
          "name": "Yang Yuan",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e7",
          "name": "Quanquan Gu",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e8",
          "name": "Andrew C Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/mGUuNpUhjfafWqcJJZ1V1.png"
      ],
      "publishedAt": "2025-05-23T06:01:21.000Z",
      "submittedOnDailyAt": "2025-05-26T00:50:14.655Z",
      "title": "KL-정규화 정책 제네트릭 경사 알고리즘의 설계 이유",
      "submittedOnDailyBy": {
        "_id": "647bf082aba7062fe5c51ca9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
        "isPro": false,
        "fullname": "Yifan Zhang",
        "user": "yifAI",
        "type": "user"
      },
      "summary": "ポリシーグラディエントアルゴリズム는 대규모 언어 모델(LLMs)의 논리 능력 향상을 위해 성공적으로 적용되고 있습니다. 그러나 KL 정규화는 ポリシーグラディエントアルゴリズム에 광범위하게 사용되고 있으며, 학습을 안정화하기 위해 있지만, 서로 다른 KL 데이터 행렬의 구성을 평가하고, 온라인 강화 학습(RL)을 위해 대리 손실 함수로 통합할 수 있는 것을 체계적으로 탐색하는 것은 복잡하고 체계적으로 탐색 가능한 설계 공간으로 변합니다. 본 논문에서는, KL 정규화된 ポリシーグラディエント 메소드의 일반화 및 분석을 위한 체계적인 프레임워크인 정규화 ポリシーグラディエント(RPG)를 제안합니다. 정규화 된 및 비정규화 된 ポリシー 분포를 모두 검토하고, 양방향과 음방향의 KL 데이터 행렬로 정규화된 목표에 대응하는 ポリシーグラディエント 및 상대적인 대리 손실 함수를 추출합니다. 또한, 모든 미분 가능한 손실 함수와 REINFORCE 스타일의 경사 추정기의 계산을 수행하고, 다양한 알고리즘의 필요를 충족시키는 것을 조정합니다. LLM의 논리에 대한 RL에서 이러한 방법을 사용하여 확장된 실험을 수행하고, 학습의 안정성과 성능에 있어 우수한 또는 경쟁적인 결과를 나타냅니다. 코드는 https://github.com/complex-reasoning/RPG에 접근할 수 있습니다.",
      "upvotes": 4,
      "discussionId": "6833cf5b2d728e2330d57313",
      "projectPage": "https://complex-reasoning.github.io/RPG",
      "githubRepo": "https://github.com/complex-reasoning/RPG",
      "ai_summary": "A regularized policy gradient framework is introduced to explore KL divergence formulations for enhancing the reasoning capabilities of LLMs in online reinforcement learning, demonstrating improved training stability and performance.",
      "ai_keywords": [
        "policy gradient algorithms",
        "KL regularization",
        "KL divergence",
        "surrogate loss functions",
        "online reinforcement learning",
        "full differentiable loss functions",
        "REINFORCE-style gradient estimators",
        "GRPO",
        "REINFORCE++",
        "DAPO",
        "regularized policy gradient (RPG)",
        "forward KL divergence",
        "reverse KL divergence",
        "normalized policy distributions",
        "unnormalized policy distributions"
      ]
    },
    "publishedAt": "2025-05-23T02:01:21.000Z",
    "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM\n  Reasoning",
    "summary": "Policy gradient algorithms have been successfully applied to enhance the\nreasoning capabilities of large language models (LLMs). Despite the widespread\nuse of Kullback-Leibler (KL) regularization in policy gradient algorithms to\nstabilize training, the systematic exploration of how different KL divergence\nformulations can be estimated and integrated into surrogate loss functions for\nonline reinforcement learning (RL) presents a nuanced and systematically\nexplorable design space. In this paper, we propose regularized policy gradient\n(RPG), a systematic framework for deriving and analyzing KL-regularized policy\ngradient methods in the online RL setting. We derive policy gradients and\ncorresponding surrogate loss functions for objectives regularized by both\nforward and reverse KL divergences, considering both normalized and\nunnormalized policy distributions. Furthermore, we present derivations for\nfully differentiable loss functions as well as REINFORCE-style gradient\nestimators, accommodating diverse algorithmic needs. We conduct extensive\nexperiments on RL for LLM reasoning using these methods, showing improved or\ncompetitive results in terms of training stability and performance compared to\nstrong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at\nhttps://github.com/complex-reasoning/RPG.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/mGUuNpUhjfafWqcJJZ1V1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647bf082aba7062fe5c51ca9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
      "fullname": "Yifan Zhang",
      "name": "yifAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17412",
      "authors": [
        {
          "_id": "6833e93697966d18e7c1e4d7",
          "name": "Shuang Wu",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4d8",
          "name": "Youtian Lin",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4d9",
          "name": "Feihu Zhang",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4da",
          "name": "Yifei Zeng",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4db",
          "name": "Yikang Yang",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4dc",
          "name": "Yajie Bao",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4dd",
          "name": "Jiachen Qian",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4de",
          "name": "Siyu Zhu",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4df",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4e0",
          "name": "Xun Cao",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4e1",
          "name": "Yao Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T02:58:01.000Z",
      "submittedOnDailyAt": "2025-05-26T03:12:06.518Z",
      "title": "Direct3D-S2: ギガスケール 3D ジェネレーション을 스펙트럴 스파르스 attention으로 쉽게 수행할 수 있습니다.",
      "submittedOnDailyBy": {
        "_id": "645a24779f06c5897254d14b",
        "avatarUrl": "/avatars/dd0a635674025dcc9a94ee0f4c952083.svg",
        "isPro": false,
        "fullname": "Youtian Lin",
        "user": "LoYoT",
        "type": "user"
      },
      "summary": "Direct3D S2는 3D 생성 프레임워크로, 3D Shapes를 생성하기 위해 Signed Distance Functions 같은 체적 표현을 사용하며, 고해상도 3D 형태를 생성하는 데는 계산량과 메모리의 큰 문제를 직면합니다. 우리는 큰 토큰 세트를 효과적으로 처리할 수 있는稀疏 volumes 내에서, 공간적稀疏 attention 구조를 도입하여 전파의 속도를 3.9배(전파), 9.6배(역전파)까지 향상시키고, 계산 오버헤드를 크게 줄였습니다. 또한, 입력, 잠재, 출력 스태이지에서 일관된稀疏 volumes 형식을 유지하는 분산 인코더를 포함하여, 3D VAE의 다른 표현을 사용하는 기존 방법과 비교하여 학습의 효율성과 안정성을 크게 향상시켰습니다. 우리의 모델은 공개 가능한 데이터셋을 사용하여 훈련되어 있으며, 실험은 Direct3D S2는 생성 품질과 효율에 대해 가장 선진한 방법보다 뛰어나게, 1024 해상도의 3D 생성을 8 커널로 실현할 수 있으며, 256 해상도의 체적 표현에서 일반적으로 32 커널 이상 필요했던 것을 실현할 수 있음을 보여주고, 3D 생성은 실용적이고 접근 가능하게 되었습니다. 프로젝트 페이지: https://nju3dv.github.io/projects/Direct3D-S2/",
      "upvotes": 4,
      "discussionId": "6833e93b97966d18e7c1e676",
      "projectPage": "https://nju-3dv.github.io/projects/Direct3D-S2/",
      "githubRepo": "https://github.com/DreamTechAI/Direct3D-S2",
      "ai_summary": "A scalable 3D shape generation framework using sparse volumes and spatial sparse attention, enabling high-resolution generation with reduced computational requirements.",
      "ai_keywords": [
        "Signed Distance Functions",
        "sparse volumes",
        "Spatial Sparse Attention",
        "Diffusion Transformer",
        "variational autoencoder",
        "gigascale 3D generation"
      ]
    },
    "publishedAt": "2025-05-22T22:58:01.000Z",
    "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse\n  Attention",
    "summary": "Generating high resolution 3D shapes using volumetric representations such as\nSigned Distance Functions presents substantial computational and memory\nchallenges. We introduce Direct3D S2, a scalable 3D generation framework based\non sparse volumes that achieves superior output quality with dramatically\nreduced training costs. Our key innovation is the Spatial Sparse Attention\nmechanism, which greatly enhances the efficiency of Diffusion Transformer\ncomputations on sparse volumetric data. SSA allows the model to effectively\nprocess large token sets within sparse volumes, significantly reducing\ncomputational overhead and achieving a 3.9x speedup in the forward pass and a\n9.6x speedup in the backward pass. Our framework also includes a variational\nautoencoder that maintains a consistent sparse volumetric format across input,\nlatent, and output stages. Compared to previous methods with heterogeneous\nrepresentations in 3D VAE, this unified design significantly improves training\nefficiency and stability. Our model is trained on public available datasets,\nand experiments demonstrate that Direct3D S2 not only surpasses\nstate-of-the-art methods in generation quality and efficiency, but also enables\ntraining at 1024 resolution using only 8 GPUs, a task typically requiring at\nleast 32 GPUs for volumetric representations at 256 resolution, thus making\ngigascale 3D generation both practical and accessible. Project page:\nhttps://nju3dv.github.io/projects/Direct3D-S2/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17412.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645a24779f06c5897254d14b",
      "avatarUrl": "/avatars/dd0a635674025dcc9a94ee0f4c952083.svg",
      "fullname": "Youtian Lin",
      "name": "LoYoT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17091",
      "authors": [
        {
          "_id": "6833cb25fe87d9433dfd2b1c",
          "name": "Prateek Verma",
          "hidden": false
        },
        {
          "_id": "6833cb25fe87d9433dfd2b1d",
          "name": "Mert Pilanci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T22:20:16.000Z",
      "submittedOnDailyAt": "2025-05-26T00:30:41.320Z",
      "title": "대 언어 모델은 읽기에 의해 자동으로 시각 및Auditory 능력 학습 중입니다.",
      "submittedOnDailyBy": {
        "_id": "62d7f1119b629105a5d84aad",
        "avatarUrl": "/avatars/c74045063e7c06cb7be0fa41ebb1d824.svg",
        "isPro": false,
        "fullname": "Prateek Verma",
        "user": "prateekv",
        "type": "user"
      },
      "summary": "이 논문에서는 흥미로운 발견을 보고합니다: 문 토큰에 대한 자동 복원형 LLM 모델을 훈련시켜 문 모델이 내적적으로 이미지와 소리를 이해하는 능력을 발전시키고, 읽기 및 시청 능력을 얻을 수 있습니다. 고급 음성 및 시각 LLM 모델은 이미지와 소리의 임베딩을 기반으로 문 출력을 제공하여 문 LLM 모델을 미세 조정하고 있습니다. 반면, 우리의 아키텍처는 이미지 패치, 소리 웨이브 포맷 또는 토큰을 입력으로 받아들입니다. 일반적인 분류 피리프라임과 같은 전형적인 임베딩이나 카테고리 레이블을 제공합니다. 우리는 문 가중치는 FSD-50K 및 GTZAN 데이터 세트의 음성 분류에서 효과적으로 사용될 수 있음을 보여주고 있습니다. 또한, CIFAR-10 및 Fashion-MNIST의 이미지 분류 및 이미지 패치에서도 동일한 효과를 보입니다. 이는 문 LLM이 학습하는 강력한 내부 회로를 활용하여 필요한 연결을 활성화하거나, 각 응용 프로그램에 새로운 모델을 만들 필요가 없도록 하는 것을 강조합니다.",
      "upvotes": 4,
      "discussionId": "6833cb26fe87d9433dfd2b64",
      "ai_summary": "Auto-regressive text LLMs trained on text can develop internal capabilities for understanding images and audio, enabling them to perform classification tasks across different modalities without fine-tuning.",
      "ai_keywords": [
        "auto-regressive",
        "LLM",
        "text tokens",
        "audio",
        "visual",
        "embeddings",
        "category labels",
        "classification",
        "FSD-50K",
        "GTZAN",
        "CIFAR-10",
        "Fashion-MNIST",
        "image patches"
      ]
    },
    "publishedAt": "2025-05-20T18:20:16.000Z",
    "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading",
    "summary": "This paper presents a fascinating find: By training an auto-regressive LLM\nmodel on text tokens, the text model inherently develops internally an ability\nto understand images and audio, thereby developing the ability to see and hear\njust by reading. Popular audio and visual LLM models fine-tune text LLM models\nto give text output conditioned on images and audio embeddings. On the other\nhand, our architecture takes in patches of images, audio waveforms or tokens as\ninput. It gives us the embeddings or category labels typical of a\nclassification pipeline. We show the generality of text weights in aiding audio\nclassification for datasets FSD-50K and GTZAN. Further, we show this working\nfor image classification on CIFAR-10 and Fashion-MNIST, as well on image\npatches. This pushes the notion of text-LLMs learning powerful internal\ncircuits that can be utilized by activating necessary connections for various\napplications rather than training models from scratch every single time.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17091.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62d7f1119b629105a5d84aad",
      "avatarUrl": "/avatars/c74045063e7c06cb7be0fa41ebb1d824.svg",
      "fullname": "Prateek Verma",
      "name": "prateekv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16270",
      "authors": [
        {
          "_id": "6833d08edf7cbb5c087a8bf1",
          "user": {
            "_id": "65c288280aa2d53135734a42",
            "avatarUrl": "/avatars/960422a1482ac8b4a52dd08c02d901f6.svg",
            "isPro": false,
            "fullname": "Jiaru Zou",
            "user": "jiaruz2",
            "type": "user"
          },
          "name": "Jiaru Zou",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-26T02:41:41.249Z",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf2",
          "name": "Yikun Ban",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf3",
          "name": "Zihao Li",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf4",
          "name": "Yunzhe Qi",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf5",
          "name": "Ruizhong Qiu",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf6",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf7",
          "name": "Jingrui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T06:00:45.000Z",
      "submittedOnDailyAt": "2025-05-26T00:53:39.701Z",
      "title": "Transformer Copilot: 학습 미세 조정의 미로 로그에서\n\n(Note: The original text was in Japanese, not English. The translation provided is from Japanese to Korean, maintaining the original meaning and context.)",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "대 언어 모델은 일반적으로 딥러닝 모델을 특정 딥러닝 덧셈에 적용하기 위해, 딥러닝 덧셈 데이터에 기반한 정규화된 미세 조정을 통해 도입됩니다. 표준적인 미세 조정은 생성 손실을 최소화하여 모델 파라미터를 최적화하는 데 중점을 두고 있지만, 우리는 모델의 학습 신호를 보존하고 이를 활용하는 데 노력을 기울입니다. 이는 인간 학습자가 과거의 오류를 반성하여 미래의 성과에 개선하는 것과 같은 개념으로 볼 수 있습니다.\n\n먼저, 모델의 학습 동작과 재현 오류를 체계적으로 기록하기 위한 \"MISTIE LOG\" 개념을 소개합니다. 원래의 transformer 기반 모델을 \"PREIO\"라고 부르며, 그에 대응하여 \"COPIO\" 모델을 설계하여 PREIO의 추론 성능을 로지ッ트 보정으로 개선합니다. \"Transformer COPIO\"라는 이름을 PREIO와 COPIO의 프레임워크 전체에 붙이고, (i) 새로운 COPIO 모델의 설계, (ii) COPIO가 PREIO와 함께 진화하는 MISTIE LOG에서 지속적으로 학습하는 JOINT TRAINING 패러다임, (iii) COPIO가 PREIO의 로지ッ트를 보정하여 생성을 향상시키는 FUNCTIONAL INFERENCE 패러다임을 도입합니다. 새로운 학습 프레임워크에 대한 이론적 및 실험적 분석을 제공합니다. 12개의 벤치마크（일반 지식, 산술, 추천 작업）에 대한 실험은 Transformer COPIO가 PREIO 모델에 비해 미세 조정으로 추가 계산량을 추가하며, 강력한 스케일러빌리티와 트랜스폼스빌리티를 보여주고, 평균 34.5% 정도의 성능 향상을 나타냅니다.",
      "upvotes": 3,
      "discussionId": "6833d08fdf7cbb5c087a8c29",
      "ai_summary": "The Transformer Copilot framework enhances large language model performance through a Copilot model that refines the Pilot's logits based on a Mistake Log, leading to consistent performance improvements across various benchmarks.",
      "ai_keywords": [
        "large language models",
        "supervised fine-tuning",
        "domain-specific data",
        "generation loss",
        "model parameters",
        "learning signals",
        "Mistake Log",
        "transformer-based model",
        "Copilot model",
        "logits rectification",
        "joint training paradigm",
        "fused inference paradigm",
        "performance improvements",
        "computational overhead",
        "scalability",
        "transferability"
      ]
    },
    "publishedAt": "2025-05-22T02:00:45.000Z",
    "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
    "summary": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17063",
      "authors": [
        {
          "_id": "6833e65bf9ae3819ea4c568e",
          "name": "Yiduo Guo",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c568f",
          "user": {
            "_id": "638e4e66629b4d0a62ce1bf3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
            "isPro": false,
            "fullname": "Zhen Guo",
            "user": "zguo0525",
            "type": "user"
          },
          "name": "Zhen Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:28.399Z",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5690",
          "name": "Chuanwei Huang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5691",
          "name": "Zi-Ang Wang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5692",
          "name": "Zekai Zhang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5693",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5694",
          "name": "Huishuai Zhang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5695",
          "name": "Yikang Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T05:35:13.000Z",
      "submittedOnDailyAt": "2025-05-26T02:26:50.238Z",
      "title": "Synthetic Data RL: Task Definition Is All You Need\n\n合成데이터 RL: 작업 정의는 모든 것을 필요로 한다.",
      "submittedOnDailyBy": {
        "_id": "638e4e66629b4d0a62ce1bf3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
        "isPro": false,
        "fullname": "Zhen Guo",
        "user": "zguo0525",
        "type": "user"
      },
      "summary": "강화학습(RL)은 기초 모델을 전문적인 태스크에 적용하는 강력한 방법 중 하나입니다が, 규모가 큰 인간 라벨 데이터의 의존성이 광범위하게 도입될 수 있습니다. 우리는 태스크 정의로부터 생성되는 합성 데이터만을 사용하여 RL로 모델을 조정하는 간단하고 일반적인 프레임워크인 \"Synthetic Data RL\"을 소개합니다. 우리의 방법은 태스크 정의와 검색된 기사로부터의 질문과 답변 쌍을 생성하고, 모델의 문제 해결 능력을 기반으로 질문의 난이도를 조정하며, 모델의 평균 합격률을 사용하여 RL 훈련용 질문을 선택합니다. Qwen-2.5-7B에서, 기초 모델에 대해 GSM8K에서 절대적으로 29.2%의 향상(실제 훈련 모델에 대해 +2.9pp, Self-Instruct에 대해 +6.6pp), MATH에서 8.7%, GPQA에서 +7.0pp(SynthLLM에 대해), MedQA에서 8.9%, CQA(법)에서 17.7%, CFA(금융)에서 13.7%의 향상을 달성했습니다. 동일한 데이터 버킷에서, 지도 학습보다 뛰어나며, 모든 인간 데이터를 사용한 RL에 근접한 성능을 달성했습니다(GSM8K에서 +17.2pp). 100건의 인간 демонстра션을 추가해도 GSM8K의 성능에 0.4pp의 향상만 보였습니다. 인간 데이터의 기록을 줄이고, 합성 데이터 RL은 스케일러블하고 효율적인 RL 기반의 모델 적용을 가능하게 합니다. 코드와 데모는 https://github.com/gydpku/Data_Synthesis_RL/ 에서 액세스할 수 있습니다.",
      "upvotes": 3,
      "discussionId": "6833e65cf9ae3819ea4c56c9",
      "projectPage": "https://github.com/gydpku/Data_Synthesis_RL",
      "githubRepo": "https://github.com/gydpku/Data_Synthesis_RL",
      "ai_summary": "Synthetic Data RL enhances foundation models through reinforcement learning using only synthetic data, achieving performance comparable to models trained with full human-labeled data.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "synthetic data",
        "reinforcement fine-tuning",
        "question and answer pairs",
        "model solvability",
        "average pass rate",
        "data budget",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-05-18T01:35:13.000Z",
    "title": "Synthetic Data RL: Task Definition Is All You Need",
    "summary": "Reinforcement learning (RL) is a powerful way to adapt foundation models to\nspecialized tasks, but its reliance on large-scale human-labeled data limits\nbroad adoption. We introduce Synthetic Data RL, a simple and general framework\nthat reinforcement fine-tunes models using only synthetic data generated from a\ntask definition. Our method first generates question and answer pairs from the\ntask definition and retrieved documents, then adapts the difficulty of the\nquestion based on model solvability, and selects questions using the average\npass rate of the model across samples for RL training. On Qwen-2.5-7B, our\nmethod achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9\npp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on\nGPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA\n(finance). It surpasses supervised fine-tuning under the same data budget and\nnearly matches RL with full human data across datasets (e.g., +17.2 pp on\nGSM8K). Adding 100 human demonstrations improves the performance of GSM8K only\nby 0.4 pp, showing a limited added value. By reducing human data annotation,\nSynthetic Data RL enables scalable and efficient RL-based model adaptation.\nCode and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17063.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638e4e66629b4d0a62ce1bf3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
      "fullname": "Zhen Guo",
      "name": "zguo0525",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17540",
      "authors": [
        {
          "_id": "683409de1869c47bd0c423a4",
          "name": "Mingrui Wu",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a5",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a6",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a7",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a8",
          "name": "Jianjin Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a9",
          "name": "Jianfeng Liu",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423aa",
          "name": "Yuefeng Zhan",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ab",
          "name": "Weihao Han",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ac",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ad",
          "name": "Jiayi Ji",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ae",
          "name": "Xiaoshuai Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423af",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b0",
          "name": "Weiwei Deng",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b1",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b2",
          "name": "Feng Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b3",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b4",
          "name": "Rongrong Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T06:44:26.000Z",
      "submittedOnDailyAt": "2025-05-26T04:59:40.648Z",
      "title": "이론에 근거한 재조정 프로ン푹트 실행에 의한 이미지 생성\n  강화 학습에 의한\n\n(Note: The original text seems to be a mix of Japanese and English, which is why the translation is also a mix of Korean and English. If you intended for the entire text to be in English, please clarify.)",
      "submittedOnDailyBy": {
        "_id": "6416d0b2058f65de43191027",
        "avatarUrl": "/avatars/2d99114e5cff39dccc385adfad7032c5.svg",
        "isPro": false,
        "fullname": "Mingrui Wu",
        "user": "mrwu",
        "type": "user"
      },
      "summary": "최근의 텍스트에서 이미지 생성(T2I)의 발전에 따라, 현재의 모델은 짧은 선물이나 결함이 있는Prompt으로부터 사용자의 의도를 정확히 이해하는 것이 어려워졌습니다. 선행 연구에서는 대규모 언어 모델(LLMs)을 사용하여 Prompt의 기능을 향상시키려 했지만, 이러한 방법들은 이미지의 의미와 현실적인 조합에 부족한 기초를 가지고 있으며, 스타일과 비현실적인 콘텐츠로도 많이 생성됩니다. 언어 모델의 추론에 대한 최근의 발전을 참고하여, 우리는 새로운 Prompt의 확장 프레임워크인 RePrompt를 제안합니다. 이 방법은 강화학습을 통해 Prompt의 확장 과정에서 명시적인 추론을 도입합니다. handcrafted 규칙이나 스타일적인 변경에 의존하지 않고, 우리의 방법은 이미지 수준의 결과를 최적화하기 위해 구조적이고 주관적인 Prompt를 생성하는 레이블 모델을 훈련합니다. 제작된 보상 모델은 생성된 이미지가 인간의 취향, 의미적 일치, 그리고 이미지의 조합에 대해 평가하여 Prompt의 생성을 보조적으로 개선합니다. 우리의 접근 방식은 사용자의 데이터를 사용하지 않고, 단말에서 훈련을 가능하게 합니다. GenEval과 T2I-Compbench에서의 실험은 RePrompt는 다양한 T2I 백엔드에서 공간적인 배치의 정확성과 조합의 일반화에서 크게 개선되었으며, 새로운 최尖端의 결과를 얻었습니다.",
      "upvotes": 2,
      "discussionId": "683409e21869c47bd0c4248e",
      "ai_summary": "RePrompt, a reprompting framework using reinforcement learning, enhances text-to-image generation by optimizing for image-level outcomes, significantly improving spatial layout and compositional generalization.",
      "ai_keywords": [
        "text-to-image",
        "T2I",
        "large language models",
        "LLMs",
        "reinforcement learning",
        "structured prompts",
        "self-reflective prompts",
        "reward models",
        "human preference",
        "semantic alignment",
        "visual composition",
        "GenEval",
        "T2I-Compbench",
        "spatial layout fidelity",
        "compositional generalization"
      ]
    },
    "publishedAt": "2025-05-23T02:44:26.000Z",
    "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation\n  via Reinforcement Learning",
    "summary": "Despite recent progress in text-to-image (T2I) generation, existing models\noften struggle to faithfully capture user intentions from short and\nunder-specified prompts. While prior work has attempted to enhance prompts\nusing large language models (LLMs), these methods frequently generate stylistic\nor unrealistic content due to insufficient grounding in visual semantics and\nreal-world composition. Inspired by recent advances in reasoning for language\nmodel, we propose RePrompt, a novel reprompting framework that introduces\nexplicit reasoning into the prompt enhancement process via reinforcement\nlearning. Instead of relying on handcrafted rules or stylistic rewrites, our\nmethod trains a language model to generate structured, self-reflective prompts\nby optimizing for image-level outcomes. The tailored reward models assesse the\ngenerated images in terms of human preference, semantic alignment, and visual\ncomposition, providing indirect supervision to refine prompt generation. Our\napproach enables end-to-end training without human-annotated data. Experiments\non GenEval and T2I-Compbench show that RePrompt significantly boosts spatial\nlayout fidelity and compositional generalization across diverse T2I backbones,\nestablishing new state-of-the-art results.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17540.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6416d0b2058f65de43191027",
      "avatarUrl": "/avatars/2d99114e5cff39dccc385adfad7032c5.svg",
      "fullname": "Mingrui Wu",
      "name": "mrwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17016",
      "authors": [
        {
          "_id": "6833f7847e0c637c71de0ec6",
          "user": {
            "_id": "64b64debeb9a833e08d079fd",
            "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
            "isPro": false,
            "fullname": "Shuhan Tan",
            "user": "tanshh97",
            "type": "user"
          },
          "name": "Shuhan Tan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:10.881Z",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec7",
          "name": "Kairan Dou",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec8",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec9",
          "name": "Philipp Krähenbühl",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:45.000Z",
      "submittedOnDailyAt": "2025-05-26T03:40:36.660Z",
      "title": "인teraktive 시각 언어 행동 모델의 후 학습",
      "submittedOnDailyBy": {
        "_id": "64b64debeb9a833e08d079fd",
        "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
        "isPro": false,
        "fullname": "Shuhan Tan",
        "user": "tanshh97",
        "type": "user"
      },
      "summary": "RIPT-VLA는 간단하고 확장可能한 강화학습 기반의 상호작용적인 후학습 패러다임입니다. 이 패러다임은 희소한 이진 성공 보상만으로 학습된 프리트레이닝 비전-언어-행동(Vision-Language-Action) 모델을 미세 조정하는 데 사용됩니다. 현재의 VLA 훈련 프로세스는 오프라인 EXPERT示范 데이터와 체인 학습을 통해 새로운 태스크와 환경에 대응할 수 있습니다. RIPT-VLA는 동적인 롤아웃 샘플링과 leave-one-out 우선 순위 평가에 기반한 안정된 정책 최적화 알고리즘을 사용하여 이러한 문제를 해결하고 있습니다.\n\nRIPT-VLA는 다음과 같은 특징을 가지고 있습니다. 첫째, 다양한 VLA 모델에 적용 가능한데, 가벼운 모델 QueST의 성능을 21.2% 상승으로, 7B OpenVLA-OFT 모델의 성공률을 전례없는 97.5%로 달성합니다. 둘째, 계산적으로 효율적이고 데이터 효율적입니다: 하나의示范 데이터만으로, RIPT-VLA는 무효한 SFT 모델(4%)를 15번의 반복으로 97%의 성공률로 작동시킵니다. 또한, RIPT-VLA가 학습한 정책은 다양한 태스크와 시나리오에 광범위하게 일반화하고, 초기 상태 컨텍스트에 강건합니다. 이러한 결과를 통해, RIPT-VLA는 최소한의 서브 프로바이시션으로 후학습 비전-언어-행동 모델을 실용적이고 효과적으로 학습하는 패러다임의 중요성을 밝혀줍니다.",
      "upvotes": 1,
      "discussionId": "6833f7857e0c637c71de0f07",
      "projectPage": "https://ariostgx.github.io/ript_vla/",
      "githubRepo": "https://github.com/Ariostgx/ript-vla",
      "ai_summary": "RIPT-VLA is a reinforcement learning-based interactive post-training paradigm that enhances pretrained Vision-Language-Action models using sparse binary success rewards, improving adaptability and generalization.",
      "ai_keywords": [
        "reinforcement-learning-based",
        "interactive post-training",
        "Vision-Language-Action (VLA) models",
        "sparse binary success rewards",
        "offline expert demonstration",
        "supervised imitation",
        "dynamic rollout sampling",
        "leave-one-out advantage estimation",
        "policy optimization",
        "lightweight QueST model",
        "OpenVLA-OFT model",
        "success rate",
        "computational efficiency",
        "data-efficient",
        "policy learned",
        "generalization",
        "initial state context"
      ]
    },
    "publishedAt": "2025-05-22T13:59:45.000Z",
    "title": "Interactive Post-Training for Vision-Language-Action Models",
    "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17016.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b64debeb9a833e08d079fd",
      "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
      "fullname": "Shuhan Tan",
      "name": "tanshh97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16293",
      "authors": [
        {
          "_id": "683400b5231225ee202c20b7",
          "user": {
            "_id": "645c26d423ed9b7788d5e24b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
            "isPro": false,
            "fullname": "Rishabh Maheshwary",
            "user": "rmahesh",
            "type": "user"
          },
          "name": "Rishabh Maheshwary",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:08:47.558Z",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20b8",
          "name": "Masoud Hashemi",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20b9",
          "name": "Khyati Mahajan",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20ba",
          "name": "Shiva Krishna Reddy Malay",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bb",
          "name": "Sai Rajeswar",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bc",
          "name": "Sathwik Tejaswi Madhusudhan",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bd",
          "name": "Spandana Gella",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20be",
          "name": "Vikas Yadav",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T06:45:05.000Z",
      "submittedOnDailyAt": "2025-05-26T04:22:12.337Z",
      "title": "LLM의 논리론리를 동적인 노트 쓰기에 의해 강화된 복잡한 QA의 해결책",
      "submittedOnDailyBy": {
        "_id": "645c26d423ed9b7788d5e24b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
        "isPro": false,
        "fullname": "Rishabh Maheshwary",
        "user": "rmahesh",
        "type": "user"
      },
      "summary": "Iterative RAG를 사용하여 멀티 홤컷 질문 답변에 직면한挑战는 긴 맥락과 부적절한 정보의 축적입니다. 이는 모델이 처리하고 retrieved 콘텐츠에 대한 추론하는 능력을 제한합니다. 최근의 방법들은 retrieved 정보를 압축하는 데 집중하지만, 이들은 단일 라운드 RAG에 제한되어있거나, finetuning이 필요하거나, 반복적인 RAG에서 확장성이 부족합니다. 이러한 문제를 해결하기 위해, 우리는 Notes Writing이라는 방법을 제안합니다. Notes Writing은 각 단계에서 retrieved 문서에서 간결하고 관련있는 노트를 생성하여 노이즈를 줄이고 필수적인 정보만 유지합니다. 이는 Large Language Models (LLMs)의 유효한 맥락 길이를 간접적으로 늘려, 더 큰 입력 텍스트 양을 처리할 때 더 효과적으로 추론하고 계획할 수 있도록 합니다. Notes Writing은 프레임워크가 독립적이며, 다양한 반복적인 RAG 방법에서 통합할 수 있습니다. 우리는 세 가지 반복적인 RAG 방법을 사용하여 이 방법을 효과성을 입증하였으며, 두 모델과 네 평가 데이터셋을 통해 평균 15.6 퍼센트 포인트의 향상을 달성하였으며, 출력 토큰의 증가는 최소화되었습니다.",
      "upvotes": 1,
      "discussionId": "683400b6231225ee202c20e3",
      "ai_summary": "Notes Writing enhances iterative RAG by generating concise notes at each step, improving reasoning and performance while minimizing output increase.",
      "ai_keywords": [
        "Iterative RAG",
        "multi-hop question answering",
        "context length",
        "irrelevant information",
        "dimensionality reduction",
        "Notes Writing",
        "Large Language Models",
        "LLMs",
        "framework agnostic",
        "evaluation datasets"
      ]
    },
    "publishedAt": "2025-05-22T02:45:05.000Z",
    "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA",
    "summary": "Iterative RAG for multi-hop question answering faces challenges with lengthy\ncontexts and the buildup of irrelevant information. This hinders a model's\ncapacity to process and reason over retrieved content and limits performance.\nWhile recent methods focus on compressing retrieved information, they are\neither restricted to single-round RAG, require finetuning or lack scalability\nin iterative RAG. To address these challenges, we propose Notes Writing, a\nmethod that generates concise and relevant notes from retrieved documents at\neach step, thereby reducing noise and retaining only essential information.\nThis indirectly increases the effective context length of Large Language Models\n(LLMs), enabling them to reason and plan more effectively while processing\nlarger volumes of input text. Notes Writing is framework agnostic and can be\nintegrated with different iterative RAG methods. We demonstrate its\neffectiveness with three iterative RAG methods, across two models and four\nevaluation datasets. Notes writing yields an average improvement of 15.6\npercentage points overall, with minimal increase in output tokens.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16293.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645c26d423ed9b7788d5e24b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
      "fullname": "Rishabh Maheshwary",
      "name": "rmahesh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16022",
      "authors": [
        {
          "_id": "68342cb2924393051af84722",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84723",
          "name": "Siya Qi",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84724",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84725",
          "name": "Chen Qian",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84726",
          "name": "Yali Du",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84727",
          "name": "Yulan He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T21:12:35.000Z",
      "submittedOnDailyAt": "2025-05-26T07:38:18.450Z",
      "title": "NOVER: 비평가를 포함하지 않은 강화학습에 의한 언어 모델의 시각적 훈련\n\n(Note: The translation provided is a direct translation of the given text. The term \"NOVER\" is not a standard term in Korean and might need to be contextualized or clarified based on the specific field or application it is used in.)",
      "submittedOnDailyBy": {
        "_id": "66e2932e5c100c12aa2def39",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FiQ5Fap-qVqnXeULGPYs6.png",
        "isPro": false,
        "fullname": "weiliu",
        "user": "thinkwee",
        "type": "user"
      },
      "summary": "최근의 발전에서 DeepSeek R1-Zero 등 기능은 보상의 계산이 언어 모델의 출력의 최종 답변 부분을 기반으로 하는 보상 학습 패러다임의 효율성을 강조하고 있습니다. 이러한 방법들은 외부의 검증 데이터를 기반으로 보상을 계산하기 때문에 수학이나 코딩 등 검증 데이터가 쉽게 얻을 수 있는 분야에 제한되어 있습니다. 보상 모델은 고품질의 데이터의 필요성과 훈련 비용의 높기 때문에 이러한 문제를 해결하는 것이 어렵습니다. 본 논문에서는 외부의 검증 데이터가 필요하지 않는 일반적인 보상 학습 프레임워크인 NOVER(NO-VERifier Reinforcement Learning)를 제안합니다. NOVER는 표준의 초피니닝 데이터가 필요하며 보상 학습을 수행하기 위한 외부의 검증 데이터가 필요하지 않게 됩니다. NOVER는 광범위한 문から 문으로의 문제 해결 태스크에서 보상 학습을 가능하게 하고, DeepSeek R1 671B와 같은 대규모의 이유 모델로부터 동일한 크기의 모델을 7.7% 이상의 개선률을 초월합니다. 또한 NOVER의 유연성은 대규모의 언어 모델의 최적화에 새로운 가능성을 제공하고, 역 보상 학습 등 새로운 가능성의 개척을 합니다.",
      "upvotes": 1,
      "discussionId": "68342cb3924393051af8476b",
      "githubRepo": "https://github.com/thinkwee/NOVER",
      "ai_summary": "NOVER, a reinforcement learning framework that eliminates the need for external verifiers, enhances language model performance across text-to-text tasks.",
      "ai_keywords": [
        "incentive training",
        "reinforcement learning",
        "NOVER",
        "NO-VERifier Reinforcement Learning",
        "DeepSeek R1-Zero",
        "DeepSeek R1 671B",
        "inverse incentive training"
      ]
    },
    "publishedAt": "2025-05-21T17:12:35.000Z",
    "title": "NOVER: Incentive Training for Language Models via Verifier-Free\n  Reinforcement Learning",
    "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e2932e5c100c12aa2def39",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FiQ5Fap-qVqnXeULGPYs6.png",
      "fullname": "weiliu",
      "name": "thinkwee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15805",
      "authors": [
        {
          "_id": "682eeb06720821973d643576",
          "user": {
            "_id": "647c4a2692182942d7c2e698",
            "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
            "isPro": false,
            "fullname": "HWANCHANG",
            "user": "HwanChang0106",
            "type": "user"
          },
          "name": "Hwan Chang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:18:42.752Z",
          "hidden": false
        },
        {
          "_id": "682eeb06720821973d643577",
          "name": "Yumin Kim",
          "hidden": false
        },
        {
          "_id": "682eeb06720821973d643578",
          "name": "Yonghyun Jun",
          "hidden": false
        },
        {
          "_id": "682eeb06720821973d643579",
          "name": "Hwanhee Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:58:11.000Z",
      "submittedOnDailyAt": "2025-05-26T07:18:19.930Z",
      "title": "보안을 지키자! 대규모 언어 모델 컨텍스트에서 보안 정책의 보존에 대한 질문형 간접 공격에 대한 벤치마크",
      "submittedOnDailyBy": {
        "_id": "647c4a2692182942d7c2e698",
        "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
        "isPro": false,
        "fullname": "HWANCHANG",
        "user": "HwanChang0106",
        "type": "user"
      },
      "summary": "LLM가 기업이나 정부 등 민감한 분야에서 날로 확장되는 가운데, 사용자 정의된 보안 정책이 컨텍스트 내에서 준수되도록 유도하는 것은 중요하며, 정보의 비공개화에 있어 특히 중요합니다. 기존의 LLM 연구는 일반적인 보안성과 사회적으로 민감한 데이터에 초점을 맞추었지만, 공격에 대한 컨텍스트적인 보안 유지에 대한 큰 벤치마크는 부족했습니다. 이에 대해, 우리는 새로운 큰 벤치마크 데이터셋인 CoPriva를 소개하고, LLM이 사용자 정의된 정책에 따라 컨텍스트적 비공개 정책을 준수하는 정도를 평가합니다. 실제적인 컨텍스트에서부터 생성된 데이터셋이며, 명시된 정책과 질문을 포함하여, 직접적인 또는 간접적인 공격에 대한 금지된 정보를 탐색하는 것입니다. 우리 벤치마크에서 10개의 LLM을 평가하고, 중요한 취약점을 밝혀냈습니다: 다수의 모델은 사용자 정의된 정책을 파괴하고 민감한 정보를 노출하고 있습니다. 이 실패는 특히 간접적인 공격에 대해 더욱 엄격합니다. 현재의 LLM의 보안성에 대한 민감한 애플리케이션에서 중요한 결점인 것을 밝혀냅니다. 우리의 분석에 따르면, 모델은 올바른 답변을 특정할 수 있는 것을 보여주지만, 생성할 때 정책의 제약을 적용하는 것이 어려워졌습니다. 반면, 명시적으로 프롬프트가 주어졌을 때 출력을 수정하는 부분의 능력을 보여주었습니다. 이러한 발견은 컨텍스트적 보안을 보장하기 위해 더욱 강력한 방법이 급급하다는 것을 강조합니다.",
      "upvotes": 1,
      "discussionId": "682eeb07720821973d6435ec",
      "githubRepo": "https://github.com/hwanchang00/CoPriva",
      "ai_summary": "LLMs frequently violate contextual security policies by leaking sensitive information, particularly under indirect attacks, indicating a critical gap in current safety mechanisms.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "security policies",
        "information non-disclosure",
        "CoPriva",
        "contextual security preservation",
        "question answering",
        "explicit policies",
        "indirect attacks",
        "virus",
        "policy constraints",
        "output revision"
      ]
    },
    "publishedAt": "2025-05-21T13:58:11.000Z",
    "title": "Keep Security! Benchmarking Security Policy Preservation in Large\n  Language Model Contexts Against Indirect Attacks in Question Answering",
    "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15805.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647c4a2692182942d7c2e698",
      "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
      "fullname": "HWANCHANG",
      "name": "HwanChang0106",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12891",
      "authors": [
        {
          "_id": "683059e8e2f446ed653e8512",
          "name": "Shaohang Wei",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8513",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8514",
          "user": {
            "_id": "6447ca6ca478b20f1755b294",
            "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
            "isPro": false,
            "fullname": "Feifan Song",
            "user": "songff",
            "type": "user"
          },
          "name": "Feifan Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:14:11.572Z",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8515",
          "name": "Wen Luo",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8516",
          "name": "Tianyi Zhuang",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8517",
          "name": "Haochen Tan",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8518",
          "name": "Zhijiang Guo",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8519",
          "name": "Houfeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T09:22:02.000Z",
      "submittedOnDailyAt": "2025-05-26T03:54:28.256Z",
      "title": "타임: 리アル워럴시나리오에서 LLMs의 시간논리적 다레벨 벤치마크\n\n(注意：翻译中保留了原文的专有名词和技术术语，以确保专业性和准确性。)",
      "submittedOnDailyBy": {
        "_id": "6447ca6ca478b20f1755b294",
        "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
        "isPro": false,
        "fullname": "Feifan Song",
        "user": "songff",
        "type": "user"
      },
      "summary": "시간논리는 대규모 언어 모델(LLMs)가 현실을 이해하는 데 중요합니다. 그러나 현재의 연구는 시간논리에서의 현실의 문제를 잘못 이해하고 있습니다. 이는 (1) 강력한 시간 정보, (2) 빠르게 변화하는 이벤트 다이나믹스, (3) 복잡한 시간 관계성 사회 상호작용으로 이루어져 있습니다. 이를 해결하기 위해, 우리는 현실의 스케일러 벤치마크인 TIME를 제안하고 있습니다. TIME는 38,522개의 QA 쌍을 포함하고 있으며, 3개의 레벨로 나뉘어 11개의 세부 태스크를 가지고 있습니다. 이 벤치마크는 3개의 서브 데이터 세트를 포함하고 있으며, 서로 다른 현실의 문제를 반영하고 있습니다: TIME-Wiki, TIME-News, TIME-Dial. 우리는 인론 모델과 비인론 모델에 대한 확장된 실험을 수행하고, 시간논리의 성능을 상세하게 분석하며, 시간논리 능력에 대한 테스트 시간 스케일링의 영향을 요약했습니다. 또한, 미래의 연구와 표준화된 평가에 대한 촉진하기 위해, TIME-Lite를 릴리즈하고 있습니다. 코드는 https://github.com/sylvain-wei/TIME에서 접근할 수 있으며, 데이터 세트는 https://huggingface.co/datasets/SylvainWei/TIME에서 접근할 수 있습니다.",
      "upvotes": 1,
      "discussionId": "683059eae2f446ed653e85d7",
      "ai_summary": "A benchmark called TIME assesses temporal reasoning in LLMs across varied real-world challenges, including intensive temporal information, fast-changing event dynamics, and complex social interactions, and evaluates the impact of test-time scaling.",
      "ai_keywords": [
        "Temporal reasoning",
        "Large Language Models (LLMs)",
        "QA pairs",
        "benchmark",
        "TIME-Wiki",
        "TIME-News",
        "TIME-Dial",
        "reasoning models",
        "non-reasoning models",
        "TIME-Lite"
      ]
    },
    "publishedAt": "2025-05-19T05:22:02.000Z",
    "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in\n  Real-World Scenarios",
    "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12891.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6447ca6ca478b20f1755b294",
      "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
      "fullname": "Feifan Song",
      "name": "songff",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16056",
      "authors": [
        {
          "_id": "6830894db51948863e05b68c",
          "user": {
            "_id": "64bfa1401d40292dd32f93d7",
            "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
            "isPro": false,
            "fullname": "Leo Liang",
            "user": "ljcleo",
            "type": "user"
          },
          "name": "Jingcong Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:13:21.361Z",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b68d",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b68e",
          "name": "Miren Tian",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b68f",
          "name": "Yitong Li",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b690",
          "name": "Duyu Tang",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b691",
          "name": "Zhongyu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T22:13:09.000Z",
      "submittedOnDailyAt": "2025-05-26T07:53:13.385Z",
      "title": "모든 모델은 전문의의 오버플로에 적합하지 않습니다: Mixture-of-Expert 모델의 로컬 루팅의 일관성",
      "submittedOnDailyBy": {
        "_id": "64bfa1401d40292dd32f93d7",
        "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
        "isPro": false,
        "fullname": "Leo Liang",
        "user": "ljcleo",
        "type": "user"
      },
      "summary": "Mixture-of-Experts (MoE)는 추론 시 희소하게 활성화되는 익스퍼트를 사용하여, 큰 규모의 언어 모델(LLMs)의 효율적인 스케일링을 가능하게 합니다. 메모리 제한이 있는 장치에서 MoE 모델을 효과적으로 적용하기 위해, 여러 시스템은 *экс퍼트 오버로딩*을 도입하여, 고속 메모리에 익스퍼트의 일부를 캐싱하고, 나머지는 CPU로 실행하거나, 필요할 때 로드하도록 설계되었습니다. 반면, 익스퍼트의 활성화의 지역성(연속된 토큰이 유사한 익스퍼트를 활성화하는 것)을 활용하는 연구도 있습니다만, **지역 루팅의 일관성**은 모델별로 다릅니다, 아직 조사가 부족합니다. 본 논문에서는 MoE 모델의 지역 루팅의 일관성을 평가하기 위해 두 가지 메트릭을 제안합니다: 1. **Segment Routing Best Performance (SRP)**는 고정 그룹의 익스퍼트가 토큰의 세그먼트의 필요를 얼마나 잘 대응하는지 평가합니다. 2. **Segment Cache Best Hit Rate (SCH)**는 주어진 캐쉬 크기 제한 하에서 최적의 세그먼트 수준의 캐쉬 히트율을 평가합니다. 20개의 효과 크기와 구조가 다른 MoE LLMs를 분석하고, 이들 모델이 각 층에서 MoE를 적용하고, 공유 익스퍼트를 사용하지 않는 모델이 가장 높은 지역 루팅의 일관성을 보여주는 것을 발견했습니다. 또한, 영역 전문적인 익스퍼트가 지역 루팅의 일관성에 비교적으로 더 많은 기여를 할 수 있으며, 거의 모든 모델은 활성 익스퍼트의 2배 정도의 캐쉬 크기로 캐쉬의 효과와 효율을 균형을 이루는 것을 보여줍니다. 이러한 발견은 추론 속도를 희생하지 않고, 메모리 효율적인 MoE의 설계와 적용에 연결됩니다. 본 논문에서는, 실험의 재현을 위해 코드를 공개합니다(https://github.com/ljcleo/moe-lrc).",
      "upvotes": 0,
      "discussionId": "6830894eb51948863e05b6e8",
      "ai_summary": "MoE models achieve efficient scaling in LLMs with expert offloading, emphasizing the importance of local routing consistency and cache effectiveness.",
      "ai_keywords": [
        "Mixture-of-Experts (MoE)",
        "large language models (LLMs)",
        "expert offloading",
        "fast memory",
        "slow memory",
        "local routing consistency",
        "Segment Routing Best Performance (SRP)",
        "Segment Cache Best Hit Rate (SCH)",
        "domain-specialized experts",
        "vocabulary-specialized experts"
      ]
    },
    "publishedAt": "2025-05-21T18:13:09.000Z",
    "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
    "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16056.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bfa1401d40292dd32f93d7",
      "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
      "fullname": "Leo Liang",
      "name": "ljcleo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11881",
      "authors": [
        {
          "_id": "6833ebdb142b0e50399413d3",
          "name": "Giyeong Oh",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d4",
          "name": "Woohyun Cho",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d5",
          "name": "Siyeol Kim",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d6",
          "name": "Suhwan Choi",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d7",
          "name": "Younjae Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-17T07:16:11.000Z",
      "submittedOnDailyAt": "2025-05-26T02:52:51.028Z",
      "title": "리턴된 잔차 커넥션: 안정된 큰 깊은 네트워크의 효율적인 정규화 업데이트",
      "submittedOnDailyBy": {
        "_id": "63d93667255ef6add20f9272",
        "avatarUrl": "/avatars/99a3aeadcc81ef85164cdfb6ab186b17.svg",
        "isPro": false,
        "fullname": "Giyeong Oh",
        "user": "BootsofLagrangian",
        "type": "user"
      },
      "summary": "残差コネクション은 깊은 뉴럴네트워크에 있어서 중요한 요소이며, 학습 과정에서의 가인 소실 현상을 완화하고 깊이를 증가시킬 수 있습니다. 그러나 일반적인 잔차 업데이트 방식은 모듈의 출력이 입력 스트리밍에 직접 추가됩니다. 이 경우, 기존 스트리밍의 방향을 주로 강화하거나 조절하는 업데이트가 발생하며, 모듈의 학습 능력을 완전히 활용하지 못할 가능성이 있습니다. 본 논문에서는 직교 잔차 업데이트를 도입합니다: 모듈의 출력을 입력 스트리밍에 대해 분해하고, 이 스트리밍에 직교된 성분만 추가합니다. 이 설계는 모듈이 새로운 표현 방향을 주로 제공하도록 촉진하고, 풍부한 특성 학습을 촉진하고, 효율적인 훈련을 추진하는 것을 목표로 합니다. 직교 업데이트 스테레타이제이션은 다양한 아키텍처(ResNetV2, Vision Transformers)와 데이터셋(CIFARs, TinyImageNet, ImageNet-1k)에서 일반화 정확도와 훈련 안정성을 향상시키는 것을 보여주며, 예를 들어, ImageNet-1k에서 ViT-B의 top-1 정확도는 +4.3%p를 향상시켰습니다.",
      "upvotes": 0,
      "discussionId": "6833ebdc142b0e5039941420",
      "ai_summary": "Orthogonal Residual Updates enhance feature learning and training stability by decomposing module outputs to contribute primarily novel features.",
      "ai_keywords": [
        "residual connections",
        "vanishing gradients",
        "orthogonal update",
        "ResNetV2",
        "Vision Transformers",
        "CIFARs",
        "TinyImageNet",
        "ImageNet-1k",
        "top-1 accuracy"
      ]
    },
    "publishedAt": "2025-05-17T03:16:11.000Z",
    "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and\n  Efficient Deep Networks",
    "summary": "Residual connections are pivotal for deep neural networks, enabling greater\ndepth by mitigating vanishing gradients. However, in standard residual updates,\nthe module's output is directly added to the input stream. This can lead to\nupdates that predominantly reinforce or modulate the existing stream direction,\npotentially underutilizing the module's capacity for learning entirely novel\nfeatures. In this work, we introduce Orthogonal Residual Update: we decompose\nthe module's output relative to the input stream and add only the component\northogonal to this stream. This design aims to guide modules to contribute\nprimarily new representational directions, fostering richer feature learning\nwhile promoting more efficient training. We demonstrate that our orthogonal\nupdate strategy improves generalization accuracy and training stability across\ndiverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,\nTinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy\ngain for ViT-B on ImageNet-1k.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11881.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d93667255ef6add20f9272",
      "avatarUrl": "/avatars/99a3aeadcc81ef85164cdfb6ab186b17.svg",
      "fullname": "Giyeong Oh",
      "name": "BootsofLagrangian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]