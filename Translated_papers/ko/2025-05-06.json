[
  {
    "paper": {
      "id": "2505.02707",
      "authors": [
        {
          "_id": "6819982f17007d963b9d4166",
          "name": "Yemin Shi",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4167",
          "name": "Yu Shu",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4168",
          "name": "Siwei Dong",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4169",
          "user": {
            "_id": "6108ae87823007eaf0c7bd1e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6108ae87823007eaf0c7bd1e/dKjdx9I5waJs6oUQ0_mmT.png",
            "isPro": false,
            "fullname": "Guangyi Liu",
            "user": "guangyil",
            "type": "user"
          },
          "name": "Guangyi Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:46:52.667Z",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416a",
          "user": {
            "_id": "6438a9027de34e8ea7e4b257",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438a9027de34e8ea7e4b257/vib8QSd1AWMr_bR9ig_xJ.jpeg",
            "isPro": false,
            "fullname": "Jaward Sesay",
            "user": "Jaward",
            "type": "user"
          },
          "name": "Jaward Sesay",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:48.746Z",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416b",
          "name": "Jingwen Li",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416c",
          "user": {
            "_id": "665bfa1b0d71762b8613282d",
            "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
            "isPro": false,
            "fullname": "Zhiting Hu",
            "user": "zhitinghu",
            "type": "user"
          },
          "name": "Zhiting Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:46:15.191Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/665bfa1b0d71762b8613282d/zbWarqt8nFt0AwhF0gElE.mp4"
      ],
      "publishedAt": "2025-05-05T15:05:01.000Z",
      "submittedOnDailyAt": "2025-05-06T03:36:16.945Z",
      "title": "Voila: 음성 언어 근본 모델에 의한 시간 단위의 자동궤도 운전 인터랙션과 음성 언어 직업 플레이",
      "submittedOnDailyBy": {
        "_id": "665bfa1b0d71762b8613282d",
        "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
        "isPro": false,
        "fullname": "Zhiting Hu",
        "user": "zhitinghu",
        "type": "user"
      },
      "summary": "Voila는 일상 생활과 세미나 생활을 융합시키고, 자동으로 변환하는, 실시간, 감정적으로 표현되는 인간과 상호작용하는 음성 AI 에이전트입니다. 단순한 명령에 대한 반응을 넘어, 연속적으로 듣기, 이유를 엿고, 주동적으로 반응을 하며, 자연스럽고 동적인, 감정적으로 공감하는 대화를 촉진합니다. Voila는 이러한 비전을 향해 걸음을 돋아 나갔습니다. Voila는 가족의 큰 규모의 음성 언어 기반 모델을 소개하고 있습니다. Voila는 기존의 파이프라인 시스템을 초월하고, 모든 이중 크로스, 저 라틴 시의 대화를 가능하게 하는 새로운 엔드 포인트부터 엔드 포인트까지의 아키텍처를 도입하여, 음조, 리듬, 감정 등 풍부한 음성의 조화를 유지합니다. 195밀리초의 응답 지연을 달성하고, 평균인간의 응답 시간보다 빠르게 응답합니다. 계층적인 다스케일 Transformer는 대규모 언어 모델(LLMs)의 이유 능력과 강력한 음향 모델링을 통합하여, 자연스럽고 전문적으로 기억할 수 있는 음성의 생성을 가능하게 합니다. 사용자는 간단한 텍스트 지시로 스피커의 인식, 음조, 기타 특성을 정의할 수 있습니다. 또한, Voila는 10초 정도의 짧은 음성 샘플에서 새로운 음성을 적절하게 커스터마이징할 수 있는 것을 권장하고, 超100만 개의 사전 구축된 음성을 지원합니다. 대화뿐만 아니라, 자동 음성 인식(ASR), 텍스트 온 소비티(TTS), 그리고 최소한의 적응을 통해 여러 언어의 문법 번역에도 대응합니다. Voila는 완전히 오픈 소스이며, 공개 연구를 지원하고, 다음 세대의 인간 기계 대화의 발전을 촉진하는 것을 목표로 합니다.",
      "upvotes": 49,
      "discussionId": "6819983117007d963b9d4247",
      "projectPage": "https://voila.maitrix.org",
      "githubRepo": "https://github.com/maitrix-org/Voila",
      "ai_keywords": [
        "full-duplex",
        "low-latency conversations",
        "hierarchical multi-scale Transformer",
        "reasoning capabilities",
        "large language models (LLMs)",
        "acoustic modeling",
        "persona-aware voice generation",
        "automatic speech recognition (ASR)",
        "Text-to-Speech (TTS)",
        "multilingual speech translation",
        "pre-built voices",
        "efficient customization"
      ]
    },
    "publishedAt": "2025-05-05T11:05:01.000Z",
    "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play",
    "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/665bfa1b0d71762b8613282d/zbWarqt8nFt0AwhF0gElE.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02707.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "665bfa1b0d71762b8613282d",
      "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
      "fullname": "Zhiting Hu",
      "name": "zhitinghu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02387",
      "authors": [
        {
          "_id": "681988d6d6a5fee26b52ac28",
          "user": {
            "_id": "6270ff726417aed8a7340c8b",
            "avatarUrl": "/avatars/3f14913c55cc4fc78678ac43fb603e80.svg",
            "isPro": false,
            "fullname": "Xiusi Chen",
            "user": "XtremSup",
            "type": "user"
          },
          "name": "Xiusi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:47:11.654Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac29",
          "user": {
            "_id": "654d784d71a30c4bca09a319",
            "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
            "isPro": false,
            "fullname": "Gaotang Li",
            "user": "gaotang",
            "type": "user"
          },
          "name": "Gaotang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:13.258Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2a",
          "name": "Ziqi Wang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2b",
          "name": "Bowen Jin",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2c",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2d",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2e",
          "user": {
            "_id": "65f906e5c3dbdcae83ff7aac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f906e5c3dbdcae83ff7aac/mdjiVkLDJgJcGLwv0rMe4.jpeg",
            "isPro": false,
            "fullname": "Hongru Wang",
            "user": "Merlin-Hongru",
            "type": "user"
          },
          "name": "Hongru Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:11.136Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2f",
          "name": "Yu Zhang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac30",
          "user": {
            "_id": "66285acb73af5913c6bbf1ec",
            "avatarUrl": "/avatars/8969e3a6ae2dcc0b1c49768fd044b9e0.svg",
            "isPro": false,
            "fullname": "Denghui Zhang",
            "user": "zhangdenghui123",
            "type": "user"
          },
          "name": "Denghui Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:48:00.793Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac31",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac32",
          "name": "Hanghang Tong",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac33",
          "name": "Heng Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:11:12.000Z",
      "submittedOnDailyAt": "2025-05-06T02:32:05.558Z",
      "title": "RM-R1: 보상 모델링을 논리론으로 이해하기",
      "submittedOnDailyBy": {
        "_id": "654d784d71a30c4bca09a319",
        "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
        "isPro": false,
        "fullname": "Gaotang Li",
        "user": "gaotang",
        "type": "user"
      },
      "summary": "보상 모델링은 대규모 언어 모델(LLMs)와 인간들의 취향을 일치시키기 위해 중요하며, 특히 인간으로부터의 강화 학습(RLHF)을 통해 이루어진다. 정확한 보상 신호를 제공하기 위해 보상 모델(RM)은 깊은 사고를 촉발시키고 해석 가능한 이유를 제시해야 한다. 그러나 현재의 RM은 불투명한 스칼라 스코어를 생성하거나 좋은 답의 예측을 직접 생성하기 때문에, 자연어의 평가를 통합하는 것이 어려워 해석성이 부족하다.\n\n최근의 긴 연속적인 추론(CoT)의 발전에 힘입어, 우리는 보상 모델링에 이유의 능력을 통합하여 RM의 해석성과 성능을 크게 향상시킬 수 있음을 가정하고 증명하였다. 본 논문에서는 새로운 보상 모델의 클래스인 논리 보상 모델(ReasRMs)을 소개하고, 보상 모델링을 이유의 임무로 구성하는 방법을 제시한다. 우리는 이유에 대한 훈련 프로세스를 제안하고, ReasRMs의 일종인 RM-R1을 훈련한다. 훈련은 두 개의 주요 단계로 이루어진다: (1) 고품질의 이유의 연속적인 세련화, (2) 검증 가능한 보상을 이용한 강화 학습. RM-R1은 자동적으로 이유의 트래스 또는 대화의 고유한 리뷰 가이드를 생성하고, 후보 답변을 비교하여 LLM의 로드아웃을 향상시킨다. 실험적으로, 우리의 모델은 여러 세부적인 보상 모델 벤치마크에서 생성적인 보상 모델의 최상위 또는 가까운 최상위 성능을 달성하고, 큰 오픈 웨이트 모델(예: Llama3.1-405B)이나 소유권 모델(예: GPT-4o)을 초과한다(최대 13.8% 이상). 최종적인 성능을 초과하기 외에도, 우리는 성공적인 ReasRM 훈련의 핵심 요소를 이해하기 위해 상세한 실험적 분석을 수행한다. 향후 연구를 위해, 우리는 6개의 ReasRM 모델, 코드 및 데이터를 공개한다(https://github.com/RM-R1-UIUC/RM-R1).",
      "upvotes": 28,
      "discussionId": "681988d7d6a5fee26b52ac7e",
      "githubRepo": "https://github.com/RM-R1-UIUC/RM-R1",
      "ai_keywords": [
        "reward modeling",
        "reinforcement learning from human feedback (RLHF)",
        "reward model (RM)",
        "scalar scores",
        "preferred answer",
        "natural language critiques",
        "long chain-of-thought (CoT)",
        "reasoning capabilities",
        "Reasoning Reward Models (ReasRMs)",
        "reasoning-oriented training pipeline",
        "distillation",
        "high-quality reasoning chains",
        "reinforcement learning",
        "verifiable rewards",
        "LLM rollouts",
        "self-generating reasoning traces",
        "chat-specific rubrics",
        "candidate responses",
        "generative reward models",
        "state-of-the-art",
        "near state-of-the-art",
        "reward model benchmarks",
        "open-weight models",
        "proprietary models",
        "empirical analysis",
        "ReasRM models"
      ]
    },
    "publishedAt": "2025-05-05T02:11:12.000Z",
    "title": "RM-R1: Reward Modeling as Reasoning",
    "summary": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654d784d71a30c4bca09a319",
      "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
      "fullname": "Gaotang Li",
      "name": "gaotang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20752",
      "authors": [
        {
          "_id": "6818c145daa8955b2085667d",
          "name": "Roman Abramov",
          "hidden": false
        },
        {
          "_id": "6818c145daa8955b2085667e",
          "user": {
            "_id": "6679882913c63ebaa8ff62fe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
            "isPro": false,
            "fullname": "Felix Steinbauer",
            "user": "fsteinbauer",
            "type": "user"
          },
          "name": "Felix Steinbauer",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-05T13:46:46.742Z",
          "hidden": false
        },
        {
          "_id": "6818c145daa8955b2085667f",
          "name": "Gjergji Kasneci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T13:33:29.000Z",
      "submittedOnDailyAt": "2025-05-06T03:38:21.809Z",
      "title": "跳跃场中的Joke场：执行现实世界多层次推理的Transformer的数据扩展",
      "submittedOnDailyBy": {
        "_id": "6679882913c63ebaa8ff62fe",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
        "isPro": false,
        "fullname": "Felix Steinbauer",
        "user": "fsteinbauer",
        "type": "user"
      },
      "summary": "Transformers는 다수의 NLP 태스크에서 큰 성공을 거뒀지만, 여러 단계의 현실적인 추론에서 분명한 한계가 남아 있습니다. 특히, 현실 세계의 지식이 부족한 경우 이러한 한계가 분명하게 드러납니다. 최근의 grokking의 발전에 따라, 신경 네트워크가 잠재적인 논리 패턴을 감지하고, 기억에서 완전히 일반화할 수 있음을 보여주었습니다. 그러나 이러한 연구는 주로 작은 합성 태스크를 사용했습니다. 이 논문에서는, 먼저, 현실적인 사실 데이터에 grokking을 확장하고, 데이터 세트의 희박성을 해결하기 위해, 신중하게 설계된 합성 데이터를 기존의 지식 그래프에 추가하여 추론된 사실의 비율 phi_r이 원실적 사실보다 임계값을 초과하는 것을 성공적으로 달성했습니다. 놀라워서, 우리는 사실적으로 잘못된 합성 데이터가 추론 사이클을 강화시키고, 기억에 의한 구조보다는 관련적인 구조를 믿는 것을 강제시켰습니다. 여러 단계 추론 벤치마크에서 평가된 부분, 우리의 접근법은 95-100%의 정확도를 2WikiMultiHopQA에서 달성하며, 강력한 baseline을 크게 향상시키고, 현재의 최선 결과를 초월할 수 있습니다. 또한, phi_r의 증가에 따른 Transformers 내부의 일반화 사이클의 형성을 자세히 분석합니다. 우리의 발견은 grokking 기반의 데이터 증강이 숨겨진 여러 단계 추론 능력을 해방하고, 큰 언어 모델에서 더 강건하고 해석 가능한 사실적인 추론을 가능하게 합니다.",
      "upvotes": 19,
      "discussionId": "6818c146daa8955b208566f1",
      "ai_keywords": [
        "Transformers",
        "multi-step factual reasoning",
        "grokking",
        "neural networks",
        "perfect generalization",
        "logical patterns",
        "real-world factual data",
        "dataset sparsity",
        "knowledge graphs",
        "synthetic data",
        "inferred facts",
        "atomic facts",
        "factually incorrect synthetic data",
        "relational structure",
        "memorization",
        "multi-hop reasoning",
        "benchmarks",
        "2WikiMultiHopQA",
        "baselines",
        "state-of-the-art results",
        "generalizing circuits",
        "grokking-based data augmentation",
        "implicit multi-hop reasoning capabilities",
        "robust",
        "interpretable factual reasoning"
      ]
    },
    "publishedAt": "2025-04-29T09:33:29.000Z",
    "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers",
    "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio phi_r of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing phi_r drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20752.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6679882913c63ebaa8ff62fe",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
      "fullname": "Felix Steinbauer",
      "name": "fsteinbauer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02819",
      "authors": [
        {
          "_id": "6819b5da3d9c61444380f4c5",
          "user": {
            "_id": "66465dfa508db0bde50d95f2",
            "avatarUrl": "/avatars/8b4a583dc0f3cab0f1cd9a1be3daa01b.svg",
            "isPro": false,
            "fullname": "Dmitry Shophoev",
            "user": "dimitriish",
            "type": "user"
          },
          "name": "Dmitriy Shopkhoev",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-06T07:10:19.519Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c6",
          "user": {
            "_id": "6166db59f78a267701a78c2a",
            "avatarUrl": "/avatars/8784efc36f67719e9455b1f081340ed9.svg",
            "isPro": false,
            "fullname": "Ammar Ali",
            "user": "ammarali32",
            "type": "user"
          },
          "name": "Ammar Ali",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:17.870Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c7",
          "name": "Magauiya Zhussip",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c8",
          "user": {
            "_id": "66b1ce4ca14db5aac3e5e755",
            "avatarUrl": "/avatars/ab55ef112fba091813e1cc1f43857cf9.svg",
            "isPro": false,
            "fullname": "Valentin Malykh",
            "user": "madrugado",
            "type": "user"
          },
          "name": "Valentin Malykh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:42.358Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c9",
          "user": {
            "_id": "6683cc62b466c0d8e60e1bbc",
            "avatarUrl": "/avatars/d781cfb113263f88eaa3250bef521c53.svg",
            "isPro": false,
            "fullname": "Stamatis Lefkimmiatis",
            "user": "stamatisl",
            "type": "user"
          },
          "name": "Stamatios Lefkimmiatis",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:13.923Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4ca",
          "name": "Nikos Komodakis",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4cb",
          "user": {
            "_id": "667e7f968c6d7aede7ecb94b",
            "avatarUrl": "/avatars/d6dabd9b909b1f20f661dc4bc07af23f.svg",
            "isPro": false,
            "fullname": "Sergey Zagoruyko",
            "user": "szagoruyko121",
            "type": "user"
          },
          "name": "Sergey Zagoruyko",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:52.244Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T17:47:42.000Z",
      "submittedOnDailyAt": "2025-05-06T07:03:26.032Z",
      "title": "레이어 리뉴얼과 선형 변환을 이용한 네트워크의 간략화",
      "submittedOnDailyBy": {
        "_id": "610e8c12119bebecb4d807b6",
        "avatarUrl": "/avatars/7230b1584ec45585c12eb5703fd80ff3.svg",
        "isPro": false,
        "fullname": "Ivan Sedykh",
        "user": "idsedykh",
        "type": "user"
      },
      "summary": "ReplaceMe는 일반화된 훈련 무제한의 깊이 감소 기법이다. 이 방법은 낮은 압축비율로 높은 성능을 유지하면서, transformer 블록을 선형 연산으로 효과적으로 대체할 수 있다. 기존의 감소 기법과 달리, 추가적인 훈련이나 미세 조정이 필요하지 않습니다. 우리의 방법은 작은 보정 데이터 세트를 사용하여, 줄인 블록을 근사하는 선형 변환을 추정하는 데만 충분하다. 이 추정된 선형 매핑은 나머지 transformer 블록과 무차별적으로 통합될 수 있으며, 추가적인 네트워크 파라미터가 필요하지 않습니다. 실험에 따르면, ReplaceMe는 다른 훈련 무제한 기법과 일치함을 보여주고, 복잡한 재훈련, 최종 조정, 아키텍처 변경을 포함하여 가장 先端의 줄인 기법과 비교하여도, 높은 경쟁력을 가지고 있습니다. ReplaceMe는 대규모 언어 모델(LLMs)에 대해, 훈련 및 회복 단계를 제외한, 오픈 벤치마크에서 약 90%의 원 모델의 성능을 유지하면서 25%의 줄인 효과를 실현할 수 있습니다. 이것은 최소한의 계산 오버헤드를 동반합니다(Fig.1 참조). ReplaceMe의 구현과 가장 先端의 깊이 감소 기법 중 몇 가지를 포함하는 오픈 소스 라이브러리를 제공합니다.",
      "upvotes": 16,
      "discussionId": "6819b5db3d9c61444380f518",
      "githubRepo": "https://github.com/mts-ai/ReplaceMe",
      "ai_keywords": [
        "training-free depth pruning",
        "transformer blocks",
        "linear operation",
        "calibration dataset",
        "linear transformation",
        "computational overhead",
        "large language models (LLMs)",
        "open benchmarks",
        "open-source library"
      ]
    },
    "publishedAt": "2025-05-05T13:47:42.000Z",
    "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations",
    "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02819.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "610e8c12119bebecb4d807b6",
      "avatarUrl": "/avatars/7230b1584ec45585c12eb5703fd80ff3.svg",
      "fullname": "Ivan Sedykh",
      "name": "idsedykh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02735",
      "authors": [
        {
          "_id": "6819742e0d1c56fe9124fe3a",
          "user": {
            "_id": "62a80fe3ac97233f1625235a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
            "isPro": false,
            "fullname": "Zhouliang Yu",
            "user": "zhouliang",
            "type": "user"
          },
          "name": "Zhouliang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:34:10.190Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3b",
          "user": {
            "_id": "662f2c8435ab6df959b005de",
            "avatarUrl": "/avatars/3e30053ecbe9cc14b5e1eb2b014755de.svg",
            "isPro": false,
            "fullname": "ruotian peng",
            "user": "prt66",
            "type": "user"
          },
          "name": "Ruotian Peng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:48:20.491Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3c",
          "name": "Keyi Ding",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3d",
          "name": "Yizhe Li",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3e",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3f",
          "user": {
            "_id": "6417d9ea8f689506e7148417",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
            "isPro": false,
            "fullname": "minghao",
            "user": "Liam-Liu",
            "type": "user"
          },
          "name": "Minghao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:31.975Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe40",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yifan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:14.785Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe41",
          "user": {
            "_id": "649da6b4599302cdb9bc232b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DxQT6LCDTZvyGUUe2t19c.jpeg",
            "isPro": false,
            "fullname": "Zheng Yuan",
            "user": "ZhengYuan",
            "type": "user"
          },
          "name": "Zheng Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:20.735Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe42",
          "user": {
            "_id": "6532a060a78e70d19c669103",
            "avatarUrl": "/avatars/3cc9309b0e31da0fb83f1c3ef87dbe9f.svg",
            "isPro": false,
            "fullname": "HuajianXin",
            "user": "HuajianXin",
            "type": "user"
          },
          "name": "Huajian Xin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:28.104Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe43",
          "user": {
            "_id": "641e5bf65f274a0a92c2f6a2",
            "avatarUrl": "/avatars/c15a54c51998c0e6367685e8e1737ec9.svg",
            "isPro": false,
            "fullname": "Wenhao Huang",
            "user": "EZ-hwh",
            "type": "user"
          },
          "name": "Wenhao Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:44.482Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe44",
          "user": {
            "_id": "643c21735fcffe09fb68a46f",
            "avatarUrl": "/avatars/76aabacd318aa954d4c53094ad456056.svg",
            "isPro": false,
            "fullname": "Yandong Wen",
            "user": "ydwen",
            "type": "user"
          },
          "name": "Yandong Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:51.642Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe45",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:59.764Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe46",
          "user": {
            "_id": "648905d1a15c43c791d4381f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
            "isPro": false,
            "fullname": "Weiyang Liu",
            "user": "wy1iu",
            "type": "user"
          },
          "name": "Weiyang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:50:07.063Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T15:37:00.000Z",
      "submittedOnDailyAt": "2025-05-06T01:00:48.636Z",
      "title": "대규모 언어 모델의 공식 수학적 추론 기준 테스트",
      "submittedOnDailyBy": {
        "_id": "62a80fe3ac97233f1625235a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
        "isPro": false,
        "fullname": "Zhouliang Yu",
        "user": "zhouliang",
        "type": "user"
      },
      "summary": "형식의 수학적 추론은 인공지능에 있어서 중요한 과제이며, 현재의 벤치마크의 범위와 규모의 제한으로 논리화되어 있습니다. 이에 대처하여, FormalMATH를 소개합니다. FormalMATH는 5,560개의 공식적으로 검증된 문제를 포함하는 큰 규모의 Lean4 벤치마크입니다. 이 문제를 포함하는 문제를 중학생 올림픽의 도전부터 학부 수준의 정리까지 다양한 분야(예를 들어, 대수, 응용수학, 미적분학, 수론, 이산수학 등)를 아우릅니다. 수동 논리화의 부적절성을 줄이기 위해, 우리는 다음과 같은 3가지 요소를 조합하여 새로운 인간이 로프 내의 자동 논리화 프로파일링 라인을 도입했습니다. 1) 논리문의 자동 논리화를 수행하는 특수화된 대형 언어 모델(LLMs), 2) 다엔드 모델의 언어적 검증, 3) 옵시즌 라인의 LLM 기반의 증명 프로바를 사용하여 부정 기준의 증명의 필터링 전략. 이 접근법은 수동 검증 전에 72.09%의 논리문을 남겨, 자연어의 문제에 대한 충실성을 보장합니다. 가장 先端의 LLM 기반의 증명 프로바의 평가에 따라, 중요한 제한이 명확히 되었습니다. 실용적인 샘플링 버지 아래 가장 강한 모델도 16.46%의 성공률을 달성하며, 영역 바이어스(예를 들어, 대수에서 뛰어난 반면, 미적분학에서 실패하는 경우)과 단순화된 자동화 기술의 과도 의존성이 관찰되었습니다. 특히, 자연어의 해결책의 가이드와 증명의 성공과의 역의 관계가 밝혀져, 인간이 쓴 비정식적인 논리가 형식적인 논리의 설정에서 잡음으로 작용하고, 이는 이해하기 쉬운 것이 아님을 보여줍니다. 우리는 FormalMATH가 형식적인 수학적 논리의 평가에 있어서 강력한 벤치마크로 제공될 수 있다는 것을 믿습니다.",
      "upvotes": 16,
      "discussionId": "6819742f0d1c56fe9124fe8a",
      "projectPage": "https://spherelab.ai/FormalMATH/",
      "githubRepo": "https://github.com/Sphere-AI-Lab/FormalMATH-Bench"
    },
    "publishedAt": "2025-05-05T11:37:00.000Z",
    "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models",
    "summary": "Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a80fe3ac97233f1625235a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
      "fullname": "Zhouliang Yu",
      "name": "zhouliang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02835",
      "authors": [
        {
          "_id": "6819762e64ae18f1b6fde347",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yi-Fan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:15.220Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde348",
          "user": {
            "_id": "664ba004bfd9b93ba4bfb353",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/UHaEcXmMSKvvFDsY3hCnb.jpeg",
            "isPro": false,
            "fullname": "LuXingyu",
            "user": "XingyuLu",
            "type": "user"
          },
          "name": "Xingyu Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:24.963Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde349",
          "name": "Xiao Hu",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34a",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34b",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34c",
          "name": "Tianke Zhang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34d",
          "user": {
            "_id": "673421bf18caf8e877861cc6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8UfIZTUTFaCnWmJ_Bztr.png",
            "isPro": false,
            "fullname": "Changyi Liu",
            "user": "bhsc24",
            "type": "user"
          },
          "name": "Changyi Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:58:28.151Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34e",
          "user": {
            "_id": "63774c47455f6ad89ac41be1",
            "avatarUrl": "/avatars/e7d6048155cdf4497d58aa18523e745e.svg",
            "isPro": false,
            "fullname": "Kaiyu Jiang",
            "user": "KaiyuValley",
            "type": "user"
          },
          "name": "Kaiyu Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:58:21.743Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34f",
          "name": "Kaibing Chen",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde350",
          "user": {
            "_id": "66c605e808fee728d0dd94f5",
            "avatarUrl": "/avatars/d2ff37fedc5ac1b5b817543b80bf5256.svg",
            "isPro": false,
            "fullname": "Kaiyu Tang",
            "user": "KevinTowne",
            "type": "user"
          },
          "name": "Kaiyu Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:58:06.587Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde351",
          "user": {
            "_id": "6610f64ee94d9046b71e19c8",
            "avatarUrl": "/avatars/11cc11199669129a740956d12c7214e8.svg",
            "isPro": false,
            "fullname": "Haojie Ding",
            "user": "haojieding",
            "type": "user"
          },
          "name": "Haojie Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:59.222Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde352",
          "user": {
            "_id": "6433abff546e16f17a0f1cd8",
            "avatarUrl": "/avatars/7c9bbcba69b823834eb0232da12cc7a9.svg",
            "isPro": false,
            "fullname": "chen",
            "user": "jiankang",
            "type": "user"
          },
          "name": "Jiankang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:51.408Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde353",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde354",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde355",
          "user": {
            "_id": "656453832bdaccfcd5379431",
            "avatarUrl": "/avatars/a0d764ce6b3fd05532c7a9cb2f263e33.svg",
            "isPro": false,
            "fullname": "Gao Ting",
            "user": "TingTingGao",
            "type": "user"
          },
          "name": "Tingting Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:35.059Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde356",
          "name": "Liang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T17:59:50.000Z",
      "submittedOnDailyAt": "2025-05-06T01:09:45.446Z",
      "title": "R1-Reward: 안정적인 강화학습을 통한 다모뎀 보상 모델의 훈련",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "다모뎔 모델 보상 모델(MRMs)은 다모뎔 모델 대 언어 모델(MLLMs)의 성능 향상에 중요한 역할을 수행하고 있습니다. 최근의 발전은 주로 모델 구조와 학습 데이터의 개선에 집중되어 왔지만, 장기적인 이유 능력의 효율성과 MRMs를 통해 이 능력을 활성화하는 방법 등에 대한 연구는 제한되어 있습니다. 본 논문에서는 보상 모델링에 대한 강화 학습(RL)의 활용 방법에 대해 검토합니다. 특히, 보상 모델링 문제를 규칙 기반의 RL 태스크로 재설정합니다. 그러나 현재의 RL 알고리즘(예: Reinforce++)를 직접 보상 모델링에 적용하면, 이 알고리즘의 내적한 한계를 통해 훈련 불안정성 또는 붕괴를招致합니다. 이러한 문제를 해결하기 위해, StableReinforce 알고리즘을 제안합니다. 이 알고리즘은 현재의 RL 방법의 훈련 손실, Advantage 추정 전략, 보상 설계를 개선하고, 이러한 개선은 훈련 동역학의 안정화와 높은 성능을 얻을 수 있습니다. MRM의 훈련을 지원하기 위해, 다양한 데이터 세트에서 200K의 선호 데이터를 수집합니다. StableReinforce 알고리즘으로 이 데이터 세트를 사용하여 훈련된 보상 모델, R1-Reward는 다모뎔 모델 보상 모델링 벤치마크에서 상당한 성능 향상을 얻을 수 있습니다. 이전의 SOTA 모델과 비교하여, VL Reward-Bench에서 8.4%의 향상률, Multimodal Reward Bench에서 14.3%의 향상률을 얻을 수 있습니다. 또한, 더 많은 추론 계산을 설정하면, R1-Reward의 성능이 발전하고, RL 알고리즘이 MRMs를 최적화하는 가능성을 밝혀줍니다.",
      "upvotes": 15,
      "discussionId": "6819762f64ae18f1b6fde387",
      "projectPage": "https://github.com/yfzhang114/r1_reward",
      "githubRepo": "https://github.com/yfzhang114/r1_reward",
      "ai_keywords": [
        "Multimodal Reward Models (MRMs)",
        "Multimodal Large Language Models (MLLMs)",
        "Reinforcement Learning (RL)",
        "rule-based RL task",
        "Reinforce++",
        "StableReinforce",
        "training loss",
        "advantage estimation strategy",
        "reward design",
        "preference data",
        "VL Reward-Bench",
        "Multimodal Reward Bench"
      ]
    },
    "publishedAt": "2025-05-05T13:59:50.000Z",
    "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning",
    "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3%\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02835.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02391",
      "authors": [
        {
          "_id": "6819a63c64ae18f1b60a5c43",
          "user": {
            "_id": "66f8689725464a7989b75845",
            "avatarUrl": "/avatars/43a61a528c5779103eaf5687ba44ee14.svg",
            "isPro": false,
            "fullname": "Jiarui Yao",
            "user": "FlippyDora",
            "type": "user"
          },
          "name": "Jiarui Yao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:24.344Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c44",
          "name": "Yifan Hao",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c45",
          "user": {
            "_id": "6470e0f1cfd57849519033a5",
            "avatarUrl": "/avatars/7ffefee3e36a4e37b9f4510bc6b689d1.svg",
            "isPro": false,
            "fullname": "Hanning Zhang",
            "user": "HanningZhang",
            "type": "user"
          },
          "name": "Hanning Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:20.459Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c46",
          "user": {
            "_id": "63a3ff69f91ad3ea5703841d",
            "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
            "isPro": false,
            "fullname": "Hanze Dong",
            "user": "hendrydong",
            "type": "user"
          },
          "name": "Hanze Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:27.200Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c47",
          "user": {
            "_id": "6319b29809baf858241f05de",
            "avatarUrl": "/avatars/29eef2c52814abea82e2aa9bf37a7f9c.svg",
            "isPro": false,
            "fullname": "Xiong",
            "user": "WeiXiong",
            "type": "user"
          },
          "name": "Wei Xiong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:34.381Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c48",
          "user": {
            "_id": "64b8922ca1827cc8d04ae919",
            "avatarUrl": "/avatars/0aaa83e3d09a82434e1d6af724aaa485.svg",
            "isPro": false,
            "fullname": "Nan Jiang",
            "user": "nanjiang",
            "type": "user"
          },
          "name": "Nan Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:47.400Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c49",
          "name": "Tong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:26:00.000Z",
      "submittedOnDailyAt": "2025-05-06T04:34:14.120Z",
      "title": "최적화된 체인오프스탄스 추론기로 인한 경사 분산 최소화\n거부 샘플링과 RL에서\n\n(Note: The original text seems to be a title or heading, and the translation maintains the structure and content of the original text.)",
      "submittedOnDailyBy": {
        "_id": "64d45451c34a346181b130dd",
        "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
        "isPro": false,
        "fullname": "Rui Yang",
        "user": "Ray2333",
        "type": "user"
      },
      "summary": "Chain-of-thought (CoT) 추론은 대규모 언어 모델 (LLMs)에서 잠재 변수 문제로 형식화될 수 있으며, 모델은 중간적인 이유 스텝을 생성하는 것이 필요합니다. 이전의 접근 방식과 같이, 반복적인 보상 평가의 미세 조정 (RAFT) 등 이러한 구성을 의존하지만, 일반적으로 Prompt 간에 일관된 추론 벡터를 적용하지만, 난이도와 수렴 행동의 변동을 고려하지 않습니다. 본 논문에서는 CoT 훈련의 주요 한계로, 정적 샘플링 전략에 의한 무작위 경사 추정의 유효화에 대한 인식을 합니다. GVM-RAFT를 제안하고, Prompt에 대응하는 동적인 샘플링 분배 전략을 설계하고, 계산 벡터 제약 아래 표준 편차를 최소화하는 것입니다. Prompt 수용률과 무작위 경사의 范数을 측정하여 계산 리소스를 동적으로 분배하고, 그 결과의 경사의 표준 편차를 최소화합니다. 이론적 분석에 따르면 제안된 동적인 샘플링 전략은 적절한 조건에서 가속화된 수렴 보장을 나타냅니다. 수학적인 이유에 대한 실험에서, GVM-RAFT는 vanilla RAFT보다 2-4배의 속도 향상과 유사한 정확도 향상을 나타냅니다. 제안된 동적인 샘플링 전략은 일반적인 것으로, GRPO 등 다른 강화 학습 알고리즘에도 통합할 수 있으며, 동일한 수렴과 테스트 정확도 향상을 얻을 수 있습니다. 코드는 https://github.com/RLHFlow/GVM에 공개되어 있습니다.",
      "upvotes": 15,
      "discussionId": "6819a63d64ae18f1b60a5c75",
      "ai_keywords": [
        "Chain-of-thought (CoT)",
        "latent variable problem",
        "iterative reward-ranked fine-tuning (RAFT)",
        "inference budget",
        "static sampling strategies",
        "GVM-RAFT",
        "Dynamic Sample Allocation Strategy",
        "prompt-specific",
        "computational budget constraint",
        "prompt acceptance rates",
        "stochastic gradient norms",
        "stochastic gradient variance",
        "accelerated convergence guarantees",
        "GRPO",
        "convergence",
        "test accuracy"
      ]
    },
    "publishedAt": "2025-05-05T02:26:00.000Z",
    "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL",
    "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d45451c34a346181b130dd",
      "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
      "fullname": "Rui Yang",
      "name": "Ray2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02222",
      "authors": [
        {
          "_id": "6819780dc3d212ad5b48cc07",
          "name": "Essential AI",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc09",
          "user": {
            "_id": "65ef97d0e5fc4abe66c05ed0",
            "avatarUrl": "/avatars/1d601a22639b3136bfb3519826451ddb.svg",
            "isPro": false,
            "fullname": "Ishaan Shah",
            "user": "ishaan-essential",
            "type": "user"
          },
          "name": "Ishaan Shah",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:24.884Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0a",
          "user": {
            "_id": "6675e3ed66c4fa6d0c10e229",
            "avatarUrl": "/avatars/f73c347d824a56079729c82d60d3edc3.svg",
            "isPro": false,
            "fullname": "Anthony Polloreno",
            "user": "ampolloreno",
            "type": "user"
          },
          "name": "Anthony M. Polloreno",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:47.445Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0b",
          "user": {
            "_id": "64d9ac38badf1110f7fcf030",
            "avatarUrl": "/avatars/c55c61af8dd52e6b4856684638b850a6.svg",
            "isPro": false,
            "fullname": "Karl Stratos",
            "user": "karlstratos",
            "type": "user"
          },
          "name": "Karl Stratos",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:53.826Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0c",
          "user": {
            "_id": "66622dacec18341b268f97a6",
            "avatarUrl": "/avatars/8bc7d6a7c28c83aacdbeeb770716b1c0.svg",
            "isPro": false,
            "fullname": "Philip Monk",
            "user": "monk-essential",
            "type": "user"
          },
          "name": "Philip Monk",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:59.992Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0d",
          "user": {
            "_id": "67bfd6daca6e3c22b6de31ee",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/TTt6_o9tYEgeWK26DgI_7.png",
            "isPro": false,
            "fullname": "Adarsh Chaluvaraju",
            "user": "cadarsh-essential",
            "type": "user"
          },
          "name": "Adarsh Chaluvaraju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:02:16.867Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0e",
          "user": {
            "_id": "6408e4f93461c51cf7345060",
            "avatarUrl": "/avatars/328b508e2de9e50dca2412adeb3542f5.svg",
            "isPro": false,
            "fullname": "Andrew Hojel",
            "user": "andrewhojel",
            "type": "user"
          },
          "name": "Andrew Hojel",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:02:24.655Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0f",
          "user": {
            "_id": "62e24efc3a616d16e2f426ea",
            "avatarUrl": "/avatars/a2433c971f80e6cf738c03e843666cff.svg",
            "isPro": false,
            "fullname": "Andrew Ma",
            "user": "AndrewMa",
            "type": "user"
          },
          "name": "Andrew Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:02:30.884Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc10",
          "name": "Anil Thomas",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc11",
          "name": "Ashish Tanwer",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc12",
          "name": "Darsh J Shah",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc13",
          "user": {
            "_id": "67ed7aa9290a7f9d33113fb5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jMMTObGZktuXWJ3wVVSxj.png",
            "isPro": false,
            "fullname": "Khoi Nguyen",
            "user": "KTLK",
            "type": "user"
          },
          "name": "Khoi Nguyen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:20.971Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc14",
          "name": "Kurt Smith",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc15",
          "name": "Michael Callahan",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc16",
          "user": {
            "_id": "66cd078ea796074d428fde0f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qThw3H6ukqIAuRy7aJTN1.jpeg",
            "isPro": false,
            "fullname": "Michael Pust",
            "user": "essentialpust",
            "type": "user"
          },
          "name": "Michael Pust",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:14.427Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc17",
          "user": {
            "_id": "674c2737d369a6de1f8f58e1",
            "avatarUrl": "/avatars/40af3aa9b9d574cc63dc328c3a465fff.svg",
            "isPro": false,
            "fullname": "Parmar Mohit",
            "user": "mohitparmar",
            "type": "user"
          },
          "name": "Mohit Parmar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:21.257Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc18",
          "name": "Peter Rushton",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc19",
          "user": {
            "_id": "67f4ced58c4cbc2f5d95cd17",
            "avatarUrl": "/avatars/3f440a59c38f5c0c7a77746ef54ed0a5.svg",
            "isPro": false,
            "fullname": "Platon Mazarakis",
            "user": "Platona",
            "type": "user"
          },
          "name": "Platon Mazarakis",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:32.983Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1a",
          "user": {
            "_id": "654bdcf2e06d25def57cc54b",
            "avatarUrl": "/avatars/2d2612bd7072edd60876b504345fbf25.svg",
            "isPro": false,
            "fullname": "Ritvik Kapila",
            "user": "rkapila",
            "type": "user"
          },
          "name": "Ritvik Kapila",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:51.154Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1b",
          "name": "Saurabh Srivastava",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1c",
          "user": {
            "_id": "679bc0b23e12a166672e5275",
            "avatarUrl": "/avatars/fe9d0e79c21c9d3594420e69e3809f0f.svg",
            "isPro": false,
            "fullname": "Somanshu Singla",
            "user": "somanshu-essential",
            "type": "user"
          },
          "name": "Somanshu Singla",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:27.349Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1d",
          "user": {
            "_id": "67101a7165442ddc48cb4b07",
            "avatarUrl": "/avatars/551777fcd1638998ad9fd16804b313ec.svg",
            "isPro": false,
            "fullname": "Tim Romanski",
            "user": "tim-essential",
            "type": "user"
          },
          "name": "Tim Romanski",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:09.387Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1e",
          "user": {
            "_id": "66f5f3f99ef08fe3c1f4c35a",
            "avatarUrl": "/avatars/41b8b6f90eb87e685b74587317296a1b.svg",
            "isPro": false,
            "fullname": "Yash Vanjani",
            "user": "yash-essential",
            "type": "user"
          },
          "name": "Yash Vanjani",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:16.859Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1f",
          "name": "Ashish Vaswani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T19:14:43.000Z",
      "submittedOnDailyAt": "2025-05-06T07:30:25.714Z",
      "title": "μ子의 예비 학습의 실용적인 효율",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "モーン（Muon）는 가장 간단한 2nd-stage 모듈 최종형으로 가장 효율적인 최적화기입니다. AdamW보다 明確に贪欲 프론티어의 계산 시간 부담을 확장하고 있으며, AdamW보다 효과적이고 효율적이며, 큰 배치 크기를 사용하여 데이터 효율성을 유지함으로써 더 효율적인 훈련을 가능하게 합니다. 이로 인해, モーン은 자원 효율적인 훈련을 가능하게 합니다. 또한, モーン와 최대 업데이트 파라미터화（muP）의 조합에 대해 효율적인 超パラメータ 転移을 조사하고, muP의 모든 오차의 원인을 고려하면서, 자원의 手動 오버헤드 를 최소화하는 효율적인 알고리즘을 제안했습니다. 이러한 발견을 검증하기 위해, 400억 매개변수 이상의 모델 크기 실험과 데이터 분포 및 아키텍처 제거 테스트를 수행했습니다.",
      "upvotes": 15,
      "discussionId": "6819780fc3d212ad5b48cc89",
      "ai_keywords": [
        "second-order optimizer",
        "Pareto frontier",
        "AdamW",
        "data efficiency",
        "critical batch size",
        "computationally efficient",
        "maximal update parameterization",
        "telescoping algorithm",
        "hyperparameter transfer",
        "error sources",
        "model sizes",
        "data distribution",
        "architecture"
      ]
    },
    "publishedAt": "2025-05-04T15:14:43.000Z",
    "title": "Practical Efficiency of Muon for Pretraining",
    "summary": "We demonstrate that Muon, the simplest instantiation of a second-order\noptimizer, explicitly expands the Pareto frontier over AdamW on the\ncompute-time tradeoff. We find that Muon is more effective than AdamW in\nretaining data efficiency at large batch sizes, far beyond the so-called\ncritical batch size, while remaining computationally efficient, thus enabling\nmore economical training. We study the combination of Muon and the maximal\nupdate parameterization (muP) for efficient hyperparameter transfer and present\na simple telescoping algorithm that accounts for all sources of error in muP\nwhile introducing only a modest overhead in resources. We validate our findings\nthrough extensive experiments with model sizes up to four billion parameters\nand ablations on the data distribution and architecture.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02222.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6784
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02094",
      "authors": [
        {
          "_id": "681992911e0fae3880173d43",
          "user": {
            "_id": "66d59dc9b005ad82ca6fc61d",
            "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
            "isPro": false,
            "fullname": "Runyi YU",
            "user": "IngridYU",
            "type": "user"
          },
          "name": "Runyi Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:00:09.333Z",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d44",
          "name": "Yinhuai Wang",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d45",
          "user": {
            "_id": "64341911546e16f17a129733",
            "avatarUrl": "/avatars/ae12aafc8932a7537838e6d3964858cb.svg",
            "isPro": false,
            "fullname": "QiHan Zhao",
            "user": "Crimnos",
            "type": "user"
          },
          "name": "Qihan Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:00:32.182Z",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d46",
          "name": "Hok Wai Tsui",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d47",
          "name": "Jingbo Wang",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d48",
          "name": "Ping Tan",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d49",
          "user": {
            "_id": "6467b121e7a6a374fd19b44b",
            "avatarUrl": "/avatars/3f2874d58986d651aef55e3408b05700.svg",
            "isPro": false,
            "fullname": "Qifeng Chen",
            "user": "cqf",
            "type": "user"
          },
          "name": "Qifeng Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:21.751Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T13:00:29.000Z",
      "submittedOnDailyAt": "2025-05-06T03:11:28.738Z",
      "title": "スキルミモク-V2: 희귀하고 노이즈가 많은 데모레이션으로부터 강건하고 일반화 가능한 상호작용스킬을 학습한다.",
      "submittedOnDailyBy": {
        "_id": "66d59dc9b005ad82ca6fc61d",
        "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
        "isPro": false,
        "fullname": "Runyi YU",
        "user": "IngridYU",
        "type": "user"
      },
      "summary": "우리는 상호작용 시뮬레이션을 통해 강화 학습(RLID)에서의 근본적인 도전을 다루고 있습니다: 시뮬레이션의 노이즈와 커버리지의 한계. 기존의 데이터 수집 접근법은 유용한 상호작용 시뮬레이션을 제공하지만, 일반적으로 희소한, 연결되지 않은, 노이즈가 많은 경로를 생성하여 가능한 기술의 변화 범위와 전환을 완전히 캡처하지 못합니다. 우리의 핵심 통찰은 노이즈와 희소한 시뮬레이션이 존재하더라도, 무한한 물리적으로 가능한 경로가 존재하며, 시뮬레이션 기술 사이에서 자연스럽게 연결되어 있으며, 이러한 경로는 가능한 기술의 변화와 전환의 연속적인 공간을 형성합니다. 이 통찰을 기반으로, 우리는 두 가지 데이터 증강 기술에 대해 제시합니다: Stitched Trajectory Graph (STG)는 시뮬레이션 기술 사이에서 잠재적인 전환을 발견하고, State Transition Field (STF)는 시뮬레이션 이웃 영역 내 임의의 상태에 대해 고유한 연결을 구축합니다. 증강된 데이터를 사용하여 효과적인 RLID를 가능하게 하기 위해, 우리는 동적 커리큘럼 생성을 위한 적응적인 경로 샘플링(ATS) 전략과 메모리에 의존하는 기술 학습을 위한 역사적 인코딩 메커니즘을 개발합니다. 우리의 접근법은 강력한 기술 획득을 가능하게 하며, 참조 시뮬레이션을 초과하는 상당한 일반화에서 크게 개선됩니다. 다양한 상호작용 태스크를 대상으로 광범위한 실험을 통해, 수렴 안정성, 일반화 능력, 회복 강건성을 고려하여 최신 방법 대비 상당한 개선을 보여주었습니다.",
      "upvotes": 12,
      "discussionId": "681992931e0fae3880173dcf",
      "ai_keywords": [
        "Reinforcement Learning from Interaction Demonstration (RLID)",
        "demonstration noise",
        "coverage limitations",
        "interaction demonstrations",
        "sparse trajectories",
        "disconnected trajectories",
        "noise",
        "skill variations",
        "transitions",
        "physically feasible trajectories",
        "Stitched Trajectory Graph (STG)",
        "State Transition Field (STF)",
        "Adaptive Trajectory Sampling (ATS)",
        "dynamic curriculum generation",
        "historical encoding mechanism",
        "skill acquisition",
        "convergence stability",
        "generalization capability",
        "recovery robustness"
      ]
    },
    "publishedAt": "2025-05-04T09:00:29.000Z",
    "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from\n  Sparse and Noisy Demonstrations",
    "summary": "We address a fundamental challenge in Reinforcement Learning from Interaction\nDemonstration (RLID): demonstration noise and coverage limitations. While\nexisting data collection approaches provide valuable interaction\ndemonstrations, they often yield sparse, disconnected, and noisy trajectories\nthat fail to capture the full spectrum of possible skill variations and\ntransitions. Our key insight is that despite noisy and sparse demonstrations,\nthere exist infinite physically feasible trajectories that naturally bridge\nbetween demonstrated skills or emerge from their neighboring states, forming a\ncontinuous space of possible skill variations and transitions. Building upon\nthis insight, we present two data augmentation techniques: a Stitched\nTrajectory Graph (STG) that discovers potential transitions between\ndemonstration skills, and a State Transition Field (STF) that establishes\nunique connections for arbitrary states within the demonstration neighborhood.\nTo enable effective RLID with augmented data, we develop an Adaptive Trajectory\nSampling (ATS) strategy for dynamic curriculum generation and a historical\nencoding mechanism for memory-dependent skill learning. Our approach enables\nrobust skill acquisition that significantly generalizes beyond the reference\ndemonstrations. Extensive experiments across diverse interaction tasks\ndemonstrate substantial improvements over state-of-the-art methods in terms of\nconvergence stability, generalization capability, and recovery robustness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02094.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d59dc9b005ad82ca6fc61d",
      "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
      "fullname": "Runyi YU",
      "name": "IngridYU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02156",
      "authors": [
        {
          "_id": "681975a9fdcf582e6d0effdb",
          "user": {
            "_id": "64bcc373ef8c0e42bf16acc5",
            "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
            "isPro": false,
            "fullname": "mz.w",
            "user": "iiiiwis",
            "type": "user"
          },
          "name": "Minzheng Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:29.678Z",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdc",
          "user": {
            "_id": "66641b2fd8e1e34bc621e688",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66641b2fd8e1e34bc621e688/csPETwnx2zCIHSWi9uAi-.png",
            "isPro": false,
            "fullname": "Yongbin Li",
            "user": "Yongbin-Li",
            "type": "user"
          },
          "name": "Yongbin Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:05:06.051Z",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdd",
          "name": "Haobo Wang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effde",
          "name": "Xinghua Zhang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdf",
          "name": "Nan Xu",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe0",
          "user": {
            "_id": "668bd45044ab5de7e4c5b1e7",
            "avatarUrl": "/avatars/9b087cfcac65a649a12568b601d5ca53.svg",
            "isPro": false,
            "fullname": "bingli wu",
            "user": "bingliwu",
            "type": "user"
          },
          "name": "Bingli Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:05:37.861Z",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe1",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe2",
          "name": "Haiyang Yu",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe3",
          "name": "Wenji Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T15:39:58.000Z",
      "submittedOnDailyAt": "2025-05-06T01:07:27.275Z",
      "title": "Think on Your Feet: Social Agents의 리뉴アル 로딩에 의한 적응적 사고\n\n(注意：原文中的“ソーシャルアガント”在韩语中通常翻译为“사회적 에이전트”或“사회 에이전트”，这里选择了“사회적 에이전트”以保持专业性和准确性。)",
      "submittedOnDailyBy": {
        "_id": "64bcc373ef8c0e42bf16acc5",
        "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
        "isPro": false,
        "fullname": "mz.w",
        "user": "iiiiwis",
        "type": "user"
      },
      "summary": "효과적인 사회인 지능 시뮬레이션에 있어서, 언어 에이전트는 이유의 깊이를 동적으로 조정하는 능력이 필요합니다. 현재의 접근 방식에서는 이러한 이유의 능력이 특히 부족합니다. 기존의 방법은 이러한 이유의 능력이 부족하거나, 모든 시나리오에서 일관된 긴 쉼표 연결된 감정을 강제하는 것입니다. 이로 인해 토큰의 사용이 과도하고, 적절한 사회인 시뮬레이션이 되지 않을 경우가 있습니다. 본 논문에서는 시간적 맥락에 기반하여 4가지의 사고 모드(직감적인 반응 → 깊은 생각)에서 전략적으로 선택하는 Adaptive Mode Learning(AML)을 제안합니다. 이 프레임워크의 핵심적인 혁신으로, Adaptive Mode Policy Optimization(AMPO) 알고리즘은 기존의 방법에서 3가지의 발전을 도입합니다. 이는 (1) 다粒도적인 사고 모드의 설계, (2) 사회인 상호작용의 맥락에 대한 모드의 변경, (3) 토큰 효율적인 이유에 의한 깊이 적응 처리입니다. 사회인 지능 태스크에서 광범위한 실험을 수행하였으며, AML은 가장 先端한 방법보다 15.6% 더 높은 태스크 성능을 구현하였습니다. 특히, 우리 방법은 GRPO를 7.0% 더 우수하게, 이유의 쉼표가 32.8% 더 짧아집니다. 이러한 결과를 통해, AMPO로 구현된 맥락적 사고 모드 선택은 GRPO의 고정된 깊이 접근보다 인간처럼 적응적인 이유를 가능하게 합니다.",
      "upvotes": 11,
      "discussionId": "681975a9fdcf582e6d0f0014",
      "githubRepo": "https://github.com/MozerWang/AMPO",
      "ai_keywords": [
        "Adaptive Mode Learning (AML)",
        "Adaptive Mode Policy Optimization (AMPO)",
        "multi-granular thinking mode design",
        "context-aware mode switching",
        "token-efficient reasoning",
        "depth-adaptive processing",
        "intuitive reaction",
        "deep contemplation",
        "social interaction",
        "reasoning chains",
        "fixed-depth approach"
      ]
    },
    "publishedAt": "2025-05-04T11:39:58.000Z",
    "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents",
    "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\napproaches. While existing methods either lack this kind of reasoning\ncapability or enforce uniform long chain-of-thought reasoning across all\nscenarios, resulting in excessive token usage and inappropriate social\nsimulation. In this paper, we propose Adaptive Mode\nLearning (AML) that strategically selects from four\nthinking modes (intuitive reaction rightarrow deep contemplation) based on\nreal-time context. Our framework's core innovation, the Adaptive\nMode Policy Optimization (AMPO)\nalgorithm, introduces three key advancements over existing methods: (1)\nMulti-granular thinking mode design, (2) Context-aware mode switching across\nsocial interaction, and (3) Token-efficient reasoning via depth-adaptive\nprocessing. Extensive experiments on social intelligence tasks confirm that AML\nachieves 15.6% higher task performance than state-of-the-art methods. Notably,\nour method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These\nresults demonstrate that context-sensitive thinking mode selection, as\nimplemented in AMPO, enables more human-like adaptive reasoning than GRPO's\nfixed-depth approach",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02156.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bcc373ef8c0e42bf16acc5",
      "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
      "fullname": "mz.w",
      "name": "iiiiwis",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01658",
      "authors": [
        {
          "_id": "6819950bd55db085708dd2e5",
          "user": {
            "_id": "670cb786e73576f33a339144",
            "avatarUrl": "/avatars/c172887c32878aebafd786061680ea1e.svg",
            "isPro": false,
            "fullname": "Sihyeong Park",
            "user": "inputsh",
            "type": "user"
          },
          "name": "Sihyeong Park",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:09:00.902Z",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e6",
          "name": "Sungryeol Jeon",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e7",
          "user": {
            "_id": "64aaa12a04e7b379fed24327",
            "avatarUrl": "/avatars/327482e569c24ee4c97064f07ddd6de7.svg",
            "isPro": false,
            "fullname": "Chaelyn Lee",
            "user": "oos2",
            "type": "user"
          },
          "name": "Chaelyn Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:09:13.239Z",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e8",
          "user": {
            "_id": "6719f17ac5837d514cfff13b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/GnL4RCj7xncVhCIFN5y35.png",
            "isPro": false,
            "fullname": "Seokhun Jeon",
            "user": "Devcow",
            "type": "user"
          },
          "name": "Seokhun Jeon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:09:18.870Z",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e9",
          "name": "Byung-Soo Kim",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2ea",
          "user": {
            "_id": "65b9dee19c4955ae7aee4954",
            "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
            "isPro": false,
            "fullname": "Jemin Lee",
            "user": "leejaymin",
            "type": "user"
          },
          "name": "Jemin Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:52.950Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-03T02:47:43.000Z",
      "submittedOnDailyAt": "2025-05-06T03:21:53.083Z",
      "title": "대 언어 모델의 추론 엔진에 대한 조사: 최적화와 효율성의 관점에서",
      "submittedOnDailyBy": {
        "_id": "65b9dee19c4955ae7aee4954",
        "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
        "isPro": false,
        "fullname": "Jemin Lee",
        "user": "leejaymin",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 챗봇, 코드 생성 서비스, 검색 엔진 등 다양한 분야에 광범위하게 적용되고 있습니다. Chain Short, 복잡한 논리, 에이전트 서비스 등 복잡한 작업은 모델을 재발견하여 추론 비용을 크게 증가시킵니다. 병렬화, 압축, 캐시 등 최적화 방법들이 도입되어 있으며, 이러한 방법을 선택하는 것은 다양한 서비스의 요구에 따라 어려워질 수 있습니다. 최근, 특화된 LLM 추론 엔진은 서비스에 대한 최적화 방법들을 통합하기 위해 중요한 구성 요소로 등장했습니다. 그러나 추론 엔진에 대한 체계적인 연구는 아직 부족합니다. 본 논문에서는 25개의 오픈 소스 및 상업적 추론 엔진에 대한 상세한 평가를 제공합니다. 각 추론 엔진의 사용 방법, 배포 방법, 일반적인 용어 지원, scalability, transportability, latin script에 대한 계산에 적합한 특성에 대해 조사합니다. 또한 각 추론 엔진의 설계 목표를 명확히 하기 위해 지원되는 최적화 기술들을 조사합니다. 또한 오픈 소스 추론 엔진의 생태계의 성숙도를 평가하고, 상업적 솔루션의 성능과 비용 정책도 평가합니다. 향후 연구 방향을 명확히하고, 복잡한 LLM 기반의 서비스와 다양한 하드웨어의 지원, 보안 향상을 포함한, 연구자와 개발자에게 실용적인 가이드라인을 제공합니다. 또한 이 빠르게 발전하는 분야의 개발을 지속적으로 추적하기 위해 공개 리포지토리를 제공합니다: https://github.com/sihyeong/Awesome-LLM-Inference-Engine",
      "upvotes": 9,
      "discussionId": "6819950cd55db085708dd32a",
      "ai_keywords": [
        "chain-of-thought",
        "complex reasoning",
        "agent services",
        "inference cost",
        "parallelism",
        "compression",
        "caching",
        "LLM inference engines",
        "ease-of-use",
        "ease-of-deployment",
        "general-purpose support",
        "scalability",
        "throughput-aware computation",
        "latency-aware computation",
        "optimization techniques",
        "ecosystem maturity",
        "performance",
        "cost policy",
        "LLM-based services",
        "enhanced security"
      ]
    },
    "publishedAt": "2025-05-02T22:47:43.000Z",
    "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
    "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01658.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b9dee19c4955ae7aee4954",
      "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
      "fullname": "Jemin Lee",
      "name": "leejaymin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01441",
      "authors": [
        {
          "_id": "68198aea57d4de18fb3e69d6",
          "user": {
            "_id": "61ffaa2943eb0913fa2df74a",
            "avatarUrl": "/avatars/a19971f830abb8a8ae95e5800beb9fcd.svg",
            "isPro": false,
            "fullname": "Singh",
            "user": "joykirat",
            "type": "user"
          },
          "name": "Joykirat Singh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:08.255Z",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d7",
          "user": {
            "_id": "622ca32345261ac5cc0bdade",
            "avatarUrl": "/avatars/7e1d633be69cf86a3affb9168b1cc27b.svg",
            "isPro": false,
            "fullname": "Raghav Magazine",
            "user": "Raghav2002",
            "type": "user"
          },
          "name": "Raghav Magazine",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:06:32.498Z",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d8",
          "user": {
            "_id": "64aba383fddf117e6e5ba818",
            "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
            "isPro": false,
            "fullname": "Akshay  Nambi",
            "user": "akshaynambi",
            "type": "user"
          },
          "name": "Yash Pandya",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-06T04:08:10.843Z",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d9",
          "user": {
            "_id": "64aba383fddf117e6e5ba818",
            "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
            "isPro": false,
            "fullname": "Akshay  Nambi",
            "user": "akshaynambi",
            "type": "user"
          },
          "name": "Akshay Nambi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:06:46.024Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T10:42:49.000Z",
      "submittedOnDailyAt": "2025-05-06T02:43:42.049Z",
      "title": "Agentic Reasoning와 Tool Integration을 LLMs에 적용하는 강화학습을 통해 실현하는 방법",
      "submittedOnDailyBy": {
        "_id": "64aba383fddf117e6e5ba818",
        "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
        "isPro": false,
        "fullname": "Akshay  Nambi",
        "user": "akshaynambi",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 복잡한 논리 문제에서 놀라움을 초래하지만, 고정된 내부 지식과 문맥만 기반으로 논리를 수행하기 때문에 근본적인 한계가 있습니다. 실제 세계적인 문제 해결은 동적인, 다단계의 논리, 적응적인 결정, 외부 도구와 환경과의 상호 작용의 능력을 요구합니다. 본 논문에서는 Agentic Reasoning and Tool Integration in Self-improving Transformers(ARTIST)라는 통합적인 프레임워크를 소개합니다. 이 프레임워크는 LLMs의 에이전트 논리, 강화 학습, 도구 통합을 엄격하게 결합하고 있습니다. ARTIST는 다단계 논리 체인 내의 특정 시점과 방법, 어떤 도구를 호출할 지 자동으로 결정할 수 있으며, 결과 기반의 강화 학습을 통해 도구 사용과 환경과의 상호 작용의 강력한 전략을 학습할 수 있습니다. 수학적 논리와 다단계 함수 호출 벤치마크에서의 확산 실험은 ARTIST가 가장 선진적인 기준을 일치하고, 기본 모델보다 최대 22%의 절대적인 향상을 나타내며, 가장 어려운 태스크에서도 강력한 향상을 나타냅니다. 세부적인 연구와 메트릭 분석은 에이전트 기반의 강화 학습이 깊은 논리를 학습하고, 더 효과적인 도구 사용, 고품질의 해결책 구현을 보여주었습니다. 우리의 결과는 LLMs에서 견고한, 해석 가능한, 일반화 가능한 문제 해결의 새로운 발전 방향으로, 도구 통합과 강화 학습을 조합한 것을 강력한 새로운 지평으로 확립했습니다.",
      "upvotes": 9,
      "discussionId": "68198aec57d4de18fb3e6a30",
      "projectPage": "https://www.microsoft.com/en-us/research/people/akshayn/unlocking-agentic-reasoning-in-llms/",
      "ai_keywords": [
        "agentic reasoning",
        "reinforcement learning",
        "tool integration",
        "ARTIST",
        "multi-turn reasoning chains",
        "outcome-based RL",
        "mathematical reasoning",
        "function calling",
        "agentic RL",
        "tool use",
        "environment interaction"
      ]
    },
    "publishedAt": "2025-04-28T06:42:49.000Z",
    "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement\n  Learning",
    "summary": "Large language models (LLMs) have achieved remarkable progress in complex\nreasoning tasks, yet they remain fundamentally limited by their reliance on\nstatic internal knowledge and text-only reasoning. Real-world problem solving\noften demands dynamic, multi-step reasoning, adaptive decision making, and the\nability to interact with external tools and environments. In this work, we\nintroduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving\nTransformers), a unified framework that tightly couples agentic reasoning,\nreinforcement learning, and tool integration for LLMs. ARTIST enables models to\nautonomously decide when, how, and which tools to invoke within multi-turn\nreasoning chains, leveraging outcome-based RL to learn robust strategies for\ntool use and environment interaction without requiring step-level supervision.\nExtensive experiments on mathematical reasoning and multi-turn function calling\nbenchmarks show that ARTIST consistently outperforms state-of-the-art\nbaselines, with up to 22% absolute improvement over base models and strong\ngains on the most challenging tasks. Detailed studies and metric analyses\nreveal that agentic RL training leads to deeper reasoning, more effective tool\nuse, and higher-quality solutions. Our results establish agentic RL with tool\nintegration as a powerful new frontier for robust, interpretable, and\ngeneralizable problem-solving in LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01441.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64aba383fddf117e6e5ba818",
      "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
      "fullname": "Akshay  Nambi",
      "name": "akshaynambi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02370",
      "authors": [
        {
          "_id": "68197c200e4203d6bc84cdfb",
          "user": {
            "_id": "637f0eb22438d7485b8ef5d7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
            "isPro": false,
            "fullname": "Ming Li",
            "user": "limingcv",
            "type": "user"
          },
          "name": "Ming Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:18.243Z",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfc",
          "name": "Xin Gu",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfd",
          "name": "Fan Chen",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfe",
          "user": {
            "_id": "64ca92f738837b12d5f63729",
            "avatarUrl": "/avatars/a361be3a5ccf9368717980d1faf69df0.svg",
            "isPro": false,
            "fullname": "Xiaoying Xing",
            "user": "xiaoying0505",
            "type": "user"
          },
          "name": "Xiaoying Xing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:06:58.770Z",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdff",
          "user": {
            "_id": "644df7eacfb40c94eae71186",
            "avatarUrl": "/avatars/1daa4967efd34d54c59aa95970093dbd.svg",
            "isPro": false,
            "fullname": "Longyin Wen",
            "user": "lionwen",
            "type": "user"
          },
          "name": "Longyin Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:07:05.196Z",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84ce00",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84ce01",
          "user": {
            "_id": "65cbdea6d6c974694f09249a",
            "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
            "isPro": false,
            "fullname": "Sijie Zhu",
            "user": "Zilence006",
            "type": "user"
          },
          "name": "Sijie Zhu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-06T03:04:04.536Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T05:19:40.000Z",
      "submittedOnDailyAt": "2025-05-06T01:34:41.608Z",
      "title": "スーパー エディット：インストラクション 기반의 이미지 편집의 시청 조정과 효율화",
      "submittedOnDailyBy": {
        "_id": "637f0eb22438d7485b8ef5d7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
        "isPro": false,
        "fullname": "Ming Li",
        "user": "limingcv",
        "type": "user"
      },
      "summary": "데이터셋의 구축 과정에서, 직접적인 정확한 편집 데이터의 수집이 어려워서, 현재의 데이터셋은 일반적으로 다양한 자동화 방법들을 사용하여 구축되어 있습니다. 이로 인해, 편집 지시와 원 이미지, 편집 후 이미지의 매칭으로 인한 노이즈로 인해 불순한 서브젝션 신호가 발생합니다. 최근의 노력은, 편집 모델의 향상을 위해 고품질의 편집 이미지의 생성, 인식 태스크의 사전 학습, 시각 언어 모델(VLMs)의 도입을 시도하고 있지만, 이 기본적인 문제를 해결하지 못했습니다. 본 논문에서는, 주어진 이미지 쌍에 대해 더 효과적인 편집 지시를 구축하는 새로운 해결책을 제안합니다. 이는 편집 지시를 정확화하고, 원 이미지, 편집 후 이미지로 더욱 잘 맞는 것, 비교적인 편집 지시를 사용하여 그 효과를 더욱 높일 것을 포함합니다. 구체적으로는, 편집 모델은 문맥에 의존하지 않도록, 추론 단계마다 특정한 생성 속성을 보여주고 있습니다. 이러한 선행 속성을 기반으로, VLMs에 대한 편집 지시를 정확화하기 위한 통일된 가이드를 정의합니다. 그러나 일부 어려운 편집 시나리오는 이러한 정확화된 지시만으로 해결되지 않는 경우가 있습니다. 따라서, 긍정적인 지시와 부정적인 지시를 사용하여 비교적인 서브젝션 신호를 구축하고, 이들을 튜닝을 위해 튜플 손실을 사용하는 딥러닝 모델의 훈련에 삽입하여, 더 효과적인 서브젝션을 촉진하는 것을 시도합니다. 우리 방법은, 이전 연구에서 사용된 VLM 모듈이나 사전 학습 태스크를 필요로 하지 않습니다. 더 직접적이고 효율적인 방법で, 더 좋은 서브젝션 신호를 제공하며, 명령 기반의 이미지 편집에 새로운, 간단하고 효과적인 해결책을 제공합니다. 다수의 벤치마크에서의 결과를 통해, 우리 방법이 현재의 접근보다 현저히 뛰어납니다. 이전의 SOTA SmartEdit과 비교하여, Real-Edit 벤치마크에서 30배 적은 학습 데이터와 13배 작은 모델 크기로 9.19%의 향상을 달성했습니다.",
      "upvotes": 8,
      "discussionId": "68197c240e4203d6bc84cee9",
      "projectPage": "https://liming-ai.github.io/SuperEdit/",
      "githubRepo": "https://github.com/bytedance/SuperEdit",
      "ai_keywords": [
        "contrastive editing instructions",
        "triplet loss",
        "instruction-based image editing",
        "contrastive supervision signals",
        "generation attributes",
        "unified guide",
        "vision-language models (VLMs)",
        "real-edit benchmark",
        "smartedit"
      ]
    },
    "publishedAt": "2025-05-05T01:19:40.000Z",
    "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing",
    "summary": "Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f0eb22438d7485b8ef5d7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
      "fullname": "Ming Li",
      "name": "limingcv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01043",
      "authors": [
        {
          "_id": "68196e23d9cad0bb5c90dd9b",
          "user": {
            "_id": "64a62e3302e46deb19a7937e",
            "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
            "isPro": false,
            "fullname": "Zhiwei Hao",
            "user": "Zhiwei840",
            "type": "user"
          },
          "name": "Zhiwei Hao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:34:14.150Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9c",
          "user": {
            "_id": "65c4a574d2db41f74ab2a808",
            "avatarUrl": "/avatars/997a8a51996e909eeb318dc592b6c67a.svg",
            "isPro": false,
            "fullname": "Jianyuan Guo",
            "user": "GGJY",
            "type": "user"
          },
          "name": "Jianyuan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:08:12.386Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9d",
          "name": "Li Shen",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9e",
          "user": {
            "_id": "6306dc1fd37ce67e0e53c202",
            "avatarUrl": "/avatars/d53a29925511a516495b1597fd5dc764.svg",
            "isPro": false,
            "fullname": "Yong Luo",
            "user": "csdvT",
            "type": "user"
          },
          "name": "Yong Luo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:08:19.043Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9f",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda0",
          "user": {
            "_id": "662520a75480987954af60b5",
            "avatarUrl": "/avatars/75d2509d21901c4bb187e93b23540e19.svg",
            "isPro": false,
            "fullname": "Guoxia Wang",
            "user": "Guoxia",
            "type": "user"
          },
          "name": "Guoxia Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:08:27.139Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda1",
          "name": "Dianhai Yu",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda2",
          "name": "Yonggang Wen",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda3",
          "name": "Dacheng Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-02T06:33:25.000Z",
      "submittedOnDailyAt": "2025-05-06T00:36:54.063Z",
      "title": "대규모 언어 모델의 저정밀도 훈련: 방법, 문제점 및 기회",
      "submittedOnDailyBy": {
        "_id": "64a62e3302e46deb19a7937e",
        "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
        "isPro": false,
        "fullname": "Zhiwei Hao",
        "user": "Zhiwei840",
        "type": "user"
      },
      "summary": "대 언어 모형(LLMs)은 다양한 분야에서 놀라울만한 성능을 달성했습니다. 그러나 이러한 모형의 훈련에 필요한 대규모 하드웨어 자원은 효율성과 scalability에 큰 장애가 되어 있습니다. 이挑戦를 완화하기 위해, 저 정밀도 훈련 기술이 광범위하게 도입되어 훈련 효율에 대한 뚜렷한 진전을 거쳤습니다. 이러한 효과에 더해, 저 정밀도 훈련은 가중치, 활성화, 경사 등 여러 구성 요소를 포함하며, 각각은 다른 숫자 형식으로 표현할 수 있습니다. 이러한 다양성은 저 정밀도 훈련 연구의 분열된 가치관을 창출하고, 연구자들이 이 분야에 대한 일관된 개요를 얻을 수 있게 되었습니다. 이 조사에서는 현재의 저 정밀도 훈련 방법들에 대해 상세한 리뷰를 제공합니다. 이러한 방법을 체계적으로 정리하기 위해, 하드웨어의 호환성, 계산 효율성, 그리고 독자적인 읽기 쉬운성을 고려한 기초적인 숫자 형식에 기반하여 3가지의 주요 그룹으로 분류합니다. 이 그룹들은 (1) 고정점 및 정수 기반의 방법, (2) 부동 소수점 기반의 방법, (3) 사용자 정의 형식 기반의 방법을 포함합니다. 또한, 정규화 훈련 접근 방식을 논의하고, 이 접근 방식은 진행 방향의 훈련과 저 정밀도 훈련의 관련성을 보여주고 있습니다. 마지막으로, 이 분야를 발전시키기 위한 많은 연구 방향을 제시합니다. 이 조사에서 논의된 논문의 컬렉션은 https://github.com/Hao840/Awesome-Low-Precision-Training에 제공됩니다.",
      "upvotes": 8,
      "discussionId": "68196e24d9cad0bb5c90de08",
      "githubRepo": "https://github.com/Hao840/Awesome-Low-Precision-Training",
      "ai_keywords": [
        "low-precision training",
        "weights",
        "activations",
        "gradients",
        "fixed-point",
        "integer-based methods",
        "floating-point-based methods",
        "customized format-based methods",
        "quantization-aware training"
      ]
    },
    "publishedAt": "2025-05-02T02:33:25.000Z",
    "title": "Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities",
    "summary": "Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several componentsx2013such\nas weights, activations, and gradientsx2013each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01043.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a62e3302e46deb19a7937e",
      "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
      "fullname": "Zhiwei Hao",
      "name": "Zhiwei840",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02471",
      "authors": [
        {
          "_id": "681973cfa70a4728958323aa",
          "user": {
            "_id": "644fcbea4f7316588267dc80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
            "isPro": false,
            "fullname": "Biao Gong",
            "user": "BiaoGong",
            "type": "user"
          },
          "name": "Biao Gong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:35.774Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ab",
          "name": "Cheng Zou",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ac",
          "user": {
            "_id": "65dd699a89a2a760d15f7d35",
            "avatarUrl": "/avatars/e098b56c413d147d1f38cf33a4b0ecde.svg",
            "isPro": false,
            "fullname": "Dandan Zheng",
            "user": "zhengdd0422",
            "type": "user"
          },
          "name": "Dandan Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:11.829Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ad",
          "name": "Hu Yu",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ae",
          "user": {
            "_id": "64575ac8cd935d48a47774ec",
            "avatarUrl": "/avatars/5d211e2c13d6c4e011e5e58b738413f7.svg",
            "isPro": false,
            "fullname": "chenjingdong ",
            "user": "chenjingdong",
            "type": "user"
          },
          "name": "Jingdong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:51.440Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323af",
          "user": {
            "_id": "6417cd278f689506e71439ac",
            "avatarUrl": "/avatars/0993d834c6c3bbc53081aa139ee14a12.svg",
            "isPro": false,
            "fullname": "jianxinsun",
            "user": "jianxinsun",
            "type": "user"
          },
          "name": "Jianxin Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:02.728Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b0",
          "name": "Junbo Zhao",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b1",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b2",
          "name": "Kaixiang Ji",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b3",
          "name": "Lixiang Ru",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b4",
          "name": "Libin Wang",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b5",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b6",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b7",
          "name": "Weilong Chai",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b8",
          "user": {
            "_id": "67cc852d2cfa481bce2dd07e",
            "avatarUrl": "/avatars/0c1c32ec066a8de9148b083b39d1fab8.svg",
            "isPro": false,
            "fullname": "xinyu xiao",
            "user": "bear-xxy",
            "type": "user"
          },
          "name": "Xinyu Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:11:49.316Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b9",
          "name": "Ziyuan Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T08:56:12.000Z",
      "submittedOnDailyAt": "2025-05-06T01:00:49.692Z",
      "title": "명명 유닛: 자연 다양성 상호작용 통합 아키텍처의 발전",
      "submittedOnDailyBy": {
        "_id": "644fcbea4f7316588267dc80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
        "isPro": false,
        "fullname": "Biao Gong",
        "user": "BiaoGong",
        "type": "user"
      },
      "summary": "명-Lite-Uni는 새로운 디자인의 통일된 시각적 시각 생성기를 포함하고, 시각과 언어를 통합하기 위해 설계된 본질적인 다모델을 제시하는 오픈 소스 멀티 모델 프레임워크입니다. 특히, 이 프로젝트는 MetaQueries와 M2-omni 프레임워크의 통합을 구현하고, 새로운 다스케일 학습 가능한 토큰과 다스케일 표현 어레이미션 스틸레이트를 도입하고 있습니다. 고정 모델과 학습 가능한 디퓨전 모델을 활용하여, 명-Lite-Uni는 텍스트로부터 이미지 생성 및 명령 기반의 이미지 편집 태스크를 수행할 수 있으며, 시각적 이해를 넘어 기능 확장을 가능하게 합니다. 실험 결과를 통해 명-Lite-Uni의 강력한 성능을 보여주고, 상호작용 프로세스의 흐름을 평가하여 제공하고 있습니다. 모든 코드와 모델 가중치는 오픈 소스로 제공되어, 커뮤니티 내에서 진보를 촉진합니다. 특히, 이 프로젝트는 2025년 3월 25일 이미지 생성을 업데이트한 ChatGPT-4o와 동기화된 다모델 AI의 마크스톤으로, 명-Lite-Uni와 같은 통합 모델의 광범위한 의미에 강조되어 있습니다. 명-Lite-Uni는 알파 스테이지에 있으며, 앞으로 발전할 예정입니다.",
      "upvotes": 6,
      "discussionId": "681973d2a70a47289583249d",
      "projectPage": "https://github.com/inclusionAI/Ming/tree/main/Ming-unify",
      "githubRepo": "https://github.com/inclusionAI/Ming/tree/main/Ming-unify",
      "ai_keywords": [
        "unified visual generator",
        "multimodal autoregressive model",
        "MetaQueries",
        "M2-omni framework",
        "multi-scale learnable tokens",
        "multi-scale representation alignment strategy",
        "MLLM",
        "learnable diffusion model",
        "text-to-image generation",
        "instruction based image editing"
      ]
    },
    "publishedAt": "2025-05-05T04:56:12.000Z",
    "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction",
    "summary": "We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02471.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644fcbea4f7316588267dc80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
      "fullname": "Biao Gong",
      "name": "BiaoGong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01583",
      "authors": [
        {
          "_id": "6819814653612b577df718e7",
          "user": {
            "_id": "65cd4d6256671dee8ee46392",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cd4d6256671dee8ee46392/SH30XVQnGiYqYQIeDk3na.jpeg",
            "isPro": false,
            "fullname": "Jen-Hao (Andy) Cheng",
            "user": "andaba",
            "type": "user"
          },
          "name": "Jen-Hao Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:15.728Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718e8",
          "name": "Vivian Wang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718e9",
          "name": "Huayu Wang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ea",
          "name": "Huapeng Zhou",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718eb",
          "name": "Yi-Hao Peng",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ec",
          "name": "Hou-I Liu",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ed",
          "user": {
            "_id": "647e4e8da49bffab5d72fbe0",
            "avatarUrl": "/avatars/c5fb00019c7cea23fe3351ecb1e43195.svg",
            "isPro": false,
            "fullname": "Hsiang-Wei Huang",
            "user": "hsiangwei0903",
            "type": "user"
          },
          "name": "Hsiang-Wei Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:19.727Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ee",
          "name": "Kuang-Ming Chen",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ef",
          "name": "Cheng-Yen Yang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f0",
          "user": {
            "_id": "637c7503fe115289cfecbe6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
            "isPro": false,
            "fullname": "Wenhao Chai",
            "user": "wchai",
            "type": "user"
          },
          "name": "Wenhao Chai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:37.240Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f1",
          "user": {
            "_id": "65f8cb651e0c65c13a2b906a",
            "avatarUrl": "/avatars/ffc8ac8f29ab1a3142fe5fab1b2302ca.svg",
            "isPro": false,
            "fullname": "Yi-Ling Chen",
            "user": "yilche",
            "type": "user"
          },
          "name": "Yi-Ling Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:43.313Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f2",
          "user": {
            "_id": "63c8527becdb7c9fdd9cacc6",
            "avatarUrl": "/avatars/c8a3f5e1e5159ae5ead41bd9fc2b9b34.svg",
            "isPro": false,
            "fullname": "Vibhav Vineet",
            "user": "vibhav-vineet",
            "type": "user"
          },
          "name": "Vibhav Vineet",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:49.482Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f3",
          "name": "Qin Cai",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f4",
          "name": "Jenq-Neng Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-02T21:00:17.000Z",
      "submittedOnDailyAt": "2025-05-06T01:56:09.960Z",
      "title": "TEMPURA: 시계열イベント 마스크付예측과 이해에 의한 행동의 이유론",
      "submittedOnDailyBy": {
        "_id": "637c7503fe115289cfecbe6b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
        "isPro": false,
        "fullname": "Wenhao Chai",
        "user": "wchai",
        "type": "user"
      },
      "summary": "Understanding causal event relationships and achieving fine-grained temporal localization in videos remains challenging for visual language models. Existing methods either compress video tokens to reduce temporal resolution or treat videos as unsegmented streams, which blurs fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Mask Prediction and Understanding for Action Reasoning), a two-stage training framework to enhance temporal understanding of videos. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generates step-by-step causal explanations from dense event annotations, drawing on effective filling techniques. TEMPURA then learns to perform video segmentation and dense captioning, decomposing videos into non-overlapping events with detailed descriptions and time-stamp alignments. We train TEMPURA on VER, a carefully curated large-scale dataset containing 1M training instances and 500K videos with time-aligned event descriptions and structured reasoning steps. Experiments on temporal localization and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming the benefit of combining causal reasoning with fine-grained temporal segmentation to enhance video understanding.",
      "upvotes": 5,
      "discussionId": "6819814853612b577df71943",
      "ai_keywords": [
        "TEMPURA",
        "masked event prediction",
        "causal explanations",
        "dense event annotations",
        "infilling techniques",
        "video segmentation",
        "dense captioning",
        "non-overlapping events",
        "timestamp-aligned descriptions",
        "VER",
        "temporal grounding",
        "highlight detection",
        "baseline models",
        "causal reasoning",
        "fine-grained temporal segmentation"
      ]
    },
    "publishedAt": "2025-05-02T17:00:17.000Z",
    "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action",
    "summary": "Understanding causal event relationships and achieving fine-grained temporal\ngrounding in videos remain challenging for vision-language models. Existing\nmethods either compress video tokens to reduce temporal resolution, or treat\nvideos as unsegmented streams, which obscures fine-grained event boundaries and\nlimits the modeling of causal dependencies. We propose TEMPURA (Temporal Event\nMasked Prediction and Understanding for Reasoning in Action), a two-stage\ntraining framework that enhances video temporal understanding. TEMPURA first\napplies masked event prediction reasoning to reconstruct missing events and\ngenerate step-by-step causal explanations from dense event annotations, drawing\ninspiration from effective infilling techniques. TEMPURA then learns to perform\nvideo segmentation and dense captioning to decompose videos into\nnon-overlapping events with detailed, timestamp-aligned descriptions. We train\nTEMPURA on VER, a large-scale dataset curated by us that comprises 1M training\ninstances and 500K videos with temporally aligned event descriptions and\nstructured reasoning steps. Experiments on temporal grounding and highlight\ndetection benchmarks demonstrate that TEMPURA outperforms strong baseline\nmodels, confirming that integrating causal reasoning with fine-grained temporal\nsegmentation leads to improved video understanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01583.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c7503fe115289cfecbe6b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
      "fullname": "Wenhao Chai",
      "name": "wchai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02823",
      "authors": [
        {
          "_id": "6819893117007d963b997a0b",
          "user": {
            "_id": "66b2e5f5523bf90aa7057467",
            "avatarUrl": "/avatars/ccdb58c2e56cf861e9dcec50c85d7778.svg",
            "isPro": false,
            "fullname": "Guo",
            "user": "Zinan123212",
            "type": "user"
          },
          "name": "Zinan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:14:34.748Z",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0c",
          "name": "Pengze Zhang",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0d",
          "user": {
            "_id": "639709c2be8a14bb9eeea8f6",
            "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
            "isPro": false,
            "fullname": "Yanze Wu",
            "user": "yanze",
            "type": "user"
          },
          "name": "Yanze Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:02.034Z",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0e",
          "name": "Chong Mou",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0f",
          "name": "Songtao Zhao",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a10",
          "user": {
            "_id": "645dcad7a19f3e64bbf35e6c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/rV1uHDSnZv7jAvFq4ftj4.jpeg",
            "isPro": false,
            "fullname": "Qian He",
            "user": "heqian",
            "type": "user"
          },
          "name": "Qian He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:28.996Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T17:50:24.000Z",
      "submittedOnDailyAt": "2025-05-06T02:30:32.888Z",
      "title": "MUSAR: 단일주제 데이터셋에서부터 다주제 맞춤형 검토를 통해 어텐션 루팅을 사용합니다.",
      "submittedOnDailyBy": {
        "_id": "639709c2be8a14bb9eeea8f6",
        "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
        "isPro": false,
        "fullname": "Yanze Wu",
        "user": "yanze",
        "type": "user"
      },
      "summary": "현재의 다논리CUSTAMIZE 접근 방식에는 두 가지 중요한 문제점이 존재한다: 다양한 다논리 훈련 데이터의 획득의 어려움과 서로 다른 논리 간의 속성의 결합이다. 이러한 문제를 해결하기 위해, 우리는 MUSAR(다논리CUSTAMIZE 접근 방식)을 제안합니다. 이는 단일논리의 훈련 데이터에서도 강력한 다논리CUSTAMIZE를 실현하는 간단하고 효과적인 프레임워크입니다. 우선, 데이터의 제한을 해결하기 위해, 우리는 디바이스 바이어스를 보정한 디피치 학습을 도입합니다. 이는 단일논리의 이미지로부터 디피치 훈련 페어를 구축하고, 디피치 구축에 의한 분포 바이어스를 동적 어텐션 루팅과 이중 필드 LoRA로 主동적으로 보정합니다. 다음으로, 서로 다른 논리 간의 결합을 제거하기 위해, 우리는 동적 어텐션 루팅 구조를 도입합니다. 이는 생성된 이미지와 조건부 논리 간의 1대1의 대응을 적응적으로 확립합니다. 이 설계는 다논리 표현의 디코딩을 실현하고, 참조 논리 간의 증가에 따라 교환 가능한 일반화 성능을 유지합니다. 자세한 실험은, MUSAR가 단일논리 데이터에서도 기존 방법보다 화질, 논리 일치성, 커뮤니케이션의 자연성을 뛰어넘는 것을 보여줍니다.",
      "upvotes": 2,
      "discussionId": "6819893317007d963b997ab1",
      "githubRepo": "https://github.com/guozinan126/MUSAR",
      "ai_keywords": [
        "debiased diptych learning",
        "diptych training pairs",
        "static attention routing",
        "dual-branch LoRA",
        "dynamic attention routing mechanism",
        "bijective mappings",
        "multi-subject representations",
        "scalable generalization performance"
      ]
    },
    "publishedAt": "2025-05-05T13:50:24.000Z",
    "title": "MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset\n  via Attention Routing",
    "summary": "Current multi-subject customization approaches encounter two critical\nchallenges: the difficulty in acquiring diverse multi-subject training data,\nand attribute entanglement across different subjects. To bridge these gaps, we\npropose MUSAR - a simple yet effective framework to achieve robust\nmulti-subject customization while requiring only single-subject training data.\nFirstly, to break the data limitation, we introduce debiased diptych learning.\nIt constructs diptych training pairs from single-subject images to facilitate\nmulti-subject learning, while actively correcting the distribution bias\nintroduced by diptych construction via static attention routing and dual-branch\nLoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic\nattention routing mechanism, which adaptively establishes bijective mappings\nbetween generated images and conditional subjects. This design not only\nachieves decoupling of multi-subject representations but also maintains\nscalable generalization performance with increasing reference subjects.\nComprehensive experiments demonstrate that our MUSAR outperforms existing\nmethods - even those trained on multi-subject dataset - in image quality,\nsubject consistency, and interaction naturalness, despite requiring only\nsingle-subject dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02823.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639709c2be8a14bb9eeea8f6",
      "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
      "fullname": "Yanze Wu",
      "name": "yanze",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 140
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02625",
      "authors": [
        {
          "_id": "681975abba26bf20601bb7ca",
          "user": {
            "_id": "65b7573482d384513443875e",
            "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
            "isPro": false,
            "fullname": "Qingkai Fang",
            "user": "poeroz",
            "type": "user"
          },
          "name": "Qingkai Fang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:43.683Z",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cb",
          "name": "Yan Zhou",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cc",
          "user": {
            "_id": "66680c0505c407bfea87667c",
            "avatarUrl": "/avatars/e3c26d2eb13fe8ad2b3fd16897e61e6d.svg",
            "isPro": false,
            "fullname": "Shoutao Guo",
            "user": "guoshoutao",
            "type": "user"
          },
          "name": "Shoutao Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:53.154Z",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cd",
          "user": {
            "_id": "64803e5dc57f629056c601f1",
            "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
            "isPro": false,
            "fullname": "Shaolei Zhang",
            "user": "zhangshaolei",
            "type": "user"
          },
          "name": "Shaolei Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:16:00.139Z",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7ce",
          "name": "Yang Feng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T12:53:09.000Z",
      "submittedOnDailyAt": "2025-05-06T01:07:20.259Z",
      "title": "LLaMA-Omni2: 시간계열 언어 모델에 기반한 실시간 대화형 챗봇과 자동 후퇴어 합성\n\n(Note: The translation is provided in Korean as requested, maintaining the original structure and meaning of the English text.)",
      "submittedOnDailyBy": {
        "_id": "65b7573482d384513443875e",
        "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
        "isPro": false,
        "fullname": "Qingkai Fang",
        "user": "poeroz",
        "type": "user"
      },
      "summary": "실시간적이고 지능적이고 자연스러운 스peech 인턴랙션은 다음 세대의 인간 컴퓨팅 인턴랙션의 중요한 부분을 차지합니다. 최근의 발전은 대규모 언어 모델(LLMs)에 기반한 지능적인 어휘 챗봇의 구축 가능성을 보여주었습니다. 본 논문에서는 0.5B부터 14B까지의 파라미터 수를 가진 스peech 언어 모델(SpeechLMs)의 시리즈인 LLaMA-Omni 2를 소개합니다. 이들은 고품질의 실시간 스peech 인턴랙션을 실현할 수 있는 능력을 가지고 있습니다. LLaMA-Omni 2는 Qwen2.5 시리즈 모델을 기반으로, 스peech 엔코더와 자동 복구 스트리밍 스peech 디코더를 통합하여 구축되었습니다. 200K의 많은 턴 스peech 다이얼로그 샘플로만 훈련된 데에도, LLaMA-Omni 2는 여러 어휘 질문응답 테스트와 스peech 지시응답 테스트에서 강력한 성능을 보여주고, 수백만 시간의 스peech 데이터로 훈련된 先進한 SpeechLMs인 GLM-4-Voice를 초월하고 있습니다.",
      "upvotes": 2,
      "discussionId": "681975abba26bf20601bb7f2",
      "ai_keywords": [
        "speech language models (SpeechLMs)",
        "Qwen2.5",
        "speech encoder",
        "autoregressive streaming speech decoder",
        "spoken question answering",
        "speech instruction following",
        "GLM-4-Voice"
      ]
    },
    "publishedAt": "2025-05-05T08:53:09.000Z",
    "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis",
    "summary": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b7573482d384513443875e",
      "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
      "fullname": "Qingkai Fang",
      "name": "poeroz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01456",
      "authors": [
        {
          "_id": "6819c7577c36c576e9cb6bfa",
          "user": {
            "_id": "64f64da90efa33bfe0a3d9ba",
            "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
            "isPro": false,
            "fullname": "Vaidehi Patil",
            "user": "vaidehi99",
            "type": "user"
          },
          "name": "Vaidehi Patil",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:16:35.372Z",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfb",
          "user": {
            "_id": "654ffe334d9e71e17becc660",
            "avatarUrl": "/avatars/022b7a77051d26c4e5cbf254b7352eb9.svg",
            "isPro": false,
            "fullname": "Yi-Lin Sung",
            "user": "a2889184",
            "type": "user"
          },
          "name": "Yi-Lin Sung",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:16:41.799Z",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfc",
          "name": "Peter Hase",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfd",
          "name": "Jie Peng",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfe",
          "name": "Tianlong Chen",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bff",
          "user": {
            "_id": "665d9d3a057f7c508f98c625",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d9d3a057f7c508f98c625/u1R9P9sJoAl4zEIcetbPy.jpeg",
            "isPro": false,
            "fullname": "Mohit Bansal",
            "user": "mohitbansal",
            "type": "user"
          },
          "name": "Mohit Bansal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:17:14.823Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T01:54:00.000Z",
      "submittedOnDailyAt": "2025-05-06T06:55:27.299Z",
      "title": "다형 LLM에서 민감정보의 학습된 데이터의 제거: 벤치마크와 공격방어 평가",
      "submittedOnDailyBy": {
        "_id": "64f64da90efa33bfe0a3d9ba",
        "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
        "isPro": false,
        "fullname": "Vaidehi Patil",
        "user": "vaidehi99",
        "type": "user"
      },
      "summary": "LLMs는 큰 데이터 세트를 학습하고 있기 때문에, 무의식적으로 감성적인 정보(개인 정보나 잠재적으로 유해한 콘텐츠)를 얻는 위험이 있습니다. 이 위험은 다양화 모델(다양화 LLMs)에서 이미지와 텍스트의 정보를 통합하기 때문에 더욱 높아집니다. 광고는 이 지식을 다양화 프로ン퓰트로 통해 감성적인 세부 정보를 추출할 수 있습니다. MLLMs에서 특정 정보를 제거하는 효과에 대한 평가는, 고품질으로 더 잘 설명된 이미지-텍스트 페어의 제작이 필요합니다. 선행 연구는 주로 텍스트에 초점을 맞추었지만, 다양화 유니폼은 조사가 부족합니다. 이 공간을 보완하기 위해, 우리는 먼저 다양화 유니폼의 잊혀짐 벤치마크(UnLOK-VQA), 공격과 방어의 프레임워크를 소개하고, MLLMs에서 특정 다양화 지식을 제거하는 방법을 평가하기 위한 것을 만들었습니다. 우리는 시각적 질문에 대한 답변 데이터 세트를 자동 프로세스로 확장하고, 일반화 및 특이성의 검증을 위해, 유사한 프로세스를 통해 샘플을 생성하고, 손으로 필터링하여 고품질을 유지합니다. 그 후, 6개의 방어 객체를 7개의 공격(4개의 화이트박스, 3개의 블랙박스)에 대해 평가하였으며, 새로운 화이트박스 메소드를 활용한 해석성을 포함하는 방법들을 사용했습니다. 결과는 다양화 공격은 텍스트나 이미지의 공격보다 우수하며, 가장 효과적인 방어는 내부 모델 상태를 통해 답변 정보를 제거하는 것입니다. 또한, 큰 모델은 후처리 로바스트성이 높고, 규모가 안전성을 높일 수 있음을 보여줍니다. UnLOK-VQA는 MLLMs의 잊혀짐을 촉진하는 엄격한 벤치마크가 될 것입니다.",
      "upvotes": 0,
      "discussionId": "6819c7597c36c576e9cb6c6b",
      "githubRepo": "https://github.com/Vaidehi99/UnLOK-VQA",
      "ai_keywords": [
        "multimodal LLMs",
        "multimodal prompts",
        "targeted unlearning",
        "high-quality, well-annotated image-text pairs",
        "multimodal unlearning",
        "UnLOK-VQA (Unlearning Outside Knowledge VQA)",
        "visual question-answering dataset",
        "varying-proximity samples",
        "whitebox attacks",
        "blackbox attacks",
        "interpretability of hidden states",
        "multimodal attacks",
        "post-editing robustness"
      ]
    },
    "publishedAt": "2025-04-30T21:54:00.000Z",
    "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and\n  Attack-Defense Evaluation",
    "summary": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01456.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f64da90efa33bfe0a3d9ba",
      "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
      "fullname": "Vaidehi Patil",
      "name": "vaidehi99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]