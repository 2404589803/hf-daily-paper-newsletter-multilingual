[
  {
    "paper": {
      "id": "2503.02682",
      "authors": [
        {
          "_id": "67c7c3d073299239b63f5378",
          "name": "Weimin Xiong",
          "hidden": false
        },
        {
          "_id": "67c7c3d073299239b63f5379",
          "name": "Yifan Song",
          "hidden": false
        },
        {
          "_id": "67c7c3d073299239b63f537a",
          "name": "Qingxiu Dong",
          "hidden": false
        },
        {
          "_id": "67c7c3d073299239b63f537b",
          "name": "Bingchan Zhao",
          "hidden": false
        },
        {
          "_id": "67c7c3d073299239b63f537c",
          "name": "Feifan Song",
          "hidden": false
        },
        {
          "_id": "67c7c3d073299239b63f537d",
          "name": "Xun Wang",
          "hidden": false
        },
        {
          "_id": "67c7c3d073299239b63f537e",
          "name": "Sujian Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T14:54:45.000Z",
      "title": "MPO: 메타 계획 최적화로 LLM 에이전트를 강화합니다.",
      "summary": "최근의 대 언어 모델(LLMs)의 발전으로 LLM 기반의 에이전트가 상호작용 계획 태스크를 성공적으로 해결할 수 있습니다. 그러나 그 성공은 계획 해시(planning hallucinations)을 포함하는 여러 문제에 부딪혔으며, 새로운 에이전트에 대한 리트레이닝이 필요합니다. 이러한 문제를 해결하기 위해 우리는 메타 계획 최적화(MPO) 프레임워크를 제안합니다. 이는 에이전트의 계획 능력을 직접적으로 명시적인 지침을 통해 강화합니다. 이전의 방법과 달리 복잡한 지식을 기반으로 되어 있으며, 이는 큰 인간 노력이 필요하거나 품질 보장이 부족할 수 있지만, MPO는 메타 계획을 통해 고 수준의 일반적인 지침을 활용하여 에이전트의 계획을 지원하고, 태스크 실행으로부터의 피드백에 기반하여 메타 계획의 지속적인 최적화를 가능하게 합니다. 대표적인 2가지 태스크에 대해 실험을 수행하여, MPO가 현재의 기준과 비교하여 유의미하게 뛰어난 결과를 보였습니다. 또한 분석에 따르면, MPO는 이전에 본 적이 없는 시나리오에서도 작업의 효율성과 일반화 능력이 양방향으로 향상되는 점과 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점, 점,",
      "upvotes": 12,
      "discussionId": "67c7c3d173299239b63f53d6",
      "githubRepo": "https://github.com/WeiminXiong/MPO"
    },
    "publishedAt": "2025-03-04T22:30:53.253Z",
    "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02682.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6225a9983207dfc568407204",
      "avatarUrl": "/avatars/c970db6232d84ae8c0fa5f11d561d67c.svg",
      "fullname": "xwm",
      "name": "xwm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02846",
      "authors": [
        {
          "_id": "67c7c3ce6f68759bf368533c",
          "name": "Yuzhe Gu",
          "hidden": false
        },
        {
          "_id": "67c7c3ce6f68759bf368533d",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "67c7c3ce6f68759bf368533e",
          "name": "Chengqi Lyu",
          "hidden": false
        },
        {
          "_id": "67c7c3ce6f68759bf368533f",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67c7c3ce6f68759bf3685340",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T18:20:24.000Z",
      "title": "마스크-DPO: 모델의 일반화 가능한, 세부화된 사실성의 어레이멘트",
      "summary": "대 언어 모델(LLMs)은 다양한 분야에서 AI 보조자 역할을 할 때, 허실러레이션(즉, 불실리거나 무의미한 정보)을 나타냅니다. LLMs의 답변에는 항상 사실적인 내용이 포함되어 있기 때문에, 과거의 사실성 일치 방법들은 훈련 과정에서 노이즈를 끌어들이지 못했습니다. 따라서, 본 논문에서는 Direct Preference Optimization(DPO)에 기반한 미세한 사실성 일치 방법을 제안하고, 이를 Mask-DPO라고 합니다. 문서 수준의 사실성을 마스크 신호로 사용하며, Mask-DPO는 선호 샘플에서만 사실적인 문서를 학습시키고, 비 선호 샘플에서 사실적인 내용을 피하는 방식으로, 선호 학습의 불확실성을 해결합니다. 광범위한 실험 결과는, Mask-DPO가 LLMs의 답변의 사실성을 크게 향상시키고, 데이터셋의 영역 내 및 영역 외 데이터셋으로부터의 질문에 대해서도, 훈련 시 본하지 않은 질문과 그에 대한 주제를 포함하는 것을 효과적으로 처리하는 것을 나타냅니다. 또한, ANAH 테스트 세트에서 Llama3.1-8B-Instruct의 점수는 49.19%에서 77.53%로 개선되었으며, 동시에, 영역 외 데이터셋인 Biography 데이터셋에서 FactScore도 30.29%에서 39.39%로 개선되었습니다. 또한, Mask-DPO의 일반화 성능을 연구하기 위해, 다른 훈련 샘플 스케일링 전략을 사용하며, 주제의 수를 스케일링하는 것이 문제를 스케일링보다 효과적이라는 것을 확인했습니다. 또한, LLMs에서 사실성 일치가 어떻게 이루어지는지, 이 현상의 의미 및 증명을 위한 실험을 수행했습니다. 향후 연구에서 사실성 일치의 스케일링에 대한 연구의 길을 마련합니다.",
      "upvotes": 11,
      "discussionId": "67c7c3d06f68759bf3685489",
      "githubRepo": "https://github.com/open-compass/ANAH"
    },
    "publishedAt": "2025-03-04T22:25:15.163Z",
    "title": "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01935",
      "authors": [
        {
          "_id": "67c7ba7f19b236e0564d1172",
          "user": {
            "_id": "66554507e6ea63012f35824c",
            "avatarUrl": "/avatars/b82de75bd60890e7bb524fc3754b131c.svg",
            "isPro": false,
            "fullname": "Kunlun_Zhu",
            "user": "Leozkl",
            "type": "user"
          },
          "name": "Kunlun Zhu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-05T02:44:18.739Z",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d1173",
          "name": "Hongyi Du",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d1174",
          "name": "Zhaochen Hong",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d1175",
          "name": "Xiaocheng Yang",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d1176",
          "name": "Shuyi Guo",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d1177",
          "name": "Zhe Wang",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d1178",
          "name": "Zhenhailong Wang",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d1179",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d117a",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d117b",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d117c",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T05:18:50.000Z",
      "title": "MultiAgentBench: LLM 에이전트의 협업과 경쟁 평가",
      "summary": "대 언어 모델(LLMs)는 자동 전이 에이전트로서 뛰어난 능력을 보여주지만, 현재의 벤치마크는 단일 에이전트의 태스크에만 집중하거나, 좁은 영역에 제한되어 있으며, 다 에이전트의 협조와 경쟁의 동적인 모습을 파악하지 못합니다. 본 논문에서는, 다 에이전트 벤치마크(MultiAgentBench)를 소개합니다. 이 벤치마크는 다양한 상호작용하는 스케너에서 LLM 기반의 다 에이전트 시스템의 성능을 평가하기 위해 설계된 상세한 벤치마크입니다. 프레임워크는 태스크의 완료 정도뿐만 아니라, 새로운, 마진 스토ن 기반의 주요 성능 지표를 사용하여 협조와 경쟁의 질을 평가합니다. 또한, 별형, 체인 구조, 나무 구조, 그래프 구조 등의 협조 프로토콜과, 그룹 토론, 인지 계획 등 혁신적인 전략을 평가합니다. 특히, gpt-4o-mini는 평균적으로 최고의 태스크 점수를 달성하며, 연구 스케너에서 그래프 구조가 가장 우수하며, 인지 계획은 마진 스토ن 달성율을 3% 높입니다. 코드와 데이터 세트는 https://github.com/MultiagentBench/MARBLE에서 공개됩니다.",
      "upvotes": 7,
      "discussionId": "67c7ba8219b236e0564d124a",
      "githubRepo": "https://github.com/MultiagentBench/MARBLE"
    },
    "publishedAt": "2025-03-04T21:46:46.873Z",
    "title": "MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01935.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64c090a9f613170e7be93d2f",
      "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
      "fullname": "KunlunZhu",
      "name": "KunlunZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01328",
      "authors": [
        {
          "_id": "67c7b5900b05ab9c7e805433",
          "name": "Xinyi Wan",
          "hidden": false
        },
        {
          "_id": "67c7b5900b05ab9c7e805434",
          "name": "Penghui Qi",
          "hidden": false
        },
        {
          "_id": "67c7b5900b05ab9c7e805435",
          "name": "Guangxing Huang",
          "hidden": false
        },
        {
          "_id": "67c7b5900b05ab9c7e805436",
          "name": "Jialin Li",
          "hidden": false
        },
        {
          "_id": "67c7b5900b05ab9c7e805437",
          "name": "Min Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T09:11:06.000Z",
      "title": "PipeOffload: 메모리 연산에 의한 파이프라인 병렬화의 scalability 향상",
      "summary": "PP는 대규모 언어 모델(LLMs)의 훈련에서 광범위하게 사용되고 있지만, PP의 scalability는 미분 배치의 수가 PP의 수준에 도달하면 높은 ACTIVE_MEMORY 소비량으로 제한됩니다. 본 논문에서는 이러한 문제를 해결하기 위해, PP에서 조사가 부족한 MEMORY_OFFLOAD 전략을 활용합니다. 실험적 연구에 따라, 표준 설정의 많은 경우, ACTIVE_MEMORY의 적어도 절반, 그리고 잠재적으로 모든 것을 무시할 수 있는微观 오버헤드(micro-overhead)로 OFFLOAD할 수 있음을 확인했습니다. 모든 OFFLOAD가 불가능할 경우, ACTIVE_MEMORY의 피크 값을 상대적으로 선형 이상으로 줄이는 새로운 선택적 OFFLOAD 전략을 도입합니다. 또한, MEMORY_OFFLOAD를 다른 기술과 통합하고 전체적인 트랜잭션과 메모리 제한을 함께 고려합니다. 실험에 따르면, 각 장치의 ACTIVE_MEMORY는 전체 스테이지수에 따라 효과적으로 줄이고, PP는 TP보다 강력한 선택이 되고, 더욱 낮은 메모리 소비량과 19%의 가속을 입증했습니다. 구현은 아래 URL에서 오픈소스로 공개되어 있습니다.\nhttps://github.com/sail-sg/zero-bubble-pipeline-parallelism{이 URL}",
      "upvotes": 7,
      "discussionId": "67c7b5970b05ab9c7e8055a1",
      "githubRepo": "https://github.com/sail-sg/zero-bubble"
    },
    "publishedAt": "2025-03-04T21:30:49.808Z",
    "title": "PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01328.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63510eea0b94548566dad923",
      "avatarUrl": "/avatars/629eaaf810718259bf7588dc2e6cc0d5.svg",
      "fullname": "Xinyi Wan",
      "name": "ufotalent",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02879",
      "authors": [
        {
          "_id": "67c7c42269d99dd25c5ba0ce",
          "name": "Siming Huang",
          "hidden": false
        },
        {
          "_id": "67c7c42269d99dd25c5ba0cf",
          "name": "Yuliang Xu",
          "hidden": false
        },
        {
          "_id": "67c7c42269d99dd25c5ba0d0",
          "user": {
            "_id": "67890323f8796231c857231e",
            "avatarUrl": "/avatars/f5ccd5186968d880fee9c36324a5f713.svg",
            "isPro": false,
            "fullname": "Mingmeng Geng",
            "user": "mgeng",
            "type": "user"
          },
          "name": "Mingmeng Geng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-05T03:25:23.013Z",
          "hidden": false
        },
        {
          "_id": "67c7c42269d99dd25c5ba0d1",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67c7c42269d99dd25c5ba0d2",
          "name": "Dongping Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T18:58:13.000Z",
      "title": "Wikipedia 문서의 시대에 있는 LLM: 진화와 위험",
      "summary": "이 논문에서는 Large Language Models（LLMs）가 Wikipedia에 미치는 영향을 상세히 분석하고, 현재의 데이터를 사용하여 Wikipedia의 발전을 조사하고 잠재적인 위험을 시뮬레이션을 통해 탐구합니다. 먼저, 페이지를 분석하고 기사 내용을 분석하여 Wikipedia의 최근 변화에 대해 연구하고 LLMs의 영향을 평가합니다. 다음으로, LLMs가 Wikipedia에 관련된 다양한 자연어 처리（NLP） 태스크에 미치는 영향을 평가합니다. 이러한 발견과 시뮬레이션 결과를 통해, Wikipedia의 기사는 LLMs에 의해 영향을 받습니다. 특정 카테고리에서 약 1%-2%의 영향을 미칩니다. Wikipedia에 기반한 기계 번역 벤치마크가 LLMs에 영향을 받는 경우, 모델의 점수가 증가하고 모델 간 비교 결과를 변경하는 가능성도 있습니다. 또한, RAG의 효과는 LLM 생성의 콘텐츠가 오염된 지식 기반에 의해 떨어질 가능성이 있습니다. LLMs는 Wikipedia의 언어와 지식 구조를 완전히 변경하지 않지만, 우리는 실험적 발견으로부터 미래의 위험에 대한 엄격한 검토가 필요함을 믿습니다.",
      "upvotes": 6,
      "discussionId": "67c7c42369d99dd25c5ba103",
      "githubRepo": "https://github.com/HSM316/LLM_Wikipedia"
    },
    "publishedAt": "2025-03-04T22:25:53.653Z",
    "title": "Wikipedia in the Era of LLMs: Evolution and Risks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02368",
      "authors": [
        {
          "_id": "67c80f94ccc2e04adfa67079",
          "name": "Zhenhua Liu",
          "hidden": false
        },
        {
          "_id": "67c80f94ccc2e04adfa6707a",
          "name": "Lijun Li",
          "hidden": false
        },
        {
          "_id": "67c80f94ccc2e04adfa6707b",
          "name": "Ruizhe Chen",
          "hidden": false
        },
        {
          "_id": "67c80f94ccc2e04adfa6707c",
          "name": "Yuxian Jiang",
          "hidden": false
        },
        {
          "_id": "67c80f94ccc2e04adfa6707d",
          "name": "Tong Zhu",
          "hidden": false
        },
        {
          "_id": "67c80f94ccc2e04adfa6707e",
          "name": "Wenliang Chen",
          "hidden": false
        },
        {
          "_id": "67c80f94ccc2e04adfa6707f",
          "name": "Jing Shao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T07:49:10.000Z",
      "title": "イテレーションドリーミング 값 함수 최적화를 이용한 가이드 디코딩",
      "summary": "RLHF（Human Feedback의 강화학습）는 언어 모델의 출력을 제어하기 위해 주요한 방법 중 하나가 되었지만, 높은 계산 비용과 훈련 불안정성에서 어려움을 겪고 있습니다. Guided decoding, 특히 가치에 의해 유도된 방법들은 모델의 재훈련을 피하면서 출력을 제어하기 위해 비용 효율적인 대체 방법이 제안되어 있습니다. 그러나 가치 함수의 정확도가 가치에 의해 유도된 디코딩의 정확성에 중요합니다. 부정확성은 최적의 결정을 불러오고 성능 저하에 이어줍니다. 현재의 방법은 효과적인 제어를 위해 최적의 가치 함수의 정확한 추정에 어려움을 겪고 성공하지 않았습니다. 우리는 Monte Carlo Value Estimation과 Iterative On-Policy Optimization의 두 가지 핵심 요소를 도입하여 새로운 프레임워크인 Iterative Value Function Optimization을 제안합니다. 이 요소들은 다양한 태로의 탐색으로 추정의 분산을 줄이고 가치에 의해 유도된 정책으로부터 태로의 수집으로 가치 추정을 진행적으로 개선합니다. 문서 요약, 다턴 다이얼로그, 지시에 따라 광범위한 실험을 통해 가치에 의해 유도된 디코딩 접근법의 효과성을 보여주고, 이러한 접근법은 가치 함수의 원리적인 최적화를 활용하여 효율적이고 효과적인 제어를 실현하고 계산 비용을 크게 줄입니다.",
      "upvotes": 5,
      "discussionId": "67c80f94ccc2e04adfa670b2"
    },
    "publishedAt": "2025-03-05T03:48:51.408Z",
    "title": "Iterative Value Function Optimization for Guided Decoding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02368.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619b00dd4b0db5ca9d3ea35f",
      "avatarUrl": "/avatars/fce5ac388b7f10cbbc63e9992a5a799f.svg",
      "fullname": "Zhenhua Liu",
      "name": "zhliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14856",
      "authors": [
        {
          "_id": "67bee83509a4524abf899511",
          "name": "Weilin Zhao",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899512",
          "name": "Tengyu Pan",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899513",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899514",
          "name": "Yudi Zhang",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899515",
          "name": "Ao Sun",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899516",
          "name": "Yuxiang Huang",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899517",
          "name": "Kaihuo Zhang",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899518",
          "name": "Weilun Zhao",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899519",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf89951a",
          "name": "Jianyong Wang",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf89951b",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf89951c",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:58:10.000Z",
      "title": "FR-Spec: 언어 모델을 가속화하기 위해 대규모 단어 어휘사전의 예측적 샘플링을 순위에 따라 둠",
      "summary": "スペシュラティブサンプリング은 대규모 언어 모델(LLMs)의 자동 회귀 생성 프로세스를 가속화하는 중요한 방법 중 하나로 등장했다. 이 방법은 1개의 순전파로 복수 토큰을 생성하기 위해 드롭을 때 확인하는 메커니즘을 활용하는 것이다. 가장 先端的 스ペシュラティブサンプリング 방법은 인상적인 층의 압축을 달성하기 위해 단일 층과 언어 모델링(LM) 헤드를 드롭 모델로 사용하지만, 비디오 칼로리의 큰 규모의 LLMs(예: Llama-3-8B, 128k 토큰의 비디오 칼로리)에 대해 효율의 향상은 크게 감소하고 있다. 이에대해, 우리는 비디오 칼로리 공간의 압축에 의한 드롭 후보 선택의 최적화를 위해 빈도 순위가 부여된 스ペシュラティブサンプリング 프레임워크 FR-Spec을 제안하고 있다. 빈도 우선 토큰 서브셋에 드롭 검색을 제한함으로써, 우리의 방법은 LM Head의 계산 오버헤드를 75% 줄이면서 최종적인 출력 분포의 등가성을 보장하는 것이다. 많은 데이터 세트에서의 실험은 가장 先端的 스ペシュラティブサンプリング 방법 EAGLE-2를 초과하는 평균 1.12배의 속도 업그레이드를 나타내고 있다.",
      "upvotes": 5,
      "discussionId": "67bee83609a4524abf899550",
      "githubRepo": "https://github.com/thunlp/FR-Spec"
    },
    "publishedAt": "2025-03-05T00:36:34.146Z",
    "title": "FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14856.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b8ff3d95bd42c770878042",
      "avatarUrl": "/avatars/564a4dccdf9e5b813a99979b0ef58183.svg",
      "fullname": "Weilin Zhao",
      "name": "Achazwl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.00955",
      "authors": [
        {
          "_id": "67c7dbff25d0b3348ddace44",
          "name": "Nam V. Nguyen",
          "hidden": false
        },
        {
          "_id": "67c7dbff25d0b3348ddace45",
          "name": "Dien X. Tran",
          "hidden": false
        },
        {
          "_id": "67c7dbff25d0b3348ddace46",
          "name": "Thanh T. Tran",
          "hidden": false
        },
        {
          "_id": "67c7dbff25d0b3348ddace47",
          "name": "Anh T. Hoang",
          "hidden": false
        },
        {
          "_id": "67c7dbff25d0b3348ddace48",
          "name": "Tai V. Duong",
          "hidden": false
        },
        {
          "_id": "67c7dbff25d0b3348ddace49",
          "name": "Di T. Le",
          "hidden": false
        },
        {
          "_id": "67c7dbff25d0b3348ddace4a",
          "name": "Phuc-Lu Le",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T16:22:46.000Z",
      "title": "SemViQA: 베트남 정보의 의미 이해를 위한 질문 대답 시스템 샹크체크",
      "summary": "미스데이터의 증가로 인해, GPT, Gemini 등 대규모 언어 모델(LLMs)에 의해 확장된 것 중, 越南語에 대한 강력한 사실검증 솔루션이 필요하게 되었습니다. 현재의 방법들은 의미의 불확실성, 동음어와 복잡한 언어 구조에 대해 어려움을 겪으며, 정확성과 효율성 사이에서 균형을 이루고 있습니다. 우리는 의미 기반의 증거 검색(SER)와 2단계의 판단 분류(TVC)를 통합한 새로운 越南語 사실검증 프레임워크 SemViQA를 소개합니다. 우리의 접근법은 정확성과 속도의 균형을 이루고, ISE-DSC01에서 엄격한 정확도 78.97%와 ViWikiFC에서 80.82%를 기록하여 가장 先端한 결과를 얻으며, UIT Data Science Challenge에서 1위를 차지했습니다. 또한, SemViQA Faster는 정확도를 유지하면서 7배 빠른 추론 속도를 높입니다. SemViQA는 越南語의 사실검증에 새로운 벤치마크를 설정하고, 미스데이터의 전쟁에 진입하도록 촉발합니다. 소스 코드는 아래 URL에서 사용 가능합니다: https://github.com/DAVID-NGUYEN-S16/SemViQA.",
      "upvotes": 5,
      "discussionId": "67c7dc0025d0b3348ddace64",
      "githubRepo": "https://github.com/DAVID-NGUYEN-S16/SemViQA"
    },
    "publishedAt": "2025-03-05T00:08:53.214Z",
    "title": "SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c2bea2ada7df214276913b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
      "fullname": "Nguyen Van Nam",
      "name": "DavidNguyen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01342",
      "authors": [
        {
          "_id": "67c6b46e8389d77f5ba87179",
          "user": {
            "_id": "6585493b53c37507639fe3ba",
            "avatarUrl": "/avatars/b7e71d4fa5ebb89a7ed6b2a8313687b5.svg",
            "isPro": false,
            "fullname": "Hao Tang",
            "user": "kanashi6",
            "type": "user"
          },
          "name": "Hao Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:43.034Z",
          "hidden": false
        },
        {
          "_id": "67c6b46e8389d77f5ba8717a",
          "name": "Chenwei Xie",
          "hidden": false
        },
        {
          "_id": "67c6b46e8389d77f5ba8717b",
          "name": "Haiyang Wang",
          "hidden": false
        },
        {
          "_id": "67c6b46e8389d77f5ba8717c",
          "name": "Xiaoyi Bao",
          "hidden": false
        },
        {
          "_id": "67c6b46e8389d77f5ba8717d",
          "name": "Tingyu Weng",
          "hidden": false
        },
        {
          "_id": "67c6b46e8389d77f5ba8717e",
          "name": "Pandeng Li",
          "hidden": false
        },
        {
          "_id": "67c6b46e8389d77f5ba8717f",
          "name": "Yun Zheng",
          "hidden": false
        },
        {
          "_id": "67c6b46e8389d77f5ba87180",
          "name": "Liwei Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T09:27:24.000Z",
      "title": "UFO: 개방된 범위의 언어 인터페이스를 통한 세부 시각 인식의 통합 접근법",
      "summary": "일반화 모델은 언어와 시각-언어 태스크에서 놀라울 정도로 성공을 거두며 연속적인 모델링의 가능성을 보여주고 있습니다. 그러나, 검출이나 분할과 같은 미세한 시각 인식 태스크를 이러한 모델에 효과적으로 통합하는 것은 매우 어려운 과제입니다. 이는 이러한 태스크 대부분이 특정 디자인이나 아키텍처에 의존하며 모델링 프로세스를 복잡화하는 데 원인입니다. 이러한 문제를 해결하기 위해, 우리는 언어 인터페이스를 개방적으로 활용하여 미세한 시각 인식 태스크를 통합하는 프레임워크를 제안합니다. 모든 인식 목표를 언어 공간으로 변환하고, 우리 프레임워크는 물체 수준의 검출, 픽셀 수준의 분할, 이미지 수준의 시각-언어 태스크를 하나의 모델로 통합합니다. 또한, 우리는 언어 인터페이스에만 의존하는 새로운 임베딩 검색 접근 방식을 도입하고, 분할 태스크를 지원합니다. 우리 프레임워크는 미세한 시각 인식 태스크와 시각-언어 태스크 사이에 있는 간격을 연결하며, 구조 설계와 훈련 전략을 크게 단순화하고, 복잡한 태스크 특화 설계 방법과 비교하여 상대적으로 높은 성능을 달성합니다. 5개의 표준 시각 인식 데이터 세트에 대한 다 태스크 훈련 후, 우리 프레임워크는 COCO 인스턴스 분할에서 일반화 모델의 최고 성능을 12.3 mAP을 초과하고, ADE20K 의미 분할에서 3.3 mIoU를 초과합니다. 또한, 우리 방법은 현재 존재하는 MLLM과 세미 아스펙트에 쉽게 통합하며, 미세한 시각 인식 능력과 발전된 언어 능력를 효과적으로 조합하여, 이유 분할과 같은 어려운 태스크를 실현할 수 있습니다. 코드와 모델은 공개적으로 제공됩니다.",
      "upvotes": 4,
      "discussionId": "67c6b4728389d77f5ba8724d",
      "githubRepo": "https://github.com/nnnth/UFO"
    },
    "publishedAt": "2025-03-04T23:55:08.057Z",
    "title": "UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01342.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6585493b53c37507639fe3ba",
      "avatarUrl": "/avatars/b7e71d4fa5ebb89a7ed6b2a8313687b5.svg",
      "fullname": "Hao Tang",
      "name": "kanashi6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02197",
      "authors": [
        {
          "_id": "67c7c183203958ca9c09171a",
          "name": "Zhixun Chen",
          "hidden": false
        },
        {
          "_id": "67c7c183203958ca9c09171b",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67c7c183203958ca9c09171c",
          "name": "Yuxuan Huang",
          "hidden": false
        },
        {
          "_id": "67c7c183203958ca9c09171d",
          "name": "Yali Du",
          "hidden": false
        },
        {
          "_id": "67c7c183203958ca9c09171e",
          "name": "Meng Fang",
          "hidden": false
        },
        {
          "_id": "67c7c183203958ca9c09171f",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:14:55.000Z",
      "title": "ATLaS: 에이전트 조정에 의한 학습의 중요한 단계",
      "summary": "대 언어 모델(LLM) 에이전트는 다양한 데이터 태스크에서 놀라운 일반화 능력을 보여주고 있습니다. 현재의 에이전트 조정 접근 방식은 일반적으로 모든 전문가 프로젝트에 대한 규범적 조정을 수행합니다. 그러나 모든 프로젝트의 시각적 추적은 전문가 편향을 일으키고 전문가 데이터에 의존하지 않는 상태를 일반화하는 데 약화하는 경우가 있습니다. 또한 계획, 중간 서브 태스크의 복잡한 이유, 그리고 전략적인 결정의 중요한 단계는 에이전트 태스크의 성공에 필수적입니다. 이러한 단계를 학습하는 것이 LLM 에이전트의 향상에 핵심입니다. 더 효과적이고 효율적인 에이전트 조정을 실현하기 위해, ATLaS를 제안합니다. ATLaS는 전문가 프로젝트의 중요한 단계를 특정하고, 이를 대상으로 LLM을 조정하여 비용을 줄입니다. 훈련의 초점을 적어도 중요な 단계에 맞추는 것이 우리 방법이, 모든 프로젝트의 과적합 위험을 줄이고 다양한 환경과 태스크에서 일반화를 촉진하는 것입니다. 확장된 실험에서, ATLaS가 선택한 30%의 중요な 단계에 대해 조정된 LLM은 모든 단계에 대해 조정된 LLM과 최근의 오픈 소스 LLM 에이전트에 비해 뛰어납니다. ATLaS는 다양한 환경과 생태계와의 상호작용을 수행하는 챗봇의 기본적인 기술을 유지하고 향상시킵니다.",
      "upvotes": 4,
      "discussionId": "67c7c185203958ca9c091751"
    },
    "publishedAt": "2025-03-04T22:17:48.386Z",
    "title": "ATLaS: Agent Tuning via Learning Critical Steps",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02197.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02878",
      "authors": [
        {
          "_id": "67c7bf7c40de8b1b534d23fa",
          "user": {
            "_id": "635d76ce94e5b275ca74b967",
            "avatarUrl": "/avatars/5d9a389e5fd558c0b8f0724bf0838a3e.svg",
            "isPro": false,
            "fullname": "Ethan Mendes",
            "user": "emendes3",
            "type": "user"
          },
          "name": "Ethan Mendes",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-05T03:06:00.588Z",
          "hidden": false
        },
        {
          "_id": "67c7bf7c40de8b1b534d23fb",
          "name": "Alan Ritter",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T18:58:11.000Z",
      "title": "언어 모델은 상태 가치의 평가를 자동으로 개선하고 더 좋은 검색을 실현할 수 있습니다.",
      "summary": "초록 데이터의 태스크 완료 보상이나 인간의 지도를 수집하기 위한 다단계 추론 태스크는 일반적으로 비용과 시간이 소요되며, 특히 웹 태스크와 같은 상호작용 영역에서는 더욱 어려워질 수 있습니다. 이러한 문제를 해결하기 위해, 우리는 상태 변화 다이나믹스를 활용한 자동 전환 학습 방법을 제안합니다. 이 방법은 언어 모델 제어된 탐색을 효과적으로 지도하는 가치 모델을 훈련하는 데 도움이 됩니다. 우리는 중간 크기(8억 파라미터)의 오픈 가중치의 가치 모델이 자동 전환 학습을 통해 프론티어의 LLM(예: gpt-4o)를 사용한 가치 모델과 같은 성능을 달성할 수 있음을 확인했습니다. 또한, 자동 전환 학습을 통해 이전의 LLM 기반의 트리 탐색에 비해 20%의 효과가 향상되었으며, 37배의 비용 절감을 확인하며, 초록 데이터의 보상에 의존하지 않도록 개선되었습니다.",
      "upvotes": 4,
      "discussionId": "67c7bf7e40de8b1b534d2491",
      "githubRepo": "https://github.com/ethanm88/self-taught-lookahead"
    },
    "publishedAt": "2025-03-04T22:07:18.480Z",
    "title": "Language Models can Self-Improve at State-Value Estimation for Better Search",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02878.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635d76ce94e5b275ca74b967",
      "avatarUrl": "/avatars/5d9a389e5fd558c0b8f0724bf0838a3e.svg",
      "fullname": "Ethan Mendes",
      "name": "emendes3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02876",
      "authors": [
        {
          "_id": "67c7bd7149b52e85403758f8",
          "user": {
            "_id": "6308bae5c038bf42d56a98e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yrslTwUe_vy_ZJha1H83m.png",
            "isPro": false,
            "fullname": "Dmitry Nechaev",
            "user": "mgvz",
            "type": "user"
          },
          "name": "Dmitry Nechaev",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-05T03:07:52.944Z",
          "hidden": false
        },
        {
          "_id": "67c7bd7149b52e85403758f9",
          "user": {
            "_id": "6655b0b9d6c043f39719eaaf",
            "avatarUrl": "/avatars/66138e67ef3be41f29857b285b37adff.svg",
            "isPro": false,
            "fullname": "Alex Pchelnikov",
            "user": "alpchel",
            "type": "user"
          },
          "name": "Alexey Pchelnikov",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-05T03:07:35.092Z",
          "hidden": false
        },
        {
          "_id": "67c7bd7149b52e85403758fa",
          "name": "Ekaterina Ivanova",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T18:57:12.000Z",
      "title": "スパイダー： 하나의 상세한 다관절 서브젝트 데이터 세트와 베이스라인 모델",
      "summary": "AI의 계산病理학의 발전에는, 큰 규모의 고품질으로 다양한 데이터 세트가 필요하지만, 현재 공개된 데이터 세트는 장기의 다양성, 클래스의 커버리지, 또는 어노테이션의 품질에 한계가 있습니다. 이를 메꾸기 위해, SPIDER(SUB-JUTUDE PATCHNESS IMAGE DISCLAIMER REPOSITORY)를 소개합니다. SPIDER는 가장 큰 공개 데이터 세트로, 다양한 장기 유형을 포함하고 있으며, 피부, 칼로린컬, 세보크스 등을 포함합니다. SPIDER는 전문가의 패치네스 체크를 통해 고품질의 어노테이션을 제공하고, 주변의 패치네스를 포함하여 공간적 컨텍스트를 제공하여 분류 성능을 향상시킵니다.\n\nSPIDER의 데이터 세트와 함께, HIBOR-L의 기초 모델을 특징 추출기로, 어텐션 기반의 분류 헤드와 결합하여 훈련된 SPIDER의 기본 모델을 소개합니다. 이러한 모델은 다양한 조직 분류에 있어 가장 선진적인 성능을 달성하며, 미래의 디지털 패치네스 연구의 강력한 벤치마크로 활용될 수 있습니다. 패치네스 분류보다, 이 모델은 중요한 영역의 신속한 인식, 정량적인 조직 메트릭을 가능하게 하며, 다모드 접근의 기반을 구축합니다.\n\n데이터 세트와 훈련된 모델은 연구의 진보, 재현성, AI 주도의 패치네스 개발에 대한 공개되어 있습니다. 아래 URL로 접근하세요: https://github.com/HistAI/SPIDER",
      "upvotes": 2,
      "discussionId": "67c7bd7649b52e8540375a34"
    },
    "publishedAt": "2025-03-04T22:08:26.811Z",
    "title": "SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and Baseline Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02876.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6308bae5c038bf42d56a98e5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yrslTwUe_vy_ZJha1H83m.png",
      "fullname": "Dmitry Nechaev",
      "name": "mgvz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02268",
      "authors": [
        {
          "_id": "67c7ebafdf15f5978ac987c3",
          "name": "Wenjia Jiang",
          "hidden": false
        },
        {
          "_id": "67c7ebafdf15f5978ac987c4",
          "name": "Yangyang Zhuang",
          "hidden": false
        },
        {
          "_id": "67c7ebafdf15f5978ac987c5",
          "name": "Chenxi Song",
          "hidden": false
        },
        {
          "_id": "67c7ebafdf15f5978ac987c6",
          "name": "Xu Yang",
          "hidden": false
        },
        {
          "_id": "67c7ebafdf15f5978ac987c7",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T04:34:09.000Z",
      "title": "GUIアガント의 발전 - 스마트폰 전문가로서의 기능성",
      "summary": "최근의 대 언어 모델(LLM)의 발전으로, GUI(그래픽 사용자 인터페이스)와 상호작용할 수 있는 지능적인 LLM 기반의 에이전트의 개발이 진행되고 있습니다. 이러한 에이전트는 강력한 인공 지능 능력과 적응성을 보여주며, 일반적으로 정의된 규칙이 필요로 하는 복잡한 작업들을 수행할 수 있습니다. 그러나 LLM 기반의 에이전트는 일반적인 작업에서 단계별 이유의 관계가 적절하지 않고, 특히 루틴적인 작업에서는 효율이 저하되어 있습니다. 반면, 전통적인 규칙 기반의 시스템은 효율이 높지만, 새로운 시나리오에 적응하는 지능과 유연성이 부족합니다. 이러한 문제를 해결하기 위해, 우리는 GUI 에이전트에 대해 새로운 진화적인 프레임워크를 제안하고, 효율을 높일 수 있는 동시에 지능과 유연성을 유지하는 것을 목표로 하고 있습니다. 우리의 접근법은 에이전트의 작업 수행 기록을 기록하는 메모리 기능을 사용합니다. 이 기록을 분석하여, 에이전트는 재현 가능한 행동 시퀀스를 특정하고, 낮은 수준의 동작을 대체하여 높은 수준의 행동을 진화시켜 효율을 향상시킬 수 있습니다. 이로써, 에이전트는 복잡한 이유를 필요로 하는 작업에 집중할 수 있으며, 루틴적인 행동을 쉽게 수행할 수 있습니다. 여러 벤치마크 태스크에 대한 실험 결과를 통해, 우리의 접근법은 현재의 방법과 비교하여 효율과 정확도의 향상이 명확히 확인되었습니다. 코드는 오픈 소스로 제공되며, 발전하는 연구에 지원을 제공합니다.",
      "upvotes": 1,
      "discussionId": "67c7ebb5df15f5978ac98975"
    },
    "publishedAt": "2025-03-05T01:15:42.467Z",
    "title": "AppAgentX: Evolving GUI Agents as Proficient Smartphone Users",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64196320ed725fef64419c2a/dUDWK6xfRd9uVZz77V0K6.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02268.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64196320ed725fef64419c2a",
      "avatarUrl": "/avatars/96feb22fb5e8931d6c9e0ea06148266f.svg",
      "fullname": "Chi Zhang",
      "name": "DrChiZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]