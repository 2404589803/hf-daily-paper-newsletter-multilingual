[
  {
    "paper": {
      "id": "2502.08590",
      "authors": [
        {
          "_id": "67ad79552fdac6537b43f120",
          "name": "Yujie Zhou",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f121",
          "name": "Jiazi Bu",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f122",
          "name": "Pengyang Ling",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f123",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f124",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f125",
          "name": "Qidong Huang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f126",
          "name": "Jinsong Li",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f127",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f128",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:31.817Z",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f129",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12a",
          "name": "Anyi Rao",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12b",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12c",
          "name": "Li Niu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T17:24:19.000Z",
      "title": "Light-A-Video: 진보적인 빛의 융합에 의한 무학습 비디오 재점화",
      "summary": "최근의 이미지의 조명 재설정 모델의 발전은 대규모 데이터셋과 사전 학습된 확산 모델에 의해 주도되어 왔습니다. 이러한 발전은 일관된 조명의 설정을 가능하게 했지만, 이미지의 조명 재설정은 주로 과도한 학습 비용과 다양성이 없는 고품질 이미지의 조명 재설정 데이터셋의 부족으로 지연되어 있습니다. 이미지의 조명 재설정 모델을 프레임별로 간단하게 적용하면 조명의 불연속성과 재설정된 외관의 불연속성이 발생하여 생성되는 이미지에 킥이 발생합니다. 본 논문에서는 Light-A-Video라는 학습이 필요 없는 접근을 제안하고, 시간에서 평활한 이미지의 조명 재설정을 실현합니다. 이미지의 조명 재설정 모델로부터 재활용된 Light-A-Video는 조명의 일관성을 향상시키기 위해 두 가지 중요한 기술에 대해 도입하고 있습니다. 먼저, 프레임 간 상호작용을 강화하고 배경의 조명의 생성을 안정화하기 위해 Consistent Light Attention (CLA) 모듈을 설계합니다. 다음으로, 빛의 전파의 물리적 원리를 활용하여 원본 이미지의 외관과 재설정된 외관을 선형 블렌딩하여 Progressive Light Fusion (PLF) 전략을 사용하여 조명의 시간의 흐름을 평활하게 보장합니다. 실험은 Light-A-Video가 재설정된 이미지의 시간의 일관성을 향상시키고 이미지의 품질을 유지하며, 프레임 간 조명의 일관성의 이동을 보장하는 것을 보여주었습니다. 프로젝트 페이지: https://bujiazi.github.io/light-a-video.github.io/",
      "upvotes": 25,
      "discussionId": "67ad79572fdac6537b43f189"
    },
    "publishedAt": "2025-02-12T23:47:56.223Z",
    "title": "Light-A-Video: Training-free Video Relighting via Progressive Light Fusion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07870",
      "authors": [
        {
          "_id": "67ad79cb60ec3f444b21cbcb",
          "name": "Alex Jinpeng Wang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcc",
          "name": "Dongxing Mao",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcd",
          "name": "Jiawei Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbce",
          "name": "Weiming Han",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcf",
          "name": "Zhuobai Dong",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd0",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd1",
          "name": "Yiqi Lin",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd2",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd3",
          "name": "Libo Qin",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd4",
          "name": "Fuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd5",
          "name": "Lijuan Wang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd6",
          "name": "Min Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:59:19.000Z",
      "title": "TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5M: TextAtlas5",
      "summary": "최근, 텍스트 조건付き 이미지 생성은 주목을 받았다고, 긴 및 복잡한 텍스트 프로ンプト를 처리할 수 있게 되었습니다. 일상 생활에서 광고, 디자인 그래픽, 시그너 등 복잡한 텍스트가 등장하며, 텍스트와 시각의 통합이 복잡한 정보를 전달하는 데 중요합니다. 그러나 이러한 발전에도 불구하고, 긴 문장을 포함하는 이미지의 생성은 장기적인 문제로, 현재의 데이터셋의 제한으로 주요 원인입니다. 이를 해결하기 위해, TextAtlas5M라는 새로운 데이터셋을 소개합니다. 이 데이터셋은 긴 문장의 시각화를 평가하기 위해 특별히 설계되었습니다. 데이터셋은 500만 장의 긴 문장 생성된 이미지로 구성되며, 다양한 데이터 타입을 조합하여, 대규모 생성 모델의 긴 문장 이미지 생성을 상세하게 평가할 수 있습니다. 또한, 3 데이터 영역을 가로지르는 3000 장의 인간 개선 테스트 세트 TextAtlasEval을 구축하여, 텍스트 조건付き 생성의 가장 넓은 벤치마크를 구축했습니다. 평가에 따르면, TextAtlasEval 벤치마크는 가장 先端의 프로プライエーションモデル（例：GPT4oとDallE-3）에 대해서도 큰 문제를 제기하지만, 그 오픈 소스 컨タラップ는 큰 성능 차이를 보여주고 있습니다. 이러한 증거로, TextAtlas5M은 미래의 텍스트 조건付き 이미지 생성 모델의 훈련과 평가에 효과적인 데이터셋으로 자리잡습니다.",
      "upvotes": 24,
      "discussionId": "67ad79d260ec3f444b21cd1f"
    },
    "publishedAt": "2025-02-12T23:50:07.130Z",
    "title": "TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08639",
      "authors": [
        {
          "_id": "67ad5f25cad644864b436186",
          "name": "Qinghe Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436187",
          "name": "Yawen Luo",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436188",
          "name": "Xiaoyu Shi",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436189",
          "name": "Xu Jia",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618a",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618b",
          "name": "Tianfan Xue",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618c",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618d",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618e",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618f",
          "name": "Kun Gai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T18:55:36.000Z",
      "title": "映画의 3D 인식과 제어 가능한 프레임워크에서 텍스트로부터 동영상 생성\n\n(注意：虽然您要求不添加任何解释或额外的文本，但为了确保翻译的准确性和专业性，我在翻译时保持了原文的结构和术语的精确性。)",
      "summary": "이 연구에서는 새로운 프레임워크인 CineMaster를 제안합니다. 이 프레임워크는 3D 인식을 수행하며, 텍스트로부터 애니메이션을 생성할 수 있습니다. 우리의 목표는 사용자에게 프로의 영화감독과 같은 수준의 제어 가능성 제공하는 것입니다. 장면 내 물체의 정확한 배치, 3D 공간에서 물체와 카메라의 유연한 조작, 렌더링 된 프레임의 직관적인 순서 제어를 가능하게 합니다. 이를 달성하기 위해, CineMaster는 두 단계로 동작합니다. 첫 번째 단계에서는 사용자가 직관적으로 3D 인식한 조건부 신호를 구축하기 위한 인터랙티브 작업 흐름을 설계합니다. 3D 공간에서 물체의 Bounding Box를 위치지정하고, 카메라의 이동을 정의합니다. 두 번째 단계에서는, 이러한 제어 신호( 렌더링 된 깊이 맵, 카메라의 이동 경로, 물체의 클래스 라벨)가 텍스트로부터 애니메이션로의 확산 모델의 가이드로 되고, 사용자가 원하는 애니메이션 내용을 생성합니다. 또한, 3D 물체의 움직임과 카메라의 자세에 대한 설명된 데이터 세트의 부족을 극복하기 위해, 큰 규모의 애니메이션 데이터에서 3D Bounding Box와 카메라의 이동 경로를 추출하기 위한 자동 데이터 설명 플러그인 플레어버를 신중히 구축합니다. 세부적인 질적 및 양적인 실험은 CineMaster가 현재의 방법보다 크게 초월하고, 명확한 3D 인식을 하는 텍스트로부터 애니메이션의 생성을 실현하는 것을 보여줍니다. 프로젝트 페이지: https://cinemaster-dev.github.io/",
      "upvotes": 22,
      "discussionId": "67ad5f26cad644864b4361cf"
    },
    "publishedAt": "2025-02-12T21:55:44.479Z",
    "title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6063
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08047",
      "authors": [
        {
          "_id": "67ad92bfbbf3810ab20595c2",
          "name": "Henry Hengyuan Zhao",
          "hidden": false
        },
        {
          "_id": "67ad92bfbbf3810ab20595c3",
          "name": "Difei Gao",
          "hidden": false
        },
        {
          "_id": "67ad92bfbbf3810ab20595c4",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T01:06:10.000Z",
      "title": "WorldGUI: 오버플로우 테스트를 위한 종합적인 데스크톱 GUI 자동화\n\n(注意：虽然您要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，我在翻译中保持了原文的结构和格式，并确保了专业术语的准确性。)",
      "summary": "현재의 GUI 에이전트는 GUI 요소의 조지닝에 기반하여 뛰어난 성능을 달성하고 있습니다. 그러나 계획이 매우 어려운 문제를 남겨두고 있으며, 특히 환경의 초기 상태에 대한 민감성에 의해 이러한 문제를 강하게 느끼게 됩니다. 구체적으로는, 목표의 소프트웨어가 열려 있지 않거나, 인터페이스가 기본 상태가 아니라는 초기 상태의 微妙한 차이는 계획 오류를 유발하는 경우가 많습니다. 이 문제는 실제 사용자 시나리오에서 광범위하게 존재하지만, 현재의 벤치마크는 이를 평가할 수 없습니다. 본 논문에서는, WorldGUI라는 새로운 GUI 벤치마크를 소개합니다. 이 벤치마크는 실제 컴퓨터 사용자 상호작용을 모방하기 위해 다양한 초기 상태를 가진 GUI 태스크를 설계하고 있습니다. 이 벤치마크는 PowerPoint, VSCode, Adobe Acrobat 등 10개의 인기 있는 소프트웨어의 광범위한 범위의 태스크를 포함합니다. 또한, 동적인 GUI 자동화 태스크에 대응하기 위해, GUI-Thinker라는 전체적인 프레임워크를 제안합니다. 이 프레임워크는 평가 구조를 활용하여 GUI 상호작용의 불확실성과 복잡성을 효과적으로 관리할 수 있습니다. 실험 결과를 통해, GUI-Thinker는 WorldGUI 태스크의 성공률에서 Claude-3.5(Computer Use)에 대해 14.9%의 개선률을 기록하며, 우리의 비판적 사고에 기반한 프레임워크의 효과성을 밝혀줍니다.",
      "upvotes": 19,
      "discussionId": "67ad92c1bbf3810ab205961c"
    },
    "publishedAt": "2025-02-13T01:39:08.775Z",
    "title": "WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08047.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647d7eb9770c299e56f5b39b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647d7eb9770c299e56f5b39b/CC5JJgkyLkXOxw-BeT4G5.jpeg",
      "fullname": "Hengyuan Zhao",
      "name": "hhenryz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07563",
      "authors": [
        {
          "_id": "67ad7929dc2968691c241147",
          "user": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "isPro": false,
            "fullname": "Weigao Sun",
            "user": "weigao266",
            "type": "user"
          },
          "name": "Weigao Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:37.445Z",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c241148",
          "user": {
            "_id": "66ea643899af9ac3463639b1",
            "avatarUrl": "/avatars/252d470e761a57834dee3dbc60dfefed.svg",
            "isPro": false,
            "fullname": "Disen Lan",
            "user": "landisen",
            "type": "user"
          },
          "name": "Disen Lan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:33.746Z",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c241149",
          "name": "Yiran Zhong",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c24114a",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c24114b",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T14:01:39.000Z",
      "title": "LASP-2: 선형 어텐션의 순서 병렬화 재고와 그 하이브리드",
      "summary": "線형 순서 모델링 접근 방식과 같이, 선형 어텐션 등은 순서 길이에 관계없이 선형 시간의 훈련과 常時 메모리의 추론을 제공하여 우수한 점들을 제공합니다. 그러나 현재의 순서 병렬 계산(SP) 방법들은 선형 어텐션의 오른쪽 곱 마크 특징에 최적화되지 않았거나, Ring Style의 통신 전략을 사용함으로써, 계산 병렬성 저하와 긴 순서 분산 시스템에서의 scalability의 제한이 있습니다. 본 논문에서는, LASP-2라는 새로운 SP 메소드를 소개하고, 선형 어텐션 transformer 모델의 훈련 시, 긴 입력 순서를 가지는 모델을 학습하는 데에 통신과 계산의 병렬성을 모두 향상시키는 것을 목표로 합니다. LASP와 비교하여, LASP-2는 선형 어텐션 계층에서 SP의 최소한의 통신 요구 사항을 재고시하고, LASP의 모든 통신-계산 작업 흐름을 재구성하고 있습니다. 이를 통해, 간접 메모리 상태상의 하나의 AllGather 통신만 필요하게 되고, 짧은 길이에 의존하지 않는 크기로, 통신과 계산의 병렬성 및 그 중첩으로 인해 큰 향상이 실현됩니다. 또한, LASP-2H라는 LASP-2의 확장 버전을 제안하고, 표준 어텐션 모듈에 유사한 통신 재설계를 적용하여, 선형 어텐션과 표준 어텐션 계층을 혼합한 혼합 모델에 대한 효율적인 SP 해결책을 제공합니다. Linear-Llama3 모델의 평가에서, Llama3의 표준 어텐션을 선형 어텐션으로 대체한 버전에서, LASP-2와 LASP-2H의 효과성을 보여주었습니다. 특히, LASP-2는 64 GPU에서 2048K의 순서 길이로 LASP에 비해 15.2%의 훈련 속도 향상, Ring Attention에 비해 36.6%의 훈련 속도 향상을 실현했습니다. 코드는 https://github.com/OpenSparseLLMs/Linear-MoE의 일부로 릴리즈되어 있습니다.",
      "upvotes": 17,
      "discussionId": "67ad792adc2968691c241173"
    },
    "publishedAt": "2025-02-12T23:47:31.651Z",
    "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07563.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07346",
      "authors": [
        {
          "_id": "67ac4e046b8c86e0cc7988f0",
          "user": {
            "_id": "649d1d4c379eada9a580cf59",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d1d4c379eada9a580cf59/ucXv7KoJDEB3Phgn-Dn5E.png",
            "isPro": false,
            "fullname": "xuhuang",
            "user": "ggdcr",
            "type": "user"
          },
          "name": "Xu Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:25:17.555Z",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f1",
          "name": "Wenhao Zhu",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f2",
          "name": "Hanxu Hu",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f3",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f4",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f5",
          "name": "Shujian Huang",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f6",
          "name": "Fei Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T08:17:19.000Z",
      "title": "BenchMAX: 대규모 언어 모델의 상세 다언어 평가 시스템",
      "summary": "이전의 다언어 벤치마크는 주로 간단한 이해 태스크에 초점을 맞추었지만, 대규모 언어 모델(LLMs)에서는 단어 따라, 이유, 긴 문맥 이해, 코드 생성 등 고차원 능력을 강조하고 있습니다. 그러나 이러한 고차원 능력을 여러 언어에서 측정하는 방법은 조사가 부족합니다. 이 문제를 해결하기 위해, BenchMAX라는 다언어 평가 벤치마크를 소개합니다. 이 것은 중요한 능력을 여러 언어에서 공정한 비교를 가능하게 합니다. 고품질을 유지하기 위해, 데이터는 영어에서 16개의 다른 언어로 기계 번역한 후, 3명의 원어민이 각 태스크의 샘플을 독립적으로 기록합니다. 또한, 데이터셋 구축에서 새로운 번역 도전도 제시됩니다. BenchMAX에서 확장된 실험은 각 언어에서의 핵심 능력의 효과적 차이를 밝혀, 모델 크기가 증가해도 성능의 차이를 보여줍니다. BenchMAX는 여러 언어의 평가를 위한 균일한 플랫폼으로 제공되며, 여러 언어의 언어 모델 개발을 촉진하는 테스트 벤치를 제공합니다. 데이터셋과 코드는 공개적으로 액세스 가능합니다.",
      "upvotes": 14,
      "discussionId": "67ac4e056b8c86e0cc798952"
    },
    "publishedAt": "2025-02-13T03:34:47.873Z",
    "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07346.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d1d4c379eada9a580cf59",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d1d4c379eada9a580cf59/ucXv7KoJDEB3Phgn-Dn5E.png",
      "fullname": "xuhuang",
      "name": "ggdcr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.08127",
      "authors": [
        {
          "_id": "67ad5ca29109885ce9b859e4",
          "name": "Lingfei Qian",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e5",
          "name": "Weipeng Zhou",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e6",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e7",
          "name": "Xueqing Peng",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e8",
          "user": {
            "_id": "63b58ed5889aa6707f0bb0f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
            "isPro": true,
            "fullname": "Jimin Huang",
            "user": "jiminHuang",
            "type": "user"
          },
          "name": "Jimin Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T02:44:52.979Z",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e9",
          "user": {
            "_id": "6479f4317c18dca75e9a9324",
            "avatarUrl": "/avatars/9aa709230b057f57ee4415c04a622c63.svg",
            "isPro": false,
            "fullname": "Xie",
            "user": "QianqianXie1994",
            "type": "user"
          },
          "name": "Qianqian Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:22:01.539Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T05:13:04.000Z",
      "title": "Theoretical Applicability of Inference-Augmented LLMs in Finance",
      "summary": "최근의 대규모 언어 모델(LLMs)의 발전은 강력한 일반적인 논리 능력을 보여주고 있지만, 금융 논리 분야에서의 효과는 조사가 부족합니다. 본 연구에서는, 금융 텍스트, 테이블 데이터, 방정식을 포함하는 3가지 복잡한 금융 태스크를 대상으로, 수치 논리, 테이블 세부 해석, 금융 용어 이해, 긴 문맥 처리, 방정식 기반의 문제 해결을 평가하고, 16가지 강력한 논리 모델과 일반적인 LLMs를 사용합니다. 결과적으로, 데이터 세트의 개선과 사전 학습의 개선이 금융 논리 분야에서의 효과를 높일 수 있다는 것을 확인했습니다. 그러나 일반적인 강화 학습에 대한 CoT 조정은 항상 일관된 효과를 얻는 것이 아닙니다. 또한, 모든 논리 전략은 긴 문맥이나 다수의 테이블 태스크에 대한 성능 향상에 문제가 있습니다. 이러한 제한을 해결하기 위해, Llama-3.1-8B-Instruct를 기반으로 한 금융 논리 향상 모델을 개발하고, CoT 조정과 설명 전문적인 논리 경로를 사용한 강화 학습을 수행했습니다. 하나의 금융 데이터 세트에서 간단한 조정만으로, 모델은 모든 태스크에서 10%의 긍정적인 성능 향상을 달성하고, 모든 8B 모델을 초과하며, 더 나아가 Llama3-70B-Instruct와 Llama3.1-70B-Instruct의 평균값을 초과했습니다. 이러한 결과를 통해, 금융 태스크에서 설명 전문적인 변경의 필요성을 강조하고, 다 테이블 논리, 긴 문맥 처리, 금융 용어 이해와 같은 미래의 방향성을 제시하고 있습니다. 모든 데이터 세트, 모델, 코드는 공개적으로 제공됩니다. 또한, 미래의 데이터 세트와 모델의 벤치마크에 대한 리드보드를 도입했습니다.",
      "upvotes": 13,
      "discussionId": "67ad5ca59109885ce9b85a5b"
    },
    "publishedAt": "2025-02-12T21:45:28.944Z",
    "title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08127.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63b58ed5889aa6707f0bb0f4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
      "fullname": "Jimin Huang",
      "name": "jiminHuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07864",
      "authors": [
        {
          "_id": "67ad5b3a007d78b391946a57",
          "user": {
            "_id": "643f55d4ec817b766686438a",
            "avatarUrl": "/avatars/0feb460432c92ab9ada0d417a7a38f6a.svg",
            "isPro": false,
            "fullname": "mengfanxu",
            "user": "fxmeng",
            "type": "user"
          },
          "name": "Fanxu Meng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:22:03.808Z",
          "hidden": false
        },
        {
          "_id": "67ad5b3a007d78b391946a58",
          "name": "Zengwei Yao",
          "hidden": false
        },
        {
          "_id": "67ad5b3a007d78b391946a59",
          "name": "Muhan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:20:18.000Z",
      "title": "TransMLA: 다헤드 잠재적 注意는, 당신에게 필요하신 것입니다.",
      "summary": "현대의 대규모 언어 모델(LLMs)은 현재의 하드웨어에서 많은 통신 버튼neck 문제를 직면하고 있으며, 또는 단순한 계산 제약으로 제한받는 경우가 많다. 다룹레이어 엔티티(MLA)는 이러한 도전을 해결하기 위해 키-밸류(KV) 레이어에서 저레킹 행렬을 사용하며, 이를 통해 압축된 잠재적인 KV 상태는 캐시에 저장될 수 있게 한다. 이 접근 방식은 전통적인 다룹레이어 엔티티에 비해 KV 캐시 크기를 크게 줄이고, 추론 속도를 크게 향상시킬 수 있다. 또한, MLA는 표현력을 향상시키기 위해 업프로젝트 행렬을 사용하며, 추가적인 계산을 통해 통신 버튼neck를 줄이는 데 기여한다. 그러나 MLA는 Deepseek V2/V3/R1에서 효율성과 효과성을 보여주지만, 많은 주요 모델 제공者是 GQA를 의존하고, MLA의 도입에 대한 계획이 없으며, 이는 MLA의更广泛 사용에 한계가 된다. 본 논문에서는, GQA는 MLA와 같은 표현력을 유지할 수 있는 반면, 반대의 경우 불가능함을 보여주고, MLA의更广泛 사용의 촉진에 대해, 우리는 **TransMLA**라는 훈련 후의 방법을 소개하며, 현재로써 광범위하게 사용되고 있는 GQA 기반의 사전 학습 모델(예: LLaMA, Qwen, Mixtral)을 MLA 기반의 모델로 변환하는 방법을 제시한다. 변환 후, 모델은 추가적인 학습을 통해 표현력을 향상시킬 수 있으며, KV 캐시 크기를 증가시키지 않는다. 또한, 우리는 MLA에 특화된 추론 속도 향상 기술을 개발하며, 변환된 모델에서도 낮은 레틴시를 유지하며, Deepseek R1의 더 효율적인 디스틸을 가능하게 한다.",
      "upvotes": 13,
      "discussionId": "67ad5b3b007d78b391946a79"
    },
    "publishedAt": "2025-02-12T21:41:19.791Z",
    "title": "TransMLA: Multi-head Latent Attention Is All You Need",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643f55d4ec817b766686438a",
      "avatarUrl": "/avatars/0feb460432c92ab9ada0d417a7a38f6a.svg",
      "fullname": "mengfanxu",
      "name": "fxmeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.08606",
      "authors": [
        {
          "_id": "67ad77f9cd8de299e5049c05",
          "name": "Dan Busbridge",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c06",
          "name": "Amitis Shidani",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c07",
          "name": "Floris Weers",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c08",
          "name": "Jason Ramapuram",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c09",
          "name": "Etai Littwin",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c0a",
          "name": "Russ Webb",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T17:52:47.000Z",
      "title": "디스티루션 스케일링 라스",
      "summary": "우리는 계산 기반의 교사와 학생 모델 간의 할당에 기반한 간략화 모델의 성능을 예측하는 간략화 스케일링 도구를 제공합니다. 우리가 발견한 것은 스케일링에 따른 간략화의 사용과 관련된 위험을 줄이는 데 도움이 됩니다. 교사와 학생 모델의 계산 할당은 학생 모델의 성능을 극대화하는 것을 목표로 수행됩니다. 교사가 존재하거나 교사가 훈련이 필요할 때의 간략화의 최적 계산 레시피를 제공합니다. 학생이 다수일 때나 교사가 이미 존재할 때, 간략화는 훈련된 예측 학습보다 계산 수준이 학생 모델의 크기에 예측적으로 증가하기 전에 향상됩니다. 반면, 학생이 한 명일 때나 교사도 훈련이 필요할 때, 훈련된 학습을 우선시해야 합니다. 또한 우리의 규모가 큰 간략화 연구의 결과를 통해 얻은 피드백을 제공하여 간략화의 이해를 깊게 하고 실험 설계에 정보를 제공합니다.",
      "upvotes": 8,
      "discussionId": "67ad77fccd8de299e5049d06"
    },
    "publishedAt": "2025-02-12T23:41:41.281Z",
    "title": "Distillation Scaling Laws",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08606.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6063
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08168",
      "authors": [
        {
          "_id": "67ad5f32d1a5243cc4fa38ad",
          "user": {
            "_id": "64a0ed5ed5374ca472cfb0ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
            "isPro": false,
            "fullname": "ZhimingMa",
            "user": "JimmyMa99",
            "type": "user"
          },
          "name": "Zhiming Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:57.239Z",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38ae",
          "name": "Xiayang Xiao",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38af",
          "name": "Sihao Dong",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b0",
          "name": "Peidong Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b1",
          "name": "HaiPeng Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b2",
          "name": "Qingyun Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T07:19:36.000Z",
      "title": "SARChat-Bench-2M: SAR 이미지의 다 태스크 비지션 언어 벤치마크에 대한 해석",
      "summary": "合成孔径雷達（SAR）遥感 이미지 해석 분야에서, 비전 언어 모델（VLMs）은 자연어 처리와 이미지 이해에 있어서 놀라운 진전을 이루고 있지만, 프로젝트 영역에서의 적용은 도메인 지식이 부족하여 제한되어 있습니다. 본 논문에서는 최초의 대규모 다형 타입 다이어로그 데이터 세트를 제안합니다. 이 데이터 세트는 약 200만 장의 고품질의 이미지-텍스트 페어를 포함하며, 세부적인 타겟 설명을 가지고 있으며 다양한 시나리오를 기록하고 있습니다. 이 데이터 세트는 시각 이해나 물체 검출 등 중요한 태스크를 지원하며, 독특한 혁신성을 가지고 있습니다: 본 논문에서는 SAR 분야의 비전 언어 데이터 세트와 벤치마크를 개발하고, VLMs의 SAR 이미지 해석 능력을 평가할 수 있으며, 다양한 원격 관측 분야에서 다형 타입 데이터 세트의 구축에 프레임워크를 제공합니다. 16개의 주요 VLMs에 대한 실험으로, 데이터 세트의 효과를 완전히 증명하였으며, SAR 분야의 첫 번째 다형 태스크 다이어로그 벤치마크가 성공적으로 설립되었습니다. 이 프로젝트는 https://github.com/JimmyMa99/SARChat에서 릴리즈되며, SAR 비전 언어 모델의 깊은 개발과 광범위한 응용을 촉진하는 것을 목표로 합니다.",
      "upvotes": 8,
      "discussionId": "67ad5f37d1a5243cc4fa399c"
    },
    "publishedAt": "2025-02-12T21:57:30.420Z",
    "title": "SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/LvHzRQCttMAvKS-LM0ZDH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08168.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64a0ed5ed5374ca472cfb0ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
      "fullname": "ZhimingMa",
      "name": "JimmyMa99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.08524",
      "authors": [
        {
          "_id": "67ad783da2808b57a3cd3316",
          "name": "Jihoon Tack",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3317",
          "name": "Jack Lanchantin",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3318",
          "name": "Jane Yu",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3319",
          "name": "Andrew Cohen",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331a",
          "name": "Ilia Kulikov",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331b",
          "name": "Janice Lan",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331c",
          "name": "Shibo Hao",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331d",
          "name": "Yuandong Tian",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331e",
          "name": "Jason Weston",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331f",
          "user": {
            "_id": "659a395421a7431643caedda",
            "avatarUrl": "/avatars/c1e0bbcedce68fe3b4fe39e0cf01c65c.svg",
            "isPro": false,
            "fullname": "Xian Li",
            "user": "xlxxl",
            "type": "user"
          },
          "name": "Xian Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T04:42:38.302Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T16:00:11.000Z",
      "title": "LLM의 연속 개념付き予習練\n\n(Note: The provided text \"只需返回翻译结果，不要添加任何解释或额外的文本。\" is in Chinese, which translates to \"Only return the translation result, do not add any explanation or additional text.\" in English. However, since the task was to translate the given English text into Korean, this note is not part of the translation.)",
      "summary": "다음 토큰 예측은 대규모 언어 모델의 사전 학습에서 표준적인 훈련 목표로 일관되게 사용되어 왔습니다. 토큰 수준의 구조 오류를 최적화함으로써 표현을 학습하고 있습니다. 우리는 새로운 사전 학습 프레임워크 \"CoCoMix\"를 제안합니다. 이 프레임워크는 연속적인 개념을 사전 학습된 희소 자동 인코더에서 예측하고 토큰의 은닉 표현과 교차하여 모델의 은닉 상태에 섞습니다. 여러 벤치마크 테스트 (언어 모델링 및 하류 이유론 태스크)에 따라 CoCoMix는 샘플 효율이 높고 표준적인 다음 토큰 예측, 지식 전파 및 포스트 토큰의 삽입을 항상 초월하는 것을 보여주었습니다. 컴퓨터 학습의 프레임워크에서 개념 학습과 교차의 조합이 성능 향상에 중요하다는 것을 발견했습니다. 또한 CoCoMix는 예측된 개념의 직접적인 검토와 수정을 가능하게 하며 모델의 내부 이유론 프로세스를 투명하게 안내하며 해석성 및 조작성을 향상시킵니다.",
      "upvotes": 6,
      "discussionId": "67ad783ea2808b57a3cd3361"
    },
    "publishedAt": "2025-02-12T23:42:44.287Z",
    "title": "LLM Pretraining with Continuous Concepts",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08524.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6063
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06533",
      "authors": [
        {
          "_id": "67accc647e1fcf03e14b1033",
          "user": {
            "_id": "6637cd3e691043ccb248d0fd",
            "avatarUrl": "/avatars/94cf09cf817327be50ecba75f7f60fa1.svg",
            "isPro": false,
            "fullname": "Jean Vassoyan",
            "user": "supertardigrade",
            "type": "user"
          },
          "name": "Jean Vassoyan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:24:24.993Z",
          "hidden": false
        },
        {
          "_id": "67accc647e1fcf03e14b1034",
          "user": {
            "_id": "63da60458658cbc1cc489bd7",
            "avatarUrl": "/avatars/620ce7ea229de7abe4dc9ea93021f0e4.svg",
            "isPro": false,
            "fullname": "Nathanaël Beau",
            "user": "Nbeau",
            "type": "user"
          },
          "name": "Nathanaël Beau",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-12T16:29:25.829Z",
          "hidden": false
        },
        {
          "_id": "67accc647e1fcf03e14b1035",
          "user": {
            "_id": "66470e227d73a39a342866e4",
            "avatarUrl": "/avatars/cb0746295492044c483a470692b9637c.svg",
            "isPro": false,
            "fullname": "Roman Plaud",
            "user": "lecraquito",
            "type": "user"
          },
          "name": "Roman Plaud",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:24:27.330Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T14:56:25.000Z",
      "title": "KL 페나ル티를 무시하고, 중요 토큰 탐색을 강화하여 RL의 미세 조정을 향상시키세요.",
      "summary": "장기적 목표를 달성하는 능력은 현재의 대규모 언어 모델（LLMs）의 개발에서 중요한 문제입니다. 이를 해결하기 위해, 강화학습（RL）을 이용한 조정 학습을 수행할 수 있습니다. 그러나 LLMs의 탐색은 매우 어려워서, 새로운 해결책을 발견하면서도 조정된 모델과 거리를 줄일 수 없기 때문에, 通常는 Kullback-Leibler（KL） 패널티로 제어됩니다. 본 논문에서는, 단순한 산술 임무를 수행하는 작은 언어 모델의 탐색 동선을 조사하고, 조정 학습이 탐색에 미치는 영향과, 최종적인 결과에 큰 영향을 미치는 「KL 토큰」의 중요성을 보여줍니다. 그 결과, KL 패널티에 대한 간단한 개선을 제안하여, RL의 조정 학습 단계의 효율화를 도모합니다.",
      "upvotes": 5,
      "discussionId": "67accc657e1fcf03e14b109e"
    },
    "publishedAt": "2025-02-13T03:47:28.654Z",
    "title": "Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06533.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66470e227d73a39a342866e4",
      "avatarUrl": "/avatars/cb0746295492044c483a470692b9637c.svg",
      "fullname": "Roman Plaud",
      "name": "lecraquito",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06145",
      "authors": [
        {
          "_id": "67ad9fb9731ff0d7da9f40e9",
          "user": {
            "_id": "67ad9f06040354c9105b00bc",
            "avatarUrl": "/avatars/39e9f4c48c93bb33f155390653936fc1.svg",
            "isPro": false,
            "fullname": "LiHu",
            "user": "Hookszdp",
            "type": "user"
          },
          "name": "Li Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:24.286Z",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ea",
          "name": "Guangyuan Wang",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40eb",
          "name": "Zhen Shen",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ec",
          "name": "Xin Gao",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ed",
          "name": "Dechao Meng",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ee",
          "name": "Lian Zhuo",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ef",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40f0",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40f1",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T04:20:11.000Z",
      "title": "アニマット・ニャンニー 2: 환경 보완을 활용한 고품질 캐릭터 이미지 애니메이션",
      "summary": "최근의 Difussion 모델 기반의 특징 맵 애니메이션 방법 중 Animate Anyone와 같은 기술이, 일관된 특징 맵 애니메이션을 생성하는 데 있어 발전하고 있는 것을 알 수 있습니다. 그러나 이러한 기술은 특징 맵과 주변 환경 사이의 합리적인 연관성을 생성할 수 있는 경우가 있습니다. 이를 해결하기 위해, 우리는 Animate Anyone 2를 소개하고, 특징 맵을 환경의 기능에 따라 애니메이션하는 것을 목표로하고 있습니다. 소스 비디오에서 동작 신호를 추출하는 것만으로는 충분하지 않으므로, 환경 표현을 조건부 입력으로捉捉한 것입니다. 환경은 특징 맵을 제외한 영역으로 구성되며, 이 영역에 특징 맵을 생성하면서 환경의 컨텍스트와의 일관성을 유지하는 것입니다. 특징 맵과 환경의 관계를 효과적으로 표현하기 위해, 모양 무관 마스크 전략을 제안하고 있습니다. 또한, 물체의 상호작용의 정확도를 높이기 위해, 물체 가이드를 사용하여 상호작용하는 물체의 특징을 추출하고, 스펙트럴 브렌딩을 사용하여 특징을 注入하는 것을 실현하고 있습니다. 또한, 다양한 동작 패턴을 처리할 수 있는 물체의 자세 조정 전략을 제안하고 있습니다. 실험 결과를 통해 제안된 방법의 우수한 성능을 보여주고 있습니다.",
      "upvotes": 3,
      "discussionId": "67ad9fbb731ff0d7da9f4145"
    },
    "publishedAt": "2025-02-13T03:45:43.646Z",
    "title": "Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06145.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ad9f06040354c9105b00bc",
      "avatarUrl": "/avatars/39e9f4c48c93bb33f155390653936fc1.svg",
      "fullname": "LiHu",
      "name": "Hookszdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06872",
      "authors": [
        {
          "_id": "67ad7da995ff670869168209",
          "name": "Bo Ni",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820a",
          "name": "Zheyuan Liu",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820b",
          "name": "Leyao Wang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820c",
          "name": "Yongjia Lei",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820d",
          "name": "Yuying Zhao",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820e",
          "name": "Xueqi Cheng",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820f",
          "name": "Qingkai Zeng",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168210",
          "name": "Luna Dong",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168211",
          "name": "Yinglong Xia",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168212",
          "name": "Krishnaram Kenthapadi",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168213",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168214",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:27.740Z",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168215",
          "name": "Md Mehrab Tanjim",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168216",
          "name": "Nesreen Ahmed",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168217",
          "name": "Xiaorui Liu",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168218",
          "name": "Wenqi Fan",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168219",
          "name": "Erik Blasch",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821a",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821b",
          "name": "Meng Jiang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821c",
          "name": "Tyler Derr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T06:50:47.000Z",
      "title": "트라スト워스트리치아우가르시온폴라르주온그야이안트란그쥬에시온모듈：아사이드",
      "summary": "レタイブルージングアウゲンレーション（RAG）는, 인공지능 생성 콘텐츠(AIGC)의 문제를 해결하기 위해 설계된 첨단 기술입니다. 콘텐츠 생성에 카테고리 검색을 통합하고, RAG는 신뢰할 수 있는 최신 외부 지식을 제공하며, 헛소리를 줄이고, 광범위한 태스크의 관련성을 보장합니다. 그러나 RAG 패러다임은 성공과 가능성에 관계없이, 최근의 연구에 따라, 강건성 문제, 프라이버시 우려, 대항 공격, 책임 문제 등 새로운 위험들이 발견되어 있습니다. 이러한 위험을 해결하는 것이 RAG 시스템의 미래적 적용에 있어서 중요하며, 그 신뢰성에 직접 영향을 미칩니다. RAG 방법의 신뢰성을 높이는 방법들은 개발되어 있지만, 이 분야의 연구에서 통일적인 시각과 프레임워크가 부족합니다. 따라서, 본 논문에서는 신뢰할 수 있는 RAG 시스템의 개발에 대한 상세한 지도를 제공하기 위해 노력합니다. 논의는 신뢰성, 프라이버시, 안전성, 공정성, 설명성, 책임 문제 등 5가지 주요한 관점으로 중심을 두고 있습니다. 각 관점에서 일반적인 프레임워크와 기술로, 현재의 문제를 이해하고, 기존의 해결책을 평가하며, 향후 연구 방향을 특정하기 위한 구조화된 접근을 제공합니다. 광범위한 도입과 혁신을 촉진하기 위해, 신뢰할 수 있는 RAG 시스템의 영향을 크게 미치는 하류의 애플리케이션도 특별히 언급합니다.",
      "upvotes": 3,
      "discussionId": "67ad7daa95ff670869168251"
    },
    "publishedAt": "2025-02-13T00:06:04.056Z",
    "title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05167",
      "authors": [
        {
          "_id": "67aa583c3a878652daeae02e",
          "user": {
            "_id": "60e4738a8c0ddd18fc27ff88",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e4738a8c0ddd18fc27ff88/lpLeeIW8r85RTY4fGZTva.jpeg",
            "isPro": false,
            "fullname": "Ali Modarressi",
            "user": "amodaresi",
            "type": "user"
          },
          "name": "Ali Modarressi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:42.560Z",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae02f",
          "name": "Hanieh Deilamsalehy",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae030",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:26:01.327Z",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae031",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae032",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae033",
          "name": "Seunghyun Yoon",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae034",
          "name": "Hinrich Schütze",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:49:46.000Z",
      "title": "NoLiMa: 문맥평가를 초과한 직역적 대응보다 문맥평가",
      "summary": "최근의 대규모 언어 모델(LLMs)은 128K에서 1M 토큰의 긴 문맥을 지원하고 있습니다. 이러한 기능들을 평가하기 위해 인기 있는 방법은 \"Nail-In-Up-Lion\"(NIAH) 테스트입니다. 이는 긴 무관한 문맥(「sack」)에서 「nail」(관련 정보)를 검색하는 방식으로 진행됩니다. 이 방법을 확장한 것은 간섭자의 증가, 사실의 순환, 텍스트 내의 논리 등 포함됩니다. 그러나 이러한 벤치마크에서 모델은 「nail」과 「sack」의 기존 문맥적 매칭을 활용하여 문제를 단순화할 수 있습니다. 이를 해결하기 위해 NIAH를 확장한 벤치마크 「NoLiMa」를 도입하고 있습니다. 이 벤치마크는 문제와 「nail」이 최소한의 단어 중복을 가지도록 설계되어 있으며, 모델이 「sack」 내의 「nail」을 위치시키기 위해 잠재적인 순환을 추론하는 것이 필요합니다. 12개의 인기 있는 LLMs를 평가하고 있습니다. 이 모델들은 짧은 문맥(＜1K)에서 잘 동작하지만, 문맥의 길이가 증가하면 성능이 현저히 떨어집니다. 예를 들어, 32K에서 10개의 모델이 강력한 짧은 문맥 기반 라인의 50%를 초과하지 않습니다. 더욱이, GPT-4o는 가장 뛰어난 예외 중 하나이며, 거의 완전한 베이스라인(99.3%)에서 69.7%로 감소합니다. 우리의 분석은 문맥의 길이가 긴 경우 문맥을 활용하는 행동 구조의 어려움이 증가하고, 관련 정보를 검색하는 것이 어려워지는 것을 원인으로 생각합니다.",
      "upvotes": 3,
      "discussionId": "67aa583d3a878652daeae06c"
    },
    "publishedAt": "2025-02-13T00:04:29.194Z",
    "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07737",
      "authors": [
        {
          "_id": "67ad5d2f8436e8ea7abb7a15",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a16",
          "name": "Shuming Ma",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a17",
          "name": "Xu Sun",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a18",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T17:57:53.000Z",
      "title": "다음 블록 예측: 반자동 회귀 모델링에 의한 비디오 생성",
      "summary": "다음 토큰 예측 (NTP)은 자동 복원 (AR) 비디오 생성에서 실제로 사용된 방법입니다만, 한방향적인 의존관계가 최적일 수 있고, 추론 속도가 느린 문제로 이어집니다. 본 논문에서는 비디오 생성을 위해 반자동 복원 (semi-AR) 프레임워크를 제안하고, Next-Block Prediction (NBP)으로 불립니다. 비디오 콘텐츠는 동일한 크기의 블록으로 분할되고, 현재 블록 내의 각 토큰이 다음 블록의 대응 토큰을 동시에 예측할 수 있도록 생성 단위를 변경합니다. 전통적인 AR 모델링과 달리, 각 블록 내에서 바이드릭션 어텐션을 사용하며, 토큰이 강한 공간 의존관계를捉えることができます. 다수의 토큰을 병렬적으로 예측함으로써, NBP 모델은 생성 단계를 크게 줄이고, 고속적이고 효율적인 추론을 실현합니다. 모델은 UCF101에서 FVD 스코어 103.3, K600에서 25.5를 달성하며, 평균 4.4점 이상의 NTP 모델을 초월합니다. 또한, 추론 단계의 줄이는 데 따라, NBP 모델은 1초당 128x128 해상도의 8.89 프레임을 생성하고, 11배의 속도업을 실현합니다. 또한, 모델 크기가 700M에서 3B 파라미터의 범위로 조사되어, 생성 품질의 큰 향상을 확인하며, UCF101에서 FVD 스코어가 103.3에서 55.3, K600에서 25.5에서 19.5로 떨어졌으며, 접근의 scalability를 보여줍니다.",
      "upvotes": 3,
      "discussionId": "67ad5d308436e8ea7abb7a3d"
    },
    "publishedAt": "2025-02-12T21:48:00.325Z",
    "title": "Next Block Prediction: Video Generation via Semi-Autoregressive Modeling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07737.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d2e681b8448e1785bbda06",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
      "fullname": "Shuhuai Ren",
      "name": "ShuhuaiRen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07599",
      "authors": [
        {
          "_id": "67ad5bd2ac32a8e230fc8996",
          "name": "Xiliang Yang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8997",
          "name": "Feng Jiang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8998",
          "name": "Qianen Zhang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8999",
          "name": "Lei Zhao",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc899a",
          "name": "Xiao Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T14:49:44.000Z",
      "title": "DPO-Shift: 직접적 선호 최적화의 분포 변환",
      "summary": "직접 선호 최적화 (Direct Preference Optimization, DPO) 및 그의 변형은 언어 모델과 인간의 취향을 맞추기 위해 점차 인기를 얻고 있습니다. 이 방법들은 모델을 선택된 (또는 선호된) 것과 거부된 (또는 선호되지 않은) 것을 더 잘 구분하는 것을 목표로 합니다. 그러나 선행 연구에서 선택된 확률이 훈련 중 감소하는 현상이 발견되어 이 현상은 \"빈도교체\"라고 불리며, 이 문제를 해결하기 위해 본 연구에서는 선택된 확률의 분포를 제어적으로 변형하는 방법을 사용합니다. 이 방법은 이론적 분석과 실험적 검증을 통해 선택확률 개선과 보상의 경계를 잃지 않는 기본적인 변환으로 그 중요성을 밝혀졌습니다. 또한 이 방법은 MT-Bench와 같은 설계된 확률 시험과 같은 하류 작업에서 DPO보다 뛰어납니다. 우리는 이 연구를 통해 DPO의 빈도교체 문제를 이론적으로 기반한 간단한 해결책으로 효과적으로 완화할 수 있다는 것을 믿습니다. 코드는 https://github.com/Meaquadddd/DPO-Shift에 접근할 수 있습니다.",
      "upvotes": 3,
      "discussionId": "67ad5bd3ac32a8e230fc89a7"
    },
    "publishedAt": "2025-02-12T21:43:42.404Z",
    "title": "DPO-Shift: Shifting the Distribution of Direct Preference Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07599.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66270fcef7cf69d4223a8a3f",
      "avatarUrl": "/avatars/115db0326737e65318c92a7b8dc5ed6a.svg",
      "fullname": "Xiao Li",
      "name": "xli0982",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04411",
      "authors": [
        {
          "_id": "67adad972883187d78409a7a",
          "name": "Kunfeng Lai",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7b",
          "name": "Zhenheng Tang",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7c",
          "name": "Xinglin Pan",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7d",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7e",
          "user": {
            "_id": "63024676056ec3a2a8714b24",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
            "isPro": false,
            "fullname": "Xiang Liu",
            "user": "Dominic789654",
            "type": "user"
          },
          "name": "Xiang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:45:17.030Z",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7f",
          "name": "Haolan Chen",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a80",
          "name": "Li Shen",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a81",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a82",
          "name": "Xiaowen Chu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T11:26:30.000Z",
      "title": "미디어 타르： 파라메타 충돌이 적고, 신뢰도 기반의 메모리 효율적인 LLM의 통합",
      "summary": "모델의 통합은 다양한 태스크에서 미세 조정된 큰 언어 모델(LLMs)을 강화한 것으로 이루어진다. 그러나 모델 간의 파라미터 충돌은 평균 성능 저하와 관련이 있다. 모델 루팅은 추론 시 개별 모델을 선택하여 이 문제를 해결하지만, 과도한 스トレー지 비용과 계산 비용이 발생하며, 다른 모델로부터의 공유 지식을 활용하지 못한다. 본 연구에서는 파라미터 충돌의 정도가 다른 레이어에 따라 달라지는 것을 관찰하고, 이를 기반으로 파라미터 충돌이 최소한인 레이어를 평균화하고, 충돌이 큰 레이어에 대해 새로운 태스크 수준의 익스퍼트 루팅을 사용하는 것이다. 스トレー지 비용을 더욱 줄이기 위해, 태스크의 수학적인 희소성을 참고하여 여러 微调된 익스퍼트를 완벽한 익스퍼트과 다수의 희소한 익스퍼트로 분리한다. 분포 외의 샘플을 고려하고, 입력 데이터의 태스크 불확실성에 기반하여 적절한 익스퍼트를 선택하여 통합한다. LLaMA와 Qwen의 실제 추론 태스크에서 다양한 파라미터 크기를 테스트하고, 현실적인 추론 태스크에서 평가한다. 결과는 현재의 방법과 비교하여 시스템 비용이 적어도 같은 성능 향상을 얻을 수 있다는 것을 보여준다.",
      "upvotes": 2,
      "discussionId": "67adad992883187d78409aa8"
    },
    "publishedAt": "2025-02-13T03:30:35.137Z",
    "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07985",
      "authors": [
        {
          "_id": "67ad9577b469222e0df18134",
          "user": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "name": "Víctor Gallego",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T06:47:20.731Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T22:06:25.000Z",
      "title": "MetaSC: 테스트 시의 안전성 스펙트럼 최적화를 위한 언어 모델",
      "summary": "우리는 추론 시 모델의 가중치를 변경하지 않는 한 언어 모델(LM)의 안전성 논리를 최적화하기 위해 새로운 동적 안전 프레임워크를 제안합니다. 최근의 자기 평가법의 발전에 기반하여, 우리 접근법은 자기 평가와 수정의 과정이 적응적으로 진행될 수 있도록, 안전성 Prompt(약간 '규칙'으로 불리며)를 반복적으로 업데이트하는 메타 평가 구조를 활용하고 있습니다. 이 추론 시의 최적화는 적대적인 '지글 브레이크' 요청에 대한 성능을 개선하고, 다양한 일반적인 안전 관련 태스크에서도, 도덕적인 피해를 피하고, 진실한 답을 요구함으로써 안전성을 향상시킵니다. 다양한 언어 모델에 대한 실험적 평가에 따라, 동적으로 최적화된 안전성 Prompt는 고정된 시스템 Prompt와 정적 자기 평가 방어에 비해 상당한 수준의 안전 스코어를 나타냅니다. 코드는 https://github.com/vicgalle/meta-self-critique.git에서 공개됩니다.",
      "upvotes": 1,
      "discussionId": "67ad9578b469222e0df18162"
    },
    "publishedAt": "2025-02-13T01:47:30.377Z",
    "title": "MetaSC: Test-Time Safety Specification Optimization for Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07985.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fad8602b8423e1d80b8a965",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
      "fullname": "Victor Gallego",
      "name": "vicgalle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 117
    },
    "isAuthorParticipating": true
  }
]