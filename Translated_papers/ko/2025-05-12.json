[
  {
    "paper": {
      "id": "2505.02550",
      "authors": [
        {
          "_id": "6819ef0b2ff435c58da4d860",
          "user": {
            "_id": "63ecbccac8827dd0f0f59579",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ecbccac8827dd0f0f59579/kz-2F9Z0QKllifgZmr8tH.jpeg",
            "isPro": false,
            "fullname": "Chris Ociepa",
            "user": "chrisociepa",
            "type": "user"
          },
          "name": "Krzysztof Ociepa",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:56.213Z",
          "hidden": false
        },
        {
          "_id": "6819ef0b2ff435c58da4d861",
          "name": "Łukasz Flis",
          "hidden": false
        },
        {
          "_id": "6819ef0b2ff435c58da4d862",
          "user": {
            "_id": "61786d0b038518aa2827c6b7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61786d0b038518aa2827c6b7/d1UnfivoVreYebS5JM3P9.jpeg",
            "isPro": false,
            "fullname": "Remek Kinas",
            "user": "Remek",
            "type": "user"
          },
          "name": "Remigiusz Kinas",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T06:51:15.217Z",
          "hidden": false
        },
        {
          "_id": "6819ef0b2ff435c58da4d863",
          "user": {
            "_id": "5e47d3eb178ca95365287400",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
            "isPro": true,
            "fullname": "Krzysztof Wróbel",
            "user": "djstrong",
            "type": "user"
          },
          "name": "Krzysztof Wróbel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:54.135Z",
          "hidden": false
        },
        {
          "_id": "6819ef0b2ff435c58da4d864",
          "name": "Adrian Gwoździej",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T10:39:51.000Z",
      "submittedOnDailyAt": "2025-05-12T07:26:20.895Z",
      "title": "Bielik v3 Small: 기술보고서",
      "submittedOnDailyBy": {
        "_id": "5e47d3eb178ca95365287400",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
        "isPro": true,
        "fullname": "Krzysztof Wróbel",
        "user": "djstrong",
        "type": "user"
      },
      "summary": "비엘릭 v3를 소개합니다. 이 모델은 파라미터 효율적인 생성 텍스트 모델 시리즈로 (150M와 450M) 있으며, 폴란드어 처리를 최적화하여 사용됩니다. 이러한 모델들은 큰 컴퓨터 시스템이 필요하지 않은 작은 필드에서도 동일한 성능을 보여주는 모델과 같습니다. 우리의 접근 방식에는 몇 가지 핵심적인 혁신이 포함되어 있습니다: 폴란드어의 사용자定制 토큰화기 (APT4)로 토큰의 효율이 크게 향상되고, 학습을 균형을 위해 가중치 인스톰션 교차 엔트로피 손실, 학습 진행에 따라 동적으로 조정되는 적응적 학습률. 292억 토큰으로 기록된 精心 기록된 코퍼스를 기반으로, 3030만 개의 문서로 구성된 모델은 오픈 PL LLM 리더보드, 복잡한 폴란드어 텍스트 이해 벤치마크, 폴란드 EQ-Bench, 폴란드 의료 벤치마크 등 여러 벤치마크에서 뛰어난 성능을 보입니다. 450M 파라미터 모델은 2-3 배 크기의 모델과 경쟁적인 결과를 얻으며, 150M 파라미터 모델은 매우 긴장된 프로파일에서도 강력한 성능을 제공합니다. 이러한 발전은 파라미터 효율적인 언어 모델링의 새로운 벤치마크를 설정하고, 자원 제한된 애플리케이션에서 폴란드어의 AI를 더 쉽게 활용할 수 있도록 합니다.",
      "upvotes": 16,
      "discussionId": "6819ef0c2ff435c58da4d892",
      "projectPage": "https://bielik.ai/",
      "githubRepo": "https://github.com/speakleash",
      "ai_keywords": [
        "parameter-efficient",
        "generative text models",
        "token efficiency",
        "custom Polish tokenizer",
        "Weighted Instruction Cross-Entropy Loss",
        "Adaptive Learning Rate"
      ]
    },
    "publishedAt": "2025-05-05T06:39:51.000Z",
    "title": "Bielik v3 Small: Technical Report",
    "summary": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02550.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e47d3eb178ca95365287400",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
      "fullname": "Krzysztof Wróbel",
      "name": "djstrong",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02410",
      "authors": [
        {
          "_id": "6819f19e5c7ea9f74284d3a3",
          "user": {
            "_id": "63ecbccac8827dd0f0f59579",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ecbccac8827dd0f0f59579/kz-2F9Z0QKllifgZmr8tH.jpeg",
            "isPro": false,
            "fullname": "Chris Ociepa",
            "user": "chrisociepa",
            "type": "user"
          },
          "name": "Krzysztof Ociepa",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:52.265Z",
          "hidden": false
        },
        {
          "_id": "6819f19e5c7ea9f74284d3a4",
          "name": "Łukasz Flis",
          "hidden": false
        },
        {
          "_id": "6819f19e5c7ea9f74284d3a5",
          "user": {
            "_id": "5e47d3eb178ca95365287400",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
            "isPro": true,
            "fullname": "Krzysztof Wróbel",
            "user": "djstrong",
            "type": "user"
          },
          "name": "Krzysztof Wróbel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:50.340Z",
          "hidden": false
        },
        {
          "_id": "6819f19e5c7ea9f74284d3a6",
          "name": "Adrian Gwoździej",
          "hidden": false
        },
        {
          "_id": "6819f19e5c7ea9f74284d3a7",
          "user": {
            "_id": "61786d0b038518aa2827c6b7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61786d0b038518aa2827c6b7/d1UnfivoVreYebS5JM3P9.jpeg",
            "isPro": false,
            "fullname": "Remek Kinas",
            "user": "Remek",
            "type": "user"
          },
          "name": "Remigiusz Kinas",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T06:51:13.426Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T07:03:41.000Z",
      "submittedOnDailyAt": "2025-05-12T07:25:02.402Z",
      "title": "비엘릭 11B v2 기술보고서",
      "submittedOnDailyBy": {
        "_id": "5e47d3eb178ca95365287400",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
        "isPro": true,
        "fullname": "Krzysztof Wróbel",
        "user": "djstrong",
        "type": "user"
      },
      "summary": "비엘릭 11B v2, 가장 선진한 언어 모델 중 하나로, 포르란어 처리에 최적화된 모델을 소개합니다. 미스트랄 7B v0.2 아키텍처에 기반하여, 깊이 업스케일링을 통해 파라미터 수를 11B로 확장하여, 포르란어 벤치마크에서 뛰어난 성능을 보여주며, 크로스 라ンゲー지 능력도 강합니다. 두 가지 주요 기술 혁신을 소개합니다: 가중치 인스톰 교차 엔트로피 손실과 적응적 학습률. 가중치 인스톰 교차 엔트로피 손실은 학습 예를 품질 기반의 가중치로 할당하여 다양한 인스톰 타입의 학습을 최적화하고, 적응적 학습률은 문장의 길이에 따라 동적으로 조정됩니다. 여러 벤치마크에서 검증 결과, 비엘릭 11B v2는 파라미터 수가 2~6배 더 큰 모델을 초과하여, 언어 이해부터 복잡한 추론까지 다양한 포르란어 모델 중 가장 뛰어난 모델입니다. 모델의 파라미터 효율성과 분산 기능은 다양한 하드웨어 구성에서 도입이 가능하게 하며, 포르란어 AI의 기능 개선과 적은 표현 언어의 자원 효율적인 언어 모델링의 새로운 벤치마크를 설정합니다.",
      "upvotes": 16,
      "discussionId": "6819f19e5c7ea9f74284d3cc",
      "projectPage": "https://bielik.ai/",
      "githubRepo": "https://github.com/speakleash",
      "ai_keywords": [
        "Weighted Instruction Cross-Entropy Loss",
        "Adaptive Learning Rate",
        "depth up-scaling",
        "parameter efficiency",
        "quantization"
      ]
    },
    "publishedAt": "2025-05-05T03:03:41.000Z",
    "title": "Bielik 11B v2 Technical Report",
    "summary": "We present Bielik 11B v2, a state-of-the-art language model optimized for\nPolish text processing. Built on the Mistral 7B v0.2 architecture and scaled to\n11B parameters using depth up-scaling, this model demonstrates exceptional\nperformance across Polish language benchmarks while maintaining strong\ncross-lingual capabilities. We introduce two key technical innovations:\nWeighted Instruction Cross-Entropy Loss, which optimizes learning across\ndiverse instruction types by assigning quality-based weights to training\nexamples, and Adaptive Learning Rate, which dynamically adjusts based on\ncontext length. Comprehensive evaluation across multiple benchmarks\ndemonstrates that Bielik 11B v2 outperforms many larger models, including those\nwith 2-6 times more parameters, and significantly surpasses other specialized\nPolish language models on tasks ranging from linguistic understanding to\ncomplex reasoning. The model's parameter efficiency and extensive quantization\noptions enable deployment across various hardware configurations, advancing\nPolish language AI capabilities and establishing new benchmarks for\nresource-efficient language modeling in less-represented languages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e47d3eb178ca95365287400",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
      "fullname": "Krzysztof Wróbel",
      "name": "djstrong",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.06046",
      "authors": [
        {
          "_id": "6821af48696b63e207ae8474",
          "user": {
            "_id": "64cb98c6f103036e23c69b1d",
            "avatarUrl": "/avatars/7ee33880ad39f5335b618dc53554124a.svg",
            "isPro": false,
            "fullname": "Harris",
            "user": "Joshua-Harris",
            "type": "user"
          },
          "name": "Joshua Harris",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:48.631Z",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8475",
          "name": "Fan Grayson",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8476",
          "name": "Felix Feldman",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8477",
          "name": "Timothy Laurence",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8478",
          "name": "Toby Nonnenmacher",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8479",
          "name": "Oliver Higgins",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847a",
          "name": "Leo Loman",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847b",
          "name": "Selina Patel",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847c",
          "name": "Thomas Finnie",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847d",
          "name": "Samuel Collins",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847e",
          "name": "Michael Borowitz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-09T13:42:59.000Z",
      "submittedOnDailyAt": "2025-05-12T07:35:55.202Z",
      "title": "건강한 LLMs? UK 정부의 공공보건정보에 대한 LLM 지식의 벤치마크",
      "submittedOnDailyBy": {
        "_id": "64cb98c6f103036e23c69b1d",
        "avatarUrl": "/avatars/7ee33880ad39f5335b618dc53554124a.svg",
        "isPro": false,
        "fullname": "Harris",
        "user": "Joshua-Harris",
        "type": "user"
      },
      "summary": "LLM가 광범위하게 접근 가능한 것이 될 때마다, 특정 분야의 지식에 대한 상세한 이해가 현실 세계의 사용에 성공하기 위해 필요하게 됩니다. 특히, 공공보건 분야에서, 관련 정보, 정확한, 현재의 정보를 얻지 못하는 경우, 영국 거주자에 큰 영향을 미칠 가능성이 높기 때문에, 이는 특히 중요합니다. 그러나, 현재, LLM가 영국 정부의 공공보건 정보에 대해 잘 알고 있는 것은 적다는 것을 알고 있습니다. 이 문제를 해결하기 위해, 본 논문에서는 LLM의 Multiple Choice Question Answering (MCQA)와 공공보건의 질문에 대한 자유 형식의 답변을 평가하기 위한 새로운 벤치마크인 PubHealthBench를 소개합니다. 이 벤치마크는 자동화 프로세스로 생성된 8000개 이상의 질문을 포함합니다. 또한, PubHealthBench의 소스 텍스트로 사용된 영국 정부의 공공보건 가이드라인 문서의 새로운 데이터 세트를 릴리즈합니다. PubHealthBench에서 24개의 LLM을 평가한 결과, 최신의 비공개 LLM (GPT-4.5, GPT-4.1, o1)은 높은 지식을 가지고 있음을 알 수 있었고, MCQA에서 90% 이상 달성했으며, 검색 엔진의 가벼운 사용보다 우수했습니다. 그러나, 자유 형식의 응답에서 75%를 초과하는 모델은 없습니다. 따라서, 최신 모델 (SOTA)이 공공보건 정보의 정확한 신호를 보여주는 것은 있지만, 자유 형식의 응답을 제공하기 위해서는 추가적인 안전 조치와 도구가 필요할 수 있습니다.",
      "upvotes": 6,
      "discussionId": "6821af49696b63e207ae84c6",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Multiple Choice Question Answering (MCQA)",
        "PubHealthBench",
        "UK Government public health information",
        "automated pipeline",
        "extracted UK Government public health guidance documents",
        "SOTA (state of the art) LLMs",
        "GPT-4.5",
        "GPT-4.1",
        "o1"
      ]
    },
    "publishedAt": "2025-05-09T09:42:59.000Z",
    "title": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information",
    "summary": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries, created via an automated pipeline. We also\nrelease a new dataset of the extracted UK Government public health guidance\ndocuments used as source text for PubHealthBench. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% in the MCQA setup, and outperform\nhumans with cursory search engine use. However, in the free form setup we see\nlower performance with no model scoring >75%. Therefore, whilst there are\npromising signs that state of the art (SOTA) LLMs are an increasingly accurate\nsource of public health information, additional safeguards or tools may still\nbe needed when providing free form responses on public health topics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06046.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64cb98c6f103036e23c69b1d",
      "avatarUrl": "/avatars/7ee33880ad39f5335b618dc53554124a.svg",
      "fullname": "Harris",
      "name": "Joshua-Harris",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.06111",
      "authors": [
        {
          "_id": "68218b847202d193249511b6",
          "name": "Qingwen Bu",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511b7",
          "name": "Yanting Yang",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511b8",
          "name": "Jisong Cai",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511b9",
          "name": "Shenyuan Gao",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511ba",
          "user": {
            "_id": "646ec9b135f55eb49e405faa",
            "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
            "isPro": false,
            "fullname": "Guanghui Ren",
            "user": "sundrops",
            "type": "user"
          },
          "name": "Guanghui Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T06:50:15.305Z",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511bb",
          "name": "Maoqing Yao",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511bc",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511bd",
          "name": "Hongyang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-09T15:11:13.000Z",
      "submittedOnDailyAt": "2025-05-12T04:30:20.087Z",
      "title": "UniVLA: 태스크센터화 잠재적 액션을 사용하여 어디든 행동 학습을 수행합니다.",
      "submittedOnDailyBy": {
        "_id": "64ac1f169dcc5787461468a4",
        "avatarUrl": "/avatars/c031a75989147009b7850df4eddfcb27.svg",
        "isPro": false,
        "fullname": "Qingwen Bu",
        "user": "qwbu",
        "type": "user"
      },
      "summary": "일반적인 로봇은 다양한 환경에서 효과적으로 동작해야 합니다. 그러나 현재 많은 접근 방식은 동작을 표명한 데이터의 스케일링을 중시하여, 그 능력을 향상시키기 위해 더 많은 데이터를 사용합니다. 이로 인해 이러한 로봇은 단일 물리적 특성을 가지고 있으며, 다른 몸체와 환경에서 학습 가능한 지식을 얻는 것이 어렵습니다. 이러한 제한에 대처하기 위해, 우리는 새로운 프레임워크 \"UniVLA\"를 제안합니다. 이 프레임워크는 다양한 몸체에서의 시각, 언어, 동작(VLA) 정책의 학습을 목표로 합니다. 우리의 주요 혁신은 잠재적인 동작 모델을 사용하여, 비디오로부터 태스크 중심적인 동작 표현을 얻는 것입니다. 이로 인해 광범위한 몸체와 관점의 범위에서 시험 데이터를 활용할 수 있습니다. 태스크 관련하지 않은 동작의 영향을 줄이기 위해, 언어 지시를 사용하며, DINO 특징 공간 내에서 잠재적인 동작 모델을 구축합니다. 인터넷 크기의 비디오에서 학습된 일반적인 정책은 효율적인 잠재적인 동작 해석에 의해, 다양한 로봇에 기능할 수 있습니다. 다양한 동작과 노미션 벤치마크에서 가장 先端 的 결과를 얻으며, 실제 로봇 디포이팅도 실현되었습니다. UniVLA는 OpenVLA보다 높은 성능을 달성하며, 학습 컴퓨터의 계산량은 1/20, 디포이팅 데이터는 1/10입니다. 다양한 데이터, 특히 인간의 비디오를 포함한 데이터를 추가하여 테스트 프로세스에 포함하여, 지속적인 성능 향상이 볼 수 있습니다. 이러한 결과는 UniVLA가 scalable하고 효율적인 로봇 정책 학습을 촉진하는 것을 보여줍니다.",
      "upvotes": 5,
      "discussionId": "68218b857202d19324951214",
      "githubRepo": "https://github.com/OpenDriveLab/UniVLA",
      "ai_keywords": [
        "UniVLA",
        "vision-language-action (VLA) policies",
        "latent action model",
        "DINO feature space",
        "latent action decoding",
        "manipulation benchmarks",
        "navigation benchmarks",
        "real-robot deployments",
        "OpenVLA"
      ]
    },
    "publishedAt": "2025-05-09T11:11:13.000Z",
    "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
    "summary": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06111.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ac1f169dcc5787461468a4",
      "avatarUrl": "/avatars/c031a75989147009b7850df4eddfcb27.svg",
      "fullname": "Qingwen Bu",
      "name": "qwbu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.05026",
      "authors": [
        {
          "_id": "6821771ddf190eabf5f666d8",
          "user": {
            "_id": "655c44752205aab35222aca3",
            "avatarUrl": "/avatars/57900539952382de0ce6892faf50b401.svg",
            "isPro": false,
            "fullname": "Jaehyun Jeon",
            "user": "jeochris",
            "type": "user"
          },
          "name": "Jaehyun Jeon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T06:50:17.832Z",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666d9",
          "name": "Jang Han Yoon",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666da",
          "name": "Min Soo Kim",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666db",
          "name": "Sumin Shim",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666dc",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666dd",
          "name": "Hanbin Kim",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666de",
          "name": "Youngjae Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T08:00:32.000Z",
      "submittedOnDailyAt": "2025-05-12T05:33:20.932Z",
      "title": "G-FOCUS: 사용자 인터페이스 디자인의 설득력을 평가하기 위한 강력한 방법론에 대한 방향\n\n(请注意：虽然您要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，我在翻译中保持了原文的格式和结构。如果您需要进一步的解释或调整，请告知。)",
      "submittedOnDailyBy": {
        "_id": "655c44752205aab35222aca3",
        "avatarUrl": "/avatars/57900539952382de0ce6892faf50b401.svg",
        "isPro": false,
        "fullname": "Jaehyun Jeon",
        "user": "jeochris",
        "type": "user"
      },
      "summary": "UI 디자인의 효과성 평가는 미술성보다 사용자의 행동에 영향을 미치는 데 초점을 맞추고 있습니다. 이는 디자인의 说服력 (Design Persuasiveness)의 핵심 원칙입니다. A/B 테스트는 사용자 참여도를 높이는 UI를 선택하는 주요 방법 중 하나입니다. 그러나 이 방법은 비용과 시간이 필요합니다. 최근의 Vision-Language Models (VLMs)는 자동화된 UI 분석을 처리할 수 있습니다. 그러나 현재의 접근 방식은 독립적인 디자인 속성에 초점을 맞추고, 상대적으로 说服력 (Persuasiveness)을 중심으로 하는 방식입니다. 이를 해결하기 위해 WiserUI-Bench를 소개합니다. 이것은 Pairwise UI Design Persuasiveness Assessment 태스크에 맞게 설계된 벤치마크입니다. 300 쌍의 실제 세계의 UI 이미지는 A/B 테스트 결과와 전문가의 이유로 레이블付け되어 있습니다. 또한 G-FOCUS를 제안합니다. 이는 VLM 기반의 说服력 평가 (Persuasiveness Evaluation)을 강화하고, 위치 편향을 줄이고 평가의 정확도를 향상시키는 새로운 추론 시점의 이유론의 전략입니다. 실험 결과를 통해 G-FOCUS는 현재의 추론 전략을 초과하는 Pairwise UI 평가의 일치성과 정확성을 설명합니다. VLM에 의한 UI 说服력 (Persuasiveness) 평가의 촉진으로, 우리의 연구는 A/B 테스트를 보완하는 접근 방식을 제공하고, 교환 가능한 UI 취향 모델링과 디자인 최적화의 발전을 촉진합니다. 코드와 데이터는 공개적으로 릴리즈됩니다.",
      "upvotes": 5,
      "discussionId": "68217722df190eabf5f66814",
      "ai_keywords": [
        "Vision-Language Models",
        "WiserUI-Bench",
        "Pairwise UI Design Persuasiveness Assessment",
        "G-FOCUS",
        "inference-time reasoning strategy",
        "position bias",
        "VLM-driven evaluation"
      ]
    },
    "publishedAt": "2025-05-08T04:00:32.000Z",
    "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness",
    "summary": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05026.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655c44752205aab35222aca3",
      "avatarUrl": "/avatars/57900539952382de0ce6892faf50b401.svg",
      "fullname": "Jaehyun Jeon",
      "name": "jeochris",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02686",
      "authors": [
        {
          "_id": "6821acfb2808328b91c0e365",
          "name": "Xiaobao Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T14:33:49.000Z",
      "submittedOnDailyAt": "2025-05-12T06:41:36.276Z",
      "title": "별에 탑승하는 AI: 대 언어 모델의 훈련 후 및 테스트 시 스케일링에 대한 보상 학습 조사",
      "submittedOnDailyBy": {
        "_id": "64cb02869e30a46f7b80b355",
        "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
        "isPro": false,
        "fullname": "Xiaobao Wu",
        "user": "bobxwu",
        "type": "user"
      },
      "summary": "최근의 대언어 모델(LLMs)의 발전은 사전 학습 스케일링에서 후반 학습 및 테스트 시 스케일링으로 이동했습니다. 이러한 발전에 있어서, 하나의 핵심적인 통일된 패러다임이 나타났습니다: 보상으로부터 학습함으로써 보상 신호가 LLM의 행동을 지도하는 별이 됩니다. 이는 강화 학습(RLHF, DPO, GRPO), 보상에 의한 디코딩, 후반 시의 보정 등 다양한 기술에 기반합니다. 중요한 점은, 이 패러다임은 정적 데이터로부터的被动的 학습에서 동적인 피드백으로부터의 능동적인 학습으로의 전환이 가능합니다. 이로써, LLMs는 일관된 취향과 깊은 논리적인 기능들을 갖습니다. 이 조사에서는 보상으로부터 학습하는 패러다임에 대한 상세한 개요를 제공합니다. 이 패러다임 아래의 학습, 추론, 추론 후 단계에 대한 전략을 분류하고 분석합니다. 또한 보상 모델의 벤치마크와 주요 응용 분야에 대해서도 논의합니다. 마지막으로, 문제점과 미래의 방향성을 특징적으로 설명합니다. 논문 컬렉션을 보유합니다: https://github.com/bobxwu/learning-from-rewards-llm-papers.",
      "upvotes": 3,
      "discussionId": "6821acfd2808328b91c0e3e3",
      "githubRepo": "https://github.com/bobxwu/learning-from-rewards-llm-papers",
      "ai_keywords": [
        "reinforcement learning",
        "RLHF",
        "DPO",
        "GRPO",
        "reward-guided decoding",
        "post-hoc correction",
        "active learning",
        "reward models"
      ]
    },
    "publishedAt": "2025-05-05T10:33:49.000Z",
    "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in\n  Post-Training and Test-Time Scaling of Large Language Models",
    "summary": "Recent developments in Large Language Models (LLMs) have shifted from\npre-training scaling to post-training and test-time scaling. Across these\ndevelopments, a key unified paradigm has arisen: Learning from Rewards, where\nreward signals act as the guiding stars to steer LLM behavior. It has\nunderpinned a wide range of prevalent techniques, such as reinforcement\nlearning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc\ncorrection. Crucially, this paradigm enables the transition from passive\nlearning from static data to active learning from dynamic feedback. This endows\nLLMs with aligned preferences and deep reasoning capabilities. In this survey,\nwe present a comprehensive overview of the paradigm of learning from rewards.\nWe categorize and analyze the strategies under this paradigm across training,\ninference, and post-inference stages. We further discuss the benchmarks for\nreward models and the primary applications. Finally we highlight the challenges\nand future directions. We maintain a paper collection at\nhttps://github.com/bobxwu/learning-from-rewards-llm-papers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02686.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb02869e30a46f7b80b355",
      "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
      "fullname": "Xiaobao Wu",
      "name": "bobxwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]