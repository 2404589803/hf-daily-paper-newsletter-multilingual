[
  {
    "paper": {
      "id": "2505.02550",
      "authors": [
        {
          "_id": "6819ef0b2ff435c58da4d860",
          "user": {
            "_id": "63ecbccac8827dd0f0f59579",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ecbccac8827dd0f0f59579/kz-2F9Z0QKllifgZmr8tH.jpeg",
            "isPro": false,
            "fullname": "Chris Ociepa",
            "user": "chrisociepa",
            "type": "user"
          },
          "name": "Krzysztof Ociepa",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:56.213Z",
          "hidden": false
        },
        {
          "_id": "6819ef0b2ff435c58da4d861",
          "name": "Łukasz Flis",
          "hidden": false
        },
        {
          "_id": "6819ef0b2ff435c58da4d862",
          "user": {
            "_id": "61786d0b038518aa2827c6b7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61786d0b038518aa2827c6b7/d1UnfivoVreYebS5JM3P9.jpeg",
            "isPro": false,
            "fullname": "Remek Kinas",
            "user": "Remek",
            "type": "user"
          },
          "name": "Remigiusz Kinas",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T06:51:15.217Z",
          "hidden": false
        },
        {
          "_id": "6819ef0b2ff435c58da4d863",
          "user": {
            "_id": "5e47d3eb178ca95365287400",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
            "isPro": true,
            "fullname": "Krzysztof Wróbel",
            "user": "djstrong",
            "type": "user"
          },
          "name": "Krzysztof Wróbel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:54.135Z",
          "hidden": false
        },
        {
          "_id": "6819ef0b2ff435c58da4d864",
          "name": "Adrian Gwoździej",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T10:39:51.000Z",
      "submittedOnDailyAt": "2025-05-12T07:26:20.895Z",
      "title": "Bielik v3 Small: 기술보고서",
      "submittedOnDailyBy": {
        "_id": "5e47d3eb178ca95365287400",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
        "isPro": true,
        "fullname": "Krzysztof Wróbel",
        "user": "djstrong",
        "type": "user"
      },
      "summary": "ビエリックv3를 소개합니다. 이 모델은 파라미터 효율적인 생성 텍스트 모델 시리즈로, 1.5B와 4.5B의 크기로 팍란드어 처리에 최적화되어 있습니다. 이 모델들은 상대적으로 작은 크기에서 큰 모델과 같은 성능을 달성할 수 있으며, 계산 자원 사용량을 크게 줄일 수 있습니다. 우리의 접근 방식에는 여러 가지 핵심적인 혁신이 포함되어 있습니다. 이는 토큰의 효율을 크게 향상시키기 위한 팍란드어의 맞춤형 토큰나이저(APT4), 명령 타입 학습을 균형을 맞추기 위한 가중치 명령 교차 엔트로피 손실, 그리고 학습 진행에 따라 동적으로 조정되는 적응적 학습률입니다. 2920억 토큰의 미세하게 필터링된 코퍼스에서 훈련되었으며, 3030만 개의 기사로 구성되어 있습니다. 이 모델들은 Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, 그리고 Polish Medical Leaderboard의 여러 벤치마크에서 우수한 성적을 기록했습니다. 4.5B 파라미터 모델은 2-3배 크기의 모델과 경쟁적인 결과를 달성하며, 1.5B 모델은 매우 작은 프로필에서도 강력한 성능을 제공합니다. 이러한 발전은 파라미터 효율적인 언어 모델링의 새로운 벤치마크를 설정하고, 자원 제한된 애플리케이션에서 고품질의 팍란드어 AI가 더 쉽게 접근할 수 있도록 합니다.",
      "upvotes": 16,
      "discussionId": "6819ef0c2ff435c58da4d892",
      "projectPage": "https://bielik.ai/",
      "githubRepo": "https://github.com/speakleash",
      "ai_keywords": [
        "parameter-efficient",
        "generative text models",
        "token efficiency",
        "custom Polish tokenizer",
        "Weighted Instruction Cross-Entropy Loss",
        "Adaptive Learning Rate"
      ]
    },
    "publishedAt": "2025-05-05T06:39:51.000Z",
    "title": "Bielik v3 Small: Technical Report",
    "summary": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02550.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e47d3eb178ca95365287400",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
      "fullname": "Krzysztof Wróbel",
      "name": "djstrong",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02410",
      "authors": [
        {
          "_id": "6819f19e5c7ea9f74284d3a3",
          "user": {
            "_id": "63ecbccac8827dd0f0f59579",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ecbccac8827dd0f0f59579/kz-2F9Z0QKllifgZmr8tH.jpeg",
            "isPro": false,
            "fullname": "Chris Ociepa",
            "user": "chrisociepa",
            "type": "user"
          },
          "name": "Krzysztof Ociepa",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:52.265Z",
          "hidden": false
        },
        {
          "_id": "6819f19e5c7ea9f74284d3a4",
          "name": "Łukasz Flis",
          "hidden": false
        },
        {
          "_id": "6819f19e5c7ea9f74284d3a5",
          "user": {
            "_id": "5e47d3eb178ca95365287400",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
            "isPro": true,
            "fullname": "Krzysztof Wróbel",
            "user": "djstrong",
            "type": "user"
          },
          "name": "Krzysztof Wróbel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:50.340Z",
          "hidden": false
        },
        {
          "_id": "6819f19e5c7ea9f74284d3a6",
          "name": "Adrian Gwoździej",
          "hidden": false
        },
        {
          "_id": "6819f19e5c7ea9f74284d3a7",
          "user": {
            "_id": "61786d0b038518aa2827c6b7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61786d0b038518aa2827c6b7/d1UnfivoVreYebS5JM3P9.jpeg",
            "isPro": false,
            "fullname": "Remek Kinas",
            "user": "Remek",
            "type": "user"
          },
          "name": "Remigiusz Kinas",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T06:51:13.426Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T07:03:41.000Z",
      "submittedOnDailyAt": "2025-05-12T07:25:02.402Z",
      "title": "비엘릭 11B v2 기술보고서",
      "submittedOnDailyBy": {
        "_id": "5e47d3eb178ca95365287400",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
        "isPro": true,
        "fullname": "Krzysztof Wróbel",
        "user": "djstrong",
        "type": "user"
      },
      "summary": "비엘릭 11B v2, 폴란드어의 텍스트 처리에 최적화된 가장 선진한 언어 모델을 소개합니다. 미스트랄 7B v0.2 아키텍처에 기반하여 깊이 업스케일링을 사용하여 파라미터 수를 11B로 확장한 모델입니다. 폴란드어의 벤치마크에서 우수한 성능을 보여주며, 크로스 라ン지스 역량을 강합니다. 두 가지 주요 기술 혁신을 소개합니다. 1. 가중치付き 인스톰션 크로스 엔트로피 손실: 다양한 인스톰션 타입의 학습을 최적화하기 위해, 학습 샘플에 질량 기반의 가중치를 할당합니다. 2. 적응 학습률: 컨텍스트의 길이에 따라 동적으로 조정됩니다. 여러 벤치마크에서의 상세한 평가는, Bielik 11B v2가 파라미터 수가 2~6배 더 많은 모델보다 뛰어나며, 언어 이해부터 복잡한 이유까지의 다양한 태스크에서, 다른 특수화된 폴란드어 모델을 크게 초과합니다. 모델의 파라미터 효율성과 분산 기능은 다양한 하드웨어 설정에서 도입이 가능하며, 폴란드어의 AI 기능 발전과 자원 효율적인 언어 모델링의 새로운 벤치마크를 설정합니다.",
      "upvotes": 16,
      "discussionId": "6819f19e5c7ea9f74284d3cc",
      "projectPage": "https://bielik.ai/",
      "githubRepo": "https://github.com/speakleash",
      "ai_keywords": [
        "Weighted Instruction Cross-Entropy Loss",
        "Adaptive Learning Rate",
        "depth up-scaling",
        "parameter efficiency",
        "quantization"
      ]
    },
    "publishedAt": "2025-05-05T03:03:41.000Z",
    "title": "Bielik 11B v2 Technical Report",
    "summary": "We present Bielik 11B v2, a state-of-the-art language model optimized for\nPolish text processing. Built on the Mistral 7B v0.2 architecture and scaled to\n11B parameters using depth up-scaling, this model demonstrates exceptional\nperformance across Polish language benchmarks while maintaining strong\ncross-lingual capabilities. We introduce two key technical innovations:\nWeighted Instruction Cross-Entropy Loss, which optimizes learning across\ndiverse instruction types by assigning quality-based weights to training\nexamples, and Adaptive Learning Rate, which dynamically adjusts based on\ncontext length. Comprehensive evaluation across multiple benchmarks\ndemonstrates that Bielik 11B v2 outperforms many larger models, including those\nwith 2-6 times more parameters, and significantly surpasses other specialized\nPolish language models on tasks ranging from linguistic understanding to\ncomplex reasoning. The model's parameter efficiency and extensive quantization\noptions enable deployment across various hardware configurations, advancing\nPolish language AI capabilities and establishing new benchmarks for\nresource-efficient language modeling in less-represented languages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e47d3eb178ca95365287400",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
      "fullname": "Krzysztof Wróbel",
      "name": "djstrong",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.06046",
      "authors": [
        {
          "_id": "6821af48696b63e207ae8474",
          "user": {
            "_id": "64cb98c6f103036e23c69b1d",
            "avatarUrl": "/avatars/7ee33880ad39f5335b618dc53554124a.svg",
            "isPro": false,
            "fullname": "Harris",
            "user": "Joshua-Harris",
            "type": "user"
          },
          "name": "Joshua Harris",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:48.631Z",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8475",
          "name": "Fan Grayson",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8476",
          "name": "Felix Feldman",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8477",
          "name": "Timothy Laurence",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8478",
          "name": "Toby Nonnenmacher",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8479",
          "name": "Oliver Higgins",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847a",
          "name": "Leo Loman",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847b",
          "name": "Selina Patel",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847c",
          "name": "Thomas Finnie",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847d",
          "name": "Samuel Collins",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847e",
          "name": "Michael Borowitz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-09T13:42:59.000Z",
      "submittedOnDailyAt": "2025-05-12T07:35:55.202Z",
      "title": "건강한 LLMs? UK 정부 공공보건정보의 LLM 지식 벤치마크",
      "submittedOnDailyBy": {
        "_id": "64cb98c6f103036e23c69b1d",
        "avatarUrl": "/avatars/7ee33880ad39f5335b618dc53554124a.svg",
        "isPro": false,
        "fullname": "Harris",
        "user": "Joshua-Harris",
        "type": "user"
      },
      "summary": "LLM의 확산은 특정 분야에서 지식의 세부적인 이해가 현실적인 사용의 성공에 필수적인 것으로 나타났습니다. 특히, 공공보건 분야에서는 적절한, 정확한, 최신 정보를 얻지 못할 경우 영국의 시민에게 중대한 영향을 미칠 가능성이 있습니다. 그러나 현재, LLM가 영국 정부의 공공보건 정보에 대해 어떤 지식이 있는지는 거의 알려져 있지 않습니다. 이 문제를 해결하기 위해, 본 논문에서는 LLM의 Multiple Choice Question Answering(MCQA)와 공공보건에 대한 자유형식의 답변을 평가하기 위한 새로운 벤치마크인 PubHealthBench를 통해 8000개 이상의 질문을 제공합니다. 이 질문들은 자동화 프로세스에서 생성되었습니다. 또한, PubHealthBench의 소스 텍스트로 사용될 영국 정부의 공공보건 지침문서의 새로운 데이터 세트도 릴리즈되었습니다. PubHealthBench에서 24개의 LLM을 평가한 결과, 최신의 공개하지 않은 LLM(GPT-4.5, GPT-4.1, o1)은 높은 지식을 보유하며, MCQA에서 90% 이상 달성하며, 검색 엔진을 사용하여 질문과 소스 텍스트를 사용하는 인간을 초과했습니다. 그러나 자유형식의 설정에서 75%를 초과하는 모델은 존재하지 않습니다. 따라서, 가장 선진된(SOTA) LLM이 공공보건 정보의 정확한 자원으로서의 가능성은 있습니다; 그러나 공공보건 주제에 대한 자유형식의 답변을 제공하기 위해서는 추가적인 보안 조치와 도구가 필요합니다.",
      "upvotes": 6,
      "discussionId": "6821af49696b63e207ae84c6",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Multiple Choice Question Answering (MCQA)",
        "PubHealthBench",
        "UK Government public health information",
        "automated pipeline",
        "extracted UK Government public health guidance documents",
        "SOTA (state of the art) LLMs",
        "GPT-4.5",
        "GPT-4.1",
        "o1"
      ]
    },
    "publishedAt": "2025-05-09T09:42:59.000Z",
    "title": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information",
    "summary": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries, created via an automated pipeline. We also\nrelease a new dataset of the extracted UK Government public health guidance\ndocuments used as source text for PubHealthBench. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% in the MCQA setup, and outperform\nhumans with cursory search engine use. However, in the free form setup we see\nlower performance with no model scoring >75%. Therefore, whilst there are\npromising signs that state of the art (SOTA) LLMs are an increasingly accurate\nsource of public health information, additional safeguards or tools may still\nbe needed when providing free form responses on public health topics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06046.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64cb98c6f103036e23c69b1d",
      "avatarUrl": "/avatars/7ee33880ad39f5335b618dc53554124a.svg",
      "fullname": "Harris",
      "name": "Joshua-Harris",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.06111",
      "authors": [
        {
          "_id": "68218b847202d193249511b6",
          "name": "Qingwen Bu",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511b7",
          "name": "Yanting Yang",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511b8",
          "name": "Jisong Cai",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511b9",
          "name": "Shenyuan Gao",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511ba",
          "user": {
            "_id": "646ec9b135f55eb49e405faa",
            "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
            "isPro": false,
            "fullname": "Guanghui Ren",
            "user": "sundrops",
            "type": "user"
          },
          "name": "Guanghui Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T06:50:15.305Z",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511bb",
          "name": "Maoqing Yao",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511bc",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511bd",
          "name": "Hongyang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-09T15:11:13.000Z",
      "submittedOnDailyAt": "2025-05-12T04:30:20.087Z",
      "title": "UniVLA: 태스크 중심의 잠재적 액션을 사용하여 어디서든 행동 학습을 수행합니다.",
      "submittedOnDailyBy": {
        "_id": "64ac1f169dcc5787461468a4",
        "avatarUrl": "/avatars/c031a75989147009b7850df4eddfcb27.svg",
        "isPro": false,
        "fullname": "Qingwen Bu",
        "user": "qwbu",
        "type": "user"
      },
      "summary": "일반적인 로봇은 다양한 환경에서 효과적으로 동작해야 합니다. 그러나 현재 많은 접근 방식은 동작 매뉴얼付き 데이터의 확장에 중점을 두고 있으며, 이를 통해 그 능력을 향상시키기 위해 강한 의존성을 가지고 있습니다. 따라서, 일반적으로 한 가지 물리적 특징을 제한하고, 서로 다른 기계체와 환경 사이에서 학습 가능한 지식을 배우는 것이 어렵습니다. 이러한 제한에 대처하기 위해, 우리는 새로운 프레임워크인 UniVLA를 제안합니다. 이것은 서로 다른 기계체 사이에서 학습 가능한 시각 언어 동작(VLA) 정책을 학습하는 새로운 프레임워크입니다. 우리의 주요 혁신은 잠재적인 동작 모델을 사용하여 비디오로부터 태스크 중심의 동작 표현을 얻을 수 있는 것입니다. 이로 인해, 다양한 기계체와 관점의 광범위한 범위의 데이터를 활용할 수 있습니다. 태스크 상관하지 않은 역학의 영향을 줄이기 위해, 언어 지시를 포함하고, DINO 특성 공간 내에서 잠재적인 동작 모델을 구축하는 것을 채택합니다. 인터넷 크기의 비디오로부터 학습된 일반적인 정책은 효율적인 잠재적인 동작 디코딩을 통해 다양한 로봇에 기계적으로 배치될 수 있습니다. 많은 동작과 네비게이션 벤치마크에서 가장 先端의 결과를 얻으며, 실제 로봇 배치도 실현되었습니다. UniVLA는 OpenVLA보다 높은 성능을 달성하며, 학습 계산량은 1/20 이하이고, 하류 데이터는 1/10 이하입니다. 인간의 비디오나 다른 종류의 데이터를 넹선 라인에 포함하여 지속적인 성능 향상이 전망됩니다. 이러한 결과는 UniVLA가 교환 가능한 효율적인 로봇 정책 학습에 도움을 줄 수 있다는 것을 보여줍니다.",
      "upvotes": 5,
      "discussionId": "68218b857202d19324951214",
      "githubRepo": "https://github.com/OpenDriveLab/UniVLA",
      "ai_keywords": [
        "UniVLA",
        "vision-language-action (VLA) policies",
        "latent action model",
        "DINO feature space",
        "latent action decoding",
        "manipulation benchmarks",
        "navigation benchmarks",
        "real-robot deployments",
        "OpenVLA"
      ]
    },
    "publishedAt": "2025-05-09T11:11:13.000Z",
    "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
    "summary": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06111.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ac1f169dcc5787461468a4",
      "avatarUrl": "/avatars/c031a75989147009b7850df4eddfcb27.svg",
      "fullname": "Qingwen Bu",
      "name": "qwbu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.05026",
      "authors": [
        {
          "_id": "6821771ddf190eabf5f666d8",
          "user": {
            "_id": "655c44752205aab35222aca3",
            "avatarUrl": "/avatars/57900539952382de0ce6892faf50b401.svg",
            "isPro": false,
            "fullname": "Jaehyun Jeon",
            "user": "jeochris",
            "type": "user"
          },
          "name": "Jaehyun Jeon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T06:50:17.832Z",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666d9",
          "name": "Jang Han Yoon",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666da",
          "name": "Min Soo Kim",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666db",
          "name": "Sumin Shim",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666dc",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666dd",
          "name": "Hanbin Kim",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666de",
          "name": "Youngjae Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T08:00:32.000Z",
      "submittedOnDailyAt": "2025-05-12T05:33:20.932Z",
      "title": "G-FOCUS: 사용자 인터페이스(UI) 디자인의 说服력(说服力)을 평가하기 위한 강력한 방법의 노력\n\n(Note: \"说服力\" is a direct translation of \"persuasiveness\" in Korean, but it is more commonly expressed as \"说服력\" in Korean. If you prefer a more natural Korean expression, it could be \"说服력(说服力)\" or simply \"说服력\".)",
      "submittedOnDailyBy": {
        "_id": "655c44752205aab35222aca3",
        "avatarUrl": "/avatars/57900539952382de0ce6892faf50b401.svg",
        "isPro": false,
        "fullname": "Jaehyun Jeon",
        "user": "jeochris",
        "type": "user"
      },
      "summary": "UI디자인의 효과성 평가는 미술적 성질을 초월하여 사용자의 행동을 영향을 미칠 수 있는 측면을 중점으로, 디자인의 설득력의 핵심 원칙에 기반하고 있다. A/B 테스트는 사용자 참여율을 높일 수 있는 UI를 선택하는 주요 방법 중 하나이며, 그러나 비용이 소요되며 시간이 걸린다. 최근의 Vision-Language 모델(VLM)은 자동화된 UI 분석을 처리할 수 있는 데에 성공하지만, 현재의 접근 방식은 독립적인 디자인 특성을 중점으로, 상대적으로 설득력을 평가하는 것이 아니라. 이에 대비하여, 우리는 WiserUI-Bench를 소개하며, 페어 UI 디자인의 설득력 평가를 위한 벤치마크로 한다. 이것은 300 쌍의 실세계의 UI 이미지를 포함하고 있으며, A/B 테스트 결과를 전문의의 이유로 레이블付け되어 있다. 또한, 우리는 G-FOCUS를 제안하며, VLM에 기반한 설득력 평가를 강화하고, 위치 편향을 줄이고 평가의 정확도를 높이는 새로운 추론 시점의 이유론 전략을 제안한다. 실험 결과는 G-FOCUS가 현재의 추론 전략을 초월하여 페어 UI 평가의 일치성과 정확성을 뛰어넘는 것으로 나타났다. VLM에 의한 UI의 설득력 평가의 촉진으로, 우리의 연구는 A/B 테스트를 보완하는 접근 방식을 제공하며, 교환 가능한 UI 취향 모델링과 디자인 최적화의 발전을 촉진한다. 코드와 데이터는 공개적으로 릴리즈된다.",
      "upvotes": 5,
      "discussionId": "68217722df190eabf5f66814",
      "ai_keywords": [
        "Vision-Language Models",
        "WiserUI-Bench",
        "Pairwise UI Design Persuasiveness Assessment",
        "G-FOCUS",
        "inference-time reasoning strategy",
        "position bias",
        "VLM-driven evaluation"
      ]
    },
    "publishedAt": "2025-05-08T04:00:32.000Z",
    "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness",
    "summary": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05026.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655c44752205aab35222aca3",
      "avatarUrl": "/avatars/57900539952382de0ce6892faf50b401.svg",
      "fullname": "Jaehyun Jeon",
      "name": "jeochris",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02686",
      "authors": [
        {
          "_id": "6821acfb2808328b91c0e365",
          "name": "Xiaobao Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T14:33:49.000Z",
      "submittedOnDailyAt": "2025-05-12T06:41:36.276Z",
      "title": "별에 탑승하는 AI: 대 언어 모델의 훈련 후 학습과 테스트 시 스케일링에 대한 보상 기반 학습 조사",
      "submittedOnDailyBy": {
        "_id": "64cb02869e30a46f7b80b355",
        "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
        "isPro": false,
        "fullname": "Xiaobao Wu",
        "user": "bobxwu",
        "type": "user"
      },
      "summary": "최근의 대언어 모델(LLMs)의 발전은 사전 학습 스케일링부터 후반 학습 및 테스트 시간 스케일링으로 진행되고 있습니다. 이러한 발전에서 하나의 핵심적인 통일된 패러다임이 나타났습니다: 보상으로부터 학습하는 것이 LLM의 행동을 지도하는 별과 역할을 합니다. 이는 강화 학습(RLHF, DPO, GRPO), 보상 가이드된 디코딩, 그리고 후 시간 보정 등 광범위하게 도입된 기술의 기초로 되어 있습니다. 중요한 점은 이 패러다임은 정적 데이터로부터的被动的 학습을 동적인 피드백으로부터의 능동적인 학습으로 전환할 수 있게 합니다. 이는 LLMs에 대해 일관된 선호와 깊은 이유론 능력을 부여합니다. 이 조사에서는 보상으로부터 학습하는 패러다임에 대한 상세한 개요를 제공합니다. 이 패러다임 아래에서의 학습, 추론, 그리고 후 추론 단계에 대한 전략을 분류하고 분석합니다. 또한 보상 모델의 벤치마크와 주요 응용에 대해 논의합니다. 마지막으로, 문제점과 미래의 방향성을 특징적으로 설명합니다. 보상으로부터 학습하는 LLMs의 논문 기록은 https://github.com/bobxwu/learning-from-rewards-llm-papers에 있습니다.",
      "upvotes": 3,
      "discussionId": "6821acfd2808328b91c0e3e3",
      "githubRepo": "https://github.com/bobxwu/learning-from-rewards-llm-papers",
      "ai_keywords": [
        "reinforcement learning",
        "RLHF",
        "DPO",
        "GRPO",
        "reward-guided decoding",
        "post-hoc correction",
        "active learning",
        "reward models"
      ]
    },
    "publishedAt": "2025-05-05T10:33:49.000Z",
    "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in\n  Post-Training and Test-Time Scaling of Large Language Models",
    "summary": "Recent developments in Large Language Models (LLMs) have shifted from\npre-training scaling to post-training and test-time scaling. Across these\ndevelopments, a key unified paradigm has arisen: Learning from Rewards, where\nreward signals act as the guiding stars to steer LLM behavior. It has\nunderpinned a wide range of prevalent techniques, such as reinforcement\nlearning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc\ncorrection. Crucially, this paradigm enables the transition from passive\nlearning from static data to active learning from dynamic feedback. This endows\nLLMs with aligned preferences and deep reasoning capabilities. In this survey,\nwe present a comprehensive overview of the paradigm of learning from rewards.\nWe categorize and analyze the strategies under this paradigm across training,\ninference, and post-inference stages. We further discuss the benchmarks for\nreward models and the primary applications. Finally we highlight the challenges\nand future directions. We maintain a paper collection at\nhttps://github.com/bobxwu/learning-from-rewards-llm-papers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02686.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb02869e30a46f7b80b355",
      "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
      "fullname": "Xiaobao Wu",
      "name": "bobxwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]