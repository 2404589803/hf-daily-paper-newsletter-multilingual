[
  {
    "paper": {
      "id": "2506.09113",
      "authors": [
        {
          "_id": "684a3b0a9b38e1e5a33a683f",
          "name": "Yu Gao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6840",
          "name": "Haoyuan Guo",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6841",
          "name": "Tuyen Hoang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6842",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6843",
          "name": "Lu Jiang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6844",
          "name": "Fangyuan Kong",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6845",
          "name": "Huixia Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6846",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6847",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6848",
          "name": "Xiaojie Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6849",
          "name": "Xunsong Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684a",
          "name": "Yifu Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684b",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684c",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684d",
          "name": "Jiawei Liu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684e",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684f",
          "name": "Xiaonan Nie",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6850",
          "name": "Zhiwu Qing",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6851",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6852",
          "name": "Li Sun",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6853",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6854",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6855",
          "name": "Sen Wang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6856",
          "name": "Guoqiang Wei",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6857",
          "name": "Guohong Wu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6858",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6859",
          "name": "Ruiqi Xia",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685a",
          "name": "Fei Xiao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685b",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685c",
          "name": "Jiangqiao Yan",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685d",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685e",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685f",
          "name": "Runkai Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6860",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6861",
          "name": "Yihang Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6862",
          "name": "Zilyu Ye",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6863",
          "name": "Xuejiao Zeng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6864",
          "name": "Yan Zeng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6865",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6866",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6867",
          "name": "Xiaozheng Zheng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6868",
          "name": "Peihao Zhu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6869",
          "name": "Jiaxin Zou",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a686a",
          "name": "Feilong Zuo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:56:11.000Z",
      "submittedOnDailyAt": "2025-06-12T01:08:56.090Z",
      "title": "Seedance 1.0: 이미지 생성 모델의 한계에 도전하기\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "6381c5d63680a7cf34e08ca9",
        "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
        "isPro": false,
        "fullname": "wujie10558@gmail.com",
        "user": "wujie10",
        "type": "user"
      },
      "summary": "현재 유사한 모델들은 덧셈소형 데이터의 정밀도와 의미있는 비디오 캡처를 추가하여 다양한 스케너에서 전방위적인 학습을 가능하게 하고, 덧셈소형 데이터의 정밀도와 의미있는 비디오 캡처를 추가하여 다양한 스케너에서 전방위적인 학습을 가능하게 하고, 효율적인 아키텍처 설계와 제안된 훈련 패러다임으로 다샷 생성과 문から 비디오, 이미지から 비디오의 두 가지 태스크의 공통 학습을 가능하게 하고, 여러 차원의 보상 구조를 사용한 비디오 고유의 RLHF와 미세한 사용자 피드백을 활용한 조정된 후 프로세스로 전체적인 성능 향상을 실현하고, 방향성 효과가 높은 모델 가속을 실현하여 다단계의 디자이너 전략과 시스템 수준의 최적화를 통해 추론 속도를 약 10배로 증가시키는 기술적 개선을 통합한 고성능 및 추론 효율이 높은 비디오 기반 모델입니다.\n\nSeedance 1.0은 NVIDIA-L20에서 41.4초 만에 1080p의 5초 비디오를 생성할 수 있습니다. 현재 가장 선진한 비디오 생성 모델과 비교하여, 고품질 및 고속의 비디오 생성, 스펙트럴 플래미디티와 구조적 안정성, 복잡한 다 섹터 컨텍스트에서 정밀한 지시의 준수, 원생적인 다샷 나눔티브이 일관된 주제 표현에 특화된 모델입니다.",
      "upvotes": 33,
      "discussionId": "684a3b0b9b38e1e5a33a686b",
      "projectPage": "https://seed.bytedance.com/seedance",
      "ai_summary": "Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.",
      "ai_keywords": [
        "diffusion modeling",
        "multi-source data curation",
        "precision and meaningful video captioning",
        "efficient architecture",
        "training paradigm",
        "multi-shot generation",
        "text-to-video",
        "image-to-video",
        "fine-grained supervised fine-tuning",
        "video-specific RLHF",
        "multi-dimensional reward mechanisms",
        "multi-stage distillation strategies",
        "model acceleration",
        "spatiotemporal fluidity",
        "structural stability",
        "instruction adherence",
        "multi-shot narrative coherence",
        "consistent subject representation"
      ]
    },
    "publishedAt": "2025-06-10T13:56:11.000Z",
    "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
    "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09113.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6381c5d63680a7cf34e08ca9",
      "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
      "fullname": "wujie10558@gmail.com",
      "name": "wujie10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06395",
      "authors": [
        {
          "_id": "68492dcf42e4f9106973f437",
          "user": {
            "_id": "6734e315c1aadce903f73aea",
            "avatarUrl": "/avatars/95d95c49419372debc201cb63c354b86.svg",
            "isPro": false,
            "fullname": "Li Pengyi",
            "user": "LiPengyi29",
            "type": "user"
          },
          "name": "Pengyi Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-11T07:18:40.287Z",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f438",
          "name": "Matvey Skripkin",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f439",
          "name": "Alexander Zubrey",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f43a",
          "name": "Andrey Kuznetsov",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f43b",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643984dceb7c5616ef3f5d54/5kHQrpj1ivFhnzHL36xhr.jpeg"
      ],
      "publishedAt": "2025-06-05T19:55:15.000Z",
      "submittedOnDailyAt": "2025-06-12T07:02:06.762Z",
      "title": "자신감은 모든 것일 수 있음: 언어 모델의 짧은 RL 미세 조정",
      "submittedOnDailyBy": {
        "_id": "643984dceb7c5616ef3f5d54",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
        "isPro": false,
        "fullname": "Andrey Kuznetsov",
        "user": "kuznetsoffandrey",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 논리론에 뛰어나지만, 토큰 이후의 훈련은 모델의 행동을 태스크의 목표에 맞게 조정하는 데 중요합니다. 현재의 강화 학습(RL) 방법은 고가의 인간의 어노테이션이나 외부의 보상 모델에 의존합니다. 우리는 모델의 자신감을 보상 신호로 사용하는 Self-Confidence 기반의 강화 학습(RLSC)을 제안합니다. 이는 라벨, 취향 모델이나 보상 학습의 필요성을 제거합니다. Qwen2.5-Math-7B에 대해, 16 샘플씩의 문제를 대상으로 10 또는 20 단계의 훈련을 수행하면, RLSC는 AIME2024에서 +13.4%, MATH500에서 +21.2%, Minerva Math에서 +21.7%, Olympiadbench에서 +20.8%, AMC23에서 +9.7%의 정확도를 향상시킵니다. RLSC는 소수점 수의 샘플과 무 라벨의 슈퍼바이온을 필요로 하는 간단하고 scalable한 토큰 이후의 훈련 방법을 제공합니다.",
      "upvotes": 27,
      "discussionId": "68492dd042e4f9106973f43c",
      "ai_summary": "Reinforcement Learning via Self-Confidence (RLSC) improves large language model accuracy using the model's confidence as a reward signal, eliminating the need for human labels or reward engineering.",
      "ai_keywords": [
        "Reinforcement Learning",
        "Large language models",
        "self-confidence",
        "RLSC"
      ]
    },
    "publishedAt": "2025-06-05T15:55:15.000Z",
    "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models",
    "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,\nRLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on\nMinerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a\nsimple, scalable post-training method for inference models, requiring only a\nsmall number of samples and unlabelled supervision.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643984dceb7c5616ef3f5d54/5kHQrpj1ivFhnzHL36xhr.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06395.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "643984dceb7c5616ef3f5d54",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
      "fullname": "Andrey Kuznetsov",
      "name": "kuznetsoffandrey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09995",
      "authors": [
        {
          "_id": "684a39639b38e1e5a33a6837",
          "name": "Yuanpeng Tu",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a6838",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a6839",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683a",
          "name": "Xiang Bai",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683b",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683c",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:59:53.000Z",
      "submittedOnDailyAt": "2025-06-12T00:50:19.796Z",
      "title": "PlayerOne: 자기중심의 세계 시뮬레이터",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "플레이어원, 최초의 중심적인 현실 세계 시뮬레이터를 소개합니다. 이 것은 활기찬 환경 내에서 제한없이 탐험을 촉진하는 것입니다. 사용자로부터 중심적인 스케마 이미지를 받아들이고, 그에 대응하는 세계를 정확히 구축하고 중심적인 비디오를 생성합니다. 이러한 비디오는 사용자가 중심적인 카메라로 촬영한 실제 스케마의 인간들의 움직임과 일치합니다. 플레이어원은粗略에서 세부로 나뭇잎처럼 진행되는 학습을 통해 배워갑니다. 먼저, 큰 규모의 중심적인 텍스트-비디오 페어에 대한 사전 학습을 수행하고, 중심적인 이해를 실현합니다. 다음으로, 중심적인 비디오 데이터 세트에서 추출된 동기화된 움직임-비디오 데이터에 기반한 조정 학습을 수행합니다. 또한, 다른 컴포넌트의 중요성 차이를 고려하여 부분별 움직임 注入 시퀀을 설계하고, 부분 수준의 움직임을 정밀하게 제어할 수 있습니다. 또한, 4D 스케마와 비디오 프레임을 모두 발전적으로 모델화하는 두 가지의 가중치 복합 재구성 프레임워크를 설계하고, 긴 비디오 생성에서의 스케마의 일관성을 보장합니다. 실험 결과를 통해 다양한 스케마의 세계적인 모델링과 결정적인 제어 능력을 보여주며, 중심적인 실세계 시뮬레이션의 첫 시도이며, 세계 모델링의 새로운 경계를 도전하는 커뮤니티의 길을 열어줍니다.",
      "upvotes": 22,
      "discussionId": "684a39639b38e1e5a33a683d",
      "projectPage": "https://playerone-hku.github.io/",
      "ai_summary": "PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.",
      "ai_keywords": [
        "egocentric realistic world simulator",
        "coarse-to-fine pipeline",
        "pretraining",
        "finetuning",
        "synchronous motion-video data",
        "automatic construction pipeline",
        "part-disentangled motion injection",
        "joint reconstruction framework",
        "4D scene",
        "video frames",
        "scene consistency",
        "long-form video generation",
        "worldconsistent modeling"
      ]
    },
    "publishedAt": "2025-06-11T13:59:53.000Z",
    "title": "PlayerOne: Egocentric World Simulator",
    "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09995.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09350",
      "authors": [
        {
          "_id": "684a79ca9b38e1e5a33a68bf",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c0",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c1",
          "name": "Hao He",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c2",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c3",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c4",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c5",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c6",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c7",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T03:04:23.000Z",
      "submittedOnDailyAt": "2025-06-12T05:25:53.654Z",
      "title": "自動회귀의 상대 누구든지 인식할 수 있는 후처리 훈련을 사용한 시간 단위의 대화 비디오 생성",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "현재의 대형 비디오 생성 모델은 계산량이 크고, 시간적으로 실행할 수 없기 때문에 실시간 또는 상호작용적인 애플리케이션에서 도입이 어렵습니다. 본 연구에서는, 전학습 라틴 비디오 디퓨저 모델을 시간적, 상호작용적인 비디오 생성기로 변환하기 위해 자동복원적 적대 후학습(AAPT)을 제안합니다. 우리 모델은 1NFE(1 뉴럴 피보로치)를 사용하여 한 시간에 한 라틴 프레임을 자동복원적으로 생성합니다. 모델은 사용자에게 시간적으로 결과를 스트리밍, 상호작용적인 응답을 받아 다음 라틴 프레임을 생성할 수 있습니다. 기존의 접근 방식과 달리, 우리 방법은 자동복원적 생성에 대한 적대적 훈련을 효과적인 패러다임으로 검토합니다. 이는 KV 캐시를 충분히 활용하여 한 단계의 생성에 효율적인 아키텍처를 설계할 수 있으며, 긴 비디오 생성 시 오류의 축적을 줄이기 위해 학습 강제 훈련 같은 방법을 모델을 훈련할 수 있습니다. 실험 결과를 통해, 우리 8B 모델은 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴 프레임을 생성하기 위해 시간적으로 실행하고, 1NFE를 사용하여 한 라틴",
      "upvotes": 22,
      "discussionId": "684a79ca9b38e1e5a33a68c8",
      "projectPage": "https://seaweed-apt.com/2",
      "ai_summary": "Autoregressive adversarial post-training transforms pre-trained latent video diffusion models into real-time, interactive video generators with reduced computational requirements.",
      "ai_keywords": [
        "autoregressive adversarial post-training",
        "latent video diffusion model",
        "autoregressive generation",
        "neural function evaluation",
        "KV cache",
        "student-forcing",
        "real-time video generation",
        "24fps",
        "736x416 resolution",
        "1280x720 resolution",
        "H100"
      ]
    },
    "publishedAt": "2025-06-10T23:04:23.000Z",
    "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
    "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09350.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7093
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08889",
      "authors": [
        {
          "_id": "684a39599b38e1e5a33a6822",
          "name": "Yizhao Gao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6823",
          "name": "Shuming Guo",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6824",
          "name": "Shijie Cao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6825",
          "name": "Yuqing Xia",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6826",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6827",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6828",
          "name": "Lingxiao Ma",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6829",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682a",
          "name": "Tianzhu Ye",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682b",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682c",
          "name": "Hayden Kwok-Hay So",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682d",
          "name": "Yu Hua",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682e",
          "name": "Ting Cao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682f",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6830",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T15:17:26.000Z",
      "submittedOnDailyAt": "2025-06-12T00:54:43.454Z",
      "title": "SeerAttention-R: 장기간 계산을 위한 희소적 어텐션 어드밴터",
      "submittedOnDailyBy": {
        "_id": "661c96f48921f03a9dae04c3",
        "avatarUrl": "/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg",
        "isPro": false,
        "fullname": "Yizhao Gao",
        "user": "LongMountain",
        "type": "user"
      },
      "summary": "我们引入了SeerAttention-R，这是一个专门针对推理模型长解码的稀疏注意力框架。从SeerAttention扩展而来，SeerAttention-R保留了通过自蒸馏门控机制学习注意力稀疏性的设计，同时移除了查询池化以适应自回归解码。通过轻量级的插件式门控机制，SeerAttention-R具有灵活性，可以轻松集成到现有的预训练模型中，而无需修改原始参数。我们证明了在仅使用0.4B个标记进行训练的情况下，SeerAttention-R在AIME基准测试中，在大稀疏注意力块大小（64/128）下，保持了近乎无损的推理精度，预算为4K个标记。使用TileLang，我们开发了一个高度优化的稀疏解码内核，在H100 GPU上以90%的稀疏度实现了接近理论极限的9倍加速，超过了FlashAttention-3。代码可在以下链接获取：https://github.com/microsoft/SeerAttention。",
      "upvotes": 15,
      "discussionId": "684a39599b38e1e5a33a6833",
      "githubRepo": "https://github.com/microsoft/SeerAttention",
      "ai_summary": "SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.",
      "ai_keywords": [
        "sparse attention",
        "reasoning models",
        "self-distilled gating mechanism",
        "query pooling",
        "lightweight plug-in gating",
        "AIME benchmark",
        "TileLang",
        "sparse decoding kernel",
        "FlashAttention-3",
        "H100 GPU"
      ]
    },
    "publishedAt": "2025-06-10T11:17:26.000Z",
    "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
    "summary": "We introduce SeerAttention-R, a sparse attention framework specifically\ntailored for the long decoding of reasoning models. Extended from\nSeerAttention, SeerAttention-R retains the design of learning attention\nsparsity through a self-distilled gating mechanism, while removing query\npooling to accommodate auto-regressive decoding. With a lightweight plug-in\ngating, SeerAttention-R is flexible and can be easily integrated into existing\npretrained model without modifying the original parameters. We demonstrate that\nSeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning\naccuracy with 4K token budget in AIME benchmark under large sparse attention\nblock sizes (64/128). Using TileLang, we develop a highly optimized sparse\ndecoding kernel that achieves near-theoretical speedups of up to 9x over\nFlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:\nhttps://github.com/microsoft/SeerAttention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08889.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661c96f48921f03a9dae04c3",
      "avatarUrl": "/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg",
      "fullname": "Yizhao Gao",
      "name": "LongMountain",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09790",
      "authors": [
        {
          "_id": "684a33989b38e1e5a33a6804",
          "name": "Zhenran Xu",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6805",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6806",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6807",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6808",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6809",
          "name": "Kaifu Zhang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a680a",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a680b",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T14:35:15.000Z",
      "submittedOnDailyAt": "2025-06-12T00:38:07.422Z",
      "title": "ComfyUI-R1: 작업 흐름 생성을 위한 추론 모델의 검토",
      "submittedOnDailyBy": {
        "_id": "639c379cdb7c5f35004066cb",
        "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
        "isPro": false,
        "fullname": "Zhenran Xu",
        "user": "imryanxu",
        "type": "user"
      },
      "summary": "AI 생성된 콘텐츠는 단일 모델에서 모듈화된 작업 프로세스로 발전하여, 특히 ComfyUI 등 플랫폼에서 창의적 프로세스의定制가 가능해진다. 그러나 효과적인 작업 프로세스를 생성하기 위해서는 많은 전문 컴포넌트를 조정하는 매우 높은 전문 지식을 필요로 한다. 이는 사용자에게 급격한 학습 곡선으로 작용한다. 이러한 도전을 대처하기 위해, 우리는 ComfyUI-R1을 소개한다. 이는 첫 번째로 작업 프로세스 자동화에 사용되는 대형 추론 모델이다. 우리는 4K 작업 프로세스 데이터셋을 신중하게 계획하여, 노드 선택, 작업 프로세스 계획, 코드 수준 작업 프로세스 표현을 포함하는 긴 체인 사고(CoT) 추론 데이터로 구축했다. ComfyUI-R1은 두 단계 프레임워크를 통해 훈련된다: (1) CoT 미세 조정을 통해 초기 시작을 위한 모델의 ComfyUI 영역에 적응; (2) 미세 조정 이후, 미세한 규칙-도메인 혼합 보상을 통해 추론 능력을 자극하는 강화 학습을 수행한다. 실험 결과, 우리의 70억 매개변수 모델은 97%의 형식 유효성율, 고 통과율, 노드 수준 및 그래프 수준 F1 점수를 달성하여, 이전의 가장 높은 수준의 방법보다, GPT-4o와 Claude 시리즈 등 선도적인 폐원 모델을 사용하는 방법보다 크게 뛰어난 성능을 보였다. 추가적인 분석은 추론 과정의 핵심 역할을 강조하고, 작업 프로세스를 코드로 변환하는 이점을 강조했다. 질적 비교는 우리가 합성된 복잡한 작업 프로세스와 다样화 된 노드에 대한 우점을 드러내며, AI 예술 창작에서 긴 CoT 추론의 잠재력을 강조했다.",
      "upvotes": 14,
      "discussionId": "684a33989b38e1e5a33a680c",
      "projectPage": "https://github.com/AIDC-AI/ComfyUI-Copilot",
      "githubRepo": "https://github.com/AIDC-AI/ComfyUI-Copilot",
      "ai_summary": "ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.",
      "ai_keywords": [
        "modular workflows",
        "ComfyUI",
        "large reasoning model",
        "automated workflow generation",
        "chain-of-thought (CoT) reasoning",
        "node selection",
        "workflow planning",
        "code-level workflow representation",
        "CoT fine-tuning",
        "reinforcement learning",
        "fine-grained rule-metric hybrid reward",
        "format validity",
        "structural integrity",
        "node-level fidelity",
        "GPT-4o",
        "Claude series",
        "pass rate",
        "node-level F1 scores",
        "graph-level F1 scores",
        "intricate workflows",
        "diverse nodes",
        "qualitative comparison",
        "AI art creation"
      ]
    },
    "publishedAt": "2025-06-11T10:35:15.000Z",
    "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation",
    "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09790.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639c379cdb7c5f35004066cb",
      "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
      "fullname": "Zhenran Xu",
      "name": "imryanxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09003",
      "authors": [
        {
          "_id": "6848eed742e4f9106973f2cf",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d0",
          "name": "Jiaxi Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d1",
          "name": "Min Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d2",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d3",
          "name": "Mouxiang Chen",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d4",
          "name": "Jiajun Zhang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d5",
          "name": "Zeyu Cui",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d6",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d7",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:23:33.000Z",
      "submittedOnDailyAt": "2025-06-12T00:17:18.390Z",
      "title": "SWE-Flow: 테스트 주도의 소프트웨어 개발 데이터의 합성",
      "submittedOnDailyBy": {
        "_id": "64c38871f9cd765462fa1a17",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg",
        "isPro": false,
        "fullname": "Lei Zhang",
        "user": "Lemoncoke",
        "type": "user"
      },
      "summary": "우리는 **SWE-Flow**를 소개합니다. 이는 테스트 주도 개발(TDD)에 기반한 새로운 데이터 합성 프레임워크입니다. 기존의 소프트웨어 엔지니어링 데이터는 인간이 제출한 이슈를 기반으로하지만, **SWE-Flow**는 단위 테스트에서 직접 증분 개발 단계를 자동적으로 추론합니다. 단위 테스트는 고차원 요구 사항을 내포하기 때문에. **SWE-Flow**의 핵심은 런타임 의존성 그래프(RDG)의 구축입니다. 이는 함수 상호작용을 정확히 캡처하여 구조화된 단계별 *개발 일정*을 생성합니다. 각 단계마다, **SWE-Flow**는 부분적인 코드베이스, 대응하는 단위 테스트, 필요한 코드 수정을 생성하여 완전히 검증 가능한 TDD 작업이 됩니다. 이 접근 방식에 따라, 우리는 실제 세계의 GitHub 프로젝트에서 16,061개의 훈련 데이터와 2,020개의 테스트 데이터를 생성하여 **SWE-Flow-Eval** 벤치마크를 만듭니다. 우리의 실험은 이 데이터셋에 대해 오픈 모델을 미세 조정하면 TDD 기반의 코딩 성능이 크게 향상되는 것을 보여주었습니다. 더 많은 연구를 위해, 우리는 [Github](https://github.com/Hambaobao/SWE-Flow)에서 모든 코드, 데이터셋, 모델, 그리고 Docker 이미지를 공개합니다.",
      "upvotes": 13,
      "discussionId": "6848eed842e4f9106973f2d8",
      "githubRepo": "https://github.com/Hambaobao/SWE-Flow",
      "ai_summary": "A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.",
      "ai_keywords": [
        "Test-Driven Development (TDD)",
        "Runtime Dependency Graph (RDG)",
        "SWE-Flow",
        "unit tests",
        "development schedule",
        "SWE-Flow-Eval",
        "fine-tuning",
        "open model"
      ]
    },
    "publishedAt": "2025-06-10T13:23:33.000Z",
    "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner",
    "summary": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in\nTest-Driven Development (TDD). Unlike existing software engineering data that\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\ndevelopment steps directly from unit tests, which inherently encapsulate\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\nenabling the generation of a structured, step-by-step *development schedule*.\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\ntests, and the necessary code modifications, resulting in fully verifiable TDD\ntasks. With this approach, we generated 16,061 training instances and 2,020\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\nbenchmark. Our experiments show that fine-tuning open model on this dataset\nsignificantly improves performance in TDD-based coding. To facilitate further\nresearch, we release all code, datasets, models, and Docker images at\n[Github](https://github.com/Hambaobao/SWE-Flow).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09003.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64c38871f9cd765462fa1a17",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg",
      "fullname": "Lei Zhang",
      "name": "Lemoncoke",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09984",
      "authors": [
        {
          "_id": "684a49fa9b38e1e5a33a6884",
          "name": "Zhenzhi Wang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6885",
          "name": "Jiaqi Yang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6886",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6887",
          "name": "Chao Liang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6888",
          "name": "Gaojie Lin",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6889",
          "name": "Zerong Zheng",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a688a",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a688b",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:57:09.000Z",
      "submittedOnDailyAt": "2025-06-12T02:02:32.983Z",
      "title": "InterActHuman: 다 개념 인간 애니메이션과 순서대로 일치하는 음성\n조건",
      "submittedOnDailyBy": {
        "_id": "6519346a186bc3b6997c1aaf",
        "avatarUrl": "/avatars/8981bd278962da50f9bbfb92c2abe2bf.svg",
        "isPro": false,
        "fullname": "Zhenzhi Wang",
        "user": "zhenzhiwang",
        "type": "user"
      },
      "summary": "終端から終端まで의 풍부한 다모달 조건（예：텍스트、이미지、음성）를 사용하여 인간 애니메이션은 최근에 놀라운 발전을 거듭하고 있습니다. 그러나 현재의 많은 방법이 단일 엔티티의 애니메이션을 수행하며, 조건을 전역적인 방법으로 주입하여, 같은 비디오에서 여러 개념이 나타날 때, 풍부한 인간 간 상호작용과 인간・물체 간 상호작용을 무시하고 있습니다. 이러한 전역적인 가정은 여러 개념（이 안에 인간이나 물체도 포함됩니다）의 정밀한, 프로젝트 광역 필드에 대한 제어를 방해하고, 애플리케이션을 방해하고 있습니다. 본 논문에서는 단일 엔티티의 가정을 버리고 새로운 프레임워크를 도입하여, 모달로부터의 조건을 각 인덴티티의 시공간적인 흔적에 강한, 영역 고유의 결합을 강제합니다. 여러 개념의 참조 이미지를 제공하면, 우리 방법은 마스크 프로듀서를 사용하여 노이즈 이미지와 각 참조 외관의 얼굴색 코드를 매칭하여 자동으로 레이ア우트 정보를 추정합니다. 또한, 로컬적인 음성 조건을 대응하는 영역에 주입하여, 여러 개념의 레이ア우트에 일치하는 모달 매칭을 진행하는 것입니다. 이 설계는 제어 가능한 인간 중심의 멀티 개념 비디오의 고품질 생성을 가능하게 합니다. 실험 결과를 통해, 우리의 명시적인 레이ア우트 제어의 효과성을 보여주고, 기존의 방법과 비교하여, 풍부한 다모달 조건의 상대적인 효과성을 증명합니다.",
      "upvotes": 9,
      "discussionId": "684a49fa9b38e1e5a33a688c",
      "ai_summary": "A new framework enables precise, per-identity control of multiple concepts in end-to-end human animation by enforcing region-specific binding of multi-modal conditions.",
      "ai_keywords": [
        "human animation",
        "multi-modal conditions",
        "mask predictor",
        "denoised video",
        "layout information",
        "local audio condition",
        "controllable multi-concept videos",
        "explicit layout control"
      ]
    },
    "publishedAt": "2025-06-11T13:57:09.000Z",
    "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions",
    "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6519346a186bc3b6997c1aaf",
      "avatarUrl": "/avatars/8981bd278962da50f9bbfb92c2abe2bf.svg",
      "fullname": "Zhenzhi Wang",
      "name": "zhenzhiwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09937",
      "authors": [
        {
          "_id": "684a3d639b38e1e5a33a686d",
          "name": "Qiao Gu",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a686e",
          "name": "Yuanliang Ju",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a686f",
          "name": "Shengxiang Sun",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6870",
          "name": "Igor Gilitschenski",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6871",
          "name": "Haruki Nishimura",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6872",
          "name": "Masha Itkina",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6873",
          "name": "Florian Shkurti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T16:59:13.000Z",
      "submittedOnDailyAt": "2025-06-12T01:08:05.216Z",
      "title": "SAFE: 시각・언어・행동 모델의 복수 태스크 실패 검출",
      "submittedOnDailyBy": {
        "_id": "63d1df92f7f31a66a2d7292c",
        "avatarUrl": "/avatars/eb3339c1f2c82742b518b8a7f142e99a.svg",
        "isPro": false,
        "fullname": "Qiao Gu",
        "user": "guqiao",
        "type": "user"
      },
      "summary": "VISION-LANGUAGE-ACTION MODEL (VLAs)은 다양한 작업 태스크에서 로봇의 행동을 보여주는 잠재력을 가지고 있지만, 새로운 태스크에서의 구현에서는 성공률에 제한이 있습니다. 이러한 정책은 환경과 상호작용을 허용하면서 안전하게 작동하기 위해 로봇이 정지, 후진하거나 도움을 요청할 수 있도록 실패 검출기가 필요합니다. 그러나 현재의 실패 검출기는 VLAs가 요구하는 새로운 태스크나 새로운 환경에서의 실패를 검출할 수 있는 일반화에는 부족합니다. 본 논문에서는 다 태스크 실패 검출 문제를 도입하고, VLAs와 같은 일반적인 로봇 정책에 적합한 실패 검출기 SAFE를 제안합니다. VLA의 특징 공간을 분석하여, VLAs는 태스크의 성공과 실패에 대한 고 수준의 지식을 다른 태스크에서도 공통적으로 가지고 있다는 것을 발견했습니다. 이러한 관점에서, SAFE는 VLA의 내부 특징으로부터 학습하여 태스크의 실패 가능성에 대한 단일 스칼라를 예측하도록 설계되었습니다. SAFE는 성공한 것과 실패한 것 모두를 훈련시키고, 새로운 태스크를 평가합니다. SAFE는 다른 정책 아키텍처와 호환성을 가지고 있습니다. OpenVLA, pi_0, pi_0-FAST의 둘 다, 문헌적 및 실세계 환경에서 엄격하게 테스트되었습니다. SAFE는 다양한 baseline과 비교하여, 정확성과 검출 시간의 균형을 통해 가장 先端한 실패 검출 성능을 달성했습니다. 자세한 질적인 결과를 https://vla-safe.github.io/ 에서 확인 가능합니다.",
      "upvotes": 3,
      "discussionId": "684a3d639b38e1e5a33a6874",
      "ai_summary": "SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.",
      "ai_keywords": [
        "vision-language-action models",
        "VLAs",
        "multitask failure detection",
        "failure detector",
        "feature space",
        "high-level knowledge",
        "scalar prediction",
        "rollout",
        "conformal prediction"
      ]
    },
    "publishedAt": "2025-06-11T12:59:13.000Z",
    "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
    "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, pi_0,\nand pi_0-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09937.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d1df92f7f31a66a2d7292c",
      "avatarUrl": "/avatars/eb3339c1f2c82742b518b8a7f142e99a.svg",
      "fullname": "Qiao Gu",
      "name": "guqiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08001",
      "authors": [
        {
          "_id": "684a649f9b38e1e5a33a6895",
          "name": "Zeju Qiu",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6896",
          "name": "Simon Buchholz",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6897",
          "name": "Tim Z. Xiao",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6898",
          "name": "Maximilian Dax",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6899",
          "name": "Bernhard Schölkopf",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a689a",
          "name": "Weiyang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:59:34.000Z",
      "submittedOnDailyAt": "2025-06-12T03:55:47.829Z",
      "title": "직교 등가 변환에 의한 재정의 LLM 훈련",
      "submittedOnDailyBy": {
        "_id": "648905d1a15c43c791d4381f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
        "isPro": false,
        "fullname": "Weiyang Liu",
        "user": "wy1iu",
        "type": "user"
      },
      "summary": "라르지 란그주주야 모델(LLMs)가 인공지능의 급속한 발전을 주도하면서, 이러한 대규모 모델의 효과적이고 신뢰성 있는 훈련은 분야의 가장 큰 문제 중 하나입니다. 이 문제를 대처하기 위해, 우리는 새로운 재파라미터 훈련 알고리즘을 제안합니다. POET. 특히, POET는 학습 가능한 두 개의 직교 행렬과 고정된 난수 가중 행렬을 사용하여 각 뉴런을 재파라미터화합니다. 이러한 설계로, POET는 가중 행렬의 스펙트럴 특성을 증명적으로 유지함으로써, 목적 함수의 안정적인 최적화와 일반화 성능의 향상을 실현할 수 있습니다. 또한, POET의 유연성과 scalability를 강화하기 위해 효율적인 근사 방법을 개발했습니다. 확산된 실험은 POET가 LLMs의 훈련에 대한 효과와 scalability를 증명했습니다.",
      "upvotes": 2,
      "discussionId": "684a649f9b38e1e5a33a689b",
      "projectPage": "https://spherelab.ai/poet/",
      "githubRepo": "https://github.com/Sphere-AI-Lab/poet",
      "ai_summary": "POET is a reParameterized training algorithm using Orthogonal Equivalence Transformation to optimize neurons in large language models, ensuring stable training and improved generalization.",
      "ai_keywords": [
        "POET",
        "reParameterized training algorithm",
        "Orthogonal Equivalence Transformation",
        "orthogonal matrices",
        "spectral properties",
        "weight matrices",
        "large language models",
        "generalization",
        "efficient approximations",
        "training",
        "scalability"
      ]
    },
    "publishedAt": "2025-06-09T13:59:34.000Z",
    "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
    "summary": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648905d1a15c43c791d4381f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
      "fullname": "Weiyang Liu",
      "name": "wy1iu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08900",
      "authors": [
        {
          "_id": "684a96ab9aebf043cf7bdb2e",
          "name": "José Morano",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb2f",
          "name": "Botond Fazekas",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb30",
          "name": "Emese Sükei",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb31",
          "name": "Ronald Fecso",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb32",
          "name": "Taha Emre",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb33",
          "name": "Markus Gumpinger",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb34",
          "name": "Georg Faustmann",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb35",
          "name": "Marzieh Oghbaie",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb36",
          "name": "Ursula Schmidt-Erfurth",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb37",
          "name": "Hrvoje Bogunović",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655b3383ed8df831286969f0/FuwagzdL2fBNhZyfwg22J.png"
      ],
      "publishedAt": "2025-06-10T15:25:55.000Z",
      "submittedOnDailyAt": "2025-06-12T07:31:36.181Z",
      "title": "MIRAGE: 다모렐의 기초 모델과 종합적인 뿔막막 OCT 영상 분석 벤치마크",
      "submittedOnDailyBy": {
        "_id": "655b3383ed8df831286969f0",
        "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
        "isPro": false,
        "fullname": "José Morano",
        "user": "j-morano",
        "type": "user"
      },
      "summary": "인공지능(AI)은 임상 의사가 사용하는 눈과 관련된 영상 분석을 위한 기본적인 도구로 작동하고 있습니다. 특히 광학 코히레릭 토포그래피(OCT)와 같은 눈과 관련된 영상 분석을 위해 사용됩니다. 그러나 AI 모델의 개발은 광범위한 어노테이션이 필요하며, 독립된, 이전에 본 적이 없는 데이터에 대한 성능 저하 경향이 있습니다. 기초 모델(FM)은 큰 AI 모델이며, 큰 무라벨 데이터 세트를 사용하여 훈련되어 있으며, 이러한 도전에 대해 뛰어난 성능을 보여주고 있습니다. 그러나 현재의 눈과 관련된 FM은 특히 분할 태스크에 대한 매우 상세한 검증이 부족하며, 하나의 영상 모델에만 집중하고 있습니다. 이러한 배경 아래, 우리는 MIRAGE, OCT와 스캔닝레이저 옥토미(SLO) 영상 분석을 위한 새로운 다 모델 FM을 제안합니다. 또한, OCT/SLO 분류와 분할 태스크에 대한 새로운 평가 벤치마크도 제안합니다. 일반 모델과 전문 FM 및 분할 방법과의 비교는 MIRAGE가 두 가지 태스크에서 뛰어난 성능을 보여주며, 옥토미의 OCT 영상 분석을 위한 강력한 AI 시스템의 개발에 기초가 되는 것을 명확히 합니다. MIRAGE와 평가 벤치마크는 공개적으로 사용할 수 있습니다: https://github.com/j-morano/MIRAGE.",
      "upvotes": 1,
      "discussionId": "684a96ab9aebf043cf7bdb38",
      "githubRepo": "https://github.com/j-morano/MIRAGE",
      "ai_summary": "MIRAGE, a multimodal foundation model, outperforms existing models in the classification and segmentation of OCT and SLO images, demonstrating its potential for robust AI in ophthalmologic image analysis.",
      "ai_keywords": [
        "foundational models",
        "multimodal models",
        "optical coherence tomography",
        "scanning laser ophthalmoscopy",
        "image segmentation",
        "evaluation benchmark"
      ]
    },
    "publishedAt": "2025-06-10T11:25:55.000Z",
    "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive\n  retinal OCT image analysis",
    "summary": "Artificial intelligence (AI) has become a fundamental tool for assisting\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\n(OCT). However, developing AI models often requires extensive annotation, and\nexisting models tend to underperform on independent, unseen data. Foundation\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\npromise in overcoming these challenges. Nonetheless, available FMs for\nophthalmology lack extensive validation, especially for segmentation tasks, and\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\nclassification and segmentation tasks. The comparison with general and\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\nboth types of tasks, highlighting its suitability as a basis for the\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\nand the evaluation benchmark are publicly available:\nhttps://github.com/j-morano/MIRAGE.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655b3383ed8df831286969f0/FuwagzdL2fBNhZyfwg22J.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655b3383ed8df831286969f0",
      "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
      "fullname": "José Morano",
      "name": "j-morano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09007",
      "authors": [
        {
          "_id": "6849516c42e4f9106973f4d1",
          "name": "Sophia Tang",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d2",
          "name": "Yinuo Zhang",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d3",
          "name": "Alexander Tong",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d4",
          "name": "Pranam Chatterjee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:29:48.000Z",
      "submittedOnDailyAt": "2025-06-12T01:32:31.298Z",
      "title": "분지 슈리닝거 브릿지 매칭",
      "submittedOnDailyBy": {
        "_id": "64cd5b3f0494187a9e8b7c69",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
        "isPro": false,
        "fullname": "Pranam Chatterjee",
        "user": "pranamanam",
        "type": "user"
      },
      "summary": "초기 및 목표 분포 사이의 중간적인 트라지젝트 리를 예측하는 것은 생성 모델링의 핵심적인 문제입니다. 현재의 접근 방식에서, 2개의 분포 사이의 매핑을 학습하기 위해, 하나의 확률 패스를 모델화하는 방법이 효과적이며, フロー マッチング, シンデローガー ブリッド マッチング 등이 있습니다. 그러나 이러한 방법은 본질적으로 한 가지 모우드의 이동에 제한되어, 한 가지 공통의 출발점으로부터 다양한 결과를 나누는 분기적 진화를 이해할 수 없습니다. 이를 해결하기 위해, BranchSBM(ブランチ シンデローガー ブリッド マッチング)를 소개합니다. BranchSBM은 분기 シンデローガー ブリッド를 학습하기 위한 새로운 프레임워크로, 시간 의존성 있는 속도 필드과 성장 과정이 여러 파라미터로 모델링되고, 인구 수준의 분기를 여러 종단 분포에 통합할 수 있습니다. BranchSBM은 다 경로 표면 ナビゲーション, 균일한 조상 상태에서 세포 유전 분기 모델링, 그리고 세포 반응의 분기를 シミュレート함으로써, 더 표현력 있는 모델로, 이러한 작업에는 필수적입니다.",
      "upvotes": 0,
      "discussionId": "6849516d42e4f9106973f4d5",
      "ai_summary": "BranchSBM, a novel generative modeling framework, extends Schr\\\"odinger Bridge Matching to model branched stochastic paths and multi-path evolution from a single initial distribution to multiple outcomes.",
      "ai_keywords": [
        "flow matching",
        "Schr\\\"odinger Bridge Matching",
        "Branched Schr\\\"odinger Bridge Matching",
        "BranchSBM",
        "time-dependent velocity fields",
        "growth processes",
        "multi-path surface navigation",
        "cell fate bifurcations",
        "cellular responses to perturbations"
      ]
    },
    "publishedAt": "2025-06-10T13:29:48.000Z",
    "title": "Branched Schrödinger Bridge Matching",
    "summary": "Predicting the intermediate trajectories between an initial and target\ndistribution is a central problem in generative modeling. Existing approaches,\nsuch as flow matching and Schr\\\"odinger Bridge Matching, effectively learn\nmappings between two distributions by modeling a single stochastic path.\nHowever, these methods are inherently limited to unimodal transitions and\ncannot capture branched or divergent evolution from a common origin to multiple\ndistinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge\nMatching (BranchSBM), a novel framework that learns branched Schr\\\"odinger\nbridges. BranchSBM parameterizes multiple time-dependent velocity fields and\ngrowth processes, enabling the representation of population-level divergence\ninto multiple terminal distributions. We show that BranchSBM is not only more\nexpressive but also essential for tasks involving multi-path surface\nnavigation, modeling cell fate bifurcations from homogeneous progenitor states,\nand simulating diverging cellular responses to perturbations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09007.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cd5b3f0494187a9e8b7c69",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
      "fullname": "Pranam Chatterjee",
      "name": "pranamanam",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]