[
  {
    "paper": {
      "id": "2502.13923",
      "authors": [
        {
          "_id": "67b6b0688b56622e70b9e83e",
          "name": "Shuai Bai",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e83f",
          "name": "Keqin Chen",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e840",
          "name": "Xuejing Liu",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e841",
          "name": "Jialin Wang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e842",
          "name": "Wenbin Ge",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e843",
          "name": "Sibo Song",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e844",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e845",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e846",
          "name": "Shijie Wang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e847",
          "name": "Jun Tang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e848",
          "name": "Humen Zhong",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e849",
          "name": "Yuanzhi Zhu",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e84a",
          "user": {
            "_id": "6417fa211f1f3b0fa811edc0",
            "avatarUrl": "/avatars/fa9e1ef1472a736c2ceebe12b77d6c89.svg",
            "isPro": false,
            "fullname": "Mingkun Yang",
            "user": "ayumiymk",
            "type": "user"
          },
          "name": "Mingkun Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-20T09:35:44.878Z",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e84b",
          "name": "Zhaohai Li",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e84c",
          "name": "Jianqiang Wan",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e84d",
          "name": "Pengfei Wang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e84e",
          "name": "Wei Ding",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e84f",
          "name": "Zheren Fu",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e850",
          "name": "Yiheng Xu",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e851",
          "name": "Jiabo Ye",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e852",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e853",
          "name": "Tianbao Xie",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e854",
          "name": "Zesen Cheng",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e855",
          "name": "Hang Zhang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e856",
          "name": "Zhibo Yang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e857",
          "user": {
            "_id": "645b10e80c73ea27d13f7aca",
            "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
            "isPro": false,
            "fullname": "xuhaiyang",
            "user": "xhyandwyy",
            "type": "user"
          },
          "name": "Haiyang Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-20T09:35:42.372Z",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e858",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T18:00:14.000Z",
      "title": "Qwen2.5-VL 기술보고서\n\nQwen2.5-VL 기술보고서는 Qwen2.5-VL 모델의 기술적 측면을 자세히 설명하고, 이 모델의 성능, 특징, 그리고 개발 배경을 소개합니다. 이 보고서는 AI 분야의 전문가 및 연구자들에게 도움이 될 수 있으며, 최신 기술 동향과 발전 방향을 이해하기 위해 필수적입니다.",
      "summary": "Qwen2.5-VL는 최신의 플래그스hip 모델이며, 시각 언어 시리즈의 리더로 기본적인 능력과 혁신적인 기능에 있어서 큰 진전을 보이고 있습니다. Qwen2.5-VL는 확장된 시각 인식, 정확한 물체 위치 지정, 강력한 문서 분석, 긴 영상의 이해에 있어서 큰 진전을 이루고 있습니다. Qwen2.5-VL의 특징적인 기능으로 물체의 위치 지정을 Bounding Box 또는 점으로 정확하게 수행할 수 있습니다. 또한, 회계 문서, 양식, 테이블에서 강력한 구조화된 데이터 추출, 그리고 차트, 다이어그램, 라우터의 상세한 분석을 수행할 수 있습니다. 복잡한 입력을 처리하기 위해 Qwen2.5-VL는 동적인 해상도 처리와 절대 시간 인코딩을 도입하여, 이미지의 크기가 다른 것나 긴 시간 비디오(최대는 수시간)을 초 단위의 이벤트 위치 지정으로 처리할 수 있습니다. 이로 인해 모델은 공간 스케일과 시간 역학을 원生地적으로 이해할 수 있으며, 전통적인 정규화 방법과 의존하지 않습니다. 원생적인 동적인 해상도의 Vision Transformer(ViT)을 처음 학습하고, 윈도우 어타션을 포함하여 계산 오버헤드를 줄이면서 원생적인 해상도를 유지할 수 있습니다. 결과적으로, Qwen2.5-VL은 정적 이미지와 문서의 이해에도, 컴퓨터나 휴대전화의 조작 등 현실 세계의 시나리오에서 명령어 처리, 도구 사용, 태스크 실행을 위한 상호작용적인 시각 에이전트로서 뛰어납니다. Qwen2.5-VL은 다양한 사용 사례를 다루기 위해 3가지 크기로 제공됩니다. 플래그스hip 모델로서, Qwen2.5-VL-72B는 GPT-4o나 Claude 3.5 Sonnet과 같은 최신 모델과 경쟁하며, 특히 문서와 다이어그램의 이해에 탁월합니다. 또한, Qwen2.5-VL은 Qwen2.5 LLM의 핵심 언어 능력을 유지하면서 강력한 언어 성능을 유지하고 있습니다.",
      "upvotes": 48,
      "discussionId": "67b6b0688b56622e70b9e875"
    },
    "publishedAt": "2025-02-19T23:35:06.194Z",
    "title": "Qwen2.5-VL Technical Report",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13923.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63451cf0a05b51f7ded25505",
      "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg",
      "fullname": "shuai bai",
      "name": "bluelike",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13144",
      "authors": [
        {
          "_id": "67b55c7fba22c1ddbb8d5746",
          "user": {
            "_id": "6536187bd34e9f02b9df1c3b",
            "avatarUrl": "/avatars/0b34d62868b93053b0a05062a018b5bd.svg",
            "isPro": false,
            "fullname": "Hao Gao",
            "user": "Hao605",
            "type": "user"
          },
          "name": "Hao Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:48.944Z",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d5747",
          "name": "Shaoyu Chen",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d5748",
          "name": "Bo Jiang",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d5749",
          "name": "Bencheng Liao",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d574a",
          "name": "Yiang Shi",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d574b",
          "name": "Xiaoyang Guo",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d574c",
          "name": "Yuechuan Pu",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d574d",
          "name": "Haoran Yin",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d574e",
          "name": "Xiangyu Li",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d574f",
          "name": "Xinbang Zhang",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d5750",
          "name": "Ying Zhang",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d5751",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d5752",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d5753",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:21.000Z",
      "title": "RAD: 3DGS 기반의 대형 강화학습을 활용한 출발지부터 도착지까지의 운전 정책의 훈련",
      "summary": "현재의 단말에서 단말까지의 자동주행(AD) 알고리즘은 일반적으로 임베디드 학습(IL) 패러다임에 따라 진행되고 있습니다. 이 방식은 원인 혼동과 폐회로 오류 등 여러 문제를 야기하고 있습니다. 본 연구에서는 3DGS를 기반으로 한 폐회로 리ン다스 트레이닝 패러다임을 구축합니다. 3DGS 기술의 활용을 통해 실제 물리 세계의 사진의 디지털 리펫을 구축하고, AD 정책이 상태 공간의 광범위한 영역을 탐색하며, 대규모의 시도 오류로 외란적인 시나리오를 처리할 수 있습니다. 안전성을 향상시키기 위해, 특수 보상을 설계하여 정책이 안전한 이벤트에 효과적으로 대응하고, 실제 세계의 원인 관계를 이해하도록 안내합니다. 인간 운전 행동의 매칭을 개선하기 위해, IL은 리ン다스 트레이닝의 정규화 항으로 통합됩니다. 다양한 이전에 본 적 없는 3DGS 환경으로 구성된 폐회로 평가 벤치마크를 통해, IL 기반의 방법과 비교하여 RAD는 많은 폐회로 메트릭에서 큰 성능을 발휘하며, 특히 3배 낮은 충돌률을 달성합니다. 자세한 폐회로 결과를 [https://hgao-cv.github.io/RAD](https://hgao-cv.github.io/RAD)에서 확인하실 수 있습니다.",
      "upvotes": 22,
      "discussionId": "67b55c80ba22c1ddbb8d579c"
    },
    "publishedAt": "2025-02-19T22:13:49.764Z",
    "title": "RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13144.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6536187bd34e9f02b9df1c3b",
      "avatarUrl": "/avatars/0b34d62868b93053b0a05062a018b5bd.svg",
      "fullname": "Hao Gao",
      "name": "Hao605",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13128",
      "authors": [
        {
          "_id": "67b6c696e9b901edeaf320d5",
          "name": "Zihan Liu",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320d6",
          "name": "Shuangrui Ding",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320d7",
          "name": "Zhixiong Zhang",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320d8",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320d9",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320da",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320db",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320dc",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320dd",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:52:21.000Z",
      "title": "SongGen: 텍스트에서 노래를 생성하기 위한 한 단계의 자동적인 추론적 채널 매핑",
      "summary": "歌曲生成는 텍스트 입력으로부터 보카럴과accompaniment를 생성하는 일이다. 이 분야는 복잡한 영역과 데이터 부족으로 큰 문제를 가지고 있다. 현재의 방법들은 주로 다단계 생성 프로세스를 사용하므로 복잡한 학습과 추론 프로세스가 발생합니다. 본 논문에서는 완전하게 오픈소스의 단일 스테이지의 자동 복원 채널 드라이빙 튜터링 머신을 제안합니다. 이 제안 모델은 노래 lyrics, 악기 구성, 장르, 분위기, 음색 등 다양한 음악적 속성을 미세하게 제어할 수 있습니다. 또한 보카럴 克隆을 위해 3초의 참조 클립을 선택할 수 있도록 설계되었습니다. 하나의 통합된 자동 복원 프레임워크 내에서, SongGen은 보카럴과accompaniment의 혼합 모드와 더블 트랙 모드를 지원합니다. 더블 트랙 모드에서는 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블 트랙 모드를 통해 더블",
      "upvotes": 20,
      "discussionId": "67b6c698e9b901edeaf321a7"
    },
    "publishedAt": "2025-02-20T01:07:44.785Z",
    "title": "SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13128.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13685",
      "authors": [
        {
          "_id": "67b6dc1ba7567156c6547880",
          "name": "Jusen Du",
          "hidden": false
        },
        {
          "_id": "67b6dc1ba7567156c6547881",
          "user": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "isPro": false,
            "fullname": "Weigao Sun",
            "user": "weigao266",
            "type": "user"
          },
          "name": "Weigao Sun",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-20T07:39:08.547Z",
          "hidden": false
        },
        {
          "_id": "67b6dc1ba7567156c6547882",
          "name": "Disen Lan",
          "hidden": false
        },
        {
          "_id": "67b6dc1ba7567156c6547883",
          "name": "Jiaxi Hu",
          "hidden": false
        },
        {
          "_id": "67b6dc1ba7567156c6547884",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T12:53:55.000Z",
      "title": "MoM: 혼돈의 메모리를 이용한 선형 시퀀스 모델링",
      "summary": "線形シーケンスモデリングメソッド、例えば線形アタション、ステートスペースモデリング、および線形RNN、訓練と推論の複雑性を減少することで、顕著な効率向上を提供します。しかし、これらのメソッドは通常、全体の入力シーケンスを1つの固定サイズのメモリ状態に圧縮し、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学び、これにより、記憶干渉を軽減するために脳の長期記憶を維持する能力を学",
      "upvotes": 16,
      "discussionId": "67b6dc1ca7567156c65478b8"
    },
    "publishedAt": "2025-02-20T02:40:09.567Z",
    "title": "MoM: Linear Sequence Modeling with Mixture-of-Memories",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13685.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13347",
      "authors": [
        {
          "_id": "67b6a7e83ef3656c48f149b9",
          "user": {
            "_id": "6135eeeb5bc6ecdf86b60f0d",
            "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
            "isPro": false,
            "fullname": "Shi Yu",
            "user": "yushi",
            "type": "user"
          },
          "name": "Shi Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-20T09:35:47.487Z",
          "hidden": false
        },
        {
          "_id": "67b6a7e83ef3656c48f149ba",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b6a7e83ef3656c48f149bb",
          "name": "Chenyan Xiong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T00:31:43.000Z",
      "title": "Craw4LLM: 효율적인 LLM 사전학습을 위한 웹 크롤링\n\n(Note: The original text \"Craw4LLM: LLM予習の効率的なWebクローリング\" contains a mix of English and Japanese characters. The translation provided above assumes the text is meant to be in Korean. If the original intent was to keep the text in English, the translation would be \"Craw4LLM: Efficient Web Crawling for LLM Pretraining\".)",
      "summary": "웹 크롤링은 대규모 언어 모델(LLMs)의 사전 훈련 데이터의 주요 자원입니다が、사전 훈련에서 낮은 데이터 품질의 많은 크롤링 된 웹 페이지는 버려집니다. 본 논문에서는 LLM의 사전 훈련의 취향에 따라 효율적인 웹 크롤링 방법인 Crawl4LLM을 소개합니다. 특히, LLM의 사전 훈련에서 웹 페이지의 영향을 크롤링 스케줄러의 우선 순위로 삼고, 표준 그래프의 연결성을 기반으로 한 우선 순위를 대체합니다. 9억 페이지의 웹 그래프를 대상으로 상업적인 검색 엔진의 인덱스로부터 실험을 통해 Crawl4LLM이 고품질의 사전 훈련 데이터를 효율적으로 얻을 수 있음을 보여주었습니다. 21%의 URL을 크롤링하면 Crawl4LLM 데이터로 사전 훈련된 LLM은 이전 크롤링과 같은 하위 성능을 달성하고, 크롤링의 에너지 소비를 크게 줄이고 웹 사이트의 부담을 완화했습니다. 코드는 https://github.com/cxcscmu/Crawl4LLM에서 공개되어 있습니다.",
      "upvotes": 16,
      "discussionId": "67b6a7e93ef3656c48f149f1"
    },
    "publishedAt": "2025-02-19T22:57:23.298Z",
    "title": "Craw4LLM: Efficient Web Crawling for LLM Pretraining",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13347.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6135eeeb5bc6ecdf86b60f0d",
      "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
      "fullname": "Shi Yu",
      "name": "yushi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13922",
      "authors": [
        {
          "_id": "67b6948dbef24bad725b5d4b",
          "name": "Guanzheng Chen",
          "hidden": false
        },
        {
          "_id": "67b6948dbef24bad725b5d4c",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "67b6948dbef24bad725b5d4d",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        },
        {
          "_id": "67b6948dbef24bad725b5d4e",
          "name": "Lidong Bing",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T17:59:03.000Z",
      "title": "LongPO: 대 언어 모델의 긴 문맥 자동 진화를 통해\n단 문에서 긴 문으로의 취향 최적화",
      "summary": "大型 언어 모델(LLMs)는 사전 학습과 어레이먼트에 의해 놀라운 능력을 보여주고 있습니다. 그러나 긴 문맥의 경우, 긴 문맥의 어레이먼트가 충분하지 않아, 높은 수준의 짧은 문맥 LLMs의 성능이 떨어질 것으로 보입니다. 긴 문맥의 어레이먼트는, 긴 문맥의 인간 어레이먼트의 비효율성과 짧은 문맥과 긴 문맥의 성능의 균형을 이루는 어려움으로 인해, 문제점이 남아 있습니다. 이러한 문제를 해결하기 위해, LongPO를 소개합니다. LongPO는 짧은 문맥 LLMs가 내부적으로 짧은 문맥의 능력을 전파시켜, 긴 문맥 태스크에서 뛰어난 성능을 보게 하는 것입니다. LongPO는, LLMs에서 자기 생성된 짧은 문맥으로부터 긴 문맥으로의 선호 데이터로 학습하여, 같은 명령으로 생성된 긴 문맥 입력과 그 압축된 짧은 문맥의 쌍의 응답을 포함합니다. 이 선호는, 짧은 문맥 어레이먼트 기간 동안 키운 LLMs의 능력과 가능성과 긴 문맥의 어레이먼트에서 떨어지는 것을 보여줍니다. 또한, LongPO는 긴 문맥 어레이먼트 기간 동안 짧은 문맥의 성능 저하를 완화하기 위해 짧은 문맥에서 긴 문맥으로의 KL 제약을 적용하고 있습니다. Mistral-7B-Instruct-v0.2를 128K에서 512K의 문맥 길이로 적용한 경우, LongPO는 짧은 문맥의 성능을 완전히 유지하면서, 긴 문맥 및 짧은 문맥의 태스크에서 나이프한 SFT와 DPO를 크게 초월합니다. 특히, LongPO에 의해 훈련된 모델은, 긴 문맥 벤치마크에서의 결과를, 긴 문맥의 광범위한 어레이먼트와 큰 파라미터 규모를 가진 높은 수준의 LLMs(예: GPT-4-128K)과 비교하거나 초월할 수 있습니다.",
      "upvotes": 16,
      "discussionId": "67b6948ebef24bad725b5d84"
    },
    "publishedAt": "2025-02-19T21:35:20.931Z",
    "title": "LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13922.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645475e2548f22be59847604",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645475e2548f22be59847604/EhSurrZ25u31qQ2TVXQXt.jpeg",
      "fullname": "Chen",
      "name": "Guanzheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12143",
      "authors": [
        {
          "_id": "67b4d05a9f8a8ab661450397",
          "name": "Yuetai Li",
          "hidden": false
        },
        {
          "_id": "67b4d05a9f8a8ab661450398",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "67b4d05a9f8a8ab661450399",
          "user": {
            "_id": "653df1323479e9ebbe3eb6cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
            "isPro": true,
            "fullname": "Zhangchen Xu",
            "user": "flydust",
            "type": "user"
          },
          "name": "Zhangchen Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-20T09:37:32.715Z",
          "hidden": false
        },
        {
          "_id": "67b4d05a9f8a8ab66145039a",
          "name": "Fengqing Jiang",
          "hidden": false
        },
        {
          "_id": "67b4d05a9f8a8ab66145039b",
          "name": "Luyao Niu",
          "hidden": false
        },
        {
          "_id": "67b4d05a9f8a8ab66145039c",
          "name": "Bill Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "67b4d05a9f8a8ab66145039d",
          "name": "Bhaskar Ramasubramanian",
          "hidden": false
        },
        {
          "_id": "67b4d05a9f8a8ab66145039e",
          "name": "Radha Poovendran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T18:56:15.000Z",
      "title": "작은 모델은 강력한 논쟁자들에게서 학습하는 것이 어렵습니다.",
      "summary": "대 언어 모형（LLMs）는 복잡한 이유론 태스크에서 뛰어난 성능을 보입니다. 그 이유론 능력이 작은 모형에 흡수되는 실험적 성공이 있습니다. 그러나 우리는 흥미로운 현상을 발견했습니다. 이것을 \"작은 모형의 학습성 오류\"라고 부르며, 작은 모형（3B 파라미터 이하）은 긴 Chain-of-Thought（CoT） 이유론이나 큰 모형으로부터의 디스테일보다 짧은, 간단한 이유론 체인에서 더 나은 성능을 보입니다. 대신, 우리는 이 현상에 대처하기 위해 \"Mix Distillation\"이라는 간단하고 효과적인 전략을 제안합니다. 이는 긴 및 짧은 CoT 예시 또는 큰 및 작은 모형으로부터의 이유론을 조합하여 이유론의 복잡성을 균형을 맞추는 것입니다. 우리의 실험은 Mix Distillation은 학습된 데이터에만 의존한 작은 모형의 이유론 성능을 크게 향상시키는 것을 보여줍니다. 이러한 발견은 직접적으로 강력한 모형 디스테일의 한계를 밝혀, 이유론의 복잡성의 적절한 조정이 유효한 이유론 능력의 이동에 중요하다는 것을 강조합니다.",
      "upvotes": 12,
      "discussionId": "67b4d05b9f8a8ab6614503cb"
    },
    "publishedAt": "2025-02-19T21:38:13.468Z",
    "title": "Small Models Struggle to Learn from Strong Reasoners",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "flydust",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13965",
      "authors": [
        {
          "_id": "67b6a3fa09841367596a1db5",
          "name": "Michael Luo",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1db6",
          "name": "Xiaoxiang Shi",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1db7",
          "name": "Colin Cai",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1db8",
          "name": "Tianjun Zhang",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1db9",
          "name": "Justin Wong",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1dba",
          "user": {
            "_id": "626e3449e7914f0d5ea78ad1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626e3449e7914f0d5ea78ad1/pVzdmdPMpNcxuj94qiIvB.jpeg",
            "isPro": false,
            "fullname": "Yichuan",
            "user": "Chrisyichuan",
            "type": "user"
          },
          "name": "Yichuan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-20T09:35:50.487Z",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1dbb",
          "name": "Chi Wang",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1dbc",
          "name": "Yanping Huang",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1dbd",
          "name": "Zhifeng Chen",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1dbe",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1dbf",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T18:59:30.000Z",
      "title": "오터리克斯：LLM 에이전트의 효율적인 서비스 엔진（일반적인 프로그램）",
      "summary": "대 언어 모델(LLM) 애플리케이션은 단순한 챗봇을 넘어 동적인 일반적인 용도에 대한 에이전트 프로그래밍으로 발전하고 있습니다. 이러한 프로그램은 LLM의 호출과 출력 토큰을 스케일링하여, AI 에이전트가 복잡한 작업을 합리적으로 탐색하고 해결할 수 있도록 도움을 줍니다. 그러나 현재의 LLM 서비스 시스템은 프로그램과 호출 사이의 의존관계를 무시하고, 중요한 최적화 기회를 잃습니다. 우리 분석에 따르면, LLM 서비스 엔진에 제출되는 프로그램은 주로 개별 LLM의 요청과 프로그램의 헤드 브로킹에 의해 긴 누적 대기 시간을 받습니다. 이에 대처하여, 우리는 LLM 서비스 시스템을 통해 프로그램을 일급 시민으로 취급하는 \"Autellix\"를 소개합니다. Autellix는 프로그램으로부터 LLM의 호출을 삽입하고, 스케줄러에 프로그램 수준의 кон텍스트를 부여합니다. 우리는 단일 스레드 및 분산 프로그램에 대해, 프로그램의 이전 호출 완료에 기반한 LLM 호출을 우선순위 부여하고 예비 처리하는 두 가지 스케줄링 알고리즘을 제안합니다. 우리 평가에 따르면, 다양한 LLM과 에이전트 워크로드에서 Autellix는 동일한 경과 시간 내에, 가장 선진한 시스템(예: vLLM)과 비교하여 프로그램의 트랜잭션을 4~15배 개선합니다.",
      "upvotes": 11,
      "discussionId": "67b6a3fb09841367596a1e06"
    },
    "publishedAt": "2025-02-19T22:42:06.502Z",
    "title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13965.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654037be97949fd2304aab7f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654037be97949fd2304aab7f/2cSME81gcwYa2OTeVlq5Q.jpeg",
      "fullname": "Michael Luo",
      "name": "michaelzhiluo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13946",
      "authors": [
        {
          "_id": "67b6b416b4ad845374143c31",
          "name": "Chak Tou Leong",
          "hidden": false
        },
        {
          "_id": "67b6b416b4ad845374143c32",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "67b6b416b4ad845374143c33",
          "name": "Jian Wang",
          "hidden": false
        },
        {
          "_id": "67b6b416b4ad845374143c34",
          "name": "Wenjie Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T18:42:45.000Z",
      "title": "안전 보호 된 선박이 침몰하지 않는 이유는 무엇일까? 어레이 된 대규모 언어 모델의 안전 구조는 거의 고정되어 있는 템플릿 영역에 있습니다.",
      "summary": "대 언어 모델(LLMs)의 안전성 맞춤은 약한 상태에 있으며, 초기 행동은 복잡한 공격에도 쉽게 파괴될 수 있습니다. 기존의 LLMs에서, 입력 지시와 첫 번째 모델 출력 사이에 고정된 템플릿을 포함하는 것이 일반적인 실습으로, 이 템플릿이 취약한 키로 작용하는 것을 가정합니다: LLMs의 안전성 관련 설계는 템플릿 영역에서 통합된 정보에 과도하게 의존하고, 이는 이러한 모델의 안전한 행동에 큰 영향을 미칩니다. 이 문제를 ' 템플릿 어노테이션 안전성 맞춤'이라고 불립니다. 이 논문에서는 확장된 실험을 수행하여, 많은 LLMs에서 템플릿 어노테이션 안전성 맞춤이 존재하는 것을 증명합니다. 구조적 분석에 따라, 이 문제는 추론 시 파괴적 공격에 직면할 때 모델의 취약성을招く 방식으로 영향을 미칩니다. 또한, 안전 기능과 템플릿 영역을 분리하는 것이 파괴적 공격의 취약성을 억제하는 것이 바람직하다고 나타냅니다. 향후 연구에서, 템플릿 영역에 의존하는 것을 줄일 수 있는 방법들을 통해 더 강력한 안전성 맞춤 기술의 개발을 촉진합니다.",
      "upvotes": 7,
      "discussionId": "67b6b416b4ad845374143c5b"
    },
    "publishedAt": "2025-02-19T23:54:57.669Z",
    "title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13946.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631326d6289cf15634c52369",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631326d6289cf15634c52369/lmPWGHLsQ36H556cqcXjT.jpeg",
      "fullname": "Cooper Leong",
      "name": "cooperleong00",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13173",
      "authors": [
        {
          "_id": "67b6b014f7e569081326494f",
          "name": "Wang Yang",
          "hidden": false
        },
        {
          "_id": "67b6b014f7e5690813264950",
          "name": "Hongye Jin",
          "hidden": false
        },
        {
          "_id": "67b6b014f7e5690813264951",
          "name": "Jingfeng Yang",
          "hidden": false
        },
        {
          "_id": "67b6b014f7e5690813264952",
          "name": "Vipin Chaudhary",
          "hidden": false
        },
        {
          "_id": "67b6b014f7e5690813264953",
          "name": "Xiaotian Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T19:56:21.000Z",
      "title": "Thinking Preference Optimization\n\n只需返回翻译结果，不要添加任何解释或额外的文本。",
      "summary": "Supervised Fine-Tuning (SFT)은 큰 LLM에서 얻은 긴 Chain-of-Thought (CoT) 응답을 사용하여 상대적으로 작은 LLM의 긴 CoT 논리 능력을 향상시키는 효과적인 방법입니다. 이 방법은 SFT를 통해 보완되어 사용됩니다. 긴 CoT 논리 능력의 지속적인 향상을 위해 새로운 고품질의 긴 CoT 논리의 SFT 데이터를 수집하거나 기존의 SFT 데이터 세트를 재훈련하는 것이 고려됩니다. 그러나 새로운 긴 CoT의 SFT 데이터를 얻는 것은 비용과 한계가 있으며, 반복적인 훈련은 성능 패턴이나 감소를招きます. SFT 데이터의 성능을 더 높일 수 있는 방법으로 Thinking Preference Optimization (ThinkPO)를 제안합니다. ThinkPO는 새로운 긴 CoT 응답이 필요하지 않고, 짧은 CoT 논리 응답을 버린 답변과 같이 같은 질문의 긴 CoT 응답을 선택하여 사용합니다. 또한 직접적인 취향 최적화를 적용하여 모델이 긴 논리 출력을 선호하도록 격려합니다. 실험은 ThinkPO가 SFT된 모델의 논리 성능을 더욱 향상시키고, 예를 들어 수학 논리의 정답률을 8.6%로 높였으며, 출력 길이를 25.9%로 증가시켰습니다. 특히, ThinkPO는 공개된 SFT 모델의 성능을 지속적으로 향상시킬 수 있으며, 예를 들어 DeepSeek-R1-Distill-Qwen-7B의 MATH500 성능을 87.4%에서 91.2%로 높일 수 있습니다.",
      "upvotes": 7,
      "discussionId": "67b6b015f7e56908132649a0"
    },
    "publishedAt": "2025-02-19T23:31:36.410Z",
    "title": "Thinking Preference Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13173.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6149
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13962",
      "authors": [
        {
          "_id": "67b691751f861500916ecd5d",
          "user": {
            "_id": "6372bc95c4267fd7cd77f4d0",
            "avatarUrl": "/avatars/17a24af68f45487e601687d777b352b6.svg",
            "isPro": false,
            "fullname": "William Jurayj",
            "user": "wjurayj",
            "type": "user"
          },
          "name": "William Jurayj",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-20T09:36:09.674Z",
          "hidden": false
        },
        {
          "_id": "67b691751f861500916ecd5e",
          "name": "Jeffrey Cheng",
          "hidden": false
        },
        {
          "_id": "67b691751f861500916ecd5f",
          "name": "Benjamin Van Durme",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T18:58:31.000Z",
      "title": "이것은 최종적인 답변입니까? 시간 스케줄링은 선택 문제의 답을 개선합니다.",
      "summary": "스칼라이닝 된 테스트 시간에의 계산량을 증가시킨 대규모 언어 모델은 논리론 벤치마크에서 놀라운 성능을 보여주었습니다. 그러나 현재의 테스트 시간에 스칼라이닝의 평가는 논리론 시스템이 제공된 질문에 답할 필요가 있다고 강제적으로 가정하고 있습니다. 이는 모델이 답에 자신감이 있는지와 항상 답변을 제공하는 것이 적절한지에 대한 우려를 피하지 않습니다. 이러한 우려를 대처하기 위해 논리론 시에 Confidence Score를 추출하여 모델의 답변을 평가합니다. 이로써 추론 시간에의 계산 부하를 증가시키면 모델이 올바르게 답할 수 있는 확률을 높일 수 있으며, 올바른 답변에 대한 Confidence를 높일 수 있습니다. 또한 현재 평가 시간에의 무リスク 답변의 패러다임을 확장하고, 답변 위험이 비-제로인 설정을 고려하여 평가를 보고하는 방법을 제안합니다.",
      "upvotes": 6,
      "discussionId": "67b691761f861500916ecd8e"
    },
    "publishedAt": "2025-02-19T23:34:43.424Z",
    "title": "Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13962.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6149
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13233",
      "authors": [
        {
          "_id": "67b689aeba514d2c2c969289",
          "user": {
            "_id": "64beb6b6140491ca9f803ebf",
            "avatarUrl": "/avatars/0daa2e813a13668b8b708cd8c12763d9.svg",
            "isPro": false,
            "fullname": "Yucheng SHi",
            "user": "YuchengShi",
            "type": "user"
          },
          "name": "Yucheng Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-20T09:36:18.925Z",
          "hidden": false
        },
        {
          "_id": "67b689aeba514d2c2c96928a",
          "name": "Tianze Yang",
          "hidden": false
        },
        {
          "_id": "67b689aeba514d2c2c96928b",
          "name": "Canyu Chen",
          "hidden": false
        },
        {
          "_id": "67b689aeba514d2c2c96928c",
          "name": "Quanzheng Li",
          "hidden": false
        },
        {
          "_id": "67b689aeba514d2c2c96928d",
          "name": "Tianming Liu",
          "hidden": false
        },
        {
          "_id": "67b689aeba514d2c2c96928e",
          "name": "Xiang Li",
          "hidden": false
        },
        {
          "_id": "67b689aeba514d2c2c96928f",
          "name": "Ninghao Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T19:12:15.000Z",
      "title": "SearchRAG: 검색엔진은 LLM 기반의 의료문의 답변에 도움을 줄 수 있을까요?",
      "summary": "대 언어 모형(LLMs)은 일반적인 분야에서 뛰어난 능력을 보여주지만, 전문적인 지식을 필요로 하는 태스크에서 때로는 어려움을 겪습니다. 일반적인 리터러시 바버 제네레이션(RAG) 방법들은 정적 지식베이스에서 외부 정보를 얻지만, 이들은 끊임없이 불완전하며, 정확한 의료 상담을 위해 필요한 세부적인 임상 정보를 부족할 수 있습니다. 본 논문에서는, RAG의 한계를 극복하기 위해, 실시간 검색 엔진을 활용한 새로운 프레임워크 \"SearchRAG\"를 제안합니다. 우리 방법들은 합성 캐쉬 생성을 통해 복잡한 의료 상담을 검색 엔진으로 처리할 수 있는 질문으로 변환하고, 확률 기반의 지식 선택을 통해 가장 관련성이 높은 정보를 필터링하여 LLM의 입력에 포함합니다. 실험 결과를 통해, 우리 방법들이 의료 상담의 답변의 정확도를 크게 향상시키고, 특히 세부적인 최신 정보를 필요로 하는 복잡한 문제를 효과적으로 처리하는 데 특히 효과적이라는 것을 보여주고 있습니다.",
      "upvotes": 6,
      "discussionId": "67b689aeba514d2c2c9692b9"
    },
    "publishedAt": "2025-02-19T22:27:22.403Z",
    "title": "SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13233.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64beb6b6140491ca9f803ebf",
      "avatarUrl": "/avatars/0daa2e813a13668b8b708cd8c12763d9.svg",
      "fullname": "Yucheng SHi",
      "name": "YuchengShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13943",
      "authors": [
        {
          "_id": "67b6a9a7c721bee91cac2888",
          "name": "Yuliang Liu",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac2889",
          "name": "Junjie Lu",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac288a",
          "name": "Zhaoling Chen",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac288b",
          "name": "Chaofeng Qu",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac288c",
          "name": "Jason Klein Liu",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac288d",
          "name": "Chonghan Liu",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac288e",
          "name": "Zefan Cai",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac288f",
          "name": "Yunhui Xia",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac2890",
          "name": "Li Zhao",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac2891",
          "name": "Jiang Bian",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac2892",
          "name": "Chuheng Zhang",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac2893",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac2894",
          "name": "Zhouhan Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T18:35:55.000Z",
      "title": "AdaptiveStep: 모델의 신뢰도를 통해 논리론의 단계를 자동으로 분할합니다.",
      "summary": "현재의 Process Reward Models (PRMs)의 훈련 접근 방식은 규칙 기반의 방법 사용하며, 답을 여러 가지 이유론의 단계로 분해하고 있습니다. 예를 들어, 예약된 占い符 태그를 사용하여 이유론의 단계의 길이를 고정 크기로 설정하는 경우가 있습니다. 이러한 접근 방식은 특정 단어가 문장의 실제 결정 지점을 표지하는 것을 잘못 인식하고 있습니다. 이에 대해, 우리는 AdaptiveStep를 제안합니다. AdaptiveStep는 모델이 다음 단어를 예측할 자신에 기반하여 이유론의 단계를 분할합니다. 이 분할 방식은 각 단계에서 더 많은 결정 정보 제공하여 하류 태스크의 효과를 향상시킵니다. 또한, 우리의 방법手工 注釈가 필요하지 않습니다. 수학적 이유론과 코드 생성의 태스크에서 AdaptiveStep를 사용한 PRMs의 훈련을 통해 그 효과를 실험으로 보여주었습니다. 실험 결과는 PRM이 가장 先端의 Best-of-N 성능을 달성하고, 토큰 수준의 값 가이드 디코딩을 사용한 greedy search 전략을 초월하고, 기존의 오픈 소스 PRMs와 비교하여 30% 이상의 구축 비용 절감이 가능한 것을 보여줍니다. 또한, PRM의 성능, 트랜지비티, 일반화 능력에 대한 상세한 분석과 사례 연구를 제공합니다.",
      "upvotes": 5,
      "discussionId": "67b6a9a8c721bee91cac28e7"
    },
    "publishedAt": "2025-02-19T23:07:01.367Z",
    "title": "AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13943.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6529f79e802e3d1a4f8ec662",
      "avatarUrl": "/avatars/d05320c370a6497d8792ef5acb563dd5.svg",
      "fullname": "Yuliang Liu",
      "name": "yuliang03181",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12638",
      "authors": [
        {
          "_id": "67b6acdb3a3df2f965e7af0b",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af0c",
          "name": "Yanchen Luo",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af0d",
          "name": "Han Huang",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af0e",
          "name": "Enzhi Zhang",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af0f",
          "name": "Sihang Li",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af10",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af11",
          "name": "Yaorui Shi",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af12",
          "user": {
            "_id": "65fca775fa59bdf4737b1a84",
            "avatarUrl": "/avatars/a161b510bde8f57e7686cbb0b4aa6a52.svg",
            "isPro": false,
            "fullname": "Xiang Wang",
            "user": "xiangwang1223",
            "type": "user"
          },
          "name": "Xiang Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-20T04:17:33.860Z",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af13",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af14",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T08:40:13.000Z",
      "title": "NExT-Mol: 3D 분자 생성을 위한 3D 분산과 1D 언어 모델링\n\n(Note: The translation is provided as requested, maintaining the professional and accurate tone of the original text.)",
      "summary": "3D 분자 생성은 약물발견과 재료설계에서 중요합니다. 기존의 시도는 3D 분산 모델의 장점을 중점적으로 다루지만, 1D SELFIES 기반의 언어 모델(LM)의 장점을 무시하고 있습니다. LM은 100% 유효한 분자를 생성할 수 있으며, 10억 규모의 1D 분자 데이터셋을 활용할 수 있습니다. 이러한 장점을 통합하여 3D 분자 생성에 적합한 NExT-Mol: 3D 분산 모델과 1D 언어 모델링의 결합을 제안합니다. NExT-Mol은 광범위하게 가능한 분자 LM을 사용하여 1D 분자 생성을 수행하고, 생성된 분자의 3D 구조를 예측합니다. LM의 모델 크기를 확장하고, 분산 모델의 구조를 개선하고, 1D에서 3D로의 트랜스핸스 헝닝을 적용하여 NExT-Mol의 성능을 향상시킵니다. 특히, 1D 분자 LM은 유효성을 보장하면서 분산적인 유사성에서 기준을 초월합니다. 또한, 3D 분산 모델은 구조 예측에서 선도적인 성능을 달성합니다. 1D 및 3D 모델링의 향상으로, NExT-Mol은 GEOM-DRUGS에서 새로운 3D 생성에서 3D FCD에서 26%의 상대적인 향상을 달성하고, QM9-2014에서 조건부 3D 생성에서 13%의 평균 상대적인 효과를 달성합니다. 우리 코드와 제공된 체크포인트는 https://github.com/acharkq/NExT-Mol에 공개되어 있습니다.",
      "upvotes": 3,
      "discussionId": "67b6acdd3a3df2f965e7af85"
    },
    "publishedAt": "2025-02-19T23:18:32.647Z",
    "title": "NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12638.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310a3cd531cc21f9e06de6a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a3cd531cc21f9e06de6a/aTGMx3O41lUARK9s3dAik.jpeg",
      "fullname": "Zhiyuan Liu",
      "name": "acharkq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13581",
      "authors": [
        {
          "_id": "67b6ee04412c9eccae5151f5",
          "user": {
            "_id": "64a62c2f500beb50968e5c9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wfL3ojJmXqyzGUmCblPf4.jpeg",
            "isPro": false,
            "fullname": "Yupeng Hou",
            "user": "hyp1231",
            "type": "user"
          },
          "name": "Yupeng Hou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-20T09:35:14.498Z",
          "hidden": false
        },
        {
          "_id": "67b6ee04412c9eccae5151f6",
          "name": "Jianmo Ni",
          "hidden": false
        },
        {
          "_id": "67b6ee04412c9eccae5151f7",
          "name": "Zhankui He",
          "hidden": false
        },
        {
          "_id": "67b6ee04412c9eccae5151f8",
          "name": "Noveen Sachdeva",
          "hidden": false
        },
        {
          "_id": "67b6ee04412c9eccae5151f9",
          "name": "Wang-Cheng Kang",
          "hidden": false
        },
        {
          "_id": "67b6ee04412c9eccae5151fa",
          "name": "Ed H. Chi",
          "hidden": false
        },
        {
          "_id": "67b6ee04412c9eccae5151fb",
          "name": "Julian McAuley",
          "hidden": false
        },
        {
          "_id": "67b6ee04412c9eccae5151fc",
          "name": "Derek Zhiyuan Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T09:45:29.000Z",
      "title": "ActionPiece: kontekst-based action sequence tokenization for generative recommendation",
      "summary": "생성 추천(GR)는 사용자의 행동을 분산 토큰 패턴으로 변환하여 자동적으로 예측되게 된 새로운 패러다임입니다. 그러나 현재의 GR 모델은 각 행동을 독립적으로 토큰화하고, 모든 시퀀스에서 같은 행동에 같은 고정 토큰을 할당하며, 문맥 관계에 대한 고려가 없습니다. 이 문맥 지식의 부족은 동일한 행동이 주변 문맥에 따라 다른 의미를 가지는 것을 고려하지 않기 때문에, 최적의 성능을 얻을 수 없게 되어 모델의 성능이 떨어질 가능성이 있습니다. 이러한 문제를 해결하기 위해, 우리는 ActionPiece를 제안합니다. ActionPiece에서 행동 시퀀스를 토큰화하는 데 문맥을 명시적으로 고려합니다. ActionPiece에서 각 행동은 아이템의 특성 벡터의 집합으로 표현되며, 이러한 특성 벡터는 초기 토큰으로 사용됩니다. 행동 시퀀스 코RP 데이터로부터 특성 벡터의 패턴을 새로운 토큰으로 결합하고, 그 공존 빈도를 고려하여 사전을 구축합니다. 특성 벡터의 집합의 순서가 없기 때문에, 집합의 교환 정규화를 도입하여 동일한 의미를 가진 행동 시퀀스의 여러 분할을 생성합니다. 공개 데이터셋에서의 실험 결과를 통해 ActionPiece는 기존의 행동 토큰화 방법과 일치하거나, NDCG@10의 값을 6.00% ~ 12.82% 정도 높였습니다.",
      "upvotes": 2,
      "discussionId": "67b6ee04412c9eccae515223"
    },
    "publishedAt": "2025-02-20T03:56:54.121Z",
    "title": "ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13581.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a62c2f500beb50968e5c9c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wfL3ojJmXqyzGUmCblPf4.jpeg",
      "fullname": "Yupeng Hou",
      "name": "hyp1231",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11995",
      "authors": [
        {
          "_id": "67b65bbe0d878eff1a6b111d",
          "name": "Siddhesh Pawar",
          "hidden": false
        },
        {
          "_id": "67b65bbe0d878eff1a6b111e",
          "name": "Arnav Arora",
          "hidden": false
        },
        {
          "_id": "67b65bbe0d878eff1a6b111f",
          "name": "Lucie-Aimée Kaffee",
          "hidden": false
        },
        {
          "_id": "67b65bbe0d878eff1a6b1120",
          "user": {
            "_id": "608918b7df398c3b285ce960",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1621507769190-608918b7df398c3b285ce960.jpeg",
            "isPro": false,
            "fullname": "Isabelle Augenstein",
            "user": "IAugenstein",
            "type": "user"
          },
          "name": "Isabelle Augenstein",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-20T09:36:32.278Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T16:35:15.000Z",
      "title": "추정된 문화 인식: 이름이 LLM의 응답을 어떻게 영향을 미치는가",
      "summary": "이름은 깊은 인간식별과 연결되어 있습니다. 이들은 개인의 특성, 문화적 재산, 개인적인 역사의 표지자로서 역할을 합니다. 그러나 이름이 식별의 중심적인 지표로 사용되면 복잡한 식별을 단순화하는 데 연결됩니다. LLM과의 상호작용에서, 사용자의 이름은 개인화의 중요한 정보원으로 중요합니다. 이름은 사용자의 직접적인 입력(챗봇이 요구한 것)에 의해 챗봇 대화에 들어가, 작업 컨텍스트의 일부(CV의 검토 등) 또는 사용자 정보를 개인화하기 위해 저장하는 내장 메모리 기능을 통해 이름이 들어가습니다. 우리는 이름에 관련된 편향을 조사하고, LLM이 일반적인 제안을 찾는 질문에 대한 생성된 답변에서 문화의 예측을 측정하고 있습니다. 이러한 분석은 여러 문화에서 LLM 생성에서 이름에 관련된 문화식별의 강력한 예측을 나타냅니다. 우리의 연구는 편향을 피하면서 의미 있는 개인화를 유지하기 위해 더 복잡한 프로세스 시스템의 설계에 영향을 미칩니다.",
      "upvotes": 1,
      "discussionId": "67b65bbf0d878eff1a6b1174"
    },
    "publishedAt": "2025-02-20T01:20:46.431Z",
    "title": "Presumed Cultural Identity: How Names Shape LLM Responses",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11995.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60c50f18754747f54fa37114",
      "avatarUrl": "/avatars/648ae58b81806dbd93a68546666047e3.svg",
      "fullname": "Siddhesh",
      "name": "sidicity",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.11573",
      "authors": [
        {
          "_id": "67b6f629d9da6999328e38f5",
          "name": "Congkai Xie",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e38f6",
          "name": "Shuo Cai",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e38f7",
          "name": "Wenjun Wang",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e38f8",
          "name": "Pengxiang Li",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e38f9",
          "name": "Zhijie Sang",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e38fa",
          "name": "Kejing Yang",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e38fb",
          "name": "Yiming Zhang",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e38fc",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e38fd",
          "name": "Guanghao Zhu",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e38fe",
          "name": "Zeyu Liu",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e38ff",
          "name": "Yang Yu",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e3900",
          "name": "Yuhang Liu",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e3901",
          "name": "Su Lu",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e3902",
          "name": "Baoyi He",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e3903",
          "name": "Qi Zhou",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e3904",
          "name": "Xiaotian Han",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e3905",
          "name": "Jianbo Yuan",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e3906",
          "name": "Shengyu Zhang",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e3907",
          "name": "Fei Wu",
          "hidden": false
        },
        {
          "_id": "67b6f629d9da6999328e3908",
          "name": "Hongxia Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T09:07:32.000Z",
      "title": "InfiR : 이유에 따른 효과적인 작은 언어 모델과 다 모델 작은 언어 모델의 제작 방법",
      "summary": "Large Language Models (LLMs)와 Multimodal Large Language Models (MLLMs)는 인공 지능의 발전을 위해 큰 진전을 이루고 있습니다. 그러나 이러한 모델들은 높은 계산 요구와 프라이버시 문제 등 여러 문제점을 보이고 있습니다. 본 논문에서는 강력한 인공 지능을 유지하기 위해 효율적인 Small Language Models (SLMs)와 Multimodal Small Language Models (MSLMs)의 개발에 초점을 맞추고 있습니다. 우리는 인공 지능을 향상시키기 위한 새로운 훈련 프로세스를 소개하고, 가장 先端한 성능을 달성하면서 개발 비용을 최소화하는 것을 목표로하고 있습니다. InfR는 인공 지능 시스템의 발전을 촉진하기 위해 인공 지능의 향상, 채용 장애의 감소, 프라이버시 문제의 해결을 위해 작은 모델 크기를 통해 목표를 설정하고 있습니다. 자원은 https://github.com/Reallm-Labs/InfiR에서 이용 가능합니다.",
      "upvotes": 0,
      "discussionId": "67b6f62ad9da6999328e3955"
    },
    "publishedAt": "2025-02-20T04:32:22.011Z",
    "title": "InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11573.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "618c1ad1c74578e0a4a4d074",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618c1ad1c74578e0a4a4d074/8u_AkeHt4d6xtQ8hzaffU.jpeg",
      "fullname": "Drishti Sharma",
      "name": "DrishtiSharma",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 60
    },
    "isAuthorParticipating": false
  }
]