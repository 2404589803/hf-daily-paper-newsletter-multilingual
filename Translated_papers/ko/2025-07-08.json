[
  {
    "paper": {
      "id": "2507.03724",
      "authors": [
        {
          "_id": "686c7266364e2ad167eb5319",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531a",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531b",
          "name": "Chenyang Xi",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531c",
          "name": "Hanyu Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531d",
          "name": "Chen Tang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531e",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531f",
          "name": "Ding Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5320",
          "name": "Jiawei Yang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5321",
          "name": "Chunyu Li",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5322",
          "name": "Qingchen Yu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5323",
          "name": "Jihao Zhao",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5324",
          "name": "Yezhaohui Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5325",
          "name": "Peng Liu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5326",
          "name": "Zehao Lin",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5327",
          "name": "Pengyuan Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5328",
          "name": "Jiahao Huo",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5329",
          "name": "Tianyi Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532a",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532b",
          "name": "Kehang Li",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532c",
          "name": "Zhen Tao",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532d",
          "name": "Junpeng Ren",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532e",
          "name": "Huayi Lai",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532f",
          "name": "Hao Wu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5330",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5331",
          "name": "Zhenren Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5332",
          "name": "Zhaoxin Fan",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5333",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5334",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5335",
          "name": "Junchi Yan",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5336",
          "name": "Mingchuan Yang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5337",
          "name": "Tong Xu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5338",
          "name": "Wei Xu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5339",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533a",
          "name": "Haofeng Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533b",
          "name": "Hongkang Yang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533c",
          "user": {
            "_id": "686c965f418acea658859af4",
            "avatarUrl": "/avatars/4f453abeb7e82a3042cbec751b5cdb63.svg",
            "isPro": false,
            "fullname": "Wentao Zhang",
            "user": "Wentao-PKU",
            "type": "user"
          },
          "name": "Wentao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:32.391Z",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533d",
          "name": "Zhi-Qin John Xu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533e",
          "name": "Siheng Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533f",
          "name": "Feiyu Xiong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/twRhDA4ThpQMfInSFdYDA.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/9xHrSG0a8X2CYHxvfMFSH.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/aaUeE4u2RhCnFYNKCqvyn.gif"
      ],
      "publishedAt": "2025-07-04T17:21:46.000Z",
      "submittedOnDailyAt": "2025-07-08T01:46:43.501Z",
      "title": "MemOS: AI 시스템에 대한 메모리 오시야스",
      "submittedOnDailyBy": {
        "_id": "669e0b93c7cb0568dac6e92e",
        "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
        "isPro": false,
        "fullname": "hanyu Wang",
        "user": "UglyToilet",
        "type": "user"
      },
      "summary": "대규모 언어 모델(LLMs)는 인공지능(AGI)의 중요한 인프라로 자리잡고 있으며, 이들의 메모리 관리 시스템의 정의 부족은 긴 문맥 논리, 연속적인 개인화, 지식의 일관성 개발에 부정적인 영향을 미칩니다. 현재의 모델은 고정된 파라미터와 짧은 시간적인 컨텍스트 상태에 의존하며, 사용자의 취향을 추적하거나 장기간 지식 업데이트를 수행할 능력이 제한되어 있습니다. 레비ューアウガーデンション(RAG)는 외부 지식을 텍스트로 추가하지만, 이는 상태 없는 가벼운 해결책이며, 생명주기 제어 또는 지속 가능한 표현의 통합에 부족합니다. 최근의 연구는 LLMs의 훈련과 추론 비용을 메모리 계층의 관점에서 모델링하고, 파라미터 메모리와 외부 리뷰 사이에 명확한 메모리 계층을 추가함으로써 특정 지식의 표현으로 이러한 비용을 크게 감소시킬 수 있음을 보여주고 있습니다. 계산 효율보다도, LLMs는 시간과 컨텍스트에 따라 발생하는 광범위한 문제를 찾아볼 수 있습니다. 이러한 문제를 해결하기 위해, 우리는 MemOS(메모리 운영 시스템)를 제안하고 있습니다. MemOS는 메모리를 관리할 수 있는 시스템 리소스로 취급하며, 텍스트, 활성화 기반, 파라미터 수준의 메모리 표현, 스케줄링, 진화를 통합하여 비용 효율적인 저장과 검색을 가능하게 합니다. 기본 단위로, MemCube는 메모리 내용과 원 데이터, 버전 정보 등 메타 데이터를 모은 것입니다. MemCube는 시간에 따라 조립, 이동, 융합할 수 있으며, 메모리 타입의 유연한 전환을 가능하게 하며, 파라미터 기반의 학습과 검색을 결합합니다. MemOS는 메모리 중심의 시스템 프레임워크를 구축하고, LLMs에 제어 가능성, 유연성, 진화 가능성을 제공하며, 지속적인 학습과 개인화 모델링의 기초를 구축하는 것을 목표로 합니다.",
      "upvotes": 65,
      "discussionId": "686c7266364e2ad167eb5340",
      "projectPage": "https://memos.openmem.net/",
      "githubRepo": "https://github.com/MemTensor/MemOS",
      "ai_summary": "MemOS is proposed as a memory operating system for Large Language Models to enhance memory management, enabling efficient storage and retrieval, and facilitating continual learning and personalized modeling.",
      "ai_keywords": [
        "LLMs",
        "Artificial General Intelligence",
        "AGI",
        "Retrieval-Augmented Generation",
        "RAG",
        "MemOS",
        "memory operating system",
        "MemCube",
        "activation-based memory"
      ],
      "githubStars": 527
    },
    "publishedAt": "2025-07-04T13:21:46.000Z",
    "title": "MemOS: A Memory OS for AI System",
    "summary": "Large Language Models (LLMs) have become an essential infrastructure for\nArtificial General Intelligence (AGI), yet their lack of well-defined memory\nmanagement systems hinders the development of long-context reasoning, continual\npersonalization, and knowledge consistency.Existing models mainly rely on\nstatic parameters and short-lived contextual states, limiting their ability to\ntrack user preferences or update knowledge over extended periods.While\nRetrieval-Augmented Generation (RAG) introduces external knowledge in plain\ntext, it remains a stateless workaround without lifecycle control or\nintegration with persistent representations.Recent work has modeled the\ntraining and inference cost of LLMs from a memory hierarchy perspective,\nshowing that introducing an explicit memory layer between parameter memory and\nexternal retrieval can substantially reduce these costs by externalizing\nspecific knowledge. Beyond computational efficiency, LLMs face broader\nchallenges arising from how information is distributed over time and context,\nrequiring systems capable of managing heterogeneous knowledge spanning\ndifferent temporal scales and sources. To address this challenge, we propose\nMemOS, a memory operating system that treats memory as a manageable system\nresource. It unifies the representation, scheduling, and evolution of\nplaintext, activation-based, and parameter-level memories, enabling\ncost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates\nboth memory content and metadata such as provenance and versioning. MemCubes\ncan be composed, migrated, and fused over time, enabling flexible transitions\nbetween memory types and bridging retrieval with parameter-based learning.\nMemOS establishes a memory-centric system framework that brings\ncontrollability, plasticity, and evolvability to LLMs, laying the foundation\nfor continual learning and personalized modeling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/twRhDA4ThpQMfInSFdYDA.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/9xHrSG0a8X2CYHxvfMFSH.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/aaUeE4u2RhCnFYNKCqvyn.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03724.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669e0b93c7cb0568dac6e92e",
      "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
      "fullname": "hanyu Wang",
      "name": "UglyToilet",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05163",
      "authors": [
        {
          "_id": "686c928b364e2ad167eb53f1",
          "name": "Yutian Chen",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f2",
          "name": "Shi Guo",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f3",
          "name": "Tianshuo Yang",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f4",
          "name": "Lihe Ding",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f5",
          "name": "Xiuyuan Yu",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f6",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f7",
          "name": "Tianfan Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T16:18:35.000Z",
      "submittedOnDailyAt": "2025-07-08T02:14:44.087Z",
      "title": "4DSloMo: 4D 구성에 고속 시나리오를 지원하는 비동기 촬영",
      "submittedOnDailyBy": {
        "_id": "64970d3d9c3b29dca8633f87",
        "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
        "isPro": false,
        "fullname": "JunhaoZhuang",
        "user": "JunhaoZhuang",
        "type": "user"
      },
      "summary": "고속 4D 재구성은 고속 이동 분석과 실시간 4D 재구성에 중요합니다. 그러나 많은 4D 촬영 시스템은 30 FPS(초당 프레임 수) 이하로 제한되어 있으며, 낮은 FPS 입력으로부터 고속 이동을 직접 4D 재구성하는 것은 바람직하지 않은 결과를 초래할 가능성이 있습니다. 본 논문에서는 새로운 촬영 및 처리 모듈을 사용하여 낮은 FPS 카메라만을 사용하여 고속 4D 촬영 시스템에 대한 제안을 합니다. 촬영 측에서는 카메라의 시작 시간을 스탯에 맞추어 효과적인 프레임 레이트를 높이는 비동기 촬영 전략을 제안하고, 기본 25 FPS의 프레임 레이트를 확장하여 100-200 FPS의 등가 프레임 레이트를 달성합니다. 처리 측에서는 4D 희소 시각점 재구성으로 인한artifact을 수정하기 위해 새로운 생성 모델을 제안하고, 특히 4D 재구성을 위한 비디오 분화 기반의 artifact 수정 모델을 학습하여 결손을 수정하고 시간적 일관성을 유지하며 전체적인 재구성 품질을 향상시키기를 목표로 합니다. 실험 결과를 통해 이 방법은 동기 촬영에 비해 고속 4D 재구성을 크게 향상시키는 것을 보여줍니다.",
      "upvotes": 29,
      "discussionId": "686c928b364e2ad167eb53f8",
      "ai_summary": "A high-speed 4D capturing system using low FPS cameras with asynchronous capture and video-diffusion-based artifact correction enhances reconstruction quality.",
      "ai_keywords": [
        "asynchronous capture",
        "video-diffusion-based artifact-fix model",
        "sparse 4D reconstruction",
        "temporal consistency"
      ]
    },
    "publishedAt": "2025-07-07T12:18:35.000Z",
    "title": "4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous\n  Capture",
    "summary": "Reconstructing fast-dynamic scenes from multi-view videos is crucial for\nhigh-speed motion analysis and realistic 4D reconstruction. However, the\nmajority of 4D capture systems are limited to frame rates below 30 FPS (frames\nper second), and a direct 4D reconstruction of high-speed motion from low FPS\ninput may lead to undesirable results. In this work, we propose a high-speed 4D\ncapturing system only using low FPS cameras, through novel capturing and\nprocessing modules. On the capturing side, we propose an asynchronous capture\nscheme that increases the effective frame rate by staggering the start times of\ncameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our\nmethod achieves an equivalent frame rate of 100-200 FPS without requiring\nspecialized high-speed cameras. On processing side, we also propose a novel\ngenerative model to fix artifacts caused by 4D sparse-view reconstruction, as\nasynchrony reduces the number of viewpoints at each timestamp. Specifically, we\npropose to train a video-diffusion-based artifact-fix model for sparse 4D\nreconstruction, which refines missing details, maintains temporal consistency,\nand improves overall reconstruction quality. Experimental results demonstrate\nthat our method significantly enhances high-speed 4D reconstruction compared to\nsynchronous capture.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05163.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64970d3d9c3b29dca8633f87",
      "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
      "fullname": "JunhaoZhuang",
      "name": "JunhaoZhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04447",
      "authors": [
        {
          "_id": "686cab67364e2ad167eb5464",
          "name": "Wenyao Zhang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5465",
          "name": "Hongsi Liu",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5466",
          "user": {
            "_id": "63c3e8abc7d7f4c63a515a02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
            "isPro": false,
            "fullname": "Zekun Qi",
            "user": "qizekun",
            "type": "user"
          },
          "name": "Zekun Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:05:57.749Z",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5467",
          "name": "Yunnan Wang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5468",
          "name": "XinQiang Yu",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5469",
          "name": "Jiazhao Zhang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546a",
          "user": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "isPro": false,
            "fullname": "Runpei Dong",
            "user": "RunpeiDong",
            "type": "user"
          },
          "name": "Runpei Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:00.401Z",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546b",
          "name": "Jiawei He",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546c",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546d",
          "name": "Zhizheng Zhang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546e",
          "name": "Li Yi",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546f",
          "name": "Wenjun Zeng",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5470",
          "name": "Xin Jin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/Oe8hIH4_I_Pql9N72iHbn.mp4"
      ],
      "publishedAt": "2025-07-06T16:14:29.000Z",
      "submittedOnDailyAt": "2025-07-08T03:55:36.991Z",
      "title": "ドリームVLA: 세계의 지식들을 포괄적으로 이해하는 시각・언어・행동 모델",
      "submittedOnDailyBy": {
        "_id": "6201fc5d91d53938a6432fbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
        "isPro": false,
        "fullname": "Runpei Dong",
        "user": "RunpeiDong",
        "type": "user"
      },
      "summary": "최근의 시각 언어 액션(VLA) 모델의 발전은 이미지 생성과 액션 예측의 통합으로 기계인 조작의 일반화와 추론에 期待的な 성과가 나타났습니다. 그러나 현재의 방법들은冗長한 정보를 포함하는 어려운 이미지 기반의 예측에 제한되어 있으며, 동적인, 공간적인, 그리고 의미적인 정보를 포함하는 구조적인 세계 지식이 부족합니다. 이러한 제한을 해결하기 위해, 우리는 역동학 모델링을 가능하게 하기 위해, 구체적인 세계 지식 예측을 통합하는 새로운 VLA 프레임워크를 제안합니다. 특히, DreamVLA는 동적인 영역을 가이드하는 세계 지식 예측과 공간적, 의미적 결합을 포함하여, 액션 계획에서 압축적이고 세부적인 표현을 제공합니다. 이 설계는 인간이 세계와 상호작용하는 것처럼, 추상적인 다 타입의 이유의 연속을 형성하여 행동하는 방식으로 일치합니다. 훈련 중 동적인, 공간적인, 의미적인 정보를 간접하도록 하기 위해, 블록 구조를 가진 어텐션 구조를 사용하며, 서로의 어텐션을 마스크하고, 정보 손실을 방지하고, 각 표현을 깨끗하게 분리합니다. 또한 미래의 액션의 조건 분포를 모델링하기 위해, 확산 기반의 트랜스포머를 사용하며, 액션 표현을 공용적인 잠재적 특성으로부터 분리합니다. 리알워ル와 시뮬레이션 환경에서 확산 실험은 DreamVLA는 리알로보트 태스크에서 76.7%의 성공률, CALVIN ABC-D 벤치마크에서 4.44의 평균 길이를 달성한 것을 보여주고 있습니다.",
      "upvotes": 26,
      "discussionId": "686cab67364e2ad167eb5471",
      "projectPage": "https://zhangwenyao1.github.io/DreamVLA/",
      "githubRepo": "https://github.com/Zhangwenyao1/DreamVLA",
      "ai_summary": "DreamVLA improves robot manipulation through a VLA framework that incorporates world knowledge, dynamic-region guidance, and a diffusion-based transformer to ensure clear, disentangled representations for action planning.",
      "ai_keywords": [
        "vision-language-action",
        "dynamic-region-guided",
        "world knowledge prediction",
        "spatial and semantic cues",
        "block-wise structured attention",
        "diffusion-based transformer"
      ],
      "githubStars": 24
    },
    "publishedAt": "2025-07-06T12:14:29.000Z",
    "title": "DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive\n  World Knowledge",
    "summary": "Recent advances in vision-language-action (VLA) models have shown promise in\nintegrating image generation with action prediction to improve generalization\nand reasoning in robot manipulation. However, existing methods are limited to\nchallenging image-based forecasting, which suffers from redundant information\nand lacks comprehensive and critical world knowledge, including dynamic,\nspatial and semantic information. To address these limitations, we propose\nDreamVLA, a novel VLA framework that integrates comprehensive world knowledge\nforecasting to enable inverse dynamics modeling, thereby establishing a\nperception-prediction-action loop for manipulation tasks. Specifically,\nDreamVLA introduces a dynamic-region-guided world knowledge prediction,\nintegrated with the spatial and semantic cues, which provide compact yet\ncomprehensive representations for action planning. This design aligns with how\nhumans interact with the world by first forming abstract multimodal reasoning\nchains before acting. To mitigate interference among the dynamic, spatial and\nsemantic information during training, we adopt a block-wise structured\nattention mechanism that masks their mutual attention, preventing information\nleakage and keeping each representation clean and disentangled. Moreover, to\nmodel the conditional distribution over future actions, we employ a\ndiffusion-based transformer that disentangles action representations from\nshared latent features. Extensive experiments on both real-world and simulation\nenvironments demonstrate that DreamVLA achieves 76.7% success rate on real\nrobot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/Oe8hIH4_I_Pql9N72iHbn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04447.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6201fc5d91d53938a6432fbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
      "fullname": "Runpei Dong",
      "name": "RunpeiDong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05197",
      "authors": [
        {
          "_id": "686c7f78364e2ad167eb5354",
          "name": "Shihan Dou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5355",
          "name": "Shichun Liu",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5356",
          "user": {
            "_id": "655c6b1abfb531437a54c0e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/V8Md2mMX83hrSowKk6qMS.jpeg",
            "isPro": false,
            "fullname": "Yuming Yang",
            "user": "Umean",
            "type": "user"
          },
          "name": "Yuming Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:28.979Z",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5357",
          "name": "Yicheng Zou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5358",
          "name": "Yunhua Zhou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5359",
          "name": "Shuhao Xing",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535a",
          "name": "Chenhao Huang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535b",
          "name": "Qiming Ge",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535c",
          "name": "Demin Song",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535d",
          "name": "Haijun Lv",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535e",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535f",
          "name": "Chengqi Lv",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5360",
          "name": "Enyu Zhou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5361",
          "name": "Honglin Guo",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5362",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5363",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5364",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5365",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5366",
          "name": "Xipeng Qiu",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5367",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5368",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5369",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T16:56:31.000Z",
      "submittedOnDailyAt": "2025-07-08T02:18:45.573Z",
      "title": "주어진 정책 식별기는 일반적인 보상 모델입니다.",
      "submittedOnDailyBy": {
        "_id": "6234238485575ce6ff1f169a",
        "avatarUrl": "/avatars/e5fff05f21cdea4e5aebc8ba426fac29.svg",
        "isPro": false,
        "fullname": "Yicheng Zou",
        "user": "RowitZou",
        "type": "user"
      },
      "summary": "우리는 보상 모델링을 정책 판별자로 공식화하여 새로운 시각을 제시합니다. 이 방법은 두 정책 사이의 차이를 측정하여 보상 신호를 생성하여 훈련 정책이 원하는 행동을 가지는 목표 정책으로 안내합니다. 이러한 개념적인 통찰에 기반하여, 우리는 Scalable Pre-training Method인 Policy Discriminative Learning (POLAR)을 제안합니다. 이 방법은 보상 모델 (RM)을 훈련하여 동일한 정책과 다른 정책을 구분합니다. 전통적인 보상 모델링 방법과 달리, POLAR은 하나의 정책과 임의의 목표 정책 사이의 상대적 차이를 파악하여 확장 가능한, 고수준의 최적화 목표를 모델링합니다. POLAR의 사전 훈련 패러다임을 활용하여, 1.8B부터 7B까지의 파라미터 규모의 RM을 제공합니다. 경험적 결과를 통해 POLAR은 전통적인 사전 훈련되지 않은 방법보다 상당한 성능 향상을 보입니다. 예를 들어, STEM 과제에서는 SOTA 기준 대비 54.8%에서 81.0%, 창의적 쓰기 과제에서는 57.9%에서 85.5%의 선호 정확도를 향상시킵니다. POLAR은 RLHF (Reinforcement Learning from Human Feedback)에서 강화 학습 미세 조정 (RFT)을 사용하여 견고한 일반화 능력을 나타냅니다. 이는 신뢰할 수 있는 보상 신호를 제공하여 정책 성능을 크게 향상시킵니다. 예를 들어, LLaMa3.1-8B는 평균 47.36%에서 56.33%, Qwen2.5-32B는 64.49%에서 70.47%까지 향상됩니다. 또한, 확장 실험은 계산과 성능 사이의 명확한 지수 법칙 관계를 보여줍니다. 놀라운 성능, 강한 일반화, 확장 특성은 POLAR이 일반적인, 강력한 보상 모델 개발의 유망한 방향임을 시사합니다.",
      "upvotes": 24,
      "discussionId": "686c7f78364e2ad167eb536a",
      "githubRepo": "https://github.com/InternLM/POLAR",
      "ai_summary": "A scalable reward modeling method, Policy Discriminative Learning (POLAR), enhances reward model performance and generalizes robustly in reinforcement learning through policy comparison.",
      "ai_keywords": [
        "policy discriminator",
        "reward model",
        "Policy Discriminative Learning",
        "POLAR",
        "Reinforcement Fine-tuning"
      ],
      "githubStars": 35
    },
    "publishedAt": "2025-07-07T12:56:31.000Z",
    "title": "Pre-Trained Policy Discriminators are General Reward Models",
    "summary": "We offer a novel perspective on reward modeling by formulating it as a policy\ndiscriminator, which quantifies the difference between two policies to generate\na reward signal, guiding the training policy towards a target policy with\ndesired behaviors. Based on this conceptual insight, we propose a scalable\npre-training method named Policy Discriminative Learning (POLAR), which trains\na reward model (RM) to discern identical policies and discriminate different\nones. Unlike traditional reward modeling methods relying on absolute\npreferences, POLAR captures the relative difference between one policy and an\narbitrary target policy, which is a scalable, high-level optimization objective\nsuitable for modeling generic ranking relationships. Leveraging the POLAR\npre-training paradigm, we present a series of RMs with parameter scales from\n1.8B to 7B. Empirical results show that POLAR substantially outperforms\ntraditional non-pre-trained methods, significantly enhancing RM performance.\nFor instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on\nSTEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA\nbaselines. POLAR also shows robust generalization capabilities in RLHF using\nReinforcement Fine-tuning (RFT), providing reliable reward signals and markedly\nenhancing policy performance--improving LLaMa3.1-8B from an average of 47.36%\nto 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover,\nscaling experiments reveal a clear power-law relationship between computation\nand performance, supported by linear correlation coefficients approaching 0.99.\nThe impressive performance, strong generalization, and scaling properties\nsuggest that POLAR is a promising direction for developing general and strong\nreward models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05197.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6234238485575ce6ff1f169a",
      "avatarUrl": "/avatars/e5fff05f21cdea4e5aebc8ba426fac29.svg",
      "fullname": "Yicheng Zou",
      "name": "RowitZou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.00994",
      "authors": [
        {
          "_id": "6864e267d59a9eda59024bab",
          "user": {
            "_id": "65fa95405355a52c784633fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fa95405355a52c784633fc/rSfBUHPa7eSAsLd8DuOq4.png",
            "isPro": false,
            "fullname": "Hippolyte Gisserot-Boukhlef",
            "user": "hgissbkh",
            "type": "user"
          },
          "name": "Hippolyte Gisserot-Boukhlef",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:53:51.109Z",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bac",
          "user": {
            "_id": "62be186a5f59ff2320e6e32b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
            "isPro": false,
            "fullname": "Nicolas-BZRD",
            "user": "Nicolas-BZRD",
            "type": "user"
          },
          "name": "Nicolas Boizard",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T12:22:12.240Z",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bad",
          "name": "Manuel Faysse",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bae",
          "name": "Duarte M. Alves",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024baf",
          "name": "Emmanuel Malherbe",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bb0",
          "name": "André F. T. Martins",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bb1",
          "name": "Céline Hudelot",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bb2",
          "name": "Pierre Colombo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-01T17:45:48.000Z",
      "submittedOnDailyAt": "2025-07-08T07:51:12.578Z",
      "title": "그대로의 마스크 언어 모델링에서 인코더를 사전 훈련 시키나요?",
      "submittedOnDailyBy": {
        "_id": "62be186a5f59ff2320e6e32b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
        "isPro": false,
        "fullname": "Nicolas-BZRD",
        "user": "Nicolas-BZRD",
        "type": "user"
      },
      "summary": "학습 고품질의 텍스트 표현은 광범위한 NLP 태스크에 기초한 기초적인 중요성을 가지고 있습니다. 마스킹된 언어 모델링(MLM)을 기반으로 한 인코더의 사전 학습은 전통적으로 사용되었지만, 최근의 증거에서, 원인 관계 언어 모델링(CLM)으로 사전 학습된 디코더 모델이 인코더로 재활용될 수 있으며, 텍스트 표현의 벤치마크에서 전통적인 인코더의 성능을 초과하는 성능을 보여주는 것을 명확히 알 수 있게 되었습니다. 그러나 이러한 효과는 CLM의 고유한 장점이든, 모델이나 데이터 크기 등 혼잡 요인이든, 아직 명확하지 않습니다. 본 논문에서는 이러한 문제를 해결하기 위해, 큰 규모의 조정된 사전 학습을 제거 실험을 수행하고, 210만부터 10억 파라미터의 범위에서 30개의 모델을 훈련시키고, 15,000 이상의 미세 조정과 평가를 수행했습니다. 우리는 MLM을 훈련하는 것은 전체적인 텍스트 표현 태스크에서 일반적으로 더 좋은 성능을 보여주는 것을 보였지만, CLM으로 훈련된 모델은 데이터 효과적이며, 미세 조정의 안정성이 향상됩니다. 이러한 발견에 기반하여, 고정된 계산 훈련 바지엇을 통해 최적의 성능을 달성하는 두 단계 훈련 전략을 실험적으로 보여주었습니다. 또한, 이 전략은 기존의 LLM 생태계에서 읽기 가능한 사전 학습 디코더 모델을 초기화하고, 가장 최신의 인코더 모델의 훈련 부담을 줄이는 방식으로, 더욱 효율적으로 작동하는 것을 보여주었습니다. 모든 프로젝트artifact를 공개하여 연구를 촉진합니다.",
      "upvotes": 23,
      "discussionId": "6864e267d59a9eda59024bb3",
      "githubRepo": "https://github.com/Nicolas-BZRD/EuroBERT",
      "githubStars": 59
    },
    "publishedAt": "2025-07-01T13:45:48.000Z",
    "title": "Should We Still Pretrain Encoders with Masked Language Modeling?",
    "summary": "Learning high-quality text representations is fundamental to a wide range of\nNLP tasks. While encoder pretraining has traditionally relied on Masked\nLanguage Modeling (MLM), recent evidence suggests that decoder models\npretrained with Causal Language Modeling (CLM) can be effectively repurposed as\nencoders, often surpassing traditional encoders on text representation\nbenchmarks. However, it remains unclear whether these gains reflect an inherent\nadvantage of the CLM objective or arise from confounding factors such as model\nand data scale. In this paper, we address this question through a series of\nlarge-scale, carefully controlled pretraining ablations, training a total of 30\nmodels ranging from 210 million to 1 billion parameters, and conducting over\n15,000 fine-tuning and evaluation runs. We find that while training with MLM\ngenerally yields better performance across text representation tasks,\nCLM-trained models are more data-efficient and demonstrate improved fine-tuning\nstability. Building on these findings, we experimentally show that a biphasic\ntraining strategy that sequentially applies CLM and then MLM, achieves optimal\nperformance under a fixed computational training budget. Moreover, we\ndemonstrate that this strategy becomes more appealing when initializing from\nreadily available pretrained CLM models (from the existing LLM ecosystem),\nreducing the computational burden needed to train best-in-class encoder models.\nWe release all project artifacts at https://hf.co/MLMvsCLM to foster further\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00994.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "62be186a5f59ff2320e6e32b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
      "fullname": "Nicolas-BZRD",
      "name": "Nicolas-BZRD",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03483",
      "authors": [
        {
          "_id": "686c90d4364e2ad167eb53d8",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53d9",
          "name": "Guanyu Li",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53da",
          "name": "Yutao Fan",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53db",
          "name": "Honglin Guo",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53dc",
          "name": "Yufang Liu",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53dd",
          "name": "Xiaoran Fan",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53de",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53df",
          "name": "Jingchao Ding",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e0",
          "name": "Wangmeng Zuo",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e1",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e2",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e3",
          "name": "Tao Ji",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e4",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e5",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e6",
          "name": "Xuanjing Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T11:20:09.000Z",
      "submittedOnDailyAt": "2025-07-08T02:00:41.875Z",
      "title": "BMMR: 대규모의 바이리언 머티컬 다스키티의 이유\n데이터셋",
      "submittedOnDailyBy": {
        "_id": "638ef0b0c67af472d31674a6",
        "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg",
        "isPro": false,
        "fullname": "Honglin Guo",
        "user": "KYLN24",
        "type": "user"
      },
      "summary": "이 논문에서는 BMMR라는 큰 규모의 바이리니어, 다중 모델, 다학류통리 이론 데이터 세트를 소개합니다. 이 데이터 세트는 커뮤니티가 큰 규모의 다중 모델(LMMs)의 개발과 평가를 위해 사용될 수 있습니다. BMMR는 110,000개의 대학 수준의 질문으로 이루어져 있으며, 300개의 UNESCO 정의的主题을 확장하고, 다양한 형식(선택肢, 빈칸, 개방형 질문)으로 구성되어 있으며, 책, 시험, Quiz 등 인쇄 및 디지털 미디어에서 샘플링되었습니다. 모든 데이터는 인간이 참여한 scalable 프레임워크를 통해 칫솔링되어 있으며, 각 인스턴스에 고품질의 이유의 패스가 붙습니다. 데이터 세트는 BMMR-Eval과 BMMR-Train의 2개의 부문으로 나뉩니다. BMMR-Eval은 20,458개의 고품질의 인스턴스를 포함하며, 중국어와 영어로 다학류통리 이론의 지식과 이유를 평가할 수 있습니다. BMMR-Train은 현재의 수학의 이유에 초점을 맞추며, 다양한 학류와 디시카드롬에 대한 연구와 개발을 지원합니다. 또한, BMMR-Verifier라는 프로세스 기반의 다학류경험 평가 도구를 제안하며, 이유의 패스의 정확한 FINEGAIN 평가를 수행합니다. 24개의 모델의 확장 실험에 따라, (i) SOTA 모델(예: o3와 Gemini-2.5-Pro)은 BMMR-Eval에서 큰 개발 여지가 남아 있습니다; (ii) 이유 모델은 학류의 편향을 나타내며, 특정 주제에서 LMMs보다 뛰어납니다; (iii) 오픈소스 모델은 프로피éta리 모델보다 뛰어납니다; (iv) BMMR-Train에서 미세 조정은 이 오류를 좁힙니다. 또한, BMMR-Verifier와 다른 상세한 연구를 사용하여, 현재의 LMMs가 다디스카드롬통리 이론에서 직면한 문제점을 밝혀냅니다. 데이터를 공개하고, 이 연구에서 커뮤니티에게 혜택을 제공하고자 합니다.",
      "upvotes": 19,
      "discussionId": "686c90d4364e2ad167eb53e7",
      "projectPage": "https://bmmr.pages.dev/",
      "githubRepo": "https://github.com/woooodyy/BMMR",
      "ai_summary": "A large-scale dataset and verification tool are introduced for assessing and improving cross-disciplinary reasoning capabilities in multimodal models.",
      "ai_keywords": [
        "bilingual",
        "multimodal",
        "multi-disciplinary",
        "large multimodal models",
        "LMMs",
        "UNESCO-defined subjects",
        "multiple-choice",
        "fill-in-the-blank",
        "open-ended QA",
        "human-in-the-loop",
        "scalable framework",
        "high-quality reasoning path",
        "multidisciplinary reasoning",
        "BMMR-Verifier",
        "reasoning models",
        "discipline bias",
        "reasoning-chain analysis"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-07-04T07:20:09.000Z",
    "title": "BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning\n  Dataset",
    "summary": "In this paper, we introduce BMMR, a large-scale bilingual, multimodal,\nmulti-disciplinary reasoning dataset for the community to develop and evaluate\nlarge multimodal models (LMMs). BMMR comprises 110k college-level questions\nspanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice,\nfill-in-the-blank, and open-ended QA-and sourced from both print and digital\nmedia such as books, exams, and quizzes. All data are curated and filtered via\na human-in-the-loop and scalable framework, and each instance is paired with a\nhigh-quality reasoning path. The dataset is organized into two parts: BMMR-Eval\nthat comprises 20,458 high-quality instances to comprehensively assess LMMs'\nknowledge and reasoning across multiple disciplines in both Chinese and\nEnglish; and BMMR-Train that contains 88,991 instances to support further\nresearch and development, extending the current focus on mathematical reasoning\nto diverse disciplines and domains. In addition, we propose the process-based\nmulti-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained\nevaluation of reasoning paths. Extensive experiments on 24 models reveal that\n(i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom\non BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs\nonly on specific subjects; (iii) open-source models still trail their\nproprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap.\nAdditionally, we conduct reasoning-chain analyses using BMMR-Verifier and other\nin-depth studies, uncovering the challenges LMMs currently face in\nmultidisciplinary reasoning. We will release the data, and we hope our work can\noffer insights and contributions to the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03483.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638ef0b0c67af472d31674a6",
      "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg",
      "fullname": "Honglin Guo",
      "name": "KYLN24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02029",
      "authors": [
        {
          "_id": "68679569213f123a1f88b87c",
          "name": "BAAI RoboBrain Team",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b87d",
          "user": {
            "_id": "668fa476cbcaf7ab0e4c58b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668fa476cbcaf7ab0e4c58b3/F5Jj-nPCjU6uxZyfkY3qw.jpeg",
            "isPro": false,
            "fullname": "Mingyu Cao",
            "user": "cmyopu",
            "type": "user"
          },
          "name": "Mingyu Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-07T11:24:50.715Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b87e",
          "name": "Huajie Tan",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b87f",
          "user": {
            "_id": "668f5478b3991ac0c3fc9c2f",
            "avatarUrl": "/avatars/a775853d3b88e7b1c8494ca837b5495c.svg",
            "isPro": false,
            "fullname": "yuhengji",
            "user": "yuheng2000",
            "type": "user"
          },
          "name": "Yuheng Ji",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:53:03.017Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b880",
          "user": {
            "_id": "67e406e64b80e9b39e2a85d6",
            "avatarUrl": "/avatars/a021be64341e5d7a079858916fa34c28.svg",
            "isPro": false,
            "fullname": "MinglanLin",
            "user": "MinglanLin",
            "type": "user"
          },
          "name": "Minglan Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:52:59.066Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b881",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b882",
          "user": {
            "_id": "66aadfc1344279e0243d4569",
            "avatarUrl": "/avatars/d2afbceb2e6279e42ed9ad98dffa7f0a.svg",
            "isPro": false,
            "fullname": "Caozhou",
            "user": "Caozhou1995",
            "type": "user"
          },
          "name": "Zhou Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:48.313Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b883",
          "name": "Pengwei Wang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b884",
          "user": {
            "_id": "63f08dc79cf89c9ed1bb89cd",
            "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
            "isPro": false,
            "fullname": "Zhoues",
            "user": "Zhoues",
            "type": "user"
          },
          "name": "Enshen Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:52:57.069Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b885",
          "name": "Yi Han",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b886",
          "name": "Yingbo Tang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b887",
          "name": "Xiangqi Xu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b888",
          "name": "Wei Guo",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b889",
          "name": "Yaoxu Lyu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88a",
          "name": "Yijie Xu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88b",
          "name": "Jiayu Shi",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88c",
          "user": {
            "_id": "650bf938677f9e45963d672e",
            "avatarUrl": "/avatars/7d4159067b5005a3a635e36b26b7b239.svg",
            "isPro": false,
            "fullname": "Cheng Chi",
            "user": "ChuckChi",
            "type": "user"
          },
          "name": "Cheng Chi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:53:01.035Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88d",
          "name": "Mengdi Zhao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88e",
          "name": "Xiaoshuai Hao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88f",
          "name": "Shanyu Rong",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b890",
          "name": "Zhengliang Cai",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b891",
          "name": "Bolun Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b892",
          "name": "Shuyi Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b893",
          "name": "Huaihai Lyu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b894",
          "name": "Mengfei Du",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b895",
          "name": "Lingfeng Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b896",
          "name": "Xi Feng",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b897",
          "name": "Xiaodan Liu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b898",
          "name": "Yance Jiao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b899",
          "name": "Chenrui He",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89a",
          "user": {
            "_id": "63157214362e3e95ea553db5",
            "avatarUrl": "/avatars/a421c25128c71be6d0c92490cebbbccc.svg",
            "isPro": false,
            "fullname": "lyu",
            "user": "ceci3",
            "type": "user"
          },
          "name": "Mengsi Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:50.528Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89b",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89c",
          "name": "Yulong Ao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89d",
          "name": "Xue Sun",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89e",
          "name": "Zheqi He",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89f",
          "name": "Jingshu Zheng",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a0",
          "name": "Xi Yang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a1",
          "name": "Donghai Shi",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a2",
          "name": "Kunchang Xie",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a3",
          "name": "Bochao Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a4",
          "name": "Shaokai Nie",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a5",
          "name": "Chunlei Men",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a6",
          "name": "Yonghua Lin",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a7",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a8",
          "name": "Tiejun Huang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a9",
          "name": "Shanghang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:05:33.000Z",
      "submittedOnDailyAt": "2025-07-08T06:32:16.956Z",
      "title": "RoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서\n\nRoboBrain 2.0 기술보고서",
      "submittedOnDailyBy": {
        "_id": "63a369d98c0c89dcae3b8329",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
        "isPro": true,
        "fullname": "Adina Yakefu",
        "user": "AdinaY",
        "type": "user"
      },
      "summary": "로보브레이인 2.0, 우리의 최신 세대의 시각화 비지션 언어 기반 모델을 소개합니다. 이 모델은 물리적 환경에서 복잡한 시각화 작업의 인식, 논리론, 계획을 통합하는 모델입니다. 이 모델은 2개의 버전을 준비하고 있습니다: 7B 모델과 32B 모델. 시각화 인코더와 언어 모델을 구성하는 유한 구조 아키텍처를 특징으로 합니다. 그 작은 크기에도 불구하고, 로보브레이인 2.0은 광범위한 시각화 논리론 작업에서 강력한 성능을 발휘하고 있습니다. 공간 벤치마크와 시간 벤치마크에서도, 32B 버전은 선두의 결과를 달성하고, 오픈 소스 모델과 소유 모델을 초과하고 있습니다. 특히, 공간 이해(예: 기능 예측, 공간 지출, 경로 예측)과 시간의 결정론(예: 폐쇄 루트 상호작용, 다 에이전트 장기 계획, 장면 그래프 업데이트)을 지원하고 있습니다. 이 보고서에서는 모델 아키텍처, 데이터 구축, 멀티 단계 훈련 전략, 인프라와 실용적인 응용 프로그램을 자세히 설명합니다. 로보브레이인 2.0은 시각화 AI의 연구를 추진하고 일반적인 시각화 에이전트의 구축에 대한 실용적인 단계로役立つ 것을 바랍니다. 코드, 체크포인트와 벤치마크는 https://superrobobrain.github.io 에서 사용 가능합니다.",
      "upvotes": 14,
      "discussionId": "68679569213f123a1f88b8aa",
      "ai_summary": "RoboBrain 2.0, a vision-language foundation model, achieves top performance in embodied tasks through its heterogeneous architecture and multi-stage training strategies.",
      "ai_keywords": [
        "embodied vision-language",
        "embodied reason",
        "vision encoder",
        "spatial understanding",
        "affordance prediction",
        "spatial referring",
        "trajectory forecasting",
        "temporal decision-making",
        "closed-loop interaction",
        "multi-agent long-horizon planning",
        "scene graph updating"
      ]
    },
    "publishedAt": "2025-07-02T13:05:33.000Z",
    "title": "RoboBrain 2.0 Technical Report",
    "summary": "We introduce RoboBrain 2.0, our latest generation of embodied vision-language\nfoundation models, designed to unify perception, reasoning, and planning for\ncomplex embodied tasks in physical environments. It comes in two variants: a\nlightweight 7B model and a full-scale 32B model, featuring a heterogeneous\narchitecture with a vision encoder and a language model. Despite its compact\nsize, RoboBrain 2.0 achieves strong performance across a wide spectrum of\nembodied reasoning tasks. On both spatial and temporal benchmarks, the 32B\nvariant achieves leading results, surpassing prior open-source and proprietary\nmodels. In particular, it supports key real-world embodied AI capabilities,\nincluding spatial understanding (e.g., affordance prediction, spatial\nreferring, trajectory forecasting) and temporal decision-making (e.g.,\nclosed-loop interaction, multi-agent long-horizon planning, and scene graph\nupdating). This report details the model architecture, data construction,\nmulti-stage training strategies, infrastructure and practical applications. We\nhope RoboBrain 2.0 advances embodied AI research and serves as a practical step\ntoward building generalist embodied agents. The code, checkpoint and benchmark\nare available at https://superrobobrain.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02029.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 857
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04009",
      "authors": [
        {
          "_id": "686cd234cc230c60b4100aec",
          "name": "Ziyang Miao",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100aed",
          "name": "Qiyu Sun",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100aee",
          "name": "Jingyuan Wang",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100aef",
          "user": {
            "_id": "66a48a77f9565635ebc33a87",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a48a77f9565635ebc33a87/WW8BFd1D9xZbGIPZPfdHk.png",
            "isPro": false,
            "fullname": "GYC",
            "user": "oGYCo",
            "type": "user"
          },
          "name": "Yuchen Gong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:42.785Z",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100af0",
          "user": {
            "_id": "642fef28a043f0ac7defa8a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
            "isPro": false,
            "fullname": "Yaowei Zheng",
            "user": "hiyouga",
            "type": "user"
          },
          "name": "Yaowei Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:39:35.178Z",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100af1",
          "name": "Shiqi Li",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100af2",
          "name": "Richong Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/Ztx5877vKyqqZINgge_1B.mp4"
      ],
      "publishedAt": "2025-07-05T11:38:59.000Z",
      "submittedOnDailyAt": "2025-07-08T06:40:18.914Z",
      "title": "Easy Dataset: 유닛 프레임워크와 확장 가능한 프레임워크를 사용하여 무구조 문서에서 LLM의 微調節 데이터의 합성\n\n(Note: \"微調節\" is a term that might not have a direct equivalent in Korean. It generally refers to \"fine-tuning\" in the context of machine learning. If you have a specific term or context for \"微調節\" in mind, please let me know for a more precise translation.)",
      "submittedOnDailyBy": {
        "_id": "642fef28a043f0ac7defa8a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
        "isPro": false,
        "fullname": "Yaowei Zheng",
        "user": "hiyouga",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)은 일반적인 태스크에서 놀라울만한 성능을 보여주지만, 특정 분야에 적용할 때 고품질의 분야 데이터의 부족이 문제로 됩니다. 현재의 데이터 합성 도구는 신뢰할 수 있는 미세 조정 데이터를 효과적으로 추출하는 데 어려움을 겪으며, 이러한 한계를 해결하기 위해 Easy Dataset라는 유닛 프레임워크를 제안합니다. 이 프레임워크는 직관적인 그래픽 사용자 인터페이스(GUI)를 통해 무구조 문서로부터 미세 조정 데이터를 합성할 수 있게 됩니다. 특히, Easy Dataset은 사용자가 간단하게 텍스트 추출 모델과 크래킹 스테이지를 설정할 수 있으며, 단순한 문서를 컬럼화한 텍스트 챗크랙을 변환할 수 있게 합니다. 이후 공개된 LLMs를 사용하여, 포터포어드 Drove된 Prompting 접근법을 활용하여 다양한 질문·답변 쌍을 생성합니다. 이 전체 파이프라인에서, 사람이 루프 내의 시각화 인터페이스를 사용하여 중간 출력의 검토와 개선을 촉진하고, 데이터의 품질을 보장합니다. 금융 태스크의 질문·답변 태스크에서 실험은 합성 데이터에 의한 LLMs의 미세 조정은 분야 고유의 성능을 크게 향상시키고 일반적인 지식을 유지하는 것을 보여주고 있습니다. 소스 코드와 설치 가능한 패키지는 https://github.com/ConardLi/easy-dataset에 접근할 수 있으며, 이 덕분에 GitHub의 별점 수는 9,000점을 초과하고 있습니다.",
      "upvotes": 12,
      "discussionId": "686cd234cc230c60b4100af3",
      "githubRepo": "https://github.com/ConardLi/easy-dataset",
      "ai_summary": "A unified framework called Easy Dataset synthesizes fine-tuning data from unstructured documents using a GUI and LLMs, improving domain-specific performance of LLMs while maintaining general knowledge.",
      "ai_keywords": [
        "Large language models",
        "fine-tuning",
        "unstructured documents",
        "graphical user interface",
        "text extraction models",
        "chunking strategies",
        "persona-driven prompting",
        "human-in-the-loop",
        "financial question-answering task"
      ],
      "githubStars": 9180
    },
    "publishedAt": "2025-07-05T07:38:59.000Z",
    "title": "Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM\n  Fine-Tuning Data from Unstructured Documents",
    "summary": "Large language models (LLMs) have shown impressive performance on\ngeneral-purpose tasks, yet adapting them to specific domains remains\nchallenging due to the scarcity of high-quality domain data. Existing data\nsynthesis tools often struggle to extract reliable fine-tuning data from\nheterogeneous documents effectively. To address this limitation, we propose\nEasy Dataset, a unified framework for synthesizing fine-tuning data from\nunstructured documents via an intuitive graphical user interface (GUI).\nSpecifically, Easy Dataset allows users to easily configure text extraction\nmodels and chunking strategies to transform raw documents into coherent text\nchunks. It then leverages a persona-driven prompting approach to generate\ndiverse question-answer pairs using public-available LLMs. Throughout the\npipeline, a human-in-the-loop visual interface facilitates the review and\nrefinement of intermediate outputs to ensure data quality. Experiments on a\nfinancial question-answering task show that fine-tuning LLMs on the synthesized\ndataset significantly improves domain-specific performance while preserving\ngeneral knowledge. The source code and installable package are available at\nhttps://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub\nstars.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/Ztx5877vKyqqZINgge_1B.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642fef28a043f0ac7defa8a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
      "fullname": "Yaowei Zheng",
      "name": "hiyouga",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2185
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03253",
      "authors": [
        {
          "_id": "686c86ff364e2ad167eb53a8",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53a9",
          "name": "Shenghua Liu",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53aa",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ab",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ac",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ad",
          "name": "Yiwei Wang",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ae",
          "user": {
            "_id": "63120517ae8896941da4c5da",
            "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
            "isPro": false,
            "fullname": "Lingrui Mei",
            "user": "Chevalier",
            "type": "user"
          },
          "name": "Lingrui Mei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:44.909Z",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53af",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53b0",
          "name": "Jiafeng Guo",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53b1",
          "name": "Xueqi Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T02:19:58.000Z",
      "submittedOnDailyAt": "2025-07-08T01:53:47.804Z",
      "title": "RefineX: 규모에 따라 전문가이드를 받은 학습 프로그램에서 준비된 데이터의 향상 학습",
      "submittedOnDailyBy": {
        "_id": "642577e06d0f0f5f1dc68904",
        "avatarUrl": "/avatars/3df7cfcf9ca6cd9c9e9b10a73f4efc35.svg",
        "isPro": false,
        "fullname": "Bibaolong",
        "user": "Bibaolong",
        "type": "user"
      },
      "summary": "대규모 언어 모델(LLMs)의 근본적인 능력은 사전 데이터의 질에 깊은 영향을 받고 있습니다. 그러나 규모로 데이터의 질을 향상시키는 것은 큰 문제로, 주로 정밀화의 효과와 처리의 효율성 사이의 트레이드오프에 의해 문제가 됩니다. 규칙 기반의 필터링이 주된 패러다임으로 있지만, 일반적으로 문서 수준에서 작동하며, 특정 내용을 정밀화하기 위해 필요한 입도성을 많이 부족합니다. ProX 등新兴의 연구를 참고하여, RefineX라는 새로운 프레임워크를 제안합니다. RefineX는 프로그래밍 편집자 태스크를 통해, 대규모의 외과적인 사전 데이터의 정밀화를 실현합니다. RefineX는 효율적인 데이터 정밀화를 가능하게 하는 同时, 원본 텍스트의 다양성과 자연성을 신뢰적으로 유지합니다. RefineX의 핵심적인 강점은, 고품질의 데이터 정밀화 결과를 최소한의 편집 프로그램으로 수용하는 것입니다. 이 고품질의 결과의 통합 파이프라인은, 효율적이고 신뢰성 있는 정밀화 모델을 훈련하는 데, 코퍼스 내의 모든 인스턴스를 규모로 체계적으로 개선할 수 있습니다. RefineX는 스크래치부터의 사전 데이터를 평가하고, 많은 모델 크기와 다양한 하류 태스크에서 일관된 최상위 성능을 보여주며, 750M 모델에서 lighteval 태스크에서 평균 2.6%-7.2%의 증가율을 기록하며, 상당한 훈련 토큰을 줄일 때도相当한 성능을 달성합니다. 진한 분석은, RefineX는 텍스트의 질을 신뢰적으로 향상시키고, 기존 방법보다 더 효과적이고, 최종 디지털 생성이나 Prox-C를 초과하는 것을 보여주며, 이러한 결과를 통해, RefineX는 현재의 LLM 프로젝션에서 사전 데이터의 최적화의 scalable 및 효과적이고 신뢰성 있는 해결책으로 자리잡습니다.",
      "upvotes": 12,
      "discussionId": "686c86ff364e2ad167eb53b2",
      "ai_summary": "RefineX is a scalable framework for improving the quality of large language model pre-training data through programmatic editing, yielding better performance than alternative methods across various downstream tasks.",
      "ai_keywords": [
        "large language models",
        "pre-training corpora",
        "rule-based filtering",
        "document level",
        "granular refinement",
        "programmatic editing",
        "refine model",
        "data refinement",
        "text quality",
        "end-to-end generation",
        "lighteval tasks"
      ]
    },
    "publishedAt": "2025-07-03T22:19:58.000Z",
    "title": "RefineX: Learning to Refine Pre-training Data at Scale from\n  Expert-Guided Programs",
    "summary": "The foundational capabilities of large language models (LLMs) are deeply\ninfluenced by the quality of their pre-training corpora. However, enhancing\ndata quality at scale remains a significant challenge, primarily due to the\ntrade-off between refinement effectiveness and processing efficiency. While\nrule-based filtering remains the dominant paradigm, it typically operates at\nthe document level and lacks the granularity needed to refine specific content\nwithin documents. Inspired by emerging work such as ProX, we propose\nRefineX, a novel framework for large-scale, surgical refinement of\npre-training data through programmatic editing tasks. RefineX enables efficient\nand fine-grained data refinement while reliably preserving the diversity and\nnaturalness of raw text. The core strength of RefineX lies in distilling\nhigh-quality, expert-guided end-to-end refinement results into minimal\nedit-based deletion programs. This high-precision distillation pipeline is used\nto train an efficient and reliable refine model that can systematically improve\nevery instance in the corpus at scale. We evaluate RefineX across from-scratch\npre-training at multiple model scales and find that it consistently outperforms\nmodels trained on raw, filtered, or alternatively refined data across diverse\ndownstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on\nlighteval tasks, and achieves comparable performance using significantly fewer\ntraining tokens. Further analysis shows that RefineX reliably enhances text\nquality with both high efficiency and precision, outperforming prior approaches\nsuch as end-to-end generation and Prox-C. These results position RefineX as a\nscalable, effective, and reliable solution for optimizing pre-training data in\nmodern LLM pipelines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642577e06d0f0f5f1dc68904",
      "avatarUrl": "/avatars/3df7cfcf9ca6cd9c9e9b10a73f4efc35.svg",
      "fullname": "Bibaolong",
      "name": "Bibaolong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05108",
      "authors": [
        {
          "_id": "686cc8bc364e2ad167eb54e3",
          "name": "Yuyi Zhang",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e4",
          "name": "Peirong Zhang",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e5",
          "name": "Zhenhua Yang",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e6",
          "name": "Pengyu Yan",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e7",
          "name": "Yongxin Shi",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e8",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e9",
          "name": "Fengjun Guo",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54ea",
          "name": "Lianwen Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T15:26:17.000Z",
      "submittedOnDailyAt": "2025-07-08T06:01:26.616Z",
      "title": "문화유산의 재생: 역사문서의 전례적인 리포맷팅의 새로운 접근법",
      "submittedOnDailyBy": {
        "_id": "65fba5700b78c48c9e393a3e",
        "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
        "isPro": false,
        "fullname": "Yuyi Zhang",
        "user": "ZZXF",
        "type": "user"
      },
      "summary": "역사문서는 무가치한 문화유산으로, 시간이 흐르면서 손상, 물의 침식, 산화 등 심각한 퇴화에 직면하여 있습니다. 현재의 역사문서의 재현(HDR)手法는 주로 단일 모델이나 제한된 크기의 재현에 초점을 맞추어 있으며, 실용적인 필요에 맞지 않습니다. 이를 해결하기 위해, FPHDR(Full-Page HDR 데이터 세트)와 AutoHDR(자동 HDR 솔루션)를 제안합니다. FPHDR는 1,633 장의 실물 이미지와 6,543 장의 합성 이미지를 포함하며, 문자 레벨과 행 레벨의 위치 정보, 그리고 손상 정도에 따른 문자 설명을 가지고 있습니다. AutoHDR은 역사학자의 재현 작업 흐름을 모방하고, OCR를 통해 손상 위치 탐지, 시각 언어 컨텍스트 텍스트 예측, 패치 자동 회귀적인 외관 재현의 3단계 접근 방식을 사용합니다. AutoHDR의 모듈화된 아키텍처에 의해, 무간간의 인간과 기계의 협업이 가능하며, 각 재현 단계마다 유연한 간섭과 최적화가 가능합니다. 실험은 AutoHDR의 놀라운 성능을 보여주며, 손상이 심한 문서 처리 시, OCR 정확도가 46.83%에서 84.05%까지 상승하여, 인간과 기계의 협업으로 94.25%까지 도달했습니다. 우리는 이 연구는 자동화된 역사문서 재현의 중요한 발전이며, 문화유산의 보존에 크게 기여합니다. 모델과 데이터 세트는 https://github.com/SCUT-DLVCLab/AutoHDR에서 사용 가능합니다.",
      "upvotes": 7,
      "discussionId": "686cc8bd364e2ad167eb54eb",
      "githubRepo": "https://github.com/SCUT-DLVCLab/AutoHDR",
      "githubStars": 22
    },
    "publishedAt": "2025-07-07T11:26:17.000Z",
    "title": "Reviving Cultural Heritage: A Novel Approach for Comprehensive\n  Historical Document Restoration",
    "summary": "Historical documents represent an invaluable cultural heritage, yet have\nundergone significant degradation over time through tears, water erosion, and\noxidation. Existing Historical Document Restoration (HDR) methods primarily\nfocus on single modality or limited-size restoration, failing to meet practical\nneeds. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel\nautomated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and\n6,543 synthetic images with character-level and line-level locations, as well\nas character annotations in different damage grades. AutoHDR mimics historians'\nrestoration workflows through a three-stage approach: OCR-assisted damage\nlocalization, vision-language context text prediction, and patch autoregressive\nappearance restoration. The modular architecture of AutoHDR enables seamless\nhuman-machine collaboration, allowing for flexible intervention and\noptimization at each restoration stage. Experiments demonstrate AutoHDR's\nremarkable performance in HDR. When processing severely damaged documents, our\nmethod improves OCR accuracy from 46.83\\% to 84.05\\%, with further enhancement\nto 94.25\\% through human-machine collaboration. We believe this work represents\na significant advancement in automated historical document restoration and\ncontributes substantially to cultural heritage preservation. The model and\ndataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05108.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fba5700b78c48c9e393a3e",
      "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
      "fullname": "Yuyi Zhang",
      "name": "ZZXF",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.03745",
      "authors": [
        {
          "_id": "686ca04e364e2ad167eb543c",
          "user": {
            "_id": "63fedca388b9695964c33ad8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fedca388b9695964c33ad8/XFE8Tm9yRRbo6XdDw7c2O.png",
            "isPro": false,
            "fullname": "Aki",
            "user": "AkiCumulo",
            "type": "user"
          },
          "name": "Akio Kodaira",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:07.401Z",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb543d",
          "name": "Tingbo Hou",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb543e",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb543f",
          "name": "Masayoshi Tomizuka",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb5440",
          "name": "Yue Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/eFed7d-PiKdUBGIPDkIoo.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/50KoaI5mgYLe_IvIOaVJh.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/zWzNfOJX8C-CNICabQyHV.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/m072g2U7H133AyOGShNZ0.mp4"
      ],
      "publishedAt": "2025-07-04T18:00:01.000Z",
      "submittedOnDailyAt": "2025-07-08T04:21:42.132Z",
      "title": "StreamDiT: 실시간 비디오 생성을 위한 맵핑 텍스트\n\n(注意：虽然要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，我提供了一个更符合韩国语表达习惯的翻译版本。)",
      "submittedOnDailyBy": {
        "_id": "63fedca388b9695964c33ad8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fedca388b9695964c33ad8/XFE8Tm9yRRbo6XdDw7c2O.png",
        "isPro": false,
        "fullname": "Aki",
        "user": "AkiCumulo",
        "type": "user"
      },
      "summary": "최근 transformer 기반의 diffusion 모델을 수 백억 개의 파라미터로 확장하여 고품질의 비디오를 생성할 수 있는 것을 통해 text-to-video (T2V) 생성에서 큰 진전을 달성했습니다. 그러나 현재의 모델은 일반적으로 온라인에서 짧은 클립만 생성하고, 인터랙티브나 실시간 애플리케이션의 사용 분야가 제한되어 있습니다. 본 논문에서는 이러한 문제를 해결하기 위해 StreamDiT, 스트리밍 비디오 생성 모델을 제안합니다. StreamDiT의 훈련은 이동 버퍼를 추가한 flow matching에 기반합니다. 서로 다른 버퍼 프레임 분할 시나믹에서 혼합 훈련을 설계하고, 콘텐츠의 일관성과 시각적 품질을 양방향으로 향상시킵니다. StreamDiT의 모델링은 시간적 인코딩과 윈도우 어텐션을 사용한 adaLN DiT에 기반합니다. 제안된 방법을 실천하기 위해 4B 파라미터의 StreamDiT 모델을 훈련합니다. 또한 StreamDiT에 적합한 multistep distillation 메소드를 제안합니다. 선택된 분할 시나믹의 각 섹션에서 샘플링 거리를 수행합니다. 거리 후, 함수 평가의 총 수 (NFEs)는 버퍼의 블록 수에 따라 감소합니다. 최종적으로, 우리의 거리 모델은 1 FPS에서 16 FPS의 실시간 동작을 구현하고, 512p 렌더링에서 비디오 스트리밍 생성이 가능합니다. 우리의 방법을 통계적 메트릭과 인간 평가로 평가합니다. 우리의 모델은 흐름 생성, 인터랙티브 생성, 비디오에서 비디오처럼 실시간 애플리케이션을 가능하게 합니다. 프로젝트 웹 사이트에서 비디오 결과와 예를 제공합니다: <a href=\"https://cumulo-autumn.github.io/StreamDiT/\">this https URL.</a>",
      "upvotes": 7,
      "discussionId": "686ca04f364e2ad167eb5441",
      "projectPage": "https://cumulo-autumn.github.io/StreamDiT/",
      "ai_summary": "A streaming video generation model named StreamDiT, based on transformer-based diffusion models, enables real-time video generation with high content consistency and visual quality.",
      "ai_keywords": [
        "transformer-based diffusion models",
        "StreamDiT",
        "flow matching",
        "moving buffer",
        "mixed training",
        "adaLN DiT",
        "varying time embedding",
        "window attention",
        "multistep distillation",
        "sampling distillation",
        "function evaluations",
        "real-time performance",
        "streaming generation",
        "interactive generation",
        "video-to-video"
      ]
    },
    "publishedAt": "2025-07-04T14:00:01.000Z",
    "title": "StreamDiT: Real-Time Streaming Text-to-Video Generation",
    "summary": "Recently, great progress has been achieved in text-to-video (T2V) generation\nby scaling transformer-based diffusion models to billions of parameters, which\ncan generate high-quality videos. However, existing models typically produce\nonly short clips offline, restricting their use cases in interactive and\nreal-time applications. This paper addresses these challenges by proposing\nStreamDiT, a streaming video generation model. StreamDiT training is based on\nflow matching by adding a moving buffer. We design mixed training with\ndifferent partitioning schemes of buffered frames to boost both content\nconsistency and visual quality. StreamDiT modeling is based on adaLN DiT with\nvarying time embedding and window attention. To practice the proposed method,\nwe train a StreamDiT model with 4B parameters. In addition, we propose a\nmultistep distillation method tailored for StreamDiT. Sampling distillation is\nperformed in each segment of a chosen partitioning scheme. After distillation,\nthe total number of function evaluations (NFEs) is reduced to the number of\nchunks in a buffer. Finally, our distilled model reaches real-time performance\nat 16 FPS on one GPU, which can generate video streams at 512p resolution. We\nevaluate our method through both quantitative metrics and human evaluation. Our\nmodel enables real-time applications, e.g. streaming generation, interactive\ngeneration, and video-to-video. We provide video results and more examples in\nour project website: <a href=\"https://cumulo-autumn.github.io/StreamDiT/\">this\nhttps URL.</a>",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/eFed7d-PiKdUBGIPDkIoo.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/50KoaI5mgYLe_IvIOaVJh.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/zWzNfOJX8C-CNICabQyHV.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/m072g2U7H133AyOGShNZ0.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fedca388b9695964c33ad8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fedca388b9695964c33ad8/XFE8Tm9yRRbo6XdDw7c2O.png",
      "fullname": "Aki",
      "name": "AkiCumulo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.04952",
      "authors": [
        {
          "_id": "686c8487364e2ad167eb5386",
          "name": "Chenchen Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5387",
          "name": "Yuhang Li",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5388",
          "name": "Can Xu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5389",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538a",
          "name": "Ao Liu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538b",
          "name": "Shihui Hu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538c",
          "name": "Dengpeng Wu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538d",
          "name": "Guanhua Huang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538e",
          "name": "Kejiao Li",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538f",
          "name": "Qi Yi",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5390",
          "name": "Ruibin Xiong",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5391",
          "name": "Haotian Zhu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5392",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5393",
          "name": "Yuhao Jiang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5394",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5395",
          "name": "Zenan Xu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5396",
          "name": "Bohui Zhai",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5397",
          "name": "Guoxiang He",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5398",
          "name": "Hebin Li",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5399",
          "name": "Jie Zhao",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539a",
          "name": "Le Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539b",
          "name": "Lingyun Tan",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539c",
          "name": "Pengyu Guo",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539d",
          "name": "Xianshu Pang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539e",
          "name": "Yang Ruan",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539f",
          "name": "Zhifeng Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a0",
          "name": "Zhonghu Wang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a1",
          "name": "Ziyan Xu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a2",
          "name": "Zuopu Yin",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a3",
          "name": "Wiggin Zhou",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a4",
          "name": "Chayse Zhou",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a5",
          "name": "Fengzong Lian",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/Ib4IFSlNDtkWKjjyVWG-S.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/4OtfeyvZ2jMLaebiU0NKA.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/k-024E68E9-6iaF0QtZad.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/VBS9HcDQ2oHj3bmxgyZVB.png"
      ],
      "publishedAt": "2025-07-07T12:53:00.000Z",
      "submittedOnDailyAt": "2025-07-08T02:53:13.802Z",
      "title": "ArtifactsBench: 시각적 상호작용 간의 간격을 통해 LLM 코드 생성 평가",
      "submittedOnDailyBy": {
        "_id": "64b74b906ab5d14ca7f289cd",
        "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
        "isPro": false,
        "fullname": "xxzcc",
        "user": "xxzcc",
        "type": "user"
      },
      "summary": "LLM의 생성 능력은 정적 코드에서 동적으로, 인터랙티브 시각적인 아드キャレート로 급격히 확장되고 있습니다. 이 발전은 기존 벤치마크가 알고리즘의 정확성을 중점적으로 두고, 현대 사용자 경험에 정의하는 시각적인 신뢰성과 인터랙티브 통합성을 고려하지 않는 중요한 평가의 간극에 의해 제한되어 있습니다. 이 간극을 해결하기 위해, 우리는 시각화 코드 생성의 자동화된, 다형 평가의 새로운 벤치마크와 패러다임으로 ArtifactsBench를 소개합니다. 우리의 프레임워크는 생성된 아드キャレー트의 각 콘텐츠들을 프로그래밍적으로 표현하고, 시퀀셜한 스크린샷으로 그 동적인 시각을 파악합니다. 이 시각적 증거와 소스 코드는 Multimodal LLM(MLLM)-as-Judge에 의해 평가되고, 세부적인 체크리스트를 기반으로 엄격한 난이도를 설정하여 전체적인 재현 가능한 점수를 보장합니다. 우리는 1,825가지 다양한 태스크의 벤치마크를 구축하고, 30개 이상의 선진적인 LLM을 평가합니다. 우리의 자동 평가는 WebDev Arena와 94.4%의 점수 일치율을 달성하고, 사용자 전문가와 90% 이상의 쌍우 동의를 초과합니다. 이로써, ArtifactsBench는 인간적인 질의 평가의 규모적 신뢰적인 자동화를 처음으로 구축하는 첫 번째 프레임워크입니다. 우리의 분석은 현재의 SOTA의 고해상도 맵을 제공하고, 일반적인 모델이 영역 전문 모델을 초과하는 것을 보여주며, ArtifactsBench의 벤치마크, 평가 해너스, 베이스라인 결과를 공개하여 https://artifactsbenchmark.github.io/에 접근할 수 있도록 합니다. 커뮤니티에게 스케일러블하고 정확한 도구를 제공하여, 사용자 중심적인 생성 모델의 개발을 가속화합니다.",
      "upvotes": 6,
      "discussionId": "686c8487364e2ad167eb53a6",
      "ai_summary": "ArtifactsBench, a novel benchmark and evaluation framework, automates the assessment of visual code generation quality using temporal screenshots and a multimodal language model judge.",
      "ai_keywords": [
        "Large Language Models",
        "LLMS",
        "dynamic",
        "interactive visual artifacts",
        "visual fidelity",
        "interactive integrity",
        "ArtifactsBench",
        "benchmark",
        "Multimodal LLM",
        "MLLM-as-Judge",
        "fine-grained",
        "per-task checklist",
        "ranking consistency",
        "WebDev Arena",
        "pairwise agreement",
        "human-perceived quality",
        "generalist models",
        "domain-specific ones"
      ]
    },
    "publishedAt": "2025-07-07T08:53:00.000Z",
    "title": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation",
    "summary": "The generative capabilities of Large Language Models (LLMs) are rapidly\nexpanding from static code to dynamic, interactive visual artifacts. This\nprogress is bottlenecked by a critical evaluation gap: established benchmarks\nfocus on algorithmic correctness and are blind to the visual fidelity and\ninteractive integrity that define modern user experiences. To bridge this gap,\nwe introduce ArtifactsBench, a new benchmark and paradigm for the automated,\nmultimodal evaluation of visual code generation. Our framework programmatically\nrenders each generated artifact and captures its dynamic behavior through\ntemporal screenshots. This visual evidence, alongside the source code, is then\nassessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a\nfine-grained, per-task checklist to ensure holistic and reproducible scoring.\nWe construct a new benchmark of 1,825 diverse tasks and evaluate over 30\nleading LLMs. Our automated evaluation achieves a striking 94.4% ranking\nconsistency with WebDev Arena, the gold-standard for human preference in web\ndevelopment, and over 90% pairwise agreement with human experts. This\nestablishes ArtifactsBench as the first framework to reliably automate the\nassessment of human-perceived quality at scale. Our analysis provides a\nhigh-resolution map of the current SOTA, revealing that generalist models often\noutperform domain-specific ones. We open-source ArtifactsBench, including the\nbenchmark, evaluation harness, and baseline results at\nhttps://artifactsbenchmark.github.io/, to provide the community with a scalable\nand accurate tool to accelerate the development of user-centric generative\nmodels.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/Ib4IFSlNDtkWKjjyVWG-S.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/4OtfeyvZ2jMLaebiU0NKA.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/k-024E68E9-6iaF0QtZad.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/VBS9HcDQ2oHj3bmxgyZVB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b74b906ab5d14ca7f289cd",
      "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
      "fullname": "xxzcc",
      "name": "xxzcc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04590",
      "authors": [
        {
          "_id": "686c96a0364e2ad167eb540c",
          "name": "Rui Meng",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb540d",
          "user": {
            "_id": "64778fb8168cb428e00f69b0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64778fb8168cb428e00f69b0/D_XYg74zHG9K3HUJj_gD4.jpeg",
            "isPro": true,
            "fullname": "Ziyan Jiang",
            "user": "ziyjiang",
            "type": "user"
          },
          "name": "Ziyan Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:15.117Z",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb540e",
          "name": "Ye Liu",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb540f",
          "name": "Mingyi Su",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5410",
          "name": "Xinyi Yang",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5411",
          "name": "Yuepeng Fu",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5412",
          "name": "Can Qin",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5413",
          "name": "Zeyuan Chen",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5414",
          "name": "Ran Xu",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5415",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5416",
          "name": "Yingbo Zhou",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5417",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5418",
          "name": "Semih Yavuz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T00:51:57.000Z",
      "submittedOnDailyAt": "2025-07-08T02:26:58.880Z",
      "title": "VLM2Vec-V2: 영화, 이미지, 시각적 문서의 다중모달 인코딩의 발전",
      "submittedOnDailyBy": {
        "_id": "64778fb8168cb428e00f69b0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64778fb8168cb428e00f69b0/D_XYg74zHG9K3HUJj_gD4.jpeg",
        "isPro": true,
        "fullname": "Ziyan Jiang",
        "user": "ziyjiang",
        "type": "user"
      },
      "summary": "다모달 모델은 다양한 하류 태스크(세마니틱 유사성, 정보 검색, 클러스터링)을 가능하게 하는 중요한 역할을 수행하고 있습니다. 그러나 현재의 다모달 모델(VLM2Vec, E5-V, GME)은 주로 자연 이미지에 집중되어 있습니다. 다른 시각형식(비디오, 시각문서)에 대한 지원은 제한되어 있으며, 실제 세계적인 상황에서 적용 범위가 좁아지는 것을 볼 수 있습니다. AI 에이전트, 다모달 검색 및 추천, 검색 어게이지 생성(RAG) 등 상황에서 이러한 제약이 있기 때문에, 이를 해결하기 위해 VLM2Vec-V2, 다양한 시각형식의 다모달 학습을 위한 통일 프레임워크를 제안합니다. 먼저, MMEB-V2, MMEB에 새로운 5가지 태스크 타입을 추가한 세부 벤치마크를 소개합니다. 그리고 VLM2Vec-V2, 문법, 이미지, 비디오, 시각문서의 입력을 지원하는 일반적인 다모달 모델을 훈련합니다. 확장된 실험 결과에 따르면 VLM2Vec-V2는 새로운 비디오 및 문서 검색 태스크에서 강력한 성능을 보여주며, 기존 이미지 벤치마크에서 선행 기준을 초과합니다. 확장된 평가를 통해, 본 연구는 다모달 모델의 일반화 능력을 설명하고, 통일된 다모달 학습의 효과적인 전략을 밝혀, 연구 및 실제 세계적인 설정에서 더 scalable 및 적용 가능한 표현 학습의 기초를 구축합니다.",
      "upvotes": 4,
      "discussionId": "686c96a0364e2ad167eb5419",
      "projectPage": "https://tiger-ai-lab.github.io/VLM2Vec/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VLM2Vec",
      "ai_summary": "A unified framework VLM2Vec-V2 is proposed for learning embeddings across diverse visual forms such as videos and documents, demonstrating strong performance on new tasks and improving upon existing benchmarks for images.",
      "ai_keywords": [
        "multimodal embedding models",
        "VLM2Vec",
        "E5-V",
        "GME",
        "MMEB-V2",
        "visual document retrieval",
        "video retrieval",
        "temporal grounding",
        "video classification",
        "video question answering",
        "general-purpose embedding model",
        "unified embedding learning",
        "representation learning"
      ],
      "githubStars": 290
    },
    "publishedAt": "2025-07-06T20:51:57.000Z",
    "title": "VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and\n  Visual Documents",
    "summary": "Multimodal embedding models have been crucial in enabling various downstream\ntasks such as semantic similarity, information retrieval, and clustering over\ndifferent modalities. However, existing multimodal embeddings like VLM2Vec,\nE5-V, GME are predominantly focused on natural images, with limited support for\nother visual forms such as videos and visual documents. This restricts their\napplicability in real-world scenarios, including AI agents, multi-modal search\nand recommendation, and retrieval-augmented generation (RAG). To close this\ngap, we propose VLM2Vec-V2, a unified framework for learning embeddings across\ndiverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark\nthat extends MMEB with five new task types: visual document retrieval, video\nretrieval, temporal grounding, video classification and video question\nanswering - spanning text, image, video, and visual document inputs. Next, we\ntrain VLM2Vec-V2, a general-purpose embedding model that supports text, image,\nvideo, and visual document inputs. Extensive experiments show that VLM2Vec-V2\nachieves strong performance not only on the newly introduced video and document\nretrieval tasks, but also improves over prior baselines on the original image\nbenchmarks. Through extensive evaluation, our study offers insights into the\ngeneralizability of various multimodal embedding models and highlights\neffective strategies for unified embedding learning, laying the groundwork for\nmore scalable and adaptable representation learning in both research and\nreal-world settings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64778fb8168cb428e00f69b0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64778fb8168cb428e00f69b0/D_XYg74zHG9K3HUJj_gD4.jpeg",
      "fullname": "Ziyan Jiang",
      "name": "ziyjiang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03607",
      "authors": [
        {
          "_id": "686cb574364e2ad167eb54c3",
          "user": {
            "_id": "65e5bc754230174d547fa1dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xQ1NGVasxexuR_Hs0dvNl.jpeg",
            "isPro": false,
            "fullname": "Cédric",
            "user": "cedricbonhomme",
            "type": "user"
          },
          "name": "Cédric Bonhomme",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:05:41.165Z",
          "hidden": false
        },
        {
          "_id": "686cb574364e2ad167eb54c4",
          "user": {
            "_id": "677d08a57038d6b09078649a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jEUTW_G_VCgbvqfCxlNTj.png",
            "isPro": false,
            "fullname": "Dulaunoy",
            "user": "adulau",
            "type": "user"
          },
          "name": "Alexandre Dulaunoy",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:39:37.700Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65e5bc754230174d547fa1dc/S4_uGwFUDxVHZqJMljzP8.jpeg"
      ],
      "publishedAt": "2025-07-04T14:28:14.000Z",
      "submittedOnDailyAt": "2025-07-08T06:42:20.895Z",
      "title": "VLAI: ROBERTa 기반 모델에 의한 자동화 버그의 엄중도 분류",
      "submittedOnDailyBy": {
        "_id": "65e5bc754230174d547fa1dc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xQ1NGVasxexuR_Hs0dvNl.jpeg",
        "isPro": false,
        "fullname": "Cédric",
        "user": "cedricbonhomme",
        "type": "user"
      },
      "summary": "이 논문에서는 Transformer 기반의 VLAI 모델을 소개합니다. VLAI는 RoBERTa에 기반하여 만들어졌으며, 600,000 이상의 실제 세계적인 취약성 데이터로 학습되어, 취약성 효과 수준 예측에서 82% 이상의 정확도를 달성합니다. 이 모델은 자동 CVSS 점수링보다 빠르고 일관된 평가가 가능합니다. 모델과 데이터셋은 오픈 소스로, Vulnerability-Lookup 서비스에 통합되어 있습니다.",
      "upvotes": 4,
      "discussionId": "686cb574364e2ad167eb54c5",
      "projectPage": "https://www.vulnerability-lookup.org",
      "githubRepo": "https://github.com/vulnerability-lookup/VulnTrain",
      "ai_summary": "A transformer-based model predicts software vulnerability severity levels directly from text, enhancing triage efficiency and consistency.",
      "ai_keywords": [
        "transformer",
        "RoBERTa",
        "parameter-efficient fine-tuning",
        "predicting severity categories",
        "CVSS scoring",
        "open-source",
        "Vulnerability-Lookup service"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-07-04T10:28:14.000Z",
    "title": "VLAI: A RoBERTa-Based Model for Automated Vulnerability Severity\n  Classification",
    "summary": "This paper presents VLAI, a transformer-based model that predicts software\nvulnerability severity levels directly from text descriptions. Built on\nRoBERTa, VLAI is fine-tuned on over 600,000 real-world vulnerabilities and\nachieves over 82% accuracy in predicting severity categories, enabling faster\nand more consistent triage ahead of manual CVSS scoring. The model and dataset\nare open-source and integrated into the Vulnerability-Lookup service.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e5bc754230174d547fa1dc/S4_uGwFUDxVHZqJMljzP8.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03607.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e5bc754230174d547fa1dc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xQ1NGVasxexuR_Hs0dvNl.jpeg",
      "fullname": "Cédric",
      "name": "cedricbonhomme",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.04036",
      "authors": [
        {
          "_id": "686c92c5364e2ad167eb53fa",
          "name": "Jingwei Shi",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fb",
          "user": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "name": "Zeyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:20.306Z",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fc",
          "name": "Biao Wu",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fd",
          "name": "Yanjie Liang",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fe",
          "name": "Meng Fang",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53ff",
          "name": "Ling Chen",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb5400",
          "name": "Yang Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/Kwa3r-mdnTK5Itx86d1f6.mp4"
      ],
      "publishedAt": "2025-07-05T13:24:15.000Z",
      "submittedOnDailyAt": "2025-07-08T02:14:06.764Z",
      "title": "현재 에이전트: 프레젠테이션 비디오 생성을 위한 멀티모달 에이전트",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "プレゼンタジェント는 긴 문서를 ナレードプレゼンテーションビデオに変換する多モダルアグェントです。現在の手法は、静的スライドまたはテキスト要約に限定されているが、我々の方法はこれらの制限を超え、完全にシンクロニズムを取る可視的および口頭内容を生成し、人間のプレゼンテーションの風格に近似したものを作成します。この統合を実現するために、プレゼンタジェントは入力文書をシステミックに分割し、スライドモード의可視的フレームを計画し渲染し、大語言モデルと文脈テキストからサイドプロデュースモデルを使用してコンテキスト付きの口頭ナレーションを生成し、最終的なビデオを精密な音声可視認識によって無隙合わせます。このような多モダル出力の評価の複雑性を考慮して、我々は、ビジョン言語モデルをもちろんして、内容忠実性、可視性、プロンプトベース評価によるオーディエンス理解の3つの重要な次元での評価を行う統一評価フレームワークフレームワークを導入します。我々の実験的検証は、30ページのドキュメントプレゼンテーションペアのカレーレッドデータセットで行われ、プレゼンタジェントはすべての評価指標で人間レベルの品質を接近していることを示します。これらの結果は、静的なテキストマテリアルを動的、有效率、アクセス可能なプレゼンテーションフォーマットに変換する制御可能な多モダルアグェントの大きなポテンシャルを明らかにしています。コードは、https://github.com/AIGeeksGroup/PresentAgent から利用可能です。",
      "upvotes": 3,
      "discussionId": "686c92c6364e2ad167eb5401",
      "githubRepo": "https://github.com/AIGeeksGroup/PresentAgent",
      "ai_summary": "A multimodal agent transforms documents into detailed presentation videos with audio, evaluated using a comprehensive framework involving vision-language models.",
      "ai_keywords": [
        "multimodal agent",
        "narrated presentation videos",
        "static slides",
        "text summaries",
        "large language models",
        "Text-to-Speech models",
        "audio-visual alignment",
        "Vision-Language Models",
        "content fidelity",
        "visual clarity",
        "audience comprehension",
        "prompt-based evaluation"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-07-05T09:24:15.000Z",
    "title": "PresentAgent: Multimodal Agent for Presentation Video Generation",
    "summary": "We present PresentAgent, a multimodal agent that transforms long-form\ndocuments into narrated presentation videos. While existing approaches are\nlimited to generating static slides or text summaries, our method advances\nbeyond these limitations by producing fully synchronized visual and spoken\ncontent that closely mimics human-style presentations. To achieve this\nintegration, PresentAgent employs a modular pipeline that systematically\nsegments the input document, plans and renders slide-style visual frames,\ngenerates contextual spoken narration with large language models and\nText-to-Speech models, and seamlessly composes the final video with precise\naudio-visual alignment. Given the complexity of evaluating such multimodal\noutputs, we introduce PresentEval, a unified assessment framework powered by\nVision-Language Models that comprehensively scores videos across three critical\ndimensions: content fidelity, visual clarity, and audience comprehension\nthrough prompt-based evaluation. Our experimental validation on a curated\ndataset of 30 document-presentation pairs demonstrates that PresentAgent\napproaches human-level quality across all evaluation metrics. These results\nhighlight the significant potential of controllable multimodal agents in\ntransforming static textual materials into dynamic, effective, and accessible\npresentation formats. Code will be available at\nhttps://github.com/AIGeeksGroup/PresentAgent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/Kwa3r-mdnTK5Itx86d1f6.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04036.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05259",
      "authors": [
        {
          "_id": "686ca7ef364e2ad167eb544d",
          "user": {
            "_id": "6499eca0685215f7247bd5ce",
            "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
            "isPro": false,
            "fullname": "Chun-Hsiao Yeh",
            "user": "danielchyeh",
            "type": "user"
          },
          "name": "Chun-Hsiao Yeh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:03.462Z",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb544e",
          "name": "Yilin Wang",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb544f",
          "name": "Nanxuan Zhao",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5450",
          "name": "Richard Zhang",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5451",
          "name": "Yuheng Li",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5452",
          "name": "Yi Ma",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5453",
          "name": "Krishna Kumar Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T17:59:56.000Z",
      "submittedOnDailyAt": "2025-07-08T03:46:34.916Z",
      "title": "X-Planner는 복잡한 명령에 기반한 이미지 편집을 가능하게 하며, 단순한 편집이 아니라 복잡한 컨텍스트나 조건에 맞는 이미지 편집을 수행할 수 있는 도구입니다.",
      "submittedOnDailyBy": {
        "_id": "6499eca0685215f7247bd5ce",
        "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
        "isPro": false,
        "fullname": "Chun-Hsiao Yeh",
        "user": "danielchyeh",
        "type": "user"
      },
      "summary": "최근의 Difussion-based 이미지 편집 방법들은 텍스트를 기반으로 된 작업에서 상당한 진전을 보였지만, 복잡한, 간접적인 지시를 이해하는 것이 어려워, 현재의 모델은 아웃렛의 유지, 비 의의적인 편집, 또는 손동적 마스크의 중요성에 의해 어려움을 겪고 있습니다. 이러한 문제를 해결하기 위해, 우리는 X-Planner라는 Multimodal Large Language Model(MLLM)에 기반한 계획 시스템을 도입하여, 사용자의 의도와 편집 모델의 능력을 효과적으로 연결하고자 합니다. X-Planner는 chain-of-thought reasoning를 사용하여, 복잡한 지시를 간단하고 명확한 단계별로 분해합니다. 각 단계별 지시에 대해, X-Planner는 높은 정확도를 가진 편집 유형과 분할 마스크를 자동으로 생성하여, 손동적 참여를 제거하고, 국소적인 아웃렛을 유지한 편집을 보장합니다. 또한, X-Planner의 훈련을 위한 큰 규모의 데이터를 생성하는 새로운 자동 프로세스를 제안하여, 현재의 벤치마크와 함께 추가된 복잡한 편집 벤치마크에서 가장 先端한 결과를 얻습니다.",
      "upvotes": 2,
      "discussionId": "686ca7f0364e2ad167eb5454",
      "projectPage": "https://danielchyeh.github.io/x-planner/",
      "ai_summary": "X-Planner, a planning system utilizing a multimodal large language model, decomposes complex text-guided image editing instructions into precise sub-instructions, ensuring localized, identity-preserving edits and achieving top performance on established benchmarks.",
      "ai_keywords": [
        "Multimodal Large Language Model",
        "MLLM",
        "chain-of-thought reasoning",
        "segmentation masks",
        "image editing",
        "identity preservation",
        "automated pipeline"
      ]
    },
    "publishedAt": "2025-07-07T13:59:56.000Z",
    "title": "Beyond Simple Edits: X-Planner for Complex Instruction-Based Image\n  Editing",
    "summary": "Recent diffusion-based image editing methods have significantly advanced\ntext-guided tasks but often struggle to interpret complex, indirect\ninstructions. Moreover, current models frequently suffer from poor identity\npreservation, unintended edits, or rely heavily on manual masks. To address\nthese challenges, we introduce X-Planner, a Multimodal Large Language Model\n(MLLM)-based planning system that effectively bridges user intent with editing\nmodel capabilities. X-Planner employs chain-of-thought reasoning to\nsystematically decompose complex instructions into simpler, clear\nsub-instructions. For each sub-instruction, X-Planner automatically generates\nprecise edit types and segmentation masks, eliminating manual intervention and\nensuring localized, identity-preserving edits. Additionally, we propose a novel\nautomated pipeline for generating large-scale data to train X-Planner which\nachieves state-of-the-art results on both existing benchmarks and our newly\nintroduced complex editing benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05259.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6499eca0685215f7247bd5ce",
      "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
      "fullname": "Chun-Hsiao Yeh",
      "name": "danielchyeh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.04562",
      "authors": [
        {
          "_id": "686ca9e2364e2ad167eb5461",
          "name": "Janna Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-06T22:26:59.000Z",
      "submittedOnDailyAt": "2025-07-08T03:51:31.373Z",
      "title": "「실세계 예측에 대한 LLMs와 인간의 초예측자의 비교 평가」",
      "submittedOnDailyBy": {
        "_id": "66b3d98e040c500914ef558f",
        "avatarUrl": "/avatars/a90f8306dbd7747520ce5b941ee3bbcb.svg",
        "isPro": false,
        "fullname": "Janna",
        "user": "jannalu",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 다양한 태스크에서 놀라울 정도로 뛰어난 능력을 보여주지만, 미래의 이벤트를 예측하는 능력은 연구가 부족합니다. 1년 전, 대 언어 모델은 인간 집단의 정확도를 도달하는 데 어려움을 겪었습니다. Metaculus에서 464건의 예측 문제를 평가하여 가장 先端의 LLMs를 평가하고, 그 성능을 인간의 초예측자와 비교했습니다. Frontier 모델은 Brier 스코어를 초과하는 것처럼 보이지만, 초예측자 그룹과 비교하여 뚜렷하게 떨어집니다.",
      "upvotes": 1,
      "discussionId": "686ca9e2364e2ad167eb5462",
      "ai_summary": "State-of-the-art large language models are evaluated on forecasting questions and show lower accuracy compared to human superforecasters.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "Brier scores",
        "human superforecasters"
      ]
    },
    "publishedAt": "2025-07-06T18:26:59.000Z",
    "title": "Evaluating LLMs on Real-World Forecasting Against Human Superforecasters",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their ability to forecast future events remains\nunderstudied. A year ago, large language models struggle to come close to the\naccuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting\nquestions from Metaculus, comparing their performance against human\nsuperforecasters. Frontier models achieve Brier scores that ostensibly surpass\nthe human crowd but still significantly underperform a group of\nsuperforecasters.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04562.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b3d98e040c500914ef558f",
      "avatarUrl": "/avatars/a90f8306dbd7747520ce5b941ee3bbcb.svg",
      "fullname": "Janna",
      "name": "jannalu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04376",
      "authors": [
        {
          "_id": "686cb0ab364e2ad167eb54a1",
          "name": "Georgios Ioannides",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a2",
          "name": "Christos Constantinou",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a3",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a4",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:05:49.355Z",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a5",
          "name": "Aaron Elkins",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-06T12:46:57.000Z",
      "submittedOnDailyAt": "2025-07-08T04:20:01.196Z",
      "title": "MOD-X: 모듈화 오픈 분산형 교환 프레임워크의 제안\n  다양한 오버라이드 가능한 인공 에이전트의 상호 교환성",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "AI 시스템이 단일 모델에서 특수화된 에이전트의 생태계로 이동하는 동안, 표준화된 통신 프로토콜의 필요성은 점점 더 중요해지고 있습니다. 이 논문에서는, 현재 프로토콜의 주요한 한계를 해결하기 위한 새로운 아키텍처 프레임워크 MOD-X(Modular Open Decentralized eXchange)를 소개합니다. 현재의 접근 방식과 달리, MOD-X는 Universal Message Bus, 세부적인 상태 관리, 번역 기능, 블록체인 기반의 보안 구조를 적용한 레이어 구조를 제안하고 있습니다. MOD-X의 아키텍처를 소개하고, 현재의 프로토콜과 비교하며, 다른 아키텍처, 비즈니스, 능력, 지식 표현의 유능한 에이전트(규칙 기반 시스템, 신경망, 기호 논리 엔진, 유전 소프트웨어)의 통합을 가능하게 하는 워크젝션을 통해, 이 아키텍처의 실용적인 구현에서의 기능들을 보여줍니다. MOD-X의 주요 혁신점은, 제품 서브스ク립 커뮤니케이션 모델, 의미적인 능력 발견, 동적인 워크 플로우 아키텍처를 포함하며, 이론적인 형식론과 실용적인 구현을 연결하는 프레임워크를 제공합니다. 이 아키텍처는, 중앙조정이 필요하지 않은 한, 효과적인 스케일링을 가능하게 하는 진정으로 분산된, 교환 가능한 에이전트 생태계의 확장을 위해 필요합니다.",
      "upvotes": 1,
      "discussionId": "686cb0ab364e2ad167eb54a6"
    },
    "publishedAt": "2025-07-06T08:46:57.000Z",
    "title": "MOD-X: A Modular Open Decentralized eXchange Framework proposal for\n  Heterogeneous Interoperable Artificial Agents",
    "summary": "As Artificial Intelligence systems evolve from monolithic models to\necosystems of specialized agents, the need for standardized communication\nprotocols becomes increasingly critical. This paper introduces MOD-X (Modular\nOpen Decentralized eXchange), a novel architectural framework proposal for\nagent interoperability that addresses key limitations of existing protocols.\nUnlike current approaches, MOD-X proposes a layered architecture with a\nUniversal Message Bus, thorough state management, translation capabilities, and\nblockchain-based security mechanisms. We present MOD-X's architecture, compare\nit with existing protocols, and demonstrate its application through a worked\nexample how it enables integration between heterogeneous specialist agents\n(agents with different architectures, vendors, capabilities, and knowledge\nrepresentations--including rule-based systems, neural networks, symbolic\nreasoning engines, and legacy software with agent wrappers). MOD-X's key\ninnovations include a publish-subscribe communication model, semantic\ncapability discovery, and dynamic workflow orchestration--providing a framework\nthat bridges theoretical formalism with practical implementation. This\narchitecture addresses the growing need for truly decentralized, interoperable\nagent ecosystems that can scale effectively without the need for central\ncoordination.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03336",
      "authors": [
        {
          "_id": "686c67e2364e2ad167eb5314",
          "user": {
            "_id": "637859f98f288aba3d01f588",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637859f98f288aba3d01f588/eP8YNMOtxTvxH-rYEYn4f.png",
            "isPro": false,
            "fullname": "Ashutosh Hathidara",
            "user": "ashutosh1919",
            "type": "user"
          },
          "name": "Ashutosh Hathidara",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:34.896Z",
          "hidden": false
        },
        {
          "_id": "686c67e2364e2ad167eb5315",
          "name": "Julien Yu",
          "hidden": false
        },
        {
          "_id": "686c67e2364e2ad167eb5316",
          "name": "Sebastian Schreiber",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T06:49:02.000Z",
      "submittedOnDailyAt": "2025-07-08T02:47:33.416Z",
      "title": "디스안바지션센터를 갖춘 미세 조정이 기업 도구 호출을 위한 LLM에 실제 가까워져 위험을 줄일 수 있습니다.",
      "submittedOnDailyBy": {
        "_id": "637859f98f288aba3d01f588",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637859f98f288aba3d01f588/eP8YNMOtxTvxH-rYEYn4f.png",
        "isPro": false,
        "fullname": "Ashutosh Hathidara",
        "user": "ashutosh1919",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 기업의 API를 호출하는 작업에 의해 증가하고 있습니다 however, 유사한 도구가 동일한 사용자의 의도에서 경쟁하거나 필요한 인수가 부족한 경우, 예를 들어, 항상 실패합니다. 여기서는, DiaFORGE(다이아포르지, 오픈소스 응답 생성 및 평가의 대화 프레임워크)를 소개합니다. DiaFORGE는 3단계의 파이프라인으로, 다이버지언스 시너지적입니다. 또한, 이는 (i) 보조자가 유사한 도구를 구분하기 위해 전문적인 다턴 다이아로그를 합성하고 (ii) 3B - 70B 파라미터의 오픈소스 모델에 이유 기록을 포함하는 서브 객체 조정을 수행하고 (iii) 실세계의 준비도를 평가하기 위한 동적인 세트입니다. Dynamic Benchmark인 DiaBENCH에서, DiaFORGE로 훈련된 모델은 최적화된 Prompt에 따라, GPT-4o를 27pp, Claude-3.5-Sonnet를 49pp 높입니다. 또한, DiaFORGE에서는 도구 호출 성공률을 향상시킵니다. 또한, DiaFORGE에서는 5000건의 기업 API의 제품 엔진规格와 엄격하게 검증된 다이버지언스 포커스 다이아로그를 페어로 오픈 코퍼스를 릴리스하고, 신뢰성 있는, 기업 준비된 도구 호출 에이전트의 구축을 위한 실질적인 계획을 제공합니다.",
      "upvotes": 1,
      "discussionId": "686c67e2364e2ad167eb5317",
      "ai_summary": "DiaFORGE is a disambiguation framework that enhances large language models' ability to invoke enterprise APIs accurately through dialogue synthesis, supervised fine-tuning, and real-world evaluation.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "enterprise APIs",
        "persona-driven",
        "multi-turn dialogues",
        "supervised fine-tuning",
        "reasoning traces",
        "end-to-end goal completion",
        "dynamic benchmark",
        "tool-invocation success",
        "disambiguation-focused dialogues"
      ]
    },
    "publishedAt": "2025-07-04T02:49:02.000Z",
    "title": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky",
    "summary": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03336.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637859f98f288aba3d01f588",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637859f98f288aba3d01f588/eP8YNMOtxTvxH-rYEYn4f.png",
      "fullname": "Ashutosh Hathidara",
      "name": "ashutosh1919",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.02659",
      "authors": [
        {
          "_id": "686736ed9db35afc9c304cea",
          "name": "Ramchalam Kinattinkara Ramakrishnan",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304ceb",
          "user": {
            "_id": "65d989790733541e06823258",
            "avatarUrl": "/avatars/cf658c5f586c02e9b1d610b5e49b2826.svg",
            "isPro": false,
            "fullname": "Zhaocong Yuan",
            "user": "justinyyy",
            "type": "user"
          },
          "name": "Zhaocong Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-07T15:46:47.568Z",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cec",
          "name": "Shaojie Zhuo",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304ced",
          "name": "Chen Feng",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cee",
          "name": "Yicheng Lin",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cef",
          "name": "Chenzheng Su",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cf0",
          "name": "Xiaopeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T14:20:41.000Z",
      "submittedOnDailyAt": "2025-07-08T02:34:16.093Z",
      "title": "オムニドラフト: 온라인으로 적용 가능한 크로스보키아리언 데코딩를 위한 온디어모용 특수 디코딩 도구\n\n(Note: The translation is provided as requested, adhering to the specified format and without additional explanations or text.)",
      "submittedOnDailyBy": {
        "_id": "65d989790733541e06823258",
        "avatarUrl": "/avatars/cf658c5f586c02e9b1d610b5e49b2826.svg",
        "isPro": false,
        "fullname": "Zhaocong Yuan",
        "user": "justinyyy",
        "type": "user"
      },
      "summary": "スペシュラティブデコーディング은 일반적으로 작은 효율적인 드라프트 모델을 사용하는 것을 권장하고 있습니다. 이 모델은 특정 타겟 모델 시리즈(예: Llama, Qwen 모델)에 미리 학습되거나 낡아진 것입니다. 그러나 온라인 배치 설정에서 두 가지 큰 문제점이 있습니다: 1) 드라프트 모델과 맞지 않는 타겟 모델의 사용; 2) 사용과 시간에 따라 레이디언의 향상을 기대합니다. 본 논문에서는 OmniDraft라는 노이드 프레임워크를 제안하여, 드라프트 모델이 모든 타겟 모델과 동적으로 대응할 수 있도록 만들 수 있습니다. 또한, 드라프트 모델과 타겟 모델의 크로스 버킷 미스 매치 문제를 해결하기 위해 조합 낡아진 최종 튜닝을 도입하고, 적응 드라프트 기술을 활용하여 디코딩 속도를 향상시킵니다. OmniDraft는 모델 비용, 효과성, 사용자 맞춤형 주요 논의점이 있는 경우, 장치 상의 LLM 애플리케이션에 특히 적합합니다. 이는 위의 문제를 해결하는 필요성을 더욱 명확히 하고, \"하나의 드라프트 타르\" 패러다임에 맞추는 것입니다. 수학 계산, 코딩, 텍스트 생성의 태스크에서 온라인 학습을 통해 OmniDraft 프레임워크의 효과를 보여주고 있습니다. 특히, OmniDraft는 Llama-68M 모델을 하나만으로, Vicuna-7B, Qwen2-7B, Llama3-8B 모델 등 다양한 타겟 모델과 특수화된 디코딩을 수행할 수 있으며, 1.5-2배의 속도 업그레이드를 제공합니다.",
      "upvotes": 0,
      "discussionId": "686736ed9db35afc9c304cf1",
      "ai_summary": "OmniDraft, a unified framework, addresses cross-vocabulary mismatch and improves decoding speed by allowing a single draft model to interact dynamically with diverse target models in online settings.",
      "ai_keywords": [
        "n-gram cache",
        "hybrid distillation fine-tuning",
        "adaptive drafting",
        "on-device LLM applications"
      ]
    },
    "publishedAt": "2025-07-03T10:20:41.000Z",
    "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
    "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the ``one drafter for all'' paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02659.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d989790733541e06823258",
      "avatarUrl": "/avatars/cf658c5f586c02e9b1d610b5e49b2826.svg",
      "fullname": "Zhaocong Yuan",
      "name": "justinyyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]