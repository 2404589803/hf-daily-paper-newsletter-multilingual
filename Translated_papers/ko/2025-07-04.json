[
  {
    "paper": {
      "id": "2507.02813",
      "authors": [
        {
          "_id": "686735e69db35afc9c304ce1",
          "name": "Fangfu Liu",
          "hidden": false
        },
        {
          "_id": "686735e69db35afc9c304ce2",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "686735e69db35afc9c304ce3",
          "name": "Jiawei Chi",
          "hidden": false
        },
        {
          "_id": "686735e69db35afc9c304ce4",
          "user": {
            "_id": "65c38f6c137aba2aee524989",
            "avatarUrl": "/avatars/a93e29f55876df3e65e2532972e057e4.svg",
            "isPro": false,
            "fullname": "Hanyang Wang",
            "user": "hanyang-21",
            "type": "user"
          },
          "name": "Hanyang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:49:31.644Z",
          "hidden": false
        },
        {
          "_id": "686735e69db35afc9c304ce5",
          "name": "Minghui Yang",
          "hidden": false
        },
        {
          "_id": "686735e69db35afc9c304ce6",
          "name": "Fudong Wang",
          "hidden": false
        },
        {
          "_id": "686735e69db35afc9c304ce7",
          "name": "Yueqi Duan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6505a02f9310ce8c400edc63/GMjqJ0RyCYifjpsev_YPY.mp4"
      ],
      "publishedAt": "2025-07-03T17:21:23.000Z",
      "submittedOnDailyAt": "2025-07-04T00:35:47.996Z",
      "title": "LangScene-X: 3차원 언어 삽입 스키멤을 확장 가능한 재구성을 위한 TriMap 비디오 디퓨저\n\n(注意：虽然要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，这里提供了一个更符合韩国语表达习惯的翻译版本。如果需要严格遵循原始文本的翻译，请告知。)",
      "submittedOnDailyBy": {
        "_id": "6505a02f9310ce8c400edc63",
        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
        "isPro": false,
        "fullname": "Fangfu Liu",
        "user": "Liuff23",
        "type": "user"
      },
      "summary": "3D 구조를 2D 이미지로부터 개방형 벡베라리리의 스케인 이해에 의한 복원은 기본적인 일이지만, 어렵고 어려운 임무입니다. 최근의 발전은, 언어 정보가 내장된 것을 사용하여 각 스케인별로 최적화를 수행하여 이를 실현했습니다. 그러나, 이들은 조정된 밀도점의 재구성 패러다임에 의해, 제한된 시각이 있을 때严峻的 렌더링artifact와 불실한 의미 합성으로 고통받습니다. 본 논문에서는, LangScene-X라는 새로운 생성 프레임워크를 소개하며, 재구성과 이해에 대한 3D 일치된 다양성 정보를 통합하여 생성하는 것을 목표로 합니다. 이 프레임워크는, 더 일치된 것을 생성하는 새로운 관찰을 갖으며, 제한된 시각으로부터 일반화 가능한 3D 언어 내장 스케인을 구축할 수 있습니다. 특히, TriMap 비디오DIFUJSION 모델을 학습하여, 희소한 입력으로부터 외관(RGB), 기하학(노르말), 의미(분할맵)을 생성할 수 있습니다. 또한, 대규모 이미지 데이터 세트를 사용하여 학습한 언어ク오ン테이션 컴프리헨서(LQC)를 제안하며, 언어 내장을 효율적으로 압축하여, 스케인별로 재학습을 피할 수 있는 크로스 스케인 일반화를 가능하게 합니다. 마지막으로, 3D 스케인의 표면 위에 언어 정보를 매핑하여, 개방형 엔드의 언어 쿼리에 대응하는 언어 표면 필드을 재구성합니다. 실제 세계의 데이터에 대한 확장된 실험에서, LangScene-X가 가장 선진한 방법보다도 최상위의 질과 일반화 능력을 보여주었습니다. 프로젝트 페이지: https://liuff19.github.io/LangScene-X.",
      "upvotes": 35,
      "discussionId": "686735e69db35afc9c304ce8",
      "projectPage": "https://liuff19.github.io/LangScene-X/",
      "githubRepo": "https://github.com/liuff19/LangScene-X/",
      "ai_summary": "A novel generative framework named LangScene-X unifies and generates 3D consistent information from sparse views using a TriMap video diffusion model and Language Quantized Compressor for high-quality scene reconstruction and understanding.",
      "ai_keywords": [
        "TriMap video diffusion model",
        "appearance (RGBs)",
        "geometry (normals)",
        "semantics (segmentation maps)",
        "progressive knowledge integration",
        "Language Quantized Compressor",
        "language surface fields",
        "open-ended language queries"
      ],
      "githubStars": 42
    },
    "publishedAt": "2025-07-03T13:21:23.000Z",
    "title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with\n  TriMap Video Diffusion",
    "summary": "Recovering 3D structures with open-vocabulary scene understanding from 2D\nimages is a fundamental but daunting task. Recent developments have achieved\nthis by performing per-scene optimization with embedded language information.\nHowever, they heavily rely on the calibrated dense-view reconstruction\nparadigm, thereby suffering from severe rendering artifacts and implausible\nsemantic synthesis when limited views are available. In this paper, we\nintroduce a novel generative framework, coined LangScene-X, to unify and\ngenerate 3D consistent multi-modality information for reconstruction and\nunderstanding. Powered by the generative capability of creating more consistent\nnovel observations, we can build generalizable 3D language-embedded scenes from\nonly sparse views. Specifically, we first train a TriMap video diffusion model\nthat can generate appearance (RGBs), geometry (normals), and semantics\n(segmentation maps) from sparse inputs through progressive knowledge\nintegration. Furthermore, we propose a Language Quantized Compressor (LQC),\ntrained on large-scale image datasets, to efficiently encode language\nembeddings, enabling cross-scene generalization without per-scene retraining.\nFinally, we reconstruct the language surface fields by aligning language\ninformation onto the surface of 3D scenes, enabling open-ended language\nqueries. Extensive experiments on real-world data demonstrate the superiority\nof our LangScene-X over state-of-the-art methods in terms of quality and\ngeneralizability. Project Page: https://liuff19.github.io/LangScene-X.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6505a02f9310ce8c400edc63/GMjqJ0RyCYifjpsev_YPY.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6505a02f9310ce8c400edc63",
      "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
      "fullname": "Fangfu Liu",
      "name": "Liuff23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02025",
      "authors": [
        {
          "_id": "686738019db35afc9c304cf3",
          "name": "The IntFold Team",
          "hidden": false
        },
        {
          "_id": "686738019db35afc9c304cf4",
          "name": "Leon Qiao",
          "hidden": false
        },
        {
          "_id": "686738019db35afc9c304cf5",
          "name": "Wayne Bai",
          "hidden": false
        },
        {
          "_id": "686738019db35afc9c304cf6",
          "name": "He Yan",
          "hidden": false
        },
        {
          "_id": "686738019db35afc9c304cf7",
          "user": {
            "_id": "63dd2bacea4d39995f581222",
            "avatarUrl": "/avatars/c117b66fdc1bcf3eed218b0b66e958cb.svg",
            "isPro": false,
            "fullname": "Liu",
            "user": "FuxuLiu",
            "type": "user"
          },
          "name": "Gary Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:49:15.140Z",
          "hidden": true
        },
        {
          "_id": "686738019db35afc9c304cf8",
          "name": "Nova Xi",
          "hidden": false
        },
        {
          "_id": "686738019db35afc9c304cf9",
          "name": "Xiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T16:09:47.000Z",
      "submittedOnDailyAt": "2025-07-04T03:52:28.488Z",
      "title": "IntFold: 일반 및 전문적인 바이오모르코ン 구조 예측을 제어 가능한 기초 모델",
      "submittedOnDailyBy": {
        "_id": "67d3a1a5943a965360fcae51",
        "avatarUrl": "/avatars/165ed684b0750e7f57b9f2babfb47a8c.svg",
        "isPro": false,
        "fullname": "Siqi Sun",
        "user": "siqisun",
        "type": "user"
      },
      "summary": "IntFold는 일반적인 및 특수화된 생물분자 구조 예측에 대한 제어 가능한 기초 모델입니다. IntFold는 최신의 AlphaFold3와 비교하여 높은 예측 정확도를 보여주고, 우수한 사용자定制된 注意力机制를 사용합니다. 표준의 구조 예측보다 IntFold는 개별의 Adapter를 사용하여 알레르기반응 상태, 제약 구조, 결합의 최종 상태의 예측에 적응할 수 있습니다. 또한 새로운 신뢰도 헤드를 소개하고, Antibody-Antigen 복합체와 같은 어려운 목표에 대한 더 복잡한 평가를 제공합니다. 마지막으로, 이 많은 계산량의 모델의 훈련 과정에서 얻은 피드백을 공유합니다.",
      "upvotes": 30,
      "discussionId": "686738029db35afc9c304cfa",
      "projectPage": "https://server.intfold.com/",
      "githubRepo": "https://github.com/IntelliGen-AI/IntFold",
      "ai_summary": "IntFold uses a customized attention kernel for biomolecular structure prediction, surpassing AlphaFold3, and includes adapters and a novel confidence head for specialized predictions and docking assessments.",
      "ai_keywords": [
        "controllable foundation model",
        "biomolecular structure prediction",
        "AlphaFold3",
        "attention kernel",
        "allosteric states",
        "constrained structures",
        "binding affinity",
        "adapters",
        "confidence head",
        "docking quality",
        "antibody-antigen complexes"
      ],
      "githubStars": 17
    },
    "publishedAt": "2025-07-02T12:09:47.000Z",
    "title": "IntFold: A Controllable Foundation Model for General and Specialized\n  Biomolecular Structure Prediction",
    "summary": "We introduce IntFold, a controllable foundation model for both general and\nspecialized biomolecular structure prediction. IntFold demonstrates predictive\naccuracy comparable to the state-of-the-art AlphaFold3, while utilizing a\nsuperior customized attention kernel. Beyond standard structure prediction,\nIntFold can be adapted to predict allosteric states, constrained structures,\nand binding affinity through the use of individual adapters. Furthermore, we\nintroduce a novel confidence head to estimate docking quality, offering a more\nnuanced assessment for challenging targets such as antibody-antigen complexes.\nFinally, we share insights gained during the training process of this\ncomputationally intensive model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02025.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67d3a1a5943a965360fcae51",
      "avatarUrl": "/avatars/165ed684b0750e7f57b9f2babfb47a8c.svg",
      "fullname": "Siqi Sun",
      "name": "siqisun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02592",
      "authors": [
        {
          "_id": "686732329db35afc9c304cb4",
          "name": "Kuan Li",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cb5",
          "name": "Zhongwang Zhang",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cb6",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cb7",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cb8",
          "name": "Litu Ou",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cb9",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cba",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cbb",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cbc",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cbd",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cbe",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cbf",
          "name": "Junkai Zhang",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cc0",
          "name": "Dingchu Zhang",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cc1",
          "user": {
            "_id": "6622132f63598534f96ca29d",
            "avatarUrl": "/avatars/34e61fc3101f8ebce1ef7041f761e108.svg",
            "isPro": false,
            "fullname": "Xixi Wu",
            "user": "xxwu",
            "type": "user"
          },
          "name": "Xixi Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:49:47.564Z",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cc2",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cc3",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cc4",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cc5",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "686732329db35afc9c304cc6",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/622f2feea32d46b4be9ed8c4/9sUyYbIYfR5wDQMMft1io.png",
        "https://cdn-uploads.huggingface.co/production/uploads/622f2feea32d46b4be9ed8c4/FLz6T05o_NbbQKrOQyTuL.png"
      ],
      "publishedAt": "2025-07-03T12:59:07.000Z",
      "submittedOnDailyAt": "2025-07-04T00:20:59.450Z",
      "title": "웹서로이: 네트워크 에이전트의 초인레지언싱을 가이드합니다.",
      "submittedOnDailyBy": {
        "_id": "622f2feea32d46b4be9ed8c4",
        "avatarUrl": "/avatars/ba25eb941a7c9a414b7fd4818adfa26b.svg",
        "isPro": false,
        "fullname": "Litu Ou",
        "user": "learn3r",
        "type": "user"
      },
      "summary": "인간의 인지 한계를 초월하는 것은 LLM의 훈련에서 중요한 경계입니다. DeepResearch와 같은 소유의 에이전트 시스템은 BrowseComp와 같은 매우 복잡한 정보 탐색 벤치마크에서 초상적인 능력을 보여주고, 이전에 달성할 수 없었던 기록을 달성했습니다. 우리는 그 성공은 오픈 소스 모델에 존재하지 않는 복잡한 추론 패턴을 기반으로 있다고 가정합니다: 정보의 규모를 따라 이동할 때 극한 불확실성을 체계적으로 줄이기 위한 능력입니다. 이러한 통찰에 기초하여, 우리는 WebSailor라는 완전한 후 훈련 메소드 로직을 소개하고, 이 중요한 능력을 습득시킬 것을 목표로 합니다. 우리의 접근 방식은 구조화된 샘플링과 정보의 혼돈, RFT의 코드 시작, 그리고 효율적인 에이전트 시스템의 RL 훈련 알고리즘, 그리고 새로운 태스크의 생성을 위해 사용된 Duplicating Sampling Policy Optimization(DUPO)를 기반으로 합니다. 이 통합 프로파일을 사용함으로써, WebSailor는 모든 오픈 소스 에이전트를 초과하고, 소유 에이전트의 성능을 경쟁하며, 능력의 차이를 줄이는 복잡한 정보 탐색 태스크에서 수행합니다.",
      "upvotes": 29,
      "discussionId": "686732339db35afc9c304cc7",
      "projectPage": "https://github.com/Alibaba-NLP/WebAgent",
      "githubRepo": "https://github.com/Alibaba-NLP/WebAgent/",
      "ai_summary": "WebSailor, a post-training methodology involving structured sampling, information obfuscation, and an efficient RL algorithm, enhances LLMs by improving their reasoning capabilities in complex information-seeking tasks to match proprietary agents.",
      "ai_keywords": [
        "LLM",
        "DeepResearch",
        "BrowseComp",
        "reasoning pattern",
        "high-uncertainty tasks",
        "structured sampling",
        "information obfuscation",
        "RFT cold start",
        "agentic RL",
        "Duplicating Sampling Policy Optimization",
        "DUPO",
        "opensource agents",
        "complex information-seeking tasks",
        "capability gap"
      ],
      "githubStars": 1262
    },
    "publishedAt": "2025-07-03T08:59:07.000Z",
    "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
    "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/622f2feea32d46b4be9ed8c4/9sUyYbIYfR5wDQMMft1io.png",
      "https://cdn-uploads.huggingface.co/production/uploads/622f2feea32d46b4be9ed8c4/FLz6T05o_NbbQKrOQyTuL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02592.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622f2feea32d46b4be9ed8c4",
      "avatarUrl": "/avatars/ba25eb941a7c9a414b7fd4818adfa26b.svg",
      "fullname": "Litu Ou",
      "name": "learn3r",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01352",
      "authors": [
        {
          "_id": "6865cdc28c83dab5f72d1e18",
          "user": {
            "_id": "658229ef5f6d83438257fce5",
            "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
            "isPro": false,
            "fullname": "Chris (Yuhao) Liu",
            "user": "chrisliu298",
            "type": "user"
          },
          "name": "Chris Yuhao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:50:33.676Z",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e19",
          "user": {
            "_id": "6621efe1a6eec3ad03e38759",
            "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
            "isPro": false,
            "fullname": "Liang Zeng",
            "user": "zengliangcs",
            "type": "user"
          },
          "name": "Liang Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T16:19:08.221Z",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e1a",
          "user": {
            "_id": "658ceb595a8f8a309ea417a1",
            "avatarUrl": "/avatars/43bfa8c919aa802f2611439ebb7430b8.svg",
            "isPro": false,
            "fullname": "Ricky Shaw",
            "user": "RickyShaw999",
            "type": "user"
          },
          "name": "Yuzhen Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:50:29.586Z",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e1b",
          "name": "Jujie He",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e1c",
          "name": "Jiacai Liu",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e1d",
          "name": "Chaojie Wang",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e1e",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e1f",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e20",
          "name": "Fuxiang Zhang",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e21",
          "name": "Jiacheng Xu",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e22",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6865cdc28c83dab5f72d1e23",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T04:40:29.000Z",
      "submittedOnDailyAt": "2025-07-04T00:12:17.371Z",
      "title": "Skywork-Reward-V2: 인간-AI의 협업으로 선호 데이터의 확장과 정리\n\n(Note: The translation is provided as requested, maintaining professionalism and accuracy.)",
      "submittedOnDailyBy": {
        "_id": "658229ef5f6d83438257fce5",
        "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
        "isPro": false,
        "fullname": "Chris (Yuhao) Liu",
        "user": "chrisliu298",
        "type": "user"
      },
      "summary": "쾌활한 모델(RMs)은 인간의 피드백(RLHF)에 의한 강화학습에서 중요한 역할을 수행하지만, 현재의 가장 先端的开放 모델은 많은 기존 평가 벤치마크에서 성능이 낮고, 복잡한 인간의 취미의 범위를 감지하지 못합니다. 발전적인 훈련 방법론을 적용한 접근도 의미 있는 성능 향상을 얻지 않습니다. 우리는 이러한 취약성은 취미 데이터 세트의 제한으로 오는 것을 가정하고 있습니다. 이러한 도전을 해결하기 위해, 우리는 4000만 개의 취미 페어를 포함하는 대규모 취미 데이터 세트를 소개합니다. 이 데이터 세트를 설계하기 위해, 인간과 AI의 협업을 활용한 2단계 파이프 라인을 설계했습니다. 이 파이프 라인에서, 인간은 확인된 注釈를 제공하고, 대규모 언어 모델은 인간의 가이드라인에 따라 자동적인 분류를 수행합니다. 이 취미 데이터 세트의 일부를 사용하여 훈련을 수행했습니다. Skywork-Reward-V2는 0.6B에서 8B 파라미터의 8개의 보상 모델의 세트로, SynPref-40M에서 2600만 개의 취미 페어의 일부를 사용하여 훈련되었습니다. Skywork-Reward-V2는 인간의 취미에 맞게, 목적의 정확성, 안전성, 스타일이론적 편향의 저항성, N의 최고의 선택으로 인한 스케일링 등 광범위한 능력을 보여주며, 7개의 주요 보상 모델 벤치마크에서 가장 先端의 성능을 달성했습니다. 제거 조사는 우리 접근 방식의 효과는 데이터 크기뿐만 아니라, 고품질의 분류에 의한 것이라는 것을 확인했습니다. Skywork-Reward-V2 시리즈는 오픈 모델의 발전을 상징하며, 현재의 취미 데이터 세트의 개발된 잠재력을 보여주고, 인간과 AI의 분류의 협업을 통해 고품질의 데이터를 개발하는 것을 보여줍니다.",
      "upvotes": 25,
      "discussionId": "6865cdc28c83dab5f72d1e24",
      "githubRepo": "https://github.com/SkyworkAI/Skywork-Reward-V2",
      "ai_summary": "A large-scale preference dataset and synergistic human-AI curation pipeline improve the quality and performance of open reward models in reinforcement learning from human feedback.",
      "ai_keywords": [
        "reward models",
        "RLHF",
        "preference datasets",
        "human-AI synergistic pipeline",
        "large language models",
        "best-of-N scaling"
      ],
      "githubStars": 16
    },
    "publishedAt": "2025-07-02T00:40:29.000Z",
    "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy",
    "summary": "Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01352.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "658229ef5f6d83438257fce5",
      "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
      "fullname": "Chris (Yuhao) Liu",
      "name": "chrisliu298",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23918",
      "authors": [
        {
          "_id": "68674e689db35afc9c304d4c",
          "name": "Zhaochen Su",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d4d",
          "name": "Peng Xia",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d4e",
          "name": "Hangyu Guo",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d4f",
          "name": "Zhenhua Liu",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d50",
          "name": "Yan Ma",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d51",
          "user": {
            "_id": "64cb54da1af278541d663708",
            "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
            "isPro": false,
            "fullname": "Xiaoye Qu",
            "user": "Xiaoye08",
            "type": "user"
          },
          "name": "Xiaoye Qu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:48:48.311Z",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d52",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d53",
          "name": "Yanshu Li",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d54",
          "name": "Kaide Zeng",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d55",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d56",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d57",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d58",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d59",
          "name": "Junxian He",
          "hidden": false
        },
        {
          "_id": "68674e689db35afc9c304d5a",
          "name": "Yi R. Fung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T14:48:35.000Z",
      "submittedOnDailyAt": "2025-07-04T02:20:13.561Z",
      "title": "화상 사용된 다모달 논리의 기초, 방법 및 미래의 선두",
      "submittedOnDailyBy": {
        "_id": "64264095ba51f8a2136946a0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
        "isPro": false,
        "fullname": "Zhaochen Su",
        "user": "Warrieryes",
        "type": "user"
      },
      "summary": "최근, 다모렐럴 로직의 발전은 언어 내 로직을 수행하는 언어 체인 오프스톡(CoT)의 패러다임으로 뚜렷하게 발전했습니다. 그러나 이 언어 중심적인 접근 방식은 시각을 정적 초기 컨텍스트로 취급하고, 풍부한 시각 데이터와 분산적인 표지적 기억 사이에 근본적인 \"언어적 간극\"을 발생시킵니다. 인간의 인지는 언어를 초월하여 시각을 동적인 마음의 스케치북처럼 활용합니다. 이러한 진화는 현재도 AI에서도 진행되고 있습니다. 이는 이미지에 대한 모델이 아니라 실제 이미지와 함께 기억을 하는 모델의 기본적인 패러다임의 변경을 기록하고 있습니다. 이 새로운 패러다임은 시각 정보를 기억의 진행의 중간 단계로 활용하는 모델이 시각을 패소폰적인 입력으로 생각한 것을 동적으로 조작 가능한 인지 공간으로 변환하는 것을 특징적으로 합니다. 이 조사에서는 인지 자율성이 증가하는 경로로 지능의 진화를 조사하고, 외부 도구 탐색에서 프로그래밍적인 조작까지 고유의 상상까지 3가지의 관련된 단계를 통해 진행됩니다. 이 급속히 발전하는 분야를 구축하기 위해, 우리의 조사는 4가지의 중요한 기여를 제공합니다. (1) 시각과 함께 기억하는 패러다임의 기본적인 원칙과 3단계의 프레임워크를 확립합니다. (2) 이 프로토콜의 각 단계에 특화된 핵심 메소드를 검토합니다. (3) 평가 벤치마크와 변환적인 응용의 중요한 계획을 분석합니다. (4) 어려움을 특정하고 미래의 가능성을 보여주는 프로젝트 방향을 명확히 합니다. 이 구조화된 개요를 제공함으로써, 향후 연구에서 더 강력한 인간 적합성의 다모렐럴 AI를 명확하게 프로그램하기 위한 明確な 프로그램을 제공하기를 목표로 합니다.",
      "upvotes": 20,
      "discussionId": "68674e699db35afc9c304d5b",
      "githubRepo": "https://github.com/zhaochen0110/Awesome_Think_With_Images",
      "ai_summary": "Multimodal reasoning models are transitioning from static text-based vision to dynamic, integrated use of visual information as part of their cognitive processes.",
      "ai_keywords": [
        "Chain-of-Thought",
        "CoT",
        "multimodal reasoning",
        "dynamic mental sketchpad",
        "cognitive workspace",
        "think with image",
        "programmatic manipulation",
        "intrinsic imagination"
      ],
      "githubStars": 534
    },
    "publishedAt": "2025-06-30T10:48:35.000Z",
    "title": "Thinking with Images for Multimodal Reasoning: Foundations, Methods, and\n  Future Frontiers",
    "summary": "Recent progress in multimodal reasoning has been significantly advanced by\ntextual Chain-of-Thought (CoT), a paradigm where models conduct reasoning\nwithin language. This text-centric approach, however, treats vision as a\nstatic, initial context, creating a fundamental \"semantic gap\" between rich\nperceptual data and discrete symbolic thought. Human cognition often transcends\nlanguage, utilizing vision as a dynamic mental sketchpad. A similar evolution\nis now unfolding in AI, marking a fundamental paradigm shift from models that\nmerely think about images to those that can truly think with images. This\nemerging paradigm is characterized by models leveraging visual information as\nintermediate steps in their thought process, transforming vision from a passive\ninput into a dynamic, manipulable cognitive workspace. In this survey, we chart\nthis evolution of intelligence along a trajectory of increasing cognitive\nautonomy, which unfolds across three key stages: from external tool\nexploration, through programmatic manipulation, to intrinsic imagination. To\nstructure this rapidly evolving field, our survey makes four key contributions.\n(1) We establish the foundational principles of the think with image paradigm\nand its three-stage framework. (2) We provide a comprehensive review of the\ncore methods that characterize each stage of this roadmap. (3) We analyze the\ncritical landscape of evaluation benchmarks and transformative applications.\n(4) We identify significant challenges and outline promising future directions.\nBy providing this structured overview, we aim to offer a clear roadmap for\nfuture research towards more powerful and human-aligned multimodal AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23918.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64264095ba51f8a2136946a0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
      "fullname": "Zhaochen Su",
      "name": "Warrieryes",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02652",
      "authors": [
        {
          "_id": "6867282b9db35afc9c304c83",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "6867282b9db35afc9c304c84",
          "name": "Xiaoxi Li",
          "hidden": false
        },
        {
          "_id": "6867282b9db35afc9c304c85",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "6867282b9db35afc9c304c86",
          "name": "Yuyao Zhang",
          "hidden": false
        },
        {
          "_id": "6867282b9db35afc9c304c87",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "6867282b9db35afc9c304c88",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "6867282b9db35afc9c304c89",
          "name": "Hongjin Qian",
          "hidden": false
        },
        {
          "_id": "6867282b9db35afc9c304c8a",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T14:18:08.000Z",
      "submittedOnDailyAt": "2025-07-04T00:07:46.543Z",
      "title": "디코ープレーディング 계획 및 실행: 깊은 검색을 위한 휴리스틱 논리 프레임워크",
      "submittedOnDailyBy": {
        "_id": "6695f14df0ffd8e3a379ad61",
        "avatarUrl": "/avatars/5ebb7e55ee9c2d93850b279f440675b0.svg",
        "isPro": false,
        "fullname": "Jiajie Jin",
        "user": "jinjiajie",
        "type": "user"
      },
      "summary": "실세계의 검색 시나리오에서 복잡한 정보 요구는 다양한 소스로부터의 심층적인 논리론과 지식 합성을 요구하지만, 전통적인 검색 어쩌구 확장 생성(RAG) 파이프라인이 효과적으로 이를 해결하는 것이 어렵습니다. 현재의 논리론 기반의 접근 방식은 기본적인 한계를 가지고 있습니다: 고수준의 계획과 세부적인 실행을 하나의 모델에서 처리하며, 부적절한 논리론과 scalability의 한계를 초래합니다. 본 논문에서는, 전략적인 계획과 전문적인 실행을 구분하는 휴리스틱 프레임워크인 Hira를 통해 복잡한 검색 태스크를 집중적인 서브 태스크로 분해하고, 각 서브 태스크를 외부 도구와 논리론 능력이 있는 영역 전문의 AGREE에 배정하여 구조적인 통합 기관으로 결과를 협조합니다. 이 구분은 실행의 세부 사항이 고수준의 논리론을 훼손하지 않도록 하며, 시스템이 다양한 정보 처리 유형에 대한 전문 지식을 활용할 수 있도록 합니다. 4개의 복잡한 크로스 모달 깊은 검색 벤치마크에 대한 실험은 Hira가 선진적인 RAG와 AGREE 기반의 시스템을 크게 초월함을 보여주고, 답의 질과 시스템의 효율성에서도 개선이 확인되어, 여러 단계의 정보 탐색 태스크에 대한 분리된 계획과 실행의 효과가 명확히 밝혀집니다. 코드는 https://github.com/ignorejjj/Hira에 액세스할 수 있습니다.",
      "upvotes": 12,
      "discussionId": "6867282c9db35afc9c304c8b",
      "ai_summary": "A hierarchical framework for deep search tasks separates strategic planning from specialized execution, improving answer quality and efficiency over traditional retrieval-augmented generation and agent-based systems.",
      "ai_keywords": [
        "retrieval-augmented generation (RAG)",
        "hierarchical framework",
        "strategic planning",
        "specialized execution",
        "domain-specific agents",
        "external tools",
        "reasoning capabilities",
        "structured integration mechanism",
        "complex search tasks",
        "focused subtasks",
        "cross-modal deep search benchmarks"
      ]
    },
    "publishedAt": "2025-07-03T10:18:08.000Z",
    "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for\n  Deep Search",
    "summary": "Complex information needs in real-world search scenarios demand deep\nreasoning and knowledge synthesis across diverse sources, which traditional\nretrieval-augmented generation (RAG) pipelines struggle to address effectively.\nCurrent reasoning-based approaches suffer from a fundamental limitation: they\nuse a single model to handle both high-level planning and detailed execution,\nleading to inefficient reasoning and limited scalability. In this paper, we\nintroduce HiRA, a hierarchical framework that separates strategic planning from\nspecialized execution. Our approach decomposes complex search tasks into\nfocused subtasks, assigns each subtask to domain-specific agents equipped with\nexternal tools and reasoning capabilities, and coordinates the results through\na structured integration mechanism. This separation prevents execution details\nfrom disrupting high-level reasoning while enabling the system to leverage\nspecialized expertise for different types of information processing.\nExperiments on four complex, cross-modal deep search benchmarks demonstrate\nthat HiRA significantly outperforms state-of-the-art RAG and agent-based\nsystems. Our results show improvements in both answer quality and system\nefficiency, highlighting the effectiveness of decoupled planning and execution\nfor multi-step information seeking tasks. Our code is available at\nhttps://github.com/ignorejjj/HiRA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02652.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6695f14df0ffd8e3a379ad61",
      "avatarUrl": "/avatars/5ebb7e55ee9c2d93850b279f440675b0.svg",
      "fullname": "Jiajie Jin",
      "name": "jinjiajie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02754",
      "authors": [
        {
          "_id": "686732e19db35afc9c304cc9",
          "name": "Aurko Roy",
          "hidden": false
        },
        {
          "_id": "686732e19db35afc9c304cca",
          "name": "Timothy Chou",
          "hidden": false
        },
        {
          "_id": "686732e19db35afc9c304ccb",
          "name": "Sai Surya Duvvuri",
          "hidden": false
        },
        {
          "_id": "686732e19db35afc9c304ccc",
          "name": "Sijia Chen",
          "hidden": false
        },
        {
          "_id": "686732e19db35afc9c304ccd",
          "name": "Jiecao Yu",
          "hidden": false
        },
        {
          "_id": "686732e19db35afc9c304cce",
          "name": "Xiaodong Wang",
          "hidden": false
        },
        {
          "_id": "686732e19db35afc9c304ccf",
          "name": "Manzil Zaheer",
          "hidden": false
        },
        {
          "_id": "686732e19db35afc9c304cd0",
          "name": "Rohan Anil",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T16:16:34.000Z",
      "submittedOnDailyAt": "2025-07-04T00:19:14.552Z",
      "title": "Fast and Simplex: 2-Simplicial Attention in Triton\n\n빠른, 단순한 2-스킵실리셜 注意력(Attention) 알고리즘이 트리톤(Triton)에서 구현되었습니다.",
      "submittedOnDailyBy": {
        "_id": "651e96991b97c9f33d26bde6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
        "isPro": false,
        "fullname": "Elie Bakouch",
        "user": "eliebak",
        "type": "user"
      },
      "summary": "최근의 연구는 모델 크기와 토큰 수 모두에 대해 훈련 손실이 비례하여 증가하고, 계산량 최적화된 모델의 구현은 모델 크기와 토큰 수를 동시에 확장하는 것이 필요함을 보여주고 있습니다. 그러나 이러한 스케일링 규칙은 무한한 데이터 공급을 가정하고 주로 계산량 제한된 환경에서 적용되어 있습니다. 모델은 인터넷 크기의 큰 데이터 세트가 매일 증가함에 따라, 계산량 제한된 가정이 점점 신뢰할 수 없게 되었습니다. 이러한 변화는 토큰 효율성을 우선시하는 아키텍처의 필요성을 강조하고 있습니다.\n\n본 연구에서는 2-스미リッ크 Transformer의 사용에 대해 검토하고 있습니다. 이 아키텍처는 효율적인 Triton kernel 구현을 통해 표준의 dot-product attention을 TRIリニアル 함수로 일반화하고 있습니다. 2-스미リッ크 Transformer는 표준의 Transformer보다 더 좋은 토큰 효율성을 달성하고 있음을 보여줍니다: 토큰 버퍼가 고정된 경우, 같은 크기의 모델은 수학, 코딩, 이유, 로직과 관련된 작업에서 dot-product 모델보다 더 잘 수행합니다. 이러한 효과를 측정하고 지식과 이유의 작업의 스케일링 규칙의 지수를 dot product attention에 대한 것과 비교하여 보여주고 있습니다.",
      "upvotes": 9,
      "discussionId": "686732e19db35afc9c304cd1",
      "ai_summary": "The 2-simplicial Transformer outperforms standard Transformers by improving token efficiency, particularly for knowledge and reasoning tasks, through an efficient Trilinear function implementation.",
      "ai_keywords": [
        "2-simplicial Transformer",
        "dot-product attention",
        "trilinear functions",
        "Triton kernel",
        "token efficiency",
        "scaling laws",
        "knowledge tasks",
        "reasoning tasks"
      ]
    },
    "publishedAt": "2025-07-03T12:16:34.000Z",
    "title": "Fast and Simplex: 2-Simplicial Attention in Triton",
    "summary": "Recent work has shown that training loss scales as a power law with both\nmodel size and the number of tokens, and that achieving compute-optimal models\nrequires scaling model size and token count together. However, these scaling\nlaws assume an infinite supply of data and apply primarily in compute-bound\nsettings. As modern large language models increasingly rely on massive\ninternet-scale datasets, the assumption that they are compute-bound is becoming\nless valid. This shift highlights the need for architectures that prioritize\ntoken efficiency.\n  In this work, we investigate the use of the 2-simplicial Transformer, an\narchitecture that generalizes standard dot-product attention to trilinear\nfunctions through an efficient Triton kernel implementation. We demonstrate\nthat the 2-simplicial Transformer achieves better token efficiency than\nstandard Transformers: for a fixed token budget, similarly sized models\noutperform their dot-product counterparts on tasks involving mathematics,\ncoding, reasoning, and logic. We quantify these gains by demonstrating that\n2-simplicial attention changes the exponent in the scaling laws for knowledge\nand reasoning tasks compared to dot product attention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02754.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "651e96991b97c9f33d26bde6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
      "fullname": "Elie Bakouch",
      "name": "eliebak",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 180
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02321",
      "authors": [
        {
          "_id": "68679bf1213f123a1f88b8bd",
          "name": "Nina Konovalova",
          "hidden": false
        },
        {
          "_id": "68679bf1213f123a1f88b8be",
          "name": "Maxim Nikolaev",
          "hidden": false
        },
        {
          "_id": "68679bf1213f123a1f88b8bf",
          "name": "Andrey Kuznetsov",
          "hidden": false
        },
        {
          "_id": "68679bf1213f123a1f88b8c0",
          "name": "Aibek Alanov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T05:25:53.000Z",
      "submittedOnDailyAt": "2025-07-04T07:48:56.769Z",
      "title": "Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback (只返回翻译结果，无额外文本)",
      "submittedOnDailyBy": {
        "_id": "66680c6451545a8b46c6fd21",
        "avatarUrl": "/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg",
        "isPro": false,
        "fullname": "Aibek Alanov",
        "user": "ai-alanov",
        "type": "user"
      },
      "summary": "テキスト・タウン・イメージ의 확산모형에서 공간적 제어를 정밀하게 수행하는 것은 어렵습니다. ControlNet은 이 문제를 해결하기 위해 보조조건부 모듈을 도입하였지만, ControlNet++은 최종적인 데노이즈 스텝에만 적용되는 순환적 일관성 손실을 통해 정렬을 개선하고 있습니다. 그러나 이 접근법은 중간의 생성 단계를 무시하고 효과가 제한되어 있습니다. 우리는 모든 확산 스텝에서 공간적 일관성을 강제하는 훈련 단계 InnerControl를 제안하고 있습니다. 우리의 방법은 각 데노이즈 스텝에서 중간의 UNet 특징으로부터 입력 제어 신호(예: 에지, 깊이)를 재구성하기 위한 가벼운 컨베이너 프로브를 훈련시키는 것입니다. 이러한 프로브는 고도 노이즈의 잠재변수로부터도 신호를 효율적으로 추출할 수 있으며, 훈련용 팩토리 제어가 가능합니다. 예측된 조건과 목표 조건 사이의 차이를 최소화하는 데 의해, 우리의 정렬 손실은 제어 정확성과 생성 품질을 모두 개선합니다. ControlNet++과 같은 기존 기술과 조합하여, InnerControl은 다양한 조건부 방법(예: 에지, 깊이)에서 가장 先端한 성능을 달성합니다.",
      "upvotes": 9,
      "discussionId": "68679bf2213f123a1f88b8c1",
      "ai_summary": "InnerControl enforces spatial consistency across all diffusion steps by training lightweight convolutional probes to improve control fidelity and generation quality in text-to-image diffusion models.",
      "ai_keywords": [
        "ControlNet",
        "diffusion models",
        "auxiliary conditioning module",
        "cycle consistency loss",
        "latent spatial consistency",
        "convolutional probes",
        "UNet features",
        "alignment loss",
        "state-of-the-art performance",
        "conditioning methods",
        "control fidelity",
        "generation quality"
      ]
    },
    "publishedAt": "2025-07-03T01:25:53.000Z",
    "title": "Heeding the Inner Voice: Aligning ControlNet Training via Intermediate\n  Features Feedback",
    "summary": "Despite significant progress in text-to-image diffusion models, achieving\nprecise spatial control over generated outputs remains challenging. ControlNet\naddresses this by introducing an auxiliary conditioning module, while\nControlNet++ further refines alignment through a cycle consistency loss applied\nonly to the final denoising steps. However, this approach neglects intermediate\ngeneration stages, limiting its effectiveness. We propose InnerControl, a\ntraining strategy that enforces spatial consistency across all diffusion steps.\nOur method trains lightweight convolutional probes to reconstruct input control\nsignals (e.g., edges, depth) from intermediate UNet features at every denoising\nstep. These probes efficiently extract signals even from highly noisy latents,\nenabling pseudo ground truth controls for training. By minimizing the\ndiscrepancy between predicted and target conditions throughout the entire\ndiffusion process, our alignment loss improves both control fidelity and\ngeneration quality. Combined with established techniques like ControlNet++,\nInnerControl achieves state-of-the-art performance across diverse conditioning\nmethods (e.g., edges, depth).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02321.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66680c6451545a8b46c6fd21",
      "avatarUrl": "/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg",
      "fullname": "Aibek Alanov",
      "name": "ai-alanov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02694",
      "authors": [
        {
          "_id": "686762e49db35afc9c304d62",
          "name": "Zhijian Xu",
          "hidden": false
        },
        {
          "_id": "686762e49db35afc9c304d63",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "686762e49db35afc9c304d64",
          "name": "Manasi Patwardhan",
          "hidden": false
        },
        {
          "_id": "686762e49db35afc9c304d65",
          "name": "Lovekesh Vig",
          "hidden": false
        },
        {
          "_id": "686762e49db35afc9c304d66",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T15:04:38.000Z",
      "submittedOnDailyAt": "2025-07-04T03:43:32.903Z",
      "title": "LLM는 과학연구의 중요한 한계를 인식할 수 있는지? 　AI연구논문에서의 체계적 평가",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "ピアレビュー는 과학 연구에서 기본적인 요소입니다만, 출판물의 증가는 이러한 지식밀집형 프로세스의 문제점을 더욱 심각하게 만듭니다. LLMs은 다양한 과학 작업에 가능성이 있습니다만, 특히 논문의 제한을 특정하는 데에 인간적인 ピアレビュー의 도움을 받을 수 있는 가능성은 연구되어 있지 않습니다. 우선, AI를 중심으로 하는 과학 연구의 제한 유형의 리스트를 제공합니다. 이 리스트를 통해 제한 연구를 수행하기 위한 LimitGen, LLMs의 능력을 평가하는 첫 번째 상세한 벤치마크를 제시합니다. 벤치마크는 두 개의 세트로 구성되어 있습니다: LimitGen-Syn, 고품질의 논문을 제어적인 perturbation으로 생성한 합성 데이터 세트이며, LimitGen-Human, 실제 사람이 작성한 제한의 모음입니다. LLM 시스템의 제한 특정 능력을 향상시키기 위해, 문헌 검색을 추가하고, 이전의 과학 논리에 기반한 제한의 특정에 필요한 것입니다. 이 접근법은 LLM 시스템이 연구 논문에 대한 제한을 생성하는 능력을 향상시키고, 더 구체적인 및 구축적인 피드백을 제공할 수 있습니다.",
      "upvotes": 7,
      "discussionId": "686762e49db35afc9c304d67",
      "ai_summary": "LimitGen, a new benchmark, evaluates LLMs in identifying limitations in scientific research, improving their feedback through literature retrieval.",
      "ai_keywords": [
        "LLMs",
        "LimitGen",
        "LimitGen-Syn",
        "LimitGen-Human",
        "literature retrieval"
      ]
    },
    "publishedAt": "2025-07-03T11:04:38.000Z",
    "title": "Can LLMs Identify Critical Limitations within Scientific Research? A\n  Systematic Evaluation on AI Research Papers",
    "summary": "Peer review is fundamental to scientific research, but the growing volume of\npublications has intensified the challenges of this expertise-intensive\nprocess. While LLMs show promise in various scientific tasks, their potential\nto assist with peer review, particularly in identifying paper limitations,\nremains understudied. We first present a comprehensive taxonomy of limitation\ntypes in scientific research, with a focus on AI. Guided by this taxonomy, for\nstudying limitations, we present LimitGen, the first comprehensive benchmark\nfor evaluating LLMs' capability to support early-stage feedback and complement\nhuman peer review. Our benchmark consists of two subsets: LimitGen-Syn, a\nsynthetic dataset carefully created through controlled perturbations of\nhigh-quality papers, and LimitGen-Human, a collection of real human-written\nlimitations. To improve the ability of LLM systems to identify limitations, we\naugment them with literature retrieval, which is essential for grounding\nidentifying limitations in prior scientific findings. Our approach enhances the\ncapabilities of LLM systems to generate limitations in research papers,\nenabling them to provide more concrete and constructive feedback.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02726",
      "authors": [
        {
          "_id": "68673e349db35afc9c304d02",
          "name": "Matthieu Zimmer",
          "hidden": false
        },
        {
          "_id": "68673e349db35afc9c304d03",
          "name": "Xiaotong Ji",
          "hidden": false
        },
        {
          "_id": "68673e349db35afc9c304d04",
          "name": "Rasul Tutunov",
          "hidden": false
        },
        {
          "_id": "68673e349db35afc9c304d05",
          "name": "Anthony Bordg",
          "hidden": false
        },
        {
          "_id": "68673e349db35afc9c304d06",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "68673e349db35afc9c304d07",
          "name": "Haitham Bou Ammar",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/631c375768f7da9ad2496bf6/j0vqZ_s8kw-ALYV8UCDlF.png"
      ],
      "publishedAt": "2025-07-03T15:41:38.000Z",
      "submittedOnDailyAt": "2025-07-04T01:09:09.946Z",
      "title": "보바르키：발생적 목표 조건을 갖는 MDP의 이론 증명을 위한 모델",
      "submittedOnDailyBy": {
        "_id": "631c375768f7da9ad2496bf6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg",
        "isPro": true,
        "fullname": "Haitham Bou Ammar",
        "user": "hba123",
        "type": "user"
      },
      "summary": "推理은 대규모 언어 모델(LLMs)에 대해 어려운 문제로, 특히 이론적인 제약을 가진 환경에서, 예를 들어 자동화된 정리 증명(ATP) 환경에서 더욱 어려워진다. 이러한 문제를 강화하기 위해 PutnamBench 같은 벤치마크에서 강조되고, 대학 수준의 문제를 해결하기 위해 필요한 복잡하고 단계별 추론이 요구된다. 이를 대처하기 위해 우리는 자동으로 생성된 목표 조건付き MDPs(sG-MDPs)를 도입하고, 이 새로운 프레임워크에서 출력의 생성과 추구를 위해 변하는 증명 상태에 기반하여 출력을 생성하고 추구한다. 이러한 목표의 구조화된 생성으로 문제를 더 넓게 조사할 수 있게 된다. 다음으로, MCTS(몬테카를로 트리 탐색) 같은 알고리즘을 적용하여 sG-MDP를 해결한다. 이 접근법은 Bourbaki(7B)에서 구현되었으며, 7B LLMs의 다수집합을 사용하여 목표의 생성과 텍스트의 합성을 수행하는 모듈화 시스템이다. PutnamBench에서 Bourbaki(7B)는 26 문제를 해결하며, 이 크기의 모델에서 새로운 최선 결과를 달성하였다.",
      "upvotes": 4,
      "discussionId": "68673e359db35afc9c304d08",
      "ai_summary": "A new framework using self-generated goal-conditioned MDPs with MCTS-like algorithms enhances LLM performance in automated theorem proving, particularly on benchmarks like PutnamBench.",
      "ai_keywords": [
        "LLMs",
        "automated theorem proving",
        "ATP",
        "sparse rewards",
        "sG-MDPs",
        "Monte Carlo Tree Search",
        "MCTS",
        "Bourbaki",
        "tactic synthesis"
      ]
    },
    "publishedAt": "2025-07-03T11:41:38.000Z",
    "title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving",
    "summary": "Reasoning remains a challenging task for large language models (LLMs),\nespecially within the logically constrained environment of automated theorem\nproving (ATP), due to sparse rewards and the vast scale of proofs. These\nchallenges are amplified in benchmarks like PutnamBench, which contains\nuniversity-level problems requiring complex, multi-step reasoning. To address\nthis, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new\nframework in which agents generate and pursue their subgoals based on the\nevolving proof state. Given this more structured generation of goals, the\nresulting problem becomes more amenable to search. We then apply Monte Carlo\nTree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our\napproach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs\nfor subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)\nsolves 26 problems, achieving new state-of-the-art results with models at this\nscale.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/631c375768f7da9ad2496bf6/j0vqZ_s8kw-ALYV8UCDlF.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02726.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631c375768f7da9ad2496bf6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg",
      "fullname": "Haitham Bou Ammar",
      "name": "hba123",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02092",
      "authors": [
        {
          "_id": "68676e4a9db35afc9c304d72",
          "name": "Alexi Gladstone",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d73",
          "name": "Ganesh Nanduru",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d74",
          "name": "Md Mofijul Islam",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d75",
          "name": "Peixuan Han",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d76",
          "name": "Hyeonjeong Ha",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d77",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:48:46.365Z",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d78",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d79",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d7a",
          "name": "Jundong Li",
          "hidden": false
        },
        {
          "_id": "68676e4a9db35afc9c304d7b",
          "name": "Tariq Iqbal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T19:17:29.000Z",
      "submittedOnDailyAt": "2025-07-04T04:39:19.016Z",
      "title": "에너지 기반 트랜스포머는 스케일러블한 학습자와 기억자입니다.",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "推論할 때의 계산 방법, 인간 시스템 2 컴퓨팅과 유사하며, 최근 모델 성능 향상에 있어서 인기를 얻고 있는 것들이다. 그러나 현재의 방법들은 많은 제한을 가지고 있다: 모델 유형에 특화된 (예: 텍스트만 동작하는 것), 문제에 특화된 (예: 수학이나 코딩과 같은 증명 가능한 분야), 또는 추가적인 서브젝트나 훈련이 필요하고, 무라벨의 사전 학습 위에 있는 것 (예: 증명자나 증명 가능한 보상). 본 논문에서는 \"이러한 시스템 2 컴퓨팅의 방법을 일반화하고, 무라벨 학습을 통해 그 컴퓨팅을 학습할 수 있는 모델을 개발할 수 있는지\"라는 문제를 과제로 삼고 있다. 흥미로운 점은 이 문제를 답하는 것은 \"예\"라는 것이었다는 것이다. 구체적으로, 입력과 후보 예측의 일치성을 명시적으로 확인하고, 그 확인기에 대한 최적화로 예측 문제를 재구성하여 이를 실현했다. 특히, Energy-Based Transformers (EBTs)를 학습시켜서 각각의 입력과 후보 예측의 조합에 에너지 값을 할당하고, 에너지 최소화의 경사 하류로 예측을 수행할 수 있도록 하였다. EBTs는 디시켄트 (텍스트)와 연속 (시각) 모델 유형에서도, 훈련 중 Transformer++보다 더 빠르게 스케일링하고, 데이터, 배치 크기, 파라미터, FLOPs, 깊이에 대해 35% 이상의 스케일링율을 달성한다. 추론 시에는 EBTs는 언어 태스크에서 Transformer++보다 29% 이상 성능을 향상시키고, 이미지 노이즈에 대해 Diffusion Transformers보다 성능을 향상시키고, 동시에 적은 흐름 패스를 사용함으로써 우위를 가져온다. 또한, 같은 또는 더 나은 사전 학습 성능을 제공하지 않는 경우, EBTs는 많은 다음 세대 태스크에서 현재의 모델보다 더 좋은 결과를 얻는다는 것을 확인하고, EBTs는 현재의 방법보다 더 일반화 가능한 것을 보여준다. 그 결과, EBTs는 모델의 학습 능력과 컴퓨팅 능력의 스케일링의 새로운 패러다임으로 기대된다.",
      "upvotes": 3,
      "discussionId": "68676e4a9db35afc9c304d7c",
      "ai_summary": "Energy-Based Transformers, trained via unsupervised learning, outperform existing models in both scaling and inference across text and image tasks by re-framing predictions as optimization problems.",
      "ai_keywords": [
        "Energy-Based Models",
        "Energy-Based Transformers",
        "System 2 Thinking",
        "Transformer++",
        "image denoising",
        "gradient descent-based energy minimization",
        "scaling rate",
        "FLOPs",
        "depth"
      ]
    },
    "publishedAt": "2025-07-02T15:17:29.000Z",
    "title": "Energy-Based Transformers are Scalable Learners and Thinkers",
    "summary": "Inference-time computation techniques, analogous to human System 2 Thinking,\nhave recently become popular for improving model performances. However, most\nexisting approaches suffer from several limitations: they are modality-specific\n(e.g., working only in text), problem-specific (e.g., verifiable domains like\nmath and coding), or require additional supervision/training on top of\nunsupervised pretraining (e.g., verifiers or verifiable rewards). In this\npaper, we ask the question \"Is it possible to generalize these System 2\nThinking approaches, and develop models that learn to think solely from\nunsupervised learning?\" Interestingly, we find the answer is yes, by learning\nto explicitly verify the compatibility between inputs and\ncandidate-predictions, and then re-framing prediction problems as optimization\nwith respect to this verifier. Specifically, we train Energy-Based Transformers\n(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy\nvalue to every input and candidate-prediction pair, enabling predictions\nthrough gradient descent-based energy minimization until convergence. Across\nboth discrete (text) and continuous (visual) modalities, we find EBTs scale\nfaster than the dominant Transformer++ approach during training, achieving an\nup to 35% higher scaling rate with respect to data, batch size, parameters,\nFLOPs, and depth. During inference, EBTs improve performance with System 2\nThinking by 29% more than the Transformer++ on language tasks, and EBTs\noutperform Diffusion Transformers on image denoising while using fewer forward\npasses. Further, we find that EBTs achieve better results than existing models\non most downstream tasks given the same or worse pretraining performance,\nsuggesting that EBTs generalize better than existing approaches. Consequently,\nEBTs are a promising new paradigm for scaling both the learning and thinking\ncapabilities of models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02092.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.22813",
      "authors": [
        {
          "_id": "6867836c11736b002cf34d41",
          "name": "Zhuojun Ding",
          "hidden": false
        },
        {
          "_id": "6867836c11736b002cf34d42",
          "name": "Wei Wei",
          "hidden": false
        },
        {
          "_id": "6867836c11736b002cf34d43",
          "user": {
            "_id": "641aa5e391e3376a057bbd4c",
            "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
            "isPro": false,
            "fullname": "Chenghao Fan",
            "user": "Facico",
            "type": "user"
          },
          "name": "Chenghao Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-04T07:52:43.972Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T08:28:52.000Z",
      "submittedOnDailyAt": "2025-07-04T06:06:27.989Z",
      "title": "선택과 통합: 대 언어 모델에 의한 적응 가능한 스케일러블 명명 에너지 인식에 관한 연구",
      "submittedOnDailyBy": {
        "_id": "641aa5e391e3376a057bbd4c",
        "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
        "isPro": false,
        "fullname": "Chenghao Fan",
        "user": "Facico",
        "type": "user"
      },
      "summary": "Supervised fine-tuning (SFT)는 대규모 언어 모델(LLMs)와 정보 추출(IE) 태스크의 일치를 실현하기 위해 광범위하게 사용되고 있습니다. 예를 들어, 명명된 엔티티 식별(NER) 등입니다. 그러나 이러한 미세한 라벨의 기록과 모델의 학습은 고비율입니다. 기존의 연구는 일반적으로 여러 분야를 아우르는 통일 모델을 학습하고 있습니다만, 이러한 접근 방식은 모든 학습 데이터가 목표 분야에 이익을 부여하지 않음으로, 학습 모델의 확장이 어려운 문제입니다. 우리는 SaM 프레임워크를 제안하고 있습니다. 이는 추론 시 동적으로 선택하고 통합하는 EXPERT 모델을 구현합니다. 특히, 목표 분야에 대해 이미 학습된 영역별 EXPERT를 선택합니다. 이는 (i) 영역의 유사성과 (ii) 샘플링 인스턴스의 성능에 기반합니다. 이후 EXPERT를 통합하고, 태스크专用 모델을 생성합니다. 목표 분야에 베리필의 EXPERT를 동적으로 통합함으로써 다양한 분야에서 일반화에 개선하고 추가 학습이 필요하지 않습니다. 또한 EXPERT의 추가 및 제거가 용이하며, 확장성이 높습니다. 여러 벤치마크에서 확장된 실험은 우리의 프레임워크의 효과성을 보여주고, 통합 모델을 평균 10% 이상 효과적으로 초과합니다. 또한 잠재적인 개선점, 실용적인 경험, 프레임워크의 확장에 대한 견해를 제공합니다.",
      "upvotes": 3,
      "discussionId": "6867836c11736b002cf34d44",
      "githubRepo": "https://github.com/Ding-ZJ/SaM",
      "ai_summary": "A framework dynamically selects and merges pre-trained domain-specific models for efficient and scalable information extraction tasks.",
      "ai_keywords": [
        "supervised fine-tuning",
        "large language models",
        "information extraction",
        "named entity recognition",
        "unified model",
        "domain-specific models",
        "SaM framework",
        "cross-domain selection",
        "performance on sampled instances",
        "task-specific models",
        "scalability"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-06-28T04:28:52.000Z",
    "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity\n  Recognition with Large Language Models",
    "summary": "Supervised fine-tuning (SFT) is widely used to align large language models\n(LLMs) with information extraction (IE) tasks, such as named entity recognition\n(NER). However, annotating such fine-grained labels and training\ndomain-specific models is costly. Existing works typically train a unified\nmodel across multiple domains, but such approaches lack adaptation and\nscalability since not all training data benefits target domains and scaling\ntrained models remains challenging. We propose the SaM framework, which\ndynamically Selects and Merges expert models at inference time. Specifically,\nfor a target domain, we select domain-specific experts pre-trained on existing\ndomains based on (i) domain similarity to the target domain and (ii)\nperformance on sampled instances, respectively. The experts are then merged to\ncreate task-specific models optimized for the target domain. By dynamically\nmerging experts beneficial to target domains, we improve generalization across\nvarious domains without extra training. Additionally, experts can be added or\nremoved conveniently, leading to great scalability. Extensive experiments on\nmultiple benchmarks demonstrate our framework's effectiveness, which\noutperforms the unified model by an average of 10%. We further provide insights\ninto potential improvements, practical experience, and extensions of our\nframework.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641aa5e391e3376a057bbd4c",
      "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
      "fullname": "Chenghao Fan",
      "name": "Facico",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.02778",
      "authors": [
        {
          "_id": "68678e2a11736b002cf34d5e",
          "name": "Ken Tsui",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T16:41:30.000Z",
      "submittedOnDailyAt": "2025-07-04T06:56:56.026Z",
      "title": "자기보정 벤치: 로봇 모델의 시각적 결점들을 명확히 드러내며 해결하는 데 사용합니다.",
      "submittedOnDailyBy": {
        "_id": "60e50ce5350d181892d5a636",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e50ce5350d181892d5a636/qjYikZtE-2kjub-6XG5Kn.jpeg",
        "isPro": false,
        "fullname": "Ken Tsui",
        "user": "kenhktsui",
        "type": "user"
      },
      "summary": "그 영어 텍스트를 일본어로 번역합니다.",
      "upvotes": 1,
      "discussionId": "68678e2a11736b002cf34d5f",
      "githubRepo": "https://github.com/kenhktsui/self-correction-bench",
      "ai_summary": "Self-Correction Bench measures the self-correction blind spot in large language models, finding that training primarily on error-free responses contributes to this issue; appending \"Wait\" notably improves their ability to correct errors in their outputs.",
      "ai_keywords": [
        "LLMs",
        "Self-Correction",
        "autoregressive LLMs",
        "Self-Correction Blind Spot",
        "Self-Correction Bench",
        "error injection",
        "human training demonstrations",
        "RL-trained models"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-07-03T12:41:30.000Z",
    "title": "Self-Correction Bench: Revealing and Addressing the Self-Correction\n  Blind Spot in LLMs",
    "summary": "Although large language models (LLMs) have become transformative, they still\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\nan important capability for a trustworthy LLM, particularly an autoregressive\nLLM. While LLMs can identify error in user input, they exhibit a systematic\n'Self-Correction Blind Spot' - failing to correct identical error in their own\noutputs. To systematically study this phenomenon, we introduce Self-Correction\nBench, a systematic framework to measure this phenomenon through controlled\nerror injection at three complexity levels. Testing 14 models, we find an\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\nrelates to training data composition: human training demonstrations\npredominantly show error-free responses rather than error-correction sequences,\nunlike RL-trained models that learn error correction through outcome feedback.\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\nthat the capability exists but requires activation. Our work highlights a\ncritical limitation in current LLMs and offers potential avenues for improving\ntheir reliability and trustworthiness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02778.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60e50ce5350d181892d5a636",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e50ce5350d181892d5a636/qjYikZtE-2kjub-6XG5Kn.jpeg",
      "fullname": "Ken Tsui",
      "name": "kenhktsui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 40
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01663",
      "authors": [
        {
          "_id": "6865e6588c83dab5f72d1e85",
          "name": "Zhenyu Han",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e86",
          "name": "Ansheng You",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e87",
          "name": "Haibo Wang",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e88",
          "name": "Kui Luo",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e89",
          "name": "Guang Yang",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e8a",
          "name": "Wenqi Shi",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e8b",
          "name": "Menglong Chen",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e8c",
          "name": "Sicheng Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e8d",
          "name": "Zeshun Lan",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e8e",
          "name": "Chunshi Deng",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e8f",
          "name": "Huazhong Ji",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e90",
          "name": "Wenjie Liu",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e91",
          "name": "Yu Huang",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e92",
          "name": "Yixiang Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e93",
          "name": "Chenyi Pan",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e94",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e95",
          "name": "Xin Huang",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e96",
          "name": "Chunsheng Li",
          "hidden": false
        },
        {
          "_id": "6865e6588c83dab5f72d1e97",
          "name": "Jianping Wu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/_y3XJtovmLBm_ol-D36bj.jpeg"
      ],
      "publishedAt": "2025-07-02T12:45:34.000Z",
      "submittedOnDailyAt": "2025-07-04T01:49:59.664Z",
      "title": "AsyncFlow: LLM의 효율적인 후처리를 위한 비동기 스트리밍 RL 프레임워크",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "강화학습(RL)은 대규모 언어 모델(LLMs)의 훈련 후반 단계에서 중요한 기술로 자리잡고 있습니다. 전통적인 RL 프레임워크는 스케일러블 백업에 어려움을 겪고, 태스크 분리의 RL 프레임워크는 복잡한 데이터 흐름과 자원의 낭비 및 부하 불균형에 직면해 있습니다. 또한 현재 많은 프레임워크는 LLM 훈련이나 추론 엔진과 엄격하게 결합되어 있으며, 사용자 정의 엔진의 지원에 어려움을 겪는 경우가 많습니다. 이러한 문제를 해결하기 위해, 우리는 비동기 스트리밍의 RL 프레임워크인 AsyncFlow를 제안합니다. 특히, 분산된 데이터 저장소와 데이터 전송 모듈을 도입하여, 완전한 스트리밍 방식의 데이터 관리와 미세한 스케줄링 기능을 제공합니다. 이 아키텍처는 자동화된 파이프라인의 반복과 동적인 부하 균형을 촉진합니다. 또한, 스태레스 에지 내에서 전략적으로 파라미터 업데이트 프로세스를 지연시키는 방법으로, 계산의 낭비를 최소화하기 위한 소비자-소비자 기반의 비동기 작업 흐름 엔진링을 제안합니다. 마지막으로, AsyncFlow의 핵심 기능은, 기본 훈련과 추론 엔진으로부터 아키텍처를 개선하고, 서비스 중심의 사용자 인터페이스로 둘러싸여 모듈화된, 사용자 정의 가능한 사용자 경험 제공입니다. 확장된 실험은, 가장 先端의 기본과 비교하여 평균 1.59의 트랜스포스 프로덕션 개선을 보여주며, 본 논문에서 소개된 아키텍처는 다음 세대의 RL 훈련 시스템의 설계에 실질적인 통찰을 제공합니다.",
      "upvotes": 1,
      "discussionId": "6865e6598c83dab5f72d1e98",
      "ai_summary": "An asynchronous streaming RL framework improves efficiency in the post-training phase of large language models by optimizing data management and computational workload balancing.",
      "ai_keywords": [
        "AsynFlow",
        "reinforcement learning (RL)",
        "large language models (LLMs)",
        "distributed data storage",
        "transfer module",
        "unified data management",
        "fine-grained scheduling",
        "streaming",
        "pipeline overlapping",
        "dynamic load balancing",
        "producer-consumer-based asynchronous workflow",
        "parameter update",
        "staleness thresholds",
        "service-oriented user interfaces"
      ]
    },
    "publishedAt": "2025-07-02T08:45:34.000Z",
    "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM\n  Post-Training",
    "summary": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/_y3XJtovmLBm_ol-D36bj.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01663.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01004",
      "authors": [
        {
          "_id": "68678cd011736b002cf34d53",
          "name": "Yuhong Chou",
          "hidden": false
        },
        {
          "_id": "68678cd011736b002cf34d54",
          "name": "Zehao Liu",
          "hidden": false
        },
        {
          "_id": "68678cd011736b002cf34d55",
          "name": "Ruijie Zhu",
          "hidden": false
        },
        {
          "_id": "68678cd011736b002cf34d56",
          "name": "Xinyi Wan",
          "hidden": false
        },
        {
          "_id": "68678cd011736b002cf34d57",
          "name": "Tianjian Li",
          "hidden": false
        },
        {
          "_id": "68678cd011736b002cf34d58",
          "name": "Congying Chu",
          "hidden": false
        },
        {
          "_id": "68678cd011736b002cf34d59",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "68678cd011736b002cf34d5a",
          "name": "Jibin Wu",
          "hidden": false
        },
        {
          "_id": "68678cd011736b002cf34d5b",
          "name": "Zejun Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-01T17:54:53.000Z",
      "submittedOnDailyAt": "2025-07-04T06:43:02.966Z",
      "title": "ZeCO: ライナーアテンション의 ゼロコミュニケーション オーバーヘッド의 シーケンスパラレリズム\n\n(Note: The original text appears to be a technical term and may not have a direct, commonly used Korean equivalent. The translation provided maintains the structure and meaning of the original text.)",
      "submittedOnDailyBy": {
        "_id": "612ee6a7b960e78c6d2319d4",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
        "isPro": false,
        "fullname": "Qian Liu",
        "user": "SivilTaram",
        "type": "user"
      },
      "summary": "線형アテンション 구조는 대규모 언어 모델 (LLMs)에 대해 선형 계산 복잡도를 제공하며, 초장 수열 (예: 1M 컨텍스트)의 효율적인 처리를 가능하게 합니다. 그러나 현재의 시퀀스 병렬 모듈 (SP) 메소드는 장치 간 전송 오버헤드로 주요한 붕대 역할을 합니다. 본 논문에서는, 선형 어텐션 모델에 대한 새로운 SP 메소드인 ZeCO (Zero Communication Overhead) 시퀀스 병렬성을 도입하여, 이러한 제한을 극복하고, 긴 시퀀스 훈련의 근사 선형 스케일러리즘을 단말에서 실현하는 것을 목표로 합니다. 예를 들어, ZeCO를 사용하여 64 장치에서 1M 시퀀스 길이의 모델을 훈련하는 데 1 장치에서 16k 시퀀스를 훈련하는 데 소요되는 시간과 같은 시간이 소요될 수 있습니다. ZeCO의 핵심은 새로운 집중 통신 요소인 All-Scan입니다. All-Scan은 최소한의 통신 피트프린트를 유지하면서, 각 SP 레ン킹에 필요한 초기 연산자 상태를 정확하게 제공합니다. 이론적으로는, ZeCO의 최적성을 보여주며, 시각적인 시간과 공간 오버헤드 발생을 보여주며, 실험적으로는 다양한 시퀀스 병렬성 전략의 통신 비용 비교를 통해, All-Scan은 SP 시나리오에서 가장 빠른 통신을 구현하는 것을 보여줍니다. 특히, 256 그래픽 프로세서와 8M 시퀀스 길이를 사용하며, ZeCO는 현재의 최선 (SOTA) SP 메소드 대비 60%의 속도 향상을 실현합니다. ZeCO는, 이전에 어려워 보이는 긴 시퀀스 길이에서 다음 세대 LLMs의 효율적인 훈련을 위한 명확한 길을 안내하는 것을 믿습니다.",
      "upvotes": 1,
      "discussionId": "68678cd111736b002cf34d5c",
      "ai_summary": "A new zero communication overhead sequence parallelism method called ZeCO enables efficient training of large language models with ultra-long sequences across multiple devices.",
      "ai_keywords": [
        "linear attention mechanisms",
        "Large Language Models",
        "LLMS",
        "Sequence Parallelism",
        "SP",
        "All-Scan",
        "collective communication primitive",
        "near-linear scalability",
        "end-to-end sequence training"
      ]
    },
    "publishedAt": "2025-07-01T13:54:53.000Z",
    "title": "ZeCO: Zero Communication Overhead Sequence Parallelism for Linear\n  Attention",
    "summary": "Linear attention mechanisms deliver significant advantages for Large Language\nModels (LLMs) by providing linear computational complexity, enabling efficient\nprocessing of ultra-long sequences (e.g., 1M context). However, existing\nSequence Parallelism (SP) methods, essential for distributing these workloads\nacross devices, become the primary bottleneck due to substantial communication\noverhead. In this paper, we introduce ZeCO (Zero Communication Overhead)\nsequence parallelism for linear attention models, a new SP method designed to\novercome these limitations and achieve end-to-end near-linear scalability for\nlong sequence training. For example, training a model with a 1M sequence length\nacross 64 devices using ZeCO takes roughly the same time as training with an\n16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new\ncollective communication primitive. All-Scan provides each SP rank with\nprecisely the initial operator state it requires while maintaining a minimal\ncommunication footprint, effectively eliminating communication overhead.\nTheoretically, we prove the optimaity of ZeCO, showing that it introduces only\nnegligible time and space overhead. Empirically, we compare the communication\ncosts of different sequence parallelism strategies and demonstrate that\nAll-Scan achieves the fastest communication in SP scenarios. Specifically, on\n256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to\nthe current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a\nclear path toward efficiently training next-generation LLMs on previously\nintractable sequence lengths.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01004.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "612ee6a7b960e78c6d2319d4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
      "fullname": "Qian Liu",
      "name": "SivilTaram",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 84
    },
    "isAuthorParticipating": false
  }
]