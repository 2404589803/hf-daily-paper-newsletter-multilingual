[
  {
    "paper": {
      "id": "2507.01853",
      "authors": [
        {
          "_id": "686b4e69213f123a1f88bd76",
          "name": "Samridhi Raj Sinha",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd77",
          "name": "Rajvee Sheth",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd78",
          "name": "Abhishek Upperwal",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd79",
          "name": "Mayank Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T16:07:54.000Z",
      "submittedOnDailyAt": "2025-07-07T03:06:37.666Z",
      "title": "ECA-EVAL : 인도어의 대규모 언어 모델의 상세한 평가 프레임워크",
      "submittedOnDailyBy": {
        "_id": "66e1425c919f283fbd7dfb5e",
        "avatarUrl": "/avatars/b43ee9b9042486b5faf91472b4c49c5d.svg",
        "isPro": false,
        "fullname": "Rajvee Sheth",
        "user": "RajveeSheth",
        "type": "user"
      },
      "summary": "LLM의 급속한 발전은 영어 중심의 벤치마크를 넘어가는 평가 프레임워크의 필요성을 강조하고, 다양한 언어를 사용하는 지역사회에 요구를 충족시키기 위한 것이 필요합니다. 우리는 EKA-EVAL이라는 통일된 프로덕션 준비된 평가 프레임워크를 소개합니다. 이 프레임워크는 35개 이상의 벤치마크를 통합하고, 10개 이상의 Indic 고유 데이터셋을 포함하며, 이유, 수학, 도구 사용, 긴 텍스트 이해, 읽기 평가 등 다양한 카테고리를 포함하고 있습니다. 현재의 인도어 평가 도구와 비교하여, EKA-EVAL은 분산 추론, 퍼포먼스, 멀티 GPU 사용 등의 기능을 제공하며, 벤치마크 커버리지를 확장합니다. 우리 시스템적인 비교는 EKA-EVAL이 글로벌 및 Indic LLM을 위한 처음으로 확장 가능한 평가 시스템으로 자리잡는 것을 의미합니다. 다언어 벤치마크의 벽을 크게 낮췄습니다. 프레임워크는 오픈소스로, https://github.com/lingo-iitgn/eka-eval에서 공개되어 있습니다. EKA 인и셔티브의 일부로, https://eka.soket.ai에서 진행 중이며, 100개 이상의 벤치마크를 확장하고, 강력한 다언어 평가 생태계를 구축하는 것을 목표로 합니다.",
      "upvotes": 2,
      "discussionId": "686b4e69213f123a1f88bd7a",
      "ai_summary": "EKA-EVAL is a comprehensive multilingual evaluation framework for large language models, supporting diverse benchmarks and features for efficient distributed inference and GPU usage.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "unified evaluation framework",
        "production-ready",
        "Indic-specific datasets",
        "reasoning",
        "mathematics",
        "tool use",
        "long-context understanding",
        "reading comprehension",
        "distributed inference",
        "quantization",
        "multi-GPU usage",
        "end-to-end",
        "extensible evaluation suite",
        "multilingual benchmarking",
        "open-source"
      ]
    },
    "publishedAt": "2025-07-02T12:07:54.000Z",
    "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages",
    "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e1425c919f283fbd7dfb5e",
      "avatarUrl": "/avatars/b43ee9b9042486b5faf91472b4c49c5d.svg",
      "fullname": "Rajvee Sheth",
      "name": "RajveeSheth",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01955",
      "authors": [
        {
          "_id": "686b8347213f123a1f88bdc8",
          "name": "Rahul Ramachandran",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdc9",
          "name": "Ali Garjani",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdca",
          "name": "Roman Bachmann",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcb",
          "name": "Andrei Atanov",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcc",
          "name": "Oğuzhan Fatih Kar",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcd",
          "name": "Amir Zamir",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:59:07.000Z",
      "submittedOnDailyAt": "2025-07-07T06:51:04.452Z",
      "title": "GPT-4o는 어떤 정도의 시각을 이해하는지 어떻게 평가되는가? 표준적인 컴퓨터 비전 태스크에서 다 모델 기반 모델의 평가입니다.",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": true,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "다모뎅기형 모델（예：GPT-4o）는 최근 놀라운 발전을 거듭하고 있지만, 이들 모델이 시각을 이해하는 정도에 대해 아직도 불확실합니다. 본 논문에서는, 표준적인 컴퓨터 시각 태스크（문자화 시각화, 물체 탐지, 이미지 분류, 깊이 및 표면 정규화 예측）에서 인기 있는 다모뎅기형 모델의 성능을 평가합니다. 평가에 사용되는 기존 데이터셋 (예: COCO, ImageNet 및 그 변형)을 사용합니다.\n\n이 평가를 위해 주요한 문제점은 다음과 같습니다. 1) 많은 모델은 텍스트를 출력하는 것을 학습하고 있기 때문에, 문장나 3D 영역을 자연스럽게 표현할 수 없기 때문입니다. 2) 많은 최신 모델은 전용 소프트웨어로 되어 있으며, API 레벨에서만 액세스가 가능하며, 가중치 액세스에 대한 적용이 불가능합니다. 이러한 문제를 해결하기 위해, 표준적인 시각 태스크를 텍스트 프로ン퓰트 비어스 및 API 호환 가능한 태스크로 번역하고, 표준화된 평가 프레임워크를 프로ン퓰트 칭화기법을 통해 만듭니다.\n\n이 모델들의 성능에 대해 다음과 같은 점이 확인됩니다. 1) 어떤 태스크에서도 가장 선진한 전문 모델에 가까운 성능을 보여주지 않지만, 2) 이들은 인정되는 일반적인 모델이며, 이는 주로 이미지 텍스트 기반의 태스크에서 학습되었기 때문입니다. 3) 문자화 태스크의 성능은 몇 가지 일반적인 태스크보다 특히 좋으며, 4) 프로ン퓰트 칭화기법의 기술은 성능에 영향을 미칩니다. 그러나 더 좋은 모델은 프로ン퓰트의 변화에 대해 민감도가 낮다고 보입니다. 5) GPT-4o는 추론 모델이 아닌 모델 중 가장 뛰어난 모델이며, 6가지 태스크 중 4가지 태스크에서 가장 높은 순위를 차지했습니다. 6) 추론 모델 (예: o3)은 일반적인 태스크에서 개선을 보여주며, 7) 최신의 GPT-4o와 같은 본래적인 이미지 생성 기능을 가진 모델의 초기 분석은 환상 및 공간 조정 오류 등의 특징을 나타냅니다.",
      "upvotes": 0,
      "discussionId": "686b8348213f123a1f88bdce",
      "ai_summary": "Multimodal foundation models, despite being primarily trained on image-text tasks, demonstrate respectable performance across various vision tasks when adapted through prompt chaining, though they fall short compared to specialized models.",
      "ai_keywords": [
        "GPT-4o",
        "o4-mini",
        "Gemini 1.5 Pro",
        "Gemini 2.0 Flash",
        "Claude 3.5 Sonnet",
        "Qwen2-VL",
        "Llama 3.2",
        "semantic segmentation",
        "object detection",
        "image classification",
        "depth prediction",
        "surface normal prediction",
        "COCO",
        "ImageNet",
        "prompt chaining",
        "reasoning models",
        "hallucinations",
        "spatial misalignments"
      ]
    },
    "publishedAt": "2025-07-02T13:59:07.000Z",
    "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation\n  Models on Standard Computer Vision Tasks",
    "summary": "Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 908
    },
    "isAuthorParticipating": false
  }
]