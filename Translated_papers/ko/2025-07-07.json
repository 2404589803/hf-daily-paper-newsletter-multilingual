[
  {
    "paper": {
      "id": "2507.01853",
      "authors": [
        {
          "_id": "686b4e69213f123a1f88bd76",
          "name": "Samridhi Raj Sinha",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd77",
          "name": "Rajvee Sheth",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd78",
          "name": "Abhishek Upperwal",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd79",
          "name": "Mayank Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T16:07:54.000Z",
      "submittedOnDailyAt": "2025-07-07T03:06:37.666Z",
      "title": "エカ-エバル : 인도어의 대규모 언어 모델의 상세 평가 프레임워크",
      "submittedOnDailyBy": {
        "_id": "66e1425c919f283fbd7dfb5e",
        "avatarUrl": "/avatars/b43ee9b9042486b5faf91472b4c49c5d.svg",
        "isPro": false,
        "fullname": "Rajvee Sheth",
        "user": "RajveeSheth",
        "type": "user"
      },
      "summary": "LLM의 급격한 발전에 따라, 영어 중심의 벤치마크를 초월하는 평가 프레임워크의 필요성이 높아졌습니다. 언어 다양성이 높은 지역인 인도 등의 요구에 대응하기 위해, EKA-EVAL라는 통일된, 생산용 평가 프레임워크를 소개합니다. 이 프레임워크는 35개 이상의 벤치마크를 통합하고, 10개 이상의 고유 데이터셋을 포함하며, 이유, 수학, 도구의 사용, 긴 문맥 이해, 읽기 평가 등 다양한 분야를 광범위하게 커버합니다. 현재의 인도어 평가 도구와 비교하여, EKA-EVAL은 벤치마크의 광범위한 커버리지, 분산 계산, 양수, 멀티 GPU의 사용 등을 내장하고 있습니다. 우리 시스템적인 비교에서, EKA-EVAL은 처음부터 끝까지 확장 가능한 평가 시스템으로 자리잡으며, 글로벌 및 인덱스 기반의 LLM에 적합한 평가 도구를 제공하며, 다언어 벤치마크링의壁를 크게 낮췄습니다. 이 프레임워크는 오픈소스이며, https://github.com/lingo-iitgn/eka-eval에서 공개되어 있습니다. 더욱이, 현재 진행 중인 EKAイニシアチブ의 일부로, 100개 이상의 벤치마크를 확장하고 강력한 다언어 평가 생태계를 구축하는 것을 목표로 합니다.",
      "upvotes": 2,
      "discussionId": "686b4e69213f123a1f88bd7a",
      "ai_summary": "EKA-EVAL is a comprehensive multilingual evaluation framework for large language models, supporting diverse benchmarks and features for efficient distributed inference and GPU usage.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "unified evaluation framework",
        "production-ready",
        "Indic-specific datasets",
        "reasoning",
        "mathematics",
        "tool use",
        "long-context understanding",
        "reading comprehension",
        "distributed inference",
        "quantization",
        "multi-GPU usage",
        "end-to-end",
        "extensible evaluation suite",
        "multilingual benchmarking",
        "open-source"
      ]
    },
    "publishedAt": "2025-07-02T12:07:54.000Z",
    "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages",
    "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e1425c919f283fbd7dfb5e",
      "avatarUrl": "/avatars/b43ee9b9042486b5faf91472b4c49c5d.svg",
      "fullname": "Rajvee Sheth",
      "name": "RajveeSheth",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01955",
      "authors": [
        {
          "_id": "686b8347213f123a1f88bdc8",
          "name": "Rahul Ramachandran",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdc9",
          "name": "Ali Garjani",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdca",
          "name": "Roman Bachmann",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcb",
          "name": "Andrei Atanov",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcc",
          "name": "Oğuzhan Fatih Kar",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcd",
          "name": "Amir Zamir",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:59:07.000Z",
      "submittedOnDailyAt": "2025-07-07T06:51:04.452Z",
      "title": "GPT-4o는 시각에 어느 정도 이해하고 있는지 평가하고, 표준적인 컴퓨터 비전 태스크에 대한 다모달 기반 모델을 평가합니다.",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": true,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "다모뎔기 모델(예: GPT-4o)는 최근에 놀라운 진전을 거치면서 있지만, 이 모델들이 시각을 이해하는 정도는 명확하지 않습니다. 본 논문에서는, 일반적인 컴퓨터 비전 태스크(세ман틱 세그먼트션, 물체 탐지, 이미지 분류, 깊이 및 표면 정규화 예측)에서 인기 있는 다모뎔기 기반 모델(GPT-4o, o4-mini, Gemini 1.5 Pro, Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2)의 성능을 벤치마크 합니다. 사용된 기존 데이터셋에는 COCO, ImageNet 및 그 변형이 포함됩니다.\n\n이것을 수행하기 위한 주요 문제점은 1) 많은 모델은 텍스트를 출력하기 위한 훈련을 받고 있으며, 기본적으로 분할이나 3D 일반화된 영역을 표현할 수 없기 때문입니다. 2) 다양한 고급 모델은 API 레벨에서만 액세스 가능하며, 가중치에 액세스하여 모델 조정이 불가능합니다. 이러한 문제를 해결하기 위해, 표준적인 시각 태스크를 등가한 텍스트 프롬프트와 API에 대응하는 태스크로 번역하고, 표준화된 벤치마크 프레임워크를 프로ン퓰트 튜닝을 사용하여 만듭니다.\n\n이 모델들의 성능에 대해 1) 어떤 태스크에서도 가장 선진한 전문 모델과 가까운 것이 아닙니다. 2) 그러나, 존중받는 일반적인 모델임을 특징으로 합니다. 이는 주로 이미지 텍스트 기반의 태스크에서 훈련되었기 때문입니다. 3) 세ман틱 태스크의 성능은 일반적인 태스크보다 훨씬 높습니다. 4) 프로ン퓰트 튜닝 기술은 성능에 영향을 미칩니다. 그러나 더 좋은 모델은 프롬프트의 변화에 대한 민감도가 낮습니다. 5) GPT-4o는 논리 모델이 아닙니다. 6) 6가지 태스크 중 4가지에서 가장 우수한 모델입니다. 6) 논리 모델인 o3은 일반적인 태스크에 대한 개선이 관찰됩니다. 7) 최신 GPT-4o 및 같은 본래적인 이미지 생성 기능을 가진 모델의 초기 분석은, 이러한 모델이 훌륭한 특징을 나타내는 것을 알 수 있습니다.",
      "upvotes": 0,
      "discussionId": "686b8348213f123a1f88bdce",
      "ai_summary": "Multimodal foundation models, despite being primarily trained on image-text tasks, demonstrate respectable performance across various vision tasks when adapted through prompt chaining, though they fall short compared to specialized models.",
      "ai_keywords": [
        "GPT-4o",
        "o4-mini",
        "Gemini 1.5 Pro",
        "Gemini 2.0 Flash",
        "Claude 3.5 Sonnet",
        "Qwen2-VL",
        "Llama 3.2",
        "semantic segmentation",
        "object detection",
        "image classification",
        "depth prediction",
        "surface normal prediction",
        "COCO",
        "ImageNet",
        "prompt chaining",
        "reasoning models",
        "hallucinations",
        "spatial misalignments"
      ]
    },
    "publishedAt": "2025-07-02T13:59:07.000Z",
    "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation\n  Models on Standard Computer Vision Tasks",
    "summary": "Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 908
    },
    "isAuthorParticipating": false
  }
]