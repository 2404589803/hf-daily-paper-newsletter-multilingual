[
  {
    "paper": {
      "id": "2503.14456",
      "authors": [
        {
          "_id": "67da21ed78c08b432f9fee0c",
          "user": {
            "_id": "62b3d8d651b07307bd12b7f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655953609090-noauth.jpeg",
            "isPro": false,
            "fullname": "BlinkDL",
            "user": "BlinkDL",
            "type": "user"
          },
          "name": "Bo Peng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:45.587Z",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee0d",
          "user": {
            "_id": "6418629fd13ffa408128d7ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679319546731-noauth.png",
            "isPro": false,
            "fullname": "Zhang Ruichong",
            "user": "ZhangRC",
            "type": "user"
          },
          "name": "Ruichong Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:54.749Z",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee0e",
          "user": {
            "_id": "647f4bac45baf21ad709fcd0",
            "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
            "isPro": false,
            "fullname": "Dan Goldstein",
            "user": "SmerkyG",
            "type": "user"
          },
          "name": "Daniel Goldstein",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:57.333Z",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee0f",
          "name": "Eric Alcaide",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee10",
          "name": "Haowen Hou",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee11",
          "name": "Janna Lu",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee12",
          "name": "William Merrill",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee13",
          "user": {
            "_id": "622c062645261ac5cc0bda94",
            "avatarUrl": "/avatars/ce544b74110f7fe1ad11a3939526f5da.svg",
            "isPro": false,
            "fullname": "Guangyu Song",
            "user": "Guangyu",
            "type": "user"
          },
          "name": "Guangyu Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-19T09:50:42.957Z",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee14",
          "name": "Kaifeng Tan",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee15",
          "user": {
            "_id": "638f1fd8c4444c6ca86ff823",
            "avatarUrl": "/avatars/405807c3868663246cfe371a2034f351.svg",
            "isPro": false,
            "fullname": "saitejautpala",
            "user": "saitejautpala",
            "type": "user"
          },
          "name": "Saiteja Utpala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:48:50.276Z",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee16",
          "user": {
            "_id": "63cac1a50932c72f13995d6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63cac1a50932c72f13995d6f/Bd9jEsCL9yWXdyAQCx61J.jpeg",
            "isPro": false,
            "fullname": "Nathan Wilce",
            "user": "m8than",
            "type": "user"
          },
          "name": "Nathan Wilce",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-19T09:50:59.151Z",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee17",
          "name": "Johan S. Wind",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee18",
          "name": "Tianyi Wu",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee19",
          "name": "Daniel Wuttke",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee1a",
          "user": {
            "_id": "6584f042b378d311dccea501",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vkq1AQhcuZwIjpFDkdgPQ.png",
            "isPro": false,
            "fullname": "Christian Zhou-Zheng",
            "user": "ChristianAzinn",
            "type": "user"
          },
          "name": "Christian Zhou-Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-19T09:51:20.835Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:31:05.000Z",
      "submittedOnDailyAt": "2025-03-19T00:29:42.147Z",
      "title": "RWKV-7 \"Goose\"는 표현적인 동적인 상태 진화 기능을 가진 모델입니다.",
      "submittedOnDailyBy": {
        "_id": "6418629fd13ffa408128d7ae",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679319546731-noauth.png",
        "isPro": false,
        "fullname": "Zhang Ruichong",
        "user": "ZhangRC",
        "type": "user"
      },
      "summary": "RWKV-7 \"Goose\"는 새로운 시퀀스 모델링 아키텍처입니다. 또한 3억 파라미터 규모로 여러 언어의 태스크에서 하류 성능을 새로운 최선으로 설정하고, 다른 3억 모델과 비교하여 대폭 적은 토큰으로 훈련해도 현재의 최선 수준의 영어 성능과 일치합니다. 그러나 RWKV-7 모델은 각 토큰당 고정 메모리 사용과 고정 추론 시간 필요합니다. RWKV-7은 벡터값付き 게이팅과 인코텍스트 학습 속도를 포함하는 delta룰의 새로운 일반화 공식을 도입하고, 값변환룰을 완화합니다. RWKV-7은 상태 트래킹을 수행하며, 정규 언어를 모두 인식할 수 있으며, 동시에 훈련할 수 있습니다. 이는 표준의 복잡도 가정에 의존하는 Transformer의 능력을 초월합니다. RWKV-7의 언어 모델링 능력을 보여주기 위해, 확장된 오픈소스 3.1트リリオン 토큰 다언어 코퍼스를 공개하고, 이 데이터셋에서 0.19억から29억 파라미터의 4개의 RWKV-7 모델을 훈련합니다.\n\nRWKV 모델과 데이터셋 리스트는 https://huggingface.co/RWKV에서 공개되며, 훈련과 추론 코드는 https://github.com/RWKV/RWKV-LM에서 공개됩니다. 모두는 Apache 2.0 License 아래 제공됩니다.",
      "upvotes": 65,
      "discussionId": "67da21ee78c08b432f9fee71",
      "projectPage": "https://rwkv.cn",
      "githubRepo": "https://github.com/RWKV/RWKV-LM",
      "ai_keywords": [
        "sequence modeling architecture",
        "pre-trained language models",
        "downstream performance",
        "multilingual tasks",
        "in-context learning rates",
        "delta rule",
        "vector-valued gating",
        "value replacement rule",
        "state tracking",
        "regular languages",
        "parallelizability of training",
        "Transformers",
        "$\\mathsf{TC}^0$"
      ]
    },
    "publishedAt": "2025-03-18T13:31:05.000Z",
    "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
    "summary": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\nTC^0. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14456.png",
    "numComments": 7,
    "submittedBy": {
      "_id": "6418629fd13ffa408128d7ae",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679319546731-noauth.png",
      "fullname": "Zhang Ruichong",
      "name": "ZhangRC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14378",
      "authors": [
        {
          "_id": "67da1ee1f1a4a52e8a1e0241",
          "user": {
            "_id": "64b7833aa5018e3c7c9b50d8",
            "avatarUrl": "/avatars/782415605ed786b73f484fcc86a6384f.svg",
            "isPro": false,
            "fullname": "Zechen Bai",
            "user": "ZechenBai",
            "type": "user"
          },
          "name": "Zechen Bai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:45:09.659Z",
          "hidden": false
        },
        {
          "_id": "67da1ee1f1a4a52e8a1e0242",
          "name": "Hai Ci",
          "hidden": false
        },
        {
          "_id": "67da1ee1f1a4a52e8a1e0243",
          "user": {
            "_id": "63a55320ce5763e06f78519c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671779060549-noauth.jpeg",
            "isPro": false,
            "fullname": "Mike Shou",
            "user": "mikeshou",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-19T01:33:23.736Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b7833aa5018e3c7c9b50d8/1jxSsiEAyMr5fSng7GOB3.mp4"
      ],
      "publishedAt": "2025-03-18T16:10:24.000Z",
      "submittedOnDailyAt": "2025-03-19T00:11:44.927Z",
      "title": "Impossible Videos는 불가능한 비디오 소스의 시리즈입니다. 이 시리즈는 비디오 소스의 가능성과 한계를 탐색하기 위해, 사용자 정의 비디오 소스의 기술을 개발하고 있습니다. 이 시리즈는 비디오 소스의 기술이 더욱 발전하여 비디오 소스의 가능성을 최대한 발휘할 수 있도록 비디오 소스의 기술의 발전을 목표로 하고 있습니다.",
      "submittedOnDailyBy": {
        "_id": "64b7833aa5018e3c7c9b50d8",
        "avatarUrl": "/avatars/782415605ed786b73f484fcc86a6384f.svg",
        "isPro": false,
        "fullname": "Zechen Bai",
        "user": "ZechenBai",
        "type": "user"
      },
      "summary": "현재의 합성 비디오는 실세계의 비디오의 데이터 부족과 다양성을 보완하기 위해 광범위하게 사용되고 있습니다. 현재의 합성 데이터셋은 주로 실세계의 시나리오를 재현하기 때문에, 불가능한, 반례적인, 또는 실제와 다른 비디오의 개념은 조사가 부족합니다. 본 논문은 두 가지 질문을 해결하기 위해 준비되어 있습니다: 1) 오늘날의 비디오 생성 모델은 불가능한 비디오 콘텐츠를 생성할 수 있는가? 2) 오늘날의 비디오 이해 모델은 불가능한 비디오를 이해할 수 있는가? 이를 위해, IPV-Bench라는 새로운 벤치마크를 소개합니다. 이 것은 비디오 이해와 생성의 발전을 평가하고 촉진하기 위해 설계되었습니다. IPV-Bench는 물리적, 생물적, 지리적, 또는 사회적인 법칙에 반하는 다양한 시나리오를 포함하는 세부적인 기술에 기반합니다. 이 기술에 기반하여, 비디오 생성 모델의 평가용 Prompt 시트를 구축하고, Prompt의 추적과 창의적인 능력에 대해 평가합니다. 또한, 불가능한 비디오의 이해 능력을 평가하기 위해 비디오 벤치마크를 선택하고, 시간적인 동작과 세계 지식의 추론이 특히 필요함을 평가합니다. 세부적인 평가는 비디오 모델의 미래 방향에 대한 제한과 조언을 제시하고, 다음 세대의 비디오 모델의 개발에 길을 명확히 열어줍니다.",
      "upvotes": 37,
      "discussionId": "67da1ee3f1a4a52e8a1e02df",
      "projectPage": "https://showlab.github.io/Impossible-Videos/",
      "githubRepo": "https://github.com/showlab/Impossible-Videos",
      "ai_keywords": [
        "IPV-Bench",
        "taxonomy",
        "prompt suite",
        "video generation models",
        "prompt following",
        "creativity capabilities",
        "video benchmark",
        "Video-LLMs",
        "temporal dynamics",
        "world knowledge"
      ]
    },
    "publishedAt": "2025-03-18T12:10:24.000Z",
    "title": "Impossible Videos",
    "summary": "Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b7833aa5018e3c7c9b50d8/1jxSsiEAyMr5fSng7GOB3.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b7833aa5018e3c7c9b50d8",
      "avatarUrl": "/avatars/782415605ed786b73f484fcc86a6384f.svg",
      "fullname": "Zechen Bai",
      "name": "ZechenBai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14478",
      "authors": [
        {
          "_id": "67da34a648348387ebac36ff",
          "name": "Xinyu Fang",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3700",
          "name": "Zhijian Chen",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3701",
          "name": "Kai Lan",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3702",
          "name": "Shengyuan Ding",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3703",
          "name": "Yingji Liang",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3704",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3705",
          "name": "Farong Wen",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3706",
          "name": "Zicheng Zhang",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3707",
          "name": "Guofeng Zhang",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3708",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3709",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac370a",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:51:34.000Z",
      "submittedOnDailyAt": "2025-03-19T01:40:40.617Z",
      "title": "Creation-MMBench: MLLM의 컨텍스트 인식 창의적 지능 평가",
      "submittedOnDailyBy": {
        "_id": "64f5f8dd9b17cd59c453c57f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
        "isPro": false,
        "fullname": "Xinyu Fang",
        "user": "nebulae09",
        "type": "user"
      },
      "summary": "창조성은 지능의 기본적인 측면이며, 다양한 맥락에서 새로운 및 적절한 해결책을 생성하는 능력에 포함됩니다. 이 분야에서 멀티모듈 대 언어 모델(MLLMs)의 창의성 평가는 아직 크게 탐구되어 있습니다. 이를 채워주기 위해, 우리는 Creation-MMBench라는, 실제 세계적인 이미지 기반 태스크에서 MLLMs의 창의성 평가를 목적으로 하는 멀티모듈 벤치마크를 소개합니다. 벤치마크는 51개의 세부화된 태스크를 거치고, 765개의 테스트 케이스를 포함합니다. 엄격한 평가를 보장하기 위해, 각 테스트 케이스에 대해 인스턴스별 평가기준을 정의하고, 일반적인 응답의 질과 시각 입력과의 사실적인 일치성을 동시에 평가하기 위해 가이드를 제공합니다. 실험 결과를 통해, 현재의 오픈 소스 MLLMs는 창의적인 태스크에서 저작권 모델에 비해 크게 떨어집니다. 또한, 우리의 분석은 시각적 최종 훈련이 베이스 LLM의 창의적 능력에 부정적인 영향을 미치는 것을 보여줍니다. Creation-MMBench는 MLLM의 창의성 발전에 대한 유익한 통찰을 제공하며, 멀티모듈 생성 지능의 미래적인 발전을 기반으로 합니다. 모든 데이터와 평가 코드는 https://github.com/open-compass/Creation-MMBench에 공개되어 있습니다.",
      "upvotes": 35,
      "discussionId": "67da34ad48348387ebac3926",
      "projectPage": "https://open-compass.github.io/Creation-MMBench/",
      "githubRepo": "https://github.com/open-compass/Creation-MMBench",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Creation-MMBench",
        "image-based tasks",
        "instance-specific evaluation criteria",
        "visual fine-tuning",
        "multimodal generative intelligence"
      ]
    },
    "publishedAt": "2025-03-18T13:51:34.000Z",
    "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
    "summary": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14478.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f5f8dd9b17cd59c453c57f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
      "fullname": "Xinyu Fang",
      "name": "nebulae09",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14476",
      "authors": [
        {
          "_id": "67da2b54e5335651349e262c",
          "name": "Qiying Yu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e262d",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e262e",
          "name": "Ruofei Zhu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e262f",
          "name": "Yufeng Yuan",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2630",
          "name": "Xiaochen Zuo",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2631",
          "name": "Yu Yue",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2632",
          "name": "Tiantian Fan",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2633",
          "name": "Gaohong Liu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2634",
          "name": "Lingjun Liu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2635",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2636",
          "name": "Haibin Lin",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2637",
          "name": "Zhiqi Lin",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2638",
          "name": "Bole Ma",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2639",
          "name": "Guangming Sheng",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263a",
          "name": "Yuxuan Tong",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263b",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263c",
          "name": "Mofan Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263d",
          "name": "Wang Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263e",
          "name": "Hang Zhu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263f",
          "name": "Jinhua Zhu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2640",
          "name": "Jiaze Chen",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2641",
          "name": "Jiangjie Chen",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2642",
          "name": "Chengyi Wang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2643",
          "name": "Hongli Yu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2644",
          "name": "Weinan Dai",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2645",
          "name": "Yuxuan Song",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2646",
          "name": "Xiangpeng Wei",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2647",
          "name": "Hao Zhou",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2648",
          "name": "Jingjing Liu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2649",
          "name": "Wei-Ying Ma",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264a",
          "name": "Ya-Qin Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264b",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264c",
          "name": "Mu Qiao",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264d",
          "name": "Yonghui Wu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264e",
          "name": "Mingxuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:49:06.000Z",
      "submittedOnDailyAt": "2025-03-19T00:56:39.773Z",
      "title": "DAPO: 규모화된 오픈소스 LLM 강화학습 시스템",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "推理스케일링은, 전례가없는 역량력을 가진 LLM에 힘을 넣어, 강화학습이 핵심기술로 복잡한 이유를 도출하는 데 핵심 역할을 한다. 그러나, 가장 先端의 LLM의 중요한 기술 세부 사항은 은닉되어있고(예를 들어, OpenAI o1 블로그와 DeepSeek R1 기술보고서), 커뮤니티는 그 RL 훈련 결과를 재현하기 위해 어려움을 겪고 있다. 우리는 Decoupled Clip and Dynamic sAmpling Policy Optimization(DAPO) 알고리즘을 제안하고, Qwen2.5-32B 모델을 사용하여 AIME 2024에서 50점을 달성하는 가장 큰 규모의 RL 시스템의 완전한 오픈소스화를 진행합니다. 이전 연구와는 다르게, 훈련 세부 사항은 은닉하지 않고, 우리는 알고리즘의 4가지 중요한 기술을 소개하고, 큰 규모의 LLM의 RL을 성공적으로 수행할 수 있음을 보여줍니다. 또한, 우리는 verlframe에 의해 구축된 훈련 코드와, 잘 선택되고 처리된 데이터셋을 오픈소스화합니다. 이러한 오픈소스 시스템의 구성 요소는 재현성을 높이며, 큰 규모의 LLM의 RL에 대한 향후 연구를 지원합니다.",
      "upvotes": 23,
      "discussionId": "67da2b55e5335651349e26c7",
      "ai_keywords": [
        "inference scaling",
        "LLMs (Large Language Models)",
        "reinforcement learning (RL)",
        "Decoupled Clip and Dynamic Sampling Action Policy Optimization (DAPO)",
        "Qwen2.5-32B",
        "AIME 2024",
        "verl framework"
      ]
    },
    "publishedAt": "2025-03-18T13:49:06.000Z",
    "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
    "summary": "Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe Decoupled Clip and Dynamic sAmpling\nPolicy Optimization (DAPO) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14476.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6398
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12797",
      "authors": [
        {
          "_id": "67da2c83aa2c34f7d95e46ff",
          "user": {
            "_id": "63c3b67ec7d7f4c63a4eea3a",
            "avatarUrl": "/avatars/4a5f98cb6b0c1e37a2c09af72f7a9946.svg",
            "isPro": false,
            "fullname": "Xinyu Ma",
            "user": "MaxyLee",
            "type": "user"
          },
          "name": "Xinyu Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:31.865Z",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4700",
          "user": {
            "_id": "65903c4aa78a277803bde77b",
            "avatarUrl": "/avatars/061388320ccf1a66e1e99519dd426a60.svg",
            "isPro": false,
            "fullname": "Ziyang Ding",
            "user": "sdudzy",
            "type": "user"
          },
          "name": "Ziyang Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:35.757Z",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4701",
          "name": "Zhicong Luo",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4702",
          "user": {
            "_id": "642086ed290342c5df85662d",
            "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
            "isPro": false,
            "fullname": "Chi Chen",
            "user": "carboncoo",
            "type": "user"
          },
          "name": "Chi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:44:39.356Z",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4703",
          "name": "Zonghao Guo",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4704",
          "name": "Derek F. Wong",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4705",
          "name": "Xiaoyi Feng",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4706",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T04:06:34.000Z",
      "submittedOnDailyAt": "2025-03-19T01:06:07.094Z",
      "title": "DeepPerception: 딥피렉션: 딥라이덴티피케이션에 의한 지식밀집형 비주얼게임팅의 발전",
      "submittedOnDailyBy": {
        "_id": "642086ed290342c5df85662d",
        "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
        "isPro": false,
        "fullname": "Chi Chen",
        "user": "carboncoo",
        "type": "user"
      },
      "summary": "人间の専門家は、領域知識を活用して視覚的特徴を詳細に整えることで、細かい視覚的単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の単語の",
      "upvotes": 20,
      "discussionId": "67da2c85aa2c34f7d95e4796",
      "projectPage": "https://deepperception-kvg.github.io",
      "githubRepo": "https://github.com/thunlp/DeepPerception",
      "ai_keywords": [
        "knowledge-intensive visual grounding (KVG)",
        "DeepPerception",
        "automated data synthesis pipeline",
        "supervised fine-tuning",
        "reinforcement learning",
        "perception-cognition synergy",
        "KVG-Bench",
        "cognitive reasoning scaffolding",
        "cross-domain generalization",
        "cognitive processes",
        "multimodal reasoning"
      ]
    },
    "publishedAt": "2025-03-17T00:06:34.000Z",
    "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding",
    "summary": "Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12797.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642086ed290342c5df85662d",
      "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
      "fullname": "Chi Chen",
      "name": "carboncoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12329",
      "authors": [
        {
          "_id": "67d8e115f55b855ae6d8f29b",
          "user": {
            "_id": "63340dbbd92c5842ae71d1e9",
            "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
            "isPro": false,
            "fullname": "Kanzhi Cheng",
            "user": "cckevinn",
            "type": "user"
          },
          "name": "Kanzhi Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T11:34:32.729Z",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f29c",
          "user": {
            "_id": "653f8cc9d4d0924ad2404d86",
            "avatarUrl": "/avatars/83defb31d669393447ee04fe9989b96b.svg",
            "isPro": false,
            "fullname": "Wenpo Song",
            "user": "songwp",
            "type": "user"
          },
          "name": "Wenpo Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:46:11.039Z",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f29d",
          "name": "Jiaxin Fan",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f29e",
          "name": "Zheng Ma",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f29f",
          "name": "Qiushi Sun",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a0",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a1",
          "name": "Chenyang Yan",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a2",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a3",
          "name": "Jianbing Zhang",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a4",
          "name": "Jiajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T02:56:09.000Z",
      "submittedOnDailyAt": "2025-03-19T02:04:01.010Z",
      "title": "CapArena: LLM 시대의 세부 이미지 설명의 기준과 분석",
      "submittedOnDailyBy": {
        "_id": "63340dbbd92c5842ae71d1e9",
        "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
        "isPro": false,
        "fullname": "Kanzhi Cheng",
        "user": "cckevinn",
        "type": "user"
      },
      "summary": "그림 캡쳐는 시각 언어 연구의 오랜 숙제 중 하나이며, LLMs의 증가로 인해 현대 시각 언어 모델(VLMs)은 세부적이고 전面的한 그림 설명을 생성하고 있습니다. 그러나 이러한 캡쳐의 품질을 평가하기 위한 벤치마크화는 해결되지 않았습니다. 본 논문은 두 가지 중요한 질문에 초점을 맞추고 있습니다: 1) 현재의 VLMs이 실제 그림 캡쳐에서 어떤 성능을 보여주고, 특히 인간과 비교했을 때 어떤 성능을 보여주고 있는지? 우리는 CapArena라는 플랫폼을 구축하여 6000개 이상의 대화형 캡쳐 대결과 고품질의 인간의 취향 투표를 제공했습니다. 이 아리언 모델 평가는 선도 모델이 GPT-4o가 인간의 성능을 달성하거나 초과하는 것을 보여주고, 많은 오픈 소스 모델이 그 뒤에 남아있는 것을 밝혀냅니다. 2) 자동화된 메트릭이 상세한 캡쳐의 품질을 신뢰적으로 평가할 수 있는지? CapArena에서 인간 Annotation을 사용하여 전통적인 및 최근의 캡쳐 메트릭 및 VLM-as-a-Judge를 평가했습니다. 분석에 따르면 특정 메트릭(예: METEOR)은 인간과 캡쳐 수준의 일치가 좋은 것을 보여주지만, 모델의 순위의 불확실성을招く 시스템적 편향을 밝혀냅니다. VLM-as-a-Judge는 반대로, 캡쳐 수준과 모델 수준에서 강한 식별력을 보여주고 있습니다. 이러한 통찰에 기반하여 CapArena-Auto라는 정확한 및 효율적인 자동화된 벤치마크를 릴리즈하고, 인간 순위와의 94.3%의 상관관계를 달성하고, 테스트의 비용은 4달러임을 보여줍니다. 데이터와 리소스는 https://caparena.github.io에서 공개됩니다.",
      "upvotes": 18,
      "discussionId": "67d8e118f55b855ae6d8f34e",
      "projectPage": "https://caparena.github.io/",
      "githubRepo": "https://github.com/njucckevin/CapArena",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "GPT-4o",
        "CapArena",
        "pairwise caption battles",
        "high-quality human preference votes",
        "VLM-as-a-Judge",
        "METEOR",
        "CapArena-Auto"
      ]
    },
    "publishedAt": "2025-03-15T22:56:09.000Z",
    "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the\n  LLM Era",
    "summary": "Image captioning has been a longstanding challenge in vision-language\nresearch. With the rise of LLMs, modern Vision-Language Models (VLMs) generate\ndetailed and comprehensive image descriptions. However, benchmarking the\nquality of such captions remains unresolved. This paper addresses two key\nquestions: (1) How well do current VLMs actually perform on image captioning,\nparticularly compared to humans? We built CapArena, a platform with over 6000\npairwise caption battles and high-quality human preference votes. Our\narena-style evaluation marks a milestone, showing that leading models like\nGPT-4o achieve or even surpass human performance, while most open-source models\nlag behind. (2) Can automated metrics reliably assess detailed caption quality?\nUsing human annotations from CapArena, we evaluate traditional and recent\ncaptioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while\nsome metrics (e.g., METEOR) show decent caption-level agreement with humans,\ntheir systematic biases lead to inconsistencies in model ranking. In contrast,\nVLM-as-a-Judge demonstrates robust discernment at both the caption and model\nlevels. Building on these insights, we release CapArena-Auto, an accurate and\nefficient automated benchmark for detailed captioning, achieving 94.3%\ncorrelation with human rankings at just $4 per test. Data and resources will be\nopen-sourced at https://caparena.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12329.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63340dbbd92c5842ae71d1e9",
      "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
      "fullname": "Kanzhi Cheng",
      "name": "cckevinn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13424",
      "authors": [
        {
          "_id": "67da22b75fe852c86d3c419b",
          "name": "Xinyu Lian",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c419c",
          "name": "Zichao Yu",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c419d",
          "name": "Ruiming Liang",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c419e",
          "name": "Yitong Wang",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c419f",
          "name": "Li Ray Luo",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a0",
          "name": "Kaixu Chen",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a1",
          "name": "Yuanzhen Zhou",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a2",
          "name": "Qihong Tang",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a3",
          "name": "Xudong Xu",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a4",
          "name": "Zhaoyang Lyu",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a5",
          "name": "Bo Dai",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a6",
          "name": "Jiangmiao Pang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63f2ec797ddf724fbcc75aee/7Is4N1AFDor-EuzkZEiui.mp4"
      ],
      "publishedAt": "2025-03-17T17:53:56.000Z",
      "submittedOnDailyAt": "2025-03-19T00:24:14.520Z",
      "title": "무한한 이동성: 구조화 생성에 의한 연결물체의 스케일러블 고품질 합성",
      "submittedOnDailyBy": {
        "_id": "63f2ec797ddf724fbcc75aee",
        "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
        "isPro": false,
        "fullname": "Zhaoyang Lyu",
        "user": "ZhaoyangLyu",
        "type": "user"
      },
      "summary": "큰 규모의 고품질의 아트 작업이 다양한 구체적인 AI에 관련된 여러 작업에 급히 필요로 되어 있습니다. 현재 아트 작업된 물체의 제작 방법은 거의 데이터 주도 또는 시뮬레이션 기반이며, 이러한 방식은 학습 데이터의 규모, 품질, 시뮬레이션의 정확성 및 복잡성에 의해 제한되어 있습니다. 본 논문에서는, 프로세스 생성을 통해 고품질의 아트 작업된 물체의 합성에 새로운 방법인 \"Infinite Mobility\"를 제안합니다. 사용자 연구와 정량적 평가에 따라, 우리의 방법은 현재의 가장 선진한 방법보다 뛰어나며, 물리적 특성 및 메쉬의 품질에 대한 인간 Annotation된 데이터셋과 비교하여 비슷한 결과를 얻을 수 있음을 보여주었습니다. 또한, 우리의 합성 데이터는 생성 모델의 학습 데이터로 사용될 수 있으며, 다음 단계의 규모 업을 가능하게 합니다. 코드는 https://github.com/Intern-Nexus/Infinite-Mobility에서 사용 가능합니다.",
      "upvotes": 13,
      "discussionId": "67da22bb5fe852c86d3c4304",
      "projectPage": "https://infinite-mobility.github.io/",
      "githubRepo": "https://github.com/Intern-Nexus/Infinite-Mobility",
      "ai_keywords": [
        "articulated objects",
        "high-fidelity",
        "embodied AI",
        "data-driven",
        "simulation-based",
        "procedural generation",
        "physics property",
        "mesh quality",
        "generative models"
      ]
    },
    "publishedAt": "2025-03-17T13:53:56.000Z",
    "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation",
    "summary": "Large-scale articulated objects with high quality are desperately needed for\nmultiple tasks related to embodied AI. Most existing methods for creating\narticulated objects are either data-driven or simulation based, which are\nlimited by the scale and quality of the training data or the fidelity and heavy\nlabour of the simulation. In this paper, we propose Infinite Mobility, a novel\nmethod for synthesizing high-fidelity articulated objects through procedural\ngeneration. User study and quantitative evaluation demonstrate that our method\ncan produce results that excel current state-of-the-art methods and are\ncomparable to human-annotated datasets in both physics property and mesh\nquality. Furthermore, we show that our synthetic data can be used as training\ndata for generative models, enabling next-step scaling up. Code is available at\nhttps://github.com/Intern-Nexus/Infinite-Mobility",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63f2ec797ddf724fbcc75aee/7Is4N1AFDor-EuzkZEiui.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13424.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f2ec797ddf724fbcc75aee",
      "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
      "fullname": "Zhaoyang Lyu",
      "name": "ZhaoyangLyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14125",
      "authors": [
        {
          "_id": "67da200db41738a058666623",
          "name": "Defa Zhu",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666624",
          "name": "Hongzhi Huang",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666625",
          "name": "Jundong Zhou",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666626",
          "user": {
            "_id": "65a62085576772f531e13856",
            "avatarUrl": "/avatars/72c67a60422e333ea4e323f7480ae0b7.svg",
            "isPro": false,
            "fullname": "Huang Zihao",
            "user": "FetchFortune",
            "type": "user"
          },
          "name": "Zihao Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:45:07.454Z",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666627",
          "user": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "isPro": false,
            "fullname": "Yutao Zeng",
            "user": "Taoer",
            "type": "user"
          },
          "name": "Yutao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:45:05.382Z",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666628",
          "name": "Banggu Wu",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666629",
          "name": "Qiyang Min",
          "hidden": false
        },
        {
          "_id": "67da200db41738a05866662a",
          "name": "Xun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T10:37:50.000Z",
      "submittedOnDailyAt": "2025-03-19T00:09:57.233Z",
      "title": "Frac-Connections: 분수 확장의 초연결\n\n(注意：虽然您要求不添加任何解释或额外的文本，但为了确保翻译的准确性和专业性，我提供了一个更自然的翻译版本。如果您需要严格保持原文的翻译，请告知。)",
      "submittedOnDailyBy": {
        "_id": "667505f4361b960c79e35486",
        "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
        "isPro": false,
        "fullname": "Defa Zhu",
        "user": "mathfinder",
        "type": "user"
      },
      "summary": "残差コネクション은 현대의 심층 학습 아키텍처의 핵심 구성 요소 중 하나이며, 경사 소실 현상을 완화하여 매우 깊은 신경망의 훈련을 가능하게 해줍니다. Hyper-Connections는 최근에 제안된residual connection을 확장하여 서로 다른 깊이에서 여러 개의 connection 强度을 도입하고, 경사 소실 및 표현 파괴의 셈플 효과에 대한 해결책을 제시하여 새로운 접근 방식을 제공합니다. 그러나 Hyper-Connections는 은닉 상태의 폭을 확장하여 메모리 액세스 비용에 영향을 미칩니다. 본 논문에서는Frac-Connections라는 새로운 접근 방식을 제안하고, 은닉 상태를 여러 부분으로 분할함으로써Hyper-Connections의 일부 이익을 유지하면서 메모리 소비를 줄일 수 있는 방법을 제시합니다. 이를 효과적으로 증명하기 위해 언어 태스크에 대한 규모가 큰 실험을 수행하였으며, 최대 7B MoE 모델을 3T 토큰으로 훈련한 것을 통해Frac-Connections가 residual connection을 크게 초월함을 보여줍니다.",
      "upvotes": 11,
      "discussionId": "67da200eb41738a058666690",
      "ai_keywords": [
        "residual connections",
        "gradient vanishing",
        "Hyper-Connections",
        "multiple connection strengths",
        "seesaw effect",
        "representation collapse",
        "Frac-Connections",
        "hidden states",
        "language tasks",
        "MoE model"
      ]
    },
    "publishedAt": "2025-03-18T06:37:50.000Z",
    "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
    "summary": "Residual connections are central to modern deep learning architectures,\nenabling the training of very deep networks by mitigating gradient vanishing.\nHyper-Connections recently generalized residual connections by introducing\nmultiple connection strengths at different depths, thereby addressing the\nseesaw effect between gradient vanishing and representation collapse. However,\nHyper-Connections increase memory access costs by expanding the width of hidden\nstates. In this paper, we propose Frac-Connections, a novel approach that\ndivides hidden states into multiple parts rather than expanding their width.\nFrac-Connections retain partial benefits of Hyper-Connections while reducing\nmemory consumption. To validate their effectiveness, we conduct large-scale\nexperiments on language tasks, with the largest being a 7B MoE model trained on\nup to 3T tokens, demonstrating that Frac-Connections significantly outperform\nresidual connections.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14125.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "667505f4361b960c79e35486",
      "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
      "fullname": "Defa Zhu",
      "name": "mathfinder",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14504",
      "authors": [
        {
          "_id": "67da436711b6db6920802e9e",
          "name": "Tao Yu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802e9f",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yi-Fan Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-19T04:09:15.055Z",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea0",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea1",
          "name": "Junkang Wu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea2",
          "name": "Jinda Lu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea3",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea4",
          "name": "Xingyu Lu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea5",
          "name": "Yunhang Shen",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea6",
          "name": "Guibin Zhang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea7",
          "name": "Dingjie Song",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea8",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea9",
          "name": "Tianlong Xu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802eaa",
          "name": "Qingsong Wen",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802eab",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802eac",
          "name": "Yan Huang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ead",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802eae",
          "name": "Tieniu Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:59:56.000Z",
      "submittedOnDailyAt": "2025-03-19T02:39:30.454Z",
      "title": "다모달 LLM와 인간 선호도 일치: 조사",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)은 간단한 프로ン퓰트에 의해 다양한 일반적인 태스크를 처리할 수 있으며, 특정 태스크에 대한 훈련이 필요하지 않습니다. 다 모델 대 언어 모델(MLLMs)은 LLMs를 기반으로 구축되어 있으며, 시각적, 听觉적, 텍스트 데이터에 대한 복잡한 태스크를 해결하기 위해 매력적인 가능성을 보여주고 있습니다. 그러나, 사실성, 안전성, O1 같은 추론, 인간적인 취향과의 어레이멘션에 대한 중요한 문제들은 충분히 해결되지 않았습니다. 이 공백은 다양한 어레이멘션 알고리즘의 발생을 촉진하고 있습니다. 최근의 연구는 어레이멘션 알고리즘이 전술로 언급된 문제를 해결하는 강력한 접근임을 보여줍니다. 본 논문에서는 MLLM의 어레이멘션 알고리즘에 대한 종합적인 시스템적 검토를 제공하는 것을 목표로 합니다. 특히, 다음 4가지 주요 면에 대해 조사할 것입니다: 1) 어레이멘션 알고리즘이 커버하는 태스크의 경우, 일반적인 이미지 이해, 멀티 이미지, 비디오, 음성, 확장된 다 모델 애플리케이션; 2) 어레이멘션 데이터 세트의 구축에 있어서 핵심적인 요인, 데이터 소스, 모델의 응답, 취향 注釈; 3) 어레이멘션 알고리즘의 평가에 사용되는 벤치마크; 4) 어레이멘션 알고리즘의 개발의 잠재적인 미래의 방향의 논의. 본 논문의 프로젝트 페이지는 https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment 에 접근 가능합니다.",
      "upvotes": 6,
      "discussionId": "67da436b11b6db6920803040",
      "githubRepo": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment",
      "ai_keywords": [
        "Large language models (LLMs)",
        "Multimodal Large Language Models (MLLMs)",
        "Truthfulness",
        "Safety",
        "o1-like reasoning",
        "Alignment with human preference",
        "Alignment algorithms",
        "General image understanding",
        "Multi-image",
        "Video",
        "Audio",
        "Extended multimodal applications",
        "Alignment datasets",
        "Data sources",
        "Model responses",
        "Preference annotations",
        "Benchmarks"
      ]
    },
    "publishedAt": "2025-03-18T13:59:56.000Z",
    "title": "Aligning Multimodal LLM with Human Preference: A Survey",
    "summary": "Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14504.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12505",
      "authors": [
        {
          "_id": "67d9442ec37d05ff0ab28e44",
          "name": "Zhaopan Xu",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e45",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e46",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e47",
          "name": "Wangbo Zhao",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e48",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e49",
          "name": "Xiaojiang Peng",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e4a",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e4b",
          "name": "Hongxun Yao",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e4c",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T13:50:38.000Z",
      "submittedOnDailyAt": "2025-03-19T00:16:20.299Z",
      "title": "MPBench: 프로세스 에러 검출을 위한 상세한 다모델 논리 벤치마크",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "이것은大型 언어 모델(LLMs)가 복잡한 작업 처리에 필수적인 능력 중 하나이며, 이 능력 향상을 위해 인식 과정 오류의 인식이 매우 중요합니다. 최근, 단계별로 보상 모델(PRMs)을 제안하여 단계별 보상을 제공하여 강화 학습과 데이터 생성을 촉진하고, 훈련 기간 동안 LLMs가 올바른 추론 단계를 진행하도록 지도하여 추론 정확도를 향상시키는 데 도움을 줍니다. 그러나 현재의 PRMs 기준 테스트는 텍스트 기반이며, 오류 감지에 중점을 두고 있으며, 추론 검색 등 다른 시나리오를 무시하고 있습니다. 이러한 차이를 해결하기 위해, 우리는 MPBench를 도입합니다. 이는 다양한 시나리오에서 PRMs의 유효성을 체계적으로 평가하기 위한 종합적이고 다 태스크, 다 모델의 기준입니다. MPBench는 추론 과정에서의 특정 역할을 위한 세 가지 평가 방식 중 하나를 적용하여 평가합니다: 1) 단계의 정확성, 각 중간 추론 단계의 정확성을 평가합니다; 2) 답변 집합, 여러 솔루션을 집합하여 가장 좋은 것을 선택합니다; 3) 추론 과정 검색, 추론 기간 동안 추론 단계의 검색을 지도합니다. 이러한 방식들로 MPBench는 전면적인 평가를 수행하며, 다 모델 PRMs 개발에 대한 통찰을 제공합니다.",
      "upvotes": 6,
      "discussionId": "67d94430c37d05ff0ab28eb3",
      "projectPage": "https://mpbench.github.io/",
      "ai_keywords": [
        "process-level reward models (PRMs)",
        "reinforcement learning",
        "step-wise rewards",
        "error detection",
        "reasoning search",
        "MPBench",
        "multi-task",
        "multimodal benchmark",
        "Step Correctness",
        "Answer Aggregation",
        "Reasoning Process Search"
      ]
    },
    "publishedAt": "2025-03-16T09:50:38.000Z",
    "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification",
    "summary": "Reasoning is an essential capacity for large language models (LLMs) to\naddress complex tasks, where the identification of process errors is vital for\nimproving this ability. Recently, process-level reward models (PRMs) were\nproposed to provide step-wise rewards that facilitate reinforcement learning\nand data production during training and guide LLMs toward correct steps during\ninference, thereby improving reasoning accuracy. However, existing benchmarks\nof PRMs are text-based and focus on error detection, neglecting other scenarios\nlike reasoning search. To address this gap, we introduce MPBench, a\ncomprehensive, multi-task, multimodal benchmark designed to systematically\nassess the effectiveness of PRMs in diverse scenarios. MPBench employs three\nevaluation paradigms, each targeting a specific role of PRMs in the reasoning\nprocess: (1) Step Correctness, which assesses the correctness of each\nintermediate reasoning step; (2) Answer Aggregation, which aggregates multiple\nsolutions and selects the best one; and (3) Reasoning Process Search, which\nguides the search for optimal reasoning steps during inference. Through these\nparadigms, MPBench makes comprehensive evaluations and provides insights into\nthe development of multimodal PRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12505.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14492",
      "authors": [
        {
          "_id": "67da2cbde5335651349e98c8",
          "name": "NVIDIA",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ca",
          "name": "Hassan Abu Alhaija",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98cb",
          "name": "Jose Alvarez",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98cc",
          "name": "Maciej Bala",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98cd",
          "name": "Tiffany Cai",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ce",
          "name": "Tianshi Cao",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98cf",
          "name": "Liz Cha",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d0",
          "name": "Joshua Chen",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d1",
          "name": "Mike Chen",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d2",
          "name": "Francesco Ferroni",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d3",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d4",
          "name": "Dieter Fox",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d5",
          "name": "Yunhao Ge",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d6",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d7",
          "name": "Ali Hassani",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d8",
          "name": "Michael Isaev",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d9",
          "name": "Pooya Jannaty",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98da",
          "name": "Shiyi Lan",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98db",
          "name": "Tobias Lasser",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98dc",
          "name": "Huan Ling",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98dd",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98de",
          "name": "Xian Liu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98df",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e0",
          "name": "Alice Luo",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e1",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e2",
          "name": "Hanzi Mao",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e3",
          "name": "Fabio Ramos",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e4",
          "name": "Xuanchi Ren",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e5",
          "name": "Tianchang Shen",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e6",
          "name": "Shitao Tang",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e7",
          "name": "Ting-Chun Wang",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e8",
          "name": "Jay Wu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e9",
          "name": "Jiashu Xu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ea",
          "name": "Stella Xu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98eb",
          "name": "Kevin Xie",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ec",
          "name": "Yuchong Ye",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ed",
          "name": "Xiaodong Yang",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ee",
          "name": "Xiaohui Zeng",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ef",
          "name": "Yu Zeng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:57:54.000Z",
      "submittedOnDailyAt": "2025-03-19T01:03:48.943Z",
      "title": "Cosmos Transformer 1: Adaptive Monomodal Control for Conditional World Generation",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "コスモス・トランスフェース, 조건付きの세계 생성 모형에 대해 소개합니다. 이 모형은 분할, 깊이, 엣지 등 다양한 모형의 여러 공간 제어 입력을 기반으로 세계 시뮬레이션을 생성할 수 있습니다. 설계적으로, 공간 조건付식 스케줄은 적응적이고 사용자定制 가능합니다. 이로 인해, 서로 다른 공간 위치에 조건付식 입력을 서로 다른 가중치로 할 수 있습니다. 이로 인해, 높은 수준의 제어 가능한 세계 생성이 가능하며, Sim2Real 등 다양한 세계에서 세계로의 피드백 시퀀스에서 사용될 수 있습니다. 이 모형의 기능에 대해 자세히 평가하고 물리 AI의 적용을 보여주고 있습니다. 특히, 로보틱스의 Sim2Real 및 자동차 데이터의 풍부화에 효과적입니다. 또한, NVIDIA GB200 NVL72 랙을 사용하여 실시간 세계 생성을 실현하는 추론 스케일링 단계를 보여주고 있습니다. 연구 개발의 가속화를 촉진하기 위해, 모형과 코드를 공개하고 있습니다. 공개 사이트는, https://github.com/nvidia-cosmos/cosmos-transfer1입니다.",
      "upvotes": 5,
      "discussionId": "67da2cc1e5335651349e9a3e",
      "ai_keywords": [
        "conditional world generation model",
        "world simulations",
        "spatial control inputs",
        "segmentation",
        "depth",
        "edge",
        "spatial conditional scheme",
        "adaptive",
        "customizable",
        "high controllable world generation",
        "world-to-world transfer",
        "Sim2Real",
        "Physical AI",
        "robotics Sim2Real",
        "autonomous vehicle data enrichment",
        "inference scaling strategy",
        "real-time world generation",
        "NVIDIA GB200 NVL72 rack"
      ]
    },
    "publishedAt": "2025-03-18T13:57:54.000Z",
    "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control",
    "summary": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6398
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14499",
      "authors": [
        {
          "_id": "67da2e831bba0f73374fd5a0",
          "name": "Thomas Kwa",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a1",
          "name": "Ben West",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a2",
          "name": "Joel Becker",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a3",
          "name": "Amy Deng",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a4",
          "name": "Katharyn Garcia",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a5",
          "name": "Max Hasin",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a6",
          "name": "Sami Jawhar",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a7",
          "name": "Megan Kinniment",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a8",
          "name": "Nate Rush",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a9",
          "name": "Sydney Von Arx",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5aa",
          "name": "Ryan Bloom",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5ab",
          "name": "Thomas Broadley",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5ac",
          "name": "Haoxing Du",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5ad",
          "name": "Brian Goodrich",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5ae",
          "name": "Nikola Jurkovic",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5af",
          "name": "Luke Harold Miles",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b0",
          "name": "Seraphina Nix",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b1",
          "name": "Tao Lin",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b2",
          "name": "Neev Parikh",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b3",
          "name": "David Rein",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b4",
          "name": "Lucas Jun Koba Sato",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b5",
          "name": "Hjalmar Wijk",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b6",
          "name": "Daniel M. Ziegler",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b7",
          "name": "Elizabeth Barnes",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b8",
          "name": "Lawrence Chan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:59:31.000Z",
      "submittedOnDailyAt": "2025-03-19T01:10:23.636Z",
      "title": "AI의 장기 태스크 완료 능력을 평가하는 방법",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "AI 벤치마크에서 급격한 발전에도 불구하고, 실제 세계적인 의미에서 벤치마크 성능은 명확하지 않습니다. 인간 능력과 비교하는 AI 시스템의 능력을 평가하기 위한 새로운 메트릭을 제안합니다: 50% 작업 완료 시간 호리조인. 이는 인간이 일반적으로 완료하는 시간으로, AI 모델이 50%의 성공률로 작업을 완료할 수 있는 데 걸리는 시간입니다. 먼저, RE-Bench, HCAST와 66건의 새로운 단축 작업의 조합으로, 관련 분야의 전문가인 인간을 시간 기록했습니다. 이 작업에서, 현재의 선도적인 AI 모델인 Claude 3.7 Sonnet은 약 50분의 호리조인을 가지고 있습니다. 또한, 2019년 이후, 선도적인 AI의 호리조인은 약 7개월마다 1배씩 증가하지만, 2024년에는 이 경향이 가속화되어 있습니다. AI 모델의 호리조인의 증가는 높은 신뢰성, 오류에 대한 적응력, 더 나은 논리적인 추론력과 도구 사용 능력의 조합으로 주로 주도되고 있습니다. 결과의 제한, 특히 외부 유효성 정도와 기능의 위험성에 대한 영향에 대해 논의합니다. 이러한 결과를 실제 세계적인 소프트웨어 작업에 일반화하는 경우, 이 경향의 외추로, 5년 이내에 현재인간이 1개월 걸리는 많은 소프트웨어 작업을 자동으로 처리하는 AI 시스템이 가능한 것으로 예측됩니다.",
      "upvotes": 4,
      "discussionId": "67da2e8a1bba0f73374fd89e",
      "ai_keywords": [
        "RE-Bench",
        "HCAST",
        "Claude 3.7 Sonnet",
        "50%-task-completion time horizon",
        "reliability",
        "ability to adapt to mistakes",
        "logical reasoning",
        "tool use capabilities"
      ]
    },
    "publishedAt": "2025-03-18T13:59:31.000Z",
    "title": "Measuring AI Ability to Complete Long Tasks",
    "summary": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark\nperformance remains unclear. To quantify the capabilities of AI systems in\nterms of human capabilities, we propose a new metric: 50%-task-completion time\nhorizon. This is the time humans typically take to complete tasks that AI\nmodels can complete with 50% success rate. We first timed humans with relevant\ndomain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter\ntasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet\nhave a 50% time horizon of around 50 minutes. Furthermore, frontier AI time\nhorizon has been doubling approximately every seven months since 2019, though\nthe trend may have accelerated in 2024. The increase in AI models' time\nhorizons seems to be primarily driven by greater reliability and ability to\nadapt to mistakes, combined with better logical reasoning and tool use\ncapabilities. We discuss the limitations of our results -- including their\ndegree of external validity -- and the implications of increased autonomy for\ndangerous capabilities. If these results generalize to real-world software\ntasks, extrapolation of this trend predicts that within 5 years, AI systems\nwill be capable of automating many software tasks that currently take humans a\nmonth.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14499.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6398
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14495",
      "authors": [
        {
          "_id": "67da454fd5132b0eebd066ff",
          "name": "Jiacheng Guo",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06700",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06701",
          "name": "Jiahao Qiu",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06702",
          "name": "Kaixuan Huang",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06703",
          "name": "Xinzhe Juan",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06704",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06705",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:58:28.000Z",
      "submittedOnDailyAt": "2025-03-19T02:48:17.494Z",
      "title": "시계열 일관성 기반 LLM 논리 과정 오류 검출",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "확인은 수학적인 이유에 의한 효과성 보장을 위해 중요합니다. 우리는 지난 평가에 기반하여 반복적으로 검토하는 새로운 시간적 일관성 방법을 제안합니다. 한번의 확인이나 다수의 모델의 디세이버즈의 접근과 달리, 우리의 방법은 자각적 반성 행동의 순서로 일관성을 활용하여 확인의 정확도를 향상시킵니다. 다양한 수학적 프로세스 에러 식별 벤치마크（Mathcheck、ProcessBench、PRM800K）에서 실험적 평가에 의하여 기준 방법보다 일관된 성능 향상이 관찰되었습니다. 최근의 DeepSeek R1의 결합 모델에 대해 이 방법을 적용하면 7B/8B의 결합 모델이 모든 70B/72B 모델이나 GPT-4o를 초과하는 강력한 성능을 보여주며, 특히 우리의 방법을 사용한 결합 14B 모델은 DeepSeek-R1과 같은 성능을 달성합니다. 우리의 코드는 https://github.com/jcguo123/Temporal-Consistency에 공개되어 있습니다.",
      "upvotes": 4,
      "discussionId": "67da4550d5132b0eebd0673c",
      "ai_keywords": [
        "temporal consistency",
        "verifiers",
        "iterative refinement",
        "self-reflection actions",
        "verification accuracy",
        "Mathcheck",
        "ProcessBench",
        "PRM800K",
        "DeepSeek R1",
        "distilled models",
        "GPT-4o",
        "performance comparable"
      ]
    },
    "publishedAt": "2025-03-18T13:58:28.000Z",
    "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
    "summary": "Verification is crucial for effective mathematical reasoning. We present a\nnew temporal consistency method where verifiers iteratively refine their\njudgments based on the previous assessment. Unlike one-round verification or\nmulti-model debate approaches, our method leverages consistency in a sequence\nof self-reflection actions to improve verification accuracy. Empirical\nevaluations across diverse mathematical process error identification benchmarks\n(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements\nover baseline methods. When applied to the recent DeepSeek R1 distilled models,\nour method demonstrates strong performance, enabling 7B/8B distilled models to\noutperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the\ndistilled 14B model with our method achieves performance comparable to\nDeepseek-R1. Our codes are available at\nhttps://github.com/jcguo123/Temporal-Consistency",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14495.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12545",
      "authors": [
        {
          "_id": "67d943d272843a36b74ab41c",
          "name": "Zhaopan Xu",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab41d",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab41e",
          "name": "Weidong Tang",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab41f",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab420",
          "name": "Wangbo Zhao",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab421",
          "name": "Xiaojiang Peng",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab422",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab423",
          "name": "Yang You",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab424",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab425",
          "name": "Hongxun Yao",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab426",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T15:26:20.000Z",
      "submittedOnDailyAt": "2025-03-19T00:14:50.269Z",
      "title": "PEBench: 가상 데이터 세트를 통해 기계 학습의 제거를 평가하기 위한 다모뎀 대규모 언어 모델",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "최근, 다모디얼 대언어 모델(MLLMs)은 이미지 질문 대답, 이미지 이해, 논리적 이유 등 다양한 문제를 해결할 때 놀라운 발전을 보여주고 있습니다. 그러나 이러한 놀라운 발전은 인터넷으로부터의 많은 데이터의 수집에 의존하고 있으며, 개인정보와 보안에 대한 큰 우려가 생겼습니다. 이러한 문제를 해결하기 위해 기계의 무학습(MU)이 적용되고, 이미 학습된 모델에서 특정 지식을 제거할 수 있게 되었습니다. 그러나 MLLMs의 MU는 주목을 받고 있지만, 현재의 효과성 평가는 완벽하지 않습니다, 기본적인 문제를 일반적으로 오답으로 간주하고 있으며, 안전하고 신뢰할 수 있는 시스템의 개발에 영향을 미칩니다. 이를 해결하기 위해, 우리는 PEBench라는 벤치마크를 소개합니다. 이 것은 개인적인 존재와 상대적인 일반적인 이벤트 시나리오의 데이터셋을 포함하며, MLLMs의 MU의 성능을 종합적으로 평가하기 위해 설계되었습니다. PEBench을 통해, 우리는 안전하고 개인정보를 보호하는 다모디얼 모델의 연구에 표준화된 프레임워크를 제공하려는 노력을 합니다. 6가지의 MU 메소드를 벤치마크로 평가하고, 그 장점과 한계를 명확히 하고, MLLMs의 MU에 대한 주요한 문제와 기회를 밝혀줍니다.",
      "upvotes": 4,
      "discussionId": "67d943db72843a36b74ab652",
      "projectPage": "https://pebench.github.io/",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "visual question answering",
        "visual understanding",
        "reasoning",
        "machine unlearning",
        "benchmark",
        "PEBench",
        "dataset",
        "personal entities",
        "general event scenes",
        "secure",
        "privacy-preserving",
        "multimodal models"
      ]
    },
    "publishedAt": "2025-03-16T11:26:20.000Z",
    "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models",
    "summary": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable advancements in tasks such as visual question answering, visual\nunderstanding, and reasoning. However, this impressive progress relies on vast\namounts of data collected from the internet, raising significant concerns about\nprivacy and security. To address these issues, machine unlearning (MU) has\nemerged as a promising solution, enabling the removal of specific knowledge\nfrom an already trained model without requiring retraining from scratch.\nAlthough MU for MLLMs has gained attention, current evaluations of its efficacy\nremain incomplete, and the underlying problem is often poorly defined, which\nhinders the development of strategies for creating more secure and trustworthy\nsystems. To bridge this gap, we introduce a benchmark, named PEBench, which\nincludes a dataset of personal entities and corresponding general event scenes,\ndesigned to comprehensively assess the performance of MU for MLLMs. Through\nPEBench, we aim to provide a standardized and robust framework to advance\nresearch in secure and privacy-preserving multimodal models. We benchmarked 6\nMU methods, revealing their strengths and limitations, and shedding light on\nkey challenges and opportunities for MU in MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12545.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14151",
      "authors": [
        {
          "_id": "67da71b1c26b43885226a72d",
          "name": "Yong Zhong",
          "hidden": false
        },
        {
          "_id": "67da71b1c26b43885226a72e",
          "name": "Zhuoyi Yang",
          "hidden": false
        },
        {
          "_id": "67da71b1c26b43885226a72f",
          "name": "Jiayan Teng",
          "hidden": false
        },
        {
          "_id": "67da71b1c26b43885226a730",
          "name": "Xiaotao Gu",
          "hidden": false
        },
        {
          "_id": "67da71b1c26b43885226a731",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T11:17:32.000Z",
      "submittedOnDailyAt": "2025-03-19T05:57:05.443Z",
      "title": "Concat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오 합성에 대한 안내\n\nConcat-ID: 일반의 정체 유지 비디오",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Concat-ID는 정체성 유지 비디오 생성을 위한 통일된 프레임워크입니다. Concat-ID는 Variational Autoencoders를 사용하여 이미지 특징을 추출하고, 비디오 잠재변수와 시퀀스 방향을 결합합니다. 3D self-attention mechanism을 사용하여 추가 모듈의 필요성을 제거합니다. 새로운 비디오 조합 전략과 다단계 학습 팀을 도입하여 정체성의 일관성과 얼굴의 읽기성을 조정하면서 비디오의 자연성을 향상시킵니다. 광범위한 실험은 Concat-ID가 현재의 방법보다 단일 및 다 정체성 생성에 뛰어넘는 것을 보여주고, 다주체 시나리오에서도无缝 scalability를 유지함을 보여줍니다. Concat-ID는 정체성 유지 비디오 합성의 새로운 기준을 세우고, 광범위한 응용 분야에 대해 다양한 scalable 솔루션을 제공합니다.",
      "upvotes": 3,
      "discussionId": "67da71bdc26b43885226ab4e",
      "projectPage": "https://ml-gsai.github.io/Concat-ID-demo/",
      "githubRepo": "https://github.com/ML-GSAI/Concat-ID",
      "ai_keywords": [
        "Variational Autoencoders",
        "3D self-attention mechanisms",
        "cross-video pairing strategy",
        "multi-stage training regimen",
        "identity consistency",
        "facial editability",
        "video naturalness",
        "identity-preserving video synthesis"
      ]
    },
    "publishedAt": "2025-03-18T07:17:32.000Z",
    "title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis",
    "summary": "We present Concat-ID, a unified framework for identity-preserving video\ngeneration. Concat-ID employs Variational Autoencoders to extract image\nfeatures, which are concatenated with video latents along the sequence\ndimension, leveraging solely 3D self-attention mechanisms without the need for\nadditional modules. A novel cross-video pairing strategy and a multi-stage\ntraining regimen are introduced to balance identity consistency and facial\neditability while enhancing video naturalness. Extensive experiments\ndemonstrate Concat-ID's superiority over existing methods in both single and\nmulti-identity generation, as well as its seamless scalability to multi-subject\nscenarios, including virtual try-on and background-controllable generation.\nConcat-ID establishes a new benchmark for identity-preserving video synthesis,\nproviding a versatile and scalable solution for a wide range of applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14151.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 35
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12271",
      "authors": [
        {
          "_id": "67d926523acb37a1cfa74cf8",
          "name": "Shufan Li",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cf9",
          "name": "Konstantinos Kallidromitis",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfa",
          "name": "Akash Gokul",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfb",
          "name": "Arsh Koneru",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfc",
          "name": "Yusuke Kato",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfd",
          "name": "Kazuki Kozuka",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfe",
          "name": "Aditya Grover",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-15T21:58:12.000Z",
      "submittedOnDailyAt": "2025-03-19T02:08:42.869Z",
      "title": "Reflect-DiT: 추론 시의 스케일링에 의한 텍스트에서 이미지로 Diffusion Transformers에 의한 인코ntext 반발",
      "submittedOnDailyBy": {
        "_id": "6310531914aa81e1044363ed",
        "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
        "isPro": false,
        "fullname": "Shufan Li",
        "user": "jacklishufan",
        "type": "user"
      },
      "summary": "문맥 생성 모델의 발전에 있어서 주요한 접근 방식은 훈련 시 스케일링이며, 더 큰 모델이 더 많은 데이터와 더 많은 계산 컴퓨팅 자원을 사용하여 훈련되어 있습니다. 이는 효과적이지만 계산적으로 고가하며, 추론 시 스케일링의 관심이 증가하고 있습니다. 현재, 문맥 생성 모델의 추론 시 스케일링은 Prompt에 대해 다수의 이미지를 생성하고 선택 모델이 최적의 출력을 선택하는 Best-of-N 샘플링에 제한되어 있습니다. DeepSeek-R1과 같은 언어 분야의 최근 성공을 참고하여, 문맥 생성 모델에 최적의 Best-of-N 샘플링을 대신하여 in-context reflection 기능을 추가하여 Reflect-DiT을 제안합니다. Reflect-DiT은 이전에 생성된 이미지의 in-context 예시와 필요한 개선을 설명한 문자열의 피라비ック을 사용하여, Diffusion Transformer의 생성을 검토할 수 있게 합니다. 랜덤한 샘플링을 의존하고, 향후 생성에서 더 좋은 결과를 기대하는 것이 아니라, Reflect-DiT은 특정 영역의 개선을 대상으로 생성을 특화합니다. 실험 결과를 GenEval 벤치마크에서 성능 향상(+0.19)을 보여주며, SANA-1.0-1.6B를 기본 모델로 사용합니다. 또한, Prompt에 대해 20개의 샘플을 생성하고, 이전의 최고 점수(0.80)를 초과하는 새로운 최고 점수(0.81)를 달성합니다. 이는 Best-of-N 접근 방식을 사용했던 더 큰 모델(SANA-1.5-4.8B)과 2048개의 샘플을 사용하여 얻은 최고 점수를 초과했습니다.",
      "upvotes": 2,
      "discussionId": "67d926543acb37a1cfa74d9f",
      "ai_keywords": [
        "diffusion models",
        "text-to-image diffusion models",
        "best-of-N sampling",
        "in-context reflection",
        "Diffusion Transformers",
        "Reflect-DiT",
        "GenEval benchmark",
        "in-context examples",
        "textual feedback",
        "performance improvement",
        "state-of-the-art"
      ]
    },
    "publishedAt": "2025-03-15T17:58:12.000Z",
    "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion\n  Transformers via In-Context Reflection",
    "summary": "The predominant approach to advancing text-to-image generation has been\ntraining-time scaling, where larger models are trained on more data using\ngreater computational resources. While effective, this approach is\ncomputationally expensive, leading to growing interest in inference-time\nscaling to improve performance. Currently, inference-time scaling for\ntext-to-image diffusion models is largely limited to best-of-N sampling, where\nmultiple images are generated per prompt and a selection model chooses the best\noutput. Inspired by the recent success of reasoning models like DeepSeek-R1 in\nthe language domain, we introduce an alternative to naive best-of-N sampling by\nequipping text-to-image Diffusion Transformers with in-context reflection\ncapabilities. We propose Reflect-DiT, a method that enables Diffusion\nTransformers to refine their generations using in-context examples of\npreviously generated images alongside textual feedback describing necessary\nimprovements. Instead of passively relying on random sampling and hoping for a\nbetter result in a future generation, Reflect-DiT explicitly tailors its\ngenerations to address specific aspects requiring enhancement. Experimental\nresults demonstrate that Reflect-DiT improves performance on the GenEval\nbenchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it\nachieves a new state-of-the-art score of 0.81 on GenEval while generating only\n20 samples per prompt, surpassing the previous best score of 0.80, which was\nobtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples\nunder the best-of-N approach.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12271.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310531914aa81e1044363ed",
      "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
      "fullname": "Shufan Li",
      "name": "jacklishufan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09443",
      "authors": [
        {
          "_id": "67da8b794e1138ddc328de09",
          "user": {
            "_id": "630a4aaa9df54451d91cc6fa",
            "avatarUrl": "/avatars/10898c7886e25c64d58334083e3cf2d5.svg",
            "isPro": false,
            "fullname": "Julian Spravil",
            "user": "Spravil",
            "type": "user"
          },
          "name": "Julian Spravil",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:43:48.893Z",
          "hidden": false
        },
        {
          "_id": "67da8b794e1138ddc328de0a",
          "name": "Sebastian Houben",
          "hidden": false
        },
        {
          "_id": "67da8b794e1138ddc328de0b",
          "name": "Sven Behnke",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T14:41:10.000Z",
      "submittedOnDailyAt": "2025-03-19T07:58:17.987Z",
      "title": "프롤렌스： 비지온-언어 모델의 시스템적 일반화에 대한 스케일링 법칙",
      "submittedOnDailyBy": {
        "_id": "630a4aaa9df54451d91cc6fa",
        "avatarUrl": "/avatars/10898c7886e25c64d58334083e3cf2d5.svg",
        "isPro": false,
        "fullname": "Julian Spravil",
        "user": "Spravil",
        "type": "user"
      },
      "summary": "クロス言語トランスファー는, 시각 언어 모델(VLMs)가 한 언어의 훈련 데이터로 다언어적인 시각 태스크를 수행할 수 있도록 가능하게 해줍니다. 현재의 접근 방식은 큰 규모의 사전 학습 멀티 언어 모델을 기반으로 하고 있습니다. 그러나, 이들은 다언어성의 '카우스'에 직면하여, 다운 스트림 태스크의 성능을 저해하면서 다언어성 능력을 유지하고, 어휘의 불확실성을 극복하며, 최근의 진보에 따라가지 못하여, 이 경향을 나타냅니다. 본 논문에서는, 멀티 태스크에 대한 시닉 일반화의 스케일러링을 조사하고, 모델 크기와 훈련 샘플의 영향에 중점을 두어 논의합니다. Florenz라는 모델을 제안합니다. 이는 사전 학습 모델 Florence-2와 큰 규모의 언어 모델 Gemma-2를 조합한 단일 언어 인코더-디코더 VLM으로, 파라미터 수가 0.4B에서 11.2B까지입니다. Florenz는, 이미지 캡처의 불완전한 언어 커버리지를 특징으로 하는 합성 데이터셋에서 계산 버젼로 훈련됩니다. 이로 인해, 완전히 커버리지된 번역 태스크에서 일반화를 측정합니다. 또한,不见의 태스크 언어 쌍의 학습은 스케일러링에 따라 하며, 데이터 생성 프로кси와 제안된 Florenz 모델 가족을 사용하여, 이미지 캡처의 능력은 번역 태스크의 데이터만 있는 경우 특정 언어에서 발생하는 것을 보여줍니다. 다운 스트림 데이터 세트의 조합으로의 微调는, 옳바른 성능을 보여주고, 다형 기계 번역(Multi30K, CoMMuTE), 어휘의 불확실성의 해결(CoMMuTE), 이미지 캡처(Multi30K, XM3600, COCO Karpathy)에 대해 기대할 수 있는 스케일러링 테ン드를 나타냅니다.",
      "upvotes": 2,
      "discussionId": "67da8b7a4e1138ddc328de44"
    },
    "publishedAt": "2025-03-12T10:41:10.000Z",
    "title": "Florenz: Scaling Laws for Systematic Generalization in Vision-Language\n  Models",
    "summary": "Cross-lingual transfer enables vision-language models (VLMs) to perform\nvision tasks in various languages with training data only in one language.\nCurrent approaches rely on large pre-trained multilingual language models.\nHowever, they face the curse of multilinguality, sacrificing downstream task\nperformance for multilingual capabilities, struggling with lexical ambiguities,\nand falling behind recent advances. In this work, we study the scaling laws of\nsystematic generalization with monolingual VLMs for multilingual tasks,\nfocusing on the impact of model size and seen training samples. We propose\nFlorenz, a monolingual encoder-decoder VLM with 0.4B to 11.2B parameters\ncombining the pre-trained VLM Florence-2 and the large language model Gemma-2.\nFlorenz is trained with varying compute budgets on a synthetic dataset that\nfeatures intentionally incomplete language coverage for image captioning, thus,\ntesting generalization from the fully covered translation task. We show that\nnot only does indirectly learning unseen task-language pairs adhere to a\nscaling law, but also that with our data generation pipeline and the proposed\nFlorenz model family, image captioning abilities can emerge in a specific\nlanguage even when only data for the translation task is available. Fine-tuning\non a mix of downstream datasets yields competitive performance and demonstrates\npromising scaling trends in multimodal machine translation (Multi30K, CoMMuTE),\nlexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO\nKarpathy).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09443.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630a4aaa9df54451d91cc6fa",
      "avatarUrl": "/avatars/10898c7886e25c64d58334083e3cf2d5.svg",
      "fullname": "Julian Spravil",
      "name": "Spravil",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13661",
      "authors": [
        {
          "_id": "67da8a9fdab8cc723c349fb0",
          "user": {
            "_id": "630a5ef0e81e1dea2cedcec0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630a5ef0e81e1dea2cedcec0/ATtyCvYoX4z7uxsm2sJU2.png",
            "isPro": false,
            "fullname": "Hà Huy Hoàng",
            "user": "HoangHa",
            "type": "user"
          },
          "name": "Huy Hoang Ha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:43:52.541Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/630a5ef0e81e1dea2cedcec0/9DxLIM8Ftd4OVGfuGqAEU.png"
      ],
      "publishedAt": "2025-03-17T19:09:11.000Z",
      "submittedOnDailyAt": "2025-03-19T07:43:37.853Z",
      "title": "프랑스 LLM을 재고찰하는 \"데이터량이 적더라도 이유를 더 잘 설명할 수 있는\" 같은 사고방식",
      "submittedOnDailyBy": {
        "_id": "630a5ef0e81e1dea2cedcec0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630a5ef0e81e1dea2cedcec0/ATtyCvYoX4z7uxsm2sJU2.png",
        "isPro": false,
        "fullname": "Hà Huy Hoàng",
        "user": "HoangHa",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)은 많은 자연어 처리 태스크에서 놀라울 정도로 뛰어난 능력을 보여주고 있습니다. 그러나 수학적 논리나 비영문 언어 등 전문 분야에서 강력한 성능을 달성하려면, 큰 데이터 세트에 의한 확장훈련이 필요합니다. 본 논문에서는 대비적인 접근을 검토합니다: 작은, 고품질의, 바이리언스(영어-프랑스어) 데이터 세트에 대한 전략적인 미세 조정을 통해, 대 언어 모델의 논리 능력과 프랑스어의 어휘력을 향상시키는 것을 조사합니다. 규모에 의존하지 않도록, 목표 데이터의 커스텀화와 최적화된 훈련이 경쟁적인, 또는 그 이상의 성능을 달성할 수 있음을 검토합니다. 2,000개의 선택된 샘플에 대한 목표 포스트 비탈 피치닝(SFT)을 통해, 수학적 논리에서 뚜렷한 향상을 나타냅니다. 특히, Pensez 7B은 기본 모델의 정확성이 AIME25에서 20% 이상, 프랑스의 MATH 레벨 5 벤치마크에서 12% 상승합니다. 이러한 결과를 통해, 대 언어 모델에서 강력한 논리 성능을 달성하기 위해 큰 데이터 세트가 필요로 하는 일반적인 가정을 의심하고, 전략적인 데이터 커스텀화와 최적화된 미세 조정이 전문적인 스킬과 다국어 능력의 양방향 증진을 할 수 있음을 명확히 합니다. 우리의 발견은 고성능의 다국어 대 언어 모델의 효율적인 개발에 영향을 미칩니다, 특히 리소스 제한된 시나리오에서尤为重要です.",
      "upvotes": 1,
      "discussionId": "67da8aa1dab8cc723c34a039"
    },
    "publishedAt": "2025-03-17T15:09:11.000Z",
    "title": "Pensez: Less Data, Better Reasoning -- Rethinking French LLM",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious natural language processing tasks. However, achieving strong\nperformance in specialized domains like mathematical reasoning and non-English\nlanguages often requires extensive training on massive datasets. This paper\ninvestigates a contrasting approach: strategic fine-tuning on a small,\nhigh-quality, bilingual (English-French) dataset to enhance both the reasoning\ncapabilities and French language proficiency of a large language model. Rather\nthan relying on scale, we explore the hypothesis that targeted data curation\nand optimized training can achieve competitive, or even superior, performance.\nWe demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000\ncarefully selected samples, significant improvements in mathematical reasoning.\nSpecifically, Pensez 7B exhibits an increase in accuracy of the base model up\nto 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark.\nThese results challenge the prevailing assumption that massive datasets are\naprerequisite for strong reasoning performance in LLMs, highlighting the\npotential of strategic data curation and optimized fine-tuning for enhancing\nboth specialized skills and multilingual capabilities. Our findings have\nimplications for the efficient development of high-performing, multilingual\nLLMs, especially in resource-constrained scenarios.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/630a5ef0e81e1dea2cedcec0/9DxLIM8Ftd4OVGfuGqAEU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13661.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630a5ef0e81e1dea2cedcec0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630a5ef0e81e1dea2cedcec0/ATtyCvYoX4z7uxsm2sJU2.png",
      "fullname": "Hà Huy Hoàng",
      "name": "HoangHa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 26
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12127",
      "authors": [
        {
          "_id": "67d95fa8fb17ef1c744db2db",
          "user": {
            "_id": "64ee11c125d2bb76c06e243d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ee11c125d2bb76c06e243d/DR2NV4NYINtpRoMd0NupA.png",
            "isPro": false,
            "fullname": "tobia poppi",
            "user": "tobi1modna",
            "type": "user"
          },
          "name": "Tobia Poppi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T14:57:56.046Z",
          "hidden": false
        },
        {
          "_id": "67d95fa8fb17ef1c744db2dc",
          "name": "Tejaswi Kasarla",
          "hidden": false
        },
        {
          "_id": "67d95fa8fb17ef1c744db2dd",
          "name": "Pascal Mettes",
          "hidden": false
        },
        {
          "_id": "67d95fa8fb17ef1c744db2de",
          "name": "Lorenzo Baraldi",
          "hidden": false
        },
        {
          "_id": "67d95fa8fb17ef1c744db2df",
          "name": "Rita Cucchiara",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-15T13:18:04.000Z",
      "submittedOnDailyAt": "2025-03-19T07:49:10.270Z",
      "title": "하이파볼리ック・사페지・아우아리비지션・라ングワイドモデル",
      "submittedOnDailyBy": {
        "_id": "64ee11c125d2bb76c06e243d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ee11c125d2bb76c06e243d/DR2NV4NYINtpRoMd0NupA.png",
        "isPro": false,
        "fullname": "tobia poppi",
        "user": "tobi1modna",
        "type": "user"
      },
      "summary": "不安全 콘텐츠의 감지와 제거를 해결하기 위해, 현실 세계에 통합하는 중요한 단계 중 하나가 비전-라ングラジュ 모델(예: CLIP)입니다. 현재의 노력을 통해, 불안정 개념의 모델의 지식을 제거하기 위해 잊은 훈련 기술이 사용되고 있습니다. 이는 불만족의 출력을 줄이는 데 효과적이지만, 불안정과 안전 사이의 차이를 제한합니다. 본 논문에서는, 하이퍼볼릭 공간의 고유의 계층적 특성을 활용하여 잊은 훈련에서 인식 패러다임으로 전환하는 새로운 접근 방식을 소개합니다. 안전과 불안정한 콘텐츠는 의미 관련 계층으로 생각하고, 하이퍼볼릭 공간의 서로 다른 영역에 배치합니다. 우리의 HySAC(Hyperbolic Safety-Aware CLIP)는, 의미 관련 손실 함수를 사용하여 안전과 불안정한 이미지-텍스트 페어의 계층적 및 비대칭적 관계를 모델링합니다. 이 모델링은 표준 비전-라ングラム 모델이 유클리드 임베딩에 의존하기 때문에, 불안정 콘텐츠의 인식에도 대응할 수 있으며, 불안정 쿼리를 동적으로 안전한 선택지에 리다이렉트하거나, 원래 출력을 유지할 수 있습니다. 확장된 실험은, 이 접근 방식이 안전 인식을 강화하고, 비전-라ングラム 모델의 콘텐츠 모델링의 적응성과 해석성을 향상시키는 프레임워크를 구축하는 것을 보여줍니다. 소스 코드는, https://github.com/aimagelab/HySAC에 공개되어 있습니다.",
      "upvotes": 1,
      "discussionId": "67d95fabfb17ef1c744db411",
      "ai_keywords": [
        "hyperbolic space",
        "entailment hierarchy",
        "entailment loss functions",
        "Euclidean embeddings",
        "multimodal unsafe classifier",
        "content retriever",
        "hyperbolic Safety-Aware CLIP",
        "HySAC"
      ]
    },
    "publishedAt": "2025-03-15T09:18:04.000Z",
    "title": "Hyperbolic Safety-Aware Vision-Language Models",
    "summary": "Addressing the retrieval of unsafe content from vision-language models such\nas CLIP is an important step towards real-world integration. Current efforts\nhave relied on unlearning techniques that try to erase the model's knowledge of\nunsafe concepts. While effective in reducing unwanted outputs, unlearning\nlimits the model's capacity to discern between safe and unsafe content. In this\nwork, we introduce a novel approach that shifts from unlearning to an awareness\nparadigm by leveraging the inherent hierarchical properties of the hyperbolic\nspace. We propose to encode safe and unsafe content as an entailment hierarchy,\nwhere both are placed in different regions of hyperbolic space. Our HySAC,\nHyperbolic Safety-Aware CLIP, employs entailment loss functions to model the\nhierarchical and asymmetrical relations between safe and unsafe image-text\npairs. This modelling, ineffective in standard vision-language models due to\ntheir reliance on Euclidean embeddings, endows the model with awareness of\nunsafe content, enabling it to serve as both a multimodal unsafe classifier and\na flexible content retriever, with the option to dynamically redirect unsafe\nqueries toward safer alternatives or retain the original output. Extensive\nexperiments show that our approach not only enhances safety recognition but\nalso establishes a more adaptable and interpretable framework for content\nmoderation in vision-language models. Our source code is available at\nhttps://github.com/aimagelab/HySAC.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12127.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ee11c125d2bb76c06e243d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ee11c125d2bb76c06e243d/DR2NV4NYINtpRoMd0NupA.png",
      "fullname": "tobia poppi",
      "name": "tobi1modna",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10546",
      "authors": [
        {
          "_id": "67da141d6b2857e3ec1412a7",
          "name": "Zixian Liu",
          "hidden": false
        },
        {
          "_id": "67da141d6b2857e3ec1412a8",
          "name": "Mingtong Zhang",
          "hidden": false
        },
        {
          "_id": "67da141d6b2857e3ec1412a9",
          "name": "Yunzhu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T16:59:17.000Z",
      "submittedOnDailyAt": "2025-03-19T00:00:05.763Z",
      "title": "KUDA: 오픈 ボークス 워드 로보트 조작에 대한 동적 학습과 시각적 피드백의 통합의 핵심 포인트",
      "submittedOnDailyBy": {
        "_id": "671c6a3e255aa50ebb504fc5",
        "avatarUrl": "/avatars/b549bb19990ae21690472799817f951e.svg",
        "isPro": false,
        "fullname": "Mingtong Zhang",
        "user": "Mingtongz",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)와 시각 언어 모델(VLMs)의 급격한 발전에 따라, 개방형 보너스 기반의 로봇 조작 시스템 개발에 상당한 진전을 보입니다. 그러나 많은 기존 접근 방식은 물체의 역학의 중요성을 간과하고, 복잡한 동적인 작업에 대한 적용 범위를 제한하고 있습니다. 본 논문에서는, 시각 언어 모델(VLMs)과 학습 기반의 신경 역학 모델을 통합한 역학 학습과 시각 프로ン퓰팅을 통해 핵심 포인트를 확장하는 오픈 보너스 조작 시스템 KUDA를 통해 이러한 문제를 해결합니다. 우리의 주요 전망은, 핵심 포인트 기반의 목표 설정은 VLMs에서 동시에 해석할 수 있으며, 모델 기반의 계획에 효율적으로 비용 함수로 번역할 수 있습니다. 언어 지시와 시각 관측을 받아 KUDA는 RGB 이미지에 핵심 포인트를 할당하고, VLM을 통해 목표 설정을 생성합니다. 이러한 추상적인 핵심 포인트 기반 표현은 학습된 역학 모델을 사용하여 비용 함수로 변환되어 로봇의 타레이트 로프를 생성합니다. KUDA는 자유형 언어 지시, 다물체 상호작용, 변형 또는 입자체 물체 등 다양한 조작 작업에 대해 평가되어, 우리 프레임워크의 효과성을 보여주고 있습니다. 프로젝트 페이지는 http://kuda-dynamics.github.io에 접근 가능합니다.",
      "upvotes": 1,
      "discussionId": "67da141e6b2857e3ec141301",
      "projectPage": "https://kuda-dynamics.github.io",
      "githubRepo": "https://github.com/StoreBlank/KUDA",
      "ai_keywords": [
        "LLMs",
        "VLMs",
        "open-vocabulary robotic manipulation systems",
        "object dynamics",
        "KUDA",
        "dynamics learning",
        "visual prompting",
        "keypoints",
        "learning-based neural dynamics models",
        "keypoint-based target specification",
        "cost functions",
        "model-based planning",
        "robotic trajectories",
        "free-form language instructions",
        "multi-object interactions",
        "deformable objects",
        "granular objects"
      ]
    },
    "publishedAt": "2025-03-13T12:59:17.000Z",
    "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation",
    "summary": "With the rapid advancement of large language models (LLMs) and\nvision-language models (VLMs), significant progress has been made in developing\nopen-vocabulary robotic manipulation systems. However, many existing approaches\noverlook the importance of object dynamics, limiting their applicability to\nmore complex, dynamic tasks. In this work, we introduce KUDA, an\nopen-vocabulary manipulation system that integrates dynamics learning and\nvisual prompting through keypoints, leveraging both VLMs and learning-based\nneural dynamics models. Our key insight is that a keypoint-based target\nspecification is simultaneously interpretable by VLMs and can be efficiently\ntranslated into cost functions for model-based planning. Given language\ninstructions and visual observations, KUDA first assigns keypoints to the RGB\nimage and queries the VLM to generate target specifications. These abstract\nkeypoint-based representations are then converted into cost functions, which\nare optimized using a learned dynamics model to produce robotic trajectories.\nWe evaluate KUDA on a range of manipulation tasks, including free-form language\ninstructions across diverse object categories, multi-object interactions, and\ndeformable or granular objects, demonstrating the effectiveness of our\nframework. The project page is available at http://kuda-dynamics.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10546.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671c6a3e255aa50ebb504fc5",
      "avatarUrl": "/avatars/b549bb19990ae21690472799817f951e.svg",
      "fullname": "Mingtong Zhang",
      "name": "Mingtongz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10410",
      "authors": [
        {
          "_id": "67d9638d92e48ed07860ecee",
          "user": {
            "_id": "67934b85c67af4a116b5594b",
            "avatarUrl": "/avatars/076cf0803b50e1ab54e5ba4f8f2a8e44.svg",
            "isPro": false,
            "fullname": "yuwendu",
            "user": "yuwendu",
            "type": "user"
          },
          "name": "Yuwen Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T14:57:49.908Z",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecef",
          "name": "Anning Hu",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf0",
          "name": "Zichen Chao",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf1",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf2",
          "name": "Junhao Ge",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf3",
          "name": "Genjia Liu",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf4",
          "name": "Weitao Wu",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf5",
          "name": "Lanjun Wang",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf6",
          "name": "Siheng Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/rDLvz9Jma7IWsaGubueis.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/1PULsx8sLyy9CqDRMuzo1.png"
      ],
      "publishedAt": "2025-03-13T14:33:42.000Z",
      "submittedOnDailyAt": "2025-03-19T00:00:36.900Z",
      "title": "RoCo-Sim: 도로측 협업 프로세스를 앞으로 이끌어 올리는 방법\n\n(注意：原文中的“道路側コラボレーションプロセッサンス”在翻译时被理解为“도로측 협업 프로세스”，以保持专业性和准确性。)",
      "submittedOnDailyBy": {
        "_id": "67934b85c67af4a116b5594b",
        "avatarUrl": "/avatars/076cf0803b50e1ab54e5ba4f8f2a8e44.svg",
        "isPro": false,
        "fullname": "yuwendu",
        "user": "yuwendu",
        "type": "user"
      },
      "summary": "도로측 코랩레이션 인식은 여러 도로측 유닛이 그 관측 데이터를 공유하여 차량의 환경 인식을 향상시키는 시스템입니다. 현재의 도로측 관측 방법은 모델 설계를 중심으로 집중하고, 표준 오차, 정보 부족, 다시점의 일관성 등 데이터 문제를 피하려고 있지만, 최근 공개된 데이터셋에서 성능이 저하되어 있습니다. 도로측 코랩레이션 인식을 크게 향상시키고 데이터 문제를 해결하기 위해, 우리는 첫 번째 도로측 코랩레이션 관측의 시뮬레이션 프레임워크 RoCo-Sim을 제안합니다. RoCo-Sim은 동적인 포로닝 편집과 한 장의 이미지의 전경 스타일 트랜스폼을 통해 다양한, 다시점의 일관성을 유지하는 관측 데이터를 생성할 수 있습니다. RoCo-Sim은 네 개의 구성 요소로 구성되어 있습니다: (1) 카메라의 외접 최적화는 도로측 카메라의 정확한 3D에서 2D의 투영을 보장합니다; (2) 새로운 다시점 마스크 인식 샘플러 (MOAS)는 3D 공간 내 다양한 디지털 자산의 배치를 결정합니다; (3) DepthSAM은 단일 프레임 고정 시점으로부터 포로닝과 배경의 관계를 혁신적으로 모델링하여 포로닝의 다시점의 일관성을 보장합니다; (4) 스케일러의 후처리 도구 패키지는 스타일 트랜스폼과 다른 확장으로 인해 더욱 현실적이고 풍부한 시나리오를 생성합니다. RoCo-Sim은 도로측 관측의 시뮬레이션에서 중요한 결함이 채워집니다. 코드와 사전 학습된 모델은 즉시 릴리즈됩니다: https://github.com/duyuwen-duen/RoCo-Sim",
      "upvotes": 1,
      "discussionId": "67d9639192e48ed07860ee1f",
      "ai_keywords": [
        "Multi-View Occlusion-Aware Sampler",
        "DepthSAM",
        "Scalable Post-Processing Toolkit",
        "3D object detection"
      ]
    },
    "publishedAt": "2025-03-13T10:33:42.000Z",
    "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation",
    "summary": "Roadside Collaborative Perception refers to a system where multiple roadside\nunits collaborate to pool their perceptual data, assisting vehicles in\nenhancing their environmental awareness. Existing roadside perception methods\nconcentrate on model design but overlook data issues like calibration errors,\nsparse information, and multi-view consistency, leading to poor performance on\nrecent published datasets. To significantly enhance roadside collaborative\nperception and address critical data issues, we present the first simulation\nframework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable\nof generating diverse, multi-view consistent simulated roadside data through\ndynamic foreground editing and full-scene style transfer of a single image.\nRoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures\naccurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View\nOcclusion-Aware Sampler (MOAS) determines the placement of diverse digital\nassets within 3D space; (3) DepthSAM innovatively models foreground-background\nrelationships from single-frame fixed-view images, ensuring multi-view\nconsistency of foreground; and (4) Scalable Post-Processing Toolkit generates\nmore realistic and enriched scenes through style transfer and other\nenhancements. RoCo-Sim significantly improves roadside 3D object detection,\noutperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on\nTUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception\nsimulation. Code and pre-trained models will be released soon:\nhttps://github.com/duyuwen-duen/RoCo-Sim",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/rDLvz9Jma7IWsaGubueis.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/1PULsx8sLyy9CqDRMuzo1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67934b85c67af4a116b5594b",
      "avatarUrl": "/avatars/076cf0803b50e1ab54e5ba4f8f2a8e44.svg",
      "fullname": "yuwendu",
      "name": "yuwendu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]