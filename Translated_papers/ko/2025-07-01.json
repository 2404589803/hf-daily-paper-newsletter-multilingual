[
  {
    "paper": {
      "id": "2506.23044",
      "authors": [
        {
          "_id": "686347cd588cea0da970c87a",
          "user": {
            "_id": "636f4c6b5d2050767e4a1491",
            "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
            "isPro": false,
            "fullname": "Guo-Hua Wang",
            "user": "Flourish",
            "type": "user"
          },
          "name": "Guo-Hua Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:51.516Z",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87b",
          "name": "Shanshan Zhao",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87c",
          "name": "Xinjie Zhang",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87d",
          "name": "Liangfu Cao",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87e",
          "name": "Pengxin Zhan",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87f",
          "name": "Lunhao Duan",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c880",
          "name": "Shiyin Lu",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c881",
          "name": "Minghao Fu",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c882",
          "name": "Xiaohao Chen",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c883",
          "name": "Jianshan Zhao",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c884",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c885",
          "name": "Qing-Guo Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-29T00:40:17.000Z",
      "submittedOnDailyAt": "2025-07-01T04:30:09.856Z",
      "title": "오비스-U1 기술보고서",
      "submittedOnDailyBy": {
        "_id": "636f4c6b5d2050767e4a1491",
        "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
        "isPro": false,
        "fullname": "Guo-Hua Wang",
        "user": "Flourish",
        "type": "user"
      },
      "summary": "이 보고서에서, 3바이릴리언 파라미터의 통합 모델인 Ovis-U1을 소개합니다. 이 모델은 다양한 타입의 이해, 텍스트로부터 이미지 생성, 이미지 편집 기능을 통합합니다. Ovis 시리즈의 기반으로, Ovis-U1은 디퓨션 기반의 시각 디코더와 바이디렉션 토큰 리파이너를 조합하여, GPT-4o와 같은 수준의 이미지 생성 태스크를 가능하게 합니다. 이전 모델과 달리, Ovis-U1은 생성 태스크에 대한 MLLM을 고정하여 사용하지 않습니다. Ovis-U1은 언어 모델에서 시작하는 새로운 통합 훈련 접근 방식을 사용합니다. 언어 이해 및 생성 태스크만 훈련된 모델과 비교하여, 통합 훈련은 더 좋은 성능을示し, 이러한 두 태스크의 통합에 의한 효과를 보여줍니다. Ovis-U1은 OpenCompass 멀티 모달 학술 벤치마크에서 69.6의 점수를 달성하며, Ristretto-3B와 SAIL-VL-1.5-2B과 같은 최근의 최전단 모델을 초과합니다. 텍스트로부터 이미지 생성에서, DPG-Bench와 GenEval 벤치마크에서 각각 83.72와 0.89의 점수를 달성하며, 이미지 편집에서 ImgEdit-Bench와 GEdit-Bench-EN에서 각각 4.00과 6.42의 점수를 달성합니다. Ovis 통합 모델 시리즈의 초기 버전인 Ovis-U1은 다양한 타입의 이해, 생성, 편집의 경계를 초월합니다.",
      "upvotes": 30,
      "discussionId": "686347cd588cea0da970c886",
      "githubRepo": "https://github.com/AIDC-AI/Ovis-U1",
      "ai_summary": "Ovis-U1, a 3-billion-parameter model, combines multimodal understanding, text-to-image generation, and image editing, achieving state-of-the-art performance in various benchmarks.",
      "ai_keywords": [
        "diffusion-based visual decoder",
        "bidirectional token refiner",
        "unified training",
        "OpenCompass",
        "DPG-Bench",
        "GenEval",
        "ImgEdit-Bench",
        "GEdit-Bench-EN"
      ],
      "githubStars": 137
    },
    "publishedAt": "2025-06-28T20:40:17.000Z",
    "title": "Ovis-U1 Technical Report",
    "summary": "In this report, we introduce Ovis-U1, a 3-billion-parameter unified model\nthat integrates multimodal understanding, text-to-image generation, and image\nediting capabilities. Building on the foundation of the Ovis series, Ovis-U1\nincorporates a diffusion-based visual decoder paired with a bidirectional token\nrefiner, enabling image generation tasks comparable to leading models like\nGPT-4o. Unlike some previous models that use a frozen MLLM for generation\ntasks, Ovis-U1 utilizes a new unified training approach starting from a\nlanguage model. Compared to training solely on understanding or generation\ntasks, unified training yields better performance, demonstrating the\nenhancement achieved by integrating these two tasks. Ovis-U1 achieves a score\nof 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent\nstate-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In\ntext-to-image generation, it excels with scores of 83.72 and 0.89 on the\nDPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves\n4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the\ninitial version of the Ovis unified model series, Ovis-U1 pushes the boundaries\nof multimodal understanding, generation, and editing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23044.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "636f4c6b5d2050767e4a1491",
      "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
      "fullname": "Guo-Hua Wang",
      "name": "Flourish",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23858",
      "authors": [
        {
          "_id": "686347d3588cea0da970c888",
          "name": "Jianzong Wu",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c889",
          "user": {
            "_id": "64560a2aaaaf85a98fa9a4b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560a2aaaaf85a98fa9a4b9/2Kp0S0sMVpKqo81s-l_Yt.png",
            "isPro": false,
            "fullname": "Liang Hou",
            "user": "lianghou",
            "type": "user"
          },
          "name": "Liang Hou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:49.312Z",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88a",
          "name": "Haotian Yang",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88b",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88c",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88d",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88e",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88f",
          "name": "Yunhai Tong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T13:52:31.000Z",
      "submittedOnDailyAt": "2025-07-01T00:59:37.837Z",
      "title": "VMoBA: 블록의 섞어합성 注意력을 활용한 비디오 디퓨션 모델",
      "submittedOnDailyBy": {
        "_id": "657a6eed1ccc3c2a5ea7b585",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RIQIF-JJdNI0SwJEq_9z7.jpeg",
        "isPro": true,
        "fullname": "Jianzong Wu",
        "user": "jianzongwu",
        "type": "user"
      },
      "summary": "全注意力 메커니즘의 두차식 복잡도는 긴 시간, 고분해도 비디오를 생성하기 위한 비디오 확산 모델(VDMs)에 큰 한계가 됩니다. 다양한 희소한 注意력 방법들이 제안되었지만, 많은 방법은 훈련과 무관한 추론 가속화기로 설계되어, 혹은 훈련되지 않은 원생 비디오 데이터에서 독특한 시간-공간 특징을 최적적으로 감지하지 못했습니다. 본 논문에서는 VDMs에专门 설계된 새로운 희소한 注意력 메커니즘인 비디오 블록 혼합 注意력(VMoBA)을 소개합니다. 미리 훈련된 비디오 트랜스포머의 注意력 패턴에 대한 심도있는 분석을 기반으로, 이 분석은 강한 시간-공간 국부성, 변화하는 쿼리 중요성과 헤드 특정의 집중 수준을 밝혀내었습니다. VMoBA는 원래의 MoBA 프레임워크를 세 가지 주요 수정으로 강화했습니다: (1) 단계별 재귀 블록 분할 계획(1D-2D-3D), 다양한 시간-공간 注意력 패턴을 동적으로 적응하고 효율성을 높입니다; (2) 전역 블록 선택, 전체 注意력 헤드에서 쿼리-키 블록 상호작용의 가장 유의미한 부분을 우선시합니다; (3) 임계값 기반의 블록 선택, 누적 유사성을 기반으로 참여하는 블록 수를 동적으로 결정합니다. 광범위한 실험은 VMoBA가 VDMs의 긴 시퀀스 학습을 크게 가속화하고, FLOPs를 2.92배, 지연을 1.48배 가속화하며, 완전한 注意력과 같은 또는 더 우수한 생성 품질을 달성함을 보여주었습니다. 또한, VMoBA는 훈련과 무관한 추론에서 경쟁적인 성능을 나타내며, 고분해도 비디오 생성에 2.40배의 FLOPs와 1.35배의 지연 가속화를 제공합니다.",
      "upvotes": 22,
      "discussionId": "686347d3588cea0da970c890",
      "projectPage": "https://github.com/KwaiVGI/VMoBA",
      "githubRepo": "https://github.com/KwaiVGI/VMoBA",
      "ai_summary": "VMoBA, a novel sparse attention mechanism for Video Diffusion Models (VDMs), accelerates training and inference by addressing the quadratic complexity of full attention mechanisms while maintaining or improving video generation quality.",
      "ai_keywords": [
        "quadartic complexity",
        "full attention mechanisms",
        "Video Diffusion Models",
        "VDMs",
        "sparse attention methods",
        "in-depth analysis",
        "attention patterns",
        "video transformers",
        "spatio-temporal locality",
        "query importance",
        "head-specific concentration",
        "layer-wise recurrent block partition scheme",
        "global block selection",
        "threshold-based block selection",
        "FLOPs",
        "latency",
        "high-res video generation"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-06-30T09:52:31.000Z",
    "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models",
    "summary": "The quadratic complexity of full attention mechanisms poses a significant\nbottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,\nhigh-resolution videos. While various sparse attention methods have been\nproposed, many are designed as training-free inference accelerators or do not\noptimally capture the unique spatio-temporal characteristics inherent in video\ndata when trained natively. This paper introduces Video Mixture of Block\nAttention (VMoBA), a novel sparse attention mechanism specifically adapted for\nVDMs. Motivated by an in-depth analysis of attention patterns within\npre-trained video transformers, which revealed strong spatio-temporal locality,\nvarying query importance, and head-specific concentration levels, VMoBA\nenhances the original MoBA framework with three key modifications: (1) a\nlayer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to\ndiverse spatio-temporal attention patterns and improve efficiency; (2) global\nblock selection to prioritize the most salient query-key block interactions\nacross an entire attention head; and (3) threshold-based block selection to\ndynamically determine the number of attended blocks based on their cumulative\nsimilarity. Extensive experiments demonstrate that VMoBA significantly\naccelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and\n1.48x latency speedup, while attaining comparable or even superior generation\nquality to full attention. Furthermore, VMoBA exhibits competitive performance\nin training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for\nhigh-res video generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23858.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a6eed1ccc3c2a5ea7b585",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RIQIF-JJdNI0SwJEq_9z7.jpeg",
      "fullname": "Jianzong Wu",
      "name": "jianzongwu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.24123",
      "authors": [
        {
          "_id": "68634673588cea0da970c862",
          "name": "Yue Ma",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c863",
          "name": "Qingyan Bai",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c864",
          "name": "Hao Ouyang",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c865",
          "name": "Ka Leong Cheng",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c866",
          "name": "Qiuyu Wang",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c867",
          "name": "Hongyu Liu",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c868",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c869",
          "name": "Haofan Wang",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c86a",
          "user": {
            "_id": "6478a982256b62e219917d67",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PUJ-N2cQxgEmDGfyjajyA.jpeg",
            "isPro": false,
            "fullname": "JingyeChen22",
            "user": "JingyeChen22",
            "type": "user"
          },
          "name": "Jingye Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:14:05.849Z",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c86b",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c86c",
          "name": "Qifeng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T17:59:06.000Z",
      "submittedOnDailyAt": "2025-07-01T01:00:32.385Z",
      "title": "카라크라가： 프리스타일 테크ス텍 이미지 커스텀화",
      "submittedOnDailyBy": {
        "_id": "63f0baf66309c84d5f4a2226",
        "avatarUrl": "/avatars/a122f7d92441bd2feef7d4eda993fab7.svg",
        "isPro": false,
        "fullname": "Meme155",
        "user": "Meme145",
        "type": "user"
      },
      "summary": "카リグラファーは、새로운 디ファンスベース의 프레임워크를 소개합니다. 이 프레임워크는 고수준의 텍스트 커스터마이징과 예술적인 텍스트 디자인을 발전적으로 통합하고, 디지털 커리グラフィー 및 디자인 앱에 적용하기 위한 새로운 아이디어를 제안합니다. 텍스트 디자인의 일반화와 데이터 의존성 문제를 해결하기 위해, 프레임워크는 3가지 주요 기술 기여를 실현합니다. 첫째, 사전 학습된 텍스트로부터 이미지로 생성된 모델과 큰 언어 모델을 활용하여, 자기 학습 구조를 개발하여 자동적으로 스타일 센티어리 텍스트 벤치마크를 구축합니다. 둘째, 트레너블 스타일 인코더를 통해 지역적 스타일 인젝션 프레임을 도입하고, 참조 이미지에서 강력한 스타일 특징을 추출합니다. 이 프레임워크는 Qformer과 선형 레이어로 구성됩니다. 또한, 이 프레임워크는 참조 이미지를 직접 디시フェイン 프로세스에 삽입하는 인코텍스트 생성 구조를 사용하며, 목표 스타일의 정밀한 일치를 촉진합니다. 다양한 폰트와 디자인 컨텍스트의 광범위한 범위에서 매우 상세한 양적 및 질적 평가를 통해, 카リグラファー는 복잡한 스타일의 세부 사항을 정확하게 재현하고, 글자의 위치를 정밀하게 결정합니다. 고품질, 시각적으로 일관된 텍스트 디자인을 자동화함으로써, 카リグラファー는 전통적인 모델을 초월하고, 디지털 아트, 브랜딩, 그리고 콘텐츠 텍스트 디자인의 현대적인 창의적인 실천자로 힘과 힘을 부여합니다.",
      "upvotes": 20,
      "discussionId": "68634673588cea0da970c86d",
      "projectPage": "https://calligrapher2025.github.io/Calligrapher/",
      "githubRepo": "https://github.com/Calligrapher2025/Calligrapher",
      "ai_summary": "Calligrapher uses a diffusion-based framework with self-distillation and localized style injection to generate high-quality, stylistically consistent digital typography.",
      "ai_keywords": [
        "diffusion-based framework",
        "self-distillation mechanism",
        "text-to-image generative model",
        "large language model",
        "localized style injection",
        "Qformer",
        "linear layers",
        "style encoder",
        "in-context generation mechanism",
        "denoising process",
        "stylistic details",
        "glyph positioning"
      ],
      "githubStars": 21
    },
    "publishedAt": "2025-06-30T13:59:06.000Z",
    "title": "Calligrapher: Freestyle Text Image Customization",
    "summary": "We introduce Calligrapher, a novel diffusion-based framework that\ninnovatively integrates advanced text customization with artistic typography\nfor digital calligraphy and design applications. Addressing the challenges of\nprecise style control and data dependency in typographic customization, our\nframework incorporates three key technical contributions. First, we develop a\nself-distillation mechanism that leverages the pre-trained text-to-image\ngenerative model itself alongside the large language model to automatically\nconstruct a style-centric typography benchmark. Second, we introduce a\nlocalized style injection framework via a trainable style encoder, which\ncomprises both Qformer and linear layers, to extract robust style features from\nreference images. An in-context generation mechanism is also employed to\ndirectly embed reference images into the denoising process, further enhancing\nthe refined alignment of target styles. Extensive quantitative and qualitative\nevaluations across diverse fonts and design contexts confirm Calligrapher's\naccurate reproduction of intricate stylistic details and precise glyph\npositioning. By automating high-quality, visually consistent typography,\nCalligrapher surpasses traditional models, empowering creative practitioners in\ndigital art, branding, and contextual typographic design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.24123.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f0baf66309c84d5f4a2226",
      "avatarUrl": "/avatars/a122f7d92441bd2feef7d4eda993fab7.svg",
      "fullname": "Meme155",
      "name": "Meme145",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22832",
      "authors": [
        {
          "_id": "6863989c588cea0da970c985",
          "user": {
            "_id": "6192657ba9638054a9818f04",
            "avatarUrl": "/avatars/35f6f6eb2e8f6b283034632a141c2670.svg",
            "isPro": false,
            "fullname": "Alexander Gambashidze",
            "user": "alexgambashidze",
            "type": "user"
          },
          "name": "Alexander Gambashidze",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:12:55.879Z",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c986",
          "name": "Li Pengyi",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c987",
          "user": {
            "_id": "6626c5d0a329de26e7eb16fa",
            "avatarUrl": "/avatars/124f389f768fb666efd8b5a9b54c3b3c.svg",
            "isPro": false,
            "fullname": "Matvey Skripkin",
            "user": "barracuda049",
            "type": "user"
          },
          "name": "Matvey Skripkin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:12:51.866Z",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c988",
          "name": "Andrey Galichin",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c989",
          "name": "Anton Gusarov",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c98a",
          "name": "Konstantin Sobolev",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c98b",
          "name": "Andrey Kuznetsov",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c98c",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6192657ba9638054a9818f04/IpysW0QkLzIgzWognSV7n.png"
      ],
      "publishedAt": "2025-06-28T09:53:17.000Z",
      "submittedOnDailyAt": "2025-07-01T06:44:35.487Z",
      "title": "Listener-Rewarded Thinking in VLMs for Image Preferences\n\n이 텍스트는 \"VLMs(Vision-Language Models)에서 Listener-Rewarded Thinking에 대한 이미지 선호에 대한 연구\"로 번역될 수 있습니다. 이 논문은 이미지 선호를 분석하는 데 사용되는 Vision-Language Models(VLMs)의 새로운 접근 방식을 제시하고 있습니다. \"Listener-Rewarded Thinking\"은 모델이 이미지에 대한 정보를 이해하고, 이를 기반으로 판단하고 행동하는 방식의 개념입니다. 이 방식은 모델이 이미지의 내용을 이해하고, 이를 기반으로 이미지 선호를 판단하는 데 도움을 줍니다. 이 연구는 이미지 분석 분야에서 중요한 발전을 나타내며, 다양한 분야에서 이미지 분석을 활용하는 데 도움이 될 수 있습니다.",
      "submittedOnDailyBy": {
        "_id": "6192657ba9638054a9818f04",
        "avatarUrl": "/avatars/35f6f6eb2e8f6b283034632a141c2670.svg",
        "isPro": false,
        "fullname": "Alexander Gambashidze",
        "user": "alexgambashidze",
        "type": "user"
      },
      "summary": "훈련 강건하고 확장 가능한 보상 모델을 인간의 시각적 취향에 맞게, 문에서 이미지 및 문에서 동영상의 생성 모델을 인간 의도에 맞게 만드는 데 중요합니다. 그러나 현재의 보상 모델은 일반적으로 확산성이 낮고, 정규적인 조정은 기억에 의존하는 복잡한 注釈 파이프라인을 필요로 합니다. 강화 학습(RL)에 특히 Group Relative Policy Optimization(GRPO)을 사용하여 확산성을 향상시킬 수 있습니다が, 우리는 주요 실패 모드를 발견했습니다: 모델의 이유의 흔적이 독립된, 냉각된 시각 언어 모델(「청취자」)의 이유와 상충하는 경우, 이유의 정확도가 크게 떨어집니다. 이를 해결하기 위해, 우리는 「청취자」를 추가한 GRPO 프레임워크를 제안합니다. 여기서, 「청취자」는 이유의 연속 오소스를 재평가하고, 밀도 조정된 신뢰도 스코어를 제공하며, RL의 보상 신호를 형성합니다. 이는 이유의 정확성과 함께, 독립된 모델에 유력한 설명을 생성하도록 이유의 모델을 권장합니다. 우리의 「청취자」를 기반으로한 보상 스키펫은 ImageReward 벤치마크에서 가장 높은 정확도(67.4%)를 달성하며, 1.2M 투표의 큰 규모의 인간 취향 데이터 세트(나일프한 이유 모델보다 최대 +6%의 향상)에서 외분역성 성능을 크게 향상시키고, 강한 GRPO와 SFT 벤치마크에 비해 이유의 상충을 줄입니다. 이러한 결과를 통해, 「청취자」 기반의 보상은 시각 언어 모델과 복잡한 인간 취향을 맞추는 데 scalable하고 데이터 효율적인 패스를 제공함을 보여줍니다. 우리의 이유 모델을 공개합니다: https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.",
      "upvotes": 13,
      "discussionId": "6863989c588cea0da970c98d",
      "ai_summary": "A listener-augmented Group Relative Policy Optimization framework improves reward models by re-evaluating reasoning processes, leading to enhanced accuracy and out-of-distribution performance in aligning vision-language models with human preferences.",
      "ai_keywords": [
        "reinforcement learning",
        "Group Relative Policy Optimization (GRPO)",
        "listener-augmented GRPO",
        "vision-language model",
        "chain-of-thought",
        "image preference reasoning",
        "ImageReward benchmark",
        "out-of-distribution (OOD) performance"
      ]
    },
    "publishedAt": "2025-06-28T05:53:17.000Z",
    "title": "Listener-Rewarded Thinking in VLMs for Image Preferences",
    "summary": "Training robust and generalizable reward models for human visual preferences\nis essential for aligning text-to-image and text-to-video generative models\nwith human intent. However, current reward models often fail to generalize, and\nsupervised fine-tuning leads to memorization, demanding complex annotation\npipelines. While reinforcement learning (RL), specifically Group Relative\nPolicy Optimization (GRPO), improves generalization, we uncover a key failure\nmode: a significant drop in reasoning accuracy occurs when a model's reasoning\ntrace contradicts that of an independent, frozen vision-language model\n(\"listener\") evaluating the same output. To address this, we introduce a\nlistener-augmented GRPO framework. Here, the listener re-evaluates the\nreasoner's chain-of-thought to provide a dense, calibrated confidence score,\nshaping the RL reward signal. This encourages the reasoner not only to answer\ncorrectly, but to produce explanations that are persuasive to an independent\nmodel. Our listener-shaped reward scheme achieves best accuracy on the\nImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)\nperformance on a large-scale human preference dataset (1.2M votes, up to +6%\nover naive reasoner), and reduces reasoning contradictions compared to strong\nGRPO and SFT baselines. These results demonstrate that listener-based rewards\nprovide a scalable, data-efficient path to aligning vision-language models with\nnuanced human preferences. We will release our reasoning model here:\nhttps://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6192657ba9638054a9818f04/IpysW0QkLzIgzWognSV7n.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22832.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6192657ba9638054a9818f04",
      "avatarUrl": "/avatars/35f6f6eb2e8f6b283034632a141c2670.svg",
      "fullname": "Alexander Gambashidze",
      "name": "alexgambashidze",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.24119",
      "authors": [
        {
          "_id": "68634850588cea0da970c892",
          "user": {
            "_id": "635e3a76106f984574c36409",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
            "isPro": false,
            "fullname": "Bo Liu",
            "user": "Benjamin-eecs",
            "type": "user"
          },
          "name": "Bo Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:46.740Z",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c893",
          "name": "Leon Guertler",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c894",
          "user": {
            "_id": "636681feaa6a4af6073ba73e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636681feaa6a4af6073ba73e/u_0moYNu6as9Sszp-ej95.png",
            "isPro": true,
            "fullname": "Simon Yu",
            "user": "simonycl",
            "type": "user"
          },
          "name": "Simon Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:44.863Z",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c895",
          "user": {
            "_id": "65f5392c68b8e0cb3c9977a2",
            "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
            "isPro": false,
            "fullname": "Zichen",
            "user": "lkevinzc",
            "type": "user"
          },
          "name": "Zichen Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:42.588Z",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c896",
          "name": "Penghui Qi",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c897",
          "name": "Daniel Balcells",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c898",
          "name": "Mickel Liu",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c899",
          "name": "Cheston Tan",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89a",
          "name": "Weiyan Shi",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89b",
          "name": "Min Lin",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89c",
          "name": "Wee Sun Lee",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89d",
          "name": "Natasha Jaques",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T17:58:13.000Z",
      "submittedOnDailyAt": "2025-07-01T01:11:49.104Z",
      "title": "SPIRAL: 0과의 게임에서 자신의 게임을 진행하면서 이유를 깊게 만드는 다 에이전트 다 단계 강화학습",
      "submittedOnDailyBy": {
        "_id": "635e3a76106f984574c36409",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
        "isPro": false,
        "fullname": "Bo Liu",
        "user": "Benjamin-eecs",
        "type": "user"
      },
      "summary": "최근의 강화학습의 발전은 언어 모델이 증명 가능한 보상을 가지는 태스크에서 훈련하여 복잡한 이유론을 개발할 수 있음을 보여주고 있지만, 이러한 접근法是 인간이 편집한 문제 해결 쌍과 라인专門的 보상 학습에 의존합니다. 우리는 SPIRAL(Self-Play Framework for Incremental Learning and Adaptation)을 소개합니다. SPIRAL은 모델이 자신의 진화하는 버전과 여러 차례, 0 합 게임을 플레이하여 학습하는 프레임워크입니다. 이로써 인간의 감독이 필요하지 않습니다. SPIRAL은 모델이 强한 상대에 대해 항상 적응해야 하므로, 발전적인 어려운 문제를 무한한 カレクリウム을 생성합니다. 이러한 순환적인 학습을 실현하기 위해, 모델의 학습을 대규모로 실현하기 위해, 모든 온라인, 여러 차례, 다 에이전트의 강화학습 시스템을 구현하고, 조건부 우선 순위 평가(RAE)를 제안합니다. SPIRAL을 사용하면, 0 합 게임에서 자신의 플레이로 인해 이유론 능력이 광범위하게 이동합니다. Qwen3-4B-Base의 단일의 Kuhn Poker에서 학습에서, 수학에 대해 8.6%의 향상, 일반적인 이유론에 대해 8.4%의 향상이 실현되어, 25,000의 효과적인 게임 트래픽에서 SFT를 초과합니다. 분석에 따르면, 이 이동은 시스템적 분해, 기대값 계산, 사례 분석의 3가지 인지 패턴으로 발생합니다. 여러 게임의 학습(TicTacToe, Kuhn Poker, Simple Negotiation)은 각 게임이 다른 이유론력을 개발하여 성능을 향상시킵니다. SPIRAL을 강력한 이유론 모델(DeepSeek-R1-Distill-Qwen-7B)에 적용하면, 평균 2.0%의 향상이 관찰됩니다. 이러한 결과를 통해, 0 합 게임이 자연스럽게 이동 가능한 이유론력을 개발하는 것을 보여줍니다, 자동 인지 개발의 유망한 방향을 밝혀줍니다.",
      "upvotes": 11,
      "discussionId": "68634850588cea0da970c89e",
      "githubRepo": "https://github.com/spiral-rl/spiral",
      "ai_summary": "Self-play in zero-sum games using SPIRAL enhances reasoning capabilities in language models through self-improvement and transfer learning.",
      "ai_keywords": [
        "reinforcement learning",
        "language models",
        "self-play framework",
        "multi-turn games",
        "zero-sum games",
        "reward engineering",
        "role-conditioned advantage estimation",
        "Kuhn Poker",
        "TicTacToe",
        "Simple Negotiation",
        "transferable reasoning"
      ],
      "githubStars": 12
    },
    "publishedAt": "2025-06-30T13:58:13.000Z",
    "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning",
    "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.24119.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635e3a76106f984574c36409",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
      "fullname": "Bo Liu",
      "name": "Benjamin-eecs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.17930",
      "authors": [
        {
          "_id": "68634a4e588cea0da970c8ba",
          "user": {
            "_id": "61e09ec13a1781f66b4e9ae2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
            "isPro": false,
            "fullname": "Jianyu Wang",
            "user": "Jianyu",
            "type": "user"
          },
          "name": "Jianyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:27.023Z",
          "hidden": false
        },
        {
          "_id": "68634a4e588cea0da970c8bb",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:31.161Z",
          "hidden": false
        },
        {
          "_id": "68634a4e588cea0da970c8bc",
          "user": {
            "_id": "6454685a548f22be598414c4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg",
            "isPro": false,
            "fullname": "Lidong Bing",
            "user": "LidongBing",
            "type": "user"
          },
          "name": "Lidong Bing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:29.213Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-22T07:53:07.000Z",
      "submittedOnDailyAt": "2025-07-01T01:09:47.554Z",
      "title": "进化하는 Prompt의 컨텍스트 내: 개방적이고 자동 복제하는 관점",
      "submittedOnDailyBy": {
        "_id": "61e09ec13a1781f66b4e9ae2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
        "isPro": false,
        "fullname": "Jianyu Wang",
        "user": "Jianyu",
        "type": "user"
      },
      "summary": "우리는 대언어 모델(LLM)의 프로ン퓰트에 대한 일반적인 지식에 도전하는 새로운 프로ン퓰트 디자인 패러다임을 제안합니다. 일반적인 지식은 프로ン퓰트 학습(ICL)을 위해 잘 만든 명령과 유도를 우선시하지만, 우리는 랜덤한 유도를 \"지베리스\"라고 부르며, 이를 줄이면 다양한 태스크에서도 성능을 놀라게 올릴 수 있다는 것을 보여주었습니다. 특히, \"지베리스\"는 최신의 자동 프로ン퓰트 최적화 방법보다 큰 효과를 낼 수 있으며, LLM의 어레이멘션과 관련없이 큰 효과를 낼 수 있습니다. 그러나 효과적인 줄임책을 찾기는 단순하지 않습니다. 기존의 속성 방법과 프로ン퓰트 압축 알고리즘은 강력한 결과를 제공하지 못하며, 인간의 직관도 한계입니다. 이 점에 대해, 우리는 자동적으로 낮은 데이터 레이ン에서 줄임책을 검색하는 자기발견 프로ン퓰트 최적화 프레임워크인 PromptQuine을 제안합니다. 이 프레임워크는 자연스러운 현상처럼, 공생과 자기조직 현상처럼, 자원 제한에 대한 반응으로 나타날 수 있는 비 전통적인 현상이 매우 효과적인 프로ン퓰트를 진화적으로 련마시킵니다. 이 프레임워크는 분류, 다중 선택 문제, 생성 및 수학적 논리론의 태스크에서도 LLM에서도 효과적이고, 더 좋은 실행 시간 효율을 달성합니다. 우리는 이 발견이 프로ン퓰트 학습의 구조 연구에 가이드를 제공하고, 더 개방적인 검색알고리즘을 위해 더 효과적인 LLM 프로ン퓰트를 위한 실험을 촉진하는 것을 희망합니다.",
      "upvotes": 10,
      "discussionId": "68634a4f588cea0da970c8bd",
      "ai_summary": "A novel prompt design paradigm, PromptQuine, shows that pruning random demonstrations into \"gibberish\" can improve large language model performance across various tasks, surpassing state-of-the-art methods.",
      "ai_keywords": [
        "prompt design paradigm",
        "in-context learning",
        "pruning",
        "demonstrations",
        "gibberish",
        "prompt optimization",
        "self-discover prompt optimization framework",
        "PromptQuine",
        "evolutionary search framework",
        "classification",
        "multi-choice question answering",
        "generation",
        "math reasoning",
        "tokens",
        "emergent complexity",
        "symbiosis",
        "self-organization"
      ]
    },
    "publishedAt": "2025-06-22T03:53:07.000Z",
    "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective",
    "summary": "We propose a novel prompt design paradigm that challenges conventional wisdom\nin large language model (LLM) prompting. While conventional wisdom prioritizes\nwell-crafted instructions and demonstrations for in-context learning (ICL), we\nshow that pruning random demonstrations into seemingly incoherent \"gibberish\"\ncan remarkably improve performance across diverse tasks. Notably, the\n\"gibberish\" always matches or surpasses state-of-the-art automatic prompt\noptimization techniques, achieving substantial gains regardless of LLM\nalignment. Nevertheless, discovering an effective pruning strategy is\nnon-trivial, as existing attribution methods and prompt compression algorithms\nfail to deliver robust results, let alone human intuition. In terms of this, we\npropose a self-discover prompt optimization framework, PromptQuine, an\nevolutionary search framework that automatically searches for the pruning\nstrategy by itself using only low-data regimes. Much like the emergent\ncomplexity in nature--such as symbiosis and self-organization--arising in\nresponse to resource constraints, our framework evolves and refines\nunconventional yet highly effective prompts by leveraging only the tokens\npresent within the context. We demonstrate its effectiveness across\nclassification, multi-choice question answering, generation and math reasoning\ntasks across LLMs, while achieving decent runtime efficiency. We hope our\nfindings can guide mechanistic studies on in-context learning, and provide a\ncall to action, to pave the way for more open-ended search algorithms for more\neffective LLM prompting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17930.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "61e09ec13a1781f66b4e9ae2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
      "fullname": "Jianyu Wang",
      "name": "Jianyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23542",
      "authors": [
        {
          "_id": "6863479d588cea0da970c86f",
          "user": {
            "_id": "661b9d96c153e4a0a25adc3e",
            "avatarUrl": "/avatars/b3099b51064c8b71a4bce24e2a49b766.svg",
            "isPro": false,
            "fullname": "Weida Wang",
            "user": "weidawang",
            "type": "user"
          },
          "name": "Weida Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:14:03.221Z",
          "hidden": false
        },
        {
          "_id": "6863479d588cea0da970c870",
          "name": "Changyong He",
          "hidden": false
        },
        {
          "_id": "6863479d588cea0da970c871",
          "name": "Jin Zeng",
          "hidden": false
        },
        {
          "_id": "6863479d588cea0da970c872",
          "name": "Di Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T06:29:24.000Z",
      "submittedOnDailyAt": "2025-07-01T01:05:13.898Z",
      "title": "그래프 정보화 제모트릭 어텐션에 의한 일관된 시간 측정 깊이 디노이징",
      "submittedOnDailyBy": {
        "_id": "6684b284dc7b0ae2cc67660c",
        "avatarUrl": "/avatars/54b3c0c4d808f293d78085d4d504570a.svg",
        "isPro": false,
        "fullname": "liuwanhao",
        "user": "wanhaoliu",
        "type": "user"
      },
      "summary": "時間フライト(ToF) センサー로부터 얻은 심도 이미지는 노이즈에 많은 영향을 받는다. 따라서, 신뢰할 수 있는 다음 단계의 애플리케이션에서 노이즈 제거가 필요합니다. 기존 연구에서는 단일 프레임 처리만 수행하거나, 프레임 간 처리를 수행하지만, 프레임 간 대응하는 픽셀의 심도 변화에 고려하지 않아 시간적 불확실성과 공간적 불확실성을招く 불만족스러운 결과를 초래합니다. 본 논문에서는, 시간적 안정성과 공간적 첨삭성을 동시에 향상시키기 위해, 움직임에 따라 변하지 않는 그래프 통합을 활용한 새로운 ToF 심도 노이즈 제거 네트워크를 제안합니다. 특히, 프레임 간 심도 이동에 관계없이, 그래프 구조는 시간적 자기 유사성을 나타내며, 프레임 간 기하학적인 변환을 수행할 수 있습니다. 다음으로, 결합된 그래프 상의 이미지의 평활성과 ToF 노이즈 분포로부터 얻은 데이터의 정확성 항을 활용하여, ToF 노이즈 제거의 최대 우선 문제를 구성합니다. 마지막으로, 그래프 정보에 기반한 기하학적인 변환으로부터 적응적으로 학습되는 가중치를 가진 반복 필터를 통해, 고성능적이고 해석 가능한 네트워크를 생성합니다. 실험 결과를 통해, 제안된 기법은 합성 데이터 세트 DVToF에서 정확성과 일치성에서 가장 先端적인 성능을 나타내며, 실제 Kinectv2 데이터 세트에서 강한 일반화 성능을 보입니다. 소스 코드는 아래 URL에서 공개됩니다.",
      "upvotes": 8,
      "discussionId": "6863479d588cea0da970c873",
      "ai_summary": "A novel ToF depth denoising network uses motion-invariant graph fusion and adaptive filters to improve temporal stability and spatial sharpness, achieving state-of-the-art performance.",
      "ai_keywords": [
        "ToF depth denoising",
        "motion-invariant graph fusion",
        "graph structures",
        "temporal self-similarity",
        "geometric attention",
        "image smoothness prior",
        "maximum a posterior problem",
        "iterative filters",
        "DVToF dataset",
        "Kinectv2 dataset"
      ]
    },
    "publishedAt": "2025-06-30T02:29:24.000Z",
    "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric\n  Attention",
    "summary": "Depth images captured by Time-of-Flight (ToF) sensors are prone to noise,\nrequiring denoising for reliable downstream applications. Previous works either\nfocus on single-frame processing, or perform multi-frame processing without\nconsidering depth variations at corresponding pixels across frames, leading to\nundesirable temporal inconsistency and spatial ambiguity. In this paper, we\npropose a novel ToF depth denoising network leveraging motion-invariant graph\nfusion to simultaneously enhance temporal stability and spatial sharpness.\nSpecifically, despite depth shifts across frames, graph structures exhibit\ntemporal self-similarity, enabling cross-frame geometric attention for graph\nfusion. Then, by incorporating an image smoothness prior on the fused graph and\ndata fidelity term derived from ToF noise distribution, we formulate a maximum\na posterior problem for ToF denoising. Finally, the solution is unrolled into\niterative filters whose weights are adaptively learned from the graph-informed\ngeometric attention, producing a high-performance yet interpretable network.\nExperimental results demonstrate that the proposed scheme achieves\nstate-of-the-art performance in terms of accuracy and consistency on synthetic\nDVToF dataset and exhibits robust generalization on the real Kinectv2 dataset.\nSource code will be released at\nhttps://github.com/davidweidawang/GIGA-ToF{https://github.com/davidweidawang/GIGA-ToF}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23542.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6684b284dc7b0ae2cc67660c",
      "avatarUrl": "/avatars/54b3c0c4d808f293d78085d4d504570a.svg",
      "fullname": "liuwanhao",
      "name": "wanhaoliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.17417",
      "authors": [
        {
          "_id": "685c8635696820ba1f28f24b",
          "user": {
            "_id": "65d3b7ec8f6b98b34ee6bbe3",
            "avatarUrl": "/avatars/53c2d4e4746147fc2559435d252e8951.svg",
            "isPro": false,
            "fullname": "Mingyuan Wu",
            "user": "Mingyuan1997",
            "type": "user"
          },
          "name": "Mingyuan Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:32:22.703Z",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24c",
          "name": "Meitang Li",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24d",
          "name": "Jingcheng Yang",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24e",
          "name": "Jize Jiang",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24f",
          "name": "Kaizhuo Yan",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f250",
          "name": "Zhaoheng Li",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f251",
          "name": "Minjia Zhang",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f252",
          "name": "Klara Nahrstedt",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T18:23:48.000Z",
      "submittedOnDailyAt": "2025-07-01T00:36:26.256Z",
      "title": "아하 모멘트 再见：VLMs는 추론 시의 스케일링에서 진정한 자동 증명 능력을 갖추는지?",
      "submittedOnDailyBy": {
        "_id": "65d3b7ec8f6b98b34ee6bbe3",
        "avatarUrl": "/avatars/53c2d4e4746147fc2559435d252e8951.svg",
        "isPro": false,
        "fullname": "Mingyuan Wu",
        "user": "Mingyuan1997",
        "type": "user"
      },
      "summary": "최근의 대규모 언어 모델(LLMs)의 발전은 추론 시의 계산 방법(예: 결정 시 스케일링과 자기 보정)이 외부 지식에 의존하지 않고 논리론의 능력을 크게 향상시키는 것을 보여줍니다. 이 성공의 핵심은 자기 보정과 자기 확인을 하는 행동이, 일반적으로 강화 학습(RL)에 의해 발생하는 것입니다. 본 논문에서는 이러한 추론 시의 방법을 시각 언어 모델(VLMs)에도 효과적으로 적용할 수 있는지 조사합니다. 또한, RL로 훈련된 것을 특히 주목합니다. 우리는 다수결과 N 중의 가장 좋은 것을 선택하는 같은 결정 전략이 모든 VLM의 논리론 성능을 향상시키고, 생성에 의존하는 방법(예: 앞서 언급한 것)이, 확인에 의존하는 방법(예: 이후 언급한 것)보다 크게 효과적이라는 것을 발견했습니다. 또한, RL 조정된 모델에 연관된 자기 보정의 행동(예: 「그 순간」)은 측정 가능한 효과를 보여주는 것이 아닙니다. 추론 시의 스케일링 프레임워크 내의 확장된 실험을 통해 주요 원인을 특정하기 위해, RL 훈련된 VLMs은 시각과 문자형 모델 모두에 강대한 자기 확인 능력이 부족하다는 것을 보여주었습니다.",
      "upvotes": 8,
      "discussionId": "685c8636696820ba1f28f253",
      "ai_summary": "Inference-time techniques like decoding-time scaling and self-refinement enhance reasoning in vision-language models, with generation-based methods providing greater improvement than verification-based methods, despite RL-trained models not showing self-correction benefits.",
      "ai_keywords": [
        "large language models",
        "inference-time computation",
        "decoding-time scaling",
        "self-refinement",
        "self-correction",
        "self-verification",
        "reinforcement learning",
        "vision-language models",
        "majority voting",
        "best-of-N selection",
        "aha moment"
      ]
    },
    "publishedAt": "2025-06-20T14:23:48.000Z",
    "title": "Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in\n  Inference-time Scaling?",
    "summary": "Recent advances in large language models (LLMs) have demonstrated that\ninference-time computation techniques, such as decoding-time scaling and\nself-refinement, can significantly enhance reasoning capabilities without\nrelying on external knowledge. A key driver of this success is the emergence of\nself-correction and self-verification behaviors, often elicited through\nreinforcement learning (RL). In this paper, we investigate whether these\ninference-time techniques extend effectively to vision-language models (VLMs),\nparticularly those trained with RL. We find that while decoding strategies such\nas majority voting and best-of-N selection with self-verification all improve\nVLM reasoning performance, generation-reliant methods such as the former\nachieve significantly higher gains versus verification-reliant methods such as\nthe latter. Additionally, the self-correction behavior often associated with\nRL-tuned models, such as aha moment, does not lead to measurable gains. We show\nvia extensive experimentation within the inference-time scaling framework to\nidentify a key root cause: RL-trained VLMs still lack robust self-verification\ncapabilities across both visual and textual modalities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d3b7ec8f6b98b34ee6bbe3",
      "avatarUrl": "/avatars/53c2d4e4746147fc2559435d252e8951.svg",
      "fullname": "Mingyuan Wu",
      "name": "Mingyuan1997",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16500",
      "authors": [
        {
          "_id": "6858a7f0c0c8e29df8ea3c06",
          "user": {
            "_id": "64b38bc2a248169796fec4fa",
            "avatarUrl": "/avatars/1371021f2ef0ce2197cc13627c9e03c9.svg",
            "isPro": false,
            "fullname": "Samir Khaki",
            "user": "Skhaki",
            "type": "user"
          },
          "name": "Samir Khaki",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:14:44.911Z",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c07",
          "name": "Xiuyu Li",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c08",
          "name": "Junxian Guo",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c09",
          "name": "Ligeng Zhu",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0a",
          "name": "Chenfeng Xu",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0b",
          "name": "Konstantinos N. Plataniotis",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0c",
          "name": "Amir Yazdanbakhsh",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0d",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0e",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0f",
          "user": {
            "_id": "650dac79b959b0e1d41d7378",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dac79b959b0e1d41d7378/mzbN0MFk3k8b94FQ40I7L.jpeg",
            "isPro": false,
            "fullname": "Zhijian Liu",
            "user": "zhijianliu",
            "type": "user"
          },
          "name": "Zhijian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:14:42.873Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T17:53:34.000Z",
      "submittedOnDailyAt": "2025-07-01T03:16:07.666Z",
      "title": "SparseLoRA: LLM의 미세 조정을 컨텍스트 함수의 희소성으로 가속화하는 방법",
      "submittedOnDailyBy": {
        "_id": "64b38bc2a248169796fec4fa",
        "avatarUrl": "/avatars/1371021f2ef0ce2197cc13627c9e03c9.svg",
        "isPro": false,
        "fullname": "Samir Khaki",
        "user": "Skhaki",
        "type": "user"
      },
      "summary": "LLM의 미세 조정은 계산량과 메모리 사용량에 대한 강력한 요구가 있습니다. QLoRA와 DoRA와 같은 파라미터 효율적인 미세 조정 방법들은 학습 가능한 파라미터의 수를 줄이고 메모리 사용량을 줄일 수 있지만, 계산 비용을 줄일 수는 없습니다. 이러한 경우, 이들은 미세 조정을 늦추는 경우가 있습니다. 본 논문에서는, LLM의 미세 조정을 가속화하기 위해 컨텍스트의 희소성을 활용하는 SparseLoRA의 방법을 소개합니다. 이로써, 계산 비용을 최대 2.2배 줄일 수 있으며, 정확도를 유지하는 동시에, 계산 속도를 최대 1.6배 빠르게 할 수 있습니다.",
      "upvotes": 7,
      "discussionId": "6858a7f0c0c8e29df8ea3c10",
      "projectPage": "https://z-lab.ai/projects/sparselora/",
      "githubRepo": "https://github.com/z-lab/sparselora",
      "ai_summary": "SparseLoRA reduces computational cost and speeds up fine-tuning of LLMs by dynamically selecting a sparse subset of weights for loss and gradient computation.",
      "ai_keywords": [
        "QLoRA",
        "DoRA",
        "parameter-efficient fine-tuning",
        "SparseLoRA",
        "contextual sparsity",
        "SVD sparsity estimator",
        "computational cost",
        "commonsense reasoning",
        "arithmetic reasoning",
        "code generation",
        "instruction following"
      ],
      "githubStars": 20
    },
    "publishedAt": "2025-06-19T13:53:34.000Z",
    "title": "SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity",
    "summary": "Fine-tuning LLMs is both computationally and memory-intensive. While\nparameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the\nnumber of trainable parameters and lower memory usage, they do not decrease\ncomputational cost. In some cases, they may even slow down fine-tuning. In this\npaper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning\nthrough contextual sparsity. We propose a lightweight, training-free SVD\nsparsity estimator that dynamically selects a sparse subset of weights for loss\nand gradient computation. Also, we systematically analyze and address\nsensitivity across layers, tokens, and training steps. Our experimental results\nshow that SparseLoRA reduces computational cost by up to 2.2 times and a\nmeasured speedup of up to 1.6 times while maintaining accuracy across various\ndownstream tasks, including commonsense and arithmetic reasoning, code\ngeneration, and instruction following.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16500.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b38bc2a248169796fec4fa",
      "avatarUrl": "/avatars/1371021f2ef0ce2197cc13627c9e03c9.svg",
      "fullname": "Samir Khaki",
      "name": "Skhaki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23151",
      "authors": [
        {
          "_id": "686399bc588cea0da970c98f",
          "name": "Vladislav Bargatin",
          "hidden": false
        },
        {
          "_id": "686399bc588cea0da970c990",
          "name": "Egor Chistov",
          "hidden": false
        },
        {
          "_id": "686399bc588cea0da970c991",
          "user": {
            "_id": "663692c75f67f8da32723bf8",
            "avatarUrl": "/avatars/258264afe2ea5048a4a7a8e9945d2f5b.svg",
            "isPro": false,
            "fullname": "Alexander Yakovenko",
            "user": "a-yakovenko",
            "type": "user"
          },
          "name": "Alexander Yakovenko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:12:16.036Z",
          "hidden": false
        },
        {
          "_id": "686399bc588cea0da970c992",
          "name": "Dmitriy Vatolin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-29T09:01:42.000Z",
      "submittedOnDailyAt": "2025-07-01T08:44:33.443Z",
      "title": "고해상도 훈련을 통한 메모리 효율적인 다 프레임 광학 흐름 추정\n\n(Note: The translation is provided as requested, maintaining professionalism and accuracy.)",
      "submittedOnDailyBy": {
        "_id": "663692c75f67f8da32723bf8",
        "avatarUrl": "/avatars/258264afe2ea5048a4a7a8e9945d2f5b.svg",
        "isPro": false,
        "fullname": "Alexander Yakovenko",
        "user": "a-yakovenko",
        "type": "user"
      },
      "summary": "최근의 광학 플로우 추론의 발전은 정밀도를 우선시하지만, GPU 메모리 소비량이 증가하는 점을 주의하고 있습니다. 특히, 고해상도(FullHD) 입력에 대해 이 문제가 심각합니다. 여기는 MEMFOF(메모리 효율적인 다 프레임 광학 플로우)를 소개합니다. MEMFOF은 다 프레임 추론과 GPU 메모리 사용량 사이의 유용한 균형을 찾는 메모리 효율적인 다 프레임 광학 플로우 방법입니다. 특히, 1080p 입력에서 실행 시 2.09 GB의 GPU 메모리를 필요로 하며, 학습 시 28.5 GB의 GPU 메모리를 필요로 합니다. 이로 인해 1080p의 원점 규모에서 학습이 가능하고, 자르기 및 다운 샘플링이 필요하지 않습니다. RAFT-like 아키텍처로부터 설계 선택을 체계적으로 재평가하고, 감소된 상관 볼륨과 고해상도 훈련 프로토콜을 다 프레임 추론과 조합하여, 여러 벤치마크에서 가장 先端의 성능을 달성하면서, 크게 메모리 오버헤드를 줄일 수 있습니다. 우리 방법은 정밀도와 실행 시간 효율의 양면에서, 더 자원 효율적인 대안보다 뛰어납니다. 이로 인해 고해상도에서의 플로우 추론의 강건성을 입증합니다. 제안 시점에서, 우리 방법은 1 픽셀(1px)의 오프 라이어 비율이 3.289이고, Spring 벤치마크에서 1위, Sintel(clean)에서 엔드 포인트 오차(EPE)가 0.963이고, KITTI-2015에서 Fl-all 오차가 2.94%로 가장 좋습니다. 코드는 https://github.com/msu-video-group/memfof에서 제공됩니다.",
      "upvotes": 5,
      "discussionId": "686399bd588cea0da970c993",
      "projectPage": "https://msu-video-group.github.io/memfof/",
      "githubRepo": "https://github.com/msu-video-group/memfof",
      "ai_summary": "MEMFOF is a memory-efficient multi-frame optical flow method that achieves state-of-the-art performance on high-resolution inputs with reduced GPU memory usage.",
      "ai_keywords": [
        "MEMFOF",
        "optical flow estimation",
        "memory-efficient",
        "multi-frame",
        "correlation volumes",
        "RAFT-like architectures",
        "high-resolution training",
        "Spring benchmark",
        "Sintel benchmark",
        "KITTI-2015 benchmark",
        "outlier rate",
        "endpoint error",
        "Fl-all error"
      ]
    },
    "publishedAt": "2025-06-29T05:01:42.000Z",
    "title": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame\n  Optical Flow Estimation",
    "summary": "Recent advances in optical flow estimation have prioritized accuracy at the\ncost of growing GPU memory consumption, particularly for high-resolution\n(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical\nflow method that identifies a favorable trade-off between multi-frame\nestimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU\nmemory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely\npositions our method to be trained at native 1080p without the need for\ncropping or downsampling. We systematically revisit design choices from\nRAFT-like architectures, integrating reduced correlation volumes and\nhigh-resolution training protocols alongside multi-frame estimation, to achieve\nstate-of-the-art performance across multiple benchmarks while substantially\nreducing memory overhead. Our method outperforms more resource-intensive\nalternatives in both accuracy and runtime efficiency, validating its robustness\nfor flow estimation at high resolutions. At the time of submission, our method\nranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,\nleads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the\nbest Fl-all error on KITTI-2015 at 2.94%. The code is available at\nhttps://github.com/msu-video-group/memfof.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23151.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "663692c75f67f8da32723bf8",
      "avatarUrl": "/avatars/258264afe2ea5048a4a7a8e9945d2f5b.svg",
      "fullname": "Alexander Yakovenko",
      "name": "a-yakovenko",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.22992",
      "authors": [
        {
          "_id": "686370bd588cea0da970c90e",
          "user": {
            "_id": "662a75287181150b857245fb",
            "avatarUrl": "/avatars/6a489bf6d3d9cd26ba88f17b35c6ecb5.svg",
            "isPro": false,
            "fullname": "Yulun Jiang",
            "user": "yljblues",
            "type": "user"
          },
          "name": "Yulun Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:16.902Z",
          "hidden": false
        },
        {
          "_id": "686370bd588cea0da970c90f",
          "name": "Yekun Chai",
          "hidden": false
        },
        {
          "_id": "686370bd588cea0da970c910",
          "name": "Maria Brbić",
          "hidden": false
        },
        {
          "_id": "686370bd588cea0da970c911",
          "user": {
            "_id": "6438d1d843d932c462404500",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
            "isPro": false,
            "fullname": "Michael Moor",
            "user": "mdmoor",
            "type": "user"
          },
          "name": "Michael Moor",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:19.043Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6438d1d843d932c462404500/FI8PgTG5g_uZ1q4y-c_Jd.png"
      ],
      "publishedAt": "2025-06-28T19:44:32.000Z",
      "submittedOnDailyAt": "2025-07-01T05:06:51.086Z",
      "title": "마버ル: 다모달스페이스 인식과 계획의 엄격한 벤치마크",
      "submittedOnDailyBy": {
        "_id": "6438d1d843d932c462404500",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
        "isPro": false,
        "fullname": "Michael Moor",
        "user": "mdmoor",
        "type": "user"
      },
      "summary": "정보의 다양한 모델링 데이터에서 처리할 수 있는 능력과 단계별로 이유를 만드는 능력은 인공지능의 발전에 있어 중요한 문제입니다. 그러나 현재의 이유의 벤치마크는 텍스트만 이유를 초점을 맞추거나 모델링 데이터에서 직접 정보를 검색할 수 있는 다양한 질의를 사용하는 것입니다. 이로 인해 복잡한 이유의 이해는 다양한 모델링 데이터 영역에서 더욱 낮은 수준으로 조사되고 있습니다. 여기서 우리는 복잡한 다양한 모델링 데이터 문제를 단계별로 이유를 만드는 능력을 평가하기 위해 설계된 도전적인 다양한 모델링 데이터 이유의 벤치마크를 소개합니다. 이는 복잡한 공간적, 시각적, 물리적 제약 아래 여러 단계의 계획의 작성과 이해를 요구하는 두 가지 높은 도전적인 태스크인 M-Portal과 M-Cube로 구성됩니다. 우리는 현재 다양한 모델링 데이터 모델(MLLMs)은 MARBLE에서 낮은 성능을 나타냅니다. 12개의 발전 모델은 모든 M-Portal에서 근사 랜덤한 성능을 보여주고, M-Cube에서 0%의 정확도를 나타냅니다. 또한 단순화된 서브 태스크에서 일부 모델은 랜덤 기반 선을 초과하고 있으며, 복잡한 이유의 이해는 현재의 MLLMs에서 여전히 도전적인 과제로 밝혀졌습니다. 또한 우리는 시각적 입력으로부터 정보를 추출하는 것이 어려운 경우가 있음을 보여주고, 시각적 인식은 붕괴점인 것으로 나타냅니다. 이로 인해 복잡한 이유의 이해의 한계가 밝혀져, 우리는 MARBLE가 다양한 이유를 단계별로 거쳐 계획을 세워내는 다음 세대 모델의 개발에 촉발하고 있습니다.",
      "upvotes": 3,
      "discussionId": "686370be588cea0da970c912",
      "ai_summary": "MARBLE is a challenging multimodal reasoning benchmark that highlights the limitations of existing multimodal language models in step-by-step reasoning and plan crafting under spatial and visual constraints.",
      "ai_keywords": [
        "multimodal reasoning",
        "multimodal language models",
        "M-Portal",
        "M-Cube",
        "multistep plans",
        "spatial constraints",
        "visual constraints"
      ]
    },
    "publishedAt": "2025-06-28T15:44:32.000Z",
    "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning",
    "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6438d1d843d932c462404500/FI8PgTG5g_uZ1q4y-c_Jd.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22992.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6438d1d843d932c462404500",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
      "fullname": "Michael Moor",
      "name": "mdmoor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23394",
      "authors": [
        {
          "_id": "686350a9588cea0da970c8d4",
          "user": {
            "_id": "645dbaa6f5760d1530d7580d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
            "isPro": true,
            "fullname": "Simeon Emanuilov",
            "user": "s-emanuilov",
            "type": "user"
          },
          "name": "Simeon Emanuilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:22.926Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-29T20:47:27.000Z",
      "submittedOnDailyAt": "2025-07-01T01:39:51.669Z",
      "title": "工具语言를 이야기하는 언어 모델의 교육 방법",
      "submittedOnDailyBy": {
        "_id": "645dbaa6f5760d1530d7580d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
        "isPro": true,
        "fullname": "Simeon Emanuilov",
        "user": "s-emanuilov",
        "type": "user"
      },
      "summary": "외부 도구의 연계는 실제 언어 모델 애플리케이션에 중요한 요소이지만, 다언어 모델은 많은 비영어언어에서 신뢰할 수 있는 도구 사용 능력이 부족합니다. 최신의 다언어 모델도 도구를 사용할 시기를 결정하고 함수 호출에 필요한 구조화된 출력을 생성하는 데 어려움을 겪으며, 낮은 리소스 언어에서 Prompt를 받으면 언어 혼동을 나타내는 경우도 있습니다. 본 논문에서는, 브루가리아를 사례로, 현재의 언어 모델을 적용하여 임의의 목표 언어에서 강력한 도구 사용을 가능하게 하는 방법을 제안합니다. 이 접근법은 MCP(모델 컨텍스트 프로토콜)과 같은 표준화된 프로토콜을 지원하기 위해 새로운 이어어 데이터셋(10,035 예)를 사용하여 BgGPT 모델 시리즈(2.6B, 9B, 27B 파라미터)의 지속적인 훈련을 포함합니다. 본 연구에서는, TUCAN(툴 사용 가능한 보조자비기)를 소개하며, 기초 모델에 대한 함수 호출 정확도가 28.75% 이상 향상되었고, 브루가리아의 기존 벤치마크에서 확인된 핵심적인 언어 이해를 유지합니다. 정확도 향상보다 TUCAN 모델은 기초 모델의冗長하고 불일치하는 출력과 비교하여, セミプライム으로 정형화된 함수 호출을 보여주며, 생산 준비가 완료되어 있습니다. 모델, 평가 프레임워크, 데이터셋은 다른 언어의 재현을 가능하게 하기 위해 공개되었습니다. 본 논문에서는, 툴 추가 기능을 영어 중심의 시스템을 초과하는 실용적인 접근법을 보여주고 있습니다.",
      "upvotes": 2,
      "discussionId": "686350aa588cea0da970c8d5",
      "ai_summary": "A methodology is presented to adapt language models for robust tool use across languages, specifically improving function-calling accuracy in Bulgarian.",
      "ai_keywords": [
        "function-calling",
        "multilingual models",
        "tool-use capabilities",
        "language confusion",
        "BgGPT",
        "bilingual dataset",
        "MCP (Model Context Protocol)",
        "TUCAN (Tool-Using Capable Assistant Navigator)",
        "function-calling accuracy",
        "production-ready response formatting"
      ]
    },
    "publishedAt": "2025-06-29T16:47:27.000Z",
    "title": "Teaching a Language Model to Speak the Language of Tools",
    "summary": "External tool integration through function-calling is essential for practical\nlanguage model applications, yet most multilingual models lack reliable\ntool-use capabilities in non-English languages. Even state-of-the-art\nmultilingual models struggle with determining when to use tools and generating\nthe structured outputs required for function calls, often exhibiting language\nconfusion when prompted in lower-resource languages. This work presents a\nmethodology for adapting existing language models to enable robust tool use in\nany target language, using Bulgarian as a case study. The approach involves\ncontinued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a\nnovel bilingual dataset of 10,035 function-calling examples designed to support\nstandardized protocols like MCP (Model Context Protocol). The research\nintroduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to\n28.75% improvement in function-calling accuracy over base models while\npreserving core language understanding, as verified on established Bulgarian\nbenchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready\nresponse formatting with clean, parsable function calls, contrasting with the\nverbose and inconsistent outputs of base models. The models, evaluation\nframework, and dataset are released to enable replication for other languages.\nThis work demonstrates a practical approach for extending tool-augmented\ncapabilities beyond English-centric systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23394.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645dbaa6f5760d1530d7580d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
      "fullname": "Simeon Emanuilov",
      "name": "s-emanuilov",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 39
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23219",
      "authors": [
        {
          "_id": "686376f6588cea0da970c92b",
          "user": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "isPro": false,
            "fullname": "Jie Feng",
            "user": "JJ-TMT",
            "type": "user"
          },
          "name": "Jie Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:14.367Z",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92c",
          "name": "Shengyuan Wang",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92d",
          "name": "Tianhui Liu",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92e",
          "name": "Yanxin Xi",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92f",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/RBLtf0_JPAX9A8msse6c0.jpeg"
      ],
      "publishedAt": "2025-06-29T13:04:27.000Z",
      "submittedOnDailyAt": "2025-07-01T04:22:03.888Z",
      "title": "도시 LLaVA: 도시 정보의 위해 공간 논리와 이해를 포함하는 다모뎀 대 언어 모델",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "都市 연구에는 다양한 시나리오와 태스크가 포함되어 있으며, 다양한 타입의 데이터 이해가 필요합니다. 현재의 방법들은 특정 데이터 타입에 집중하고, 도시 분야에 적합한 프레임워크를 갖지 않습니다. 최근의 다 타입 대 언어 모델(MLLM)의 성공은 이러한 제한을 극복할 가능성을 보여주고 있습니다. 본 논문에서는 도시 LLaVA라는 다 타입 대 언어 모델을 소개합니다. 이 모델은 4가지 데이터 타입을 동시에 처리하고, 일반적인 MLLM과 비교하여 다양한 도시 태스크에서 강력한 성능을 달성하는 것을 목표로 합니다. 도시 LLaVA에서는 우선, 도시 데이터에 포함되는 단일 모달과 크로스 모달의 도시 데이터로 구성된 다양한 도시 인스트럭션 데이터 세트를 편집합니다. 또한, 공간적인 이유의 향상과 영역 지식의 학습을 구분하여 다 단계의 훈련 프레임워크를 제안하고, 도시 LLaVA의 적용성과 다운 스트리م 성능을 개선합니다. 마지막으로, 도시 연구의 현재 벤치마크를 확장하고, MLLM의 다양한 도시 태스크에서 성능을 평가하는 것을 시도합니다. 3개 도시에서의 실험 결과를 통해 도시 LLaVA가 오픈 소스 및 소유권의 MLLM을 초과하고, 단일 모달 태스크와 복잡한 크로스 모달 태스크에서 성능을 보여주며, 도시 간에 강한 일반화 능력을 보여주는 것을 명확히 합니다. 소스 코드와 데이터는 https://github.com/tsinghua-fib-lab/UrbanLLaVA에서 공개적으로 액세스 가능합니다.",
      "upvotes": 2,
      "discussionId": "686376f6588cea0da970c930",
      "githubRepo": "https://github.com/tsinghua-fib-lab/UrbanLLaVA",
      "ai_summary": "UrbanLLaVA, a multi-modal large language model, effectively processes urban datasets for various tasks, outperforming existing models in both single-modal and complex cross-modal scenarios.",
      "ai_keywords": [
        "multi-modal large language models",
        "spatial reasoning",
        "domain knowledge learning",
        "urban instruction dataset",
        "single-modal",
        "cross-modal",
        "benchmark",
        "urban research"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-06-29T09:04:27.000Z",
    "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence\n  with Spatial Reasoning and Understanding",
    "summary": "Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce UrbanLLaVA, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\nUrbanLLaVA, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of UrbanLLaVA across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that UrbanLLaVA outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/RBLtf0_JPAX9A8msse6c0.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23219.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.22694",
      "authors": [
        {
          "_id": "68637193588cea0da970c914",
          "name": "Raghavv Goel",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c915",
          "name": "Sudhanshu Agrawal",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c916",
          "name": "Mukul Gagrani",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c917",
          "name": "Junyoung Park",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c918",
          "name": "Yifan Zao",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c919",
          "name": "He Zhang",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91a",
          "name": "Tian Liu",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91b",
          "name": "Yiping Yang",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91c",
          "name": "Xin Yuan",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91d",
          "name": "Jiuyan Lu",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91e",
          "name": "Chris Lott",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91f",
          "name": "Mingu Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T00:26:40.000Z",
      "submittedOnDailyAt": "2025-07-01T04:07:47.278Z",
      "title": "VOCABTRIM: 딕셔너리 훈련에서 효율적인 예측적 언어 처리 모델의 프로그래밍",
      "submittedOnDailyBy": {
        "_id": "649b6eb2f7cc759ab756adaf",
        "avatarUrl": "/avatars/a0f7c7345dd653887122200fbe375c2b.svg",
        "isPro": false,
        "fullname": "Raghavv Goel",
        "user": "RaghavvGoel",
        "type": "user"
      },
      "summary": "이 논문에서는 SpD(Special Decoding) 메소드의 성능 향상에 대한 간단한 스태틱 없는 방법들을 소개합니다. 이 방법은 SpD의 시작 프로세스에서 LM head(Language Model Head)를 포함시켜 SpD를 효과적으로 개선합니다. SpD는 작은 언어 모델(Draft Model)을 사용하여 여러 토큰으로 구성된 드래프트 시퀀스 또는 트리를 샘플링하고, 그 후 Base LLM(타겟 모델)으로 확인하여 일부를 Base LLM의 유효한 생성으로 받아들입니다. 일반적으로 SpD는 타겟 모델과 드래프트 모델의 비오카라비티의 1대1 매핑이 필요로 여겨서, 비오카라비티를 공유하거나 LM head를 공유하는 EAGLE나 Medusa와 같이 자연스럽게 됩니다. 먼저, 이 드래프트 토큰 샘플링 기법은 드래프트 시에 불필요한 추론 오버헤드를 포함하는 것을 인식합니다. 특히, 타겟 LLM의 비오카라비티가 매우 크다면, 이 오버헤드는 특히 커집니다. 따라서, VocabTrim이라는 간단한 방법을 제안하고, 메모리 바네트트 환경에서 생성 속도를 향상시키기 위해 드래프트 오버헤드를 줄입니다. VocabTrim은 드래프트 모델의 LM head를 타겟 모델의 비오카라비티에서 가장 빈번하게 샘플링되는 토큰의 제한된 집합으로 제한합니다. 드래프트 시 비오카라비티를 제한하는 것은 수용률을 조금 줄이는 데만입니다. 그러나 메모리 바네트트 프로세스의 라틴 시를 크게 줄이고, 에지 장치에서 일반적인 경우, 메모리 바네트트 속도 업(MBSU)를 높입니다. 또한, 우리 방법은 Spec-Bench에서 Llama-3 모델의 메모리 바네트트 속도 업을 약 16% 정도 높고, 특히 Llama-3.2-3B-Instruct에서 약 16% 정도 높는 것을 보여줍니다.",
      "upvotes": 2,
      "discussionId": "68637193588cea0da970c920",
      "ai_summary": "A technique called VocabTrim improves drafter-based speculative decoding by reducing the vocabulary of the drafter language model, thus decreasing drafting latency in memory-bound environments.",
      "ai_keywords": [
        "drafter-based speculative decoding",
        "speculative decoding",
        "LM head",
        "drafters",
        "token sampling",
        "inference overhead",
        "target LLM",
        "vocabulary sharing",
        "EAGLE",
        "Medusa",
        "VocabTrim",
        "acceptance rate",
        "drafting latency",
        "memory-bound speed up",
        "Spec-Bench",
        "Llama-3"
      ]
    },
    "publishedAt": "2025-06-27T20:26:40.000Z",
    "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs",
    "summary": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649b6eb2f7cc759ab756adaf",
      "avatarUrl": "/avatars/a0f7c7345dd653887122200fbe375c2b.svg",
      "fullname": "Raghavv Goel",
      "name": "RaghavvGoel",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23135",
      "authors": [
        {
          "_id": "68638235588cea0da970c966",
          "name": "Yu Shang",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c967",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c968",
          "name": "Yinzhou Tang",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c969",
          "name": "Lei Jin",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c96a",
          "name": "Chen Gao",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c96b",
          "name": "Wei Wu",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c96c",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/oiWNtmiOwH-sWe0ewE2YQ.jpeg"
      ],
      "publishedAt": "2025-06-29T08:19:45.000Z",
      "submittedOnDailyAt": "2025-07-01T05:11:31.002Z",
      "title": "RoboScape: 물리정보가 있는 시각화 월드 모델",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "세계 모델은 몸이 된 지능의 필수적인 도구로서 작동하고 있습니다. 이들은 실제 로봇의 사진 생성을 위해 강력한 시뮬레이터로 작용하며, 중요한 데이터 부족 문제를 해결하고 있습니다. 그러나 현재 몸이 된 세계 모델은 3D 일반화 및 동작 역학의 물리적 인식이 제한되어 있기 때문에, 접촉에 풍부한 로봇 스캐너에서 현실적인 사진 생성에 오류가 존재합니다. 본 논문에서는 RGB 비디오 생성과 물리 지식을 통합한 인터랙티브 프레임 워크를 통해 학습하는 물리 정보를 가지는 통합 물리 모델을 제안합니다. 두 가지 물리 정보를 가지는 연결 훈련 태스크를 도입합니다: 시간적인 깊이 예측은 비디오 렌더링에서 3D 일반화의 일치성을 높입니다, 키포인트 역학 학습은 물리적 속성(예: 물체의 모양과 재료의 특성)를 은닉적으로 인코딩하여 복잡한 동작 모델을 개선합니다. 확장된 실험은 RoboScape은 다양한 로봇 스캐너에서도 우수한 시각적 신뢰성과 물리적 가능성성을 보여주며, 생성 데이터를 이용한 로봇 정책 훈련 및 정책 평가의 다운 스트림 애플리케이션에서 실용적인 효과를 입증합니다. 우리의 연구는 물리 정보가 있는 세계 모델의 효율적인 구축에 새로운 지침을 제공하며, 몸이 된 지능 연구의 발전에 기여합니다. 코드는 다음 URL에서 제공됩니다: https://github.com/tsinghua-fib-lab/RoboScape.",
      "upvotes": 1,
      "discussionId": "68638235588cea0da970c96d",
      "ai_summary": "RoboScape is a unified physics-informed world model that enhances visual fidelity and physical plausibility in robotic video generation by integrating temporal depth prediction and keypoint dynamics learning.",
      "ai_keywords": [
        "world models",
        "embodied intelligence",
        "RGB video generation",
        "physics knowledge",
        "temporal depth prediction",
        "keypoint dynamics learning",
        "visual fidelity",
        "physical plausibility"
      ]
    },
    "publishedAt": "2025-06-29T04:19:45.000Z",
    "title": "RoboScape: Physics-informed Embodied World Model",
    "summary": "World models have become indispensable tools for embodied intelligence,\nserving as powerful simulators capable of generating realistic robotic videos\nwhile addressing critical data scarcity challenges. However, current embodied\nworld models exhibit limited physical awareness, particularly in modeling 3D\ngeometry and motion dynamics, resulting in unrealistic video generation for\ncontact-rich robotic scenarios. In this paper, we present RoboScape, a unified\nphysics-informed world model that jointly learns RGB video generation and\nphysics knowledge within an integrated framework. We introduce two key\nphysics-informed joint training tasks: temporal depth prediction that enhances\n3D geometric consistency in video rendering, and keypoint dynamics learning\nthat implicitly encodes physical properties (e.g., object shape and material\ncharacteristics) while improving complex motion modeling. Extensive experiments\ndemonstrate that RoboScape generates videos with superior visual fidelity and\nphysical plausibility across diverse robotic scenarios. We further validate its\npractical utility through downstream applications including robotic policy\ntraining with generated data and policy evaluation. Our work provides new\ninsights for building efficient physics-informed world models to advance\nembodied intelligence research. The code is available at:\nhttps://github.com/tsinghua-fib-lab/RoboScape.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/oiWNtmiOwH-sWe0ewE2YQ.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23135.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22753",
      "authors": [
        {
          "_id": "6863b259588cea0da970c9bf",
          "name": "Jianing Zhang",
          "hidden": false
        },
        {
          "_id": "6863b259588cea0da970c9c0",
          "name": "Jiayi Zhu",
          "hidden": false
        },
        {
          "_id": "6863b259588cea0da970c9c1",
          "name": "Feiyu Ji",
          "hidden": false
        },
        {
          "_id": "6863b259588cea0da970c9c2",
          "name": "Xiaokang Yang",
          "hidden": false
        },
        {
          "_id": "6863b259588cea0da970c9c3",
          "user": {
            "_id": "67761e674467879a54b4624a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hbniDcGynaZIUZTNSgc2G.jpeg",
            "isPro": false,
            "fullname": "Xiaoyun Yuan",
            "user": "XiaoyunYuan",
            "type": "user"
          },
          "name": "Xiaoyun Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:12:11.708Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T04:48:37.000Z",
      "submittedOnDailyAt": "2025-07-01T08:43:39.113Z",
      "title": "트랜스ベーション モデル付き의 다중 패스 디퓨저 모델을 활용한 튜닝블 메라니스키 포토그래피",
      "submittedOnDailyBy": {
        "_id": "67761e674467879a54b4624a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hbniDcGynaZIUZTNSgc2G.jpeg",
        "isPro": false,
        "fullname": "Xiaoyun Yuan",
        "user": "XiaoyunYuan",
        "type": "user"
      },
      "summary": "금속 퀄리티는 초소형화된 컴퓨터 이미지에 큰 가능성들을 가지고 있지만, 복잡한 광학의 감쇠와 계산적인 리스トア바이디션의 어려움을 직면하고 있습니다. 현재의 방법들은 일반적으로 정밀한 광학 조정이나 큰 페어로드 데이터 세트를 의존합니다が, 실제적인 이미지 시스템에 대해 복잡합니다. 또한, 추론 프로세스의 제어의 결함이 불행하게도 이상한 어티팩트가 발생할 수 있습니다. 우리는 훈련된 모델로부터 강력한 자연 이미지의 선두를 활용하여, 큰 데이터 세트를 대신하여 사용함으로써, Degradation-Modeled Multipath Diffusion을 도입하고, 적응적인 금속 퀄리티 이미지를 구현합니다. 우리의 프레임워크는 고주파의 세부 생성, 구조적의존성, 금속 퀄리티 고유의 감쇠 억제, 그리고 팩토리 로딩을 위한 보조로 긍정적인, 중적인, 부정적인 프로ン퓰트 패스를 사용하여 균형을 조정합니다. 적응적인 디코더는 신뢰성과 시각적 품질 사이의 제어된 트레이드오프를 가능하게 합니다. 또한, 공간적으로 변화하는 감쇠에 관심 있는 어텐션 (SVDA) 모듈은 복잡한 광学和 센서에 의한 감쇠를 적응적으로 모델링합니다. 마지막으로, 실제적인 검증을 위해, 미리미터 스케일의 MetaCamera를 설계하고 구축했습니다. 확장된 결과를 통해, 우리의 접근法是 가장 先端한 방법들을 초월하고, 고의존성과 쾌한 이미지의 재구성을 실현하였습니다. 추가적인 재료: https://dmdiff.github.io/",
      "upvotes": 1,
      "discussionId": "6863b259588cea0da970c9c4",
      "projectPage": "https://dmdiff.github.io/",
      "githubRepo": "https://github.com/yuanxy92/DMDiff_ICCV2025",
      "ai_summary": "The proposed Degradation-Modeled Multipath Diffusion framework improves metalens image quality by using natural image priors and specific modules to balance detail, fidelity, and perceptual quality while addressing optical degradation.",
      "ai_keywords": [
        "multipath diffusion",
        "natural image priors",
        "positive-prompt paths",
        "neutral-prompt paths",
        "negative-prompt paths",
        "pseudo data augmentation",
        "tunable decoder",
        "spatially varying degradation-aware attention",
        "SVDA module",
        "high-fidelity image reconstruction"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-06-28T00:48:37.000Z",
    "title": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography",
    "summary": "Metalenses offer significant potential for ultra-compact computational\nimaging but face challenges from complex optical degradation and computational\nrestoration difficulties. Existing methods typically rely on precise optical\ncalibration or massive paired datasets, which are non-trivial for real-world\nimaging systems. Furthermore, a lack of control over the inference process\noften results in undesirable hallucinated artifacts. We introduce\nDegradation-Modeled Multipath Diffusion for tunable metalens photography,\nleveraging powerful natural image priors from pretrained models instead of\nlarge datasets. Our framework uses positive, neutral, and negative-prompt paths\nto balance high-frequency detail generation, structural fidelity, and\nsuppression of metalens-specific degradation, alongside pseudo data\naugmentation. A tunable decoder enables controlled trade-offs between fidelity\nand perceptual quality. Additionally, a spatially varying degradation-aware\nattention (SVDA) module adaptively models complex optical and sensor-induced\ndegradation. Finally, we design and build a millimeter-scale MetaCamera for\nreal-world validation. Extensive results show that our approach outperforms\nstate-of-the-art methods, achieving high-fidelity and sharp image\nreconstruction. More materials: https://dmdiff.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22753.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67761e674467879a54b4624a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hbniDcGynaZIUZTNSgc2G.jpeg",
      "fullname": "Xiaoyun Yuan",
      "name": "XiaoyunYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.21448",
      "authors": [
        {
          "_id": "68636ecd588cea0da970c905",
          "name": "Huadai Liu",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c906",
          "name": "Jialei Wang",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c907",
          "name": "Kaicheng Luo",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c908",
          "name": "Wen Wang",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c909",
          "name": "Qian Chen",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c90a",
          "name": "Zhou Zhao",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c90b",
          "name": "Wei Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T16:32:06.000Z",
      "submittedOnDailyAt": "2025-07-01T03:47:20.627Z",
      "title": "ThinkSound: Chain-of-Thought Reasoning를 이용한 음성 생성 및 편집 (단어의 순서대로 논리적으로 진행되는 방식을 사용하여 음성을 생성하고 편집합니다.)",
      "submittedOnDailyBy": {
        "_id": "63d8c0d3da4f72339241c7dd",
        "avatarUrl": "/avatars/c5852fa7d2b8ffb7a76f0143faa453ef.svg",
        "isPro": false,
        "fullname": "liuhuadai",
        "user": "liuhuadai",
        "type": "user"
      },
      "summary": "ビデオから의 소리 생성은 크게 발전했습니다が, 고품질의 소리를 생성하고, 비디오 콘텐츠의 微妙한 감각을 실제적으로 이해하는 것은 어렵습니다. 디즈니 엔터테인먼트 산업의 전문가와 마찬가지로, 이 생성은 시각적 동작, 음향 환경, 시간적 관계 등 복잡한 이유를 이해해야 합니다. 우리는 Chain-of-Thought (CoT) 이유를 활용하여, 비디오의 각 단계별로 상호작용적인 소리 생성과 편집을 가능하게 하는 새로운 프레임워크 \"ThinkSound\"를 소개합니다. 우리의 접근법은 세 개의 보간 단계로 분해됩니다: 기능적인 Fourier 생성, 높은 정확도의 사용자 인터랙션을 통한 객체 씬 코нф이언, 자연어 지시에 의한 목표화 편집. 각 단계에서, 여러 모델의 큰 규모의 언어 모델이 맥락적으로 일치하는 CoT 이유를 생성하고, 통일적인 소리 기반 모델을 지도합니다. 또한, AudioCoT이라는 구조화된 이유 注釈를 가진 상세한 데이터 세트를 소개합니다. 이것은 시각적 콘텐츠, 맥락 설명, 소리 합성 사이의 연계를 확립합니다. 실험은 ThinkSound는 비디오로부터의 소리 생성에서, 소리 평가와 CoT 평가 모두 가장 先端의 성능을 달성하고, Movie Gen Audio 벤치마크에서 오프라인 분포를 초과하여 뛰어난 성능을 보였습니다. Demo 페이지는 https://ThinkSound-Project.github.io 에서 사용 가능합니다.",
      "upvotes": 1,
      "discussionId": "68636ecd588cea0da970c90c",
      "projectPage": "https://thinksound-project.github.io/",
      "githubRepo": "https://github.com/liuhuadai/ThinkSound",
      "ai_summary": "ThinkSound, a novel framework, uses Chain-of-Thought reasoning with a multimodal large language model to generate high-quality audio from videos, achieving state-of-the-art results in various benchmarks.",
      "ai_keywords": [
        "Chain-of-Thought (CoT) reasoning",
        "multimodal large language model",
        "unified audio foundation model",
        "AudioCoT",
        "video-to-audio generation",
        "foley generation",
        "object-centric refinement",
        "targeted editing",
        "Audio Metrics",
        "CoT metrics",
        "Movie Gen Audio benchmark"
      ],
      "githubStars": 23
    },
    "publishedAt": "2025-06-26T12:32:06.000Z",
    "title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language\n  Models for Audio Generation and Editing",
    "summary": "While end-to-end video-to-audio generation has greatly improved, producing\nhigh-fidelity audio that authentically captures the nuances of visual content\nremains challenging. Like professionals in the creative industries, such\ngeneration requires sophisticated reasoning about items such as visual\ndynamics, acoustic environments, and temporal relationships. We present\nThinkSound, a novel framework that leverages Chain-of-Thought (CoT) reasoning\nto enable stepwise, interactive audio generation and editing for videos. Our\napproach decomposes the process into three complementary stages: foundational\nfoley generation that creates semantically coherent soundscapes, interactive\nobject-centric refinement through precise user interactions, and targeted\nediting guided by natural language instructions. At each stage, a multimodal\nlarge language model generates contextually aligned CoT reasoning that guides a\nunified audio foundation model. Furthermore, we introduce AudioCoT, a\ncomprehensive dataset with structured reasoning annotations that establishes\nconnections between visual content, textual descriptions, and sound synthesis.\nExperiments demonstrate that ThinkSound achieves state-of-the-art performance\nin video-to-audio generation across both audio metrics and CoT metrics and\nexcels in out-of-distribution Movie Gen Audio benchmark. The demo page is\navailable at https://ThinkSound-Project.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d8c0d3da4f72339241c7dd",
      "avatarUrl": "/avatars/c5852fa7d2b8ffb7a76f0143faa453ef.svg",
      "fullname": "liuhuadai",
      "name": "liuhuadai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]