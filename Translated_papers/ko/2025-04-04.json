[
  {
    "paper": {
      "id": "2504.01990",
      "authors": [
        {
          "_id": "67ef8723d325fe100f36107e",
          "user": {
            "_id": "654a97282d2fcd6bf2851173",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png",
            "isPro": false,
            "fullname": "Bang Liu",
            "user": "Bang-UdeM-Mila",
            "type": "user"
          },
          "name": "Bang Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-04T07:15:51.456Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36107f",
          "user": {
            "_id": "65af5f8f3db2280ece7fac79",
            "avatarUrl": "/avatars/66d88b2d744c8d00e11d39a55ab86c2e.svg",
            "isPro": false,
            "fullname": "Xin-Feng Li",
            "user": "xinfeng1i",
            "type": "user"
          },
          "name": "Xinfeng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:45.785Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361080",
          "user": {
            "_id": "64b78a39954ae43365984448",
            "avatarUrl": "/avatars/6de9d4bf320a69eca6b758e718ee116c.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Peiyan",
            "type": "user"
          },
          "name": "Jiayi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:49.444Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361081",
          "user": {
            "_id": "64324bb3034ecbefddd99863",
            "avatarUrl": "/avatars/3b8cdc2066251999a3a7e6d5565dceb5.svg",
            "isPro": false,
            "fullname": "Jinlin Wang",
            "user": "JinlinW",
            "type": "user"
          },
          "name": "Jinlin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:27:02.727Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361082",
          "name": "Tanjin He",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361083",
          "name": "Sirui Hong",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361084",
          "name": "Hongzhang Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361085",
          "name": "Shaokun Zhang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361086",
          "user": {
            "_id": "5fc0b2b61160c47d1d438568",
            "avatarUrl": "/avatars/90beea6b452c662d579197dbf592423a.svg",
            "isPro": false,
            "fullname": "Kaitao Song",
            "user": "KaitaoSong",
            "type": "user"
          },
          "name": "Kaitao Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:27:47.151Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361087",
          "user": {
            "_id": "64c090a9f613170e7be93d2f",
            "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
            "isPro": false,
            "fullname": "KunlunZhu",
            "user": "KunlunZhu",
            "type": "user"
          },
          "name": "Kunlun Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:03.582Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361088",
          "user": {
            "_id": "6664783b0ab8b63cbb4a3156",
            "avatarUrl": "/avatars/71859e6f76c157191bd2e968061f08b0.svg",
            "isPro": false,
            "fullname": "cyh",
            "user": "chengyuheng",
            "type": "user"
          },
          "name": "Yuheng Cheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:14.543Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361089",
          "user": {
            "_id": "62bb1e0f3ff437e49a3088e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62bb1e0f3ff437e49a3088e5/bcUQmH8tKfI6DIWH9IcYp.jpeg",
            "isPro": true,
            "fullname": "Suyuchen Wang",
            "user": "sheryc",
            "type": "user"
          },
          "name": "Suyuchen Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:21.351Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108a",
          "user": {
            "_id": "655c092183186f133f959108",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WMVgGhjQbVJXK9eh4EuT9.jpeg",
            "isPro": false,
            "fullname": "Xiaoqiang Wang",
            "user": "qindomitable",
            "type": "user"
          },
          "name": "Xiaoqiang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:26.707Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108b",
          "name": "Yuyu Luo",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108c",
          "user": {
            "_id": "648ee5fe9ae7cc4fcffa9aef",
            "avatarUrl": "/avatars/9bcc5eb91452c1360b9a0a4f9def8af8.svg",
            "isPro": false,
            "fullname": "Haibo Jin",
            "user": "Nick233",
            "type": "user"
          },
          "name": "Haibo Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:40.177Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108d",
          "user": {
            "_id": "64b78a39954ae43365984448",
            "avatarUrl": "/avatars/6de9d4bf320a69eca6b758e718ee116c.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Peiyan",
            "type": "user"
          },
          "name": "Peiyan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:02.679Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108e",
          "user": {
            "_id": "66197a8afeb55cbe39e50ae8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pcSS1TsCoHxRcAEkcMNm0.png",
            "isPro": false,
            "fullname": "Ollie Liu",
            "user": "oliu-io",
            "type": "user"
          },
          "name": "Ollie Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:10.114Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108f",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361090",
          "user": {
            "_id": "6719d581a6cad13741b8bc7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719d581a6cad13741b8bc7f/w4EttqfXRgWZJc6HpYOS9.jpeg",
            "isPro": false,
            "fullname": "Huan Zhang",
            "user": "huanzhang12",
            "type": "user"
          },
          "name": "Huan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:24.614Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361091",
          "user": {
            "_id": "640dc84b474aa6f89554d518",
            "avatarUrl": "/avatars/64f47f76d97c5e91b7ab8380bcada61c.svg",
            "isPro": false,
            "fullname": "Zhaoyang Yu",
            "user": "MoshiQAQ",
            "type": "user"
          },
          "name": "Zhaoyang Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:38.969Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361092",
          "name": "Haochen Shi",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361093",
          "name": "Boyan Li",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361094",
          "name": "Dekun Wu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361095",
          "user": {
            "_id": "6402e8fb06c715b93407442d",
            "avatarUrl": "/avatars/12b67f0632be5a53b56d8a68586a7f98.svg",
            "isPro": false,
            "fullname": "Fengwei Teng",
            "user": "leavendough",
            "type": "user"
          },
          "name": "Fengwei Teng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:09.492Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361096",
          "name": "Xiaojun Jia",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361097",
          "name": "Jiawei Xu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361098",
          "name": "Jinyu Xiang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361099",
          "name": "Yizhang Lin",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109a",
          "name": "Tianming Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109b",
          "name": "Tongliang Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109c",
          "name": "Yu Su",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109d",
          "name": "Huan Sun",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109e",
          "user": {
            "_id": "66a8fa5fd909c30167f1f5cd",
            "avatarUrl": "/avatars/c9b26d5b2dd78bed9661df429012fd97.svg",
            "isPro": false,
            "fullname": "Glen Berseth",
            "user": "gberseth",
            "type": "user"
          },
          "name": "Glen Berseth",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:19.433Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109f",
          "name": "Jianyun Nie",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a0",
          "name": "Ian Foster",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a1",
          "name": "Logan Ward",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a2",
          "name": "Qingyun Wu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a3",
          "user": {
            "_id": "5e7e595230dc073f817a2bb5",
            "avatarUrl": "/avatars/d5ff36e45555d9e169cf56c845736444.svg",
            "isPro": false,
            "fullname": "Yu Gu",
            "user": "entslscheia",
            "type": "user"
          },
          "name": "Yu Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:50:40.603Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a4",
          "user": {
            "_id": "64403daae44f30a72323e4ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
            "isPro": false,
            "fullname": "mingchen zhuge",
            "user": "tjpxiaoming",
            "type": "user"
          },
          "name": "Mingchen Zhuge",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:26:28.011Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a5",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a6",
          "name": "Haohan Wang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a7",
          "name": "Jiaxuan You",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a8",
          "name": "Chi Wang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a9",
          "user": {
            "_id": "670918b02806bda07e44780c",
            "avatarUrl": "/avatars/c08ba5048d9e911ef488862e8869792f.svg",
            "isPro": false,
            "fullname": "Jian Pei",
            "user": "StrawHat2333",
            "type": "user"
          },
          "name": "Jian Pei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:36:21.496Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610aa",
          "name": "Qiang Yang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610ab",
          "user": {
            "_id": "6342829eb9454d65a2e7a4c4",
            "avatarUrl": "/avatars/4438abdf189dbe26a52948800d79a7c5.svg",
            "isPro": false,
            "fullname": "Xiaoliang Qi",
            "user": "phynics",
            "type": "user"
          },
          "name": "Xiaoliang Qi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:59.175Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610ac",
          "name": "Chenglin Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T18:00:29.000Z",
      "submittedOnDailyAt": "2025-04-04T05:46:58.338Z",
      "title": "기초 에이전트의 발전과 문제: 뇌 인스피레이션에서 진화하는 협력적이고 안전한 시스템으로 발전\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "64403daae44f30a72323e4ca",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
        "isPro": false,
        "fullname": "mingchen zhuge",
        "user": "tjpxiaoming",
        "type": "user"
      },
      "summary": "LLM의 등장은 인공지능 분야에서 혁신적인 변화를 촉진하고, 복잡한 논리론, 강력한 인식, 다양한 분야에서 전환적인 행동을 가능하게 하는 고수능 지능 에이전트를 개발하는 데 기회를 제공했습니다. 이러한 에이전트가 인공지능의 연구와 실용적인 적용에 더 많은 영향을 미칠 수 있도록 가정하고, 에이전트의 설계, 평가, 연속적인 개선은 복잡하고 다면적인 도전으로 변했습니다. 이 조사에서는 인지과학, 신경과학, 계산연구의 원리를 통합한 모듈화된, 인간 뇌 기능에 비슷한 아키텍처를 기반으로 지능 에이전트를 구성하고, 그 설계, 평가, 연속적인 개선의 세부적인 개요를 제공합니다. 우리의 탐구는 4개의 상호작용하는 부분으로 나눌 수 있습니다. 첫째로, 지능 에이전트의 모듈화된 기초에 대해 자세히 조사하고, 인간 뇌의 기능에 대응하는 코가치랄, 인식, 동작의 모듈을 매핑하고, 메모리, 세계 모델링, 보상 처리, 감정 시스템과 같은 핵심 요소를 밝혀냅니다. 둘째로, 자동완성과 적응적인 진화 구조에 대해 논의하고, 에이전트가 자동으로 능력을 평가하고 동적인 환경에 적응하며, 자동화 최적화 패러다임을 통해 지속적인 학습을 달성하는 것을 조사합니다. 이는 새로운 AutoML과 LLM 주도의 최적화 스테레디에 포함됩니다. 셋째로, 협력과 진화의 다 에이전트 시스템에 대해 조사하고, 에이전트의 상호작용, 협업, 사회적 구조로부터 발생하는 집단적 지능을 조사하고, 인간 사회 동태에 대한 병행 관계를 명확히 합니다. 마지막으로, 안전, 안전, 유익한 인공지능 시스템의 구축의 중요한 의무에 대해 논의하고, 내외관계의 보안 위협, 윤리적 합의, 강건성, 신뢰성 위해 실질적인 완화 전략을 강조합니다.",
      "upvotes": 55,
      "discussionId": "67ef8727d325fe100f3611aa",
      "githubRepo": "https://github.com/FoundationAgents/awesome-foundation-agents"
    },
    "publishedAt": "2025-03-31T14:00:29.000Z",
    "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems",
    "summary": "The advent of large language models (LLMs) has catalyzed a transformative\nshift in artificial intelligence, paving the way for advanced intelligent\nagents capable of sophisticated reasoning, robust perception, and versatile\naction across diverse domains. As these agents increasingly drive AI research\nand practical applications, their design, evaluation, and continuous\nimprovement present intricate, multifaceted challenges. This survey provides a\ncomprehensive overview, framing intelligent agents within a modular,\nbrain-inspired architecture that integrates principles from cognitive science,\nneuroscience, and computational research. We structure our exploration into\nfour interconnected parts. First, we delve into the modular foundation of\nintelligent agents, systematically mapping their cognitive, perceptual, and\noperational modules onto analogous human brain functionalities, and elucidating\ncore components such as memory, world modeling, reward processing, and\nemotion-like systems. Second, we discuss self-enhancement and adaptive\nevolution mechanisms, exploring how agents autonomously refine their\ncapabilities, adapt to dynamic environments, and achieve continual learning\nthrough automated optimization paradigms, including emerging AutoML and\nLLM-driven optimization strategies. Third, we examine collaborative and\nevolutionary multi-agent systems, investigating the collective intelligence\nemerging from agent interactions, cooperation, and societal structures,\nhighlighting parallels to human social dynamics. Finally, we address the\ncritical imperative of building safe, secure, and beneficial AI systems,\nemphasizing intrinsic and extrinsic security threats, ethical alignment,\nrobustness, and practical mitigation strategies necessary for trustworthy\nreal-world deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64403daae44f30a72323e4ca",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
      "fullname": "mingchen zhuge",
      "name": "tjpxiaoming",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02826",
      "authors": [
        {
          "_id": "67ef4be0985aa66b67021ddc",
          "user": {
            "_id": "6530e62f536dbca918e71c3e",
            "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
            "isPro": false,
            "fullname": "Xiangyu Z",
            "user": "PhoenixZ",
            "type": "user"
          },
          "name": "Xiangyu Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:07.389Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021ddd",
          "user": {
            "_id": "6710be3e6d1b33cf24417e38",
            "avatarUrl": "/avatars/f60bc9a67bb58f5997cbcc28cb93c079.svg",
            "isPro": false,
            "fullname": "zpy",
            "user": "zpy777",
            "type": "user"
          },
          "name": "Peiyuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:11.816Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021dde",
          "user": {
            "_id": "662516d72419feed62fb3a0a",
            "avatarUrl": "/avatars/24c4157829e70a4e346aa984885aa5ad.svg",
            "isPro": false,
            "fullname": "Dian",
            "user": "KexianTang",
            "type": "user"
          },
          "name": "Kexian Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:21:18.584Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021ddf",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de0",
          "name": "Zicheng Zhang",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de1",
          "user": {
            "_id": "65535b125413c1a54e6fb243",
            "avatarUrl": "/avatars/03bcf1d58865f5406aff49a415e78bdc.svg",
            "isPro": false,
            "fullname": "Guangtao Zhai",
            "user": "GTZhai",
            "type": "user"
          },
          "name": "Guangtao Zhai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:23.057Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de2",
          "user": {
            "_id": "667289f903c802764985d8c6",
            "avatarUrl": "/avatars/916befcbf0e52ce56be49617f31c7bb2.svg",
            "isPro": false,
            "fullname": "Junchi Yan",
            "user": "Rethinker",
            "type": "user"
          },
          "name": "Junchi Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:30.311Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de3",
          "name": "Hua Yang",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de4",
          "user": {
            "_id": "648e77184cae4f6921dbb382",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648e77184cae4f6921dbb382/zAAJRvOStC0wZplqVWrk_.jpeg",
            "isPro": false,
            "fullname": "Xue Yang",
            "user": "yangxue",
            "type": "user"
          },
          "name": "Xue Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:09.267Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de5",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:37.146Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/PQFr3_6S3BeUSNb79jMSO.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/0rGBBJT_JcUPBk5cE_Te0.png"
      ],
      "publishedAt": "2025-04-03T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-04T01:35:35.280Z",
      "title": "픽셀을 초월하는 상상: 근거에 기반한 시각적 편집의 벤치마크",
      "submittedOnDailyBy": {
        "_id": "63ee1379190ddd6214efd73a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
        "isPro": false,
        "fullname": "HAODONG DUAN",
        "user": "KennyUTC",
        "type": "user"
      },
      "summary": "대규모 다모델（LMMs）는 시각 이해와 생성 분야에서 발전하고 있지만, 일반적인 시각 편집에 있어서는 복잡한 지시를 추적하고, 외관의 일관성을 유지하고, 유연한 입력 형식을 지원하는 데에 여전히 도전이 있습니다. 이러한 영역을 폐쇄하기 위해, 우리는 평가용 벤치마크인 RISEBench를 소개합니다. RISEBench는 시간적, 원인적, 공간적, 로직적 원리를 중점적으로 강조합니다. 각 카테고리에 대해 고품질의 테스트 케이스를 제공하며, 인간 판단자와 LMM-as-a-judge 접근 방식을 사용하여 명령의 이유, 외관의 일관성, 시각적 가능성성을 평가하는 프레임워크를 제안합니다. 실험 결과에 따르면, GPT-4o-Native는 다른 오픈 소스 모델이나 소유 모델을 크게 초월하지만, 이 선진 시스템은 로직적 원리의 작업에서도 어려움을 인식하고, 이 분야의 조사가 부족한 것을 명확히 나타냅니다. 초기의 노력을 통해, RISEBench는 논리 기반의 시각 편집에 대한 기초적인 통찰을 제공하고, 향후 연구를 촉진하는 것을 목표로 합니다. 아직 초기 단계이지만, 우리는 다음 세대 다모델 시스템의 평가에 있어서, 더 엄격하고 scalable한 평가 지원을 위해 벤치마크를 지속적으로 확장하고 개선하는 데努ム을 기울입니다. 코드와 데이터는 https://github.com/PhoenixZ810/RISEBench에서 공개됩니다.",
      "upvotes": 40,
      "discussionId": "67ef4be4985aa66b67021ef2",
      "githubRepo": "https://github.com/PhoenixZ810/RISEBench",
      "ai_keywords": [
        "Large Multi-modality Models (LMMs)",
        "General Visual Editing",
        "Temporal Reasoning",
        "Causal Reasoning",
        "Spatial Reasoning",
        "Logical Reasoning",
        "RISEBench",
        "Instruction Reasoning",
        "Appearance Consistency",
        "Visual Plausibility",
        "GPT-4o-Native",
        "multimodal systems"
      ]
    },
    "publishedAt": "2025-04-03T13:59:56.000Z",
    "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
    "summary": "Large Multi-modality Models (LMMs) have made significant progress in visual\nunderstanding and generation, but they still face challenges in General Visual\nEditing, particularly in following complex instructions, preserving appearance\nconsistency, and supporting flexible input formats. To address this gap, we\nintroduce RISEBench, the first benchmark for evaluating Reasoning-Informed\nviSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal,\nCausal, Spatial, and Logical Reasoning. We curate high-quality test cases for\neach category and propose an evaluation framework that assesses Instruction\nReasoning, Appearance Consistency, and Visual Plausibility with both human\njudges and an LMM-as-a-judge approach. Our experiments reveal that while\nGPT-4o-Native significantly outperforms other open-source and proprietary\nmodels, even this state-of-the-art system struggles with logical reasoning\ntasks, highlighting an area that remains underexplored. As an initial effort,\nRISEBench aims to provide foundational insights into reasoning-aware visual\nediting and to catalyze future research. Though still in its early stages, we\nare committed to continuously expanding and refining the benchmark to support\nmore comprehensive, reliable, and scalable evaluations of next-generation\nmultimodal systems. Our code and data will be released at\nhttps://github.com/PhoenixZ810/RISEBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/PQFr3_6S3BeUSNb79jMSO.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/0rGBBJT_JcUPBk5cE_Te0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02826.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ee1379190ddd6214efd73a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
      "fullname": "HAODONG DUAN",
      "name": "KennyUTC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02782",
      "authors": [
        {
          "_id": "67ef502ce803d818f00e1b94",
          "name": "Zhiyuan Yan",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b95",
          "user": {
            "_id": "66978ee0b8656f6506b4acb2",
            "avatarUrl": "/avatars/298acb8222e189fce4368985ee5374a1.svg",
            "isPro": false,
            "fullname": "Junyan Ye",
            "user": "Yejy53",
            "type": "user"
          },
          "name": "Junyan Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:10.032Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b96",
          "user": {
            "_id": "66d5b56c77a026c3d2086a79",
            "avatarUrl": "/avatars/45da07fd82fd455955faa05b27a6393f.svg",
            "isPro": false,
            "fullname": "Weijia Li",
            "user": "liweijia",
            "type": "user"
          },
          "name": "Weijia Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:16:44.819Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b97",
          "user": {
            "_id": "6487e158f675b4a7867f45fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg",
            "isPro": false,
            "fullname": "Zilong Huang",
            "user": "SereinH",
            "type": "user"
          },
          "name": "Zilong Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:56.501Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b98",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:05.246Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b99",
          "user": {
            "_id": "67ef53656c7ba428e7c2e605",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/fFVJTpMRKF15Bf63yZEG_.png",
            "isPro": false,
            "fullname": "He",
            "user": "shawnxyh",
            "type": "user"
          },
          "name": "Xiangyang He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:02.573Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9a",
          "user": {
            "_id": "6459a47e4fe72fae522b4fc9",
            "avatarUrl": "/avatars/a4139f8e348081e45b28dd99d96908d3.svg",
            "isPro": false,
            "fullname": "Kaiqing.Lin",
            "user": "lin6123",
            "type": "user"
          },
          "name": "Kaiqing Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:16:28.452Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9b",
          "user": {
            "_id": "670ddb69d6ac6394419d88c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XxnGNaX3FWug4aiZVjg93.png",
            "isPro": false,
            "fullname": "Jun He",
            "user": "JunHe0915",
            "type": "user"
          },
          "name": "Jun He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:59.877Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9c",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:18.123Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9d",
          "user": {
            "_id": "66135a5e50350afe76beebce",
            "avatarUrl": "/avatars/370a4b83949355feb050c2cb0425c264.svg",
            "isPro": false,
            "fullname": "yl2488",
            "user": "yl2488",
            "type": "user"
          },
          "name": "Li Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:40.281Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:23:16.000Z",
      "submittedOnDailyAt": "2025-04-04T01:51:34.697Z",
      "title": "GPT-ImgEval: GPT4o의 이미지 생성에 대한 진단용 세부적인 벤치마크",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "최근 OpenAI의 GPT4o 모델의 발전은 이미지 생성과 편집에 있어서 매우 뛰어난 능력을 보여주고 있으며, 커뮤니티에 매우 흥미로운 결과를 얻습니다. 이 기술보고서에서는 GPT-ImgEval이라는 이름으로 첫 번째 전망 평가 벤치마크를 사용하여, 이미지 생성, 편집, 세계지식에 기반한 의미 합성의 3가지 중요한 측면에서 GPT-4o의 성능을 정량적이고 정성적으로 평가합니다. 3가지의 태스크에서 GPT-4o는 강력한 성능을 보여주고, 이미지 생성의 제어와 출력의 품질에 있어서 기존의 방법보다 크게 뛰어넘으며, 특히 지식 추론 능력을 보여주고 있습니다. 또한 GPT-4o가 생성한 데이터에 기반하여 모델의 잠재적 아키텍처를 조사하기 위해 클래스 분류 모델 기반의 접근법을 제안하고, 실험 결과를 통해 모델은 자동 회귀(AR)와 분기 기반의 헤드를 조합한 구조로 있음을 보여줍니다. 또한 GPT-4o의 이미지 생성에서 보이는 구체적인 한계와 합성된 피드백을 식별하고 시각화했습니다. 또한 GPT-4o와 Gemini 2.0 Flash의 반복적인 이미지 편집을 비교 연구를 수행하고, GPT-4o의 출력의 안전성에 대한 영향을 논의했습니다. GPT-4o의 평가에 사용된 코드와 데이터셋은 https://github.com/PicoTrex/GPT-ImgEval에서 얻을 수 있습니다.",
      "upvotes": 28,
      "discussionId": "67ef502fe803d818f00e1c70",
      "githubRepo": "https://github.com/PicoTrex/GPT-ImgEval",
      "ai_keywords": [
        "auto-regressive (AR)",
        "diffusion-based head",
        "VAR-like architectures",
        "multi-round image editing",
        "image forensic models"
      ]
    },
    "publishedAt": "2025-04-03T13:23:16.000Z",
    "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation",
    "summary": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated\nsurprisingly good capabilities in image generation and editing, resulting in\nsignificant excitement in the community. This technical report presents the\nfirst-look evaluation benchmark (named GPT-ImgEval), quantitatively and\nqualitatively diagnosing GPT-4o's performance across three critical dimensions:\n(1) generation quality, (2) editing proficiency, and (3) world\nknowledge-informed semantic synthesis. Across all three tasks, GPT-4o\ndemonstrates strong performance, significantly surpassing existing methods in\nboth image generation control and output quality, while also showcasing\nexceptional knowledge reasoning capabilities. Furthermore, based on the\nGPT-4o's generated data, we propose a classification-model-based approach to\ninvestigate the underlying architecture of GPT-4o, where our empirical results\nsuggest the model consists of an auto-regressive (AR) combined with a\ndiffusion-based head for image decoding, rather than the VAR-like\narchitectures. We also provide a complete speculation on GPT-4o's overall\narchitecture. In addition, we conduct a series of analyses to identify and\nvisualize GPT-4o's specific limitations and the synthetic artifacts commonly\nobserved in its image generation. We also present a comparative study of\nmulti-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the\nsafety implications of GPT-4o's outputs, particularly their detectability by\nexisting image forensic models. We hope that our work can offer valuable\ninsight and provide a reliable benchmark to guide future research, foster\nreproducibility, and accelerate innovation in the field of image generation and\nbeyond. The codes and datasets used for evaluating GPT-4o can be found at\nhttps://github.com/PicoTrex/GPT-ImgEval.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02587",
      "authors": [
        {
          "_id": "67ef3f9804be7fba0c882738",
          "user": {
            "_id": "633fc70529b5a95f6e15a6b7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
            "isPro": false,
            "fullname": "Yan Ma",
            "user": "ManTle",
            "type": "user"
          },
          "name": "Yan Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:53.820Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c882739",
          "user": {
            "_id": "64b370fe6d953e7c75ede314",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b370fe6d953e7c75ede314/RdP2q3hGXWE4E2zfSv0KU.png",
            "isPro": false,
            "fullname": "Steffi Chern",
            "user": "steffichern",
            "type": "user"
          },
          "name": "Steffi Chern",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:14.660Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273a",
          "user": {
            "_id": "642e4d4d6748dd4f8eeb7732",
            "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
            "isPro": false,
            "fullname": "Xuyang Shen",
            "user": "Ryan1122",
            "type": "user"
          },
          "name": "Xuyang Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:40.397Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273b",
          "user": {
            "_id": "64c525e4d68946edad6c7067",
            "avatarUrl": "/avatars/1b108661634af602717a4ab4b66a151f.svg",
            "isPro": false,
            "fullname": "Yiran Zhong",
            "user": "IanZhong",
            "type": "user"
          },
          "name": "Yiran Zhong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:16.707Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273c",
          "user": {
            "_id": "6144a0c4ff1146bbd84d9865",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661715958139-6144a0c4ff1146bbd84d9865.png",
            "isPro": false,
            "fullname": "Pengfei Liu",
            "user": "Pengfei",
            "type": "user"
          },
          "name": "Pengfei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:34.472Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T13:53:28.000Z",
      "submittedOnDailyAt": "2025-04-04T00:42:23.044Z",
      "title": "다시 생각해보기 RL 스케일링을 시각 언어 모델에 적용하는 방법: 투명한, 시작부터 시작하는 프레임워크와 세부적인 평가 스크리멤서",
      "submittedOnDailyBy": {
        "_id": "633fc70529b5a95f6e15a6b7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
        "isPro": false,
        "fullname": "Yan Ma",
        "user": "ManTle",
        "type": "user"
      },
      "summary": "강화 학습(RL)은 최근 대규모 언어 모델의 성능을 향상시킬 강력한 가능성으로 주목받고 있으며, 이를 시각 언어 모델(VLMs)에 활용하는 방향으로 발전 중입니다. 그러나 현재의 VLMs에서 RL의 적용은 재현성과 접근성을 저해하는 복잡한 엔지니어링 프레임워크를 주로 사용하고 있으며, 표준화된 평가 프로토콜이 부족하여 결과의 비교와 학습 다이나믹스의 해석이 어려워졌습니다. 본 논문에서는 VLMs에서의 RL의 간단하고 기능적인 4단계 파이프라인을 제공하고, 다양한 모델과 데이터셋에서 검증된 투명한 프레임워크를 통해 평가 프로토콜을 표준화하고, 학습 다이나믹스와 반성적 행동을 평가하기 위한 표준화된 평가 기법을 제안합니다. 시각적 이유 태스크에 대한 분산된 실험에서 다음과 같은 주요한 실험 발견이 밝혀졌습니다: 답변의 길이는 랜덤 시드에 민감하며, 반성은 출력의 길이와 상관관계를 가지고 있으며, RL은 고품질의 데이터에서도 일반화 성능에 있어서 SFT보다 일관된 성능을 보입니다. 이러한 발견과 제안된 프레임워크는 재현 가능한 기본라인을 확립하고, RL 기반의 VLM 연구의 광범위한 협력을 지원하는 것을 목표로 합니다.",
      "upvotes": 19,
      "discussionId": "67ef3f9904be7fba0c882772",
      "ai_keywords": [
        "reinforcement learning",
        "reasoning capabilities",
        "large language models",
        "vision-language models",
        "reproducibility",
        "accessibility",
        "standardized evaluation protocols",
        "transparent framework",
        "four-step pipeline",
        "training dynamics",
        "reflective behaviors",
        "visual reasoning tasks",
        "response length",
        "reflection",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-04-03T09:53:28.000Z",
    "title": "Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme",
    "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02587.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "633fc70529b5a95f6e15a6b7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
      "fullname": "Yan Ma",
      "name": "ManTle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02436",
      "authors": [
        {
          "_id": "67ef3dfae8b932ae7a832950",
          "user": {
            "_id": "617ba1820e4237bd1731b867",
            "avatarUrl": "/avatars/f9de06363e64bddd7dc977e96e85df8a.svg",
            "isPro": false,
            "fullname": "zhengcong fei",
            "user": "onion",
            "type": "user"
          },
          "name": "Zhengcong Fei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:16.548Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832951",
          "user": {
            "_id": "65dc3a850af7e21ba40e939f",
            "avatarUrl": "/avatars/e129c64617675edd05d4317d39604318.svg",
            "isPro": false,
            "fullname": "Li",
            "user": "Debang",
            "type": "user"
          },
          "name": "Debang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:27.042Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832952",
          "user": {
            "_id": "65bef422fdb8d33cefeaccc3",
            "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
            "isPro": false,
            "fullname": "Qiu Di",
            "user": "diqiu7",
            "type": "user"
          },
          "name": "Di Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:41.458Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832953",
          "name": "Jiahua Wang",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832954",
          "name": "Yikun Dou",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832955",
          "user": {
            "_id": "62e0f1314db2175cd270ad08",
            "avatarUrl": "/avatars/1d3d6af6c63557f4abf0484e028fa942.svg",
            "isPro": false,
            "fullname": "Rui Wang",
            "user": "ruiwang",
            "type": "user"
          },
          "name": "Rui Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:11.206Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832956",
          "user": {
            "_id": "666a674967c686801acf25bb",
            "avatarUrl": "/avatars/c1f3edd63fd378dfb555e6413a966932.svg",
            "isPro": false,
            "fullname": "jingtao xu",
            "user": "raul678",
            "type": "user"
          },
          "name": "Jingtao Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:20.880Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832957",
          "user": {
            "_id": "634672bfb7b4e71c7f45360f",
            "avatarUrl": "/avatars/4b646fc3e271be90b9ec619d42ce3e99.svg",
            "isPro": false,
            "fullname": "Fan Mingyuan",
            "user": "MichaelFan",
            "type": "user"
          },
          "name": "Mingyuan Fan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:32.597Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832958",
          "name": "Guibin Chen",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832959",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a83295a",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T09:50:50.000Z",
      "submittedOnDailyAt": "2025-04-04T00:33:57.000Z",
      "title": "SkyReels-A2: 영화 디퓨전 트랜스포머로 모든 것을 만들 수 있습니다.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "이 논문에서는 SkyReels-A2라는 제어 가능한 비디오 생성 프레임워크를 소개합니다. 이 프레임워크는 문자열 프로ン퓰트에 기반하여 임의의 시각 요소(예: 인물, 물체, 배경)를 합성 비디오에 조합할 수 있으며, 각 요소에 대해 참조 이미지와의 엄격한 일치성을 유지합니다. 이 작업은 요소에서 비디오(E2V)라고 부르며, 주요 문제점은 참조 요소의 진실성 유지, 사진의 연속성 보장, 자연스러운 출력 달성에 있습니다. 이러한 문제에 대해 먼저, 모델 훈련에 사용되는 프로ン퓰트-참조 이미지-비디오의 튜플을 구축하기 위해 상세한 데이터 파이프라인을 설계합니다. 다음으로, 이미지-문자의 공통 매핑 모델을 제안하고, 생성 프로세스에 다요소 표현을 주입하여 요소의 특정적 일치성과 글로벌의 연속성 및 맥락의 대응을 균형을 맞추기 위해 노력합니다. 또한, 추론 파이프라인을 속도와 출력의 안정성으로 최적화합니다. 또한, 시스템적인 평가의 목적에 A2 Bench라는 평가된 벤치마크를 소개합니다. 실험은 우리의 프레임워크가 다양한 품질의 비디오를 생성하고 요소의 정밀한 제어를 가능하게 하는 것을 보여주고 있습니다. SkyReels-A2는 E2V의 생성에서 처음으로 오픈 소스의 상업 수준 모델이며, 발전된 폐쇄 소스 상업 모델과 비교하여도 잘 평가되어 있습니다. 우리는 SkyReels-A2가 드라마나 가상 상거래 등 창의적인 애플리케이션에서 비디오 생성의 제어 가능성의 경계를 초월하는 것을 기대하고 있습니다.",
      "upvotes": 17,
      "discussionId": "67ef3dfee8b932ae7a832a97",
      "ai_keywords": [
        "elements-to-video (E2V)",
        "image-text joint embedding model",
        "prompt-reference-video triplets",
        "generative process",
        "multi-element representations",
        "strict consistency",
        "coherent composition",
        "natural outputs",
        "output stability",
        "A2 Bench (benchmark)",
        "high-quality videos",
        "precise element control",
        "open-source commercial grade model"
      ]
    },
    "publishedAt": "2025-04-03T05:50:50.000Z",
    "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
    "summary": "This paper presents SkyReels-A2, a controllable video generation framework\ncapable of assembling arbitrary visual elements (e.g., characters, objects,\nbackgrounds) into synthesized videos based on textual prompts while maintaining\nstrict consistency with reference images for each element. We term this task\nelements-to-video (E2V), whose primary challenges lie in preserving the\nfidelity of each reference element, ensuring coherent composition of the scene,\nand achieving natural outputs. To address these, we first design a\ncomprehensive data pipeline to construct prompt-reference-video triplets for\nmodel training. Next, we propose a novel image-text joint embedding model to\ninject multi-element representations into the generative process, balancing\nelement-specific consistency with global coherence and text alignment. We also\noptimize the inference pipeline for both speed and output stability. Moreover,\nwe introduce a carefully curated benchmark for systematic evaluation, i.e, A2\nBench. Experiments demonstrate that our framework can generate diverse,\nhigh-quality videos with precise element control. SkyReels-A2 is the first\nopen-source commercial grade model for the generation of E2V, performing\nfavorably against advanced closed-source commercial models. We anticipate\nSkyReels-A2 will advance creative applications such as drama and virtual\ne-commerce, pushing the boundaries of controllable video generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02436.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6573
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00502",
      "authors": [
        {
          "_id": "67ef72898667ee5c99026d16",
          "user": {
            "_id": "67014d33126f9ab39fc52481",
            "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
            "isPro": false,
            "fullname": "Qianhao Yuan",
            "user": "yuanqianhao",
            "type": "user"
          },
          "name": "Qianhao Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:23:28.149Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d17",
          "name": "Qingyu Zhang",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d18",
          "name": "Yanjiang Liu",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d19",
          "user": {
            "_id": "654c7fbe6b51714c2a6ff590",
            "avatarUrl": "/avatars/db217415c56730872b9a807f3afb4e5b.svg",
            "isPro": false,
            "fullname": "Jiawei Chen",
            "user": "chenjiawei-icip",
            "type": "user"
          },
          "name": "Jiawei Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:04.134Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1a",
          "user": {
            "_id": "6216496a9b34d2fb49144599",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg",
            "isPro": false,
            "fullname": "Yaojie Lu",
            "user": "luyaojie",
            "type": "user"
          },
          "name": "Yaojie Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:09.511Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1b",
          "user": {
            "_id": "6711c702f858a456b4b9f3a4",
            "avatarUrl": "/avatars/178e9567c3111ab22717c3c0dd003a6a.svg",
            "isPro": false,
            "fullname": "Hongyu  Lin",
            "user": "sanmusunrise",
            "type": "user"
          },
          "name": "Hongyu Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:15.188Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1c",
          "name": "Jia Zheng",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1d",
          "user": {
            "_id": "65e99a77e71555ed193609cf",
            "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
            "isPro": false,
            "fullname": "Xianpei Han",
            "user": "xphan",
            "type": "user"
          },
          "name": "Xianpei Han",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:23.046Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1e",
          "name": "Le Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T07:47:55.000Z",
      "submittedOnDailyAt": "2025-04-04T04:19:46.946Z",
      "title": "無효한 레이어에서 이미지 토큰의 froze로 효율적인 다 모델 대 언어 모델",
      "submittedOnDailyBy": {
        "_id": "67014d33126f9ab39fc52481",
        "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
        "isPro": false,
        "fullname": "Qianhao Yuan",
        "user": "yuanqianhao",
        "type": "user"
      },
      "summary": "다모달 대언어 모델(MLLMs)은 큰 규모와 큰 시각 토큰으로 인해 높은 계산 비용이 발생합니다. 본 논문에서는, 각 층의 무효성을 조사하기 위해, 층의 변환이 시각 토큰과 맥락 토큰에 미치는 영향을 정량화하기 위해 새로운 메트릭인 '층의 기여도(LC)'를 도입하였다. LC의 계산은 지정된 토큰에 대한 층의 변환을 제거한 것과 모델 출력의 분산을 측정하여 이루어진다. 피로 실험에서, MLLM의 많은 층이 시각 토큰 처리 중 최소한의 기여를 나타내는 것을 명확히 확인하였다. 이러한 발견에 기반하여, LC를 사용하여 무효한 층을 특정하고, 그 층에서의 시각 토큰의 업데이트를 중지시키는 무학습 방법인 'ShortV'를 제안하였다. 실험 결과를 통해, ShortV는 MLLM의 약 60%의 층에서 시각 토큰의 업데이트를 중지하고, 시각 토큰의 업데이트에 관련된 계산 비용이 크게 감소할 수 있음을 확인하였다. 예를 들어, LLaVA-NeXT-13B에서, 성능을 유지하면서 FLOPs를 50% 줄일 수 있음을 확인할 수 있다. 코드는 https://github.com/icip-cas/ShortV에서 공개되어 있습니다.",
      "upvotes": 11,
      "discussionId": "67ef728a8667ee5c99026d69",
      "githubRepo": "https://github.com/icip-cas/ShortV",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Layer Contribution (LC)",
        "visual tokens",
        "transformations",
        "layer-wise redundancy",
        "model output",
        "divergence",
        "ineffective layers",
        "training-free method",
        "visual token updates",
        "computational costs",
        "FLOPs",
        "LLaVA-NeXT-13B"
      ]
    },
    "publishedAt": "2025-04-01T03:47:55.000Z",
    "title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual\n  Tokens in Ineffective Layers",
    "summary": "Multimodal Large Language Models (MLLMs) suffer from high computational costs\ndue to their massive size and the large number of visual tokens. In this paper,\nwe investigate layer-wise redundancy in MLLMs by introducing a novel metric,\nLayer Contribution (LC), which quantifies the impact of a layer's\ntransformations on visual and text tokens, respectively. The calculation of LC\ninvolves measuring the divergence in model output that results from removing\nthe layer's transformations on the specified tokens. Our pilot experiment\nreveals that many layers of MLLMs exhibit minimal contribution during the\nprocessing of visual tokens. Motivated by this observation, we propose ShortV,\na training-free method that leverages LC to identify ineffective layers, and\nfreezes visual token updates in these layers. Experiments show that ShortV can\nfreeze visual token in approximately 60\\% of the MLLM layers, thereby\ndramatically reducing computational costs related to updating visual tokens.\nFor example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while\nmaintaining superior performance. The code will be publicly available at\nhttps://github.com/icip-cas/ShortV",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67014d33126f9ab39fc52481",
      "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
      "fullname": "Qianhao Yuan",
      "name": "yuanqianhao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02542",
      "authors": [
        {
          "_id": "67ef3773ac0c701df7fd98aa",
          "user": {
            "_id": "6264a7dfc39850dc093eb68a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650763566575-noauth.png",
            "isPro": false,
            "fullname": "Fa-Ting Hong",
            "user": "HarlanHong",
            "type": "user"
          },
          "name": "Fa-Ting Hong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:38.641Z",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ab",
          "user": {
            "_id": "6481523b3fb124fc9850afed",
            "avatarUrl": "/avatars/ddde178c88713662800aafd2343647a4.svg",
            "isPro": false,
            "fullname": "Zunnan Xu",
            "user": "xuzn",
            "type": "user"
          },
          "name": "Zunnan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:23.733Z",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ac",
          "name": "Zixiang Zhou",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ad",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ae",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98af",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98b0",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98b1",
          "user": {
            "_id": "66feab48651e00e22f33222e",
            "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
            "isPro": false,
            "fullname": "Dan Xu",
            "user": "danxuhk",
            "type": "user"
          },
          "name": "Dan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:20.987Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/VOMaQBXsDjs1295R5NVOh.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/tksGfiG1zEDZ4QUg4IorF.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/aUp8tmNM2pdPYCa3A3Noh.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/MpCNSPEP6OSjZR_0m70PP.mp4"
      ],
      "publishedAt": "2025-04-03T12:44:41.000Z",
      "submittedOnDailyAt": "2025-04-04T01:37:45.934Z",
      "title": "음성 비디오 제어 바이アス 모델링 마스크된 선택적 상태 스테이지 공간 모델링 자연스러운 토큰 헤드 생성",
      "submittedOnDailyBy": {
        "_id": "66feab48651e00e22f33222e",
        "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
        "isPro": false,
        "fullname": "Dan Xu",
        "user": "danxuhk",
        "type": "user"
      },
      "summary": "Talking head synthesis는 뷰터뷰와 인간・컴퓨터 상호작용에 있어서 중요하지만, 현재의 방법들은 일반적으로 한 가지 주요 모델으로부터의 제어를 받기에 제한되어 있으며, 실용적인 역할을 수행하기가 어려워진다. 이러한 점에 대해 우리는 ACTalker를 소개합니다. ACTalker는 다 신호 제어와 단일 신호 제어를 모두 지원하는 视频生成 프레임워크입니다. 다 신호 제어를 위해, 병렬적인 mamba 구조를 설계하고, 각 분기로는 다른 구동 신호를 사용하여 특정 얼굴 영역을 제어할 수 있도록 합니다. 모든 분기에서 게이트 구조를 적용하여, 视频 생성에 대한 유연한 제어를 제공합니다. 시간적 및 공간적 자연스러운 연계를 보장하기 위해, mamba 구조를 사용하며, 각 분기에서 구동 신호는 두 차원에서 특징 토큰을 조작할 수 있습니다. 또한, 마스크 드롭 전략을 도입하여, 각 구동 신호가 mamba 구조 내의 대응하는 얼굴 영역을 독립적으로 제어할 수 있도록 하여, 제어의 충돌을 방지합니다. 실험 결과를 통해, 우리의 방법은 다양한 신호로 구동되는 자연스러운 얼굴의 视频를 생성하고, mamba 레이어는 충돌 없이 다수의 구동 모델을 연속적으로 통합합니다.",
      "upvotes": 8,
      "discussionId": "67ef3775ac0c701df7fd994c",
      "projectPage": "https://harlanhong.github.io/publications/actalker/index.html",
      "githubRepo": "https://github.com/harlanhong/ACTalker",
      "ai_keywords": [
        "ACTalker",
        "video diffusion framework",
        "multi-signals control",
        "parallel mamba structure",
        "driving signals",
        "gate mechanism",
        "temporal coordination",
        "spatial coordination",
        "feature tokens",
        "mask-drop strategy",
        "facial videos",
        "multiple driving modalities"
      ]
    },
    "publishedAt": "2025-04-03T08:44:41.000Z",
    "title": "Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation",
    "summary": "Talking head synthesis is vital for virtual avatars and human-computer\ninteraction. However, most existing methods are typically limited to accepting\ncontrol from a single primary modality, restricting their practical utility. To\nthis end, we introduce ACTalker, an end-to-end video diffusion\nframework that supports both multi-signals control and single-signal control\nfor talking head video generation. For multiple control, we design a parallel\nmamba structure with multiple branches, each utilizing a separate driving\nsignal to control specific facial regions. A gate mechanism is applied across\nall branches, providing flexible control over video generation. To ensure\nnatural coordination of the controlled video both temporally and spatially, we\nemploy the mamba structure, which enables driving signals to manipulate feature\ntokens across both dimensions in each branch. Additionally, we introduce a\nmask-drop strategy that allows each driving signal to independently control its\ncorresponding facial region within the mamba structure, preventing control\nconflicts. Experimental results demonstrate that our method produces\nnatural-looking facial videos driven by diverse signals and that the mamba\nlayer seamlessly integrates multiple driving modalities without conflict.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/VOMaQBXsDjs1295R5NVOh.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/tksGfiG1zEDZ4QUg4IorF.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/aUp8tmNM2pdPYCa3A3Noh.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/MpCNSPEP6OSjZR_0m70PP.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02542.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66feab48651e00e22f33222e",
      "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
      "fullname": "Dan Xu",
      "name": "danxuhk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02507",
      "authors": [
        {
          "_id": "67ef5a3d4417508df8d99dad",
          "user": {
            "_id": "62cd4b03c5cc157be82f0b56",
            "avatarUrl": "/avatars/351e963c1c763d507ae78cbcd62966a3.svg",
            "isPro": false,
            "fullname": "Abhay kumar",
            "user": "akanyaani",
            "type": "user"
          },
          "name": "Abhay Kumar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:18.512Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99dae",
          "user": {
            "_id": "6071c4b270e11b30cfcfd7a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
            "isPro": false,
            "fullname": "Louis Owen",
            "user": "louisowen6",
            "type": "user"
          },
          "name": "Louis Owen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:21.051Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99daf",
          "user": {
            "_id": "645a0d3dd6648853107c5fdc",
            "avatarUrl": "/avatars/1e3b6a4f5ce81a707ba7cbdf81631091.svg",
            "isPro": false,
            "fullname": "Nilabhra Roy Chowdhury",
            "user": "nilabhra",
            "type": "user"
          },
          "name": "Nilabhra Roy Chowdhury",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:12.764Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99db0",
          "user": {
            "_id": "65e4be59e8b017ee1310a1b6",
            "avatarUrl": "/avatars/c3f7cdf5d0859cb80bfb2b970a675dfa.svg",
            "isPro": false,
            "fullname": "Fabian",
            "user": "gueraf",
            "type": "user"
          },
          "name": "Fabian Güra",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:16.132Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T11:41:55.000Z",
      "submittedOnDailyAt": "2025-04-04T02:34:36.631Z",
      "title": "ZClip: 모델의 적응적인 스패크 완화에 의한 LLM의 사전 학습",
      "submittedOnDailyBy": {
        "_id": "6071c4b270e11b30cfcfd7a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
        "isPro": false,
        "fullname": "Louis Owen",
        "user": "louisowen6",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）의 훈련에 있어서는, 勾配不穩定과 損失のスパイクなどの複数の課題이 존재합니다. これらの現象は, 破壊的なデジバンジュを引き起こし, 費用の高いチェックポイントの復元やデータバッチのスキップが必要となります。常に固定の閾値やヒューリスティックに基づく勾配クリッピング手法は, これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾値やヒューリスティックに基づく勾配クリッピング手法は、これらの問題を効果的に解決することができません。これらの手法は、固定の閾",
      "upvotes": 7,
      "discussionId": "67ef5a3e4417508df8d99dfc",
      "githubRepo": "https://github.com/bluorion-com/ZClip/",
      "ai_keywords": [
        "large language models (LLMs)",
        "gradient instability",
        "loss spikes",
        "catastrophic divergence",
        "checkpoint restoration",
        "data batch skipping",
        "traditional gradient clipping techniques",
        "norm-based methods",
        "adaptive gradient clipping",
        "clipping threshold",
        "statistical properties of gradient norms",
        "z-score-based anomaly detection",
        "malignant loss spikes",
        "convergence"
      ]
    },
    "publishedAt": "2025-04-03T07:41:55.000Z",
    "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
    "summary": "Training large language models (LLMs) presents numerous challenges, including\ngradient instability and loss spikes. These phenomena can lead to catastrophic\ndivergence, requiring costly checkpoint restoration and data batch skipping.\nTraditional gradient clipping techniques, such as constant or norm-based\nmethods, fail to address these issues effectively due to their reliance on\nfixed thresholds or heuristics, leading to inefficient learning and requiring\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\ngradient clipping algorithm that dynamically adjusts the clipping threshold\nbased on statistical properties of gradient norms over time. Unlike prior\nreactive strategies, ZClip proactively adapts to training dynamics without\nmaking any prior assumptions on the scale and the temporal evolution of\ngradient norms. At its core, it leverages z-score-based anomaly detection to\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\nwhile not interfering with convergence otherwise. Our code is available at:\nhttps://github.com/bluorion-com/ZClip.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6071c4b270e11b30cfcfd7a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
      "fullname": "Louis Owen",
      "name": "louisowen6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02398",
      "authors": [
        {
          "_id": "67ef63b5e8b932ae7a8d3043",
          "user": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "isPro": false,
            "fullname": "Gallil Maimon",
            "user": "gallilmaimon",
            "type": "user"
          },
          "name": "Gallil Maimon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:06.509Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3044",
          "user": {
            "_id": "6547411a9295970f878aa52e",
            "avatarUrl": "/avatars/6e240f0add27bf1a6c04a9618eccdf83.svg",
            "isPro": false,
            "fullname": "Michael Hassid",
            "user": "hassid",
            "type": "user"
          },
          "name": "Michael Hassid",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:39.934Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3045",
          "user": {
            "_id": "64b7b7b38ba7d6c922d753d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7b7b38ba7d6c922d753d6/rt0thjYa84VZHy1BEcW4p.jpeg",
            "isPro": false,
            "fullname": "Amit Roth",
            "user": "MajoRoth",
            "type": "user"
          },
          "name": "Amit Roth",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:48.483Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3046",
          "user": {
            "_id": "6481e135578646b5c2386728",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6481e135578646b5c2386728/SPva4iNw0pORiCXD45cx9.jpeg",
            "isPro": false,
            "fullname": "Yossi Adi",
            "user": "adiyoss",
            "type": "user"
          },
          "name": "Yossi Adi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:54.851Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/wqUq-bT-DvKoNybPX46uL.png"
      ],
      "publishedAt": "2025-04-03T08:46:56.000Z",
      "submittedOnDailyAt": "2025-04-04T03:52:15.607Z",
      "title": "인트라라이브 드 스ピーチ - 텍스트언어 모델의 스케일링 분석",
      "submittedOnDailyBy": {
        "_id": "66b9bc2dacdbc1d0b39c3b50",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
        "isPro": false,
        "fullname": "Gallil Maimon",
        "user": "gallilmaimon",
        "type": "user"
      },
      "summary": "현재의 언어 모델(SLM) 스케일링 분석은 어두운 플레이어를 묘사하고 있습니다. 그들은 SLM이 문서에 비해 큰 계산량과 데이터가 필요할 것으로 예측하고, 이로 인해 고품질의 SLM의 훈련 가능성에 대한 의문이 들게 되는 사람들을 불러일으키고 있습니다. 그러나 현대의 SLM은 일반적으로 언어와 문자 사이에 교차하는 방법을 통해 사전 학습된 TextLM로부터 초기화되고, 지식의 전달을 허용하도록 설계되어 있습니다. 이는 간극 없는 SLM이 무문자 SLM보다 효율적으로 스케일링 할 수 있는지의 문제를 제기하고 있습니다. 이 논문에서는 이 문제를 엄격히 해결하고, 결과를 긍정적으로 보여줍니다. 우리는 간극 없는 SLM의 스케일링 분석을 수행하고 수십 개의 모델을 훈련하며, 스케일링의 경향을 분석하고 있습니다. 이 설정에서, SLM은 계산량에 의해 더 효율적으로 스케일링 할 수 있다는 것을 알게 되었습니다. 또한, 우리의 결과를 통해 스케일링의 동적이 무문자 SLM과 크게 다르다는 것을 보여주고, 모델 크기의 확장에 대한 계산량 관리를 크게 증가해야 하는 것을 나타내며 있습니다. 또한, 합성 데이터와 TextLM 모델의 가족의 역할도 연구하고, 이 잠재력을 방출하려는 시도를 하고 있습니다. 결과는 우리의 스케일링 모델은 다른 접근보다 계산량과 데이터를 최소한으로 사용하면서, 언어 의미 평가에서 리딩 모델과 같은 성능을 달성하는 것을 보여주고 있습니다. 모델, 샘플, 데이터는 공개되어 있습니다 - https://pages.cs.huji.ac.il/adiyoss-lab/sims.",
      "upvotes": 7,
      "discussionId": "67ef63b6e8b932ae7a8d306d",
      "projectPage": "https://pages.cs.huji.ac.il/adiyoss-lab/sims/",
      "githubRepo": "https://github.com/slp-rl/slamkit",
      "ai_keywords": [
        "Speech Language Model (SLM)",
        "TextLMs",
        "speech-text interleaving",
        "scaling analysis",
        "compute",
        "knowledge transfer",
        "textless-SLMs",
        "scaling trends",
        "scaling-dynamics",
        "training tokens",
        "synthetic data",
        "model families",
        "speech semantic metrics"
      ]
    },
    "publishedAt": "2025-04-03T04:46:56.000Z",
    "title": "Scaling Analysis of Interleaved Speech-Text Language Models",
    "summary": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture.\nThey predict that SLMs require much more compute and data compared to text,\nleading some to question the feasibility of training high-quality SLMs.\nHowever, modern SLMs are often initialised from pre-trained TextLMs using\nspeech-text interleaving to allow knowledge transfer. This raises the question\n- Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper\nwe answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by\ntraining several dozen and analysing the scaling trends. We see that under this\nsetup SLMs scale more efficiently with compute. Additionally, our results\nindicate that the scaling-dynamics are significantly different than\ntextless-SLMs, suggesting one should allocate notably more of the compute\nbudget for increasing model size over training tokens. We also study the role\nof synthetic data and TextLM model families in unlocking this potential.\nResults suggest, that our scaled up model achieves comparable performance with\nleading models on speech semantic metrics while using less compute and data\nthan other approaches. We open source models, samples, and data -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/sims.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/wqUq-bT-DvKoNybPX46uL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02398.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b9bc2dacdbc1d0b39c3b50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
      "fullname": "Gallil Maimon",
      "name": "gallilmaimon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02012",
      "authors": [
        {
          "_id": "67ef5af0724d484dd41afe5c",
          "user": {
            "_id": "66189b980da4c017c401fb5d",
            "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
            "isPro": false,
            "fullname": "soro bedio",
            "user": "bedio",
            "type": "user"
          },
          "name": "Soro Bedionita",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:51:20.621Z",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5d",
          "name": "Bruno Andreis",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5e",
          "name": "Song Chong",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5f",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T05:50:19.000Z",
      "submittedOnDailyAt": "2025-04-04T02:38:11.321Z",
      "title": "インストラクションガイド ドル ノライブ アーガレック ネットワーク パラメーター 생성",
      "submittedOnDailyBy": {
        "_id": "66189b980da4c017c401fb5d",
        "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
        "isPro": false,
        "fullname": "soro bedio",
        "user": "bedio",
        "type": "user"
      },
      "summary": "학습 튜크러룰에 따라 조건에 따라 뉴럴네트워크 파라미터를 생성하는 것은 모델의 적응성과 트렌지샹 학습의 발전에 중요합니다. 현재의 방법들은, 특히 딥러닝 모델을 기반으로 한 것들은, 큰 아키텍처에 대한 스케일러블성의 한계, 서로 다른 네트워크의 깊이를 처리할 때의 刚성, 그리고 간접 계층의 Collaboration을 파괴하는 파라미터 생성의 분리된 특성이 있습니다. 본 연구에서는, 다양한 태스크와 아키텍처에서 파라미터 합성을 통일하는 자동회복 프레임워크인 IGPG(인스톨레이션 가이드 파라미터 생성)를 제안합니다. IGPG는 VQ-VAE와 자동회복 모델을 활용하여, 태스크 지시, 데이터 셋, 아키텍처의 세부 사항을 기반으로 뉴럴네트워크 파라미터를 생성합니다. 자동회복적으로 네트워크의 가중치를 토큰으로 생성하는 것입니다. IGPG는 간접 계층의 Collaboration을 보장하고, 모델과 데이터 셋 사이에 효율적인 적응성을 허용합니다. 토큰 수준에서 작동하는 IGPG는, 광범위한 학습된 모델로부터 축소된 복잡한 파라미터 분포를 효과적으로捉捉합니다. 많은 시각 데이터 셋에 대한 확산된 실험은, IGPG가 다양한 학습된 모델을 하나의 유연한 생성 프레임워크에 통합하고, 상태의 최전단 방법과 비교하여 경쟁적 또는 더 높은 성능을 얻는 것을 보여줍니다. 특히, 큰 아키텍처에 대한 스케일러블성과 효율성에서, 이러한 결과를 통해, IGPG는 학습된 가중치의 검색, 모델 선택, 그리고 빠른 태스크 특화된 미세 조정에 강력한 도구로서의 가능성을 강조합니다.",
      "upvotes": 5,
      "discussionId": "67ef5af1724d484dd41afef3",
      "ai_keywords": [
        "diffusion models",
        "IGPG (Instruction Guided Parameter Generation)",
        "VQ-VAE",
        "autoregressive framework",
        "token level",
        "parameter synthesis",
        "inter-layer coherence",
        "vision datasets",
        "pretrained models",
        "pretrained weight retrieval",
        "model selection",
        "task-specific fine-tuning"
      ]
    },
    "publishedAt": "2025-04-02T01:50:19.000Z",
    "title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
    "summary": "Learning to generate neural network parameters conditioned on task\ndescriptions and architecture specifications is pivotal for advancing model\nadaptability and transfer learning. Existing methods especially those based on\ndiffusion models suffer from limited scalability to large architectures,\nrigidity in handling varying network depths, and disjointed parameter\ngeneration that undermines inter-layer coherence. In this work, we propose IGPG\n(Instruction Guided Parameter Generation), an autoregressive framework that\nunifies parameter synthesis across diverse tasks and architectures. IGPG\nleverages a VQ-VAE and an autoregressive model to generate neural network\nparameters, conditioned on task instructions, dataset, and architecture\ndetails. By autoregressively generating neural network weights' tokens, IGPG\nensures inter-layer coherence and enables efficient adaptation across models\nand datasets. Operating at the token level, IGPG effectively captures complex\nparameter distributions aggregated from a broad spectrum of pretrained models.\nExtensive experiments on multiple vision datasets demonstrate that IGPG\nconsolidates diverse pretrained models into a single, flexible generative\nframework. The synthesized parameters achieve competitive or superior\nperformance relative to state-of-the-art methods, especially in terms of\nscalability and efficiency when applied to large architectures. These results\nunderscore ICPG potential as a powerful tool for pretrained weight retrieval,\nmodel selection, and rapid task-specific fine-tuning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66189b980da4c017c401fb5d",
      "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
      "fullname": "soro bedio",
      "name": "bedio",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02119",
      "authors": [
        {
          "_id": "67ef41e7efcb0a2fbfbb6a32",
          "user": {
            "_id": "670826649e319cca029ff240",
            "avatarUrl": "/avatars/6d12b3abf75f714d75d1775d88885345.svg",
            "isPro": false,
            "fullname": "rtfvbhkuj",
            "user": "wwdd7718",
            "type": "user"
          },
          "name": "Wang Wei",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-04T02:20:24.253Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a33",
          "user": {
            "_id": "66e4e50a52356419c4a1ad14",
            "avatarUrl": "/avatars/4be3ce17671785cbe7126b9c1141478b.svg",
            "isPro": false,
            "fullname": "Tiankai Yang",
            "user": "tiankaiy",
            "type": "user"
          },
          "name": "Tiankai Yang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-04T02:25:15.747Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a34",
          "name": "Hongjie Chen",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a35",
          "user": {
            "_id": "62a3ab83e4dd6252344d27cd",
            "avatarUrl": "/avatars/7ca8510f70a58dc207b104240e30c35c.svg",
            "isPro": false,
            "fullname": "Ryan A. Rossi",
            "user": "ryanrossi",
            "type": "user"
          },
          "name": "Ryan A. Rossi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:23:05.421Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a36",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a37",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-04T02:34:51.212Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a38",
          "name": "Hoda Eldardiry",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T20:33:27.000Z",
      "submittedOnDailyAt": "2025-04-04T00:50:35.167Z",
      "title": "시계열예측에서의 LLMs를 활용한 효율적인 모델선택법",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "모델 선택은 시간 시퀀스 예측의 중요한 단계 중 하나이며, 지금까지 다양한 데이터 세트에 대해 매우 엄격한 성능 평가가 필요했습니다. 메타 학습 접근법은 이 과정을 자동화하기 위해 시도하고 있지만, 일반적으로 이러한 성능 매트릭스의 사전 구축에 의존하며, 이는 비용적으로 고렴합니다. 본 연구에서는 Large Language Models(LLMs)를 모델 선택의 경량한 대체로 활용하는 것을 제안하고 있습니다. 우리 방법은 LLMs의 고유한 지식과 추론 능력을 활용하여 명시적인 성능 매트릭스의 필요성을 제거합니다. LLaMA, GPT와 Gemini를 활용한 확장 실험에서, 우리 접근법이 전통적인 메타 학습 방법과 휴리스틱 기반 라인 워크를 초월하고, 계산 오버헤드를 크게 줄일 수 있음을 보여주었습니다. 이러한 발견은 LLMs가 시간 시퀀스 예측의 효율적인 모델 선택에 대한 가능성을 강조하고 있습니다.",
      "upvotes": 4,
      "discussionId": "67ef41e8efcb0a2fbfbb6a93",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "model selection",
        "time series forecasting",
        "meta-learning approaches",
        "pre-constructed performance matrices",
        "reasoning capabilities",
        "experiments",
        "LLaMA",
        "GPT",
        "Gemini",
        "heuristic baselines",
        "computational overhead"
      ]
    },
    "publishedAt": "2025-04-02T16:33:27.000Z",
    "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
    "summary": "Model selection is a critical step in time series forecasting, traditionally\nrequiring extensive performance evaluations across various datasets.\nMeta-learning approaches aim to automate this process, but they typically\ndepend on pre-constructed performance matrices, which are costly to build. In\nthis work, we propose to leverage Large Language Models (LLMs) as a lightweight\nalternative for model selection. Our method eliminates the need for explicit\nperformance matrices by utilizing the inherent knowledge and reasoning\ncapabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini,\nwe demonstrate that our approach outperforms traditional meta-learning\ntechniques and heuristic baselines, while significantly reducing computational\noverhead. These findings underscore the potential of LLMs in efficient model\nselection for time series forecasting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02119.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00891",
      "authors": [
        {
          "_id": "67ef62342a18e60aeee0ea02",
          "name": "Jian Zhao",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea03",
          "user": {
            "_id": "667187ba9ab144eb3ac43a1b",
            "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
            "isPro": false,
            "fullname": "Runze Liu",
            "user": "RyanLiu112",
            "type": "user"
          },
          "name": "Runze Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:09.923Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea04",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:22.709Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea05",
          "name": "Zhimu Zhou",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea06",
          "user": {
            "_id": "67ab05fe4c6ca2d5db4c0c52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QpGUNDkeuKjX71s2GXlXF.png",
            "isPro": false,
            "fullname": "Junqi Gao",
            "user": "ChetKao",
            "type": "user"
          },
          "name": "Junqi Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:38.624Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea07",
          "name": "Dong Li",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea08",
          "user": {
            "_id": "6562db314e8918182da42706",
            "avatarUrl": "/avatars/b113bbbb496bf4dac254f0e840f08e10.svg",
            "isPro": false,
            "fullname": "Jiafei Lyu",
            "user": "dmux",
            "type": "user"
          },
          "name": "Jiafei Lyu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:45.394Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea09",
          "user": {
            "_id": "65b34c5785b6c2144807db37",
            "avatarUrl": "/avatars/4c1cb03cda250d4ec760ebf7815a3bce.svg",
            "isPro": false,
            "fullname": "Qianzhouyi",
            "user": "Saputello",
            "type": "user"
          },
          "name": "Zhouyi Qian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:00.763Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0a",
          "user": {
            "_id": "645d9c3058f9ee315148116d",
            "avatarUrl": "/avatars/165e18f27b5a50738bf1d22857118478.svg",
            "isPro": false,
            "fullname": "Biqing Qi",
            "user": "jackqi7",
            "type": "user"
          },
          "name": "Biqing Qi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:06.517Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0b",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0c",
          "user": {
            "_id": "669f614b59adf5b56e05bce3",
            "avatarUrl": "/avatars/ffd4189efbceb0e63a03db273065a44b.svg",
            "isPro": false,
            "fullname": "BowenZhou",
            "user": "bowenZhou",
            "type": "user"
          },
          "name": "Bowen Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:14.312Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T15:21:05.000Z",
      "submittedOnDailyAt": "2025-04-04T03:13:15.991Z",
      "title": "GenPRM: 테스트 시의 계산량을 확장하는 프로세스 보상 모델에서 생성적인 이유론법",
      "submittedOnDailyBy": {
        "_id": "667187ba9ab144eb3ac43a1b",
        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
        "isPro": false,
        "fullname": "Runze Liu",
        "user": "RyanLiu112",
        "type": "user"
      },
      "summary": "최근의 대 언어 모델(LLMs)의 발전에 따라, 프로세스 보상 모델(PRMs)을 검증기로 활용하여 LLMs의 성능을 향상시키는 것이 바람직하다고 제시되어 있습니다. 그러나 현재의 PRMs은 3가지의 중요한 문제를 직면하고 있습니다: 1) 프로세스의 제어와 일반화 능력의 한계, 2) 스칼라 예측에 의존하며 LLMs의 생성 능력을 활용하지 않는, 3) 테스트 시의 계산량의 스케일링이 가능한지 없는. 본 논문에서는 Chain-of-Thought(CoT) 논리와 코드 검증을 수행하고, 각 논리 단계에 대해 판단을 제공하는 생성적인 프로세스 보상 모델 GenPRM을 통해 프로세스의 제어 라벨과 이유 데이터를 고품질으로 얻을 수 있는 방법을 제시합니다. Relative Progress Evaluation(RPE)과 이유의 합성 프레임워크를 제안하여, ProcessBench와 수학적 논리 태스크의 실험 결과를 통해 GenPRM은 MATH 데이터셋에서 23K의 훈련 데이터를 사용하여, 기존의 PRMs를 크게 초월하는 것을 보여주었습니다. 테스트 시의 스케일링에 따라, 1.5B의 GenPRM은 GPT-4o를 초월하고, 7B의 GenPRM은 ProcessBench에서 Qwen2.5-Math-PRM-72B를 초월했습니다. 또한, GenPRM은 정책 모델의 훈련을 위한 평가 모델로서 강력한 능력을 보여주고 있습니다. 본 논문은 PRMs와 평가 모델 사이의 간극을 메우는 새로운 패러다임에 대한 권장이며, 코드, 모델, 데이터는 https://ryanliu112.github.io/GenPRM에서 접근할 수 있습니다.",
      "upvotes": 4,
      "discussionId": "67ef62352a18e60aeee0ea4b",
      "projectPage": "https://ryanliu112.github.io/GenPRM",
      "githubRepo": "https://github.com/RyanLiu112/GenPRM",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Process Reward Models (PRMs)",
        "Chain-of-Thought (CoT) reasoning",
        "Relative Progress Estimation (RPE)",
        "ProcessBench",
        "MATH dataset",
        "GPT-4",
        "Qwen2.5-Math-PRM-72B",
        "critic model",
        "policy model refinement"
      ]
    },
    "publishedAt": "2025-04-01T11:21:05.000Z",
    "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning",
    "summary": "Recent advancements in Large Language Models (LLMs) have shown that it is\npromising to utilize Process Reward Models (PRMs) as verifiers to enhance the\nperformance of LLMs. However, current PRMs face three key challenges: (1)\nlimited process supervision and generalization capabilities, (2) dependence on\nscalar value prediction without leveraging the generative abilities of LLMs,\nand (3) inability to scale the test-time compute of PRMs. In this work, we\nintroduce GenPRM, a generative process reward model that performs explicit\nChain-of-Thought (CoT) reasoning with code verification before providing\njudgment for each reasoning step. To obtain high-quality process supervision\nlabels and rationale data, we propose Relative Progress Estimation (RPE) and a\nrationale synthesis framework that incorporates code verification. Experimental\nresults on ProcessBench and several mathematical reasoning tasks show that\nGenPRM significantly outperforms prior PRMs with only 23K training data from\nMATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and\na 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally,\nGenPRM demonstrates strong abilities to serve as a critic model for policy\nmodel refinement. This work establishes a new paradigm for process supervision\nthat bridges the gap between PRMs and critic models in LLMs. Our code, model,\nand data will be available in https://ryanliu112.github.io/GenPRM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00891.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667187ba9ab144eb3ac43a1b",
      "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
      "fullname": "Runze Liu",
      "name": "RyanLiu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.22444",
      "authors": [
        {
          "_id": "67ef33a4456bcf30fa95b2f1",
          "user": {
            "_id": "655fb8a122ce47e5fa491c72",
            "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
            "isPro": false,
            "fullname": "Pengsong Zhang",
            "user": "universea",
            "type": "user"
          },
          "name": "Pengsong Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:42.152Z",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f2",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f3",
          "name": "Huazhe Xu",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f4",
          "name": "Renjun Xu",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f5",
          "name": "Zhenting Wang",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f6",
          "name": "Cong Wang",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f7",
          "name": "Animesh Garg",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f8",
          "name": "Zhibin Li",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f9",
          "name": "Arash Ajoudani",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2fa",
          "name": "Xinyu Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655fb8a122ce47e5fa491c72/d-38XQKRw3vnyFhzZFTEd.jpeg"
      ],
      "publishedAt": "2025-03-28T14:00:27.000Z",
      "submittedOnDailyAt": "2025-04-04T06:29:27.852Z",
      "title": "AI와 로봇 과학자에 의한 과학적 발견의 규모 법칙",
      "submittedOnDailyBy": {
        "_id": "655fb8a122ce47e5fa491c72",
        "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
        "isPro": false,
        "fullname": "Pengsong Zhang",
        "user": "universea",
        "type": "user"
      },
      "summary": "과학적 발견은 고도의 로봇 기술과 인공지능에 의해 급격히 발전할 것으로 예상됩니다. 현재의 과학 연구는手工실험이 많은 시간과 자원을 소모하며, 다양한 과학 분야의 연구는 개별 연구자의 전문 지식의 범위를 초과하는 지식 통합이 필요합니다. 여기서 우리는 자동 변환형 일반적인 과학자(AGS)의 개념을 상상하고 있습니다. 이 개념은 효과적인 AI와 추상화된 로봇을 조합하여, 전체 연구 주기를 자동화하는 것을 목표로 합니다. 이 시스템은 물리적 및 시각적 환경과 동적인 상호작용을 촉진하고, 다양한 과학 분야의 지식 통합을 지원할 수 있습니다. 이러한 기술은 연구의 모든 단계에서 직접 확장될 수 있도록 연구(문헌 검색, 가설 생성, 실험, 연구보고 작성)을 목표로 하며, 내부 반성 및 외부 피드백의 도입으로, 과학적 발견에 필요한 시간과 자원을 크게 줄일 수 있습니다. 가상의 AI 과학자가 기능적인 일반적인 AI 기반의 로봇 과학자로 진화하여, AGS는 글로벌적인 가능성을 제공합니다. 이러한 자동 변환 시스템이 연구 프로세스에 더욱 엄격하게 통합될 것으로 예상하며, 과학적 발견이 새로운 규모 법칙에 따라 할 수 있는 가능성을 가정하고, 지식의 생성과 진화를 새로운 시각에서 고려할 수 있는 것이 될 수 있다는 가정입니다. 추상화된 로봇의 극한 환경에 대한 적응성과 과학 지식의 적극적인 증가에 의한 자유 레벨 효과는 물리적 및 지적 경계를 지속적으로 초월하는 것을 목표로 합니다.",
      "upvotes": 4,
      "discussionId": "67ef33a5456bcf30fa95b35e",
      "githubRepo": "https://github.com/openags/Awesome-AI-Scientist-Papers"
    },
    "publishedAt": "2025-03-28T10:00:27.000Z",
    "title": "Scaling Laws in Scientific Discovery with AI and Robot Scientists",
    "summary": "Scientific discovery is poised for rapid advancement through advanced\nrobotics and artificial intelligence. Current scientific practices face\nsubstantial limitations as manual experimentation remains time-consuming and\nresource-intensive, while multidisciplinary research demands knowledge\nintegration beyond individual researchers' expertise boundaries. Here, we\nenvision an autonomous generalist scientist (AGS) concept combines agentic AI\nand embodied robotics to automate the entire research lifecycle. This system\ncould dynamically interact with both physical and virtual environments while\nfacilitating the integration of knowledge across diverse scientific\ndisciplines. By deploying these technologies throughout every research stage --\nspanning literature review, hypothesis generation, experimentation, and\nmanuscript writing -- and incorporating internal reflection alongside external\nfeedback, this system aims to significantly reduce the time and resources\nneeded for scientific discovery. Building on the evolution from virtual AI\nscientists to versatile generalist AI-based robot scientists, AGS promises\ngroundbreaking potential. As these autonomous systems become increasingly\nintegrated into the research process, we hypothesize that scientific discovery\nmight adhere to new scaling laws, potentially shaped by the number and\ncapabilities of these autonomous systems, offering novel perspectives on how\nknowledge is generated and evolves. The adaptability of embodied robots to\nextreme environments, paired with the flywheel effect of accumulating\nscientific knowledge, holds the promise of continually pushing beyond both\nphysical and intellectual frontiers.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655fb8a122ce47e5fa491c72/d-38XQKRw3vnyFhzZFTEd.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22444.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655fb8a122ce47e5fa491c72",
      "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
      "fullname": "Pengsong Zhang",
      "name": "universea",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.01871",
      "authors": [
        {
          "_id": "67eea9e5117231f8bb04402b",
          "user": {
            "_id": "65d0c00b0954f06e472909f4",
            "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
            "isPro": false,
            "fullname": "tom bush",
            "user": "tuphs",
            "type": "user"
          },
          "name": "Thomas Bush",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T19:20:34.085Z",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402c",
          "name": "Stephen Chung",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402d",
          "name": "Usman Anwar",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402e",
          "user": {
            "_id": "645ecd18f0f92653b9f33d4e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645ecd18f0f92653b9f33d4e/nHDMWtM9ZHrji0c4Y4XW1.jpeg",
            "isPro": false,
            "fullname": "Adrià Garriga-Alonso",
            "user": "agaralon",
            "type": "user"
          },
          "name": "Adrià Garriga-Alonso",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-03T15:31:53.577Z",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402f",
          "name": "David Krueger",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T16:24:23.000Z",
      "submittedOnDailyAt": "2025-04-04T07:00:29.802Z",
      "title": "无模型强化学习中的回溯规划解释",
      "submittedOnDailyBy": {
        "_id": "65d0c00b0954f06e472909f4",
        "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
        "isPro": false,
        "fullname": "tom bush",
        "user": "tuphs",
        "type": "user"
      },
      "summary": "이곳에서는 기계적인 증거로 계획의 학습을 수행하는 모델 없는 강화 학습 에이전트를 제공합니다. 이 방법은 컴퓨터 게임 \"SoCoBaN\"에서 모델 없는 에이전트에 기반한 개념 기반 설명성을 적용하여 실현되었습니다. 특히, DRC(Guez et al. (2019)에 의해 도입된 장르의 모델 없는 에이전트)은 학습된 개념 표현을 내부적으로 계획 구성에 사용하며, 행동의 장기적인 환경 효과를 예측하고 행동 선택에 영향을 미칩니다. 우리의 방법은 (1) 계획에 관련된 개념의 검출, (2) 에이전트의 표현 내의 계획 형성 조사, (3) 발견된 계획(에이전트의 표현 내)이 에이전트의 행동에因果적 영향을 미치는 것을 확인하는 세 단계로 구성됩니다. 또한, 이러한 계획의 발견은 계획적 성능과 같은 것을 발견하고 테스트 시 추가 컴퓨팅의 이점을 얻을 수 있는 능력을 볼 수 있습니다. 마지막으로, 에이전트가 학습한 계획 알고리즘에 대해 질적인 분석을 수행하고, 병렬화된 양방향 탐색에 강한 유사성을 발견했습니다. 이러한 발견은 에이전트의 계획 행동에 기반한 내부 구조를 더 깊게 이해할 수 있으며, 최근의 LLM의 강화 학습에 의한 계획과 추론 능력의 현상을 이해하는 데 중요합니다.",
      "upvotes": 3,
      "discussionId": "67eea9e9117231f8bb044167",
      "ai_keywords": [
        "model-free reinforcement learning",
        "concept-based interpretability",
        "Sokoban",
        "DRC",
        "learned concept representations",
        "plan formation",
        "causal effect",
        "parallelized bidirectional search",
        "LLMs",
        "emergent planning",
        "reasoning capabilities"
      ]
    },
    "publishedAt": "2025-04-02T12:24:23.000Z",
    "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
    "summary": "We present the first mechanistic evidence that model-free reinforcement\nlearning agents can learn to plan. This is achieved by applying a methodology\nbased on concept-based interpretability to a model-free agent in Sokoban -- a\ncommonly used benchmark for studying planning. Specifically, we demonstrate\nthat DRC, a generic model-free agent introduced by Guez et al. (2019), uses\nlearned concept representations to internally formulate plans that both predict\nthe long-term effects of actions on the environment and influence action\nselection. Our methodology involves: (1) probing for planning-relevant\nconcepts, (2) investigating plan formation within the agent's representations,\nand (3) verifying that discovered plans (in the agent's representations) have a\ncausal effect on the agent's behavior through interventions. We also show that\nthe emergence of these plans coincides with the emergence of a planning-like\nproperty: the ability to benefit from additional test-time compute. Finally, we\nperform a qualitative analysis of the planning algorithm learned by the agent\nand discover a strong resemblance to parallelized bidirectional search. Our\nfindings advance understanding of the internal mechanisms underlying planning\nbehavior in agents, which is important given the recent trend of emergent\nplanning and reasoning capabilities in LLMs through RL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01871.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d0c00b0954f06e472909f4",
      "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
      "fullname": "tom bush",
      "name": "tuphs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02821",
      "authors": [
        {
          "_id": "67ef9a8885ea9d1d7db3fdb5",
          "name": "Mateusz Pach",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb6",
          "name": "Shyamgopal Karthik",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb7",
          "name": "Quentin Bouniot",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb8",
          "name": "Serge Belongie",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb9",
          "name": "Zeynep Akata",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:58:35.000Z",
      "submittedOnDailyAt": "2025-04-04T07:08:47.568Z",
      "title": "스파스 어코더는 시각 언어 모델에서 단일 의미의 특징을 학습합니다.",
      "submittedOnDailyBy": {
        "_id": "6254599b6e36fe62e141c8f9",
        "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
        "isPro": false,
        "fullname": "Shyamgopal Karthik",
        "user": "shyamgopal",
        "type": "user"
      },
      "summary": "Sparse Autoencoders (SAEs)는 최근 Large Language Models (LLMs)의 해석성과 제어성을 향상시키는 데에 기여하는 것으로 나타났습니다. 본 논문에서는 SAEs의 적용 범위를 Vision-Language Models (VLMs)에 확장하고, 시각 표현의 단일 의미성을 평가하기 적합한 상세한 프레임워크를 제안합니다. 실험 결과를 통해, VLMs에서 훈련된 SAEs는 개별 뉴런의 단일 의미성을 크게 향상시키고, 전문가가 정의한 구조와 일치하는 계층적 표현을 나타냅니다 (예: iNaturalist 분류법). 특히, CLIP의 시각 인코더에 SAEs를 적용하여, LLaVA와 같은 다형 모델의 출력을 직접 조작할 수 있음을 보여주었습니다. 이러한 발견은 SAEs가 VLMs의 해석성과 제어성을 향상시키는 무한制的, 무 도전적인 접근 방식의 실용성과 효과성을 강조합니다.",
      "upvotes": 2,
      "discussionId": "67ef9a8985ea9d1d7db3fe20",
      "ai_keywords": [
        "Sparse Autoencoders (SAEs)",
        "Large Language Models (LLMs)",
        "Vision-Language Models (VLMs)",
        "CLIP",
        "monosemanticity",
        "vision representations",
        "hierarchical representations",
        "iNaturalist taxonomy",
        "multimodal LLMs",
        "LLaVA",
        "unsupervised approach"
      ]
    },
    "publishedAt": "2025-04-03T13:58:35.000Z",
    "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models",
    "summary": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6254599b6e36fe62e141c8f9",
      "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
      "fullname": "Shyamgopal Karthik",
      "name": "shyamgopal",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]