[
  {
    "paper": {
      "id": "2503.16905",
      "authors": [
        {
          "_id": "67e0c13fe5fa0da84e134581",
          "user": {
            "_id": "658be7fe135580745c510323",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
            "isPro": false,
            "fullname": "Jian Zhang",
            "user": "VentureZJ",
            "type": "user"
          },
          "name": "Jian Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:07:08.476Z",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134582",
          "name": "Zhiyuan Wang",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134583",
          "name": "Zhangqi Wang",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134584",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134585",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134586",
          "user": {
            "_id": "66ac77011cfb12c087605acb",
            "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg",
            "isPro": false,
            "fullname": "Lin",
            "user": "Qika",
            "type": "user"
          },
          "name": "Qika Lin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-24T02:19:51.913Z",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134587",
          "name": "Rui Mao",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134588",
          "name": "Erik Cambria",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134589",
          "name": "Jun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T07:13:45.000Z",
      "submittedOnDailyAt": "2025-03-24T00:51:41.644Z",
      "title": "MAPS: 빅세븐페이지어퍼숍과 소클라트릭 가이드닝에 기반한 다모둠 과학 문제 해결을 위한 다중 에이전트 프레임워크",
      "submittedOnDailyBy": {
        "_id": "658be7fe135580745c510323",
        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
        "isPro": false,
        "fullname": "Jian Zhang",
        "user": "VentureZJ",
        "type": "user"
      },
      "summary": "다형성 과학 문제(MSPs)는 텍스트나 그래픽 등 여러 모델의 통합이 필요하며, 인공지능에 있어 큰 과제가 됩니다. 전통적인 과학 문제에 대한 발전이 있었음에도 불구하고, MSPs는 주로 두 가지 문제를 보아야 합니다: 과학 문제 해결에 있어서의 다형성의 체계적인 추론의 과제와, 반성 및 재검토 능력의 부족입니다. 이러한 문제를 해결하기 위해, 우리는 Big Seven Personality와 Socratic Guideline에 기반한 Multi-Agent 프레임워크(MAPS)를 제안합니다. 이 프레임워크는 7개의 서로 다른 에이전트를 사용하며, 피드백 구조와 Socratic 방법의 활용을 통해 MSPs의 해결을 가이드합니다. 첫 번째 문제를 해결하기 위해, 우리는 발전적인 4 에이전트의 해결책 전략을 제안하고, 각 에이전트는 문제 해결 프로세스의 특정 단계를 중점적으로 합니다. 두 번째 문제를 해결하기 위해, Socratic의 질문에 모델링된 Critic 에이전트를 도입하며, 비판적인 사고를 촉발하고, 자동 학습을 자극합니다. EMMA, Olympia, 그리고 MathVista 데이터 세트에 대해 확장된 실험을 수행하였으며, 모든 태스크에서 현재의 SOTA 모델을 15.84% 초과하여 기대하는 열망적인 결과를 얻었습니다. 동시에, 추가적인 분석적인 실험도 모델의 발전과 일반화 능력에 대한 증명을 합니다.",
      "upvotes": 33,
      "discussionId": "67e0c147e5fa0da84e1347f5",
      "githubRepo": "https://github.com/exoskeletonzj/MAPS"
    },
    "publishedAt": "2025-03-21T03:13:45.000Z",
    "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving",
    "summary": "Multimodal scientific problems (MSPs) involve complex issues that require the\nintegration of multiple modalities, such as text and diagrams, presenting a\nsignificant challenge in artificial intelligence. While progress has been made\nin addressing traditional scientific problems, MSPs still face two primary\nissues: the challenge of multi-modal comprehensive reasoning in scientific\nproblem-solving and the lack of reflective and rethinking capabilities. To\naddress these issues, we introduce a Multi-Agent framework based on the Big\nSeven Personality and Socratic guidance (MAPS). This framework employs seven\ndistinct agents that leverage feedback mechanisms and the Socratic method to\nguide the resolution of MSPs. To tackle the first issue, we propose a\nprogressive four-agent solving strategy, where each agent focuses on a specific\nstage of the problem-solving process. For the second issue, we introduce a\nCritic agent, inspired by Socratic questioning, which prompts critical thinking\nand stimulates autonomous learning. We conduct extensive experiments on the\nEMMA, Olympiad, and MathVista datasets, achieving promising results that\noutperform the current SOTA model by 15.84% across all tasks. Meanwhile, the\nadditional analytical experiments also verify the model's progress as well as\ngeneralization ability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658be7fe135580745c510323",
      "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
      "fullname": "Jian Zhang",
      "name": "VentureZJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16874",
      "authors": [
        {
          "_id": "67e0c1ce151ca9ed9284dc52",
          "user": {
            "_id": "658be7fe135580745c510323",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
            "isPro": false,
            "fullname": "Jian Zhang",
            "user": "VentureZJ",
            "type": "user"
          },
          "name": "Jian Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:07:05.271Z",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc53",
          "name": "Zhangqi Wang",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc54",
          "name": "Haiping Zhu",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc55",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc56",
          "user": {
            "_id": "66ac77011cfb12c087605acb",
            "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg",
            "isPro": false,
            "fullname": "Lin",
            "user": "Qika",
            "type": "user"
          },
          "name": "Qika Lin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-24T02:22:15.812Z",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc57",
          "name": "Erik Cambria",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T06:19:55.000Z",
      "submittedOnDailyAt": "2025-03-24T00:56:26.218Z",
      "title": "MARS: 스코라테이크 가이드링을 채택한 효과적인 프로노프토 최적화 프레임워크",
      "submittedOnDailyBy": {
        "_id": "658be7fe135580745c510323",
        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
        "isPro": false,
        "fullname": "Jian Zhang",
        "user": "VentureZJ",
        "type": "user"
      },
      "summary": "대 언어 모델의 기본적인 질문 대답 형식은 입력된 폼을 포함하며, 폼의 품질이 답변의 유효성에 직접적인 영향을 미칩니다. 자동화 폼 최적화(APO)는 자동화 설계된 폼의 인지적 편향으로부터 자유롭게 벗어나, 폼의 설계 공간의 확장을 목표로 합니다. 그러나 현재의 APO手法는 고정 템플릿의 유연성 한계와 폼 공간의 효율적인 검색을 주요 문제로 합니다. 이에 대해 우리는, 다 에이전트 융합 기술에 기반한 자동 계획을 수행하는 Multi-Agent 프레임워크인 MARS를 제안합니다. 특히, MARS는 7개의 에이전트를 구성하고, 각각의 기능이 다른 것을 특징으로 하며, 주요 기능으로는 자동적으로 Planner를 사용하여 유연성을 보장하는 최적화 경로를 설계하는 것입니다. 또한, Teacher-Critic-Student의 Socratic 대화 패턴을 사용하여 효과적인 검색을 수행하는 동안 폼을 연속적으로 최적화합니다. 우리는 다양한 데이터 세트에 대해 광범위한 실험을 수행하고, 방법의 유효성을 증명하고, 추가적인 분석적인 실험을 수행하여 모델의 발전과 해석성을 평가합니다.",
      "upvotes": 31,
      "discussionId": "67e0c1d7151ca9ed9284ded7",
      "githubRepo": "https://github.com/exoskeletonzj/MARS",
      "ai_keywords": [
        "Multi-Agent framework",
        "Socratic guidance",
        "multi-agent fusion technology",
        "Planner",
        "Teacher-Critic-Student Socratic dialogue pattern"
      ]
    },
    "publishedAt": "2025-03-21T02:19:55.000Z",
    "title": "MARS: A Multi-Agent Framework Incorporating Socratic Guidance for\n  Automated Prompt Optimization",
    "summary": "The basic question-answering format of large language models involves\ninputting a prompt and receiving a response, and the quality of the prompt\ndirectly impacts the effectiveness of the response. Automated Prompt\nOptimization (APO) aims to break free from the cognitive biases of manually\ndesigned prompts and explores a broader design space for prompts. However,\nexisting APO methods suffer from limited flexibility of fixed templates and\ninefficient search in prompt spaces as key issues. To this end, we propose a\nMulti-Agent framework Incorporating Socratic guidance (MARS), which utilizes\nmulti-agent fusion technology for automatic planning, with gradual continuous\noptimization and evaluation. Specifically, MARS comprises seven agents, each\nwith distinct functionalities, which autonomously use the Planner to devise an\noptimization path that ensures flexibility. Additionally, it employs a\nTeacher-Critic-Student Socratic dialogue pattern to iteratively optimize the\nprompts while conducting effective search. We conduct extensive experiments on\nvarious datasets to validate the effectiveness of our method, and perform\nadditional analytical experiments to assess the model's advancement as well as\nthe interpretability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16874.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658be7fe135580745c510323",
      "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
      "fullname": "Jian Zhang",
      "name": "VentureZJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16408",
      "authors": [
        {
          "_id": "67dcdedbeff29d0d52c739e4",
          "user": {
            "_id": "658a6c1399ed106ac8c822b1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a6c1399ed106ac8c822b1/Wk2KXCcK39rUvXx6mpmGD.jpeg",
            "isPro": false,
            "fullname": "yiranqin",
            "user": "IranQin",
            "type": "user"
          },
          "name": "Yiran Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-21T11:40:36.686Z",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e5",
          "user": {
            "_id": "64eadcb03d76028d805a7818",
            "avatarUrl": "/avatars/528e4fded4419caf08589b2ed40437bc.svg",
            "isPro": false,
            "fullname": "Li Kang",
            "user": "FACEONG",
            "type": "user"
          },
          "name": "Li Kang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-21T11:40:38.834Z",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e6",
          "name": "Xiufeng Song",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e7",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e8",
          "name": "Xiaohong Liu",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e9",
          "name": "Xihui Liu",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739ea",
          "name": "Ruimao Zhang",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739eb",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:58:38.000Z",
      "submittedOnDailyAt": "2025-03-24T01:35:09.139Z",
      "title": "로보팩토리：구성적 제약을 갖는 구체적인 에이전트의 협조를 탐구합니다.",
      "submittedOnDailyBy": {
        "_id": "658a6c1399ed106ac8c822b1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a6c1399ed106ac8c822b1/Wk2KXCcK39rUvXx6mpmGD.jpeg",
        "isPro": false,
        "fullname": "yiranqin",
        "user": "IranQin",
        "type": "user"
      },
      "summary": "실체화 다중 에이전트 시스템의 효과적인 설계는 복잡한 실세계의 태스크를 해결하기 위해 중요합니다. 다중 에이전트 시스템의 복잡성에 따라, 현재의 방법들은 이러한 시스템에 대한 안전하고 효율적인 훈련 데이터를 자동적으로 생성할 수 없습니다. 이러한 점에 대해, 우리는 실체화 다중 에이전트 시스템의 구성 제약 개념을 제안하고, 에이전트 간 협력으로 문제를 해결하고자 합니다. 다양한 종류의 제약에 맞는 형식의 필드를 설계하여 물리적인 세계와의 쉽게 상호작용을 실현합니다. 구성 제약과 특히 설계된 필드를 활용하여, 우리는 실체화 다중 에이전트 시스템의 자동 데이터 수집 프레임워크를 개발하고, 최초의 실체화 다중 에이전트 동작의 벤치마크인 \"RoboFactory\"를 통해 진행합니다. \"RoboFactory\" 벤치마크에 기반하여, 우리는 학습을 모방하는 방법을 적용하고, 그 성능을 다양한 난이도의 에이전트 태스크로 분석합니다. 또한, 다중 에이전트의 학습을 모방하기 위한 아키텍처와 훈련 전략을 검토하고, 안전하고 효율적인 실체화 다중 에이전트 시스템 구축을 목표로 합니다.",
      "upvotes": 27,
      "discussionId": "67dcdedeeff29d0d52c73abc",
      "projectPage": "https://iranqin.github.io/robofactory/",
      "ai_keywords": [
        "compositional constraints",
        "embodied multi-agent systems",
        "data collection framework",
        "benchmark",
        "RoboFactory",
        "imitation learning",
        "multi-agent imitation learning",
        "training strategies"
      ]
    },
    "publishedAt": "2025-03-20T13:58:38.000Z",
    "title": "RoboFactory: Exploring Embodied Agent Collaboration with Compositional\n  Constraints",
    "summary": "Designing effective embodied multi-agent systems is critical for solving\ncomplex real-world tasks across domains. Due to the complexity of multi-agent\nembodied systems, existing methods fail to automatically generate safe and\nefficient training data for such systems. To this end, we propose the concept\nof compositional constraints for embodied multi-agent systems, addressing the\nchallenges arising from collaboration among embodied agents. We design various\ninterfaces tailored to different types of constraints, enabling seamless\ninteraction with the physical world. Leveraging compositional constraints and\nspecifically designed interfaces, we develop an automated data collection\nframework for embodied multi-agent systems and introduce the first benchmark\nfor embodied multi-agent manipulation, RoboFactory. Based on RoboFactory\nbenchmark, we adapt and evaluate the method of imitation learning and analyzed\nits performance in different difficulty agent tasks. Furthermore, we explore\nthe architectures and training strategies for multi-agent imitation learning,\naiming to build safe and efficient embodied multi-agent systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16408.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658a6c1399ed106ac8c822b1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a6c1399ed106ac8c822b1/Wk2KXCcK39rUvXx6mpmGD.jpeg",
      "fullname": "yiranqin",
      "name": "IranQin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16430",
      "authors": [
        {
          "_id": "67e0bd81b04d9e836829c468",
          "user": {
            "_id": "63ea23b9dedfeebe54d02bdf",
            "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
            "isPro": false,
            "fullname": "Yuqing Wang",
            "user": "Epiphqny",
            "type": "user"
          },
          "name": "Yuqing Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:10:42.267Z",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c469",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46a",
          "name": "Yao Teng",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46b",
          "name": "Yuanzhi Zhu",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46c",
          "user": {
            "_id": "60d2e681b8448e1785bbda06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
            "isPro": false,
            "fullname": "Shuhuai Ren",
            "user": "ShuhuaiRen",
            "type": "user"
          },
          "name": "Shuhuai Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:10:44.045Z",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46d",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46e",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:59.000Z",
      "submittedOnDailyAt": "2025-03-24T00:50:05.627Z",
      "title": "연속 및 이산 토큰의 값을 통합한 자동 순차적 시각 생성",
      "submittedOnDailyBy": {
        "_id": "63ea23b9dedfeebe54d02bdf",
        "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
        "isPro": false,
        "fullname": "Yuqing Wang",
        "user": "Epiphqny",
        "type": "user"
      },
      "summary": "自動회귀의 이미지 생성 모형은 일반적으로 토큰나이저를 사용하여 이미지를 토큰으로 압축하고 순서대로 예측할 수 있도록 합니다. 토큰 표현에는 기본적인 논리적 문제를 있습니다: 이산 토큰은 표준적인 교차 엔트로피 손실을 사용하여 간단하게 모델링할 수 있지만, 정보 손실과 토큰나이저의 훈련 불안정성에 어려움을 겪습니다; 연속 토큰은 시각적 세부 사항을 더 잘 보존할 수 있지만, 복잡한 분포 모델링이 필요하고, 생성 파이프라인을 복잡하게 만듭니다. 본 논문에서는, 연속 토큰의 강력한 표현 능력과 이산 토큰의 모델링의 간단성을 유지하면서 이 차이를 극복하기 위해 TokenBridge를 제안합니다. 이를 실현하기 위해, 토큰나이저의 훈련 프로세스에서 이산화를 분리하고, 연속 표현으로부터 직접 이산 토큰을 얻기 위한 후학습 저감화를 사용합니다. 특히, 차원별 저감화 전략을 도입하고, 각 특성 차원을 독립적으로 이산화하여 그 결과를 효율적으로 모델링할 수 있는 가벼운 자동회귀적인 예측 구조를 조합합니다. 엄격한 실험은 표준적인 분류 예측을 사용하여 연속적인 방법과 동일한 재구성과 생성 품질을 달성함을 보여줍니다. 이 연구는 이산과 연속의 패러다임을 통해 두 가지 접근 방식의 강점을 효과적으로 활용할 수 있음을 보여주고, 간단한 자동회귀적인 모델링을 사용하여 고품질 이미지 생성의 가능성을 제공합니다. 프로젝트 페이지: https://yuqingwang1029.github.io/TokenBridge.",
      "upvotes": 19,
      "discussionId": "67e0bd85b04d9e836829c55f",
      "projectPage": "https://yuqingwang1029.github.io/TokenBridge/",
      "githubRepo": "https://github.com/YuqingWang1029/TokenBridge",
      "ai_keywords": [
        "autoregressive visual generation models",
        "tokenizers",
        "tokens",
        "discrete tokens",
        "continuous tokens",
        "cross-entropy loss",
        "tokenizer training",
        "TokenBridge",
        "post-training quantization",
        "dimension-wise quantization",
        "lightweight autoregressive prediction mechanism",
        "reconstruction quality",
        "generation quality"
      ]
    },
    "publishedAt": "2025-03-20T13:59:59.000Z",
    "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual\n  Generation",
    "summary": "Autoregressive visual generation models typically rely on tokenizers to\ncompress images into tokens that can be predicted sequentially. A fundamental\ndilemma exists in token representation: discrete tokens enable straightforward\nmodeling with standard cross-entropy loss, but suffer from information loss and\ntokenizer training instability; continuous tokens better preserve visual\ndetails, but require complex distribution modeling, complicating the generation\npipeline. In this paper, we propose TokenBridge, which bridges this gap by\nmaintaining the strong representation capacity of continuous tokens while\npreserving the modeling simplicity of discrete tokens. To achieve this, we\ndecouple discretization from the tokenizer training process through\npost-training quantization that directly obtains discrete tokens from\ncontinuous representations. Specifically, we introduce a dimension-wise\nquantization strategy that independently discretizes each feature dimension,\npaired with a lightweight autoregressive prediction mechanism that efficiently\nmodel the resulting large token space. Extensive experiments show that our\napproach achieves reconstruction and generation quality on par with continuous\nmethods while using standard categorical prediction. This work demonstrates\nthat bridging discrete and continuous paradigms can effectively harness the\nstrengths of both approaches, providing a promising direction for high-quality\nvisual generation with simple autoregressive modeling. Project page:\nhttps://yuqingwang1029.github.io/TokenBridge.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16430.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63ea23b9dedfeebe54d02bdf",
      "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
      "fullname": "Yuqing Wang",
      "name": "Epiphqny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16660",
      "authors": [
        {
          "_id": "67e0ffb029682c8065e1c223",
          "name": "Eduard Allakhverdov",
          "hidden": false
        },
        {
          "_id": "67e0ffb029682c8065e1c224",
          "name": "Elizaveta Goncharova",
          "hidden": false
        },
        {
          "_id": "67e0ffb029682c8065e1c225",
          "name": "Andrey Kuznetsov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T19:17:08.000Z",
      "submittedOnDailyAt": "2025-03-24T05:20:19.676Z",
      "title": "그 정도의 설명이 없습니다.",
      "submittedOnDailyBy": {
        "_id": "6310ff34bc152fa3e810c186",
        "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
        "isPro": false,
        "fullname": "Elizaveta Goncharova",
        "user": "Elizaveta",
        "type": "user"
      },
      "summary": "비젼엔코더는 일반적으로 많은 비젼 토큰을 생성하고 정보가 풍부한 표현을 제공하지만, 계산 부담을 크게 증가시킵니다. 이로 인해, 모든 생성된 토큰이 등가한지, 일부를 버려 계산 비용 감소할 수 있는지의 문제가 제기됩니다. 본 논문에서는, 가치 낮은 특성량은 가치 높은 특성량에서 재구성될 수 있다는 가정에 기초하여 특성량의 유용성을 결정하는 새로운 방법을 제시합니다. 이를 실현하기 위해, 가ン블루・소프마克斯 선택 기능을 가진 자동 엔코더와 통합하여, 정보 가장 풍부한 비젼 토큰만 특정하여 남겨두는 것이 가능합니다. 이 방법에 의한 특성량을 선택한 경우와 랜덤으로 선택한 경우의 LLaVA-NeXT 모델의 성능을 비교하였으며, OCR 기반의 태스크에서 50% 이상의 비젼 컨텍스트를 제거할 수 있음을 확인하였으나, 같은 비율의 특성량을 랜덤으로 버린 경우 모델의 성능이 크게 영향을 받습니다. 또한, 일반적인 영역의 태스크에서 랜덤으로 30%의 토큰을 남겨두었을 때도, 모든 비젼 토큰을 사용했을 때와 같은 성능을 달성합니다. 이러한 결과를 통해, 성능 손실 없이 scalable하고 부하가 낮은 추론을 구현하기 위한 적응적이고 효율적인 다형 타입 프린팅의 유망한 방향을 제시합니다.",
      "upvotes": 17,
      "discussionId": "67e0ffb229682c8065e1c2c6",
      "ai_keywords": [
        "autoencoder",
        "Gumbel-Softmax selection mechanism",
        "feature utility",
        "LLaVA-NeXT model",
        "OCR-based tasks",
        "visual context",
        "performance loss",
        "general-domain tasks",
        "multimodal pruning"
      ]
    },
    "publishedAt": "2025-03-20T15:17:08.000Z",
    "title": "When Less is Enough: Adaptive Token Reduction for Efficient Image\n  Representation",
    "summary": "Vision encoders typically generate a large number of visual tokens, providing\ninformation-rich representations but significantly increasing computational\ndemands. This raises the question of whether all generated tokens are equally\nvaluable or if some of them can be discarded to reduce computational costs\nwithout compromising quality. In this paper, we introduce a new method for\ndetermining feature utility based on the idea that less valuable features can\nbe reconstructed from more valuable ones. We implement this concept by\nintegrating an autoencoder with a Gumbel-Softmax selection mechanism, that\nallows identifying and retaining only the most informative visual tokens. To\nvalidate our approach, we compared the performance of the LLaVA-NeXT model,\nusing features selected by our method with randomly selected features. We found\nthat on OCR-based tasks, more than 50% of the visual context can be removed\nwith minimal performance loss, whereas randomly discarding the same proportion\nof features significantly affects the model capabilities. Furthermore, in\ngeneral-domain tasks, even randomly retaining only 30% of tokens achieves\nperformance comparable to using the full set of visual tokens. Our results\nhighlight a promising direction towards adaptive and efficient multimodal\npruning that facilitates scalable and low-overhead inference without\ncompromising performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16660.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310ff34bc152fa3e810c186",
      "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
      "fullname": "Elizaveta Goncharova",
      "name": "Elizaveta",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17352",
      "authors": [
        {
          "_id": "67e0bcc9e5fa0da84e121032",
          "name": "Yihe Deng",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121033",
          "name": "Hritik Bansal",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121034",
          "name": "Fan Yin",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121035",
          "name": "Nanyun Peng",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121036",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121037",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:52:43.000Z",
      "submittedOnDailyAt": "2025-03-24T00:31:06.884Z",
      "title": "OpenVLThinker: 복잡한 시각 언어 이유론의 초기 탐색\n\n그렇다면, 반복적인 자기 개선을 통해",
      "submittedOnDailyBy": {
        "_id": "642f4c789b2484d7d8551a93",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
        "isPro": true,
        "fullname": "Yihe Deng",
        "user": "ydeng9",
        "type": "user"
      },
      "summary": "최근의 DeepSeek-R1의 발전은 대규모 언어 모델(LLMs)에서 복잡한 논리론 능력을 구현할 수 있는 것을 보여주고 있으며, 특히 自覚証明과 自覚補正 같은 복잡한 행동을 보상이 보장되는 RL에 의해 실현할 수 있다는 점을 보여줍니다. 또한, AIME 같은 어려운 작업에서 모델의 성능을 크게 향상시킬 수 있다는 것을 보여주고 있습니다. 이러한 발견에 힘입어,本研究는 이러한 논리론 능력이 대규모 비전 언어 모델(LVLMs)에 성공적으로 통합될 수 있는지 조사하고, 그 영향을 평가하고 있습니다. 이러한 조사는 경량 데이터에 대한 지도 학습(SFT)과 반복적인 강화 학습(RL)의 활용을 고려하고 있습니다. 처음으로, 논리론 능력은 다양한 비전 데이터 세트에서 생성된 고품질 이미지의 논리론 단계를 사용하여 pure-text R1 모델로부터 문서화됩니다. 다음으로, 반복적인 RL 훈련은 각 반복의 RL 향상 모델이 다음 라운드의 SFT 데이터 세트를 생성함으로써 논리론 스킬을 발전시킵니다. 이 반복적인 프로세스는 MathVista, MathVerse, MathVision 등 어려운 벤치마크에서 논리론 성능을 지속적으로 향상시키는 LVLM인 OpenVLThinker를 얻으며, 우리 전략의 강력한 비전 언어 논리론의 가능성을 보여줍니다. 코드, 모델과 데이터는 https://github.com/yihedeng9/OpenVLThinker에 액세스할 수 있습니다.",
      "upvotes": 11,
      "discussionId": "67e0bccae5fa0da84e121079",
      "projectPage": "https://yihe-deng.notion.site/openvlthinker",
      "githubRepo": "https://github.com/yihedeng9/OpenVLThinker",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "verifiable rewards",
        "large language models (LLMs)",
        "self-verification",
        "self-correction",
        "large vision-language models (LVLMs)",
        "multimodal reasoning tasks",
        "supervised fine-tuning (SFT)",
        "lightweight training data",
        "reasoning steps",
        "high-quality captions",
        "diversity",
        "visual datasets",
        "iterative process",
        "OpenVLThinker",
        "reasoning performance",
        "challenging benchmarks",
        "MathVista",
        "MathVerse",
        "MathVision",
        "robust vision-language reasoning"
      ]
    },
    "publishedAt": "2025-03-21T13:52:43.000Z",
    "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement",
    "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17352.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642f4c789b2484d7d8551a93",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
      "fullname": "Yihe Deng",
      "name": "ydeng9",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17126",
      "authors": [
        {
          "_id": "67e0beb474fc794321fb4ad7",
          "name": "John Joon Young Chung",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4ad8",
          "name": "Vishakh Padmakumar",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4ad9",
          "name": "Melissa Roemmele",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4ada",
          "name": "Yuqian Sun",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4adb",
          "name": "Max Kreminski",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T13:21:45.000Z",
      "submittedOnDailyAt": "2025-03-24T00:39:24.717Z",
      "title": "大规模 언어 모델 훈련 후의 다양한 창의적인 입력을 위한 조정",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "시나리오 웜 팜 태스크는 단일의 정확한 답이 없기 때문에, 이러한 태스크에 대한 훈련된 대형 언어 모델(LLMs)은 다양한 유효한 출력을 생성할 수 있어야 한다. 그러나, LLM의 훈련 후 처리는 주로 생성 품질의 향상을 우선시하고 출력의 다양성을 촉진하지는 않는다. 따라서, 시나리오 웜 팜의 생성에서 출력의 다양성과 질을 모두 촉진하는 훈련 후 접근 방식을 검토하였다. 우리의 핵심 아이디어는 훈련 샘플과 같은 Prompt를 가진 모든 샘플과의 차이 정도(deviation)를 훈련 목표에 포함하고, 희귀한 고품질의 인스턴스로 학습을 촉진하는 것입니다. 우리의 접근 방식은 직접 선호 최적화(DPO)와 확률 비율 선호 최적화(ORPO)에 적용되었으며, 훈련된 모델의 출력의 다양성을 촉진하는 동시에 질을 최소한으로 줄일 수 있음을 보여주었습니다. 우리의 최선 모델(8B 파라미터)은 인간이 만든 데이터 세트와 같은 다양성을 달성할 수 있으며, 조사한 최고의 지시 모델(GPT-4o와 DeepSeek-R1)과 같은 수준의 출력 품질을 유지할 수 있음을 보여주었습니다. 또한, 우리의 접근 방식은 인간 평가, 삭제 시험, 기존의 다양성 접근 방식(DivPO)과 비교하여 더욱 검증되었습니다.",
      "upvotes": 9,
      "discussionId": "67e0beb574fc794321fb4b04",
      "ai_keywords": [
        "direct preference optimization (DPO)",
        "odds ratio preference optimization (ORPO)",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-03-21T09:21:45.000Z",
    "title": "Modifying Large Language Model Post-Training for Diverse Creative\n  Writing",
    "summary": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6443
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16867",
      "authors": [
        {
          "_id": "67e0d31f151ca9ed92898fff",
          "user": {
            "_id": "63bbf071d8d676a2299c7d0b",
            "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
            "isPro": false,
            "fullname": "Guan",
            "user": "Guan123",
            "type": "user"
          },
          "name": "Kaisi Guan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:06:57.786Z",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899000",
          "name": "Zhengfeng Lai",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899001",
          "name": "Yuchong Sun",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899002",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899003",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899004",
          "name": "Kieran Liu",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899005",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899006",
          "name": "Ruihua Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T05:52:50.000Z",
      "submittedOnDailyAt": "2025-03-24T07:22:50.935Z",
      "title": "ETVA: 문자에서 동영상의 어레이멘트의 평가에 의한 미세한 문제 생성 및 답안",
      "submittedOnDailyBy": {
        "_id": "63bbf071d8d676a2299c7d0b",
        "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
        "isPro": false,
        "fullname": "Guan",
        "user": "Guan123",
        "type": "user"
      },
      "summary": "텍스트 프로노트와 생성된 비디오의 의미적인 대응 관계를 정밀하게 평가하는 것은 텍스트에서 비디오(T2V) 생성에서 어려운 문제입니다. 기존의 텍스트에서 비디오의 대응 관계 평가 지표인 CLIPScore는 세부적인 대응을 가지고 있지 않고, 간략한 점수를 생성하여 인간적인 취향에 맞지 않습니다. 이러한 한계를 해결하기 위해, 우리는 ETVA(텍스트에서 비디오의 대응 관계를 평가하는 새로운 방법)를 제안합니다. ETVA는 세부적인 질의 생성과 답을 통해 대응 관계를 평가하는 새로운 방법입니다. 먼저, 다변수 시스템이 프로노트를 의미적인 스케인 그래프로 해석하고 원자의 질의를 생성합니다. 다음으로, 지식 확장된 다단계 추론 프레임워크를 설계하고, 협력 LLM은 물리의 법칙과 같은 관계 지식을 검색하고, 비디오 LLM은 생성된 질의를 다단계 추론 메커니즘을 통해 답합니다. 확장된 실험에 따라, ETVA는 샴먼의 상관 계수 58.47을 달성하며, 인간 판단에 대한 더 높은 상관을 나타내며, 기존의 지표와 비교하여 31.0의 상관계수를 초과합니다. 또한, ETVA는 15개의 기존의 텍스트에서 비디오 모델의 주요 능력과 한계를 특정하고, 다음 세대의 T2V 생성을 위한 길을 열어줍니다.",
      "upvotes": 5,
      "discussionId": "67e0d322151ca9ed928990c0",
      "projectPage": "https://eftv-eval.github.io/etva-eval/",
      "ai_keywords": [
        "semantic alignment",
        "Text-to-Video (T2V) Generation",
        "CLIPScore",
        "fine-grained alignment details",
        "multi-agent system",
        "semantic scene graphs",
        "atomic questions",
        "knowledge-augmented",
        "multi-stage reasoning framework",
        "auxiliary LLM",
        "common-sense knowledge",
        "video LLM",
        "multi-stage reasoning mechanism",
        "Spearman's correlation coefficient",
        "benchmark",
        "text-to-video models"
      ]
    },
    "publishedAt": "2025-03-21T01:52:50.000Z",
    "title": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question\n  Generation and Answering",
    "summary": "Precisely evaluating semantic alignment between text prompts and generated\nvideos remains a challenge in Text-to-Video (T2V) Generation. Existing\ntext-to-video alignment metrics like CLIPScore only generate coarse-grained\nscores without fine-grained alignment details, failing to align with human\npreference. To address this limitation, we propose ETVA, a novel Evaluation\nmethod of Text-to-Video Alignment via fine-grained question generation and\nanswering. First, a multi-agent system parses prompts into semantic scene\ngraphs to generate atomic questions. Then we design a knowledge-augmented\nmulti-stage reasoning framework for question answering, where an auxiliary LLM\nfirst retrieves relevant common-sense knowledge (e.g., physical laws), and then\nvideo LLM answers the generated questions through a multi-stage reasoning\nmechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's\ncorrelation coefficient of 58.47, showing a much higher correlation with human\njudgment than existing metrics which attain only 31.0. We also construct a\ncomprehensive benchmark specifically designed for text-to-video alignment\nevaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10\ncategories. Through a systematic evaluation of 15 existing text-to-video\nmodels, we identify their key capabilities and limitations, paving the way for\nnext-generation T2V generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16867.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbf071d8d676a2299c7d0b",
      "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
      "fullname": "Guan",
      "name": "Guan123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16549",
      "authors": [
        {
          "_id": "67e0d11eb04d9e83682f222a",
          "name": "Felix Chen",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222b",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:06:59.943Z",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222c",
          "name": "Yunqiu Xu",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222d",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222e",
          "name": "Jun Cen",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222f",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f2230",
          "name": "Zeying Huang",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f2231",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T11:46:19.000Z",
      "submittedOnDailyAt": "2025-03-24T01:59:23.638Z",
      "title": "MathFlow: 시각화 수학 문제의 MLLM 인식 흐름을 높여라.",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "다모달 대언어 모델(MLLMs)는 다양한 태스크에서 놀라운 성과를 보여주지만, 시각적 수리 문제 해결에 있어서 그 잠재적인 가능성은 완전히 드러내지 못합니다, 특히 그림의 정확한 인식과 해석에 대해. 인간의 일반적인 프로세스를 가장 유사하게 하는 시각적 인식 능력이, 그림에서 의미 있는 정보를 추출하는 데 중요하다고 가정하고 있습니다. 이 가정을 검증하기 위해 FlowVerse라는 상세한 벤치마크를 개발했습니다. 이 벤치마크는 문제를 해결할 때 사용하는 모든 정보를 4개의 구성 요소로 분류하고, 이들을 6개의 문제 버전으로 조합하여 평가하는 것입니다. FlowVerse에서 초기한 결과를 통해, 현재의 MLLM은 그림에서 중요한 정보를 추출하고, 그 기반으로 복잡한 추론을 수행하는 데는 매우 제한적이며, 이를 보완하기 위해 MathFlow라는 모듈화된 문제 해결 파이프라인을 도입했습니다. 이 파이프라인은 시각적 인식과 추론을 다른 단계로 나누고, 각각 독립적으로 최적화하는 것입니다. 현재의 MLLM의 시각적 인식의 한계에 맞게, MathFlow-P-7B라는 전문적인 인식 모델을 훈련했습니다. 실험 결과를 통해, MathFlow-P-7B와 폐원 또는 오픈소스의 추론 모델을 조합할 때, 상당한 성능 향상을 나타냅니다. 이는 MathFlow 파이프라인의 효과와 다양한 추론 프레임워크의 호환성을 보여주고 있습니다. FlowVerse 벤치마크와 코드는 https://github.com/MathFlow-zju/MathFlow에 공개되어 있습니다.",
      "upvotes": 4,
      "discussionId": "67e0d11fb04d9e83682f2267",
      "githubRepo": "https://github.com/MathFlow-zju/MathFlow",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "visual mathematical problem-solving",
        "diagrams",
        "perception capabilities",
        "inference processes",
        "FlowVerse",
        "problem-solving",
        "essential information",
        "reasoned property",
        "MathFlow",
        "problem-solving pipeline",
        "perception model",
        "MathFlow-P-7B",
        "closed-source inference models",
        "open-source inference models"
      ]
    },
    "publishedAt": "2025-03-19T07:46:19.000Z",
    "title": "MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical\n  Problems",
    "summary": "Despite impressive performance across diverse tasks, Multimodal Large\nLanguage Models (MLLMs) have yet to fully demonstrate their potential in visual\nmathematical problem-solving, particularly in accurately perceiving and\ninterpreting diagrams. Inspired by typical processes of humans, we hypothesize\nthat the perception capabilities to extract meaningful information from\ndiagrams is crucial, as it directly impacts subsequent inference processes. To\nvalidate this hypothesis, we developed FlowVerse, a comprehensive benchmark\nthat categorizes all information used during problem-solving into four\ncomponents, which are then combined into six problem versions for evaluation.\nOur preliminary results on FlowVerse reveal that existing MLLMs exhibit\nsubstantial limitations when extracting essential information and reasoned\nproperty from diagrams and performing complex reasoning based on these visual\ninputs. In response, we introduce MathFlow, a modular problem-solving pipeline\nthat decouples perception and inference into distinct stages, thereby\noptimizing each independently. Given the perceptual limitations observed in\ncurrent MLLMs, we trained MathFlow-P-7B as a dedicated perception model.\nExperimental results indicate that MathFlow-P-7B yields substantial performance\ngains when integrated with various closed-source and open-source inference\nmodels. This demonstrates the effectiveness of the MathFlow pipeline and its\ncompatibility to diverse inference frameworks. The FlowVerse benchmark and code\nare available at https://github.com/MathFlow-zju/MathFlow.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16549.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16983",
      "authors": [
        {
          "_id": "67e0c303ff27a08e3896134a",
          "name": "Xu Zhang",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134b",
          "name": "Hao Zhou",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134c",
          "name": "Haoming Qin",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134d",
          "name": "Xiaobin Lu",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134e",
          "name": "Jiaxing Yan",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134f",
          "name": "Guanzhong Wang",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e38961350",
          "name": "Zeyu Chen",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e38961351",
          "name": "Yi Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T09:48:00.000Z",
      "submittedOnDailyAt": "2025-03-24T00:58:01.102Z",
      "title": "「비디오 디퓨전 모델의 기능을 광범위하게 제어하기 위한 기능」",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "텍스트 태그 생성의 발전으로, 비디오 생성 연구에서 정밀한 공간 시간 속성에 대한 높은 정확성과 유연성을 구현하는 것은 어려운 과제입니다. 이러한 제한을 해결하기 위해, 새로운 프레임워크 VCtrl (또는 PP-VCtrl이라고 불리며)를 소개합니다. 이 프레임워크는 일관된 가이드라인을 제공하며, 사전 학습된 비디오 디퓨션 모델에 대한 정밀한 제어가 가능합니다. VCtrl은 Canny 에지, 분할 마스크, 히마ン 키 포인트 등 다양한 사용자 지정 제어 신호를 변경하지 않도록 하여, 일반화된 조건 모듈에 의해 사전 학습된 비디오 디퓨션 모델에 일관된 구성을 합니다. 또한, 일관된 제어 신호의 인코딩 파이프라인과 희소한 잔차 연결 구조를 설계하여, 효율적으로 제어 표현을 구성합니다. 세부적인 실험 및 인간 평가에 따라, VCtrl은 제어 가능성과 생성 품질의 향상을 실현합니다. 소스 코드와 사전 학습 모델은 PaddlePaddle 프레임워크를 사용하여 공개적으로 제공되며, 다음 URL에서 사용 가능합니다: http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.",
      "upvotes": 3,
      "discussionId": "67e0c306ff27a08e38961428",
      "ai_keywords": [
        "VCtrl",
        "PP-VCtrl",
        "fine-grained control",
        "pre-trained video diffusion models",
        "conditional module",
        "Canny edges",
        "segmentation masks",
        "human keypoints",
        "unified control signal encoding pipeline",
        "sparse residual connection mechanism",
        "controllability",
        "PaddlePaddle",
        "PaddleMIX",
        "ppdiffusers"
      ]
    },
    "publishedAt": "2025-03-21T05:48:00.000Z",
    "title": "Enabling Versatile Controls for Video Diffusion Models",
    "summary": "Despite substantial progress in text-to-video generation, achieving precise\nand flexible control over fine-grained spatiotemporal attributes remains a\nsignificant unresolved challenge in video generation research. To address these\nlimitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework\ndesigned to enable fine-grained control over pre-trained video diffusion models\nin a unified manner. VCtrl integrates diverse user-specified control\nsignals-such as Canny edges, segmentation masks, and human keypoints-into\npretrained video diffusion models via a generalizable conditional module\ncapable of uniformly encoding multiple types of auxiliary signals without\nmodifying the underlying generator. Additionally, we design a unified control\nsignal encoding pipeline and a sparse residual connection mechanism to\nefficiently incorporate control representations. Comprehensive experiments and\nhuman evaluations demonstrate that VCtrl effectively enhances controllability\nand generation quality. The source code and pre-trained models are publicly\navailable and implemented using the PaddlePaddle framework at\nhttp://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16983.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6443
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16921",
      "authors": [
        {
          "_id": "67e0bb1665e294ad989334ea",
          "name": "Lingfan Zhang",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334eb",
          "name": "Chen Liu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ec",
          "name": "Chengming Xu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ed",
          "name": "Kai Hu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ee",
          "name": "Donghao Luo",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ef",
          "name": "Chengjie Wang",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334f0",
          "name": "Yanwei Fu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334f1",
          "name": "Yuan Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T07:33:44.000Z",
      "submittedOnDailyAt": "2025-03-24T00:24:41.729Z",
      "title": "다소자에게 대한 인식을 위한 적응적인 DPO를 통해 확산 모델을 조정하는 시기: 정책이 분리됨",
      "submittedOnDailyBy": {
        "_id": "652fab9d04a34a9282bf29d6",
        "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
        "isPro": false,
        "fullname": "Chengming Xu",
        "user": "ChengmingX",
        "type": "user"
      },
      "summary": "최근, 이미지 생성 분야에서, 특히 모델을 일반적인 인간 취향에 맞게 미세 조정하는 방법들에서 눈에 띄는 발전이 보입니다. 본 논문에서는, 이미지 생성 과정에서 취향 데이터의 중요한 역할을 조사하고, 특히 Diffusion-DPO와 그 후속적인 변경의 배경에서의 중요성을 설명합니다. 우리는, 이미지 생성에 있어서 일반적인 인간 취향에 대한 복잡성을 조사하고, 이러한 취향의 주관적 특성과 취향 데이터 세트에서 다수의 샘플에 의한 문제점을 밝혀냅니다. 피루트 실험을 통해 다수의 샘플의 존재와 모델의 성능에 미치는 악의적인 영향을 보여주었습니다. 우리는 다수의 샘플에 대한 관심 있는 메트릭을 DPO의 목표 함수로 통합하는 새로운 접근법인 Adaptive-DPO를 제안합니다. 이 메트릭은 내부의 신뢰도와 외부의 안정성을 포함하여 다수의 샘플과 다수의 샘플을 구분합니다. 우리는 Adaptive-DPO 손실 함수를 소개하고, 이 함수가 DPO 손실을 두 가지 방법으로 개선함을 설명합니다: 다수의 라벨 학습을 강화하면서 다수의 샘플의 부정적인 영향을 완화합니다. 우리의 실험은, 합성된 다수의 데이터와 실제 세계의 취향 데이터의 효과적인 처리를 보여주고, 이미지 생성 태스크에서 더 효과적인 훈련 메소드의 개발에 길을 열어줍니다.",
      "upvotes": 3,
      "discussionId": "67e0bb1a65e294ad9893361c",
      "ai_keywords": [
        "diffusion models",
        "Diffusion-DPO",
        "adaptive-DPO",
        "intra-annotator confidence",
        "inter-annotator stability",
        "DPO objective",
        "Adaptive-DPO loss function"
      ]
    },
    "publishedAt": "2025-03-21T03:33:44.000Z",
    "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware\n  Adaptive DPO",
    "summary": "In recent years, the field of image generation has witnessed significant\nadvancements, particularly in fine-tuning methods that align models with\nuniversal human preferences. This paper explores the critical role of\npreference data in the training process of diffusion models, particularly in\nthe context of Diffusion-DPO and its subsequent adaptations. We investigate the\ncomplexities surrounding universal human preferences in image generation,\nhighlighting the subjective nature of these preferences and the challenges\nposed by minority samples in preference datasets. Through pilot experiments, we\ndemonstrate the existence of minority samples and their detrimental effects on\nmodel performance. We propose Adaptive-DPO -- a novel approach that\nincorporates a minority-instance-aware metric into the DPO objective. This\nmetric, which includes intra-annotator confidence and inter-annotator\nstability, distinguishes between majority and minority samples. We introduce an\nAdaptive-DPO loss function which improves the DPO loss in two ways: enhancing\nthe model's learning of majority labels while mitigating the negative impact of\nminority samples. Our experiments demonstrate that this method effectively\nhandles both synthetic minority data and real-world preference data, paving the\nway for more effective training methodologies in image generation tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652fab9d04a34a9282bf29d6",
      "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
      "fullname": "Chengming Xu",
      "name": "ChengmingX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16025",
      "authors": [
        {
          "_id": "67dd02594aa37abf77af416b",
          "user": {
            "_id": "63eb8b1113a3eb9b0dc89d8c",
            "avatarUrl": "/avatars/d9cb7bdf4f3d2218f7d84120a00054bb.svg",
            "isPro": false,
            "fullname": "Yair Shpitzer",
            "user": "yairshp",
            "type": "user"
          },
          "name": "Yair Shpitzer",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-21T11:40:09.783Z",
          "hidden": false
        },
        {
          "_id": "67dd02594aa37abf77af416c",
          "name": "Gal Chechik",
          "hidden": false
        },
        {
          "_id": "67dd02594aa37abf77af416d",
          "name": "Idan Schwartz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T10:45:04.000Z",
      "submittedOnDailyAt": "2025-03-24T08:16:33.033Z",
      "title": "싱글 이미지 일러스트레이션 주제 주도 생성 및 편집",
      "submittedOnDailyBy": {
        "_id": "63eb8b1113a3eb9b0dc89d8c",
        "avatarUrl": "/avatars/d9cb7bdf4f3d2218f7d84120a00054bb.svg",
        "isPro": false,
        "fullname": "Yair Shpitzer",
        "user": "yairshp",
        "type": "user"
      },
      "summary": "개인화 이미지 생성 및 편집은 주제 이미지가 적은 경우 또는 한 장의 이미지만 있는 경우 특히 어려워진다. 개인화의 일반적인 접근 방식에는 개념 학습이 있으며, 주제를 기존 모델에 통합할 수 있지만, 주제 이미지의 수가 적으면 이미지 품질이 급격히 저하된다. 품질을 향상시키기 위해 사전 학습을 수행할 수 있지만, 이것은 생성을 학습 분포에 제한하고 시간이 걸린다. 단일 이미지에서 개인화된 이미지 생성 및 편집은 어려운 과제이며, 아직 해결되지 않은 문제다. 여기서는, 입력된 주제 이미지와의 유사도 스코어를 최적화하기 위해 새로운, 학습 필요 없는 접근 방식을 소개한다. 구체적으로는, SISO는 유사도 손실에 기반하여 이미지를 연속적으로 생성하고 모델을 최적화하며, 만족스러운 수준의 유사도를 달성하기까지 반복적으로 수행하며, 어떤 이미지 생성기에도 플러그인 및 패턴의 최적화를 가능하게 한다. SISO는 개인의 다양한 데이터 세트를 사용하여 이미지 편집 및 이미지 생성의 두 가지 태스크로 평가되었으며, 이미지 품질, 주제의 정확성, 배경의 보존에 있어서 기존 방법보다 유의미한 향상을 나타냈다.",
      "upvotes": 3,
      "discussionId": "67dd025f4aa37abf77af42db",
      "projectPage": "https://siso-paper.github.io/",
      "githubRepo": "https://github.com/yairshp/SISO",
      "ai_keywords": [
        "concept learning",
        "encoder",
        "pre-training",
        "similarity score",
        "iterative generation",
        "model optimization",
        "plug-and-play optimization"
      ]
    },
    "publishedAt": "2025-03-20T06:45:04.000Z",
    "title": "Single Image Iterative Subject-driven Generation and Editing",
    "summary": "Personalizing image generation and editing is particularly challenging when\nwe only have a few images of the subject, or even a single image. A common\napproach to personalization is concept learning, which can integrate the\nsubject into existing models relatively quickly, but produces images whose\nquality tends to deteriorate quickly when the number of subject images is\nsmall. Quality can be improved by pre-training an encoder, but training\nrestricts generation to the training distribution, and is time consuming. It is\nstill an open hard challenge to personalize image generation and editing from a\nsingle image without training. Here, we present SISO, a novel, training-free\napproach based on optimizing a similarity score with an input subject image.\nMore specifically, SISO iteratively generates images and optimizes the model\nbased on loss of similarity with the given subject image until a satisfactory\nlevel of similarity is achieved, allowing plug-and-play optimization to any\nimage generator. We evaluated SISO in two tasks, image editing and image\ngeneration, using a diverse data set of personal subjects, and demonstrate\nsignificant improvements over existing methods in image quality, subject\nfidelity, and background preservation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16025.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63eb8b1113a3eb9b0dc89d8c",
      "avatarUrl": "/avatars/d9cb7bdf4f3d2218f7d84120a00054bb.svg",
      "fullname": "Yair Shpitzer",
      "name": "yairshp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12821",
      "authors": [
        {
          "_id": "67e0cc432bbf376bdb18623b",
          "user": {
            "_id": "66aca01e33f6b27979856f6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
            "isPro": false,
            "fullname": "Mingyang Song",
            "user": "hitsmy",
            "type": "user"
          },
          "name": "Mingyang Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:07:01.991Z",
          "hidden": false
        },
        {
          "_id": "67e0cc432bbf376bdb18623c",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67e0cc432bbf376bdb18623d",
          "name": "Jiawei Zhou",
          "hidden": false
        },
        {
          "_id": "67e0cc432bbf376bdb18623e",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T05:01:09.000Z",
      "submittedOnDailyAt": "2025-03-24T02:58:13.280Z",
      "title": "전체적으로 균형있는 표현을 목표로 하는 대규모 시각 언어 모델에서 적응적인 데이터 조정을 통해",
      "submittedOnDailyBy": {
        "_id": "66aca01e33f6b27979856f6f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
        "isPro": false,
        "fullname": "Mingyang Song",
        "user": "hitsmy",
        "type": "user"
      },
      "summary": "대규모 시각 언어 모델(LVLMs)은 시각 이해와 언어 생성의 통합에 상당한 발전을 달성했습니다. 이 성공에 더불어, LVLMs의 훈련 데이터는 데이터 분포가 매우 불균형한 '장 꼬리(LT) 문제'에 직면하여 있습니다. 기존 연구들은 주로 CLIP나 ViT와 같은 전통적인 VLM 아키텍처나 특정 작업(예: 인식 및 분류)에 초점을 맞추었습니다. 그러나 LVLM(예: LLaVA)과 더 일반적인 작업(예: 시각 문제 해결 및 시각 추론)에 대한 연구는 아직 문제로 남아 있습니다. 본 논문에서는 LVLM의 LT 문제를 심도 있게 분석하고, 머리 개념의 과도 표현과 꼬리 개념의 부족 표현이 두 가지 핵심적인 원인으로 인식되었습니다. 이 관점에서, 우리는 데이터 재배치(DR)와 데이터 합성(DS)를 포함하는 두 단계 구조를 구성하는 적응적인 데이터 정밀화 프레임워크(ADR)를 제안합니다. DR 단계에서는 엔티티 분포에 기반하여 부적절한 데이터를 적응적으로 재배치하고, DS 단계에서는 노이즈DIFFIJENSHIP PROBESTIC MODEL(DDPMs)과 희귀한 이미지를 활용하여 부족한 부분을 보완합니다. 11개의 테스트 벤치마크를 통해 제안된 ADR은 훈련 데이터의 장 꼬리 문제를 효과적으로 해결하고, LLaVA 1.5의 평균 성능을 약 4.36% 높일 수 있습니다.",
      "upvotes": 3,
      "discussionId": "67e0cc442bbf376bdb186293",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "Long-Tail (LT) problems",
        "CLIP",
        "ViT",
        "LLaVA",
        "Visual Question Answering",
        "Visual Reasoning",
        "Adaptive Data Refinement Framework (ADR)",
        "Data Rebalancing (DR)",
        "Data Synthesis (DS)",
        "Denoising Diffusion Probabilistic Models (DDPMs)"
      ]
    },
    "publishedAt": "2025-03-17T01:01:09.000Z",
    "title": "From Head to Tail: Towards Balanced Representation in Large\n  Vision-Language Models through Adaptive Data Calibration",
    "summary": "Large Vision-Language Models (LVLMs) have achieved significant progress in\ncombining visual comprehension with language generation. Despite this success,\nthe training data of LVLMs still suffers from Long-Tail (LT) problems, where\nthe data distribution is highly imbalanced. Previous works have mainly focused\non traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as\nrecognition and classification. Nevertheless, the exploration of LVLM (e.g.\nLLaVA) and more general tasks (e.g. Visual Question Answering and Visual\nReasoning) remains under-explored. In this paper, we first conduct an in-depth\nanalysis of the LT issues in LVLMs and identify two core causes: the\noverrepresentation of head concepts and the underrepresentation of tail\nconcepts. Based on the above observation, we propose an Adaptive\nData Refinement Framework (ADR), which\nconsists of two stages: Data Rebalancing (DR)\nand Data Synthesis (DS). In the DR stage, we\nadaptively rebalance the redundant data based on entity distributions, while in\nthe DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and\nscarce images to supplement underrepresented portions. Through comprehensive\nevaluations across eleven benchmarks, our proposed ADR effectively mitigates\nthe long-tail problem in the training data, improving the average performance\nof LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66aca01e33f6b27979856f6f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
      "fullname": "Mingyang Song",
      "name": "hitsmy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17287",
      "authors": [
        {
          "_id": "67e0bfcc8fb92b0edaa78dc0",
          "name": "Mingyang Song",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc1",
          "name": "Mao Zheng",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc2",
          "name": "Zheng Li",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc3",
          "name": "Wenjie Yang",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc4",
          "name": "Xuan Luo",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc5",
          "name": "Yue Pan",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc6",
          "name": "Feng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T16:35:31.000Z",
      "submittedOnDailyAt": "2025-03-24T00:43:41.116Z",
      "title": "FastCuRL: 발전된 컨텍스트를 활용한 커리큘럼 강화 학습\n  R1에서부터 적응성 높은 이유 구조 모델의 효율적인 훈련 확장",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "이 논문에서는 간단하고 효율적인 코스트루리닝 강화학습 접근법인 \\textsc{FastCuRL}을 제안합니다. 이 접근법은 긴 문맥 확장 전략을 사용하여 R1 같은 이유론 모델의 학습 효율성을 향상시키고, 긴 문맥을 포함한 복잡한 이유론 태스크를 해결하기 위해 설계되었습니다. 특히, 1.5B 파라미터의 언어 모델을 사용합니다.\n\n\\textsc{FastCuRL}은 주로 두 가지 단계로 구성됩니다: 입력 프롬프트의 길이에 따라 훈련 데이터를 3개의 레벨로 분할하고 긴 문맥 확장 훈련을 수행합니다. 구체적으로, 첫 번째 단계는 입력 프롬프트의 길이에 따라 원의 훈련 데이터를 3개의 레벨로 분할하고, 두 번째 단계는 긴 문맥 확장 컨텍스트 윈도우를 사용하여 분할된 훈련 데이터를 사용하여 이유론 모델을 훈련시킵니다.\n\n실험 결과를 통해 \\textsc{FastCuRL}-1.5B-Preview는 5개의 데이터셋 (MATH 500, AIME 2024, AMC 2023, Minerva Math, OlympiadBench) 모두에 대해 DeepScaleR-1.5B-Preview를 초과하며, 50%의 훈련 단계를 사용합니다. 또한, \\textsc{FastCuRL}-1.5B-Preview의 모든 훈련 단계는 8개의 노드에서 수행되었습니다.",
      "upvotes": 2,
      "discussionId": "67e0bfcd8fb92b0edaa78e17",
      "githubRepo": "https://github.com/nick7nlp/FastCuRL",
      "ai_keywords": [
        "Curriculum Reinforcement Learning",
        "context window",
        "reinforcement learning",
        "training efficiency",
        "R1-like reasoning models",
        "long chain-of-thought",
        "rationales",
        "length-aware training",
        "training data segmentation",
        "context window extension",
        "progressively increasing context window length",
        "DeepScaleR-1.5B-Preview",
        "MATH 500",
        "AIME 2024",
        "AMC 2023",
        "Minerva Math",
        "OlympiadBench",
        "training steps"
      ]
    },
    "publishedAt": "2025-03-21T12:35:31.000Z",
    "title": "FastCuRL: Curriculum Reinforcement Learning with Progressive Context\n  Extension for Efficient Training R1-like Reasoning Models",
    "summary": "In this paper, we propose \\textsc{FastCuRL}, a simple yet efficient\nCurriculum Reinforcement Learning approach with\ncontext window extending strategy to accelerate the reinforcement learning\ntraining efficiency for R1-like reasoning models while enhancing their\nperformance in tackling complex reasoning tasks with long chain-of-thought\nrationales, particularly with a 1.5B parameter language model.\n\\textsc{FastCuRL} consists of two main procedures: length-aware\ntraining data segmentation and context window extension training. Specifically,\nthe former first splits the original training data into three different levels\nby the input prompt length, and then the latter leverages segmented training\ndatasets with a progressively increasing context window length to train the\nreasoning model. Experimental results demonstrate that\n\\textsc{FastCuRL}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview\nacross all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva\nMath, and OlympiadBench) while only utilizing 50\\% of training steps.\nFurthermore, all training stages for FastCuRL-1.5B-Preview are completed using\njust a single node with 8 GPUs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17287.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6443
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17069",
      "authors": [
        {
          "_id": "67e0ba47753cfd5e438d3814",
          "user": {
            "_id": "63be636387619d1458c2e8e0",
            "avatarUrl": "/avatars/83e14735760c5cadd5341ebcb4cf9556.svg",
            "isPro": false,
            "fullname": "SHI YUFEI",
            "user": "Master-Shi",
            "type": "user"
          },
          "name": "Yufei Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:10:50.772Z",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3815",
          "name": "Weilong Yan",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3816",
          "name": "Gang Xu",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3817",
          "name": "Yumeng Li",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3818",
          "name": "Yuchen Li",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3819",
          "name": "Zhenxi Li",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d381a",
          "name": "Fei Richard Yu",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d381b",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d381c",
          "name": "Si Yong Yeo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T11:50:06.000Z",
      "submittedOnDailyAt": "2025-03-24T07:28:42.136Z",
      "title": "PVChat: 개인화 비디오 채팅（단회 학습）",
      "submittedOnDailyBy": {
        "_id": "63be636387619d1458c2e8e0",
        "avatarUrl": "/avatars/83e14735760c5cadd5341ebcb4cf9556.svg",
        "isPro": false,
        "fullname": "SHI YUFEI",
        "user": "Master-Shi",
        "type": "user"
      },
      "summary": "Video large language models (ViLLMs) are specialized in general movie understanding, successfully recognizing activities like talking and eating, but struggle with understanding identity information, such as \"Wilson is receiving chemotherapy\" or \"Tom is arguing with Sarah,\" which limits their application in smart homes and environments. To address this limitation, we propose PVChat, a one-step learning framework for personalized ViLLMs. This is the first personalized ViLLM that provides answers to topic-related questions from a single movie for each individual. Our approach optimizes a MoH (Mixture-of-Heads) enhanced ViLLM using a synthetically enhanced movie QA dataset and leverages advanced image-based movie learning strategies. Specifically, we introduce an automated data augmentation pipeline to search for positive and negative examples with identity information from existing movie corpora, generating diverse learning datasets with four types of QA: existence, appearance, action, and location. To enhance topic-specific learning, we propose ReLU Routing MoH attention mechanisms and introduce two new objectives: 1) Smooth nearest neighbor normalization using exponential distance scaling, and 2) balanced attention routing through Head Activation Enhancement. Finally, we adopt a two-step learning strategy, transitioning from pre-training on images to fine-tuning on movies, enabling progressive learning from static attributes to dynamic expressions. PVChat is evaluated on diverse datasets including medical scanners, TV series, animations, and real-world food uploads, demonstrating advanced levels of personalized feature understanding and showing superior performance compared to other ViLLMs after learning from a single movie.",
      "upvotes": 2,
      "discussionId": "67e0ba4b753cfd5e438d391e",
      "ai_keywords": [
        "ViLLMs",
        "one-shot learning",
        "PVChat",
        "Mixture-of-Heads (MoH)",
        "image-to-video learning",
        "automated augmentation pipeline",
        "identity-preserving positive samples",
        "hard negatives",
        "QA types",
        "ReLU Routing MoH attention mechanism",
        "Smooth Proximity Regularization",
        "Head Activation Enhancement",
        "two-stage training strategy",
        "image pre-training",
        "video fine-tuning",
        "personalized feature understanding",
        "state-of-the-art ViLLMs"
      ]
    },
    "publishedAt": "2025-03-21T07:50:06.000Z",
    "title": "PVChat: Personalized Video Chat with One-Shot Learning",
    "summary": "Video large language models (ViLLMs) excel in general video understanding,\ne.g., recognizing activities like talking and eating, but struggle with\nidentity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or\n\"Tom is discussing with Sarah\", limiting their applicability in smart\nhealthcare and smart home environments. To address this limitation, we propose\na one-shot learning framework PVChat, the first personalized ViLLM that enables\nsubject-aware question answering (QA) from a single video for each subject. Our\napproach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically\naugmented video-QA dataset, leveraging a progressive image-to-video learning\nstrategy. Specifically, we introduce an automated augmentation pipeline that\nsynthesizes identity-preserving positive samples and retrieves hard negatives\nfrom existing video corpora, generating a diverse training dataset with four QA\ntypes: existence, appearance, action, and location inquiries. To enhance\nsubject-specific learning, we propose a ReLU Routing MoH attention mechanism,\nalongside two novel objectives: (1) Smooth Proximity Regularization for\nprogressive learning through exponential distance scaling and (2) Head\nActivation Enhancement for balanced attention routing. Finally, we adopt a\ntwo-stage training strategy, transitioning from image pre-training to video\nfine-tuning, enabling a gradual learning process from static attributes to\ndynamic representations. We evaluate PVChat on diverse datasets covering\nmedical scenarios, TV series, anime, and real-world footage, demonstrating its\nsuperiority in personalized feature understanding after learning from a single\nvideo, compared to state-of-the-art ViLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17069.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63be636387619d1458c2e8e0",
      "avatarUrl": "/avatars/83e14735760c5cadd5341ebcb4cf9556.svg",
      "fullname": "SHI YUFEI",
      "name": "Master-Shi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11572",
      "authors": [
        {
          "_id": "67e0530f151ca9ed9265e949",
          "user": {
            "_id": "64c5d832d68946edad7d5536",
            "avatarUrl": "/avatars/5d38217d4ab99cdcf53c52661b8baa0d.svg",
            "isPro": false,
            "fullname": "Messi Lee",
            "user": "l048596",
            "type": "user"
          },
          "name": "Messi H. J. Lee",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-23T18:30:22.473Z",
          "hidden": false
        },
        {
          "_id": "67e0530f151ca9ed9265e94a",
          "name": "Calvin K. Lai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T16:40:02.000Z",
      "submittedOnDailyAt": "2025-03-24T02:15:38.327Z",
      "title": "이 문장은 한국어로 번역하면 다음과 같습니다:\n\n\"이론 모델에서 보이는 인풋 바이어스 같은 패턴\"",
      "submittedOnDailyBy": {
        "_id": "64c5d832d68946edad7d5536",
        "avatarUrl": "/avatars/5d38217d4ab99cdcf53c52661b8baa0d.svg",
        "isPro": false,
        "fullname": "Messi Lee",
        "user": "l048596",
        "type": "user"
      },
      "summary": "インプリットバイアス는 관찰, 판단, 행동을 형성하는 자동적이고 비유동적인锁鎖적 프로세스를 가리키는 용어입니다. 선행 연구에서, 대규모 언어 모델(LLMs)에 대한 인풋트 바이어스의 연구는 인간에 대한 연구 방법과 다르며, 주로 모델의 출력을 중심으로 진행되었습니다. 모델의 과정을 조사하기 위해, 이유 모델 인풋트 아소시언스 테스트(RM-IAT)라는 방법을 제안합니다. 이 방법을 사용하면, 이유 모델은 관련 정보를 비교하면서, 관련없는 정보를 처리할 때 모듈 수가 증가하는 것을 확인했습니다. 이러한 발견은 AI 시스템이 정보 처리에 있어서 인간의 인풋트 바이어스를 유사하게 지닌 패턴을 가지고 있음을 보여줍니다. 이러한 인풋트 바이어스의 패턴이 현실적인 응용 프로그램에서의 기능에 어떤 영향을 미치는지에 대한 논의를 진행합니다.",
      "upvotes": 2,
      "discussionId": "67e05311151ca9ed9265e9c1",
      "githubRepo": "https://github.com/lee-messi/RM-IAT",
      "ai_keywords": [
        "Reasoning Model Implicit Association Test (RM-IAT)",
        "reasoning models",
        "LLMs (Large Language Models)",
        "tokens",
        "association-incompatible information",
        "association-compatible information",
        "implicit bias-like patterns"
      ]
    },
    "publishedAt": "2025-03-14T12:40:02.000Z",
    "title": "Implicit Bias-Like Patterns in Reasoning Models",
    "summary": "Implicit bias refers to automatic or spontaneous mental processes that shape\nperceptions, judgments, and behaviors. Previous research examining `implicit\nbias' in large language models (LLMs) has often approached the phenomenon\ndifferently than how it is studied in humans by focusing primarily on model\noutputs rather than on model processing. To examine model processing, we\npresent a method called the Reasoning Model Implicit Association Test (RM-IAT)\nfor studying implicit bias-like patterns in reasoning models: LLMs that employ\nstep-by-step reasoning to solve complex tasks. Using this method, we find that\nreasoning models require more tokens when processing association-incompatible\ninformation compared to association-compatible information. These findings\nsuggest AI systems harbor patterns in processing information that are analogous\nto human implicit bias. We consider the implications of these implicit\nbias-like patterns for their deployment in real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11572.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c5d832d68946edad7d5536",
      "avatarUrl": "/avatars/5d38217d4ab99cdcf53c52661b8baa0d.svg",
      "fullname": "Messi Lee",
      "name": "l048596",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]