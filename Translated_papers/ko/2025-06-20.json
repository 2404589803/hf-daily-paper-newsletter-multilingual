[
  {
    "paper": {
      "id": "2506.14965",
      "authors": [
        {
          "_id": "68538be099bf39f9665c79b9",
          "name": "Zhoujun Cheng",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79ba",
          "name": "Shibo Hao",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bb",
          "name": "Tianyang Liu",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bc",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bd",
          "name": "Yutao Xie",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79be",
          "name": "Feng Yao",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bf",
          "name": "Yuexin Bian",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c0",
          "name": "Yonghao Zhuang",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c1",
          "name": "Nilabjo Dey",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c2",
          "name": "Yuheng Zha",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c3",
          "name": "Yi Gu",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c4",
          "name": "Kun Zhou",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c5",
          "name": "Yuqi Wang",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c6",
          "name": "Yuan Li",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c7",
          "name": "Richard Fan",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c8",
          "name": "Jianshu She",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c9",
          "name": "Chengqian Gao",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79ca",
          "name": "Abulhair Saparov",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cb",
          "name": "Haonan Li",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cc",
          "name": "Taylor W. Killian",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cd",
          "name": "Mikhail Yurochkin",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79ce",
          "name": "Zhengzhong Liu",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cf",
          "name": "Eric P. Xing",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79d0",
          "name": "Zhiting Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T20:24:00.000Z",
      "submittedOnDailyAt": "2025-06-20T06:25:47.447Z",
      "title": "안녕히 계세요, LLM의 추론을 복원하기 위한 강화학습의 관점에서의 재검토",
      "submittedOnDailyBy": {
        "_id": "6083902e1e36b13a64497d91",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6083902e1e36b13a64497d91/h4rGHMn2c6z5GesF0F6VU.png",
        "isPro": false,
        "fullname": "cheng",
        "user": "zhoujun",
        "type": "user"
      },
      "summary": "강화학습(RL)은 대규모 언어 모델(LLM)의 논리론을 개선하는 잠재적인 접근법으로 등장하지만, 많은 오픈된 노력을 수학과 코드에 집중하여 일반적인 논리론에 대한 광범위한 적용을 이해하는 데 제한되어 있습니다. 문제점은 다양한 논리론 분야에서 신뢰할 수 있는 Scalable RL 보상 신호의 부족입니다. 우리는 Guru라는 수집된 RL 논리론 코퍼스를 소개합니다. 이 코퍼스는 수학, 코드, 과학, 로직, 시뮬레이션, 테이블의 6가지 논리론 분야를 확장하고, 신뢰성과 효율성을 보장하기 위해 영역专門적인 보상 설계, 제거, 필터링을 통해 구축되었습니다. Guru를 기반으로, 우리는 LLM의 논리론에서 기존의 발견을 체계적으로 재검토하고, 영역 간에 뚜렷한 변화를 관찰했습니다. 예를 들어, 기존 연구는 RL이 주로 사전 학습 모델에서 기존의 지식을 추출하는 것을 보여주고 있지만, 우리의 결과를 통해 더 복잡한 패턴을 나타냅니다: 사전 학습 시 자주 볼 수 있는 영역(수학, 코드, 과학)은 크로스 도메인의 RL 학습에서 쉽게 이점을 얻을 수 있지만, 사전 학습에 제한된 범위의 영역(로직, 시뮬레이션, 테이블)은 영역专門적인 학습이 의미 있는 성능 향상을 실현하는 데 필요합니다. 최종적으로, 우리는 Guru-7B과 Guru-32B라는 두 가지 모델을 소개합니다. 이 모델은 공개 데이터에서 RL 학습된 오픈 모델 중 가장 선진적인 성능을 갖으며, 6가지 논리론 분야의 17가지 태스크 평가 시스템에서 7.9%와 6.7%를 초월했습니다. 또한, 우리의 모델은 기초 모델의 Pass@k 성능을 효과적으로 향상시키고, 특히 사전 학습 데이터에 등장하지 않는 복잡한 태스크에 특히 효과적입니다. 우리는 데이터, 모델, 학습 및 평가 코드를 릴리스하고, 일반적인 논리론을 실현하기 위한 프레임워크를 제공합니다. https://github.com/LLM360/Reasoning360",
      "upvotes": 6,
      "discussionId": "68538be099bf39f9665c79d1",
      "ai_summary": "Guru, a diverse RL reasoning corpus, highlights domain-specific training needs and demonstrates improved performance in complex tasks for RL-enhanced LLMs.",
      "ai_keywords": [
        "reinforcement learning",
        "large language model",
        "RL reasoning",
        "curated RL reasoning corpus",
        "domain-specific reward design",
        "dereplication",
        "filtering",
        "cross-domain RL training",
        "in-domain training",
        "Guru-7B",
        "Guru-32B",
        "Pass@k performance"
      ]
    },
    "publishedAt": "2025-06-17T16:24:00.000Z",
    "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective",
    "summary": "Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14965.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6083902e1e36b13a64497d91",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6083902e1e36b13a64497d91/h4rGHMn2c6z5GesF0F6VU.png",
      "fullname": "cheng",
      "name": "zhoujun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15154",
      "authors": [
        {
          "_id": "685393f499bf39f9665c79db",
          "name": "Anuradha Chopra",
          "hidden": false
        },
        {
          "_id": "685393f499bf39f9665c79dc",
          "name": "Abhinaba Roy",
          "hidden": false
        },
        {
          "_id": "685393f499bf39f9665c79dd",
          "name": "Dorien Herremans",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655431b2997379e9b0999d23/zAwOQ2zY8-8Eu7bUhFURQ.png"
      ],
      "publishedAt": "2025-06-18T05:51:36.000Z",
      "submittedOnDailyAt": "2025-06-20T04:32:59.096Z",
      "title": "SonicVerse: 음악특징량에 기반한 다태스크 학습에 의한 캡션\n\n(Note: The original text \"SonicVerse: 音楽特徴量に基づく多タスク学習によるキャプション\" was translated to Korean as \"SonicVerse: 음악특징량에 기반한 다태스크 학습에 의한 캡션\". This translation maintains the professional and accurate nature of the original text.)",
      "submittedOnDailyBy": {
        "_id": "655431b2997379e9b0999d23",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
        "isPro": false,
        "fullname": "Dorien Herremans",
        "user": "dorienh",
        "type": "user"
      },
      "summary": "詳細한 캡처팅이 음악의 특징을 정확히 반영하고 있는 점으로 음악 데이터베이스를 풍부하게 만들 수 있으며, 음악 AI 연구도 발전할 수 있습니다. 본 논문에서는, 캡처팅 생성과 음악 특징 검출 태스크(예: 키 검출, 보카르 검출 등)을 통합한 다임업 음악 캡처팅 모델 SonicVerse를 통해, 낮은 수준의 음향 세부 사항과 높은 수준의 음악 속성을 직접 파악할 수 있는 방법을 보고합니다. 주요 기여는 음성 입력을 언어 토큰으로 변환하는 프로젝션 기반의 아키텍처가 있으며, 음악 특징을 특정하기 위한 특수한 보조기를 사용합니다. 이러한 보조기의 출력도 언어 토큰으로 변환되어 캡처팅 입력을 강화합니다. 이 프레임워크는, 풍부한 설명을 포함하는 짧은 음악 프레임을 생성함으로써, 긴 음악 작품에 대한 시간 정보를 포함한 상세한 설명을 생성할 수 있습니다. 모델의 훈련에는 MIRFLEX라는 모듈화된 음악 특징 추출기를 사용하여 MusicBench 데이터 세트를 음악 특징에 주석하여 음음, 캡처팅과 음악 특징 데이터 쌍을 생성했습니다. 실험 결과를 통해 이러한 특징의 삽입이 생성된 캡처팅의 품질과 세부 사항이 향상되는 것을 알 수 있습니다.",
      "upvotes": 2,
      "discussionId": "685393f599bf39f9665c79de",
      "ai_summary": "SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.",
      "ai_keywords": [
        "multi-task music captioning",
        "SonicVerse",
        "caption generation",
        "key detection",
        "vocals detection",
        "projection-based architecture",
        "language tokens",
        "auxiliary heads",
        "time-informed descriptions",
        "large-language model",
        "MusicBench dataset",
        "MIRFLEX",
        "music feature extractor"
      ]
    },
    "publishedAt": "2025-06-18T01:51:36.000Z",
    "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
    "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655431b2997379e9b0999d23/zAwOQ2zY8-8Eu7bUhFURQ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15154.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "655431b2997379e9b0999d23",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
      "fullname": "Dorien Herremans",
      "name": "dorienh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09827",
      "authors": [
        {
          "_id": "685519bb4f1add9d4c5c5cbd",
          "user": {
            "_id": "61a24fc72101184cfb29c965",
            "avatarUrl": "/avatars/e32aa61016caef50de28c16b30196799.svg",
            "isPro": false,
            "fullname": "Christoph Schuhmann",
            "user": "ChristophSchuhmann",
            "type": "user"
          },
          "name": "Christoph Schuhmann",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-20T08:20:12.243Z",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cbe",
          "name": "Robert Kaczmarczyk",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cbf",
          "name": "Gollam Rabby",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc0",
          "user": {
            "_id": "62e7dd4036a8e8a82700041c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
            "isPro": false,
            "fullname": "Felix Friedrich",
            "user": "felfri",
            "type": "user"
          },
          "name": "Felix Friedrich",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-20T08:57:36.090Z",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc1",
          "name": "Maurice Kraus",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc2",
          "name": "Kourosh Nadi",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc3",
          "name": "Huu Nguyen",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc4",
          "name": "Kristian Kersting",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc5",
          "name": "Sören Auer",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62e7dd4036a8e8a82700041c/tvkMYrIKiGhbIVAuaxvlt.png"
      ],
      "publishedAt": "2025-06-11T15:06:59.000Z",
      "submittedOnDailyAt": "2025-06-20T06:53:47.262Z",
      "title": "EmoNet-Voice: 대화의 감정 검출의 미세 수준에서 전문가가 확인한 기준점",
      "submittedOnDailyBy": {
        "_id": "62e7dd4036a8e8a82700041c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
        "isPro": false,
        "fullname": "Felix Friedrich",
        "user": "felfri",
        "type": "user"
      },
      "summary": "テキストから話音への変換モデルの進歩は、AIシステム의感情理解能力を評価するために強力的なベンチマークが必要となります。現在の話音感情認識（SER）データセットは、感情の細分化、プライバシーの懸念、または演技された表現に依存していることが多いです。本論文では、話音感情検出のための新しいリソースEmoNet-Voiceを紹介します。EmoNet-Voice Bigは、大規模な事前学習データセットです（11人の声、40種類の感情、4言語、4,500時間以上の話音を扱います）、EmoNet-Voice Benchは、人間の専門家の注釈を含む新しいベンチマークデータセットです。EmoNet-Voiceは、40種類の感情の細かいスペクトルでSERモデルを評価することを目的としています。最先端の声生成を活用し、演員が特定の感情を引き起こす場面を模倣した合成音声サンプルをカレーリングしました。重要なことに、心理学の専門家による厳密な検証を行い、認知された強度ラベルを割り当てました。この合成的、プライバシーを保護するアプローチで、現在のデータセットに欠けていた敏感な感情状態を含めることができます。最後に、Empathic Insight Voiceモデルを紹介します。これらのモデルは、高い人間の専門家との高い協議率で、話音感情認識の新たな標準を設定します。現在のモデルのランドスケープでの評価で、高アローレーション感情（例えば怒り）が低アローレーション状態（例えば集中）よりも容易に検出されやすいことなど、有價値な発見を示しています。",
      "upvotes": 1,
      "discussionId": "685519bb4f1add9d4c5c5cc6",
      "ai_summary": "EmoNet-Voice, a new resource with large pre-training and benchmark datasets, advances speech emotion recognition by offering fine-grained emotion evaluation with synthetic, privacy-preserving audio.",
      "ai_keywords": [
        "speech emotion recognition",
        "SER",
        "EmoNet-Voice",
        "EmoNet-Voice Big",
        "EmoNet-Voice Bench",
        "human expert annotations",
        "synthetic audio snippets",
        "psychology experts",
        "high-arousal emotions",
        "low-arousal states",
        "Empathic Insight Voice models"
      ]
    },
    "publishedAt": "2025-06-11T11:06:59.000Z",
    "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech\n  Emotion Detection",
    "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62e7dd4036a8e8a82700041c/tvkMYrIKiGhbIVAuaxvlt.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09827.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e7dd4036a8e8a82700041c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
      "fullname": "Felix Friedrich",
      "name": "felfri",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.14837",
      "authors": [
        {
          "_id": "6854ea7a7bc8d012d4ca998d",
          "name": "Chengzhi Xu",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca998e",
          "name": "Yuyang Wang",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca998f",
          "user": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "WaltonFuture",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-20T08:57:40.628Z",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca9990",
          "name": "Lichao Sun",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca9991",
          "name": "Weiran Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-15T14:10:16.000Z",
      "submittedOnDailyAt": "2025-06-20T03:28:58.032Z",
      "title": "구조화된 인스톰션을 통한 차트에서 코드 생성 개선 반복 개선",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "최근, 다모달 대언어 모델(MLLMs)는 강력한 시각 이해 능력을 통해 연구의 주목을 받고 있습니다. 이들은 다양한 시각 작업에서 놀라운 성과를 거두지만, 차트에서 코드의 생성에 대해서는 최선의 성능을 달성하지 못합니다. 이 작업은 주어진 차트를 재현하기 위해 실행 가능한 코드를 생성하는 것이 요구되며, 시각 이해의 정확성과 시각 요소를 구조화된 코드로 정확한 번역이 필요합니다. MLLMs이 이러한 복잡한 작업에 직접적으로Prompt를 받을 경우, 불만족스러운 결과를 많이 가져옵니다. 이러한 도전에 대응하기 위해, 우리는 {ChartIR}을 제안합니다. 이는 구조화된 명령에 기반한 이터레이션적인 리팩토링 방법이다. 먼저, 우리는 두 가지 작업을 구분합니다: 시각 이해와 코드 번역. 시각 이해 부분을 수행하기 위해, 우리는 두 가지 구조화된 명령을 설계합니다: 설명과 차이. 설명 명령은 참조 차트의 시각 요소를捉捉, 차이 명령은 참조 차트와 생성된 차트 사이의 차이를 특징화합니다. 이러한 명령은 시각 특징을 언어 표현으로 변환하고, 그 후의 코드 번역 프로세스를 지원합니다. 다음으로, 우리는 전체 차트 생성 파이프라인을 두 단계로 분해합니다: 초기 코드 생성과 이터레이션적인 리팩토링, 최종적인 출력을 단계적으로 향상시키는 것이 가능합니다. 실험 결과를 통해, 다른 방법과 비교하여, 우리의 방법은 오픈 소스 모델 Qwen2-VL과 클로즈드 소스 모델 GPT-4o에서도 더 우수한 성능을 거두었습니다.",
      "upvotes": 0,
      "discussionId": "6854ea7a7bc8d012d4ca9992",
      "ai_summary": "ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "visual understanding",
        "code translation",
        "structured instruction",
        "description instruction",
        "difference instruction",
        "language representations",
        "initial code generation",
        "iterative refinement",
        "Qwen2-VL",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-06-15T10:10:16.000Z",
    "title": "Improved Iterative Refinement for Chart-to-Code Generation via\n  Structured Instruction",
    "summary": "Recently, multimodal large language models (MLLMs) have attracted increasing\nresearch attention due to their powerful visual understanding capabilities.\nWhile they have achieved impressive results on various vision tasks, their\nperformance on chart-to-code generation remains suboptimal. This task requires\nMLLMs to generate executable code that can reproduce a given chart, demanding\nnot only precise visual understanding but also accurate translation of visual\nelements into structured code. Directly prompting MLLMs to perform this complex\ntask often yields unsatisfactory results. To address this challenge, we propose\n{ChartIR}, an iterative refinement method based on structured instruction.\nFirst, we distinguish two tasks: visual understanding and code translation. To\naccomplish the visual understanding component, we design two types of\nstructured instructions: description and difference. The description\ninstruction captures the visual elements of the reference chart, while the\ndifference instruction characterizes the discrepancies between the reference\nchart and the generated chart. These instructions effectively transform visual\nfeatures into language representations, thereby facilitating the subsequent\ncode translation process. Second, we decompose the overall chart generation\npipeline into two stages: initial code generation and iterative refinement,\nenabling progressive enhancement of the final output. Experimental results show\nthat, compared to other method, our method achieves superior performance on\nboth the open-source model Qwen2-VL and the closed-source model GPT-4o.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14837.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  }
]