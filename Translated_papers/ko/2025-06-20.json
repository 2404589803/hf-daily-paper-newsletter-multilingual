[
  {
    "paper": {
      "id": "2506.14965",
      "authors": [
        {
          "_id": "68538be099bf39f9665c79b9",
          "name": "Zhoujun Cheng",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79ba",
          "name": "Shibo Hao",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bb",
          "name": "Tianyang Liu",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bc",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bd",
          "name": "Yutao Xie",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79be",
          "name": "Feng Yao",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bf",
          "name": "Yuexin Bian",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c0",
          "name": "Yonghao Zhuang",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c1",
          "name": "Nilabjo Dey",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c2",
          "name": "Yuheng Zha",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c3",
          "name": "Yi Gu",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c4",
          "name": "Kun Zhou",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c5",
          "name": "Yuqi Wang",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c6",
          "name": "Yuan Li",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c7",
          "name": "Richard Fan",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c8",
          "name": "Jianshu She",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c9",
          "name": "Chengqian Gao",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79ca",
          "name": "Abulhair Saparov",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cb",
          "name": "Haonan Li",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cc",
          "name": "Taylor W. Killian",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cd",
          "name": "Mikhail Yurochkin",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79ce",
          "name": "Zhengzhong Liu",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cf",
          "name": "Eric P. Xing",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79d0",
          "name": "Zhiting Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T20:24:00.000Z",
      "submittedOnDailyAt": "2025-06-20T06:25:47.447Z",
      "title": "再見せられたLLM의 논리론리에 대한 리노ー스포러밍 학습의 재고 - 사용자 정의 영역에서의 시각\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "6083902e1e36b13a64497d91",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6083902e1e36b13a64497d91/h4rGHMn2c6z5GesF0F6VU.png",
        "isPro": false,
        "fullname": "cheng",
        "user": "zhoujun",
        "type": "user"
      },
      "summary": "강화학습(RL)은 대규모 언어 모델(LLM)의 논리론을 개선하는 잠재적인 접근 방식으로 등장했지만, 많은 오픈된 노력을 수학과 코드에 집중하여, 일반적인 논리론에 적용할 수 있는 것을 이해하는 데 제한되어 있습니다. 관련없는 논리론 분야에서 신뢰할 수 있는 스케일러블한 RL 보상 신호의 부족이 주요 문제로 남아 있습니다. 우리는 수학, 코드, 과학, 로지ック, 시뮬레이션, 탭의 6가지 논리론 분야를 가로지르는 92K의 증거를 엮은 RL 논리론 코퍼스인 'Guru'를 소개합니다. 이들은 영역별 보상 설계, 제거, 필터링을 통해 신뢰성과 효능성을 보장하고 있습니다. 'Guru'를 기반으로, 우리는 LLM의 논리론에 대한 기존의 RL을 체계적으로 재검토하고, 영역간의 유의미한 변화를 관찰했습니다. 예를 들어, 기존 연구는 RL이 주로 사전 학습 모델에서 이미 있는 지식을 추출하는 것을 주장하지만, 우리의 결과를 통해, 더 복잡한 패턴을 보여줍니다: 사전 학습 시 자주 볼 수 있는 영역(수학, 코드, 과학)은 크로스 도메인의 RL 학습에서 쉽게 이점을 얻을 수 있지만, 사전 학습 시에 제한된 경험의 영역(로지ック, 시뮬레이션, 탭)은 영역 내의 학습이 의미 있는 성능 향상을 실현하는 것이 필요합니다. 최종적으로, 우리는 'Guru-7B'와 'Guru-32B'라는 두 가지 모델을 소개합니다. 이들은 공개 데이터에 기반한 RL 훈련을 받은 오픈 모델 중 가장 선진적인 성능을 달성하고, 6가지 논리론 분야의 17가지 태스크를 평가하는 시스템에서 7.9%와 6.7%를 초과했습니다. 또한, 우리의 모델은 사전 학습 데이터에 나타나지 않는 복잡한 태스크에서, 기본 모델의 Pass@k 성능을 효과적으로 향상시키는 것을 보여주었습니다. 우리는 https://github.com/LLM360/Reasoning360에서 데이터, 모델, 훈련 및 평가 코드를 릴리스하고, 일반적인 논리론을 실현하는 것을 촉진하고 있습니다.",
      "upvotes": 6,
      "discussionId": "68538be099bf39f9665c79d1",
      "ai_summary": "Guru, a diverse RL reasoning corpus, highlights domain-specific training needs and demonstrates improved performance in complex tasks for RL-enhanced LLMs.",
      "ai_keywords": [
        "reinforcement learning",
        "large language model",
        "RL reasoning",
        "curated RL reasoning corpus",
        "domain-specific reward design",
        "dereplication",
        "filtering",
        "cross-domain RL training",
        "in-domain training",
        "Guru-7B",
        "Guru-32B",
        "Pass@k performance"
      ]
    },
    "publishedAt": "2025-06-17T16:24:00.000Z",
    "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective",
    "summary": "Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14965.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6083902e1e36b13a64497d91",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6083902e1e36b13a64497d91/h4rGHMn2c6z5GesF0F6VU.png",
      "fullname": "cheng",
      "name": "zhoujun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15154",
      "authors": [
        {
          "_id": "685393f499bf39f9665c79db",
          "name": "Anuradha Chopra",
          "hidden": false
        },
        {
          "_id": "685393f499bf39f9665c79dc",
          "name": "Abhinaba Roy",
          "hidden": false
        },
        {
          "_id": "685393f499bf39f9665c79dd",
          "name": "Dorien Herremans",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655431b2997379e9b0999d23/zAwOQ2zY8-8Eu7bUhFURQ.png"
      ],
      "publishedAt": "2025-06-18T05:51:36.000Z",
      "submittedOnDailyAt": "2025-06-20T04:32:59.096Z",
      "title": "ソニックバース: 음악특징량에 기반한 다임스 학습에 의한 캡션\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "655431b2997379e9b0999d23",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
        "isPro": false,
        "fullname": "Dorien Herremans",
        "user": "dorienh",
        "type": "user"
      },
      "summary": "상세한 캡처팅이 음악의 특징을 정확히 반영하는 것은 음악 데이터베이스를 풍부하게 만들고 음악 AI의 연구를 추진할 수 있는 방법입니다. 본 논문에서는 캡처팅 생성과 음악 특징 검출 태스크(예: 키 검출, 가수 검출 등)을 통합한 다 태스크 음악 캡처팅 모델인 SonicVerse를 소개합니다. 주요 기여는 음성 입력을 언어 토큰으로 변환하는 프로젝션 기반 아키텍처입니다. 동시에 음악 특징을 특정하기 위해 특수한 보조를 사용합니다. 이러한 보조의 출력도 언어 토큰으로 변환되어 캡처팅 입력을 강화합니다. 이 프레임워크는 짧은 음악 요소에 풍부하고 상세한 캡처팅을 생성하고 긴 음악 작품에서도 시간 정보를 포함한 상세한 설명을 생성할 수 있습니다. 모델의 훈련에는 MIRFLEX(모듈화된 음악 특징 추출기)를 사용하여 MusicBench 데이터 세트를 음악 특징에 注釈하고 쌍을 이루는 음성, 캡처팅, 음악 특징 데이터를 생성했습니다. 실험 결과를 통해 이러한 특징을 가졌을 때 생성되는 캡처팅의 품질과 상세도가 향상된 것을 보여줍니다.",
      "upvotes": 2,
      "discussionId": "685393f599bf39f9665c79de",
      "ai_summary": "SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.",
      "ai_keywords": [
        "multi-task music captioning",
        "SonicVerse",
        "caption generation",
        "key detection",
        "vocals detection",
        "projection-based architecture",
        "language tokens",
        "auxiliary heads",
        "time-informed descriptions",
        "large-language model",
        "MusicBench dataset",
        "MIRFLEX",
        "music feature extractor"
      ]
    },
    "publishedAt": "2025-06-18T01:51:36.000Z",
    "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
    "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655431b2997379e9b0999d23/zAwOQ2zY8-8Eu7bUhFURQ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15154.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "655431b2997379e9b0999d23",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
      "fullname": "Dorien Herremans",
      "name": "dorienh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09827",
      "authors": [
        {
          "_id": "685519bb4f1add9d4c5c5cbd",
          "user": {
            "_id": "61a24fc72101184cfb29c965",
            "avatarUrl": "/avatars/e32aa61016caef50de28c16b30196799.svg",
            "isPro": false,
            "fullname": "Christoph Schuhmann",
            "user": "ChristophSchuhmann",
            "type": "user"
          },
          "name": "Christoph Schuhmann",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-20T08:20:12.243Z",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cbe",
          "name": "Robert Kaczmarczyk",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cbf",
          "name": "Gollam Rabby",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc0",
          "user": {
            "_id": "62e7dd4036a8e8a82700041c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
            "isPro": false,
            "fullname": "Felix Friedrich",
            "user": "felfri",
            "type": "user"
          },
          "name": "Felix Friedrich",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-20T08:57:36.090Z",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc1",
          "name": "Maurice Kraus",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc2",
          "name": "Kourosh Nadi",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc3",
          "name": "Huu Nguyen",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc4",
          "name": "Kristian Kersting",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc5",
          "name": "Sören Auer",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62e7dd4036a8e8a82700041c/tvkMYrIKiGhbIVAuaxvlt.png"
      ],
      "publishedAt": "2025-06-11T15:06:59.000Z",
      "submittedOnDailyAt": "2025-06-20T06:53:47.262Z",
      "title": "EmoNet-Voice: 대화의 감정 검출의 미세 수준에서 전문가들이 확인한 기준점",
      "submittedOnDailyBy": {
        "_id": "62e7dd4036a8e8a82700041c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
        "isPro": false,
        "fullname": "Felix Friedrich",
        "user": "felfri",
        "type": "user"
      },
      "summary": "텍스트 모우드와 음성 생성 모우드의 발전은 AI 시스템의 감정 이해 능력을 평가하기 위해 강력한 벤치마크가 필요하게 되었습니다. 현재의 음성의 감정 인식 데이터 세트는感情의 미세화, 프라이버시의 우려, 또는 연기된 표현에 의존하는 경우가 많습니다. 본 논문에서는 음성의 감정 검출을 위한 새로운 리소스인 EmoNet-Voice를 소개합니다. EmoNet-Voice에는 규모가 큰 사전 학습 데이터 세트인 EmoNet-Voice Big(11인의 목소리, 40종류의 감정, 4언어, 4,500시간 이상의 음성)와 인간의 전문가의 注釈을 포함하는 새로운 벤치마크 데이터 세트인 EmoNet-Voice Bench가 포함되어 있습니다. EmoNet-Voice는 40종류의 감정의 미세한 스펙트럼에서 SER 모우드의 평가를 수행하는 것을 목표로 합니다. 최신의 음성 생성 기술에 활용하여, 연기자가 특정 감정을 불러일으키는 시나리오를 모방하여 합성음성 샘플을 준비했습니다. 중요한 점은 심리학의 전문가에 의해 엄격한 검증을 수행하고, 인지된 강도 레이블을 분배했습니다. 이 합성적이고 프라이버시를 보호하는 접근 방식에서, 기존의 데이터 세트에 부족했던 민감한 감정 상태들을 포함할 수 있습니다. 마지막으로, 인간의 전문가와 높은 협업률로 감정 인식을 수행하는 새로운 기준을 설정하는 Empathic Insight Voice 모우드를 소개합니다. 현재의 모우드의 관점에서의 평가로, 고알로레레이션 감정(예: 분노)이 저알로레레이션 상태(예: 집중)보다 쉽게 검출될 수 있다는 등, 유익한 발견이 얻어졌습니다.",
      "upvotes": 1,
      "discussionId": "685519bb4f1add9d4c5c5cc6",
      "ai_summary": "EmoNet-Voice, a new resource with large pre-training and benchmark datasets, advances speech emotion recognition by offering fine-grained emotion evaluation with synthetic, privacy-preserving audio.",
      "ai_keywords": [
        "speech emotion recognition",
        "SER",
        "EmoNet-Voice",
        "EmoNet-Voice Big",
        "EmoNet-Voice Bench",
        "human expert annotations",
        "synthetic audio snippets",
        "psychology experts",
        "high-arousal emotions",
        "low-arousal states",
        "Empathic Insight Voice models"
      ]
    },
    "publishedAt": "2025-06-11T11:06:59.000Z",
    "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech\n  Emotion Detection",
    "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62e7dd4036a8e8a82700041c/tvkMYrIKiGhbIVAuaxvlt.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09827.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e7dd4036a8e8a82700041c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
      "fullname": "Felix Friedrich",
      "name": "felfri",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.14837",
      "authors": [
        {
          "_id": "6854ea7a7bc8d012d4ca998d",
          "name": "Chengzhi Xu",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca998e",
          "name": "Yuyang Wang",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca998f",
          "user": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "WaltonFuture",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-20T08:57:40.628Z",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca9990",
          "name": "Lichao Sun",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca9991",
          "name": "Weiran Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-15T14:10:16.000Z",
      "submittedOnDailyAt": "2025-06-20T03:28:58.032Z",
      "title": "구조화된 인스톰션을 통한 차트에서 코드 생성 개선 반복 훈련",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "최근, 다모달 대언어 모델(MLLM)은 강력한 시각 이해 능력을 가지고 있어 연구의 주목을 받고 있습니다. 그러나, 이들은 많은 시각 작업에서 놀라운 성과를 거두는 반면, 차트에서 코드의 생성에 있어서는 성능이 최고치를 달성하지 않습니다. 이 작업은 MLLM이 제공된 차트를 구현 가능한 코드로 재현할 수 있는 것을 요구하고, 시각적인 이해의 정확도를 제외한 시각적인 요소를 구조화된 코드에 정확히 번역하는 것이 필요합니다. MLLM이 이러한 복잡한 작업에 직접적으로 Prompt를 받을 수는 없습니다. 이 도전을 해결하기 위해, 우리는 {ChartIR}을 제안합니다. 이는 구조화된 명령에 기반한 이터레이션적인 리핏 메소드입니다. 먼저, 우리는 두 가지 작업을 구분합니다: 시각의 이해와 코드의 번역. 시각의 이해를 수행하기 위해, 우리는 두 가지 구조화된 명령을 설계합니다: 설명과 차이. 설명 명령은 참조 차트의 시각적인 요소를捉捉하고, 차이 명령은 참조 차트와 생성된 차트 사이의 차이를 특징화합니다. 이 명령들은 시각적인 특징을 언어 표현으로 변환하고, 후속의 코드 번역 프로세스를 효과적으로 촉진합니다. 다음으로, 우리는 전체 차트 생성 파이프라인을 두 단계로 분해합니다: 초기 코드 생성과 이터레이션적인 리핏, 최종적인 출력의 진화적인 향상을 가능하게 합니다. 실험 결과는, 다른 방법과 비교하여, 우리의 방법은 오픈 소스 모델 Qwen2-VL과 클로즈드 소스 모델 GPT-4o에서도 더 우수한 성능을 거두는 것을 보여줍니다.",
      "upvotes": 0,
      "discussionId": "6854ea7a7bc8d012d4ca9992",
      "ai_summary": "ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "visual understanding",
        "code translation",
        "structured instruction",
        "description instruction",
        "difference instruction",
        "language representations",
        "initial code generation",
        "iterative refinement",
        "Qwen2-VL",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-06-15T10:10:16.000Z",
    "title": "Improved Iterative Refinement for Chart-to-Code Generation via\n  Structured Instruction",
    "summary": "Recently, multimodal large language models (MLLMs) have attracted increasing\nresearch attention due to their powerful visual understanding capabilities.\nWhile they have achieved impressive results on various vision tasks, their\nperformance on chart-to-code generation remains suboptimal. This task requires\nMLLMs to generate executable code that can reproduce a given chart, demanding\nnot only precise visual understanding but also accurate translation of visual\nelements into structured code. Directly prompting MLLMs to perform this complex\ntask often yields unsatisfactory results. To address this challenge, we propose\n{ChartIR}, an iterative refinement method based on structured instruction.\nFirst, we distinguish two tasks: visual understanding and code translation. To\naccomplish the visual understanding component, we design two types of\nstructured instructions: description and difference. The description\ninstruction captures the visual elements of the reference chart, while the\ndifference instruction characterizes the discrepancies between the reference\nchart and the generated chart. These instructions effectively transform visual\nfeatures into language representations, thereby facilitating the subsequent\ncode translation process. Second, we decompose the overall chart generation\npipeline into two stages: initial code generation and iterative refinement,\nenabling progressive enhancement of the final output. Experimental results show\nthat, compared to other method, our method achieves superior performance on\nboth the open-source model Qwen2-VL and the closed-source model GPT-4o.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14837.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  }
]