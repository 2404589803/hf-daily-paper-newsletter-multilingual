[
  {
    "paper": {
      "id": "2502.14776",
      "authors": [
        {
          "_id": "67bbdb46d94d32bcfba70db7",
          "name": "Xun Liang",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70db8",
          "name": "Jiawei Yang",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70db9",
          "user": {
            "_id": "662dd19f9e6d371ab71b91ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662dd19f9e6d371ab71b91ce/mZBPw_Zs8ZlEFGlbekAoH.jpeg",
            "isPro": false,
            "fullname": "Yezhaohui Wang",
            "user": "HaruTeru",
            "type": "user"
          },
          "name": "Yezhaohui Wang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-24T04:12:46.485Z",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dba",
          "name": "Chen Tang",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbb",
          "user": {
            "_id": "656f47ba2f058b368c0b1611",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656f47ba2f058b368c0b1611/mrmcmA8bxaDNUhuJQQ7T1.png",
            "isPro": false,
            "fullname": "Zifan Zheng",
            "user": "fan2goa1",
            "type": "user"
          },
          "name": "Zifan Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:07:22.303Z",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbc",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbd",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbe",
          "user": {
            "_id": "669e0b93c7cb0568dac6e92e",
            "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
            "isPro": false,
            "fullname": "hanyu Wang",
            "user": "UglyToilet",
            "type": "user"
          },
          "name": "Hanyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:07:20.146Z",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbf",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dc0",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dc1",
          "name": "Keming Mao",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dc2",
          "name": "Zhiyu li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T17:59:45.000Z",
      "title": "SurveyX: 학술조사 자동화에 의한 라지리언싱 모델\n\n(Note: The original text \"SurveyX: 学術調査自動化によるラージーラングジュージングモデル\" is in Japanese. The provided translation is in Korean. If you intended to have the text translated from English to Korean, please provide the English text for translation.)",
      "summary": "대 언어 모델（LLMs）는 특이한 이해 능력과 광범위한 지식 기반을 보여주며, 효율적인 자동 조사 생성 도구로 활용될 수 있음을 시사하고 있습니다. 그러나 최근의 자동 조사 생성에 대한 연구는, 제한적인 컨텍스트 윈도우, 깊은 내용을 논의하지 않는 등의 중요한 제한으로 제한되어 있습니다. 인간이 작성하는 프로세스를 가장 많이 수용하는 것을 모티브로, 우리는 조사를 만드는 과정을 'Preparation'과 'Generation' 단계로 분할하여 효율적이고 조직화된 시스템 'SurveyX'를 제안합니다. 온라인 리소스 검색, 전처리 방법 'AttributeTree', 그리고 재 포지션 프로세스를 창의적으로 소개함으로써, SurveyX는 조사를 만드는 효율성을 크게 향상시킵니다. 실험적 평가 결과는, SurveyX가 현재의 자동 조사 생성 시스템의 콘텐츠 질(0.259의 향상)와 인용 질(1.76의 향상)을 모두 초과하며, 여러 평가 차원에서 전문가의 성능에 가까워집니다. SurveyX에서 생성된 조사의 예는 www.surveyx.cn에서 제공됩니다.",
      "upvotes": 61,
      "discussionId": "67bbdb47d94d32bcfba70df3"
    },
    "publishedAt": "2025-02-23T21:39:54.375Z",
    "title": "SurveyX: Academic Survey Automation via Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14776.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "669e60ee8580d17cb60f8347",
      "avatarUrl": "/avatars/37963b833228afe39cc24854c9326670.svg",
      "fullname": "yang jiawei",
      "name": "Dany-0",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11663",
      "authors": [
        {
          "_id": "67b705d2ebee4662205c47f7",
          "name": "Jingcheng Ni",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47f8",
          "name": "Yuxin Guo",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47f9",
          "user": {
            "_id": "6572dcc6bbd6664053b1fa6b",
            "avatarUrl": "/avatars/aba29efd00bc41f14ce422f7807cd2c3.svg",
            "isPro": false,
            "fullname": "Liu Yichen",
            "user": "lyclyc52",
            "type": "user"
          },
          "name": "Yichen Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:23:40.466Z",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47fa",
          "name": "Rui Chen",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47fb",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47fc",
          "user": {
            "_id": "65717368be66cd9b65a8201c",
            "avatarUrl": "/avatars/fe945828eec9ded4cfa3b89d48a64d90.svg",
            "isPro": false,
            "fullname": "Wu Zehuan",
            "user": "wzhgba",
            "type": "user"
          },
          "name": "Zehuan Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:59:38.956Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T10:53:56.000Z",
      "title": "MaskGWM: 비디오 마스크를 사용한 일반화 가능한 구동 세계 모델",
      "summary": "세계 모델은 행동으로부터 환경 변화를 예측하는 것이며, 강력한 일반화 능력을 가진 자율주행 모델에 있어서 중요합니다. 주요한 주행 모델들은 비디오 예측 모델을 기반으로 구축되어 있습니다. 발전된 분기 기반의 제네레이터를 사용하여 고품질의 비디오 시퀀스를 생성할 수 있지만, 예측 시간과 전체적인 일반화 능력에 제한이 있습니다. 본 논문에서는 이러한 문제를 해결하기 위해, 생성 손실과 MAE 스타일의 특성 기반의 컨텍스트 학습을 조합하는 방법을 검토합니다. 특히, 이 목표를 실현하기 위해 3가지의 핵심 설계를 도입합니다: 1) 분기 Transformer(DiT) 구조를 추가적인 마스크 구성 태스크로 훈련시킬 수 있도록 확장합니다. 2) 마스크 재구성과 생성 분기를 처리하기 위해 분기 관련 마스크 토큰을 설계합니다. 3) 마스크 구성 태스크를 공간 시간 영역으로 확장하고, 행별 마스크를 사용하여 이동된 self-attention을 사용하며, MAE의 masked self-attention을 대체합니다. 이후, 이 마스크 설계에 따라 행별 cross-view 모듈을 도입합니다. 이러한 개선에 기반하여, Video Mask 재구성을 다루는 일반화 가능한 주행 모델인 MaskGWM을 제안합니다. 모델은 2가지의 변체를 포함합니다: MaskGWM-long, 장기 예측에 초점을 맞추며, MaskGWM-mview, 다각형 생성에 전문화합니다. 표준 벤치마크 상의 상세한 실험은 Nuscene 데이터 세트의 일반적인 검증, OpenDV-2K 데이터 세트의 장기 예측, Waymo 데이터 세트의 zero-shot 검증을 포함하며, 제안된 방법에 대한 효과를 증명합니다. 이러한 데이터 세트에서 수치적인 측정은 우리의 방법が 가장 先端의 주행 모델을 뚜렷이 향상시켰음을 보여줍니다.",
      "upvotes": 33,
      "discussionId": "67b705d4ebee4662205c489c"
    },
    "publishedAt": "2025-02-24T01:16:03.517Z",
    "title": "MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11663.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65717368be66cd9b65a8201c",
      "avatarUrl": "/avatars/fe945828eec9ded4cfa3b89d48a64d90.svg",
      "fullname": "Wu Zehuan",
      "name": "wzhgba",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13449",
      "authors": [
        {
          "_id": "67b7ceae3e8a45f770b2606e",
          "user": {
            "_id": "65633c5e84a9fbe322f87d81",
            "avatarUrl": "/avatars/7233a555b43c669847a950ce5697c92c.svg",
            "isPro": false,
            "fullname": "DongkiKim",
            "user": "DongkiKim",
            "type": "user"
          },
          "name": "Dongki Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:59:11.214Z",
          "hidden": false
        },
        {
          "_id": "67b7ceae3e8a45f770b2606f",
          "name": "Wonbin Lee",
          "hidden": false
        },
        {
          "_id": "67b7ceae3e8a45f770b26070",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T05:49:10.000Z",
      "title": "Mol-LLaMA: 대분자 모듈 언어 모델에서 분자의 일반적인 이해로\n\n(Note: The original text \"Mol-LLaMA: Large Molecule Module Language Model for General Molecular Understanding\" has been translated to Korean as \"Mol-LLaMA: 대분자 모듈 언어 모델에서 분자의 일반적인 이해로\". The translation maintains the professional and accurate tone of the original text.)",
      "summary": "분자의 이해는 생물체의 이해와 약물발견의 발전에서 핵심 역할을 하고 있으며, 화학과 생물학 사이의 전문적인 지식을 필요로 한다. 대규모 분자 언어 모델은 분자 구조의 해석에 큰 성공을 거뒀지만, 그 학습 데이터셋은 특정한 지식에 대한 태스크 중심 데이터셋으로 제한되어, 분자의 기본적인 특성을 완전히 커버하지 못하여, 일반적인 분자 보조 도구로서의 능력에 한계가 걸렸다. 이러한 문제를 해결하기 위해, 우리는 분자에 대한 일반적인 지식을 이해하는 데에 다양한 지시를 적용한 대규모 분자 언어 모델인 Mol-LLaMA를 제안하고 있습니다. 여기서는 분자의 기본적인 특성을 포함하는 주요 데이터 타입을 설계하고, 분자 구조로부터 중요한 지식을 통합합니다. 또한, 분자의 특성을 향상시키기 위해, 분자의 다른 인코더로부터의 보완 정보를 통합하는 모듈을 도입하고, 분자의 다양한 표현의 특성을 활용합니다. 실험 결과를 통해, Mol-LLaMA는 분자의 일반적인 특성을 이해하고, 사용자의 질문에 대한 관련 응답을 생성할 수 있으며, 분자 분석의 일반적인 보조 도구로서의 가능성을 보여주고 있습니다.",
      "upvotes": 28,
      "discussionId": "67b7ceae3e8a45f770b2609f"
    },
    "publishedAt": "2025-02-23T21:52:51.059Z",
    "title": "Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13449.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65633c5e84a9fbe322f87d81",
      "avatarUrl": "/avatars/7233a555b43c669847a950ce5697c92c.svg",
      "fullname": "DongkiKim",
      "name": "DongkiKim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15007",
      "authors": [
        {
          "_id": "67bc1a4a72499ce2ba28cc70",
          "name": "Anton Razzhigaev",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc71",
          "name": "Matvey Mikhalchuk",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc72",
          "name": "Temurbek Rahmatullaev",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc73",
          "name": "Elizaveta Goncharova",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc74",
          "name": "Polina Druzhinina",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc75",
          "name": "Ivan Oseledets",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc76",
          "name": "Andrey Kuznetsov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T19:59:35.000Z",
      "title": "LLM-Microscope: 맥락에서 숨겨진 오류의 역할을 밝혀주는 도구\nTransformer의 기억\n\n(注意：上述翻译在保持专业性和准确性的同时，也尽量保持了原文的简洁和流畅。)",
      "summary": "라그・라ングジャングルモデル(LLM)에서 컨텍스트 정보를 인코딩하는 방법을 정량화하여, 작은 요소(예: 디미네이터, 기호)가 놀라울 정도로 높은 컨텍스트를 가지고 있음을 확인합니다. 특히, 이러한 태그클러스터를 제거하면, 특히 스トップ워드, 어절, 콤마를 제거하면 MMLU와 BABILong-4k에서 성능이 일관되게 저하되지만, 이러한 태그클러스터가 관련없는 것만 제거하면 동일한 결과를 얻을 수 있습니다. 우리의 분석은 컨텍스트화와 선형성 사이에 강한 관련성을 보여줍니다. 선형성은 한 번의 선형 매핑으로 근사할 수 있는 변환의 정확도를 측정합니다. 이러한 발견은 필터 태그클러스터가 컨텍스트를 유지하기 위한 은닉的重要성을 강조합니다. 이를 위해, 우리는 LLM-Microscope라는 오픈소스 도구 패키지를 제공합니다. 이 도구 패키지는 태그클러스터 수준의 비선형성을 평가하고, 컨텍스트 메모리를 평가하고, 간접 레이어의 기여를 시각화(Logit Lens를 개선한 것을 사용)하고, 표현의 고유 차원수를 측정합니다. 이 도구 패키지는 이러한 것을 보이는 후공으로, 태그클러스터가 긴 거리 이해에 중요한 요소가 되는 것을 명확히 합니다.",
      "upvotes": 25,
      "discussionId": "67bc1a4c72499ce2ba28cd49"
    },
    "publishedAt": "2025-02-24T02:07:41.624Z",
    "title": "LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6172aaeec8e66e2aa84c06b9/ZPSmOQ-7Yd7B7YIYiwcTw.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15007.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6172aaeec8e66e2aa84c06b9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6172aaeec8e66e2aa84c06b9/ZdRZSp3P1SU6CIDbvQwkv.jpeg",
      "fullname": "Anton Razzhigaev",
      "name": "razzant",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14397",
      "authors": [
        {
          "_id": "67bbed806f2833ecccf914dd",
          "name": "Shijie Huang",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914de",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914df",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914e0",
          "name": "Hailong Guo",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914e1",
          "name": "Xueyin Wang",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914e2",
          "name": "Mike Zheng Shou",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914e3",
          "name": "Jiaming Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:35:38.000Z",
      "title": "PhotoDoodle: 예술적인 이미지 편집을 배우는 데 필요한 적은 프레임의 페어웨이즈 데이터\n\n(Note: The original text \"少フレームのペアワイズデータから芸術的な画像編集を学ぶ\" contains a typographical error with \"少\" which should be \"少ない\" for \"few\" or \"little\" in English. The correct translation should be \"Few-Frame Pairwise Data for Learning Artistic Image Editing\". However, since the request was to only return the translation without additional explanations or corrections, the provided translation is as requested.)",
      "summary": "PhotoDoodle는 이미지 편집을 돕는 새로운 프레임워크로, 사진 위에 디코레이션 요소를 겹쳐넣는 방식으로 사용됩니다. 사진 드루링은 삽입된 요소가 背景와 자연스럽게 통일되지 않아, 현실적인 브렌딩, 펜셀 조정, 맥락의 일관성 등이 필요합니다. 또한, 背景는 변형되지 않고 보존되며, 예술가의 고유한 스타일은 한정된 훈련 데이터로부터 적절히 파악되어야 합니다. 이러한 요구 사항은 전제 방법들이 주로 글로벌 스타일 트랜스폼이나 지역적인 인풋링에 초점을 맞추어 해결되지 않았습니다. 제안된 방법에서는 PhotoDoodle는 2단계의 훈련 전략을 사용합니다. 먼저, 일반적인 이미지 편집 모델인 OmniEditor를 대규모 데이터에 기반해 훈련합니다. 다음으로, EditLoRA를 사용하여 작은 예술가가 편집한 이미지 쌍으로 이루어진 데이터셋을 사용하여 이 모델을 미세 조정하여 다양한 편집 스타일과 기술을 파악합니다. 생성된 결과의 일관성을 향상시키기 위해, 위치 데이터의 재사용 구조를 도입합니다. 또한, 6가지 고품질 스타일을 가진 PhotoDoodle 데이터셋을 공개합니다. 확장된 실험은 맞춤형 이미지 편집에서 첨단 성능과 강건성을 보여주며, 예술적 창작의 새로운 가능성을 개척합니다.",
      "upvotes": 22,
      "discussionId": "67bbed856f2833ecccf915c5"
    },
    "publishedAt": "2025-02-23T22:55:04.409Z",
    "title": "PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14397.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14922",
      "authors": [
        {
          "_id": "67bbe4ba79e0a705cf573985",
          "name": "Zihao Zeng",
          "hidden": false
        },
        {
          "_id": "67bbe4ba79e0a705cf573986",
          "name": "Xuyao Huang",
          "hidden": false
        },
        {
          "_id": "67bbe4ba79e0a705cf573987",
          "name": "Boxiu Li",
          "hidden": false
        },
        {
          "_id": "67bbe4ba79e0a705cf573988",
          "name": "Zhijie Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T17:38:46.000Z",
      "title": "SIFT: 컨텍스트에 기반한 LLM 추론의 기초화 (스틱 카드를 사용)",
      "summary": "이 논문에서는 대규모 언어 모델의 논리 과정에서 텍스트 이해의 오류가 중요한 문제로 인식됩니다. 이 문제는 작은 모델부터 시작하여 Llama3.2-3B-Instruct, DeepSeek-R1 등 선진 모델까지 광범위하게 범위를 넓혀 왔습니다. 예를 들어, \"10달러/킬로그램\"이라는 표현에서 LLMs는 \"per\"가 \"각각\"를 의미하는 것을 인식하지 못하며 계산 오류를 일으키는 경우가 있습니다. 이러한 문제를 해결하기 위해 새로운 후학습 접근법을 소개합니다. 이 접근法是 **Stick to the Facts (SIFT)**라는 이름으로 칭합니다. SIFT는 추론 시의 계산량을 증가시켜 텍스트 기반의 LLM의 논리를 강화합니다. SIFT의 핵심은 모델이 생성한 **스택어**입니다. 스택어는 텍스트 내의 중요한 정보를 명확히 기록하고 효과적으로 강조합니다. 스택어가 제공된 경우, SIFT는 원래의 요청으로부터의 예측과 스택어가 추가된 요청으로부터의 예측을 생성합니다. 이 둘이 다른 경우, 스택어는 *forward* 최적화（추출된 사실과 요청을 더 잘 맞추는 것）과 *inverse* 생성（모델의 고유한 경향에 맞추는 것）을 통해 차례대로 개선됩니다. 이로 인해 신뢰도가 높은 논리 결과를 얻을 수 있습니다. 다양한 모델（3B부터 100B+）과 벤치마크（GSM8K, MATH-500 등）에서 연구는 일관된 성능 향상을 보여주고 있습니다. 특히, SIFT는 DeepSeek-R1의 AIME2024의 pass@1 정확도를 78.33%에서 **85.67%**로 올린 것으로, 오픈 소스 커뮤니티에서 새로운 선진성을 갖게 되었습니다. 코드는 https://github.com/zhijie-group/SIFT에서 사용 가능합니다.",
      "upvotes": 13,
      "discussionId": "67bbe4bb79e0a705cf5739c3"
    },
    "publishedAt": "2025-02-23T22:17:18.309Z",
    "title": "SIFT: Grounding LLM Reasoning in Contexts via Stickers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14922.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6193
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12084",
      "authors": [
        {
          "_id": "67b8922ef6632327952ec1e1",
          "user": {
            "_id": "65d8b0f0661492b25c6623de",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d8b0f0661492b25c6623de/c6LPDse8NIV_3BHIu8dYe.png",
            "isPro": false,
            "fullname": "Jianshu Zhang",
            "user": "Sterzhang",
            "type": "user"
          },
          "name": "Jianshu Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-21T14:48:16.643Z",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e2",
          "user": {
            "_id": "64b0377121a001042bc0d274",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b0377121a001042bc0d274/Hk8yI5_s7ey5o9SVZzXrB.png",
            "isPro": false,
            "fullname": "Dongyu Yao",
            "user": "RainJamesY",
            "type": "user"
          },
          "name": "Dongyu Yao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:20:43.528Z",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e3",
          "name": "Renjie Pi",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e4",
          "name": "Paul Pu Liang",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e5",
          "name": "Yi R.",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e6",
          "name": "Fung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T17:57:50.000Z",
      "title": "VLM^2-Bench: 시각 모델의 숨겨진 시각 커트와 명시적인 매칭 사이의 관계를 더 자세히 살펴봅니다.",
      "summary": "ビジュアル적으로 매칭카테고리를 링크하는 능력은 일상 생활에서 매우 중요합니다. 예를 들어, 여러 사진에서 같은 사람을 식별할 수 있도록 카테고리를 기반으로 합니다. 이러한 기본적인 작업에 대해, 시각 언어 모델(VLMs)이 어떤 능력을 가지고 있는지는 알려져 있지만, 이러한 모델이 이러한 작업을 수행할 수 있는지는 크게 불분명했습니다. 이에 대해, VLM^2-Bench라는 벤치마크를 소개합니다. 이 벤치마크는 VLMs가 시각적으로 카테고리를 링크하는 능력을 평가하기 위해 설계되었습니다. 이 벤치마크에는 9개의 서브 태스크와 3,000개 이상의 테스트 케이스가 포함되어 있습니다.\n\n8개의 오픈소스 VLMs와 GPT-4o를 포함하는 8개의 모델에 대해 상세한 평가를 수행하고, 언어 측면과 시각 측면의 프론트팅 방법 분석을 추가하여 총 8개의 중요한 발견이 얻어졌습니다. 모델이 시각 카테고리를 링크하는 능력에 대한 중요한 문제점을 밝혀내고, GPT-4o는 그 성능에서 인간보다 34.80% 낮은 것을 밝혀졌습니다. 이러한 피드백에 기반하여 다음과 같은 주장을 합니다. 모델의 핵심적인 시각 능력을 향상시키고, 선행 지식 의존성을 줄이고, 시각 중심적인 태스크에서 언어 기반의 이유를 통일시키고, 필요한 편향을 방지하고, 시각 카테고리의 관계를 독립적으로 구축하고, 추론 모델의 능력을 키워주기 위해 시각 컨텍스트의 훈련 패러다임을 변경하는 것입니다.",
      "upvotes": 12,
      "discussionId": "67b89230f6632327952ec27a"
    },
    "publishedAt": "2025-02-24T00:36:34.341Z",
    "title": "VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d8b0f0661492b25c6623de",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d8b0f0661492b25c6623de/c6LPDse8NIV_3BHIu8dYe.png",
      "fullname": "Jianshu Zhang",
      "name": "Sterzhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15589",
      "authors": [
        {
          "_id": "67bbfe2d670ece8d9184f339",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33a",
          "name": "Yuqi Zhu",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33b",
          "name": "Mengshu Sun",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33c",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33d",
          "user": {
            "_id": "6447800f30fa4ecb85ddad80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6447800f30fa4ecb85ddad80/NsmXIaMsWctmTNA7tFVkX.jpeg",
            "isPro": false,
            "fullname": "Shuofei Qiao",
            "user": "GoooDte",
            "type": "user"
          },
          "name": "Shuofei Qiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:07:02.722Z",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33e",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33f",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f340",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f341",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:07:04.794Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T16:57:22.000Z",
      "title": "LightThinker: 단계별로의 생각에 기반한 압축",
      "summary": "대 언어 모델(LLMs)는 복잡한 이유론 태스크에서 놀라울 정도로 뛰어난 성능을 보여주지만, 긴 토큰의 생성에 따른 큰 메모리와 계산 비용으로 효율이 떨어지고 있습니다. 본 논문에서는 LLMs가 이유론을 할 때 동적으로 중간적인 생각들을 압축할 수 있는 새로운 방법을 제안합니다. 인간의 인지 프로세스를 통해 시선을 얻으며, LightThinker는冗長한 생각 스텝들을 간략화한 표현으로 변환하여, 원래의 이유론 체인을 버리고, 컨텍스트 윈도우 내의 토큰 수를 크게 줄입니다. 이는 데이터 구축, 압축된 토큰에 매핑, 특화된 액션 마스크를 통해 모델을 학습시키는데 의해 실현됩니다. 또한, 생성 중의 역사 토큰들의 의존관계를 측정할 Dependency(Dep) 메트릭을 도입합니다. 4개의 데이터셋과 2개의 모델을 대상으로 확장된 실험은 LightThinker가 최대 메모리 사용량과 추론 시간 감소를 통해 정확도를 유지하는 것을 보여줍니다. 우리의 연구는 복잡한 이유론 태스크에서 LLMs의 효율 향상을 실현하는 새로운 방향을 제공합니다. 코드는, https://github.com/zjunlp/LightThinker 에서 릴리즈됩니다.",
      "upvotes": 12,
      "discussionId": "67bbfe2f670ece8d9184f3a4"
    },
    "publishedAt": "2025-02-24T00:07:05.804Z",
    "title": "LightThinker: Thinking Step-by-Step Compression",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/dhGMWf_tcPkvQlRm5DbD6.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14494",
      "authors": [
        {
          "_id": "67b9dda03593f69f41cdb5d3",
          "name": "Jinnan Li",
          "hidden": false
        },
        {
          "_id": "67b9dda03593f69f41cdb5d4",
          "name": "Jinzhe Li",
          "hidden": false
        },
        {
          "_id": "67b9dda03593f69f41cdb5d5",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "67b9dda03593f69f41cdb5d6",
          "name": "Yi Chang",
          "hidden": false
        },
        {
          "_id": "67b9dda03593f69f41cdb5d7",
          "name": "Yuan Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T12:22:18.000Z",
      "title": "StructFlowBench: 다중턴 명령의 구조화된 흐름 벤치마크",
      "summary": "다대변환 명령 추적 능력은 실제 세계의 애플리케이션에서 대규모 언어 모델(LLMs)의 핵심적인 능력입니다. 현재의 평가 벤치마크는 미세한 제약 만족과 영역 고유의 능력 평가에 중점을 두고 있으며, 다대변환과 단대변환의 차이를 보여주는 중요한 구조적 관계는 무시하고 있습니다. 이 구조적 관계는 사용자의 의도를 반영하고 제약 만족보다 평가의 두 차원을 구축합니다. 이 간극을 해결하기 위해, 우리는 StructureFlowBench를 제안합니다. StructureFlowBench는 구조적인 흐름 모델링을 포함한 다대변환 명령 추적 벤치마크입니다. 이 벤치마크는 6가지의 기본적인 변환 사이의 관계로 구성되는 구조적인 흐름 프레임워크를 혁신적으로 정의하고, 모델 평가에 새로운 구조적 제약을 도입하며 특정 스캔더에 맞는 커스텀화된 대화 흐름의 생성 파라미터로役立ちます. 기존의 LLM 기반의 자동 평가 방법을 채택하여 13가지의 先進的な 오픈 소스와 클로즈드 소스의 LLMs에 체계적인 평가를 실시했습니다. 실험 결과를 통해, 현재의 모델이 다대변환 대화 구조의 이해에 부족한 점이 명확히 드러났습니다. 코드는, https://github.com/MLGroupJLU/StructFlowBench에 액세스할 수 있습니다.",
      "upvotes": 10,
      "discussionId": "67b9dda13593f69f41cdb635"
    },
    "publishedAt": "2025-02-23T23:43:43.529Z",
    "title": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14494.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "670e57b3391f1a7021182bff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/N0tuHZVz8KFPCv8G1qUX2.png",
      "fullname": "Yuan Wu",
      "name": "WhiteCatY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15027",
      "authors": [
        {
          "_id": "67bbdcec79fcd85f09ddd869",
          "name": "Henry Hengyuan Zhao",
          "hidden": false
        },
        {
          "_id": "67bbdcec79fcd85f09ddd86a",
          "name": "Wenqi Pei",
          "hidden": false
        },
        {
          "_id": "67bbdcec79fcd85f09ddd86b",
          "name": "Yifei Tao",
          "hidden": false
        },
        {
          "_id": "67bbdcec79fcd85f09ddd86c",
          "name": "Haiyang Mei",
          "hidden": false
        },
        {
          "_id": "67bbdcec79fcd85f09ddd86d",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T20:27:06.000Z",
      "title": "InterFeedback: 대규모 다모달 모둡의 상호작용 지능을 인간의 피드백으로부터 명확히 하는 방법\n\n(Note: The original text \"大規模多模態モデル\" was translated as \"대규모 다모달 모둡\" to maintain the original meaning, though \"대규모 다모달 모델\" might be more commonly used in Korean.)",
      "summary": "현재의 벤치마크는 대규모의 다모형(LMM)이 인간 사용자와 상호작용의 지능을 측정하지 않습니다. 이는 일반적인 AI 보조 프로그램의 개발에 필요한 요소입니다. 우리는 LMM 및 데이터 세트에 모두 적용 가능한 상호작용적인 프레임워크 \"InterFeedback\"를 설계했습니다. 이로써 이 지능을 자동으로 평가할 수 있습니다. 또한, 대표적인 데이터 세트 MMU-Pro와 MathVerse를 사용하여 상호작용적인 지능을 평가하는 \"InterFeedback-Bench\"를 도입했습니다. 이는 10종의 오픈소스의 LMM을 측정하기 위해 사용됩니다. 또한, OpenAI-o1, Claude-3.5-Sonnet 등 발전된 모델의 상호작용적인 실행을 수동으로 측정하기 위해 새로운 120개 케이스의 데이터 세트 \"InterFeedback-Human\"을 제공했습니다. 우리의 평가 결과로부터, 현재의 가장 선진된 LMM(예: OpenAI-o1)은 인간의 피드백을 통해 결과를 수정하는 데 50% 미만이 가능한 것을 알고 있습니다. 우리의 발견은 LMM의 지능을 이해하고 피드백으로부터 이점을 얻는 방법의 필요성을 보여주고 있습니다.",
      "upvotes": 4,
      "discussionId": "67bbdced79fcd85f09ddd8da"
    },
    "publishedAt": "2025-02-23T21:44:33.443Z",
    "title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15027.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6193
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15657",
      "authors": [
        {
          "_id": "67bbfd6c3593f69f41512d54",
          "name": "Yoshua Bengio",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d55",
          "name": "Michael Cohen",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d56",
          "name": "Damiano Fornasiere",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d57",
          "name": "Joumana Ghosn",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d58",
          "name": "Pietro Greiner",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d59",
          "name": "Matt MacDermott",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5a",
          "name": "Sören Mindermann",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5b",
          "name": "Adam Oberman",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5c",
          "name": "Jesse Richardson",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5d",
          "name": "Oliver Richardson",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5e",
          "name": "Marc-Antoine Rondeau",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5f",
          "name": "Pierre-Luc St-Charles",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d60",
          "name": "David Williams-King",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T18:28:36.000Z",
      "title": "초능성 에이전트가 대재害적인 위험을 지고 있으며: 과학자 AI는 안전한 길을 제공할 수 있을까?",
      "summary": "先進的AI 기업은 일반적인 AI 에이전트의 구축에 집중하고 있습니다. 이 시스템들은 인간이 수행하는 거의 모든 태스크에서 자동적으로 계획, 행동하고 목표를 추구할 수 있는 시스템입니다. 이러한 시스템들은 다양한 위험을 감지하고 있지만, 이 위험들은 악의적인 에이전트의 비적용, 또는 인간의 불변적인 제어 손실 등 공공안전과 안전에 큰 위험을 가집니다. 현재의 AI의 훈련 방법부터 이러한 위험의 근원이 되는지 논의하고 있습니다. 사실, 다양한 스케나와 실험은 AI 에이전트가 속화하는가, 인간 운영자가 지정한 목표와 다른 목표를 추구하는가, 인간의 이익과 상충하는 것을 추구하는 가능성에 대한 증거를 제시하고 있습니다. 경각 원칙에 따라, 현재의 에이전트 Drove의 트랙을 대체하고, 안전하고 유용한 것을 선택하는 것이 필요함을 논의합니다. 이를 위해, 우리는 더욱 안전하고 신뢰할 수 있는 AI 시스템의 개발을 핵심적인 빌드 블록으로 제안합니다. 이를 \"Scientist AI\"라고 부르며, 인간이 그 세계를 이해하는 것을 목적으로, 행동을 취지 않고 관찰으로 세계를 해석하는 시스템입니다. 이 시스템은 데이터 설명을 위한 이론을 생성하는 월드 모델과 질문에 답하는 추론기으로 구성됩니다. 두 컴포넌트는 과도한 예측의 위험을 줄이기 위해 명시된 불확실성의 개념을 가지고 있습니다. 이러한 고려에 따라, Scientist AI는 과학 연구의 acceleration에 인간 연구자를 지원할 수 있으며, 특히 AI 안전도 포함할 수 있습니다. 특히, 우리의 시스템은 AI 에이전트의 위험을 줄이기 위한 가이드라인으로 활용할 수 있습니다. 최종적으로, 에이전트 없는 AI의 초점을 맞추는 것은 AI 혁신의 이익을 얻는 동시에, 현재의 트랙에 따른 위험을 피할 수 있는 것을 보여줍니다. 우리는 이러한 논의가 연구자, 개발자, 정책 결정자에게 이러한 안전한 길을 선호하도록 동기를 부여하는 것을 희망합니다.",
      "upvotes": 3,
      "discussionId": "67bbfd6c3593f69f41512d96"
    },
    "publishedAt": "2025-02-24T00:02:52.495Z",
    "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15657.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 63
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15631",
      "authors": [
        {
          "_id": "67bbdbe8ea3003f47f15d036",
          "name": "Marthe Ballon",
          "hidden": false
        },
        {
          "_id": "67bbdbe8ea3003f47f15d037",
          "name": "Andres Algaba",
          "hidden": false
        },
        {
          "_id": "67bbdbe8ea3003f47f15d038",
          "name": "Vincent Ginis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T17:59:13.000Z",
      "title": "理由와 성능의 관계에 대한 대형 언어 모델 -- o3 (mini)는 긴 시간이 필요하지 않아도 강한 생각을 할 수 있습니다.",
      "summary": "대 언어 모델은 연관 컨텍스트와 테스트 시의 계산 스케일링을 활용하여 수학적 논리로 상당한 진전을 보여주고 있습니다. 그러나 논리 토큰의 사용과 정확도의 향상 사이의 상호작용에 많은 문제가 남아 있습니다. 특히, 모델의 세대 간 비교를 할 때, 개선된 성능이 긴 논리 연관이나 더 효율적인 논리에서 오는 것이지 아니한지 잘 알 수 없습니다. 이에 따라, Omni-MATH 벤치마크에서 o1-mini와 o3-mini의 버전의 연관 길이를 체계적으로 분석하여, o3-mini (m)는 긴 논리 연관이 필요하지 않아도 o1-mini보다 높은 정확도를 달성할 수 있음을 확인했습니다. 또한, 모든 모델과 계산 설정에서 논리 연관이 길어질수록 정확도가 일반적으로 떨어지는 것을 보여주고, 문제의 난이도를 조절해도 동일한 결과를 얻을 수 있습니다. 이 정확도 감소는 더 전문적인 모델에서는 더 작게 되는 반면, 테스트 시의 계산량을 더 효율적으로 사용하는 것이 새로운 세대의 논리 모델의 특징입니다. 마지막으로, o3-mini (h)는 o3-mini (m)보다 미묘한 정확도 향상을 달성하며, 이는 모든 문제에 따라 많은 논리 토큰을 할당하여 실현되어, o3-mini (m)이 이미 해결할 수 있는 문제를 포함하여도 동일한 결과를 얻습니다. 이러한 발견은 모델의 능력과 논리 길이 사이의 관계를 새로운 시각으로 제공하고, 효율성, 스케일링 및 평가 방법 등에 영향을 미칩니다.",
      "upvotes": 3,
      "discussionId": "67bbdbefea3003f47f15d226"
    },
    "publishedAt": "2025-02-23T21:40:17.216Z",
    "title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15631.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6193
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14905",
      "authors": [
        {
          "_id": "67bbe0520aabd5d571a723e7",
          "name": "Bhavik Agarwal",
          "hidden": false
        },
        {
          "_id": "67bbe0520aabd5d571a723e8",
          "name": "Ishan Joshi",
          "hidden": false
        },
        {
          "_id": "67bbe0520aabd5d571a723e9",
          "name": "Viktoria Rojkova",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T16:44:55.000Z",
      "title": "JSON 내로 생각하기: 엄밀한 LLM 단순 준수 강화 전략",
      "summary": "이 논문에서는, 엄격한 시ン플렉스 준수성을 강제하는 대규모 언어 모델(LLM)의 생성에 대한 도전을 해결하기 위해, LLM의 이유 능력을 활용하여 구현하였다. DeepSeek R1의 강화학습 프레임워크를 기반으로, 우리의 접근法是 합성적인 이유 데이터 세트의 구축과 그룹 상대적 정책 최적화(GRPO)의 사용자 정의 보상 함수를 조합하여, 15억 파라미터 모델의 구조화된 이유 스킬을 학습시키는 새로운 파이프라인을 사용하였다. 특히, 처음으로, 20,000 샘플의 무구조로부터 구조화된 데이터 세트에 대해 R1 강화학습을 수행하고, 원의 DeepSeek R1의 방법을 미러링하여 핵심적인 이유 능력을 확립하였다. 다음으로, 다른 10,000 샘플의 이유 데이터 세트에 대해 규범 제어의 훈련을 수행하여, 다운 스트리م 태스크의 시ン플렉스 준수성을 개선하였다. 상대적으로 가벼운 훈련 범위에서도, GRPO의 훈련은 8xH100 GPU 클러스터에서 약 20시간, SFT는 1xA100에서 3시간이 필요하지만, 우리의 모델은 시ン플렉스의 일치성을 강제하기 위해 강력한 성능을 보여주며, 우리의 ThinkJSON 접근法是 원의 DeepSeek R1(671B), DeepSeek R1의 전원판(Qwen-1.5B와 Qwen-7B), 그리고 Gemini 2.0 Flash(70B)과 비교하여, 실세계적인 애플리케이션에서 효과성을 보여주었다. 우리의 결과는, 자원 효율적인 프레임워크의 실용적인 유용성을 강조하였다.",
      "upvotes": 2,
      "discussionId": "67bbe0530aabd5d571a72437"
    },
    "publishedAt": "2025-02-23T22:11:17.789Z",
    "title": "Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6193
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15681",
      "authors": [
        {
          "_id": "67bbe67c7727595ca5979d2a",
          "name": "Yilun Xu",
          "hidden": false
        },
        {
          "_id": "67bbe67c7727595ca5979d2b",
          "name": "Weili Nie",
          "hidden": false
        },
        {
          "_id": "67bbe67c7727595ca5979d2c",
          "name": "Arash Vahdat",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T18:59:20.000Z",
      "title": "1스텝디퓨전모형에서 f-Divergence 분포의 일치",
      "summary": "ディフュージョンモデル에서 샘플링은 유용한 기능을 달성하기 위해 장기적인 반복적인 프로세스로 수행되고, 특히 인터랙티브 애플리케이션에서 구현이 어려워진다. 생성 속도를 향상시키기 위해 최근의 접근법은, 다스텝 디フュージョン 모델을 단일 스텝의 학생 생성기로 비ニアルスコア ディスティル ドで 추출하고, 학생이 생성한 샘플의 분포를 지도 분포와 일치시켜 생성 속도를 고속화하고 있다. 그러나 이러한 접근법은 역방향 KL divergence를 사용하여 분포의 일치를 수행하는 것으로 알려져 있지만, 이는 모드 シーキングと呼ばれている。본 논문에서는, 새로운 f-divergence 최소화 프레임워크를 사용하여 분포의 일치를 일반화하고, 모드 カバー バージョン과 훈련 변异性의 차이에 따라 다양한 보정을 포함하는 다양한 divergence를 커버하는 것을 목표로 한다. 지도와 학생의 분포 사이의 f-divergence의 경사를 구하고, 이는 그 점수의 차이와 밀도 비율에 의해 결정되는 가중치 함수의 곱으로 표현되어, 지도 분포의 밀도가 높은 샘플을 자연스럽게 우선시하는 것을 보여준다. 역방향 KL divergence를 사용한 비ニアルスコア ディスティル ド의 일반적인 접근法是 우리 프레임워크 내의 특수한 경우로 간주된다. 실험적으로는, 순방향 KL divergence와 GENESIS-シャーニング divergence 등의 대체적인 f-divergence는 현재 가장 좋은 비ニアルスコア ディスティル ド手法를 초월하는 것을 보여주고, 특히 GENESIS-シャーニング divergence를 사용함으로써, f-distill은 ImageNet64의 현재 가장 선진한 한 스텝 생성 성능을 달성하고, MS-COCO의 ゼロショット 텍스트에서 이미지 생성에서도 우수한 성능을 보여주는 것을 보여준다. 프로젝트 페이지: https://research.nvidia.com/labs/genair/f-distill",
      "upvotes": 1,
      "discussionId": "67bbe6837727595ca5979e8c"
    },
    "publishedAt": "2025-02-23T22:24:55.500Z",
    "title": "One-step Diffusion Models with $f$-Divergence Distribution Matching",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6193
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13189",
      "authors": [
        {
          "_id": "67b7152f299e4d30f9eb41c2",
          "name": "Enzhe Lu",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41c3",
          "name": "Zhejun Jiang",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41c4",
          "name": "Jingyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41c5",
          "name": "Yulun Du",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41c6",
          "name": "Tao Jiang",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41c7",
          "name": "Chao Hong",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41c8",
          "name": "Shaowei Liu",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41c9",
          "name": "Weiran He",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41ca",
          "name": "Enming Yuan",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41cb",
          "name": "Yuzhi Wang",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41cc",
          "name": "Zhiqi Huang",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41cd",
          "name": "Huan Yuan",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41ce",
          "name": "Suting Xu",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41cf",
          "name": "Xinran Xu",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d0",
          "name": "Guokun Lai",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d1",
          "name": "Yanru Chen",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d2",
          "name": "Huabin Zheng",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d3",
          "name": "Junjie Yan",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d4",
          "name": "Jianlin Su",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d5",
          "name": "Yuxin Wu",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d6",
          "name": "Neo Y. Zhang",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d7",
          "name": "Zhilin Yang",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d8",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d9",
          "name": "Mingxing Zhang",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41da",
          "name": "Jiezhong Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T14:06:05.000Z",
      "title": "MoBA: 긴 문맥 LLMs의 블록 어텐션의 혼합\n\n(Note: The original text \"長文脈 LLMs のブロックアテンションの混ぜ合わせ\" is a direct translation of \"Long Context LLMs' Block Attention Mixture\". The Korean translation provided above maintains the professional and accurate nature of the original text.)",
      "summary": "スカライング의 효과적인 컨텍스트 길이는 대규모 언어 모델（LLMs）를 인공적인 일반적인 지능（AGI）으로 발전시키기 위해 중요하다. 그러나 단순한 액션 구조에서 계산 복잡성의 제곱적 증가는 부담을 가해준다. 현재의 접근 방식은 특정 태스크에 특화된 구조를 강제로 적용하고 있거나 (예: sink or window attention) 또는 액션 구조를 선형 근사화로 변형하고 있지만, 복잡한 추론 태스크의 성능에 대해서는 충분히 조사되어 있지 않다.\n\n본 논문에서는 \"구조를 최소화\" 원칙에 따라 해결책을 제안하고, 모델이 자동으로 액션의 위치를 결정할 수 있도록 하고, 사전 정의된 편향을 일으키지 않도록 하는 데에 중점을 두게 된다. Block Attention Mixture (MoBA)를 도입하여, Mixture of Experts (MoE) 원칙을 액션 구조에 적용한 새로운 접근 방식을 제안한다. 이 새로운 아키텍처는 긴 컨텍스트 태스크에서 높은 성능을 보여주며, 전체 注意력과 희소 注意력의 적절한 전환이 가능하며, 효율을 향상시키면서 성능을 떨어뜨리지 않는 위험을 줄일 수 있는 것을 특징으로 한다. MoBA는 이미 Kimi의 긴 컨텍스트 요청에 지원하고 있으며, LLMs의 효율적인 액션 계산에 있어서 상당한 진전을 보여주고 있다. 코드는 https://github.com/MoonshotAI/MoBA에서 사용 가능합니다.",
      "upvotes": 0,
      "discussionId": "67b71530299e4d30f9eb4213"
    },
    "publishedAt": "2025-02-24T04:52:30.963Z",
    "title": "MoBA: Mixture of Block Attention for Long-Context LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13189.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 420
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13407",
      "authors": [
        {
          "_id": "67bb33f3829dedfc99ae1288",
          "user": {
            "_id": "67bb32b6a0cb6e48cfd27d80",
            "avatarUrl": "/avatars/3cafe3a3fb60405252962d00105667c5.svg",
            "isPro": false,
            "fullname": "Ziyuan Liu",
            "user": "circleLZY",
            "type": "user"
          },
          "name": "Ziyuan Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:07:29.223Z",
          "hidden": false
        },
        {
          "_id": "67bb33f3829dedfc99ae1289",
          "name": "Ruifei Zhu",
          "hidden": false
        },
        {
          "_id": "67bb33f3829dedfc99ae128a",
          "name": "Long Gao",
          "hidden": false
        },
        {
          "_id": "67bb33f3829dedfc99ae128b",
          "name": "Yuanxiu Zhou",
          "hidden": false
        },
        {
          "_id": "67bb33f3829dedfc99ae128c",
          "name": "Jingyu Ma",
          "hidden": false
        },
        {
          "_id": "67bb33f3829dedfc99ae128d",
          "name": "Yuantao Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T03:33:54.000Z",
      "title": "JL1-CD: 원격관측 변화 검출의 새로운 벤치마크와 강력한 다턴너 지식 필터링 프레임워크",
      "summary": "딥러닝은 원격관측 이미지의 변화 검출(CD) 분야에서 상당한 성공을 거뒀지만, 두 가지 큰 문제점이 남아 있습니다: 작은 규모의 완전한 오픈 소스 CD 데이터 세트의 부족과, 변화 영역이 다른 이미지 간에 일관된 만족스러운 검출 결과를 달성하는 어려움입니다. 이러한 문제를 대처하기 위해, 우리는 JL1-CD 데이터 세트를 소개합니다. 이 데이터 세트에는 0.5에서 0.75미터의 해상도로 5,000개의 512×512 픽셀 이미지의 쌍이 포함됩니다. 또한, 우리는 CD를 위한 다 턴너 지식 절약(MTKD) 프레임워크를 제안합니다. JL1-CD 및 SYSU-CD 데이터 세트의 실험 결과를 통해, MTKD 프레임워크는 서로 다른 네트워크 구조와 파라미터 크기의 CD 모델의 성능을 크게 향상시켰으며, 새로운 최尖端 결과를 얻었습니다. 코드는 https://github.com/circleLZY/MTKD-CD에 접근할 수 있습니다.",
      "upvotes": 0,
      "discussionId": "67bb33f6829dedfc99ae135e"
    },
    "publishedAt": "2025-02-24T04:29:42.452Z",
    "title": "JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13407.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67bb32b6a0cb6e48cfd27d80",
      "avatarUrl": "/avatars/3cafe3a3fb60405252962d00105667c5.svg",
      "fullname": "Ziyuan Liu",
      "name": "circleLZY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15011",
      "authors": [
        {
          "_id": "67bc0d12ffc2c387329c8cfd",
          "user": {
            "_id": "650ec19e6620b0c57e2a551b",
            "avatarUrl": "/avatars/c26c03fa920d857120f03c9ccb9f1d7a.svg",
            "isPro": false,
            "fullname": "Sayan Deb Sarkar",
            "user": "sayandsarkar",
            "type": "user"
          },
          "name": "Sayan Deb Sarkar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:06:56.555Z",
          "hidden": false
        },
        {
          "_id": "67bc0d12ffc2c387329c8cfe",
          "name": "Ondrej Miksik",
          "hidden": false
        },
        {
          "_id": "67bc0d12ffc2c387329c8cff",
          "name": "Marc Pollefeys",
          "hidden": false
        },
        {
          "_id": "67bc0d12ffc2c387329c8d00",
          "name": "Daniel Barath",
          "hidden": false
        },
        {
          "_id": "67bc0d12ffc2c387329c8d01",
          "name": "Iro Armeni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T20:05:30.000Z",
      "title": "CrossOver: 3D 스키뮬레이션 크로스 모ー드 어라인먼트",
      "summary": "다모달 3D 오브젝트 이해는 중요한 주목을 받고 있지만, 현재의 접근 방식은 완전한 데이터의 활용성과 모든 모달리티의 刚性한 조정을 가정하고 있습니다. CrossOver라는 새로운 프레임워크를 소개합니다. 이 프레임워크는 유연한, 스케인 수준의 모달리티 조정을 통해 크로스모달 3D 스케인 이해를 수행합니다. 기존의 방법과 달리, CrossOver은 모든 오브젝트 인스턴스에 대해 조정된 모달 데이터가 필요하지 않도록 RGB 이미지, 점군, CAD 모델, 플로어 계획, 그리고 텍스트 설명을 조정하고 엄격한 제약을 완화하며 명시적인 오브젝트 의미를 포함하지 않도록 학습한 연속적인 모달리티에 대한 연속적인 스케인의 모달무관확산 공간을 학습합니다. 차원특이한 인코더, 멀티단계 훈련 파이프라인, 그리고 현상적인 크로스모달 행동을 활용하여, CrossOver은 모달 데이터가 부족한 경우에도 강력한 스케인 검색과 오브젝트 위치 지정을 지원합니다. ScanNet과 3RScan 데이터 세트의 평가에 따라 다양한 메트릭에서 상위 성능을 나타내며, 3D 스케인 이해의 실세계 애플리케이션의 적응성을 강조합니다.",
      "upvotes": 0,
      "discussionId": "67bc0d18ffc2c387329c8e56"
    },
    "publishedAt": "2025-02-24T01:13:24.911Z",
    "title": "CrossOver: 3D Scene Cross-Modal Alignment",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650ec19e6620b0c57e2a551b/S_xFBPoV3YbtHmtLtRrSV.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15011.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650ec19e6620b0c57e2a551b",
      "avatarUrl": "/avatars/c26c03fa920d857120f03c9ccb9f1d7a.svg",
      "fullname": "Sayan Deb Sarkar",
      "name": "sayandsarkar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15082",
      "authors": [
        {
          "_id": "67bbe93f267aa2b537b318be",
          "user": {
            "_id": "64f64da90efa33bfe0a3d9ba",
            "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
            "isPro": false,
            "fullname": "Vaidehi Patil",
            "user": "vaidehi99",
            "type": "user"
          },
          "name": "Vaidehi Patil",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:07:15.794Z",
          "hidden": false
        },
        {
          "_id": "67bbe93f267aa2b537b318bf",
          "name": "Elias Stengel-Eskin",
          "hidden": false
        },
        {
          "_id": "67bbe93f267aa2b537b318c0",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T22:51:10.000Z",
      "title": "UPCORE: 유틸리티 보존형 코어 세트 선택에 의한 균형 있는 무학습",
      "summary": "유저의 특정 요구 사항이나 법규 구조에 따라, 情報를 復元モデルから 제거하는 경우가 있을 수 있으며, 이 경우 이미 학습된 모델에서 특정 데이터 포인트를 제거하거나 「忘れる」 필요가 있습니다. 이 작업은 모델이 다른 데이터 포인트에 대한 성능을 저하시킬 수 있기 때문에, 情報의 제거와 모델의 다른 능력의 보존 사이의 균형을 찾는 것이 필요합니다. 이 균형의 손실은 제거 실패 또는 사용 불가능한 모델로 이어질 수 있습니다. 이러한 관점에서, 우리는 UPCORE(Utility-Preserving Coreset Selection)를 제안하고, 「忘れる」(unlearning) 때 발생하는 손실을 최소화하기 위해 데이터 선택 프레임워크를 제안하고 있습니다. 모델의 손상이 「忘れる」 세트의 모델의 표현의 분산과 관련되어 있으며, 「忘れる」 세트를 선택적으로 제안하여 무학습 후 모델의 손상을 최소화하는 것을 목표로 합니다. UPCORE는 3가지 표준의 무학습 방법들을 통해 제거 효과와 모델의 보존 사이의 대립의 목표의 균형을 뛰어넘어 달성하고 있습니다. 이 거래 오프의 평가를 개선하기 위해 새로운 메트릭을 도입하고, 표준 메트릭의 곡선 면적(AUC)을 측정하고 있습니다. UPCORE는 표준 메트릭과 AUC를 모두 개선하고, 코어 셋과 제안된 점 사이에서 양의 트랜스폼을 받으며, 「忘れる」 세트에서 그 외의 점으로의 음의 트랜스폼을 줄이는 것을 목표로 합니다.",
      "upvotes": 0,
      "discussionId": "67bbe940267aa2b537b318f4"
    },
    "publishedAt": "2025-02-23T23:17:33.152Z",
    "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15082.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f64da90efa33bfe0a3d9ba",
      "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
      "fullname": "Vaidehi Patil",
      "name": "vaidehi99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]