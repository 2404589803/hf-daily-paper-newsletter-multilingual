[
  {
    "paper": {
      "id": "2507.09477",
      "authors": [
        {
          "_id": "68787030001546c83aa4f9ae",
          "name": "Yangning Li",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9af",
          "user": {
            "_id": "6667e801fd95ddf66cac84ff",
            "avatarUrl": "/avatars/b75f6253160c81f558e6b40ed2ca1755.svg",
            "isPro": false,
            "fullname": "Weizhi Zhang",
            "user": "WZDavid",
            "type": "user"
          },
          "name": "Weizhi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:48.379Z",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b0",
          "name": "Yuyao Yang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b1",
          "name": "Wei-Chieh Huang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b2",
          "name": "Yaozu Wu",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b3",
          "name": "Junyu Luo",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b4",
          "name": "Yuanchen Bei",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b5",
          "user": {
            "_id": "633f112013e836a0fc4fa567",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665077519962-noauth.jpeg",
            "isPro": false,
            "fullname": "Henry Peng Zou",
            "user": "TreeForest",
            "type": "user"
          },
          "name": "Henry Peng Zou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:46.466Z",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b6",
          "name": "Xiao Luo",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b7",
          "name": "Yusheng Zhao",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b8",
          "name": "Chunkit Chan",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b9",
          "name": "Yankai Chen",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9ba",
          "name": "Zhongfen Deng",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bb",
          "name": "Yinghui Li",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bc",
          "name": "Hai-Tao Zheng",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bd",
          "name": "Dongyuan Li",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9be",
          "name": "Renhe Jiang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bf",
          "name": "Ming Zhang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9c0",
          "name": "Yangqiu Song",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9c1",
          "name": "Philip S. Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-13T03:29:41.000Z",
      "submittedOnDailyAt": "2025-07-17T02:27:52.541Z",
      "title": "LLM 내의 RAG-론론 시스템의 개요: 에이전트형 RAG와 깊은론론을 실현하기 위한 시스템의 심플렉스",
      "submittedOnDailyBy": {
        "_id": "6667e801fd95ddf66cac84ff",
        "avatarUrl": "/avatars/b75f6253160c81f558e6b40ed2ca1755.svg",
        "isPro": false,
        "fullname": "Weizhi Zhang",
        "user": "WZDavid",
        "type": "user"
      },
      "summary": "レビュアルアウゲーション（RAG）는 외부 지식을 注入して 大規模言語モデル（LLMs）의 事実性を 上げるが、多段階推論の問題に対しては 欠点があります。反対に、単に 理由取りに向けたアプローチは、ハロカノイゼーションや 事実の 不正な基礎を与えることもあります。本調査は、一つの 理由取り・レビュアルの視点で両方のトレンドを 合成しています。まず、進捗の 理由取りがRAGの各ステージを 最適化する方法を 地囲化します（Reasoning-Enhanced RAG）。次に、異なるタイプのレビュアル知識が 欠損の前提を 補完し、複雑な推論の コンテキストを 拡張する方法を示します（RAG-Enhanced Reasoning）。最後に、レビュアルと 理由取りの 協調化が 進む シンプレックスフレームワークを特別に 強調し、知識密集型ベンチマークで 最先端の性能を 達成するために、探索と 理由取りを 交互に 繰り返す（agentic）LLMsを中心にしています。方法、データセット、開放された課題を 分類し、RAG-Reasoningシステムの 深さや 効果性、多モデル慣れ性、信頼性、人間中心性に向けた 研究の ポートを設定します。このコレクションは、https://github.com/DavidZWZ/Awesome-RAG-Reasoningにアクセスできます。",
      "upvotes": 35,
      "discussionId": "68787031001546c83aa4f9c2",
      "githubRepo": "https://github.com/DavidZWZ/Awesome-RAG-Reasoning",
      "ai_summary": "This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.",
      "ai_keywords": [
        "Retrieval-Augmented Generation",
        "Large Language Models",
        "Reasoning-Enhanced RAG",
        "RAG-Enhanced Reasoning",
        "Synergized RAG-Reasoning",
        "knowledge-intensive benchmarks",
        "multimodally-adaptive",
        "trustworthy",
        "human-centric"
      ],
      "githubStars": 46
    },
    "publishedAt": "2025-07-12T23:29:41.000Z",
    "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs",
    "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09477.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6667e801fd95ddf66cac84ff",
      "avatarUrl": "/avatars/b75f6253160c81f558e6b40ed2ca1755.svg",
      "fullname": "Weizhi Zhang",
      "name": "WZDavid",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12465",
      "authors": [
        {
          "_id": "6878635e001546c83aa4f979",
          "user": {
            "_id": "65af6f6b52e1b2aae437af2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
            "isPro": false,
            "fullname": "Ziang Cao",
            "user": "Caoza",
            "type": "user"
          },
          "name": "Ziang Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:44.605Z",
          "hidden": false
        },
        {
          "_id": "6878635e001546c83aa4f97a",
          "user": {
            "_id": "62fc8cf7ee999004b5a8b982",
            "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
            "isPro": false,
            "fullname": "Zhaoxi Chen",
            "user": "FrozenBurning",
            "type": "user"
          },
          "name": "Zhaoxi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:22:11.615Z",
          "hidden": false
        },
        {
          "_id": "6878635e001546c83aa4f97b",
          "name": "Linag Pan",
          "hidden": false
        },
        {
          "_id": "6878635e001546c83aa4f97c",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:21:56.595Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65af6f6b52e1b2aae437af2e/nrL7wGaZ1Z5FZWuu0GTbg.mp4"
      ],
      "publishedAt": "2025-07-16T17:59:35.000Z",
      "submittedOnDailyAt": "2025-07-17T01:26:11.001Z",
      "title": "PhysX: 물리 기반 3D 자산 생성",
      "submittedOnDailyBy": {
        "_id": "65af6f6b52e1b2aae437af2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
        "isPro": false,
        "fullname": "Ziang Cao",
        "user": "Caoza",
        "type": "user"
      },
      "summary": "3D 모델링은 가상에서 물리적으로 이동하고 있습니다. 현재의 3D 생성은 기오메트리와 테크스쳐를 주력으로 하고 있으며, 물리적으로 기반된 모델링을 뛰어넘고 있습니다. 그 결과, 3D 생성 모델의 급격한 발전과 함께 합성된 3D 자산은 풍부한 중요한 물리적 속성을 뛰어넘고, 물리적 영역의 시뮬레이션이나 현실 세계의 구체적인 AI의 적용을 방해하고 있습니다. 이러한 도전을 처음으로 해결하기 위해, 우리는 PhysX, 물리적으로 기반된 3D 자산 생성의 일관된 패러다임에 대해 제안합니다.\n\n1) 물리적인 어노테이션이 부족한 3D 데이터 세트의 중요한 오류를 지나, PhysXNet, 물리적으로 기반된 5가지 기초적인 차원으로 체계적으로 어노테이션된 첫 번째 3D 데이터 세트를 제시합니다. 특히, 시각 언어 모델에 기반한 scalableな 인간 루프 어노테이션 프로세스를 개발하고, 효율적으로 생산적인 자산을 만들 수 있습니다.\n\n2) 또한, PhysXGen, 물리적으로 기반된 이미지에서 3D 자산 생성의 전파 프레임워크를 제안하고, 사전 학습된 3D 구조 공간에 미리 학습된 물리적 지식을 주입합니다. 특히, PhysXGen은 3D 구조와 물리적 속성의 잠재적인 상관관계를 명확히 모델화하고, 합리적인 물리적 예측을 가지는 3D 자산을 생성하고, 자연스러운 기오메트리 품질을 유지하는 것을 목표로 합니다. 광범위한 실험은 우리의 프레임워크의 높은 성능과 기대되는 일반화 능력을 증명합니다. 모든 코드, 데이터, 모델은 생성적인 물리적 AI의 미래 연구를 촉진하기 위해 공개합니다.",
      "upvotes": 19,
      "discussionId": "6878635e001546c83aa4f97d",
      "projectPage": "https://physx-3d.github.io/",
      "githubRepo": "https://github.com/ziangcao0312/PhysX",
      "ai_summary": "PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.",
      "ai_keywords": [
        "physics-grounded 3D asset generation",
        "PhysXNet",
        "physics-annotated 3D datasets",
        "vision-language models",
        "human-in-the-loop annotation pipeline",
        "PhysXGen",
        "feed-forward framework",
        "dual-branch architecture",
        "latent correlations",
        "physical predictions",
        "geometry quality"
      ],
      "githubStars": 20
    },
    "publishedAt": "2025-07-16T13:59:35.000Z",
    "title": "PhysX: Physical-Grounded 3D Asset Generation",
    "summary": "3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose PhysX, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose PhysXGen, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65af6f6b52e1b2aae437af2e/nrL7wGaZ1Z5FZWuu0GTbg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12465.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65af6f6b52e1b2aae437af2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
      "fullname": "Ziang Cao",
      "name": "Caoza",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.11949",
      "authors": [
        {
          "_id": "687872c3001546c83aa4f9cf",
          "user": {
            "_id": "683d94e5ba11bab2cc848aab",
            "avatarUrl": "/avatars/4a1915ad48c78b3a71733b3282f2f93c.svg",
            "isPro": false,
            "fullname": "Shuyang Xu",
            "user": "JimSYXu",
            "type": "user"
          },
          "name": "Shuyang Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:22:22.253Z",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d0",
          "user": {
            "_id": "645223fb01d7bd9555ea399a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
            "isPro": false,
            "fullname": "Zhiyang Dou",
            "user": "frankzydou",
            "type": "user"
          },
          "name": "Zhiyang Dou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:59.825Z",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d1",
          "name": "Mingyi Shi",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d2",
          "name": "Liang Pan",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d3",
          "name": "Leo Ho",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d4",
          "name": "Jingbo Wang",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d5",
          "name": "Yuan Liu",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d6",
          "name": "Cheng Lin",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d7",
          "name": "Yuexin Ma",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d8",
          "name": "Wenping Wang",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d9",
          "name": "Taku Komura",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645223fb01d7bd9555ea399a/KoPBJkGwyft7hfVmM8ikZ.mp4"
      ],
      "publishedAt": "2025-07-16T06:33:11.000Z",
      "submittedOnDailyAt": "2025-07-17T02:48:15.112Z",
      "title": "MOSPA: 공간음향에 의한 인간 동작 생성",
      "submittedOnDailyBy": {
        "_id": "645223fb01d7bd9555ea399a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
        "isPro": false,
        "fullname": "Zhiyang Dou",
        "user": "frankzydou",
        "type": "user"
      },
      "summary": "虚擬인간이 다양한 청각 자극에 대한 동적하고 현실적인 반응을 구현하는 것은 인물 애니메이션에서 중요한 과제이며, 관측 인지 모델링과 동작 합성의 통합이 요구되어 있습니다. 이 과제는 중요하지만 거의 조사되지 않았습니다. 지금까지의 연구는 주로 대화어, 음성, 음악 등 모델을 기능과 동작에 대응시키기를 중심으로 했습니다. 아직 이러한 모델은 공간 음향 신호에 포함된 공간 특성의 영향을 고려하지 않았습니다. 이를 메꾸고 공간 음향에 의한 고품질 인물 동작 모델링을 가능하게 하려고, 우리는 첫 번째 상세한 공간 음향 구동 인물 동작(SAM) 데이터 세트를 소개합니다. 이 데이터 세트는 다양한, 고품질의 공간 음향과 동작 데이터를 포함합니다. 벤치마크를 위해, 우리는 간단하고 효과적인 확산 기반의 생성 프레임워크를 개발하고, 공간 음향에 의한 인물 동작을 생성하는 것을 MOSPA라는 이름으로 나타냅니다. 이 프레임워크는 신체 동작과 공간 음향의 관계를 신뢰적으로 파악하기 위해, 효과적인 융합 구조를 사용합니다. 훈련 후, MOSPA는 공간 음향 입력에 대해 다양한 현실적인 인물 동작을 생성할 수 있습니다. 우리의 데이터 세트를 자세히 조사하고, 분산 실험을 수행하여 벤치마크를 위해입니다. 이 작업에서 우리의 방법는 가장 선진적인 성능을 달성했습니다. 우리의 모델과 데이터 세트는 받아들여지면 공개됩니다. 자세한 내용은 서브 프로젝트의 비디오에 참고해주세요.",
      "upvotes": 10,
      "discussionId": "687872c9001546c83aa4f9da",
      "ai_summary": "A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.",
      "ai_keywords": [
        "diffusion-based generative framework",
        "spatial audio",
        "human motion",
        "SAM dataset",
        "MOSPA",
        "spatial features",
        "perceptual modeling",
        "motion synthesis"
      ]
    },
    "publishedAt": "2025-07-16T02:33:11.000Z",
    "title": "MOSPA: Human Motion Generation Driven by Spatial Audio",
    "summary": "Enabling virtual humans to dynamically and realistically respond to diverse\nauditory stimuli remains a key challenge in character animation, demanding the\nintegration of perceptual modeling and motion synthesis. Despite its\nsignificance, this task remains largely unexplored. Most previous works have\nprimarily focused on mapping modalities like speech, audio, and music to\ngenerate human motion. As of yet, these models typically overlook the impact of\nspatial features encoded in spatial audio signals on human motion. To bridge\nthis gap and enable high-quality modeling of human movements in response to\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\nmotion data. For benchmarking, we develop a simple yet effective\ndiffusion-based generative framework for human MOtion generation driven by\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\nbody motion and spatial audio through an effective fusion mechanism. Once\ntrained, MOSPA could generate diverse realistic human motions conditioned on\nvarying spatial audio inputs. We perform a thorough investigation of the\nproposed dataset and conduct extensive experiments for benchmarking, where our\nmethod achieves state-of-the-art performance on this task. Our model and\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\nvideo for more details.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645223fb01d7bd9555ea399a/KoPBJkGwyft7hfVmM8ikZ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645223fb01d7bd9555ea399a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
      "fullname": "Zhiyang Dou",
      "name": "frankzydou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12463",
      "authors": [
        {
          "_id": "68788789001546c83aa4f9e4",
          "name": "Renjie Li",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e5",
          "user": {
            "_id": "6825fc0e58cf56d164cb339d",
            "avatarUrl": "/avatars/4ade4a5bbb0a805a92a83bfb233f805f.svg",
            "isPro": false,
            "fullname": "Ruijie Ye",
            "user": "jerryye0110",
            "type": "user"
          },
          "name": "Ruijie Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:17:53.937Z",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e6",
          "name": "Mingyang Wu",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e7",
          "name": "Hao Frank Yang",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e8",
          "user": {
            "_id": "63b354bb7091e602f1a0e2e8",
            "avatarUrl": "/avatars/a388d93c0af2f57eadb6fa60d6789041.svg",
            "isPro": false,
            "fullname": "wayne",
            "user": "waynefan",
            "type": "user"
          },
          "name": "Zhiwen Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:23:18.102Z",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e9",
          "name": "Hezhen Hu",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9ea",
          "user": {
            "_id": "62548d5fef3debb2ddf91217",
            "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
            "isPro": false,
            "fullname": "Zhengzhong Tu",
            "user": "vztu",
            "type": "user"
          },
          "name": "Zhengzhong Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:23:16.047Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ARblfCBCEmNaS2zzbTJLm.jpeg"
      ],
      "publishedAt": "2025-07-16T17:59:30.000Z",
      "submittedOnDailyAt": "2025-07-17T03:48:41.381Z",
      "title": "MMHU: 대규모 다모델 벤치마크에서 인간 행동을 이해하는 것",
      "submittedOnDailyBy": {
        "_id": "62548d5fef3debb2ddf91217",
        "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
        "isPro": false,
        "fullname": "Zhengzhong Tu",
        "user": "vztu",
        "type": "user"
      },
      "summary": "인간은 교통기관 시스템의 중요한 구성 요소이며, 그 행동을 이해하는 것이 안전 운행 시스템의 개발에서 중요하다. 과거의 발전은 인간 행동의 다양한 면에서 연구를 수행하고, 움직임, 경로, 그리고 의도 등을 검토했으나, 자율주행에서 인간 행동을 이해하기 위한 일관된 벤치마크는 아직 존재하지 않는다. 본 연구에서는 MMHU라는 대규모인간 행동 분석 벤치마크를 제안하고, 풍부한 注釈을 다루는 방식으로, 인간의 움직임과 경로, 인간 행동의 설명, 인간 의도, 운전 안전에 관련된 중요한 행동 라벨 등을 포함하여 제안한다. 데이터 셋은 Waymo의 기존 운행 데이터 셋, YouTube에서 얻은 'in-the-wild' 비디오, 그리고 자기집계 데이터에서 57k인간의 움직임 클립과 1.73M 프레임으로 구성되어 있다. 인간이 注釈 파이프라인 개발을 통해 풍부한 행동 캡처를 생성함으로써, 데이터 셋의 상세한 분석과 다양한 태스크의 벤치마크를 제공하며, 움직임 예측부터 움직임 생성까지, 인간 행동에 대한 광범위한 평가 시스템을 제공하여, 인간 행동에 대한 질의응답을 포함하여 다양한 평가 시스템을 제공한다. 프로젝트 페이지: https://MMHU-Benchmark.github.io.",
      "upvotes": 9,
      "discussionId": "6878878a001546c83aa4f9eb",
      "projectPage": "https://mmhu-benchmark.github.io/",
      "ai_summary": "A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.",
      "ai_keywords": [
        "human behavior analysis",
        "motion prediction",
        "motion generation",
        "human behavior question answering",
        "human-in-the-loop annotation",
        "Waymo",
        "YouTube",
        "self-collected data"
      ]
    },
    "publishedAt": "2025-07-16T13:59:30.000Z",
    "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\n  Understanding",
    "summary": "Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behaviorx2014such as motion, trajectories, and\nintentionx2014a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose MMHU, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasksx2014ranging from motion prediction to motion\ngeneration and human behavior question answeringx2014thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ARblfCBCEmNaS2zzbTJLm.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12463.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62548d5fef3debb2ddf91217",
      "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
      "fullname": "Zhengzhong Tu",
      "name": "vztu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12415",
      "authors": [
        {
          "_id": "68788b9b001546c83aa4f9ed",
          "name": "Xinyi He",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9ee",
          "user": {
            "_id": "612ee6a7b960e78c6d2319d4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "isPro": false,
            "fullname": "Qian Liu",
            "user": "SivilTaram",
            "type": "user"
          },
          "name": "Qian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:17:46.375Z",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9ef",
          "user": {
            "_id": "61711f02e0b1ddb56eb9b526",
            "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
            "isPro": true,
            "fullname": "Mingzhe Du",
            "user": "Elfsong",
            "type": "user"
          },
          "name": "Mingzhe Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T09:08:30.259Z",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f0",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f1",
          "name": "Zhijie Fan",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f2",
          "name": "Yiming Huang",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f3",
          "name": "Zejian Yuan",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f4",
          "name": "Zejun Ma",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/_XkK-c8Xm-G1Ui5AwtGdu.png"
      ],
      "publishedAt": "2025-07-16T17:05:17.000Z",
      "submittedOnDailyAt": "2025-07-17T04:10:30.408Z",
      "title": "SWE-Perf: 언어 모델이 실세계의 리포지토리에서 코드의 성능을 최적화할 수 있는지?",
      "submittedOnDailyBy": {
        "_id": "612ee6a7b960e78c6d2319d4",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
        "isPro": false,
        "fullname": "Qian Liu",
        "user": "SivilTaram",
        "type": "user"
      },
      "summary": "코드의 효율화는 실제 세계적인 소프트웨어 개발에서 가장 높은 중요성을 지닌 것이며, 생산 수준의 시스템에서 더욱 중요합니다. 라ージュ 언어 모델(LLMs)은 코드 생성과 버그 수정에 뛰어난 능력을 보여주었습니다が, 리포지토리 수준의 코드 효율화에 대한 뛰어난 능력은 주로 탐색되어 있지 않습니다. 이러한 빈도를 메우기 위해, 우리는 SWE-Perf를 소개합니다. 이것은 LLMs가 실제 리포지토리 컨텍스트에서 코드 효율화 작업에 대한 체계적인 평가를 수행하기 위해 특별히 설계된 첫 번째 벤치마크입니다. SWE-Perf는 140개의 サイン(包括性能提升的Pull Request)를 선택하여 구성되며, 각 벤치마크 인스턴스에는 관련된 코드베이스, 목표 함수, 성능 관련 테스트, 경험의 포터즈의 패치, 실행 가능한 환경이 포함됩니다. 파일 수준과 리포지토리 수준의 접근 방식(例如Agentless와 OpenHands)을 확장적으로 평가함으로써, 현재의 LLMs와 경험 수준의 최적화 성능 사이의 큰 능력 차이를 명확히 하고, 이新兴 분야의 중요한 연구 기회를 강조합니다.",
      "upvotes": 8,
      "discussionId": "68788b9b001546c83aa4f9f5",
      "projectPage": "https://swe-perf.github.io/",
      "githubRepo": "https://github.com/SWE-Perf/SWE-Perf",
      "ai_summary": "SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.",
      "ai_keywords": [
        "Large Language Models",
        "code performance optimization",
        "benchmark",
        "performance-improving pull requests",
        "codebase",
        "target functions",
        "performance-related tests",
        "expert-authored patches",
        "executable environments",
        "Agentless",
        "OpenHands"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-07-16T13:05:17.000Z",
    "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?",
    "summary": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/_XkK-c8Xm-G1Ui5AwtGdu.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "612ee6a7b960e78c6d2319d4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
      "fullname": "Qian Liu",
      "name": "SivilTaram",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 85
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.11527",
      "authors": [
        {
          "_id": "68785ee1001546c83aa4f967",
          "user": {
            "_id": "65c950ebd908bd52a4477116",
            "avatarUrl": "/avatars/bc6ba0dd2903c7bea37f7b9c40857718.svg",
            "isPro": false,
            "fullname": "Yinsheng Li",
            "user": "Eason666",
            "type": "user"
          },
          "name": "Yinsheng Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:42.470Z",
          "hidden": false
        },
        {
          "_id": "68785ee1001546c83aa4f968",
          "user": {
            "_id": "643ba2f725681c3afaa8f05e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
            "isPro": false,
            "fullname": "Zhen Dong",
            "user": "zhendongucb",
            "type": "user"
          },
          "name": "Zhen Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:37.102Z",
          "hidden": false
        },
        {
          "_id": "68785ee1001546c83aa4f969",
          "name": "Yi Shao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643ba2f725681c3afaa8f05e/2T1jxRrhqMGVLESbEDwJT.png"
      ],
      "publishedAt": "2025-07-15T17:56:04.000Z",
      "submittedOnDailyAt": "2025-07-17T01:30:58.515Z",
      "title": "DrafterBench: 토목공학의 업무 자동화에 대한 대규모 언어 모델의 벤치마크",
      "submittedOnDailyBy": {
        "_id": "643ba2f725681c3afaa8f05e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
        "isPro": false,
        "fullname": "Zhen Dong",
        "user": "zhendongucb",
        "type": "user"
      },
      "summary": "大型 언어 모델(LLM) 에이전트는 현실적인 문제를 해결하기 위한 큰 잠재력을 보여주고 있으며, 산업에서의 작업 자동화에 대한 가능성도 제시하고 있습니다. 그러나 산업적인 관점에서 에이전트의 자동화를 체계적으로 평가하기 위해 더 많은 벤치마크가 필요합니다. 이에 따라, 우리는 토목공학의 표현 태스크인 기술적인 도식의 수정을 위한 LLM 에이전트의 세부적인 평가에 목적을 두고 DrafterBench를 제안합니다. DrafterBench는 실제적인 도식 파일을 기반으로 12가지의 태스크를 통합하고, 46가지의 사용자 정의 함수/툴과 1920가지의 태스크를 포함합니다. DrafterBench는 AI 에이전트의 복잡한 긴 문맥 명령을 이해하는 능력, 전진 지식의 활용, 동적인 명령 품질에 대한潜在적인 정책 인식을 활용하여 엄격한 테스트 벤치마크입니다. 도구킷은 구조화된 데이터의 이해, 함수 실행, 명령 따라행, 비판적인 이유를 포함한 다양한 능력을 종합적으로 평가합니다. DrafterBench는 작업의 정확성과 오류의 통계를 상세히 분석하고, 에이전트의 능력을 깊은 시각을 제공하며, 토목공학 애플리케이션에서 LLM의 통합 개선 목표를 특정하는 데 목적이 있습니다. 우리 벤치마크는 https://github.com/Eason-Li-AIS/DrafterBench에 접근할 수 있으며, 테스트 세트는 https://huggingface.co/datasets/Eason666/DrafterBench에 업로드되어 있습니다.",
      "upvotes": 8,
      "discussionId": "68785ee1001546c83aa4f96a",
      "githubRepo": "https://github.com/Eason-Li-AIS/DrafterBench",
      "ai_summary": "DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "technical drawing revision",
        "structured data comprehension",
        "function execution",
        "instruction following",
        "critical reasoning",
        "benchmark",
        "open-source",
        "implicit policy awareness"
      ],
      "githubStars": 29
    },
    "publishedAt": "2025-07-15T13:56:04.000Z",
    "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering",
    "summary": "Large Language Model (LLM) agents have shown great potential for solving\nreal-world problems and promise to be a solution for tasks automation in\nindustry. However, more benchmarks are needed to systematically evaluate\nautomation agents from an industrial perspective, for example, in Civil\nEngineering. Therefore, we propose DrafterBench for the comprehensive\nevaluation of LLM agents in the context of technical drawing revision, a\nrepresentation task in civil engineering. DrafterBench contains twelve types of\ntasks summarized from real-world drawing files, with 46 customized\nfunctions/tools and 1920 tasks in total. DrafterBench is an open-source\nbenchmark to rigorously test AI agents' proficiency in interpreting intricate\nand long-context instructions, leveraging prior knowledge, and adapting to\ndynamic instruction quality via implicit policy awareness. The toolkit\ncomprehensively assesses distinct capabilities in structured data\ncomprehension, function execution, instruction following, and critical\nreasoning. DrafterBench offers detailed analysis of task accuracy and error\nstatistics, aiming to provide deeper insight into agent capabilities and\nidentify improvement targets for integrating LLMs in engineering applications.\nOur benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,\nwith the test set hosted at\nhttps://huggingface.co/datasets/Eason666/DrafterBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643ba2f725681c3afaa8f05e/2T1jxRrhqMGVLESbEDwJT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643ba2f725681c3afaa8f05e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
      "fullname": "Zhen Dong",
      "name": "zhendongucb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.02857",
      "authors": [
        {
          "_id": "68788cd9001546c83aa4f9f7",
          "user": {
            "_id": "66aef8691dd7d0a8c6584724",
            "avatarUrl": "/avatars/df9c2a56f3d0746cf64a330137a105b4.svg",
            "isPro": false,
            "fullname": "Ziye Li",
            "user": "TribeRinb",
            "type": "user"
          },
          "name": "Ziye Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:48:49.437Z",
          "hidden": false
        },
        {
          "_id": "68788cd9001546c83aa4f9f8",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "68788cd9001546c83aa4f9f9",
          "user": {
            "_id": "6335c7fa2db86a181cca723f",
            "avatarUrl": "/avatars/13f04af01914f8473f9939f49b4eecd4.svg",
            "isPro": false,
            "fullname": "XC",
            "user": "XinchengShuai",
            "type": "user"
          },
          "name": "Xincheng Shuai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:48:34.400Z",
          "hidden": false
        },
        {
          "_id": "68788cd9001546c83aa4f9fa",
          "user": {
            "_id": "67ff29ecbf6889a333c69c7a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
            "isPro": false,
            "fullname": "Henghui Ding",
            "user": "HenghuiDing",
            "type": "user"
          },
          "name": "Henghui Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:48:26.454Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T17:59:02.000Z",
      "submittedOnDailyAt": "2025-07-17T04:11:55.501Z",
      "title": "조건에 따라 이미지를 동작 제어로 애니메이션 처리합니다.",
      "submittedOnDailyBy": {
        "_id": "67ff29ecbf6889a333c69c7a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
        "isPro": false,
        "fullname": "Henghui Ding",
        "user": "HenghuiDing",
        "type": "user"
      },
      "summary": "최근의 이미지 생성의 발전, 특히 분산 모델에서의 발전은 텍스트에서 이미지(T2V)와 이미지에서 이미지(I2V)의 합성에 있어 뚜렷한 발전을 주도하고 있습니다. 그러나 동적인 움직임 신호와 유연한 공간 제약의 효과적인 통합에 여전히 문제점이 남아 있습니다. 현재의 T2V手法는 일반적으로 텍스트플랜트에 의존하며, 생성된 콘텐츠의 공간 배치의 정밀한 제어를 부족할 수 있습니다. 반면, I2V手法는 실상 이미지에 의존하며, 합성된 콘텐츠의 편집 가능성은 제한되어 있습니다. 그러나 일부 방법들은 ControlNet을 도입하여 이미지 기반의 조건付け을 도입하고 있지만, 일반적으로 명확한 움직임 제어를 부족하며, 계산량의 부담을 가중합니다. 이러한 제한에대해, 우리는 사용자 정의된 움직임 트래지로 조건付け 이미지를 동화하기 위한 제한없는 프레임워크인 AnyI2V를 제안하고 있습니다. AnyI2V는 ControlNet이 지원하지 않는 데이터 타입처럼 멱집합이나 점센터 등 더 광범위한 모델 타입을 조건付け 이미지로 지원하며, 더 유연하고 다양한 동화 생성을 가능하게 합니다. 또한, 혼합 데이터의 조건付け 입력을 지원하며, LoRA와 텍스트플랜트를 통해 스테이트 트랜스费和 편집을 가능하게 합니다. 확장된 실험에 의하여, 제안된 AnyI2V는 공간과 움직임의 제어를 가능하게 하는 새로운 시각을 제공하며, 우수한 성능을 갖습니다. 코드는 https://henghuiding.com/AnyI2V/ 에 제공됩니다.",
      "upvotes": 3,
      "discussionId": "68788cda001546c83aa4f9fb",
      "projectPage": "https://henghuiding.com/AnyI2V/",
      "githubRepo": "https://github.com/FudanCVL/AnyI2V",
      "ai_summary": "AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.",
      "ai_keywords": [
        "diffusion models",
        "text-to-video",
        "image-to-video",
        "ControlNet",
        "motion trajectories",
        "conditional images",
        "meshes",
        "point clouds",
        "mixed conditional inputs",
        "style transfer",
        "LoRA",
        "text prompts"
      ],
      "githubStars": 83
    },
    "publishedAt": "2025-07-03T13:59:02.000Z",
    "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
    "summary": "Recent advancements in video generation, particularly in diffusion models,\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\nsynthesis. However, challenges remain in effectively integrating dynamic motion\nsignals and flexible spatial constraints. Existing T2V methods typically rely\non text prompts, which inherently lack precise control over the spatial layout\nof generated content. In contrast, I2V methods are limited by their dependence\non real images, which restricts the editability of the synthesized content.\nAlthough some methods incorporate ControlNet to introduce image-based\nconditioning, they often lack explicit motion control and require\ncomputationally expensive training. To address these limitations, we propose\nAnyI2V, a training-free framework that animates any conditional images with\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\nas the conditional image, including data types such as meshes and point clouds\nthat are not supported by ControlNet, enabling more flexible and versatile\nvideo generation. Additionally, it supports mixed conditional inputs and\nenables style transfer and editing via LoRA and text prompts. Extensive\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\nand provides a new perspective in spatial- and motion-controlled video\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02857.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ff29ecbf6889a333c69c7a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
      "fullname": "Henghui Ding",
      "name": "HenghuiDing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12462",
      "authors": [
        {
          "_id": "68785eb6001546c83aa4f95b",
          "name": "Yuxi Xiao",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f95c",
          "user": {
            "_id": "649bf403fd9cea8366d669ad",
            "avatarUrl": "/avatars/27bd8ca9a948ec38fee950b64f669ce3.svg",
            "isPro": false,
            "fullname": "Jianyuan Wang",
            "user": "JianyuanWang",
            "type": "user"
          },
          "name": "Jianyuan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:37:32.434Z",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f95d",
          "user": {
            "_id": "6485ce5ec7f19728a49df17a",
            "avatarUrl": "/avatars/e83966e6906c1d0f151300981e30f85a.svg",
            "isPro": true,
            "fullname": "Nan",
            "user": "cherubicxn",
            "type": "user"
          },
          "name": "Nan Xue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:15:41.525Z",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f95e",
          "user": {
            "_id": "6393a73584c565d2c3416cb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6393a73584c565d2c3416cb9/OGtX-i-_MLg5--qA054ti.jpeg",
            "isPro": true,
            "fullname": "Nikita Karaev",
            "user": "nikkar",
            "type": "user"
          },
          "name": "Nikita Karaev",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:40:30.784Z",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f95f",
          "name": "Yuri Makarov",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f960",
          "user": {
            "_id": "647b5fef6a79fbf5e996c47c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647b5fef6a79fbf5e996c47c/IkSMnDsCY_CyEFCiMDuxe.jpeg",
            "isPro": false,
            "fullname": "Bingyi Kang",
            "user": "bykang",
            "type": "user"
          },
          "name": "Bingyi Kang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:40:39.614Z",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f961",
          "name": "Xing Zhu",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f962",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f963",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f964",
          "name": "Xiaowei Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-16T17:59:03.000Z",
      "submittedOnDailyAt": "2025-07-17T06:55:04.439Z",
      "title": "스펙트럴트랙커 V2: 3차원 점 추적에 쉽게 접근할 수 있는 소프트웨어",
      "submittedOnDailyBy": {
        "_id": "6688a8f30bf195d6e53ac28d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6688a8f30bf195d6e53ac28d/5izbVmdCWWwA1wBjcxZPB.jpeg",
        "isPro": true,
        "fullname": "Yuxi Xiao",
        "user": "Yuxihenry",
        "type": "user"
      },
      "summary": "스페이스트ラッカーV2를 소개합니다. 이는 단점 카메라용 3차원 포인트 추적 방법입니다. 기존의 완성된 구성 요소를 기반으로 구축된 모댄레이션 플러그인에 의한 3차원 추적보다 발전된 것으로, 포인트 추적, 단점 카메라의 깊이, 카메라의 자세 추정 사이의 내적 연결을 고성능으로 3차원 포인트 추적기로 통합했습니다. 세계 공간의 3차원 이동을 스케니지너이틱, 카메라의 자동 이동, 픽셀 단위의 물체의 이동으로 분해하고, 완전하게 미분 가능한 엔드 투 엔드 아키텍처를 채택하여 합성 시퀀스, 자세付き RGB-D 비디오, 무 라벨의 프리랜드 파일이 포함된 광범위한 데이터 세트를 통해 스케일러블한 훈련이 가능합니다. 이러한 다양한 데이터에서 구조와 이동을 함께 학습함으로써, 스페이스트ラッカーV2는 기존의 3차원 추적 방법의 30% 이상의 효율성을 초월하고, 先進的な 동적인 3차원 재구성 접근법의 정확도를 초월하며, 50배 빠른 동작을 합니다.",
      "upvotes": 2,
      "discussionId": "68785eb7001546c83aa4f965",
      "projectPage": "https://spatialtracker.github.io",
      "githubRepo": "https://github.com/henry123-boy/SpaTrackerV2",
      "ai_summary": "SpatialTrackerV2 is a feed-forward 3D point tracking method for monocular videos that integrates point tracking, monocular depth, and camera pose estimation into a unified, end-to-end architecture, achieving high performance and speed.",
      "ai_keywords": [
        "feed-forward",
        "3D point tracking",
        "monocular videos",
        "intrinsic connections",
        "monocular depth",
        "camera pose estimation",
        "fully differentiable",
        "end-to-end architecture",
        "scene geometry",
        "camera ego-motion",
        "pixel-wise object motion",
        "synthetic sequences",
        "posed RGB-D videos",
        "unlabeled in-the-wild footage",
        "dynamic 3D reconstruction"
      ],
      "githubStars": 467
    },
    "publishedAt": "2025-07-16T13:59:03.000Z",
    "title": "SpatialTrackerV2: 3D Point Tracking Made Easy",
    "summary": "We present SpatialTrackerV2, a feed-forward 3D point tracking method for\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\ncomponents for 3D tracking, our approach unifies the intrinsic connections\nbetween point tracking, monocular depth, and camera pose estimation into a\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\nwith a fully differentiable and end-to-end architecture, allowing scalable\ntraining across a wide range of datasets, including synthetic sequences, posed\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\ndynamic 3D reconstruction approaches while running 50times faster.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12462.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6688a8f30bf195d6e53ac28d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6688a8f30bf195d6e53ac28d/5izbVmdCWWwA1wBjcxZPB.jpeg",
      "fullname": "Yuxi Xiao",
      "name": "Yuxihenry",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.09025",
      "authors": [
        {
          "_id": "68786e45001546c83aa4f9a0",
          "name": "Chien Van Nguyen",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a1",
          "name": "Ruiyi Zhang",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a2",
          "user": {
            "_id": "652767bfbdcf00b9b9ac9a74",
            "avatarUrl": "/avatars/2cc8e9167562f364f0c25410f13a9d62.svg",
            "isPro": false,
            "fullname": "Hanieh Deilamsalehy",
            "user": "haniehds",
            "type": "user"
          },
          "name": "Hanieh Deilamsalehy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:36:52.637Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a3",
          "name": "Puneet Mathur",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a4",
          "name": "Viet Dac Lai",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a5",
          "name": "Haoliang Wang",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a6",
          "user": {
            "_id": "66d32a678819c81cce2052f4",
            "avatarUrl": "/avatars/3a5b40ef9e9e73512743756d1c24ab6c.svg",
            "isPro": false,
            "fullname": "Jayakumar Subramanian",
            "user": "jasubram",
            "type": "user"
          },
          "name": "Jayakumar Subramanian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:37:03.196Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a7",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a8",
          "user": {
            "_id": "67f1035155fe17a33dc71f23",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ohlRrzUI8VwvyYYxFXJuY.png",
            "isPro": false,
            "fullname": "Trung Bui",
            "user": "TrungBui1111",
            "type": "user"
          },
          "name": "Trung Bui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:37:13.349Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a9",
          "user": {
            "_id": "675346f0ab1d47a36ca60b89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mqWUUxAuFn3DsgIqoF_ah.png",
            "isPro": false,
            "fullname": "Nikos Vlassis",
            "user": "Nikosapa",
            "type": "user"
          },
          "name": "Nikos Vlassis",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:36:17.107Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9aa",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:14:23.916Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9ab",
          "user": {
            "_id": "64804fad8c6a3b8f11f73912",
            "avatarUrl": "/avatars/61e37a91d4bba35fda9bf52aadd87745.svg",
            "isPro": false,
            "fullname": "Thien Huu Nguyen",
            "user": "anoperson",
            "type": "user"
          },
          "name": "Thien Huu Nguyen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:36:11.546Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T21:19:18.000Z",
      "submittedOnDailyAt": "2025-07-17T02:00:25.332Z",
      "title": "Lizard: 대규모 언어 모델의 효율적인 선형화 프레임워크",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "라이터는 무한 컨텍스트 생성을 위한 유연한, 다음 제곱 구조의 Transformer 기반의 대두 언어 모델(LLMs)을 변환하는 선형화 프레임워크입니다. Transformer 기반의 LLMs은 컨텍스트 길이가 증가함에 따라 소프트맥스 어텐션의 제곱 복잡성과 키・값(KV) 캐쉬의 증가로 메모리와 계산 백업이 발생합니다. 라이터는 소프트맥스 어텐션을 근사하면서 출력 품질을 유지한 다음 제곱 어텐션 구조를 도입하여 이러한 제한을 해결합니다. 이전의 선형화 방법과 달리 고정된 모델 구조에 의한 제한이 있었기 때문에 게이팅 구조를 많이 포함하지 않았지만, 라이터는 최근의 선형 모델을 기반으로 한 게이팅 모듈을 모델화하고 적응적인 메모리 제어를 가능하게 하며, 상수 메모리 추론을 지원하며, 강한 길이 일반화를 제공하며, 모델 설계의 유연성을 향상시킵니다. 라이터는 게이팅 라인 어텐션을 사용한 글로벌 컨텍스트 압축과 메타메모리에 의한 슬라이딩 윈도우 어텐션을 조합하여, 긴 거리 의존관계와 미세한 지역적 상호작용을 동시에捉捉하는 하이브리드 구조를 형성합니다. 또한 하드웨어에 의한 알고리즘을 도입하여 모델의 훈련 속도를 가속시킵니다. 확장된 실험은 라이터는 표준 언어 모델링 태스크에서 튜치 모델의 성능을 근사무손실적으로 회복하고 이전의 선형화 방법보다 유의미하게 뛰어넘는 것을 보여주고 있습니다. 5샷 MMLU 벤치마크에서 라이터는 선두 모델보다 18점 이상 개선을 나타내며, 연관 기억 태스크에서도 유의미한 개선을 나타냅니다.",
      "upvotes": 2,
      "discussionId": "68786e45001546c83aa4f9ac",
      "ai_summary": "Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.",
      "ai_keywords": [
        "Transformer-based LLMs",
        "subquadratic architectures",
        "softmax attention",
        "key-value (KV) cache",
        "subquadratic attention mechanism",
        "gating module",
        "adaptive memory control",
        "constant-memory inference",
        "length generalization",
        "gated linear attention",
        "sliding window attention",
        "meta memory",
        "hardware-aware algorithm",
        "MMLU benchmark",
        "associative recall tasks"
      ]
    },
    "publishedAt": "2025-07-11T17:19:18.000Z",
    "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
    "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09025.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07451",
      "authors": [
        {
          "_id": "68747204257d4f04353702de",
          "user": {
            "_id": "6474b290d815855e4ef59b05",
            "avatarUrl": "/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg",
            "isPro": false,
            "fullname": "Hongzhi Zhang",
            "user": "hongzhizhang",
            "type": "user"
          },
          "name": "Hongzhi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:21:31.114Z",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702df",
          "name": "Jia Fu",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e0",
          "name": "Jingyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e1",
          "name": "Kai Fu",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e2",
          "name": "Qi Wang",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e3",
          "user": {
            "_id": "67c5945da1661d5fa6f29adb",
            "avatarUrl": "/avatars/62561f3875c0c251cae949acc38d72dc.svg",
            "isPro": false,
            "fullname": "Fuzheng Zhang",
            "user": "Edrex",
            "type": "user"
          },
          "name": "Fuzheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:35:13.823Z",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e4",
          "user": {
            "_id": "67c6c570cf87e2d2ebfc81aa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c6c570cf87e2d2ebfc81aa/7qAstZtIT86Uwrz3u_anv.jpeg",
            "isPro": false,
            "fullname": "Guorui Zhou",
            "user": "GuoruiZhou",
            "type": "user"
          },
          "name": "Guorui Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:34:55.007Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T05:58:55.000Z",
      "submittedOnDailyAt": "2025-07-17T07:36:19.087Z",
      "title": "RLEP: 경험 재사용을 이용한 강화 학습에 의한 LLM의 논리",
      "submittedOnDailyBy": {
        "_id": "6474b290d815855e4ef59b05",
        "avatarUrl": "/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg",
        "isPro": false,
        "fullname": "Hongzhi Zhang",
        "user": "hongzhizhang",
        "type": "user"
      },
      "summary": "강화학습(RL)를 대형 언어 모델에 적용하는 것은 에너지의 큰 낭비일 수 있는 것입니다: 훈련은 안정적이지 않을 수 있으며, 정책은 학습 전의 가중치로부터 점차적으로 유출하는 경우가 있습니다. 우리는 경험 재현을 기반으로 한 강화학습(RLEP)을 소개합니다. RLEP는 처음에 확인된 타록을 수집하고, 그 후 후속 훈련 중 재현하는 2단계 프레임워크입니다. 업데이트 단계마다 정책은 새로 생성된 로우아웃과 이러한 재현된 성공을 혼합한 미니 배치에 의해 최적화됩니다. 고품질의 예를 재현하는 데 힘입어, RLEP는 모델에서 무용의 탐색을 빼내어, 잠재적인 이유의 경로에 집중하고, 빠른 수렴과 최종적으로 강력한 성능을 제공합니다. Qwen2.5-Math-7B 기반 모델에 대해, RLEP는 기준의 최고 정확도를 달성할 수 있으며, 큰 업데이트 없이도 달성하며, AIME-2024의 정확도를 38.2%에서 39.9%, AIME-2025의 정확도를 19.8%에서 22.3%, AMC-2023의 정확도를 77.0%에서 82.2%로 개선합니다. 우리의 코드, 데이터셋, 체크 포인트는 https://github.com/Kwai-Klear/RLEP에서 공개되어 있으며, 재현성 및 연구의 발전을 촉진하는 것이 목적입니다.",
      "upvotes": 1,
      "discussionId": "68747204257d4f04353702e5",
      "ai_summary": "RLEP, a reinforcement learning framework with experience replay, enhances large language model training by focusing on high-quality examples, leading to faster convergence and improved performance on math-related benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "experience replay",
        "trajectories",
        "rollouts",
        "policy optimization",
        "convergence",
        "performance",
        "Qwen2.5-Math-7B",
        "AIME-2024",
        "AIME-2025",
        "AMC-2023"
      ]
    },
    "publishedAt": "2025-07-10T01:58:55.000Z",
    "title": "RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning",
    "summary": "Reinforcement learning (RL) for large language models is an energy-intensive\nendeavor: training can be unstable, and the policy may gradually drift away\nfrom its pretrained weights. We present RLEP\\, -- \\,Reinforcement\nLearning with Experience rePlay\\, -- \\,a two-phase framework that first\ncollects verified trajectories and then replays them during subsequent\ntraining. At every update step, the policy is optimized on mini-batches that\nblend newly generated rollouts with these replayed successes. By replaying\nhigh-quality examples, RLEP steers the model away from fruitless exploration,\nfocuses learning on promising reasoning paths, and delivers both faster\nconvergence and stronger final performance. On the Qwen2.5-Math-7B base model,\nRLEP reaches baseline peak accuracy with substantially fewer updates and\nultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,\non AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our\ncode, datasets, and checkpoints are publicly available at\nhttps://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07451.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6474b290d815855e4ef59b05",
      "avatarUrl": "/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg",
      "fullname": "Hongzhi Zhang",
      "name": "hongzhizhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05065",
      "authors": [
        {
          "_id": "68776e57ff8f47a7f86442bd",
          "user": {
            "_id": "669e707ec517d804cfce91c5",
            "avatarUrl": "/avatars/fc18e64e100cbf28763db04b44242747.svg",
            "isPro": false,
            "fullname": "Corrado Rainone",
            "user": "crainone",
            "type": "user"
          },
          "name": "Corrado Rainone",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-16T15:05:55.722Z",
          "hidden": false
        },
        {
          "_id": "68776e57ff8f47a7f86442be",
          "name": "Tim Bakker",
          "hidden": false
        },
        {
          "_id": "68776e57ff8f47a7f86442bf",
          "name": "Roland Memisevic",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T14:49:18.000Z",
      "submittedOnDailyAt": "2025-07-17T06:25:52.334Z",
      "title": "논리론을 소규모 언어 모델에서 가능하게 할 수 있는 방법 중 하나는 사고를 도구의 사용으로 대체하는 것입니다.",
      "submittedOnDailyBy": {
        "_id": "669e707ec517d804cfce91c5",
        "avatarUrl": "/avatars/fc18e64e100cbf28763db04b44242747.svg",
        "isPro": false,
        "fullname": "Corrado Rainone",
        "user": "crainone",
        "type": "user"
      },
      "summary": "최근의 발전은 추론 시와 학습 시의 계산량의 확장에 기반한 새로운 기계 학습 패러다임의 확립을 이루었습니다. 그 연구의 흐름에서, 합성된 예시에 대한 Supervised Fine-Tuning (SFT)과 증명 가능한 보상을 가지는 Reinforcement Learning (RLVR)의 조합을 사용하여, 큰 규모의 언어 모델을 학습시키고, 추론 시에 자연어로 표현되는 「생각」의 형태를 가미하여 추가적인 계산량을 부담하는 것을 목표로 하고 있습니다. 이 논문에서는, 이러한 토큰을 상태 유지 도구와의 다턴 상호작용 트레이스 형식으로 구성하는 것을 제안합니다. 각 턴에서, 새로운 도구의 상태는 모델의 컨텍스트에 추가되고, 모델의 작업은 사용자 정의의 DSL을 기반으로 토큰을 생성하여 도구를 제어하는 것을 목표로 합니다. Python 코드의 수정 문제를 이 접근 방식을 벤치마크하고, 이 제한된 환경에서, 경험의 샘플링이 빠르게, 보상 신호가 밀집해지고, 더 나아가 3B 파라미터의 모델을 학습할 수 있음을 보여줍니다.",
      "upvotes": 1,
      "discussionId": "68776e58ff8f47a7f86442c0",
      "ai_summary": "A new approach formats tokens as a multi-turn interaction trace with a stateful tool for training Large Language Models, enabling faster sampling and denser reward signals for tasks like repairing Python code.",
      "ai_keywords": [
        "Supervised Fine-Tuning",
        "Reinforcement Learning with Verifiable Rewards",
        "Large Language Models",
        "multi-turn interaction trace",
        "stateful tool",
        "custom DSL"
      ]
    },
    "publishedAt": "2025-07-07T10:49:18.000Z",
    "title": "Replacing thinking with tool usage enables reasoning in small language\n  models",
    "summary": "Recent advances have established a new machine learning paradigm based on\nscaling up compute at inference time as well as at training time. In that line\nof work, a combination of Supervised Fine-Tuning (SFT) on synthetic\ndemonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is\nused for training Large Language Models to expend extra compute during\ninference in the form of \"thoughts\" expressed in natural language. In this\npaper, we propose to instead format these tokens as a multi-turn interaction\ntrace with a stateful tool. At each turn, the new state of the tool is appended\nto the context of the model, whose job is to generate the tokens necessary to\ncontrol the tool via a custom DSL. We benchmark this approach on the problem of\nrepairing malfunctioning Python code, and show that this constrained setup\nallows for faster sampling of experience and a denser reward signal, allowing\neven models of size up to 3B parameters to learn how to proficiently expend\nadditional compute on the task.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05065.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669e707ec517d804cfce91c5",
      "avatarUrl": "/avatars/fc18e64e100cbf28763db04b44242747.svg",
      "fullname": "Corrado Rainone",
      "name": "crainone",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]