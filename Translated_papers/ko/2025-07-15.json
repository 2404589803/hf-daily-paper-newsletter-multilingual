[
  {
    "paper": {
      "id": "2507.09862",
      "authors": [
        {
          "_id": "6875c14a257d4f043537056b",
          "name": "Youliang Zhang",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f043537056c",
          "name": "Zhaoyang Li",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f043537056d",
          "name": "Duomin Wang",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f043537056e",
          "name": "Jiahe Zhang",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f043537056f",
          "name": "Deyu Zhou",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f0435370570",
          "name": "Zixin Yin",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f0435370571",
          "name": "Xili Dai",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f0435370572",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f0435370573",
          "name": "Xiu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-14T02:22:47.000Z",
      "submittedOnDailyAt": "2025-07-15T01:26:49.276Z",
      "title": "대규모 고품질 데이터셋의 음성-시각적 관련성있는 딥인턴트 인터랙티브 인간 생성",
      "submittedOnDailyBy": {
        "_id": "64ae9b88a22a179fc4d07992",
        "avatarUrl": "/avatars/c9065f04a1188ea3129e56a90328ffd3.svg",
        "isPro": false,
        "fullname": "wang",
        "user": "dorni",
        "type": "user"
      },
      "summary": "대규모 모델의 급격한 개발은 디지털 인간 영역에서 큰 진전을 촉발시켰습니다. 이러한 첨단 방법들은 Avatar-driven 및 Rendering에 고품질의 해결책을 제공하며, 학계가 다음 큰 도전으로 음성-시각 이중 상호작용 가상 인간에 초점을 맞추고 있습니다. 이 새로운 영역의 연구를 촉진하기 위해, SpeakerVid-5M 데이터셋을 제공합니다. 이는 첫 번째 대규모, 고품질의 데이터셋으로, Alodia Virtual Human의 상호작용을 위한 설계되었습니다. 총 시간은 8,743 시간 이상, 5,200만 이상의 인물의 비디오 클립을 포함합니다. 이는 단일 대화, 듣기, 그리고 이중 상호작용을 포함한 다양한 규모와 상호작용 유형을 커버합니다. 중요한 점은, 데이터셋은 상호작용 유형과 데이터 품질의 두 가지 주요 차원에 따라 구성됩니다. 먼저, 상호작용 시나리오에 기반하여 4가지 유형으로 분류됩니다 (다이라그브 분야, 단일 분야, 듣기 분야, 다중 타어 영역). 다음으로, 규모적인 사전 편집 서브셋과, 서브 채널에서 고품질의 서브셋은 서브 채널 최종 훈련(SFT)에 사용됩니다. 이 이중 구조는 2D 가상 인간의 다양한 태스크를 수행할 수 있습니다. 또한, 이 데이터에 기반하여 자동 복원(AR) 기반의 비디오 채팅 기반 라인을 제공하고, 특별한 메트릭 세트와 테스트 데이터 제공을 통해 향후 연구를 지원하는 비디오 채팅 벤치마크(VidChatBench)를 제공합니다. 이 데이터셋과 대응하는 데이터 처리 코드는 공개적으로 릴리즈됩니다. 프로젝트 페이지: https://dorniwang.github.io/SpeakerVid-5M/",
      "upvotes": 27,
      "discussionId": "6875c14a257d4f0435370574",
      "projectPage": "https://dorniwang.github.io/SpeakerVid-5M/",
      "ai_summary": "A large-scale dataset named SpeakerVid-5M is introduced for audio-visual dyadic interactive virtual human generation, featuring diverse interactions and high-quality data for various virtual human tasks.",
      "ai_keywords": [
        "audio-visual dyadic interactive virtual human",
        "SpeakerVid-5M",
        "video clips",
        "monadic talking",
        "listening",
        "dyadic conversations",
        "dialogue branch",
        "single branch",
        "listening branch",
        "multi-turn branch",
        "pre-training subset",
        "Supervised Fine-Tuning",
        "autoregressive",
        "video chat baseline",
        "VidChatBench"
      ]
    },
    "publishedAt": "2025-07-13T22:22:47.000Z",
    "title": "SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual\n  Dyadic Interactive Human Generation",
    "summary": "The rapid development of large-scale models has catalyzed significant\nbreakthroughs in the digital human domain. These advanced methodologies offer\nhigh-fidelity solutions for avatar driving and rendering, leading academia to\nfocus on the next major challenge: audio-visual dyadic interactive virtual\nhuman. To facilitate research in this emerging area, we present SpeakerVid-5M\ndataset, the first large-scale, high-quality dataset designed for audio-visual\ndyadic interactive virtual human generation. Totaling over 8,743 hours,\nSpeakerVid-5M contains more than 5.2 million video clips of human portraits. It\ncovers diverse scales and interaction types, including monadic talking,\nlistening, and dyadic conversations. Crucially, the dataset is structured along\ntwo key dimensions: interaction type and data quality. First, it is categorized\ninto four types (dialogue branch, single branch, listening branch and\nmulti-turn branch) based on the interaction scenario. Second, it is stratified\ninto a large-scale pre-training subset and a curated, high-quality subset for\nSupervised Fine-Tuning (SFT). This dual structure accommodates a wide array of\n2D virtual human tasks. In addition, we provide an autoregressive (AR)-based\nvideo chat baseline trained on this data, accompanied by a dedicated set of\nmetrics and test data to serve as a benchmark VidChatBench for future work.\nBoth the dataset and the corresponding data processing code will be publicly\nreleased. Project page: https://dorniwang.github.io/SpeakerVid-5M/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09862.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64ae9b88a22a179fc4d07992",
      "avatarUrl": "/avatars/c9065f04a1188ea3129e56a90328ffd3.svg",
      "fullname": "wang",
      "name": "dorni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.10548",
      "authors": [
        {
          "_id": "6875d6e7257d4f04353705b5",
          "name": "Mingxian Lin",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705b6",
          "name": "Wei Huang",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705b7",
          "name": "Yitang Li",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705b8",
          "name": "Chengjie Jiang",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705b9",
          "name": "Kui Wu",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705ba",
          "name": "Fangwei Zhong",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705bb",
          "name": "Shengju Qian",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705bc",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705bd",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/sVJXSwN-mBG1ahHjfHx7V.gif"
      ],
      "publishedAt": "2025-07-14T17:59:46.000Z",
      "submittedOnDailyAt": "2025-07-15T03:13:21.491Z",
      "title": "EmbRACE-3K: 복잡한 환경에서 구체적인 이유와 행동",
      "submittedOnDailyBy": {
        "_id": "656db3f53dc1d277e5a64410",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png",
        "isPro": false,
        "fullname": "Wei Huang",
        "user": "AaronHuangWei",
        "type": "user"
      },
      "summary": "최근 발전된 시각 언어 모델(VLMs)은 과거의 이미지 및 비디오 이해 태스크에서 강력한 성능을 보여주고 있습니다. 그러나 이들이 실제 환경에서 효과적인 성능을 발휘하려면, 온라인 인터랙션과 동적인 시나리오 이해가 필요합니다. 이러한 시나리오에서, 에이전트는 한 사람의 관점에서 환경을 감시하고, 각 행동이 후속의 감시를 동적으로 형성합니다. 가장 최신의 모델인 GPT-4o, Claude 3.5 Sonnet, 그리고 Gemini 2.5 Pro는 개방형 엔비라멘트 인터랙션에서 문제를 발견하고, 공간 추론과 장기 계획의 명확한 한계가 있음을 보여주고 있습니다. 이러한 한계를 보완하기 위해, 우리는 EmRACE-3K를 소개합니다. EmRACE-3K는 Unreal Engine와 UnrealCV-Zoo 프레임워크를 사용하여 구축된 다양한 현실적인 환경에서 3,000 이상의 언어指南된 태스크의 데이터셋입니다. 태스크는 다양한 실제 환경의 도전을 포함하며, 지도, 객체 조작, 그리고 단계별 목표 실행을 포함합니다. 각 태스크는 단계별 프로젝트로 진행되며, 한 사람의 시각 감시와 고 수준의 지시, 기준 행동, 그리고 자연어로 표현된 이유를 결합하여 에이전트의 의도를 표현합니다. EmRACE-3K를 사용하여, 우리는 VLMs의 실제 환경에서 추론 능력을 평가하는 벤치마크를 구축합니다. 이 벤치마크에서, 탐색, 동적인 스펙트럴 세ман틱 추론, 그리고 단계별 목표 실행의 3가지 주요 차원을 제시합니다. 0 shot 설정에서, 모든 모델의 성공율은 20% 미만입니다. 이는 우리의 벤치마크에 의한 도전과 VLMs의 현재 한계를 명확히 합니다. EmRACE-3K의 유용성을 보여주기 위해, 우리는 Qwen2.5-VL-7B를 지도 학습을 사용하여 발전시키고, 강화 학습을 계속합니다. 이 접근법은 3가지 도전 카테고리 모두에서 큰 개선을 가져오고, 데이터셋의 효율성을 에이전트의 실제 환경에서 추론 능력의 개발에 따라 보여주고 있습니다.",
      "upvotes": 21,
      "discussionId": "6875d6e7257d4f04353705be",
      "projectPage": "https://mxllc.github.io/EmbRACE-3K/",
      "githubRepo": "https://github.com/mxllc/EmbRACE-3K",
      "ai_summary": "A new dataset, EmRACE-3K, evaluates vision-language models in embodied settings, showing limitations in spatial reasoning and long-horizon planning, and demonstrates improvements through supervised and reinforcement learning fine-tuning.",
      "ai_keywords": [
        "vision-language models",
        "embodied settings",
        "first-person perspective",
        "dynamic spatial reasoning",
        "long-horizon planning",
        "EmRACE-3K",
        "Unreal Engine",
        "UnrealCV-Zoo",
        "navigation",
        "object manipulation",
        "multi-stage goal execution",
        "zero-shot settings",
        "supervised learning",
        "reinforcement learning"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-07-14T13:59:46.000Z",
    "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments",
    "summary": "Recent advanced vision-language models(VLMs) have demonstrated strong\nperformance on passive, offline image and video understanding tasks. However,\ntheir effectiveness in embodied settings, which require online interaction and\nactive scene understanding remains limited. In such scenarios, an agent\nperceives the environment from a first-person perspective, with each action\ndynamically shaping subsequent observations. Even state-of-the-art models such\nas GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment\ninteractions, exhibiting clear limitations in spatial reasoning and\nlong-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset\nof over 3,000 language-guided tasks situated in diverse, photorealistic\nenvironments constructed using Unreal Engine and the UnrealCV-Zoo framework.\nThe tasks encompass a wide range of embodied challenges, including navigation,\nobject manipulation, and multi-stage goal execution. Each task unfolds as a\nmulti-step trajectory, pairing first-person visual observations with high-level\ninstructions, grounded actions, and natural language rationales that express\nthe agent's intent at every step. Using EmRACE-3K, we establish a benchmark to\nevaluate the embodied reasoning capabilities of VLMs across three key\ndimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage\nGoal Execution. In zero-shot settings, all models achieve success rates below\n20%, underscoring the challenge posed by our benchmark and the current\nlimitations of VLMs in interactive environments. To demonstrate the utility of\nEmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning\nfollowed by reinforcement learning. This approach yields substantial\nimprovements across all three challenge categories, highlighting the dataset's\neffectiveness in enabling the development of embodied reasoning capabilities.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/sVJXSwN-mBG1ahHjfHx7V.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10548.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "656db3f53dc1d277e5a64410",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png",
      "fullname": "Wei Huang",
      "name": "AaronHuangWei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.10532",
      "authors": [
        {
          "_id": "6875f107257d4f0435370613",
          "name": "Mingqi Wu",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f0435370614",
          "name": "Zhihao Zhang",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f0435370615",
          "name": "Qiaole Dong",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f0435370616",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f0435370617",
          "name": "Jun Zhao",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f0435370618",
          "name": "Senjie Jin",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f0435370619",
          "name": "Xiaoran Fan",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f043537061a",
          "name": "Yuhao Zhou",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f043537061b",
          "name": "Yanwei Fu",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f043537061c",
          "name": "Qin Liu",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f043537061d",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f043537061e",
          "name": "Qi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-14T17:55:15.000Z",
      "submittedOnDailyAt": "2025-07-15T04:41:41.806Z",
      "title": "이론 또는 기억? 비정상적인 결과를 초래하는 강화학습에서 데이터의 오염에 의한 영향",
      "submittedOnDailyBy": {
        "_id": "630716d11801ecc7d2595021",
        "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
        "isPro": false,
        "fullname": "Songyang Zhang",
        "user": "zsytony",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)의 인력은 장기적인 연구의 초점이었습니다. 최근의 연구는 강화 학습(RL)을 사용하여 이러한 능력을 진화시키고, 외부의 서브 프로덕션을 최소한으로 또는 전혀 개선하여 새로운 방법을 제시하고 있습니다. 놀라울 정도로 일부 연구는 무작위하거나 부정확한 보상 신호가 인력 향상을 유도하는 것을 시사하고 있습니다. 그러나 이러한 혁신은 주로 Qwen2.5 모델 가족에 대해 보고되어 있으며, MATH-500, AMC, AIME 등 유명한 벤치마크에서 평가되어 왔지만, Llama나 다른 모델에 대해 동일한 효과를 달성하지 못합니다. 우리 분석에 따르면 Qwen2.5는 수학적 인력을 높일 수 있습니다만, 규모가 큰 웹 크롤링 데이터에 의한 사전 학습에 의해, 인기 벤치마크에서 데이터 복사에 취약합니다. 그 결과, 이러한 벤치마크에서 얻은 결과를 믿지 않습니다. 이러한 문제를 해결하기 위해, 임의의 길이와 난이도의 합성적인 계산 문제를 생성하는 생성자를 소개합니다. 이러한 데이터 세트를 사용하여, 데이터 복사 없이 데이터 세트를 사용하여, 정확한 보상 신호만 일관되게 성능을 향상시키고, 노이즈나 부정확한 신호는 개선되지 않는 것을 보여줍니다. 이러한 결과를 믿을 수 있도록, RL 방법의 평가는 데이터 복사 없이 벤치마크와 다양한 모델 가족에 대해 수행하는 것을 주장합니다.",
      "upvotes": 20,
      "discussionId": "6875f107257d4f043537061f",
      "ai_summary": "Research on enhancing LLM reasoning through RL reveals that accurate reward signals are crucial for performance improvement, and current benchmarks may be unreliable due to data contamination.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "reinforcement learning",
        "RL",
        "Qwen2.5",
        "MATH-500",
        "AMC",
        "AIME",
        "Llama",
        "pretraining",
        "large-scale web corpora",
        "data contamination",
        "synthetic arithmetic problems",
        "RandomCalculation",
        "leakage-free datasets",
        "reward signals"
      ]
    },
    "publishedAt": "2025-07-14T13:55:15.000Z",
    "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning\n  Due to Data Contamination",
    "summary": "The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10532.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630716d11801ecc7d2595021",
      "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
      "fullname": "Songyang Zhang",
      "name": "zsytony",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04404",
      "authors": [
        {
          "_id": "6875d1a3257d4f043537058e",
          "name": "Jingze Zhu",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f043537058f",
          "name": "Yongliang Wu",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370590",
          "name": "Wenbo Zhu",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370591",
          "name": "Jiawang Cao",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370592",
          "name": "Yanqiang Zheng",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370593",
          "name": "Jiawei Chen",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370594",
          "name": "Xu Yang",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370595",
          "name": "Bernt Schiele",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370596",
          "name": "Jonas Fischer",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370597",
          "name": "Xinting Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-06T14:35:43.000Z",
      "submittedOnDailyAt": "2025-07-15T02:28:43.418Z",
      "title": "레이어케이크: 대 언어 모델 내부에서 토큰 정보를 인식한 비교적 결정\n  층",
      "submittedOnDailyBy": {
        "_id": "66f6bc97980d52c75c300511",
        "avatarUrl": "/avatars/f7c23c4b09701580b533212ec9b6e306.svg",
        "isPro": false,
        "fullname": "Yongliang",
        "user": "Liang0223",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 자연어 이해와 생성에 뛰어난 성능을 가지고 있지만, 사실적인 오류에 취약하며, 지식 밀도형 태스크에서 신뢰도를 제한하고 있습니다. 디코딩 시의 전략은 훈련을 필요로 하지 않고 효율적인 해결책을 제공하지만, 현재의 방법은 일반적으로 토큰 수준과 레이어 수준의 신호를 분리하여 분석하고, 그 사이의 미세한 움직임을 감지하지 않습니다. 본 연구에서는 특정 토큰 타입과 가장 영향력 있는 트랜스포머 레이어를 결합한 토큰 관찰적, 레이어 국부적, 비교적 디코딩 방법을 소개합니다. 실험적인 어턴션 분석을 통해 두 가지 주요 패턴을 식별했습니다: 기호 토큰은 초기 레이어에서 주된 어턴션을 받습니다, 개념적인 토큰은 중간 레이어에서 언어학적 이유를 주도합니다. 이러한 토큰 타입의 선택적 어턴션의 억제로, 제어된 사실적인 악화의 도입과 최종적인 사실적인 디코딩을 유도하는 비교적 신호를 얻을 수 있습니다. 우리의 방법은 추가적인 훈련이나 모델의 수정이 필요하지 않습니다, 실험은 여러 LLMs와 다양한 벤치마크에서 일관된 사실성을 개선하는 것을 보여주었습니다.",
      "upvotes": 16,
      "discussionId": "6875d1a3257d4f0435370598",
      "ai_summary": "A token-aware, layer-localized contrastive decoding method improves factual accuracy in large language models by selectively suppressing attention to specific token types at their respective depths.",
      "ai_keywords": [
        "large language models",
        "natural language understanding",
        "natural language generation",
        "factual errors",
        "decoding-time strategies",
        "token-level signals",
        "layer-level signals",
        "token-aware",
        "layer-localized",
        "contrastive decoding",
        "transformer layers",
        "punctuation tokens",
        "conceptual tokens",
        "semantic reasoning",
        "empirical attention analysis",
        "factual generation",
        "controlled factual degradation",
        "contrastive signals",
        "factual decoding"
      ]
    },
    "publishedAt": "2025-07-06T10:35:43.000Z",
    "title": "LayerCake: Token-Aware Contrastive Decoding within Large Language Model\n  Layers",
    "summary": "Large language models (LLMs) excel at natural language understanding and\ngeneration but remain vulnerable to factual errors, limiting their reliability\nin knowledge-intensive tasks. While decoding-time strategies provide a\npromising efficient solution without training, existing methods typically treat\ntoken-level and layer-level signals in isolation, overlooking the joint\ndynamics between them. In this work, we introduce a token-aware,\nlayer-localized contrastive decoding method that aligns specific token types\nwith their most influential transformer layers to improve factual generation.\nThrough empirical attention analysis, we identify two key patterns: punctuation\ntokens receive dominant attention in early layers, while conceptual tokens\ngovern semantic reasoning in intermediate layers. By selectively suppressing\nattention to these token types at their respective depths, we achieve the\ninduction of controlled factual degradation and derive contrastive signals to\nguide the final factual decoding. Our method requires no additional training or\nmodel modification, and experiments demonstrate that our method consistently\nimproves factuality across multiple LLMs and various benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04404.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66f6bc97980d52c75c300511",
      "avatarUrl": "/avatars/f7c23c4b09701580b533212ec9b6e306.svg",
      "fullname": "Yongliang",
      "name": "Liang0223",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.10541",
      "authors": [
        {
          "_id": "6875e5f0257d4f04353705de",
          "name": "Zhuoshi Pan",
          "hidden": false
        },
        {
          "_id": "6875e5f0257d4f04353705df",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "6875e5f0257d4f04353705e0",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "6875e5f0257d4f04353705e1",
          "name": "Qiyao Sun",
          "hidden": false
        },
        {
          "_id": "6875e5f0257d4f04353705e2",
          "name": "Zinan Tang",
          "hidden": false
        },
        {
          "_id": "6875e5f0257d4f04353705e3",
          "name": "H. Vicky Zhao",
          "hidden": false
        },
        {
          "_id": "6875e5f0257d4f04353705e4",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "6875e5f0257d4f04353705e5",
          "name": "Lijun Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-14T17:58:47.000Z",
      "submittedOnDailyAt": "2025-07-15T05:04:35.807Z",
      "title": "REST: 다수의 문제를 동시에 요청하여 대규모의 논리모형에 효과적으로 에스트테스 테스트를 수행할 수 있습니다.",
      "submittedOnDailyBy": {
        "_id": "66580d3d80ee5b1e11a94e57",
        "avatarUrl": "/avatars/1a88e7337f9095c40c6d402fab797d83.svg",
        "isPro": false,
        "fullname": "Zinan Tang",
        "user": "Word2Li",
        "type": "user"
      },
      "summary": "최근의 대규모 논리 모델(LRMs)은 특정한 태스크에 대한 벤치마크에서 놀라운 진전을 달성하지만, 평가 방법이 분리된 문제 해결 패러다임에 제한되어 있습니다. 현재의 벤치마크는 순차적인 테스트를 통해 단일 질문의 논리 모델을 평가하며, 다음 중요한 제한을 가지고 있습니다: 1) 데이터 오염에 취약하며, 문제가 적은 상태(예: DeepSeek-R1은 MATH500에서 97.0%를 달성), 새로운 질문의 생성에 큰 인간적인 노력을 필요로 합니다, 2) 실제 세계적인 도입에 필요한 다 컨텍스트의 압력 하에서의 모델 평가에 실패합니다. 이를 보완하기 위해, 우리는 동시적인 테스트를 통해 논리 평가(REST)를 제안합니다. REST는 기본적인 논리 모델보다, 컨텍스트의 우선 순위 배정, 문제 간 간섭抵抗, 동적인 인지부하 관리 평가를 수행합니다. 평가 결과, 다음 놀라운 발견이 밝혀졌습니다: 상태의 가장 선진 모델인 DeepSeek-R1도 스트레스 테스트에서 성능의 큰 하락을 나타냅니다. 중요한 것은, REST는 현재의 벤치마크보다 더 강한 구분력을 나타내며, 근사한 임계점 성능을 나타내는 모델 사이에 명확한 성능 차이를 나타냅니다. 분석에서 다음 기계적인 관점을 도출했습니다: 1) \"초과적 생각의 헛간\"은 성능 저하에 중요한 원인입니다; 2) \"긴 2 짧은\" 기술로 훈련된 모델은 REST에서 단일 문제의 성능의 정확도를 더 잘 유지하고, 표준 훈련 모델을 초과합니다. 이러한 결과는, REST는 미래의 평가 패러다임에서 비용 효율적이고, 실제 세계적인 논리 요구를 더 정확하게 반영하고, 지속적인 인간 Annotation 의존성을 줄이는 것을 보여줍니다.",
      "upvotes": 14,
      "discussionId": "6875e5f0257d4f04353705e6",
      "projectPage": "https://opendatalab.github.io/REST/",
      "githubRepo": "https://github.com/opendatalab/REST",
      "ai_summary": "REST evaluates large reasoning models under simultaneous multi-context pressure, revealing performance differences not apparent in single-question tests and highlighting the importance of contextual priority allocation and cognitive load management.",
      "ai_keywords": [
        "Large Reasoning Models",
        "REST",
        "stress-testing framework",
        "contextual priority allocation",
        "cross-problem interference resistance",
        "dynamic cognitive load management",
        "overthinking trap",
        "long2short technique"
      ],
      "githubStars": 14
    },
    "publishedAt": "2025-07-14T13:58:47.000Z",
    "title": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems\n  at Once",
    "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual\ncreation of new questions with large human efforts, (2) failure to evaluate\nmodels under multi-context pressure, a key requirement for real-world\ndeployment. To bridge this gap, we present REST (Reasoning Evaluation through\nSimultaneous Testing), a stress-testing framework that concurrently exposes\nLRMs to multiple problems simultaneously. Beyond basic reasoning, REST\nspecifically evaluates several under-tested capabilities: contextual priority\nallocation, cross-problem interference resistance, and dynamic cognitive load\nmanagement. Our evaluation reveals several striking findings: Even\nstate-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance\ndegradation under stress testing. Crucially, REST demonstrates stronger\ndiscriminative power than existing benchmarks, revealing pronounced performance\ndifferences among models that exhibit similar, near-ceiling performance under\nsingle-question evaluations. Some key mechanistic insights emerge from our\nanalysis: (1) the \"overthinking trap\" is a critical factor contributing to the\nperformance degradation; (2) the models trained with \"long2short\" technique\npreserve more accuracy of their single-problem performance under REST,\noutperforming standard-trained counterparts. These results establish REST as a\ncost-efficient, future-proof evaluation paradigm that better reflects\nreal-world reasoning demands while reducing reliance on continuous human\nannotation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10541.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66580d3d80ee5b1e11a94e57",
      "avatarUrl": "/avatars/1a88e7337f9095c40c6d402fab797d83.svg",
      "fullname": "Zinan Tang",
      "name": "Word2Li",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.10524",
      "authors": [
        {
          "_id": "6875e531257d4f04353705d1",
          "name": "Sangmin Bae",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d2",
          "name": "Yujin Kim",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d3",
          "name": "Reza Bayat",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d4",
          "name": "Sungnyun Kim",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d5",
          "name": "Jiyoun Ha",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d6",
          "name": "Tal Schuster",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d7",
          "name": "Adam Fisch",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d8",
          "name": "Hrayr Harutyunyan",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d9",
          "name": "Ziwei Ji",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705da",
          "name": "Aaron Courville",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705db",
          "name": "Se-Young Yun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-14T17:49:00.000Z",
      "submittedOnDailyAt": "2025-07-15T03:52:43.642Z",
      "title": "Mixture-of-Recursions: 재귀 깊이에 따른 동적 토큰 수준 계산의 적응성 학습",
      "submittedOnDailyBy": {
        "_id": "6602ca1e10a1441af41637be",
        "avatarUrl": "/avatars/5880e699def320beb352cbed77495b2f.svg",
        "isPro": false,
        "fullname": "Sangmin Bae",
        "user": "raymin0223",
        "type": "user"
      },
      "summary": "スケーリングラングアウェイモデルは卓越した能力を開発しますが、伴う計算量とメモリーの要求はトレーニングおよび部署にも高額な費用を伴っています。現在の効率向上の努力は通常パラメーター共有や適応計算を目指していますが、両方を同時に達成する方法は不明です。私たちは、Recursive Transformer内で効率の2つの軸を統合するMixture-of-Recursions (MoR)を紹介します。MoRは、再利用可能な層のスタックを各再帰ステップで共有してパラメーター効率を達成し、軽量なルーターは個々のトークンに対して動的に異なる再帰深さを割り当ててトークンレベルの思考を適応的に行うことを可能にします。これにより、MoRは、特定の再帰深さでまだ活性化されているトークンの間のみオーナシャン計算を焦点にし、選択的にそのみのKey-Valueペアをキャッシュすることでメモリアクセスの効率を進めます。これらの核心機能の上で、KV共有の変体も提案します。KVペアを最初の再帰から再利用し、特にプリフィルラテンシーとメモリーフットプリースを減少させることを目的としています。モデルサイズが135Mから1.7Bパラメーターまで範囲を広げることで、MoRは新たなパロードファントリーを形成します：等しいトレーニングFLOPと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さなモデルサイズで、訓練フロープと小さ",
      "upvotes": 13,
      "discussionId": "6875e531257d4f04353705dc",
      "githubRepo": "https://github.com/raymin0223/mixture_of_recursions",
      "ai_summary": "Mixture-of-Recursions (MoR) achieves parameter and computational efficiency in large language models through shared layers and adaptive recursion depths, improving performance metrics and throughput.",
      "ai_keywords": [
        "Mixture-of-Recursions",
        "MoR",
        "Recursive Transformer",
        "parameter efficiency",
        "adaptive computation",
        "lightweight routers",
        "token-level thinking",
        "recursion depth",
        "quadratic attention computation",
        "key-value pairs",
        "KV sharing",
        "prefill latency",
        "memory footprint",
        "validation perplexity",
        "few-shot accuracy",
        "throughput"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-07-14T13:49:00.000Z",
    "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
    "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10524.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602ca1e10a1441af41637be",
      "avatarUrl": "/avatars/5880e699def320beb352cbed77495b2f.svg",
      "fullname": "Sangmin Bae",
      "name": "raymin0223",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.09104",
      "authors": [
        {
          "_id": "6875bfaa257d4f0435370564",
          "name": "Taolin Zhang",
          "hidden": false
        },
        {
          "_id": "6875bfaa257d4f0435370565",
          "name": "Maosong Cao",
          "hidden": false
        },
        {
          "_id": "6875bfaa257d4f0435370566",
          "name": "Alexander Lam",
          "hidden": false
        },
        {
          "_id": "6875bfaa257d4f0435370567",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "6875bfaa257d4f0435370568",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-12T01:34:24.000Z",
      "submittedOnDailyAt": "2025-07-15T02:47:17.884Z",
      "title": "CompassJudger-2: 보상의 가능성에 기반한 일반적인 판단 모델의 방향을 확인합니다.",
      "submittedOnDailyBy": {
        "_id": "630716d11801ecc7d2595021",
        "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
        "isPro": false,
        "fullname": "Songyang Zhang",
        "user": "zsytony",
        "type": "user"
      },
      "summary": "최근 LLM-as-judge는 대 언어 모델의 평가에서 중요한 역할을 하고 있습니다. 그러나 현재의 판단 모델은 좁은 전문화와 제한된 강건성을 가지고 있으며, 전체적인 평가 능력이 저하되어 있습니다. 본 논문에서는, 태스크를 주도하는 다중 분야 데이터의 카레팅 전략을 통해 이러한 제한을 극복하는 새로운 일반적인 판단 모델인 CompassJudger-2를 소개합니다. 우리의 접근의 핵심은, 검증 가능한 보상을 가지는 판단 태스크를 대상으로 하고, 거부 샘플링을 통해 내적적인 비판적인 이유를 이어나가며, 강건하고 일반화 가능한 판단 능력을 양성하는 것입니다. 또한, 마진 정책 그레이디ン드 스키 기법을 사용한 개선된 학습 목표를 소개합니다. 실험적으로는, CompassJudger-2는 여러 판단 벤치마크에서 상위 결과를 얻으며, 우리 7B 모델은 DeepSeek-V3, Qwen3-235B-A22B와 같은 큰 모델과 비교하여 경쟁적인 판단 정확도를 나타냅니다. 또한, 우리는, 크로스 도메인의 판단 정확도와 순위의 일치성을 평가하는 세부적인 벤치마크인 JudgerBenchV2를 제안하고, 판단 모델의 평가를 표준화하는 것을 목표로 합니다. 이러한 기여는 강건하고 scalable한 LLM의 판단에 기여하고, 새로운 성능과 평가 기준을 확립합니다.",
      "upvotes": 12,
      "discussionId": "6875bfaa257d4f0435370569",
      "githubRepo": "https://github.com/open-compass/CompassJudger",
      "ai_summary": "CompassJudger-2, a generalist judge model, achieves superior performance across multiple benchmarks through task-driven data curation, verifiable rewards, and a refined learning objective with margin policy gradient loss.",
      "ai_keywords": [
        "LLM-as-judge",
        "generalist judge model",
        "task-driven",
        "multi-domain data curation",
        "verifiable rewards",
        "rejection sampling",
        "margin policy gradient loss",
        "JudgerBenchV2",
        "cross-domain judgment accuracy",
        "rank consistency"
      ],
      "githubStars": 98
    },
    "publishedAt": "2025-07-11T21:34:24.000Z",
    "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards",
    "summary": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09104.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630716d11801ecc7d2595021",
      "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
      "fullname": "Songyang Zhang",
      "name": "zsytony",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.10065",
      "authors": [
        {
          "_id": "6875ed50257d4f04353705f1",
          "name": "Chenguo Lin",
          "hidden": false
        },
        {
          "_id": "6875ed50257d4f04353705f2",
          "name": "Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "6875ed50257d4f04353705f3",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "6875ed50257d4f04353705f4",
          "name": "Yifan Yu",
          "hidden": false
        },
        {
          "_id": "6875ed50257d4f04353705f5",
          "name": "Honglei Yan",
          "hidden": false
        },
        {
          "_id": "6875ed50257d4f04353705f6",
          "name": "Katerina Fragkiadaki",
          "hidden": false
        },
        {
          "_id": "6875ed50257d4f04353705f7",
          "name": "Yadong Mu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/SYUoEzWMnM6UPGTKHPqzn.mp4"
      ],
      "publishedAt": "2025-07-14T08:49:57.000Z",
      "submittedOnDailyAt": "2025-07-15T04:26:29.071Z",
      "title": "ムービーズ：동작에 관심있는 4D 동적인 시각합성 1초 만에\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "62e18206926f4892a4c782bd",
        "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
        "isPro": false,
        "fullname": "Chenguo Lin",
        "user": "chenguolin",
        "type": "user"
      },
      "summary": "MovieS는, 1초당 4차원적인 동적인 새로운 시각을 합성하는 새로운 방향성 모델로, Monocular Video에서 시작하여 만들어졌습니다. MovieS는 Gaussian primitives를 사용하여 동적인 3차원 공간을 표현하며, 그 시간적 변화의 움직임을 명시적으로 제어합니다. 이로 인해, 外観, 형상 및 움직임의 통합 모델링이 가능해, 시선 합성, 재구성 및 3차원 포인트 트래킹을 학습 기반 프레임워크 내에서 실현할 수 있습니다. 새로운 시선 합성과 동적인 형상 재구성을 연결하여, MovieS는 복잡한 태스크 고유의 제어에 비해 최소한의 의존성을 가집니다. 이로써, 시선 흐름 예측 및 동적인 물체 분할 등 광범위한 영역의 0샷 애플리케이션을 자연스럽게 지원할 수 있습니다. 여러 태스크에서도 효과성과 효율성이 입증되어, 대응하는 성능을 얻으며, 수 차례의 속도 향상을 제공합니다.",
      "upvotes": 6,
      "discussionId": "6875ed51257d4f04353705f8",
      "projectPage": "https://chenguolin.github.io/projects/MoVieS",
      "githubRepo": "https://github.com/chenguolin/MoVieS",
      "ai_summary": "MoVieS synthesizes 4D dynamic novel views from monocular videos using Gaussian primitives, enabling unified modeling of appearance, geometry, and motion with minimal task-specific supervision.",
      "ai_keywords": [
        "feed-forward model",
        "Gaussian primitives",
        "time-varying motion",
        "view synthesis",
        "reconstruction",
        "3D point tracking",
        "scene flow estimation",
        "moving object segmentation"
      ],
      "githubStars": 35
    },
    "publishedAt": "2025-07-14T04:49:57.000Z",
    "title": "MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second",
    "summary": "We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic\nnovel views from monocular videos in one second. MoVieS represents dynamic 3D\nscenes using pixel-aligned grids of Gaussian primitives, explicitly supervising\ntheir time-varying motion. This allows, for the first time, the unified\nmodeling of appearance, geometry and motion, and enables view synthesis,\nreconstruction and 3D point tracking within a single learning-based framework.\nBy bridging novel view synthesis with dynamic geometry reconstruction, MoVieS\nenables large-scale training on diverse datasets with minimal dependence on\ntask-specific supervision. As a result, it also naturally supports a wide range\nof zero-shot applications, such as scene flow estimation and moving object\nsegmentation. Extensive experiments validate the effectiveness and efficiency\nof MoVieS across multiple tasks, achieving competitive performance while\noffering several orders of magnitude speedups.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/SYUoEzWMnM6UPGTKHPqzn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10065.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "62e18206926f4892a4c782bd",
      "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
      "fullname": "Chenguo Lin",
      "name": "chenguolin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08924",
      "authors": [
        {
          "_id": "6875aa3c257d4f043537052c",
          "name": "Seokhee Hong",
          "hidden": false
        },
        {
          "_id": "6875aa3c257d4f043537052d",
          "name": "Sunkyoung Kim",
          "hidden": false
        },
        {
          "_id": "6875aa3c257d4f043537052e",
          "name": "Guijin Son",
          "hidden": false
        },
        {
          "_id": "6875aa3c257d4f043537052f",
          "name": "Soyeon Kim",
          "hidden": false
        },
        {
          "_id": "6875aa3c257d4f0435370530",
          "name": "Yeonjung Hong",
          "hidden": false
        },
        {
          "_id": "6875aa3c257d4f0435370531",
          "name": "Jinsik Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:56:32.000Z",
      "submittedOnDailyAt": "2025-07-15T05:31:41.102Z",
      "title": "KMMLU-Redux에서 KMMLU-Pro로: LLM 평가에 적합한 전문적인 한국 벤치마크 시스템",
      "submittedOnDailyBy": {
        "_id": "60d3e619b8448e1785bbda2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
        "isPro": false,
        "fullname": "GUIJIN SON",
        "user": "amphora",
        "type": "user"
      },
      "summary": "LLM의 개발에는 학계뿐만 아니라 산업 분야까지 강력한 벤치마크가 필요합니다. 본 논문에서는 두 가지 한국의 전문가 수준 벤치마크를 소개합니다. KMMLU-Redux는 현재의 KMMLU에서 재구성되어 있습니다. 이 것은 한국의 국가 기술시험에서 나온 문제를 사용하며, 중요한 오류를 제거하고 신뢰성을 향상시켰습니다. KMMLU-Pro는 한국의 국가 전문직 시험을 기반으로 한국의 전문 지식에 반영되어 있습니다. 실험 결과를 통해 이러한 벤치마크는 한국의 산업 지식을 전반적으로 표현하고 있음을 시사합니다. 우리 데이터셋은 공개적으로 사용 가능합니다.",
      "upvotes": 1,
      "discussionId": "6875aa3c257d4f0435370532",
      "ai_summary": "Korean expert-level benchmarks, KMMLU-Redux and KMMLU-Pro, are introduced to evaluate Large Language Models across academic and industrial domains in Korea.",
      "ai_keywords": [
        "Large Language Models",
        "LLMS",
        "benchmarks",
        "KMMLU-Redux",
        "KMMLU-Pro",
        "Korean National Technical Qualification exams",
        "Korean National Professional Licensure exams",
        "industrial knowledge"
      ]
    },
    "publishedAt": "2025-07-11T13:56:32.000Z",
    "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for\n  LLM Evaluation",
    "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08924.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d3e619b8448e1785bbda2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
      "fullname": "GUIJIN SON",
      "name": "amphora",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 57
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08267",
      "authors": [
        {
          "_id": "6875e45f257d4f04353705cc",
          "name": "Hiroshi Yoshihara",
          "hidden": false
        },
        {
          "_id": "6875e45f257d4f04353705cd",
          "name": "Taiki Yamaguchi",
          "hidden": false
        },
        {
          "_id": "6875e45f257d4f04353705ce",
          "name": "Yuichi Inoue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T02:26:01.000Z",
      "submittedOnDailyAt": "2025-07-15T06:15:48.021Z",
      "title": "수학 LLMs의 실용적인 2단계 알고리즘: SFT를 사용하여 정확도를 최대화하고 강화학습을 사용하여 효율화하는 방법",
      "submittedOnDailyBy": {
        "_id": "63233b16462470712718c2a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1663253229258-noauth.png",
        "isPro": false,
        "fullname": "Inoue Yuichi",
        "user": "Inoichan",
        "type": "user"
      },
      "summary": "수학적 추론을 강화하기 위한 대규모 언어 모델(LLMs)의 발전은 인공지능 능력 향상에 있어서 중요한 문제입니다. SFT(지도 학습 미세 조정)과 RL(강화 학습)이 주도하는 훈련 패러다임이지만, 이들을 조합하여 정확성과 효율성을 최대화하는 체계적인 방법은 주로 탐색되고 있습니다. 본 논문에서는 실용적이고 효과적인 훈련 레시피를 통해, SFT의 확장과 RL(GRPO)를 전략적으로 조합하는 방법을 소개합니다. 이 방법은 서로 보완적인 역할을 하며, SFT의 장기 훈련은 모델의 정확도를 극한까지 높일 수 있고, 다음으로 GRPO의 단계에서는 토큰의 효율성을 크게 향상시키고, 이 뛰어난 성능을 유지하는 것을 주장합니다. 실험에 따르면, SFT의 장기 훈련은 성능의 큰突破에 중요하다는 것을 명확히 확인하였으며, 이 프레임워크에서 GRPO의 주요 역할은 결정의 길이를 최적화하는 것입니다. 이 레시피의 효과는 어려운 벤치마크에서 최상위 성능으로 엄격하게 검증되었으며, AI Mathematical Olympiad(AIMO)에서 2,200보다 많은 팀 중에서 높은 순위를 얻었습니다. 이 연구는 정확성과 실용적인 효율성을 겸비한 가장 先端의 수학적 추론 모델의 개발에 대한 실용적인 계획을 커뮤니티에 제공합니다. 완전한 재현성과 미래의 연구의 발전을 보장하기 위해, 오픈 소스화된 프레임워크, 모든 코드, 모델 체크포인트, 훈련 설정을 공개합니다. https://github.com/analokmaus/kaggle-aimo2-fast-math-r1",
      "upvotes": 1,
      "discussionId": "6875e45f257d4f04353705cf",
      "ai_summary": "A combination of extended supervised fine-tuning and reinforcement learning from online inference enhances the mathematical reasoning capabilities of large language models, achieving top-tier performance on benchmarks like the AI Mathematical Olympiad.",
      "ai_keywords": [
        "Supervised Fine-Tuning",
        "Reinforcement Learning",
        "GRPO",
        "token efficiency",
        "solution length optimization",
        "AI Mathematical Olympiad"
      ]
    },
    "publishedAt": "2025-07-10T22:26:01.000Z",
    "title": "A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy\n  with SFT and Efficiency with Reinforcement Learning",
    "summary": "Enhancing the mathematical reasoning of Large Language Models (LLMs) is a\npivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning\n(SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a\nsystematic methodology for combining them to maximize both accuracy and\nefficiency remains largely unexplored. This paper introduces a practical and\neffective training recipe that strategically integrates extended SFT with RL\nfrom online inference (GRPO). We posit that these methods play complementary,\nnot competing, roles: a prolonged SFT phase first pushes the model's accuracy\nto its limits, after which a GRPO phase dramatically improves token efficiency\nwhile preserving this peak performance. Our experiments reveal that extending\nSFT for as many as 10 epochs is crucial for performance breakthroughs, and that\nthe primary role of GRPO in this framework is to optimize solution length. The\nefficacy of our recipe is rigorously validated through top-tier performance on\nchallenging benchmarks, including a high rank among over 2,200 teams in the\nstrictly leak-free AI Mathematical Olympiad (AIMO). This work provides the\ncommunity with a battle-tested blueprint for developing state-of-the-art\nmathematical reasoners that are both exceptionally accurate and practically\nefficient. To ensure full reproducibility and empower future research, we will\nopen-source our entire framework, including all code, model checkpoints, and\ntraining configurations at\nhttps://github.com/analokmaus/kaggle-aimo2-fast-math-r1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08267.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63233b16462470712718c2a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1663253229258-noauth.png",
      "fullname": "Inoue Yuichi",
      "name": "Inoichan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  }
]