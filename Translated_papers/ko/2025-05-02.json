[
  {
    "paper": {
      "id": "2504.21853",
      "authors": [
        {
          "_id": "681441e64d6a681c7c840b1f",
          "name": "Jiwen Yu",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b20",
          "name": "Yiran Qin",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b21",
          "name": "Haoxuan Che",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b22",
          "name": "Quande Liu",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b23",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b24",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b25",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b26",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b27",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b28",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/Y9dvfNIIrOJ4rCzwIB6XI.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/5kPNXDj9LIsgShhFz5WSg.jpeg"
      ],
      "publishedAt": "2025-04-30T17:59:02.000Z",
      "submittedOnDailyAt": "2025-05-02T02:29:37.187Z",
      "title": "인터페이스 생성 비디오 조사",
      "submittedOnDailyBy": {
        "_id": "64105a6d14215c0775dfdd14",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
        "isPro": false,
        "fullname": "Jiwen Yu",
        "user": "VictorYuki",
        "type": "user"
      },
      "summary": "インタラクティブ・ジェネレーションビデオ(IGV)는 고품질의 인터랙티브 비디오 콘텐츠의 수요가 증가함에 따라 중요한 기술로 등장하게 되었습니다. 본 논문에서는 IGV를, 생성 능력과 함께 다양한 고품질의 비디오 콘텐츠를 생성하고 사용자의 참여를 촉진하기 위해 제공하는 인터랙티브 기능을 제공하는 기술로 정의합니다. 현재의 IGV 앱의 상태를 조사하고, 게임, 구체화 AI, 자율주행의 3가지 주요 분야에 초점을 맞추고 있습니다. 게임에서 IGV는 가상 세계의 무한한 탐험을 가능하게 하고 있습니다. 구체화 AI에서, 다양한 인터랙션을 가능하게 하는 동적인 환경 합성을 수행합니다. 자율주행에서, 안전을 위해 중요한 테스트와 검증을 위해 폐쇄된 후처리 시뮬레이션 기능을 제공합니다. 미래의 개발을 가이드하기 위해, ideale한 IGV 시스템을 생성, 제어, 메모리, 동역학, 지능 등 5가지 기본 모듈로 분해하여 상세한 프레임워크를 제안하고 있습니다. 또한 ideale한 IGV 시스템의 각 구성 요소의 구현에 대한 기술적 문제와 미래의 방향성을 체계적으로 분석하고 있습니다. 이 체계적인 분석은 IGV 분야의 향후 연구와 개발을 촉진하고, 더 복잡한 실용적인 애플리케이션으로 발전시키는 데 도움이 될 것이라고 믿습니다.",
      "upvotes": 24,
      "discussionId": "681441e84d6a681c7c840bae",
      "ai_keywords": [
        "generative capabilities",
        "interactive features",
        "control signals",
        "responsive feedback",
        "virtual worlds",
        "physics-aware environment synthesizer",
        "multimodal interaction",
        "dynamically evolving scenes",
        "closed-loop simulation",
        "safety-critical testing",
        "validation",
        "real-time generation",
        "open-domain control",
        "long-term coherence",
        "accurate physics",
        "causal reasoning"
      ]
    },
    "publishedAt": "2025-04-30T13:59:02.000Z",
    "title": "A Survey of Interactive Generative Video",
    "summary": "Interactive Generative Video (IGV) has emerged as a crucial technology in\nresponse to the growing demand for high-quality, interactive video content\nacross various domains. In this paper, we define IGV as a technology that\ncombines generative capabilities to produce diverse high-quality video content\nwith interactive features that enable user engagement through control signals\nand responsive feedback. We survey the current landscape of IGV applications,\nfocusing on three major domains: 1) gaming, where IGV enables infinite\nexploration in virtual worlds; 2) embodied AI, where IGV serves as a\nphysics-aware environment synthesizer for training agents in multimodal\ninteraction with dynamically evolving scenes; and 3) autonomous driving, where\nIGV provides closed-loop simulation capabilities for safety-critical testing\nand validation. To guide future development, we propose a comprehensive\nframework that decomposes an ideal IGV system into five essential modules:\nGeneration, Control, Memory, Dynamics, and Intelligence. Furthermore, we\nsystematically analyze the technical challenges and future directions in\nrealizing each component for an ideal IGV system, such as achieving real-time\ngeneration, enabling open-domain control, maintaining long-term coherence,\nsimulating accurate physics, and integrating causal reasoning. We believe that\nthis systematic analysis will facilitate future research and development in the\nfield of IGV, ultimately advancing the technology toward more sophisticated and\npractical applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/Y9dvfNIIrOJ4rCzwIB6XI.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/5kPNXDj9LIsgShhFz5WSg.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64105a6d14215c0775dfdd14",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
      "fullname": "Jiwen Yu",
      "name": "VictorYuki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.00662",
      "authors": [
        {
          "_id": "68142e4a551709da9244e8d1",
          "user": {
            "_id": "64b7df742f5a966b973e25f7",
            "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
            "isPro": false,
            "fullname": "Wenkai Yang",
            "user": "Keven16",
            "type": "user"
          },
          "name": "Wenkai Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-02T06:34:18.531Z",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d2",
          "name": "Jingwen Chen",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d3",
          "name": "Yankai Lin",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d4",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T17:03:17.000Z",
      "submittedOnDailyAt": "2025-05-02T01:12:33.949Z",
      "title": "DeepCritic: 신중하게 대 언어 모델로 비판을 수행하세요.",
      "submittedOnDailyBy": {
        "_id": "64b7df742f5a966b973e25f7",
        "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
        "isPro": false,
        "fullname": "Wenkai Yang",
        "user": "Keven16",
        "type": "user"
      },
      "summary": "LLM의 급속한 발전에 따라, 그 출력에 대한 정확한 피드백과 scalable한 모니터링을 제공하는 것이 긴급하고 중요한 문제로 자리잡고 있습니다. LLM을 평가모델로 자동화된 감독을 실현하는 것은 바람직한 해결책입니다. 본 연구에서는, LLM의 수학적 평가능력을 연구하고 향상시키기 위한 데 초점을 맞추고 있습니다. 현재의 LLM의 평가자는 각 단계에 대해 얕고 표면적인 평가를 제공하며, 판단의 정확도가 낮고, LLM의 생성기에 충분한 피드백을 제공하지 못하는 점을 간과하고 있습니다. 이러한 문제를 대처하기 위해, 우리는 수학적 답안의 각 논리적 단계에 대해 의도적으로 평가하는 능력을 가진 LLM의 평가자를 개발하기 위한 새로운 효과적인 2단계 프레임워크를 제안합니다. 첫 번째 단계는, Qwen2.5-72B-Instruct를 활용하여, 4.5K의 긴 문体形식의 평가를 생성하고, 매뉴얼 피드백의 미세 조정 종류의 데이터를 제공합니다. 각 평가는 다양한 검증과 논리적 단계별로의 초기 평가의 깊은 평가를 포함합니다. 이어서, PRM800K에서부터의 인간 라벨 데이터와, 모ンテカルロサンプリング에 의한 정확도 추정에 의한 자동 注釈 데이터를 활용하여, 미세 조정 모델에 강화학습을 수행하고, 평가능력을 향상시키기 위해 격려합니다. Qwen2.5-7B-Instruct에 기반한 평가 모델은, 현재의 LLM의 평가자(같은 크기의 DeepSeek-R1-distill 모델이나 GPT-4o를 포함한)와 비교하여, 오류 인식 벤치마크에서 유의미하게 우수하며, 더 상세한 피드백을 통해 LLM의 생성기의 오류의 수정을 효과적으로 돕는 것을 할 수 있습니다.",
      "upvotes": 21,
      "discussionId": "68142e4b551709da9244e8f8",
      "ai_keywords": [
        "LLMs (Large Language Models)",
        "critique models",
        "automated supervision",
        "math critique ability",
        "supervised fine-tuning",
        "Qwen2.5-72B-Instruct",
        "seed data",
        "deliberate step-wise critiques",
        "multi-perspective verifications",
        "reinforcement learning",
        "PRM800K",
        "Monte Carlo sampling-based correctness estimation",
        "Qwen2.5-7B-Instruct",
        "DeepSeek-R1-distill models",
        "GPT-4o",
        "error identification benchmarks"
      ]
    },
    "publishedAt": "2025-05-01T13:03:17.000Z",
    "title": "DeepCritic: Deliberate Critique with Large Language Models",
    "summary": "As Large Language Models (LLMs) are rapidly evolving, providing accurate\nfeedback and scalable oversight on their outputs becomes an urgent and critical\nproblem. Leveraging LLMs as critique models to achieve automated supervision is\na promising solution. In this work, we focus on studying and enhancing the math\ncritique ability of LLMs. Current LLM critics provide critiques that are too\nshallow and superficial on each step, leading to low judgment accuracy and\nstruggling to offer sufficient feedback for the LLM generator to correct\nmistakes. To tackle this issue, we propose a novel and effective two-stage\nframework to develop LLM critics that are capable of deliberately critiquing on\neach reasoning step of math solutions. In the first stage, we utilize\nQwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for\nsupervised fine-tuning. Each seed critique consists of deliberate step-wise\ncritiques that includes multi-perspective verifications as well as in-depth\ncritiques of initial critiques for each reasoning step. Then, we perform\nreinforcement learning on the fine-tuned model with either existing\nhuman-labeled data from PRM800K or our automatically annotated data obtained\nvia Monte Carlo sampling-based correctness estimation, to further incentivize\nits critique ability. Our developed critique model built on Qwen2.5-7B-Instruct\nnot only significantly outperforms existing LLM critics (including the\nsame-sized DeepSeek-R1-distill models and GPT-4o) on various error\nidentification benchmarks, but also more effectively helps the LLM generator\nrefine erroneous steps through more detailed feedback.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00662.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "64b7df742f5a966b973e25f7",
      "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
      "fullname": "Wenkai Yang",
      "name": "Keven16",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.00703",
      "authors": [
        {
          "_id": "681428debcdf962d03da2797",
          "name": "Dongzhi Jiang",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da2798",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da2799",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279a",
          "name": "Zhuofan Zong",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279b",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279c",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279d",
          "name": "Shilin Yan",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279e",
          "name": "Pheng-Ann Heng",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279f",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T17:59:46.000Z",
      "submittedOnDailyAt": "2025-05-02T00:38:40.412Z",
      "title": "T2I-R1: 복합적인 의미 수준과 토큰 수준의 CoT을 활용한 이미지 생성의 강화",
      "submittedOnDailyBy": {
        "_id": "6349214f8146350b3a4c5cdf",
        "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
        "isPro": false,
        "fullname": "Dongzhi Jiang",
        "user": "CaraJ",
        "type": "user"
      },
      "summary": "최근의 대 언어 모델의 발전은 chain-of-thought (CoT)와 reinforcement learning (RL)가 성능을 향상시키는 것을 보여주고 있습니다. 그러나 이러한 논리적인 전략을 시각화 생성 분야에 적용하는 것은 주로 탐색되어 있지 않습니다. 본 논문에서는 RL을 포함하고, bi-level CoT 논리론 프로세스를 채택한 새로운 논리론 강화 텍스트를 이미지 생성 모델 T2I-R1에 소개합니다. 특히, 생성하는 다른 단계를 강화하기 위해 2개의 CoT 레벨을 특정합니다: (1) 의미 레벨의 CoT으로 Prompt의 고 수준 계획을 (2) 토큰 레벨의 CoT으로 패치별로 이미지 처리의 저 수준 계획을 강화합니다. 이러한 2개의 CoT 레벨을 더 잘 조정하기 위해, BiCoT-GRPO를 도입하고, 생성 보상을 조합한 앙상블을 사용하여, 같은 훈련 단계에서 두 개의 생성 CoT을 연속적으로 최적화합니다. 기본 모델 Janus-Pro에 이러한 논리론의 전략을 적용하여, T2I-CompBench에서 13%의 개선, WISE 벤치마크에서 19%의 개선을 실현하고, 가장 先端 모델 FLUX를 초월했습니다. 코드는 다음 URL에서 사용 가능합니다: https://github.com/CaraJ7/T2I-R1",
      "upvotes": 13,
      "discussionId": "681428dfbcdf962d03da281c",
      "githubRepo": "https://github.com/CaraJ7/T2I-R1",
      "ai_keywords": [
        "chain-of-thought (CoT)",
        "reinforcement learning (RL)",
        "text-to-image generation model",
        "bi-level CoT reasoning process",
        "semantic-level CoT",
        "token-level CoT",
        "BiCoT-GRPO",
        "generation rewards",
        "Janus-Pro",
        "T2I-CompBench",
        "WISE benchmark",
        "FLUX"
      ]
    },
    "publishedAt": "2025-05-01T13:59:46.000Z",
    "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level\n  and Token-level CoT",
    "summary": "Recent advancements in large language models have demonstrated how\nchain-of-thought (CoT) and reinforcement learning (RL) can improve performance.\nHowever, applying such reasoning strategies to the visual generation domain\nremains largely unexplored. In this paper, we present T2I-R1, a novel\nreasoning-enhanced text-to-image generation model, powered by RL with a\nbi-level CoT reasoning process. Specifically, we identify two levels of CoT\nthat can be utilized to enhance different stages of generation: (1) the\nsemantic-level CoT for high-level planning of the prompt and (2) the\ntoken-level CoT for low-level pixel processing during patch-by-patch\ngeneration. To better coordinate these two levels of CoT, we introduce\nBiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes\nboth generation CoTs within the same training step. By applying our reasoning\nstrategies to the baseline model, Janus-Pro, we achieve superior performance\nwith 13% improvement on T2I-CompBench and 19% improvement on the WISE\nbenchmark, even surpassing the state-of-the-art model FLUX.1. Code is available\nat: https://github.com/CaraJ7/T2I-R1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6349214f8146350b3a4c5cdf",
      "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
      "fullname": "Dongzhi Jiang",
      "name": "CaraJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21659",
      "authors": [
        {
          "_id": "68142de6111ccf18a993c890",
          "name": "Haotian Luo",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c891",
          "name": "Haiying He",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c892",
          "name": "Yibo Wang",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c893",
          "name": "Jinluan Yang",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c894",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c895",
          "name": "Naiqiang Tan",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c896",
          "name": "Xiaochun Cao",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c897",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c898",
          "name": "Li Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T14:01:45.000Z",
      "submittedOnDailyAt": "2025-05-02T01:01:49.479Z",
      "title": "长期上下文中的混合上下文的双层自适应逻辑优化",
      "submittedOnDailyBy": {
        "_id": "632ab8f5a968c34257da5c52",
        "avatarUrl": "/avatars/59df09e6c9e1e633170514d950ad7981.svg",
        "isPro": false,
        "fullname": "Haotian Luo",
        "user": "LordNoah",
        "type": "user"
      },
      "summary": "최근, 장기적 고려를 기반으로 하는 이유론 모델은 복잡한 이유론 태스크에서 강력한 성능을 보여주지만, 일반적으로 추론 오버헤드가 커져 효율성이 중요한 문제로 부각됩니다. 우리의 실험적 분석은 장기적 고려의 장점이 문제에 따라 달라지는 것을 밝혀내었습니다: 한 쪽은 Detailed reason-giving가 필요하고, 다른 쪽은 개선되지 않거나 정확도가 떨어집니다. 이로 인해 입력에 맞는 이유론 구조를 조정하는 적응적인 이유론 전략이 필요했습니다. 그러나 기존 연구는 주로 긴 이유론 패스 내의冗長를 줄이고, Long-CoT 패러다임을 초과한 더 효율적인 전략의 탐색에 제한되어 있습니다. 이에 대해 우리는 적응적이고 효율적인 이유론을 위한 새로운 2단계 프레임워크를 제안합니다. 먼저, 장기적과 단기의 CoT 모델을 통합하여 다양한 이유론 스타일을 가능하게 합니다. 다음으로, bi-level 선호 훈련을 적용하여 모델을 적절한 이유론 스타일을 선택하도록 유도하고, 각 스타일 그룹 내의 간결하고 정확한 이유론을 우선시하도록 안내합니다. 실험은 다른 기본 라인 접근법과 비교하여 추론 비용을 크게 줄이고 성능을 유지하는 것을 보여주었습니다. 특히, 5개의 수학 데이터 세트에서 이유론의 평균 길이가 50% 이상 줄였으며, 적응적인 전략이 대규모 언어 모델의 이유론 효율화에 대한 가능성을 밝혀냅니다. 우리의 코드는 곧 여기에 공개됩니다: https://github.com/StarDewXXX/AdaR1",
      "upvotes": 3,
      "discussionId": "68142de7111ccf18a993c8ba",
      "ai_keywords": [
        "CoT models",
        "Long-CoT",
        "hybrid reasoning model",
        "bi-level preference training",
        "adaptive reasoning strategies"
      ]
    },
    "publishedAt": "2025-04-30T10:01:45.000Z",
    "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization",
    "summary": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21659.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632ab8f5a968c34257da5c52",
      "avatarUrl": "/avatars/59df09e6c9e1e633170514d950ad7981.svg",
      "fullname": "Haotian Luo",
      "name": "LordNoah",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.00497",
      "authors": [
        {
          "_id": "68147d4d687b82a9b6308cfd",
          "name": "Antoni Bigata",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308cfe",
          "name": "Rodrigo Mira",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308cff",
          "name": "Stella Bounareli",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d00",
          "name": "Michał Stypułkowski",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d01",
          "name": "Konstantinos Vougioukas",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d02",
          "name": "Stavros Petridis",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d03",
          "name": "Maja Pantic",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/640777812e309e65452491dd/4ao3SssNM8wCiTo5amAxn.mp4"
      ],
      "publishedAt": "2025-05-01T12:56:17.000Z",
      "submittedOnDailyAt": "2025-05-02T06:38:20.471Z",
      "title": "KeySync: 강력한 방법で 고해상도, 부드러운 입술 동기화를 실현합니다.",
      "submittedOnDailyBy": {
        "_id": "640777812e309e65452491dd",
        "avatarUrl": "/avatars/01b927675e78f212408168522f65fe36.svg",
        "isPro": true,
        "fullname": "Antoni Bigata",
        "user": "toninio19",
        "type": "user"
      },
      "summary": "唇同步는 기존 비디오의 입술 움직임과 새로운 입력 음성을 일치시키는 작업으로, 일반적으로 음성 주도의 얼굴 움직임의 단순 변형으로 처리됩니다. 그러나, 唇同步는 시간적 일관성 등 토크헤드 생성의 일반적인 문제에 더해 입력 비디오에서 표정의 손실과 얼굴의 가려짐 등 새로운 큰 문제를 제시하고 있습니다. 이러한 결점을 해결하기 위해, 우리는 KeySync라는 2단계 프레임워크를 제안합니다. 이 프레임워크는 시간적 일관성 문제를 해결하고 조정된 마스크 기술로 손실과 가려짐의 문제를 대응하는 것을 성공적으로 수행합니다. 우리는 KeySync는 우리의 새로운 손실 메트릭인 \"LipLeak\"에 의해 입술의 재구성과 크로스 싱크로니즈션에서 가장 최신의 결과를 얻으며 시각적 질량을 향상시키고 손실을 줄이는 것을 보여주며, 또한 가려짐 처리에서 새로운 마스크 접근 방식의 효과를 보여주고 구조적인 선택을 정당화하기 위해 여러 축소 연구를 수행합니다. 코드와 모델의 가중치는 https://antonibigata.github.io/KeySync에서 얻을 수 있습니다.",
      "upvotes": 2,
      "discussionId": "68147d53687b82a9b6308e59",
      "projectPage": "https://antonibigata.github.io/KeySync/",
      "githubRepo": "https://github.com/antonibigata/keysync",
      "ai_keywords": [
        "KeySync",
        "lip synchronization",
        "audio-driven facial animation",
        "talking head generation",
        "temporal consistency",
        "expression leakage",
        "facial occlusions",
        "automated dubbing",
        "lip reconstruction",
        "cross-synchronization",
        "visual quality",
        "LipLeak",
        "masking strategy",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-05-01T08:56:17.000Z",
    "title": "KeySync: A Robust Approach for Leakage-free Lip Synchronization in High\n  Resolution",
    "summary": "Lip synchronization, known as the task of aligning lip movements in an\nexisting video with new input audio, is typically framed as a simpler variant\nof audio-driven facial animation. However, as well as suffering from the usual\nissues in talking head generation (e.g., temporal consistency), lip\nsynchronization presents significant new challenges such as expression leakage\nfrom the input video and facial occlusions, which can severely impact\nreal-world applications like automated dubbing, but are often neglected in\nexisting works. To address these shortcomings, we present KeySync, a two-stage\nframework that succeeds in solving the issue of temporal consistency, while\nalso incorporating solutions for leakage and occlusions using a carefully\ndesigned masking strategy. We show that KeySync achieves state-of-the-art\nresults in lip reconstruction and cross-synchronization, improving visual\nquality and reducing expression leakage according to LipLeak, our novel leakage\nmetric. Furthermore, we demonstrate the effectiveness of our new masking\napproach in handling occlusions and validate our architectural choices through\nseveral ablation studies. Code and model weights can be found at\nhttps://antonibigata.github.io/KeySync.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/640777812e309e65452491dd/4ao3SssNM8wCiTo5amAxn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00497.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640777812e309e65452491dd",
      "avatarUrl": "/avatars/01b927675e78f212408168522f65fe36.svg",
      "fullname": "Antoni Bigata",
      "name": "toninio19",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20605",
      "authors": [
        {
          "_id": "68131e73f0f2a4d8b2d4b06a",
          "user": {
            "_id": "642bcb8ae5b6823cde9301bd",
            "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
            "isPro": false,
            "fullname": "Mihai Dan Nadăș",
            "user": "mihainadas",
            "type": "user"
          },
          "name": "Mihai Nadas",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-02T06:34:48.889Z",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06b",
          "name": "Laura Diosan",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06c",
          "user": {
            "_id": "67b2344d0ce2aaa57c8c9997",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b2344d0ce2aaa57c8c9997/LSMjuQNjRsUllUQyt9vNo.jpeg",
            "isPro": false,
            "fullname": "Andrei Piscoran",
            "user": "andreiPiscoran",
            "type": "user"
          },
          "name": "Andrei Piscoran",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-01T07:11:31.916Z",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06d",
          "user": {
            "_id": "677e4393ef848c5a5352d082",
            "avatarUrl": "/avatars/bcb60ead58969601e2911053550fec62.svg",
            "isPro": false,
            "fullname": "Andreea Tomescu",
            "user": "andreeatomescu",
            "type": "user"
          },
          "name": "Andreea Tomescu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-01T07:10:43.780Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T10:15:28.000Z",
      "submittedOnDailyAt": "2025-05-02T06:22:34.343Z",
      "title": "TF1-EN-3M: 3백만 개의 합성된 모라ル 이야기를 사용하여 훈련하는 작은, 개방적인 언어 모델의 데이터 세트",
      "submittedOnDailyBy": {
        "_id": "642bcb8ae5b6823cde9301bd",
        "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
        "isPro": false,
        "fullname": "Mihai Dan Nadăș",
        "user": "mihainadas",
        "type": "user"
      },
      "summary": "모르의 이야기는 역사적인 도구로 가치와 의미를 전달하는 데 사용되고 있으며, 이에 반해 현대의 NLP는 일관된 이야기와 명시적인 윤리적 교훈을 결합한 큰 구조화된 코퍼스를 부족하게 한다. 이러한 오류를 보완하기 위해, TF1-EN-3M라는 데이터 세트를 준비합니다. 이것은 처음 공개 데이터 세트이며, 300만 건의 영어의 파벨을 8억 파라미터 이하의 인스톰션 조정된 모델로 전문적으로 생성합니다. 각 이야기는 \"인물 -> 특징 -> 설정 -> 충돌 -> 해결 -> 윤리\"라는 6 포터 스케폴드를 포함하고 있으며, 조합된 프로ン폼 엔진을 통해 장르의 일관성을 유지하면서 넓은 주제 공간을 덮습니다.\n\n하이브리드 평가 파이프라인은 (i) GPT 기반의 평가자가 문법, 창의성, 윤리적 명확성, 템플릿의 준수성을 점수를 매기는 것과 (ii) 참조없이 다양성과 읽기 쉬운 메트릭을 결합합니다. 10개의 개방형 웨이트 후보 중 8억 파라미터의 Llama-3 버전이 가장 좋은 품질과 속도의 조정을 제공하며, 1,000 건의 파벨에 대해 높은 점수를 받은 파벨을 생성합니다. 이는 1 콘솔 GPU (<24GB VRAM)에서 약 13.5 세ン티플로 수행할 수 있습니다.\n\n데이터 세트, 생성 코드, 평가 스크립트, 완전한 메타 데이터를 자유롭게 공개하며, 완전한 재현성과 비용 벤치마크를 가능하게 합니다. TF1-EN-3M는 명령의 순서성, 이야기의 지능, 가치의 조정, 아동을 위한 교육 AI 연구에 길을 열어, 큰 규모의 윤리적인 이야기의 전달 방식은 큰 모델이 필요하지 않은 것으로 보여줍니다.",
      "upvotes": 2,
      "discussionId": "68131e73f0f2a4d8b2d4b087",
      "githubRepo": "https://github.com/klusai/tinyfabulist",
      "ai_keywords": [
        "instruction-tuned models",
        "combinatorial prompt engine",
        "GPT-based critic",
        "template adherence",
        "reference-free diversity",
        "Llama-3 variant",
        "computational efficiency",
        "permissive license",
        "child-friendly educational AI",
        "moral storytelling"
      ]
    },
    "publishedAt": "2025-04-29T06:15:28.000Z",
    "title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open\n  Language Models",
    "summary": "Moral stories are a time-tested vehicle for transmitting values, yet modern\nNLP lacks a large, structured corpus that couples coherent narratives with\nexplicit ethical lessons. We close this gap with TF1-EN-3M, the first open\ndataset of three million English-language fables generated exclusively by\ninstruction-tuned models no larger than 8B parameters. Each story follows a\nsix-slot scaffold (character -> trait -> setting -> conflict -> resolution ->\nmoral), produced through a combinatorial prompt engine that guarantees genre\nfidelity while covering a broad thematic space.\n  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores\ngrammar, creativity, moral clarity, and template adherence with (ii)\nreference-free diversity and readability metrics. Among ten open-weight\ncandidates, an 8B-parameter Llama-3 variant delivers the best quality-speed\ntrade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)\nat approximately 13.5 cents per 1,000 fables.\n  We release the dataset, generation code, evaluation scripts, and full\nmetadata under a permissive license, enabling exact reproducibility and cost\nbenchmarking. TF1-EN-3M opens avenues for research in instruction following,\nnarrative intelligence, value alignment, and child-friendly educational AI,\ndemonstrating that large-scale moral storytelling no longer requires\nproprietary giant models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20605.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642bcb8ae5b6823cde9301bd",
      "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
      "fullname": "Mihai Dan Nadăș",
      "name": "mihainadas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18983",
      "authors": [
        {
          "_id": "681470d72175e5e7ca0ea002",
          "name": "Xuyin Qi",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea003",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea004",
          "name": "Canxuan Gang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea005",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea006",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea007",
          "name": "Zhiwei Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea008",
          "name": "Yang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-26T17:56:56.000Z",
      "submittedOnDailyAt": "2025-05-02T05:44:53.105Z",
      "title": "MediAug: 의료 영상의 시각화 어우그먼테이션 연구",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "데이터 증강은 의료 이미지에서 데이터 조건이 제한되어 있어 클래스 분류의 정확도, 손상 감지, 장기분할을 개선하는 데 필수적이다. 그러나 두 가지 큰 문제를 남아 있습니다. 첫 번째는 자연 사진과 의료 이미지 사이의 뚜렷한 도메인 간격이 중요한 질병 특징을 왜곡하는 데 영향을 미친다. 두 번째는 의료 이미지의 데이터 증강 연구는 단일의 태스크 또는 아키텍처에 제한되어 있으며, 先進的な mix 기반의 전략의 이익이 명확하지 않다. 이러한 문제를 해결하기 위해, 우리는 뇌종양의 MRI 이미지와 눈질환의 눈부신 이미지 데이터 세트에서 두 가지 Convolutionnal 및 Transformer 백본을 통합한 6가지의 mix 기반의 데이터 증강 방법론을 사용함으로써 독특한 평가 프레임워크를 제안합니다. 우리의 기여는 세 가지입니다. 1. 우리는 MediAug라는, 의료 이미지의 데이터 증강을 위한 전진적인 평가 지표를 제시합니다. 2. ResNet-50 및 ViT-B 백본을 사용하여 MixUp, YOCO, CropMix, CutMix, AugMix, SnapMix를 체계적으로 평가합니다. 3. 매우 광범위한 실험을 통해 ResNet-50에서 뇌종양 분류 태스크에서 MixUp이 79.19%의 정확도를 가장 크게 향상시켰으며, ViT-B에서 SnapMix가 99.44%의 정확도를 가장 크게 향상시켰으며, ResNet-50에서 눈질환 분류 태스크에서 YOCO가 91.60%의 정확도를 가장 크게 향상시켰으며, ViT-B에서 CutMix가 97.94%의 정확도를 가장 크게 향상시켰습니다. 코드는 https://github.com/AIGeeksGroup/MediAug에서 사용 가능합니다.",
      "upvotes": 1,
      "discussionId": "681470d92175e5e7ca0ea065",
      "ai_keywords": [
        "MediAug",
        "MixUp",
        "YOCO",
        "CropMix",
        "CutMix",
        "AugMix",
        "SnapMix",
        "ResNet-50",
        "ViT-B",
        "brain tumour MRI",
        "eye disease fundus datasets",
        "domain gap",
        "lesion detection",
        "organ segmentation",
        "classification accuracy"
      ]
    },
    "publishedAt": "2025-04-26T13:56:56.000Z",
    "title": "MediAug: Exploring Visual Augmentation in Medical Imaging",
    "summary": "Data augmentation is essential in medical imaging for improving\nclassification accuracy, lesion detection, and organ segmentation under limited\ndata conditions. However, two significant challenges remain. First, a\npronounced domain gap between natural photographs and medical images can\ndistort critical disease features. Second, augmentation studies in medical\nimaging are fragmented and limited to single tasks or architectures, leaving\nthe benefits of advanced mix-based strategies unclear. To address these\nchallenges, we propose a unified evaluation framework with six mix-based\naugmentation methods integrated with both convolutional and transformer\nbackbones on brain tumour MRI and eye disease fundus datasets. Our\ncontributions are threefold. (1) We introduce MediAug, a comprehensive and\nreproducible benchmark for advanced data augmentation in medical imaging. (2)\nWe systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix\nwith ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive\nexperiments that MixUp yields the greatest improvement on the brain tumor\nclassification task for ResNet-50 with 79.19% accuracy and SnapMix yields the\ngreatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the\ngreatest improvement on the eye disease classification task for ResNet-50 with\n91.60% accuracy and CutMix yields the greatest improvement for ViT-B with\n97.94% accuracy. Code will be available at\nhttps://github.com/AIGeeksGroup/MediAug.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18983.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]