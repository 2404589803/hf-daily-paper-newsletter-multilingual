[
  {
    "paper": {
      "id": "2503.05236",
      "authors": [
        {
          "_id": "67ce37239f9aaaae837f3894",
          "user": {
            "_id": "654c6845bac6e6e49895a5b5",
            "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
            "isPro": false,
            "fullname": "Yibin Wang",
            "user": "CodeGoat24",
            "type": "user"
          },
          "name": "Yibin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:37:51.835Z",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3895",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:24.660Z",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3896",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3897",
          "name": "Cheng Jin",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3898",
          "user": {
            "_id": "64638c4d51fa6e63060521b5",
            "avatarUrl": "/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg",
            "isPro": false,
            "fullname": "JIaqi",
            "user": "Jiaqiwang",
            "type": "user"
          },
          "name": "Jiaqi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:38:17.938Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T08:36:05.000Z",
      "title": "통합 상금 모델의 다중 모드 이해와 생성",
      "summary": "최근의 인간의 취미의 조정이 진행되고 있는 가운데, 다 모델의 생성과 이해에 있어서 큰 발전을 보입니다. 주요 접근 방식 중 하나는 보상 모델의 훈련을 통해 취미 최적화를 수행하는 데 중요합니다. 그러나 현재의 모델들은 일반적으로 특정한 작업에 특화된 상태이며, 다양한 시각적 애플리케이션에 적응하는 데 제한이 있습니다. 또한 여러 작업 동시에 평가하여 상호작용을 촉진할 수 있다는 주장이 있습니다. 이에 따라, 본 논문에서는 첫 번째 유닛 마이너스 모델인 \"UnifiedReward\"를 제안합니다. 이 모델은 다 모델의 이해와 생성 평가를 가능하게 하며, 페어랭킹과 포인트 스코어를 수행할 수 있습니다. 구체적으로는, (1) 우리가 구축한 규모가 큰 인간 취미 데이터 세트에 유닛 마이너스 모델을 개발했습니다. 이 데이터 세트에는 이미지와 비디오의 생성/ 이해 작업이 포함됩니다. (2) 다음으로, 이러한 시각 모델을 기반으로 고품질의 취미 페어 데이터를 자동적으로 구축하고, 페어랭킹과 포인트 스코어를 통해 출력을 필터링합니다. (3) 마지막으로, 이러한 데이터를 직접적인 취미 최적화(DPO)에 사용하여 취미 조정을 수행합니다. 실험 결과를 통해, 다양한 시각 작업의 평가로 상호 이익을 크게 끌어낼 수 있음을 보여주고, 이미지와 비디오의 이해/ 생성 작업에 이 프로세스를 적용하여 각 분야의 성능을 크게 향상시켰습니다.",
      "upvotes": 72,
      "discussionId": "67ce37259f9aaaae837f3948",
      "projectPage": "https://codegoat24.github.io/UnifiedReward/",
      "githubRepo": "https://github.com/CodeGoat24/UnifiedReward"
    },
    "publishedAt": "2025-03-09T22:20:09.137Z",
    "title": "Unified Reward Model for Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05236.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "654c6845bac6e6e49895a5b5",
      "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
      "fullname": "Yibin Wang",
      "name": "CodeGoat24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05179",
      "authors": [
        {
          "_id": "67ce4bff5847e4787a7ebedd",
          "user": {
            "_id": "65f4060754ecda1ecb5797a0",
            "avatarUrl": "/avatars/f8b44524d36b505673cb538fd7895a82.svg",
            "isPro": false,
            "fullname": "Simon Aytes",
            "user": "saytes",
            "type": "user"
          },
          "name": "Simon A. Aytes",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:10.363Z",
          "hidden": false
        },
        {
          "_id": "67ce4bff5847e4787a7ebede",
          "user": {
            "_id": "63036b6c5c70c21d0ea79d48",
            "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
            "isPro": false,
            "fullname": "Jinheon Baek",
            "user": "jinheon",
            "type": "user"
          },
          "name": "Jinheon Baek",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:13.328Z",
          "hidden": false
        },
        {
          "_id": "67ce4bff5847e4787a7ebedf",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T06:57:17.000Z",
      "title": "スキートオブスモード：EFFICIENT LLM Logic에 의한 Adaptive Cognition을 모델로 하는 스키트",
      "summary": "최근의 대언어 모델의 발전은 Chain of Thought (CoT) Prompting에 의해 매력적인 논리적인 능력을 보여주었지만, 중간 출력의 과도한 길이는 계산 오버헤드를 증가시켰습니다. 우리는 Sketch-of-Thought (SoT)라는 새로운 Prompting 프레임워크를 소개합니다. 이 프레임워크는 인지과학에 기반한 논리적인 패러다임과 언어 제약을 조합하여 논리적인 정확도를 유지하는 데에 토큰 사용량을 최소화하는 것을 목표로 합니다. SoT는 인지과학에 기반한 임의의 사용자 정의 논리적인 패러다임을 포함할 수 있는 유연한 프레임워크로 설계되었습니다. 우리는 Conceptual Chaining, Chunked Symbolism, Expert Lexicons의 3가지 패러다임을 구현하고, 각각의 논리적인 태스크에 적절하며, 가벼운 루팅 모델에 의해 동적으로 선택됩니다. SoT는 15개의 논리적인 데이터 세트를 구성하는 여러 언어와 다 모델 스케너리로 검증하여 토큰 줄이기율이 76%에 도달하며 오류율의 영향을 미칠 수 없다고 나타냅니다. 수학이나 다단계 논리와 같은 특정 분야에서는如此少量의 토큰을 사용하여 정확도를 향상시킬 수 있습니다. 우리의 코드는 공개적으로 사용할 수 있습니다: https://www.github.com/SimonAytes/SoT.",
      "upvotes": 27,
      "discussionId": "67ce4c035847e4787a7ebf4c",
      "projectPage": "https://huggingface.co/saytes/SoT_DistilBERT",
      "githubRepo": "https://github.com/SimonAytes/SoT"
    },
    "publishedAt": "2025-03-09T22:25:52.244Z",
    "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05179.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63036b6c5c70c21d0ea79d48",
      "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
      "fullname": "Jinheon Baek",
      "name": "jinheon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05500",
      "authors": [
        {
          "_id": "67ce9626e5cdfda52b9e8839",
          "user": {
            "_id": "62be186a5f59ff2320e6e32b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
            "isPro": false,
            "fullname": "Nicolas-BZRD",
            "user": "Nicolas-BZRD",
            "type": "user"
          },
          "name": "Nicolas Boizard",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:06.860Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883a",
          "user": {
            "_id": "65fa95405355a52c784633fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fa95405355a52c784633fc/rSfBUHPa7eSAsLd8DuOq4.png",
            "isPro": false,
            "fullname": "Hippolyte Gisserot-Boukhlef",
            "user": "hgissbkh",
            "type": "user"
          },
          "name": "Hippolyte Gisserot-Boukhlef",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:13.176Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883b",
          "user": {
            "_id": "64132452d8a418df415a6ded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64132452d8a418df415a6ded/qkjL5G89uldHUXlCI3n4f.jpeg",
            "isPro": false,
            "fullname": "Duarte Alves",
            "user": "DuarteMRAlves",
            "type": "user"
          },
          "name": "Duarte M. Alves",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:23.055Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883c",
          "name": "André Martins",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883d",
          "user": {
            "_id": "63937b399762cdd66be2a32f",
            "avatarUrl": "/avatars/7aefd888a3c54673d5881dcef61f771b.svg",
            "isPro": false,
            "fullname": "Ayoub Hammal",
            "user": "ayoubhammal",
            "type": "user"
          },
          "name": "Ayoub Hammal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:42.527Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883e",
          "user": {
            "_id": "677bedd522ca8585ede98470",
            "avatarUrl": "/avatars/54bca410c446610f02aca55918c74518.svg",
            "isPro": false,
            "fullname": "Caio Corro",
            "user": "caiocorro",
            "type": "user"
          },
          "name": "Caio Corro",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:48.603Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883f",
          "user": {
            "_id": "61efea03a57920a251ec19b8",
            "avatarUrl": "/avatars/f47c8e3cb17a2bf7d43f2c152bb86885.svg",
            "isPro": false,
            "fullname": "Celine Hudelot",
            "user": "CelineH",
            "type": "user"
          },
          "name": "Céline Hudelot",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:51:41.273Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8840",
          "user": {
            "_id": "66f2d6a684a241caac8e16dc",
            "avatarUrl": "/avatars/81acb87c2b07bea938251b40a2139911.svg",
            "isPro": false,
            "fullname": "Emmanuel Malherbe",
            "user": "emmanuelmalherbe",
            "type": "user"
          },
          "name": "Emmanuel Malherbe",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:51:47.996Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8841",
          "name": "Etienne Malaboeuf",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8842",
          "user": {
            "_id": "6708db59caf70ddea8e1355d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6708db59caf70ddea8e1355d/C6T16AdpqoeWCk7Gg9wSH.jpeg",
            "isPro": false,
            "fullname": "Fanny Jourdan",
            "user": "Fannyjrd",
            "type": "user"
          },
          "name": "Fanny Jourdan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:09.223Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8843",
          "user": {
            "_id": "67cafedda972115e89972cd7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P_xComqG9IttvluN-6tyB.png",
            "isPro": false,
            "fullname": "Gabriel Hautreux",
            "user": "GabrielHau",
            "type": "user"
          },
          "name": "Gabriel Hautreux",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:15.512Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8844",
          "user": {
            "_id": "6772bde5c997eeb5550e80ea",
            "avatarUrl": "/avatars/8134a4d9330317e748dc7b33e1bb25f6.svg",
            "isPro": false,
            "fullname": "João Alves",
            "user": "albusonrails",
            "type": "user"
          },
          "name": "João Alves",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:22.630Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8845",
          "user": {
            "_id": "66e2c22d7cc3edd60d725267",
            "avatarUrl": "/avatars/b217c5708c7dba8b1c220f37984ccc1e.svg",
            "isPro": false,
            "fullname": "Kevin El Haddad",
            "user": "kelhad",
            "type": "user"
          },
          "name": "Kevin El-Haddad",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:31.191Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8846",
          "user": {
            "_id": "60f2e021adf471cbdf8bb660",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg",
            "isPro": false,
            "fullname": "Manuel Faysse",
            "user": "manu",
            "type": "user"
          },
          "name": "Manuel Faysse",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:38.114Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8847",
          "user": {
            "_id": "6369394dd322a76e1ea4bdf6",
            "avatarUrl": "/avatars/a4e5ab0167025fbbfc970d54630ce754.svg",
            "isPro": false,
            "fullname": "Maxime Peyrard",
            "user": "peyrardm",
            "type": "user"
          },
          "name": "Maxime Peyrard",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:44.389Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8848",
          "user": {
            "_id": "67b622d2df3a86fbca306c43",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lNlshrl56oaKslArMzSzj.png",
            "isPro": false,
            "fullname": "Nuno  Guerreiro",
            "user": "nunogj",
            "type": "user"
          },
          "name": "Nuno M. Guerreiro",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:54.367Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8849",
          "name": "Patrick Fernandes",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e884a",
          "name": "Ricardo Rei",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e884b",
          "user": {
            "_id": "644a900e3a619fe72b14af0f",
            "avatarUrl": "/avatars/e2d5dac3d92757ed48e37e126a3464a3.svg",
            "isPro": false,
            "fullname": "Colombo",
            "user": "PierreColombo",
            "type": "user"
          },
          "name": "Pierre Colombo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:41:26.353Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T15:13:58.000Z",
      "title": "유로베르트: 유로 베르트: 동양어 언어의 인코더를 확장하는 방법\n\n(注意：此翻译保持了原文的专业性和准确性，同时确保了语言的自然流畅。)",
      "summary": "일반적인 언어의 다언어 벡터 표현은 검색, 회귀 및 분류에 사용됩니다. 이들은 양방향 인코더 모델로부터 얻을 수 있습니다. 또한 그들이 광범위하게 적용될 수 있습니다. 그러나 최근에는 인코더는 생성적인 디코더 모델의 발전에 의해 뛰어난 성능을 보입니다. 그러나 이러한 발전의 대부분은 디코더와 고유하게 연결되지 않습니다. 본 논문에서는 이러한 발전을 중심으로 다언어 인코더의 개발을 재평가하고, EuroBERT, 즉 유럽의 여러 국가와 세계에서 광범위하게 사용되는 언어를 커버하는 다언어 인코더의 가족을 소개합니다. 우리의 모델은 현재보다 여러 분야의 다양한 태스크에서 다언어 능력, 수학, 코딩의 폭을 넓게 향상시켰습니다. 또한 기본적으로 8,192 토큰의 길이의 시퀀스를 지원합니다. 또한 EuroBERT의 설계 결정에 대해 검토하고, 데이터 세트의 구성 및 훈련 프로세스에 대한 피드백을 제공합니다. EuroBERT 모델을 공개하고 중간적인 훈련 체크포인트 및 훈련 프레임워크를 포함하여 공개합니다.",
      "upvotes": 26,
      "discussionId": "67ce9627e5cdfda52b9e88a4"
    },
    "publishedAt": "2025-03-10T03:42:45.848Z",
    "title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62be186a5f59ff2320e6e32b/NxwS9WJrRc9D3q9awbn_X.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05500.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62be186a5f59ff2320e6e32b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
      "fullname": "Nicolas-BZRD",
      "name": "Nicolas-BZRD",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02130",
      "authors": [
        {
          "_id": "67cc697fa029f09af72cca01",
          "user": {
            "_id": "6694cc1009326cb83f2d11bb",
            "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
            "isPro": false,
            "fullname": "Zhixuan Lin",
            "user": "zhixuan-lin",
            "type": "user"
          },
          "name": "Zhixuan Lin",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-08T16:00:21.933Z",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca02",
          "user": {
            "_id": "64234eadd654afd6931a288b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/UTU-XcO_ssKpIYr5MBujK.jpeg",
            "isPro": false,
            "fullname": "Evgenii Nikishin",
            "user": "nikishin",
            "type": "user"
          },
          "name": "Evgenii Nikishin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:54:09.945Z",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca03",
          "user": {
            "_id": "66906c4e37eadb9c577984d3",
            "avatarUrl": "/avatars/b81765472942fdf94c0ee885ca62df2d.svg",
            "isPro": false,
            "fullname": "Owen He",
            "user": "littleowen",
            "type": "user"
          },
          "name": "Xu Owen He",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-08T16:00:01.392Z",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca04",
          "name": "Aaron Courville",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T23:35:23.000Z",
      "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
      "summary": "현대의 리카렌트 시퀀스 모형의 중요한 구성 요소 중 하나는 잊기 게이트입니다. Transformer는 명시적인 리카렌트 형식을 갖지 않지만, 데이터 의존적인 방법으로 정규화되지 않은 注意 스코어를 낮추어 자연스럽게 잊기 게이트를 삽입할 수 있습니다. 이 注意 기능에 \" 잊기 게이트 注意\" 이라는 이름을 붙이고, 그 결과로 생성된 모형에 \" 잊기 게이트 Transformer (FoX)\" 이라는 이름이 붙습니다. FoX는 긴 문맥의 언어 모델링, 긴 문맥의 길이 추정, 짧은 문맥의 하류 태스크에서 Transformer보다 뛰어난 성능을 보여주며, 동시에 긴 문맥의 하류 태스크에서는 Transformer와 같은 성능을 나타냅니다. 또한, FlashAttention 알고리즘과 호환하며, 위치 벡터는 필요하지 않습니다. \"하이 스택의 니드ル 테스트\" 등 분석을 포함하여, FoX는 리카렌트 시퀀스 모형의 예인 Mamba-2, HGRN2, DeltaNet과 비교하여 Transformer의 뛰어난 긴 문맥 능력을 유지하고 있음을 보여주고 있습니다. 또한, \"프로\" 블록 설계를 도입하고, 리카렌트 시퀀스 모형에 공통적인 아키텍처 구성 요소를 삽입하여 FoX와 Transformer의 두 가지 모형의 성능을 크게 향상시키는 것을 발견했습니다. 코드는 다음 URL에서 사용할 수 있습니다.\nhttps://github.com/zhixuan-lin/forgetting-transformer",
      "upvotes": 12,
      "discussionId": "67cc6981a029f09af72ccac1",
      "githubRepo": "https://github.com/zhixuan-lin/forgetting-transformer"
    },
    "publishedAt": "2025-03-09T22:02:39.842Z",
    "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02130.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6694cc1009326cb83f2d11bb",
      "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
      "fullname": "Zhixuan Lin",
      "name": "zhixuan-lin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05639",
      "authors": [
        {
          "_id": "67ce5ad85847e4787a82242d",
          "user": {
            "_id": "650447dd52ca06fef957f05d",
            "avatarUrl": "/avatars/511c11ac9b3cc7a162bda5e07f6ee0a3.svg",
            "isPro": true,
            "fullname": "Yuxuan BIAN",
            "user": "BianYx",
            "type": "user"
          },
          "name": "Yuxuan Bian",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-10T03:22:04.947Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a82242e",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a82242f",
          "user": {
            "_id": "62d4577bc85b0fcf7fde39bb",
            "avatarUrl": "/avatars/a3a5729e33ae89ce9ba408830db3c835.svg",
            "isPro": false,
            "fullname": "Xuan Ju",
            "user": "juxuan27",
            "type": "user"
          },
          "name": "Xuan Ju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:57:06.955Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822430",
          "user": {
            "_id": "6374a02d0856ac905bfc6113",
            "avatarUrl": "/avatars/2cbe75c9cc818a647ca6e416f129c96f.svg",
            "isPro": false,
            "fullname": "Mingdeng Cao",
            "user": "Ljzycmd",
            "type": "user"
          },
          "name": "Mingdeng Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:56:45.412Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822431",
          "name": "Liangbin Xie",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822432",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:56:34.001Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822433",
          "name": "Qiang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T17:59:46.000Z",
      "title": "VideoPainter: ネーブル 길이의 비디오 인플레이팅 및 편집에 플러그인 플레이팅\n컨텍스트 제어",
      "summary": "Video inpainting, 비디오 inpainting 기술은 봇을 복원하기 위해 개발된 비디오 처리 기술로 발전 중입니다. 이러한 발전에도 불구하고, 현재의 방법들은 광학 흐름과 수용 영역의 사전 지식을 사용하여 숨겨진 영역의 픽셀을 확장하거나, 이미지 inpainting 모델을 시간적으로 확장하여 완전히 숨겨진 객체의 생성과 배경의 컨텍스트를 유지하고 포콘트의 생성을 조화시키는 어려움을 가지고 있습니다. 이러한 제한을 해결하기 위해, 우리는 새로운 VideoPainter를 제안합니다. 이 방법은 6%의 백본 파라미터를 가진 효율적인 컨텍스트 인코더를 포함하고, 숨겨진 비디오를 처리하고, 백본에 의한 배경 컨텍스트 정보를 비디오 디텍터 모델에 주입하여 일관된 콘텐츠 생성을 목표로 합니다. 이 아키텍처의 분리는 학습 복잡성을 크게 줄이고, 중요한 배경 컨텍스트의 복잡한 통합을 가능하게 합니다. 또한, 비디오 inpainting의 실용적인 적용을 크게 향상시키기 위해, 새로운 목표 영역 ID의 리샘플링 기술을 도입하고, 긴 비디오를 처리할 수 있도록 합니다. 또한, 현재의 시각 이해 모델을 사용하여 scalable 데이터 세트 파이프라인을 구축하고, VPData와 VPBench를 제공하여, 분할 기반의 inpainting의 훈련과 평가를 촉진하고, 지금까지 가장 큰 비디오 inpainting 데이터 세트와 벤치마크를 390K 이상의 다양한 클립을 포함하여 제공합니다. inpainting을 파이프라인의 기반으로, 비디오 편집과 비디오 편집 페어 데이터의 생성 등 하류 애플리케이션도 검토하고, 경쟁적인 성능과 실용적인 가능성에 대한 증거를 제시했습니다. 확장된 실험은 VideoPainter의 8개의 주요 메트릭에서 우수한 성능을 보여주며, 비디오 inpainting과 비디오 편집 모두에 효과적인 적용을 보여줍니다.",
      "upvotes": 10,
      "discussionId": "67ce5adc5847e4787a822524",
      "projectPage": "https://yxbian23.github.io/project/video-painter/",
      "githubRepo": "https://github.com/TencentARC/VideoPainter"
    },
    "publishedAt": "2025-03-10T04:30:00.983Z",
    "title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650447dd52ca06fef957f05d/VSg-Ti5epQJbVp20s1ILN.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650447dd52ca06fef957f05d",
      "avatarUrl": "/avatars/511c11ac9b3cc7a162bda5e07f6ee0a3.svg",
      "fullname": "Yuxuan BIAN",
      "name": "BianYx",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05592",
      "authors": [
        {
          "_id": "67ce5fd2e5cdfda52b9123a4",
          "user": {
            "_id": "66163dc8c7f45b3f893ff40b",
            "avatarUrl": "/avatars/801043dac0caae90bbca8c9d3e2e203b.svg",
            "isPro": false,
            "fullname": "Song Huatong",
            "user": "XXsongLALA",
            "type": "user"
          },
          "name": "Huatong Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:03:49.730Z",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a5",
          "user": {
            "_id": "61b8405b516a20acdf3b85ff",
            "avatarUrl": "/avatars/3d2eae7c163a80b73260087b05a4230b.svg",
            "isPro": false,
            "fullname": "Jinhao Jiang",
            "user": "Boru",
            "type": "user"
          },
          "name": "Jinhao Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:04:22.446Z",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a6",
          "user": {
            "_id": "6703ac76ea890f0ca5b225eb",
            "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
            "isPro": false,
            "fullname": "Yingqian Min",
            "user": "EliverQ",
            "type": "user"
          },
          "name": "Yingqian Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T09:40:54.171Z",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a7",
          "name": "Jie Chen",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a8",
          "name": "Zhipeng Chen",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a9",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123aa",
          "name": "Lei Fang",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123ab",
          "user": {
            "_id": "64b8c89052b7353d8c6a1013",
            "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
            "isPro": false,
            "fullname": "Ji-Rong Wen",
            "user": "jrwen",
            "type": "user"
          },
          "name": "Ji-Rong Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:04:33.194Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T17:14:44.000Z",
      "title": "R1-Searcher: 강화학습에 의한 LLM의 검색 능력의 촉진\n\n(Note: The original text \"R1-Searcher: 强化学習によるLLMの検索能力の奨励\" is a title or heading and does not contain a complete sentence structure. The translation provided maintains the title's meaning while ensuring it is grammatically correct in Korean.)",
      "summary": "현재의 대규모 추론 모형(LRMs)은 강화 학습(RL)이 대규모 언어 모형(LLMs)의 복잡한 추론 능력을 강화하는 가능성에 대한 가능성을 보여주고 있습니다. 이들은 수학이나 코딩과 같은 어려운 작업에서 뛰어난 성능을 보여주지만, 시간적 긴급성 또는 지식밀집형 질문에 대해 내부 지식에 의존하여 문제를 해결하는 것이 많은 경우, 부족한 부분이 있습니다. 이에 대처하여, 우리는 R1-Searcher를 제안합니다. 이는 새로운 2단계의 결과를 기반으로 하는 RL 접근 방식이며, LLMs의 검색 능력을 강화하기 위해 설계되었습니다. 이 방법은 추론 과정에서 외부 검색 시스템의 자동 호출을 가능하게 합니다. 우리 프레임워크는 초기에 프로세스 보상이나 가우시안 분산화가 필요하지 않습니다. 이 방법은 외란적인 데이터 세트에서도 효과적으로 일반화하고, 베이스 모형이나 명령 모형 모두를 지원합니다. 우리의 실험은 기존의 강력한 RAG 메소드보다 유의미하게 뛰어난 것을 보여주고, 폐원 GPT-4o-mini와 비교하여 동일한 결과를 보입니다.",
      "upvotes": 9,
      "discussionId": "67ce5fd3e5cdfda52b912436",
      "githubRepo": "https://github.com/SsmallSong/R1-Searcher"
    },
    "publishedAt": "2025-03-09T23:43:27.151Z",
    "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05592.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05638",
      "authors": [
        {
          "_id": "67ce8388764226f050ad18b3",
          "name": "Mark YU",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b4",
          "user": {
            "_id": "657a7458afbb0117ba15c59f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
            "isPro": false,
            "fullname": "Wenbo Hu",
            "user": "wbhu-tc",
            "type": "user"
          },
          "name": "Wenbo Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:42.681Z",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b5",
          "user": {
            "_id": "64770e86d7cf39f2e937ae9a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64770e86d7cf39f2e937ae9a/pLqGg2z1KzQxCGpMwds-9.jpeg",
            "isPro": false,
            "fullname": "Jinbo Xing",
            "user": "Doubiiu",
            "type": "user"
          },
          "name": "Jinbo Xing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:40.328Z",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b6",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:06:56.510Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T17:57:53.000Z",
      "title": "トラジェクトライナー：디퓨전 모형을 통해 모노라ル 비디오의 카메라トラジェク트를 리다이렉트합니다.",
      "summary": "이곳에서는 TrajectoryCrafter라는 새로운 접근 방식을 소개합니다. 이 방법은 단점의 비디오의 카메라의 경로를 재방향하는 방법입니다. 확실한 시각변환과 스트로크적인 콘텐츠 생성을 분리하고, 사용자가 지정한 카메라의 경로에 엄격한 제어를 구현합니다. 새로운 이중 스트리밍 조건付き 비디오 디퓨저 모델을 제안합니다. 이 모델은 점 云 렌더링과 원 비디오를 모두 조건으로 통합하여 정확한 시각변환과 컬러로운 4D 콘텐츠 생성을 보장합니다. 다점의 비디오를 사용하지만, 네트워크 크기의 단점의 비디오와 정적 다점 데이터 세트를 조합하여 하이브리드 훈련 데이터 세트를 생성합니다. 이는 우리만의 이중 리프로젝션 전략을 통해 다양한 스케인에 대한 강력한 일반화를 촉진합니다. 다점 및 대규모 단점의 비디오에서 확장된 평가는 우리 방법의 높은 성능을 보여주며, 이는 우리의 방법의 우수성을 입증합니다.",
      "upvotes": 8,
      "discussionId": "67ce838a764226f050ad1952",
      "projectPage": "https://trajectorycrafter.github.io/",
      "githubRepo": "https://github.com/TrajectoryCrafter/TrajectoryCrafter"
    },
    "publishedAt": "2025-03-10T02:24:39.763Z",
    "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/lpXbCmGz-upwRVSEUzBjV.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05638.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a7458afbb0117ba15c59f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
      "fullname": "Wenbo Hu",
      "name": "wbhu-tc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05652",
      "authors": [
        {
          "_id": "67ce524ee969bc5fd69c9388",
          "user": {
            "_id": "61e9f5398c237a147a3f4ab5",
            "avatarUrl": "/avatars/afd4ec17cb132b5ab56e50a678c4786d.svg",
            "isPro": false,
            "fullname": "Yunfan Jiang",
            "user": "yunfanj",
            "type": "user"
          },
          "name": "Yunfan Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:07.901Z",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9389",
          "name": "Ruohan Zhang",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938a",
          "name": "Josiah Wong",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938b",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938c",
          "user": {
            "_id": "63509bc859bfa9a85d4220aa",
            "avatarUrl": "/avatars/ca2cc9b87f5ca5cd51606b2f9edf89d0.svg",
            "isPro": false,
            "fullname": "Yanjie Ze",
            "user": "yjze",
            "type": "user"
          },
          "name": "Yanjie Ze",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:05.242Z",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938d",
          "name": "Hang Yin",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938e",
          "name": "Cem Gokmen",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938f",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9390",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9391",
          "name": "Li Fei-Fei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T18:15:21.000Z",
      "title": "보이ア우트로보ット 슈트: 일상 생활 활동의 전신 동작을 간단히 할 수 있습니다.",
      "summary": "실세계의 가정 생활 업무는 모바일 작동 로봇에게 중대한 문제를 보여주는 데 있습니다. 현재 존재하는 로봇 기술 벤치마크 분석에서 성공한 업무의 실행은 3가지 주요한 전체적인 신체 제어 능력을 의존하는 것으로 나타났습니다: 비마니 유코더레이션, 안정적이고 정밀한 네비게이션, 그리고 분산된 끝부 효과자의 접근성. 이러한 능력을 달성하기 위해 엄밀한 하드웨어 설계가 필요하지만, 그 결과로는 비지니스 정책 학습이 더 복잡해지게 됩니다. 이러한 문제를 대처하기 위해 우리는 다양한 가정 생활 업무에서 전체적인 작동을 수행하기 위한 엄밀한 프레임워크를 통해, BEHAVIOR 로봇 슈트(BRS)를 소개합니다. 비마니 유코더레이션, 홀더 로봇으로 구성되며 4자유도의 타로스를 가지는 BRS는 데이터 수집과 전체적인 텔레오프로브 라이ن 엣프펙터의 수집을 위해 비용 효율적인 전체적인 텔레오프로브 라이ن 엣프펙터 인터페이스를 통합하고, 새로운 알고리즘을 사용하여 전체적인 비지니스 정책 학습을 위한 알고리즘을 통합하고 있습니다. BRS는 3가지 핵심적인 능력을 강조하면서, 긴 거리 네비게이션, 아키텍처 및 가변성 대상과의 상호작용, 그리고 좁은 공간에서의 작업 등 추가적인 복잡성을 도입한 5가지 어려운 가정 생활 업무에 대해 평가되었습니다. BRS의 통합된 로봇 에프메바봇, 데이터 수집 인터페이스, 그리고 학습 프레임워크는 일상적인 가정 생활 업무의 실세계적인 전체적인 작동을 가능하게 하는 중요한 단계로 인정되어 있습니다. BRS는 https://behavior-robot-suite.github.io/에서 오픈소스로 제공됩니다.",
      "upvotes": 7,
      "discussionId": "67ce5294e969bc5fd69c9a2c",
      "projectPage": "https://behavior-robot-suite.github.io/",
      "githubRepo": "https://github.com/behavior-robot-suite/brs-algo"
    },
    "publishedAt": "2025-03-09T22:51:04.616Z",
    "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/61e9f5398c237a147a3f4ab5/WD3cV-QLiHRgh4IUOrikN.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05652.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e9f5398c237a147a3f4ab5",
      "avatarUrl": "/avatars/afd4ec17cb132b5ab56e50a678c4786d.svg",
      "fullname": "Yunfan Jiang",
      "name": "yunfanj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05379",
      "authors": [
        {
          "_id": "67ce5f2389663abdbc364495",
          "name": "Jiaxing Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5f2389663abdbc364496",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "67ce5f2389663abdbc364497",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T12:46:42.000Z",
      "title": "R1-Omni: 해석 가능한 옴니모달 감정 인식과 강화 학습을 활용한",
      "summary": "이 연구에서는 감정 인식의 중심에서 증명 가능한 보상을 가지는 강화 학습(RLVR)의 첫 번째 적용을 Omni-multimodal 대 언어 모델에 적용합니다. 이 작업에서 시각과 음성 모델이 중요한 역할을 합니다. RLVR를 활용하여 Omni 모델을 최적화하고, 논리력, 감정 인식 정확도, 일반화 능력의 3가지 주요 면에서 성능을 크게 향상시킵니다. RLVR의 도입은 내부 데이터의 전체적인 성능을 향상시키는 데만 아니라, 외부 데이터 세트에서 우수한 강건성을 보여주는 데도 기여합니다. 더 중요한 것은 향상된 논리력은 다양한 모델이 감정 인식 프로세스에서 미치는 영향을 명확하게 분석할 수 있다는 것입니다. 이는 다 모델 대 언어 모델의 최적화에 일반적으로 필요한 기여를 제공합니다.",
      "upvotes": 6,
      "discussionId": "67ce5f2489663abdbc3644d0"
    },
    "publishedAt": "2025-03-09T23:40:46.906Z",
    "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05379.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04872",
      "authors": [
        {
          "_id": "67ce5deedb623d45a95deb72",
          "user": {
            "_id": "632c30576bcb864974cc40a8",
            "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
            "isPro": false,
            "fullname": "sunlin",
            "user": "lincharliesun",
            "type": "user"
          },
          "name": "Lin Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:47.601Z",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb73",
          "name": "Guangxiang Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb74",
          "name": "Xiaoqi Jian",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb75",
          "name": "Yuhan Wu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb76",
          "name": "Weihong Lin",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb77",
          "name": "Yongfu Zhu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb78",
          "name": "Change Jia",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb79",
          "name": "Linglin Zhang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7a",
          "name": "Jinzhu Wu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7b",
          "name": "Junfeng Ran",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7c",
          "name": "Sai-er Hu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7d",
          "name": "Zihan Jiang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7e",
          "name": "Junting Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7f",
          "name": "Wenrui Liu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb80",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb81",
          "name": "Tong Yang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb82",
          "name": "Xiangzheng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T16:25:53.000Z",
      "title": "TinyR1-32B-Preview: 브랜치 팀 결합 튜닝 학습으로 정확도 향상",
      "summary": "LLM의 크기를 줄이면서 성능을 유지하는 문제를 주목하고 있습니다. 그러나 현재의 방법들은 모델의 煉熱과トランスファーム学习などが高い精度を達成することが難しい場合が多いです。この制限を解決するために、Branch-Merge의 煉熱アプローチを提案します。このアプローチは、2つのステップでモデルの圧縮を強化します。まずは、(1) Branch Phaseで、大きな教師モデルからの知識を選択的に、領域専門的な観察学習(SFT)を通じて、特別化された学生モデルに 煉熱させます。次に、(2) Merge Phaseで、これらの学生モデルを統合し、領域間の知識移行を可能にし、一般化を向上させます。DeepSeek-R1を教師モデルとし、DeepSeek-R1-Distill-Qwen-32Bを学生モデルとして、この 煉熱アプローチを検証しました。その結果、合成されたモデルであるTinyR1-32B-Previewは、数学(+5.5점)、コーディング(+4.4点)、科学(+2.9点)の複数のベンチマークで、DeepSeek-R1-Distill-Qwen-32Bよりも優れています。また、AIME 2024ではDeepSeek-R1と近い性能を達成しました。Branch-Mergeの 煉熱アプローチは、計算コストと時間を減らしながら、小さくなった高性能のLLMの作成にスケーラブルな解決策を提供します。",
      "upvotes": 5,
      "discussionId": "67ce5df0db623d45a95dec1f"
    },
    "publishedAt": "2025-03-09T23:35:58.424Z",
    "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05447",
      "authors": [
        {
          "_id": "67ceab4132a6585cecad2c36",
          "user": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "isPro": false,
            "fullname": "Weigao Sun",
            "user": "weigao266",
            "type": "user"
          },
          "name": "Weigao Sun",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-10T09:09:22.436Z",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c37",
          "name": "Disen Lan",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c38",
          "name": "Tong Zhu",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c39",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c3a",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T14:17:45.000Z",
      "title": "Linear-MoE: 선형 시퀀스 모델링과混合의 포더의 융합",
      "summary": "線형シーケンスモデリング(LSM)와 같은 선형アタッチン, 상태공간モデル, 선형RNN, 그리고 ミックスオブエクスプレッショナー(MoE)가 최근 중요한 아키텍처 개선으로 광범위하게 사용되고 있습니다. 본 논문에서는 LSM과 MoE를 통합한 대형 모델의 모델링과 훈련을 위한 생산 수준 시스템인 'Linear-MoE'를 소개합니다. Linear-MoE는 선형 복잡도의 시퀀스 모델링의 우수한 성능과 희소한 활성화를 구현하는 MoE 계층의 장점을 활용하여 효율적인 훈련을 통해 높은 성능을 제공합니다. Linear-MoE 시스템은 1) 모델링 서브 시스템과 2) 훈련 서브 시스템으로 구성됩니다. 모델링 서브 시스템은 모든 LSM의 인스턴스를 지원하는 통합 프레임워크를 제공하며, 훈련 서브 시스템은 진행적인 병렬화 기술로 효율적인 훈련을 촉진합니다. 특히, Linear-MoE 모델에 대한 시퀀스 병렬화가 설계되었습니다. 또한, Linear-MoE 계층과 표준의 Transformer-MoE 계층을 조합한 하이브리드 모델도 검토하고, 이를 통해 모델의 유연성과 성능을 향상시키기 위한 시퀀스 병렬화를 사용합니다. A0.3B-2B와 A1B-7B의 두 모델 시리즈에 대한 평가는 Linear-MoE가 훈련 효율을 향상시키면서 다양한 벤치마크에서 경쟁적인 성능을 유지하는 것을 보여주며, 다음 세대의 기본 모델 아키텍처의 가능성을 보여주고 있습니다. 코드는 https://github.com/OpenSparseLLMs/Linear-MoE.",
      "upvotes": 3,
      "discussionId": "67ceab4232a6585cecad2c82",
      "githubRepo": "https://github.com/OpenSparseLLMs/Linear-MoE"
    },
    "publishedAt": "2025-03-10T05:05:22.522Z",
    "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05447.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01713",
      "authors": [
        {
          "_id": "67c75e18cb29e2a4b0eb0293",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:26:50.367Z",
          "hidden": false
        },
        {
          "_id": "67c75e18cb29e2a4b0eb0294",
          "name": "Guoliang Li",
          "hidden": false
        },
        {
          "_id": "67c75e18cb29e2a4b0eb0295",
          "name": "Jinyang Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T16:25:58.000Z",
      "title": "SAGE: 적절한 평가 프레임워크",
      "summary": "レタイブレーディングアウガイナション（RAG）는 특정 코퍼스 내의 질문응답（QA） 태스크에 있어 상당한 효과를 보입니다. 그러나 RAG의 QA에서 많은 실패 사례가 존재합니다. 이러한 실패는 대규모 언어 모델（LLMs）의 제한이 아니라, 두 가지 주요 제한으로 인해 발생합니다: 1. 현재의 RAG 방법들은 코퍼스를 семанти스 고려 없이 분할하여, 문제와 분할된 영역 사이의 관련성이 손실되어, 관련성이 있는 맥락을 특정하기 어려워집니다. 2. 맥락의 양과 추출된 맥락의 양 사이에 전환이 있습니다. 이 논문에서는 이러한 제한을 극복하기 위해 RAG 프레임워크（SAGE）를 소개합니다. 먼저, семанти스 고려하지 않은 분할 문제를 해결하기 위해, семанти스 분할 모델을 제안합니다. 이 모델은 코퍼스를 완전한 семанти스 기반의 맥락으로 분할하도록 훈련되었습니다. 2. 가장 적절한 맥락만 얻고, 관련없는 맥락을 무시하기 위해, 관련성 점수의 감소 속도에 기반하여 동적으로 맥락을 선택하는 맥락 선택 알고리즘을 설계합니다. 3. 맥락의 정확도를 향상시키기 위해, LLMs가 얻은 맥락이 과도하거나 부족한지 평가하고 그 양을 조정하는 것을 제안합니다. 실험은 SAGE는 QA의 품질에 대해 평균 61.25%의 개선률을 보입니다. 또한, 노이즈가 있는 맥락을 얻는 것을 피하여, LLM 추론에서 사용된 토큰의 비용을 줄이고, 평균 49.41%의 비용 효율 향상을 실현합니다. 우리의 연구는 RAG의 강화에 있어 유익한 시각을 제공합니다.",
      "upvotes": 3,
      "discussionId": "67c75e1ccb29e2a4b0eb03a9"
    },
    "publishedAt": "2025-03-10T03:23:51.482Z",
    "title": "SAGE: A Framework of Precise Retrieval for RAG",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01713.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05315",
      "authors": [
        {
          "_id": "67ce6db07110b8bedb3344a7",
          "name": "Saumya Chaturvedi",
          "hidden": false
        },
        {
          "_id": "67ce6db07110b8bedb3344a8",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:45.150Z",
          "hidden": false
        },
        {
          "_id": "67ce6db07110b8bedb3344a9",
          "name": "Laurent Bindschaedler",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T10:50:45.000Z",
      "title": "LoRACode: LoRA 어댑터즈 for Code 엮기\n\n(Note: The original text \"LoRA アダプターズ for Code エンベディング\" was translated to \"LoRA 어댑터즈 for Code 엮기\" to maintain the closest meaning in Korean. The term \"エンベディング\" was translated to \"엮기\" as it is a common term used in the context of code embedding or integration in Korean.)",
      "summary": "코드埋め込み는 セマンティックな 코드 검색에 중요하지만, 현재의 접근 방식은 코드에 고유한 정확한 문法和 컨텍스트적인 뉴アンス를 쉽게捉えることが多いです. 오픈 소스 모델의 코드BERT나 UniXcoder는 scalability와 효율성에 제한이 있으며, 고성능의 프로퍼티 시스템은 계산 비용이 크게 들게 됩니다. 우리는 Low-Rank Adaptation(LoRA)에 기반한 파라미터 효율적인 微調節 방법を 도입하고, 코드 검색에 적용可能な 任務専用 아다プターを 構築します. 우리의 접근 방식은 기본 모델의 학습 파라미터 수를 2%以下に抑え、2台の H100GPUで 25분で 200만 샘플의 매우 광범위한 코드 코퍼スに 효율적으로 微調節できます. 실험은 Code2Code 검색의 Mean Reciprocal Rank(MRR)が 최대 9.1% 증가し、多数の 프로그래밍 언어で의 Text2Code 검색 태스크では 최대 86.69% 증가しました. 任務ごとおよび 언어ごとの 適応性 の差異を もって, 코드 검색에 おける 문法和 言語的な 変化 の 敏感性 を 調査します.",
      "upvotes": 2,
      "discussionId": "67ce6db17110b8bedb3344c5"
    },
    "publishedAt": "2025-03-10T00:51:02.203Z",
    "title": "LoRACode: LoRA Adapters for Code Embeddings",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05132",
      "authors": [
        {
          "_id": "67ce5ec17c6e6ea1cc5649c2",
          "name": "Hengguang Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c3",
          "name": "Xirui Li",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c4",
          "name": "Ruochen Wang",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c5",
          "name": "Minhao Cheng",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c6",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c7",
          "name": "Cho-Jui Hsieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T04:21:47.000Z",
      "title": "R1-Zero의 \"Ahaモーメント\"、2B非SFTモデル에서 시각화된 이유론 연구",
      "summary": "최근 DeepSeek R1는 단순한 규칙 기반의 보상을 사용한 강화학습이 대규모 언어 모델에 있어서 복잡한 이유론을 자동으로 개발하는 것을 보여주고, \"에이호모멘트\"라는 특징을 가지고 있음을 보여주었다. 그러나 이러한 성공을 더 많은 이유론에 확장하려는 시도는 이러한 중요한 특징을 재현하는 데 실패했다. 본 보고서에서는 이 특징의 첫 번째 성공적인 재현을 非SFT 2B 모델에서 보고합니다. Qwen2-VL-2B을 시작으로 SAT 데이터셋에 직접 강화학습을 적용하고, CVBench에서 59.47%의 정확도를 달성하며, 기본 모델보다 약 30% 이상 개선되었으며, SFT 설정보다 약 2% 이상 개선되었다. 또한 R1 모델의 이유론을 실현하기 위한 강화학습을 사용한 시도의 실패와 피드백을 공유하고, 문제를 명확히 하기 위해 시도를 하였다. 주요한 견해에는 (1) 강화학습을 도입된 지시 모델에서 논리의 추적이 경량화될 확률이 높으며, (2) 랜덤한 길이 보상은 이유론의 능력에 효과가 없으며, 이러한 점이 있다. 프로젝트 코드는 https://github.com/turningpoint-ai/VisualThinker-R1-Zero에 공개되어 있습니다.",
      "upvotes": 2,
      "discussionId": "67ce5ec27c6e6ea1cc564a01"
    },
    "publishedAt": "2025-03-09T23:39:12.374Z",
    "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05132.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04808",
      "authors": [
        {
          "_id": "67ce5c7065b141ae6b0d3957",
          "name": "Stephen Chung",
          "hidden": false
        },
        {
          "_id": "67ce5c7065b141ae6b0d3958",
          "name": "Wenyu Du",
          "hidden": false
        },
        {
          "_id": "67ce5c7065b141ae6b0d3959",
          "name": "Jie Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:53:39.000Z",
      "title": "학습실패로부터의 반복학습",
      "summary": "최근의 대 언어 모델(LLM)의 강화 학습(RL)의 발전, DeepSeek R1를 예로 들면, 간단한 질문 대답 태스크에서도 LLM의 논리 능력이 크게 향상된 것을 보여주고 있습니다. 본 논문에서는 이 접근 방식을 확장하여, 다 타입 설정으로 변경합니다. 문제를 대답하기보다, 모델은 오답 후 피드백을 제공하여 여러 시도를 수행할 수 있습니다. 다 타입 태스크는, 모델이 이전 시도를 개선하고 검색 효율성을 향상시키기 위해 촉발됩니다. 실험 결과, 수학 벤치마크를 평가할 때, 1 타입에서 45.6%에서 2 타입에서 52.5%까지 정확도가 크게 향상되었습니다. 비교적으로, 같은 LLM이 표준의 1 타입 태스크로 훈련된 경우, 평가 시에 더 많은 시도를 제공하면 이러한 개선이 보이지 않습니다. 결과는, 표준의 1 타입 태스크에 비해, 다 타입 태스크로 훈련된 LLM은 수학 벤치마크에서 더 나은 성능을 달성하고, 동시에 사용자의 피드백에 따라 효과적으로 대답을 개선하는 것을 배울 수 있습니다. 완전한 코드는, https://github.com/DualityRL/multi-attempt 에 있습니다.",
      "upvotes": 2,
      "discussionId": "67ce5c7165b141ae6b0d39c6"
    },
    "publishedAt": "2025-03-09T23:29:35.505Z",
    "title": "Learning from Failures in Multi-Attempt Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04808.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04548",
      "authors": [
        {
          "_id": "67cbff8e4dedec48bdec8a99",
          "name": "Zhipeng Chen",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9a",
          "user": {
            "_id": "6703ac76ea890f0ca5b225eb",
            "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
            "isPro": false,
            "fullname": "Yingqian Min",
            "user": "EliverQ",
            "type": "user"
          },
          "name": "Yingqian Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:02:04.349Z",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9b",
          "name": "Beichen Zhang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9c",
          "name": "Jie Chen",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9d",
          "name": "Jinhao Jiang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9e",
          "name": "Daixuan Cheng",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9f",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa0",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa1",
          "name": "Xu Miao",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa2",
          "name": "Yang Lu",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa3",
          "name": "Lei Fang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa4",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa5",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T15:34:27.000Z",
      "title": "「R1モデル의 발발 및 향상에 관한 연구」",
      "summary": "이 보고서에서는 STILL 프로젝트의 일부로 스로티닝 모델의 개발에 관한 세 번째 유형의 기술 보고서를 제공합니다. 기술의 패턴이 명확해지는 데 따라, 이 논리 모델의 구현의 중심적인 기술로 RL 훈련의 스케일링이 되었습니다. RL 훈련에 영향을 미치는 다양한 요인의 효과를 체계적으로 실험하고 기록했습니다. 기초 모델과 미세 조정 모델을 모두 실험했습니다. 특히, 우리의 RL 훈련 접근 방식이 Qwen2.5-32B의 기초 모델을 일관되게 개선하고, 응답의 길이와 테스트 정확도를 향상시키는 것을 보여주었습니다. 또한, DeepSeek-R1-Distill-Qwen-1.5B와 같은 모델이 높은 성능 수준을 달성하는 경우, RL 훈련으로 발전하여 AIME 2024에서 39.33%의 정확도를 달성하는 것을 보여주었습니다. RL 훈련을 초과하여, 도구 사용도 검토했으며, 대규모 논리 모델의 논리 성능을 크게 향상시키는 것을 발견했습니다. 이 접근 방식은 AIME 2024에서 greedy search를 사용하여 86.67%의 정확도를 달성하고, 모델의 능력을 향상시키는 효과성을 명확히 보여주었습니다. STILL 프로젝트의 웹 사이트에서 리소스를 공개합니다: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
      "upvotes": 1,
      "discussionId": "67cbff8f4dedec48bdec8af3"
    },
    "publishedAt": "2025-03-10T05:23:26.375Z",
    "title": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04548.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6703ac76ea890f0ca5b225eb",
      "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
      "fullname": "Yingqian Min",
      "name": "EliverQ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04504",
      "authors": [
        {
          "_id": "67cb8e882cfa481bcee9455e",
          "user": {
            "_id": "66a07c07b7f0bb64d3b35497",
            "avatarUrl": "/avatars/c38af1ddb7a5b625e26b7ff05957ff7c.svg",
            "isPro": false,
            "fullname": "SunghyunAhn",
            "user": "SkiddieAhn",
            "type": "user"
          },
          "name": "Sunghyun Ahn",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:02:12.798Z",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee9455f",
          "user": {
            "_id": "673d7b70713e4b8db2d5ca94",
            "avatarUrl": "/avatars/b9e89eba62eb939ddd93c1cb91744e93.svg",
            "isPro": false,
            "fullname": "Youngwan Jo",
            "user": "jyy1551",
            "type": "user"
          },
          "name": "Youngwan Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:02:08.710Z",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94560",
          "name": "Kijung Lee",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94561",
          "name": "Sein Kwon",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94562",
          "name": "Inpyo Hong",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94563",
          "name": "Sanghyun Park",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T14:52:34.000Z",
      "title": "AnyAnomaly: LVLM을 활용한 0샷 기반의 사용자定制 가능한 비디오 이상 감지",
      "summary": "ビデオ異常検出(VAD)는 컴퓨터 비전의 비디오 분석과 시청 건강에 중요합니다. 그러나 현재의 VAD 모델은 학습된 정상 패턴에 의존하여 다양한 환경에 적용하기 어렵습니다. 그 결과, 사용자는 새로운 환경에 대응하기 위해 모델을 재학습하거나 다른 AI 모델을 개발하는 것이 필요합니다. 이는 기계 학습의 전문 지식, 고성능의 하드웨어, 그리고 확장된 데이터의 수집방법이 필요하며, VAD의 실용적인 활용에 제한됩니다. 이러한 문제를 해결하기 위해, 본 연구에서는 사용자 정의 가능한 비디오 이상 검출(C-VAD) 기술과 AnyAnomaly 모델을 제안합니다. C-VAD는 사용자 정의된 텍스트를 이상 이벤트로 처리하고, 비디오 내 특정 이벤트를 포함하는 프레임을 검출합니다. AnyAnomaly는 대규모 비디오 언어 모델의 微調節을 제외한, 맥락에 대한 시각 질문에 의해 효과적으로 구현되었습니다. 제안된 모델의 유효성을 증명하기 위해, C-VAD 데이터셋을 구축하고, AnyAnomaly의 우월성을 보여주었습니다. 또한, 우리의 접근法是 VAD 벤치마크 데이터셋에서 경쟁적인 성능을 보여주며, UBnormal 데이터셋에서 가장 先端의 결과를 달성했으며, 모든 데이터셋의 확장성 측면에서 다른 방법보다 뛰어납니다. 우리의 코드는 github.com/SkiddieAhn/Paper-AnyAnomaly에서 공개되어 있습니다.",
      "upvotes": 0,
      "discussionId": "67cb8e8a2cfa481bcee945cd",
      "githubRepo": "https://github.com/SkiddieAhn/Paper-AnyAnomaly"
    },
    "publishedAt": "2025-03-10T04:06:49.447Z",
    "title": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/TNPrQD3FdFBVVW2pUoEnU.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/cA30QnSF_7AeHciWhCFSN.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/T9EwE4Ea7DrH3XVZ_eJ1g.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/Duuqk_Ph1GNfz6HW2Qv5g.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/-Dze7JwIaBTbCfXUetsCd.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04504.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a07c07b7f0bb64d3b35497",
      "avatarUrl": "/avatars/c38af1ddb7a5b625e26b7ff05957ff7c.svg",
      "fullname": "SunghyunAhn",
      "name": "SkiddieAhn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]