[
  {
    "paper": {
      "id": "2503.07920",
      "authors": [
        {
          "_id": "67d0f9c95f0fcc0c38902b8e",
          "name": "Samuel Cahyawijaya",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b8f",
          "name": "Holy Lovenia",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b90",
          "name": "Joel Ruben Antony Moniz",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b91",
          "name": "Tack Hwa Wong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b92",
          "name": "Mohammad Rifqi Farhansyah",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b93",
          "name": "Thant Thiri Maung",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b94",
          "name": "Frederikus Hudi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b95",
          "name": "David Anugraha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b96",
          "user": {
            "_id": "63ddfced5ea8577c8d5fb421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677144169806-63ddfced5ea8577c8d5fb421.jpeg",
            "isPro": false,
            "fullname": "Muhammad Ravi Shulthan Habibi",
            "user": "muhammadravi251001",
            "type": "user"
          },
          "name": "Muhammad Ravi Shulthan Habibi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:20.672Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b97",
          "name": "Muhammad Reza Qorib",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b98",
          "name": "Amit Agarwal",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b99",
          "name": "Joseph Marvin Imperial",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9a",
          "name": "Hitesh Laxmichand Patel",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9b",
          "user": {
            "_id": "67d1039a3e0dca11407f9460",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tN5K_Gc8oAlw0ADYuyc1s.png",
            "isPro": false,
            "fullname": "Vicky Feliren",
            "user": "feliren",
            "type": "user"
          },
          "name": "Vicky Feliren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:30.804Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9c",
          "name": "Bahrul Ilmi Nasution",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9d",
          "user": {
            "_id": "67559e52860bd4d8f4e9beeb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/I_SNcfwTifgHtL9NFLLli.jpeg",
            "isPro": false,
            "fullname": "Manuel Antonio Rufino",
            "user": "antonrufino",
            "type": "user"
          },
          "name": "Manuel Antonio Rufino",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:33.476Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9e",
          "name": "Genta Indra Winata",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9f",
          "name": "Rian Adam Rajagede",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba0",
          "name": "Carlos Rafael Catalan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba1",
          "name": "Mohamed Fazli Imam",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba2",
          "name": "Priyaranjan Pattnayak",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba3",
          "name": "Salsabila Zahirah Pranida",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba4",
          "name": "Kevin Pratama",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba5",
          "name": "Yeshil Bangera",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba6",
          "user": {
            "_id": "66d03a984505b8d635183aaa",
            "avatarUrl": "/avatars/0eab10dfad243d9dc19318b0f88de496.svg",
            "isPro": false,
            "fullname": "Adisai Na-Thalang",
            "user": "ensmart72",
            "type": "user"
          },
          "name": "Adisai Na-Thalang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:50:17.122Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba7",
          "name": "Patricia Nicole Monderin",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba8",
          "name": "Yueqi Song",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba9",
          "name": "Christian Simon",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902baa",
          "name": "Lynnette Hui Xian Ng",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bab",
          "name": "Richardy Lobo' Sapan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bac",
          "name": "Taki Hasan Rafi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bad",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bae",
          "name": "Supryadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902baf",
          "name": "Kanyakorn Veerakanjana",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb0",
          "name": "Piyalitt Ittichaiwong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb1",
          "name": "Matthew Theodore Roque",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb2",
          "name": "Karissa Vincentio",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb3",
          "name": "Takdanai Kreangphet",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb4",
          "user": {
            "_id": "631a4855300a072a8da70abd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631a4855300a072a8da70abd/jRnzdW5JBjICYKCmkUFI-.jpeg",
            "isPro": false,
            "fullname": "phakphum artkaew",
            "user": "pakphum",
            "type": "user"
          },
          "name": "Phakphum Artkaew",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:41.811Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb5",
          "name": "Kadek Hendrawan Palgunadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb6",
          "name": "Yanzhi Yu",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb7",
          "name": "Rochana Prih Hastuti",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb8",
          "name": "William Nixon",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb9",
          "name": "Mithil Bangera",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bba",
          "name": "Adrian Xuan Wei Lim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbb",
          "user": {
            "_id": "64f2e3b87244601d8f4365cf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f2e3b87244601d8f4365cf/QHKB8DOMBoKSXgMo6nY6z.jpeg",
            "isPro": false,
            "fullname": "Aye Hninn Khine",
            "user": "ayehninnkhine",
            "type": "user"
          },
          "name": "Aye Hninn Khine",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:27.900Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbc",
          "name": "Hanif Muhammad Zhafran",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbd",
          "name": "Teddy Ferdinan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbe",
          "name": "Audra Aurora Izzani",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbf",
          "name": "Ayushman Singh",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc0",
          "name": "Evan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc1",
          "name": "Jauza Akbar Krito",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc2",
          "name": "Michael Anugraha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc3",
          "name": "Fenal Ashokbhai Ilasariya",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc4",
          "name": "Haochen Li",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc5",
          "name": "John Amadeo Daniswara",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc6",
          "name": "Filbert Aurelian Tjiaranata",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc7",
          "name": "Eryawan Presma Yulianrifat",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc8",
          "name": "Can Udomcharoenchaikit",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc9",
          "name": "Fadil Risdian Ansori",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bca",
          "name": "Mahardika Krisna Ihsani",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcb",
          "name": "Giang Nguyen",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcc",
          "name": "Anab Maulana Barik",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcd",
          "name": "Dan John Velasco",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bce",
          "name": "Rifo Ahmad Genadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcf",
          "name": "Saptarshi Saha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd0",
          "user": {
            "_id": "66a31819b839c8994e5c3815",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a31819b839c8994e5c3815/ARVZtfxJYyGvZ0zHyaaBP.png",
            "isPro": false,
            "fullname": "Chengwei Wei",
            "user": "amao0o0",
            "type": "user"
          },
          "name": "Chengwei Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:39.101Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd1",
          "name": "Isaiah Flores",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd2",
          "name": "Kenneth Ko Han Chen",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd3",
          "name": "Anjela Gail Santos",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd4",
          "name": "Wan Shen Lim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd5",
          "name": "Kaung Si Phyo",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd6",
          "name": "Tim Santos",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd7",
          "name": "Meisyarah Dwiastuti",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd8",
          "name": "Jiayun Luo",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd9",
          "name": "Jan Christian Blaise Cruz",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bda",
          "name": "Ming Shan Hee",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdb",
          "name": "Ikhlasul Akmal Hanif",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdc",
          "name": "M. Alif Al Hakim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdd",
          "name": "Muhammad Rizky Sya'ban",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bde",
          "name": "Kun Kerdthaisong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdf",
          "name": "Lester James V. Miranda",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be0",
          "name": "Fajri Koto",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be1",
          "name": "Tirana Noor Fatyanosa",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be2",
          "name": "Alham Fikri Aji",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be3",
          "name": "Jostin Jerico Rosal",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be4",
          "name": "Jun Kevin",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be5",
          "user": {
            "_id": "640ead243830fd441c2e9838",
            "avatarUrl": "/avatars/4083942ce6b432a4cfb3524f72bcffb0.svg",
            "isPro": false,
            "fullname": "Robert Wijaya",
            "user": "wijayarobert",
            "type": "user"
          },
          "name": "Robert Wijaya",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:17.433Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be6",
          "name": "Onno P. Kampman",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be7",
          "name": "Ruochen Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be8",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:36.440Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be9",
          "name": "Peerat Limkonchotiwat",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T23:54:52.000Z",
      "title": "クラウドソース, 크로일, 또는 생성? SEA-VL, 동남아의 다중카르쳐의 비전 언어 데이터 세트를 생성する",
      "summary": "東南アジア（SEA）는 다양한 언어와 문화의 다양성을 특징으로 하고 있지만, 이 지역에서의 시각언어（VL） 연구에 대표적인 존재가 아니다. 이는 AI 모델이 SEA 문화의 微妙な nuance을 이해하지 못하여 실패하는 것을招致하는 것입니다. 이를填补하기 위해, SEA-VL라는 오픈소스 프로젝트를 제안하고 있습니다. 이 프로젝트는 SEA 언어에 관련된 고품질 데이터의 개발을 목표로 합니다. SEA-VL은 SEA 국가의 기여자들을 포함한, 문화의 관련성과 다양성을 보장하고, VL 연구에서 대표적인 언어의 더 넓은 확산을 촉진합니다. 클라우드 솔루션보다 비용과 시간 비용이 낮고, 문화의 관련성은 약 85%에 도달합니다. 그러나, 생성적인 시각 모델의 발전에도 불구하고, 합성된 이미지는 SEA 문화를 정확히 반영할 수 없게 됨을 명확히 합니다. 생성된 이미지는 지역의 微妙한 전통과 문화의 맥락을 반영할 수 없습니다. SEA-VL에서는, SEA 문화에 관련된 128만 장의 이미지를 모으고, 그 크기는 다른 데이터셋보다 50배 이상 큰 것을 목표로 합니다. SEA-VL에서는 SEA의 대표적인 부족을 채우고, 다양한 문화를 실제적으로 표현하는 더 넓은 AI 시스템의 개발을 촉진하고자 합니다.",
      "upvotes": 44,
      "discussionId": "67d0f9cd5f0fcc0c38902cdf",
      "ai_keywords": [
        "vision-language (VL) research",
        "cultural relevance",
        "crowdsourcing",
        "image crawling",
        "image generation",
        "generative vision models",
        "synthesized images",
        "datasets"
      ]
    },
    "publishedAt": "2025-03-10T19:54:52.000Z",
    "title": "Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia",
    "summary": "Southeast Asia (SEA) is a region of extraordinary linguistic and cultural\ndiversity, yet it remains significantly underrepresented in vision-language\n(VL) research. This often results in artificial intelligence (AI) models that\nfail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an\nopen-source initiative dedicated to developing high-quality, culturally\nrelevant data for SEA languages. By involving contributors from SEA countries,\nSEA-VL aims to ensure better cultural relevance and diversity, fostering\ngreater inclusivity of underrepresented languages in VL research. Beyond\ncrowdsourcing, our initiative goes one step further in the exploration of the\nautomatic collection of culturally relevant images through crawling and image\ngeneration. First, we find that image crawling achieves approximately ~85%\ncultural relevance while being more cost- and time-efficient than\ncrowdsourcing. Second, despite the substantial progress in generative vision\nmodels, synthetic images remain unreliable in accurately reflecting SEA\ncultures. The generated images often fail to reflect the nuanced traditions and\ncultural contexts of the region. Collectively, we gather 1.28M SEA\nculturally-relevant images, more than 50 times larger than other existing\ndatasets. Through SEA-VL, we aim to bridge the representation gap in SEA,\nfostering the development of more inclusive AI systems that authentically\nrepresent diverse cultures across SEA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07920.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07536",
      "authors": [
        {
          "_id": "67d04f248f79213c2fc0ba04",
          "name": "Yingzhe Peng",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba05",
          "name": "Gongrui Zhang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba06",
          "name": "Miaosen Zhang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba07",
          "name": "Zhiyuan You",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba08",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba09",
          "name": "Qipeng Zhu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0a",
          "name": "Kai Yang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0b",
          "name": "Xingzhong Xu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0c",
          "name": "Xin Geng",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0d",
          "name": "Xu Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:04:14.000Z",
      "title": "LMM-R1: 두 단계 규칙 기반의 RL을 기반으로 강력한 논리력 능력을 가진 30억 단어 모델을 강화합니다.",
      "summary": "대규모 다모둠 모델(LMMs)의 이유론을 강화하기 위해서는 시각적 인식과 이성적인 이유론의 복잡한 상호작용에서 발생하는 고유한 문제를 해결해야 합니다. 특히, 30억 파라미터를 줄인 아키텍처에서는 아키텍처의 제약이 이유론의 능력과 모델의 대립을 제한합니다.\n\n규칙 기반의 강화학습(RL)은 문맥만 있는 영역에서 뛰어난 성능을 보입니다만, 그 다모둠 모델 확장판에서는 두 가지 큰 장애물이 있습니다. 1. 데이터의 제한으로 인해 불분명한 답변이나 복잡한 이유론의 예가 부족합니다. 2. 모델의 사전 학습에 의한 기초적인 이유론의 저하도 원인입니다.\n\n이러한 문제를 해결하기 위해, 우리는 \\method라는 두 단계 프레임워크를 제안합니다. 이 프레임워크는 RL을 다모둠 이유론에 적용하기 위한 기초적인 이유론의 향상(FRE)과, MGT(다모둠의 일반화 훈련)을 통해 수행됩니다. FRE 단계는 RL을 사용하여 문맥만 있는 데이터로 이유론의 능력을 강화하고, 그 후 MGT 단계에서는 이러한 이유론의 능력을 다모둠 영역에 일반화합니다.\n\nQwen2.5-VL-Instruct-3B에서의 실험은 \\method가 다모둠 벤치마크와 뇌만 벤치마크에서 각각 4.83%와 4.5%의 평균 개선률을 달성하며, 복잡한 축구 게임 태스크에서 3.63%의 효과를 보였습니다. 이러한 결과를 통해 뇌 기반의 이유론의 향상이 다모둠의 일반화에 효과적이며, 고품질의 다모둠 훈련 데이터를 필요로 하는 고비용의 패러다임을 피하는 데이터 효율적인 패러다임을 제공함을 입증합니다.",
      "upvotes": 41,
      "discussionId": "67d04f268f79213c2fc0ba8b",
      "projectPage": "https://forjadeforest.github.io/LMM-R1-ProjectPage",
      "githubRepo": "https://github.com/TideDra/lmm-r1",
      "ai_keywords": [
        "Large Multimodal Models (LMMs)",
        "visual perception",
        "logical reasoning",
        "3B-parameter architectures",
        "rule-based reinforcement learning (RL)",
        "multimodal extension",
        "ambiguous answers",
        "complex reasoning examples",
        "degraded foundational reasoning",
        "multimodal pretraining",
        "Foundational Reasoning Enhancement (FRE)",
        "Multimodal Generalization Training (MGT)",
        "Qwen2.5-VL-Instruct-3B",
        "multimodal benchmarks",
        "text-only benchmarks",
        "complex Football Game tasks",
        "text-based reasoning enhancement",
        "data-efficient paradigm"
      ]
    },
    "publishedAt": "2025-03-10T13:04:14.000Z",
    "title": "LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through\n  Two-Stage Rule-Based RL",
    "summary": "Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges\nfrom the complex interplay between visual perception and logical reasoning,\nparticularly in compact 3B-parameter architectures where architectural\nconstraints limit reasoning capacity and modality alignment.\n  While rule-based reinforcement learning (RL) excels in text-only domains, its\nmultimodal extension confronts two critical barriers: (1) data limitations due\nto ambiguous answers and scarce complex reasoning examples, and (2) degraded\nfoundational reasoning induced by multimodal pretraining.\n  To address these challenges, we propose \\method, a two-stage\nframework adapting rule-based RL for multimodal reasoning through\nFoundational Reasoning Enhancement (FRE) followed by\nMultimodal Generalization Training (MGT). The FRE stage first\nstrengthens reasoning abilities using text-only data with rule-based RL, then\nthe MGT stage generalizes these reasoning capabilities to multimodal domains.\n  Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves\n4.83\\% and 4.5\\% average improvements over baselines in multimodal and\ntext-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game\ntasks. These results validate that text-based reasoning enhancement enables\neffective multimodal generalization, offering a data-efficient paradigm that\nbypasses costly high-quality multimodal training data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07536.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08638",
      "authors": [
        {
          "_id": "67d1027435066eade61549ae",
          "user": {
            "_id": "5fd6f670053c8345eddc1b68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd6f670053c8345eddc1b68/cuTsu2krRYHC6zYGD2dpQ.jpeg",
            "isPro": false,
            "fullname": "Ruibin Yuan",
            "user": "a43992899",
            "type": "user"
          },
          "name": "Ruibin Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:33.054Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549af",
          "name": "Hanfeng Lin",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b0",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b1",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-12T06:24:13.961Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b2",
          "name": "Jiahao Pan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b3",
          "name": "Yongyi Zang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b4",
          "name": "Haohe Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b5",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b6",
          "name": "Wenye Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b7",
          "user": {
            "_id": "654907a4a1faff97850c4eff",
            "avatarUrl": "/avatars/458c90151614bc7f116943b6e67d6b8a.svg",
            "isPro": false,
            "fullname": "du",
            "user": "dododododo",
            "type": "user"
          },
          "name": "Xingjian Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:36.330Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b8",
          "name": "Xinrun Du",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b9",
          "name": "Zhen Ye",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ba",
          "name": "Tianyu Zheng",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bb",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bc",
          "user": {
            "_id": "6417d9ea8f689506e7148417",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
            "isPro": false,
            "fullname": "minghao",
            "user": "Liam-Liu",
            "type": "user"
          },
          "name": "Minghao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:39.193Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bd",
          "name": "Zeyue Tian",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549be",
          "name": "Ziya Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bf",
          "name": "Liumeng Xue",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c0",
          "user": {
            "_id": "6628adb14277eae0da5eee28",
            "avatarUrl": "/avatars/6cb41b80cc5e014e455dfc2a22682e64.svg",
            "isPro": true,
            "fullname": "HKUST Audio",
            "user": "HKUST-Audio",
            "type": "user"
          },
          "name": "Xingwei Qu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-12T03:41:43.139Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c1",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c2",
          "name": "Shangda Wu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c3",
          "name": "Tianhao Shen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c4",
          "name": "Ziyang Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c5",
          "name": "Jun Zhan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c6",
          "name": "Chunhui Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c7",
          "name": "Yatian Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c8",
          "name": "Xiaowei Chi",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c9",
          "name": "Xinyue Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ca",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cb",
          "name": "Xiangzhou Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cc",
          "name": "Shansong Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cd",
          "name": "Lingrui Mei",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ce",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cf",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d0",
          "name": "Jianwei Yu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d1",
          "name": "Guojian Pang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d2",
          "name": "Xu Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d3",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d4",
          "name": "Xiaohuan Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d5",
          "name": "Lijun Yu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d6",
          "name": "Emmanouil Benetos",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d7",
          "name": "Yong Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d8",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d9",
          "name": "Xie Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549da",
          "name": "Gus Xia",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549db",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549dc",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549dd",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549de",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549df",
          "name": "Xipeng Qiu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e0",
          "name": "Roger Dannenberg",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e1",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e2",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e3",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e4",
          "name": "Wei Xue",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e5",
          "name": "Xu Tan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e6",
          "name": "Yike Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:26:50.000Z",
      "title": "유어：장음의 음악 생성에 대한 개방형 기초 모델의 스케일링\n\n(注意：虽然任务要求不添加任何解释或额外的文本，但为了确保翻译的准确性和专业性，这里提供了一个更符合韩文表达习惯的翻译版本。如果需要严格按照原文翻译，请告知。)",
      "summary": "우리는, 긴 문장의 음악 생성 임무를 특히 가사에서 노래로 어려운 문제를 해결하기 위해, LLaMA2 아키텍처를 기반으로 하는 개방적인 기초 모델의 가족 \"YuE\"를 소개합니다. 특히, YuE는 토큰 수 삼천억을 초과하고, 5분의 음악을 생성할 수 있으며, 동시에 가사의 일치성, 콜라르지 음악 구조, 매력적인 보카럴 멜로디를 유지합니다. 이는 (1) 트랙 분리의 다음 토큰 예측, (2) 긴 문맥의 가사 일치성의 발전적인 조건부, (3) 다 태스크, 다 단계의 사전 학습 프로кси를 통해 실현됩니다. 또한, YuE는 음악 생성의 사전 학습 기술을 재설계하고, 다양한 스타일 트랜스포어션 (예: 일본의 시티 팝을 영어의 랩으로 변환하면서 원본accompaniment를 유지)과 양방향 생성을 가능하게 합니다. 세부적인 평가에 따르면, YuE는 음악성 및 보카럴의 유연성에서 일부 프로피에이티 시스템을 초월할 수 있음을 보여주었습니다. 또한, YuE의 미세 조정에서 추가 제어와 추가 언어 지원이 가능합니다. 또한, 생성 결과도, YuE의 학습된 표현은 음악 이해 임무에서도 우수한 성능을示す 수능이 있으며, MARBLE 벤치마크에서 최상위 방법보다 뛰어넘을 수 있습니다. 키워드: 가사2노래, 노래 생성, 긴 문장, 기초 모델, 음악 생성",
      "upvotes": 39,
      "discussionId": "67d1027735066eade6154a7e",
      "ai_keywords": [
        "track-decoupled next-token prediction",
        "dense mixture signals",
        "structural progressive conditioning",
        "long-context lyrical alignment",
        "multitask, multiphase pre-training",
        "in-context learning",
        "versatile style transfer",
        "bidirectional generation",
        "musicality",
        "vocal agility",
        "tail languages",
        "music understanding tasks",
        "MARBLE benchmark"
      ]
    },
    "publishedAt": "2025-03-11T13:26:50.000Z",
    "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
    "summary": "We tackle the task of long-form music generation--particularly the\nchallenging lyrics-to-song problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08638.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08120",
      "authors": [
        {
          "_id": "67d0f5ace3c8042929eea946",
          "user": {
            "_id": "64c860d23a3f428da65ea499",
            "avatarUrl": "/avatars/f0bcc6ae7e558babe691b6bbf1059c9d.svg",
            "isPro": false,
            "fullname": "lijunzhe",
            "user": "tulvgengenr",
            "type": "user"
          },
          "name": "Junzhe Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:39:26.197Z",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea947",
          "name": "Xuerui Qiu",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea948",
          "name": "Linrui Xu",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea949",
          "name": "Liya Guo",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94a",
          "user": {
            "_id": "64daecec888b7e9c400f59b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png",
            "isPro": false,
            "fullname": "Delin Qu",
            "user": "delinqu",
            "type": "user"
          },
          "name": "Delin Qu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:39:29.349Z",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94b",
          "name": "Tingting Long",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94c",
          "name": "Chun Fan",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94d",
          "name": "Ming Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T07:34:59.000Z",
      "title": "UniF^2ace: 분화화된 얼굴 이해와 생성을 수행하는 통일된 모노모달 모델",
      "summary": "유니fied 멀티모달 모델 (UMMs)는 기초적인 컴퓨터 비전 연구의 패러다임으로서 강력한 역할을 수행하고, 이미지 이해와 생성에 있어 상당한 잠재력을 보여주고 있습니다. 그러나 기존 얼굴 파일 영역의 연구는 근간적인 얼굴 특징 이해를 중심으로 초점을 맞추고, 微妙한 얼굴 특징 처리 능력이 제한되어 생성 능력이 문제로 여겨졌습니다. 이러한 제한을 극복하기 위해, 우리는 UniF^2ace를 제안합니다. 이는 처음으로, 微妙한 얼굴 이해와 생성에 특화된 UMM입니다. 일반적으로, 우리는 2가지 상호 이익을 얻는 분기 방법과 2단계의 미크스 오브 익스플로레이터 아키텍처를 사용하여, 우리가 구축한 특별한 데이터 세트를 통해 UniF^2ace를 훈련합니다. 특히, 우리는 처음으로, 130K 이미지-텍스트 페어를 포함하는 대규모 얼굴 데이터 세트인 UniF^2ace-130K를 구축했으며, 이는 100만 개의 질문-답변 페어로 다양한 얼굴 특징을 확장한 것입니다. 다음으로, 분기 점수 매칭과 마스크付き 생성 모델의 이론적 연계를 확립하고, 두 가지의 증명 하한을 동시에 최적화하여, 모델의 얼굴의 세부 정보를 합성하는 능력을 크게 향상시킵니다. 마지막으로, 토큰 수준과 시퀀스 수준의 미크스 오브 익스플로레이터를 도입하여, 이해와 생성 태스크의 두 가지에 효율적인 微妙한 표현 학습을 가능하게 합니다. UniF^2ace-130K에서 확장된 실험은 UniF^2ace가 기존의 UMMs와 생성 모델을 초과하여, 이해와 생성 태스크의 두 가지에서 우수한 성능을 달성하고 있음을 보여주고 있습니다.",
      "upvotes": 23,
      "discussionId": "67d0f5b4e3c8042929eeab49",
      "ai_keywords": [
        "diffusion score matching",
        "masked generative models",
        "evidence lower bounds",
        "mixture-of-experts",
        "token-level",
        "sequence-level"
      ]
    },
    "publishedAt": "2025-03-11T03:34:59.000Z",
    "title": "UniF^2ace: Fine-grained Face Understanding and Generation\n  with Unified Multimodal Models",
    "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm in\nfoundational computer vision research, demonstrating significant potential in\nboth image understanding and generation. However, existing research in the face\ndomain primarily focuses on coarse facial attribute understanding,\nwith limited capacity to handle fine-grained facial attributes and\nwithout addressing generation capabilities. To overcome these limitations, we\npropose UniF^2ace, the first UMM tailored specifically for\nfine-grained face understanding and generation. In general, we train\nUniF^2ace on a self-constructed, specialized dataset utilizing two\nmutually beneficial diffusion techniques and a two-level mixture-of-experts\narchitecture. Specifically, we first build a large-scale facial dataset,\nUniF^2ace-130K, which contains 130K image-text pairs with one\nmillion question-answering pairs that span a wide range of facial attributes.\nSecond, we establish a theoretical connection between discrete diffusion score\nmatching and masked generative models, optimizing both evidence lower bounds\nsimultaneously, which significantly improves the model's ability to synthesize\nfacial details. Finally, we introduce both token-level and sequence-level\nmixture-of-experts, enabling efficient fine-grained representation learning for\nboth understanding and generation tasks. Extensive experiments on\nUniF^2ace-130K demonstrate that UniF^2ace outperforms\nexisting UMMs and generative models, achieving superior performance across both\nunderstanding and generation tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08120.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07703",
      "authors": [
        {
          "_id": "67d0f422a3158b8e55d3562f",
          "name": "Lixue Gong",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35630",
          "name": "Xiaoxia Hou",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35631",
          "name": "Fanshi Li",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35632",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35633",
          "name": "Xiaochen Lian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35634",
          "name": "Fei Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35635",
          "name": "Liyang Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35636",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35637",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35638",
          "name": "Yichun Shi",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35639",
          "name": "Shiqi Sun",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563a",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563b",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563c",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563d",
          "name": "Xun Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563e",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563f",
          "name": "Guofeng Wu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35640",
          "user": {
            "_id": "6381c5d63680a7cf34e08ca9",
            "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
            "isPro": false,
            "fullname": "wujie10558@gmail.com",
            "user": "wujie10",
            "type": "user"
          },
          "name": "Jie Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:40:44.088Z",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35641",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35642",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35643",
          "name": "Linjie Yang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35644",
          "name": "Zhonghua Zhai",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35645",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35646",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35647",
          "name": "Yuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35648",
          "name": "Shijia Zhao",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35649",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3564a",
          "name": "Weilin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:33.000Z",
      "title": "Seedream 2.0: シードリーム 2.0: 중국어-영어의 언어간 이미지 생성 기본 모델",
      "summary": "Rapidな ディフォーマンス モデル の 進歩 は、 画像 生成 分野 に おける 驚異的な 進歩 を 促進しています。しかし、 Flux、 SD3.5、 Midjourneyなどの 一般的な モデル は、 モデル バイアス、 限定的な テキストレンディング 能力、 中国文化 の ニュアンス の 理解 不足などの 問題 に 直面しています。 これらの 制限 を 解決するために、 発表します。 Seedream 2.0は、 多様な 次元で 優れている 母国語 と 英語 の バイリンガル 画像 生成 ベース モデルです。 これは、 中国語 と 英語 の 両方で テキスト プロンプト を 手に入れ、 バイリンガル 画像 生成 と テキストレンディング を サポートする ことを 通じて、 テキスト プロンプト を 管理します。 私たちは、 知識統合 を 促進する 強力な データ システム と、 画像 の 説明 の 精度 と 豊富さ を バランスに する キャプション システム を 開発しました。 特に、 Seedreamは、 自発的な バイリンガル 大規模言語 モデル を テキスト エンコーダー として 組み込み、 マススターデータ から 直接の 母国語 知識 を 学習する ことを 可能に します。 これにより、 中国語 または 英語 で 記述された 文化 の ニュアンス と 美術的 表現 を 高精度で 生成する ことができます。 また、 Glyph-Aligned ByT5は、 柔軟な 文字レベル の テキストレンディング を 支援し、 Scaled ROPEは 未学習の 解像度 に 対して よい 拡張性 を 持ちます。 多段階の 後学習 最適化、 SFTとRLHFのイテレーションを 含む、 わすれずの ファイナル モデル の 強力な 編集能力 を 進化させます。 拡張的な 実験 を 通じて、 Seedream 2.0は、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、 テキストレンディング、 構造的な 正確性など、 複数の 面で 最先端の 性能 を 達成している ことを 示します。 また、 Seedream 2.0は、 人間の 好み に 対して 最も 近い 出力 を 実現するために、 プロンプト従順性、 美術性、",
      "upvotes": 21,
      "discussionId": "67d0f42fa3158b8e55d358ea",
      "projectPage": "https://team.doubao.com/zh/tech/seedream",
      "ai_keywords": [
        "diffusion models",
        "Flux",
        "SD3.5",
        "Midjourney",
        "model bias",
        "Seedream 2.0",
        "bilingual image generation",
        "text prompt",
        "data system",
        "caption system",
        "bilingual large language model",
        "high-fidelity images",
        "cultural nuances",
        "aesthetic expressions",
        "Glyph-Aligned ByT5",
        "character-level text rendering",
        "Scaled ROPE",
        "multi-phase post-training optimizations",
        "SFT",
        "RLHF",
        "prompt-following",
        "structural correctness",
        "ELO score",
        "instruction-based image editing model",
        "SeedEdit"
      ]
    },
    "publishedAt": "2025-03-10T13:58:33.000Z",
    "title": "Seedream 2.0: A Native Chinese-English Bilingual Image Generation\n  Foundation Model",
    "summary": "Rapid advancement of diffusion models has catalyzed remarkable progress in\nthe field of image generation. However, prevalent models such as Flux, SD3.5\nand Midjourney, still grapple with issues like model bias, limited text\nrendering capabilities, and insufficient understanding of Chinese cultural\nnuances. To address these limitations, we present Seedream 2.0, a native\nChinese-English bilingual image generation foundation model that excels across\ndiverse dimensions, which adeptly manages text prompt in both Chinese and\nEnglish, supporting bilingual image generation and text rendering. We develop a\npowerful data system that facilitates knowledge integration, and a caption\nsystem that balances the accuracy and richness for image description.\nParticularly, Seedream is integrated with a self-developed bilingual large\nlanguage model as a text encoder, allowing it to learn native knowledge\ndirectly from massive data. This enable it to generate high-fidelity images\nwith accurate cultural nuances and aesthetic expressions described in either\nChinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible\ncharacter-level text rendering, while a Scaled ROPE generalizes well to\nuntrained resolutions. Multi-phase post-training optimizations, including SFT\nand RLHF iterations, further improve the overall capability. Through extensive\nexperimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art\nperformance across multiple aspects, including prompt-following, aesthetics,\ntext rendering, and structural correctness. Furthermore, Seedream 2.0 has been\noptimized through multiple RLHF iterations to closely align its output with\nhuman preferences, as revealed by its outstanding ELO score. In addition, it\ncan be readily adapted to an instruction-based image editing model, such as\nSeedEdit, with strong editing capability that balances instruction-following\nand image consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07703.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05978",
      "authors": [
        {
          "_id": "67d129d732b4bbfb938321a1",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a2",
          "user": {
            "_id": "66015e8aa4d296af07de538e",
            "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
            "isPro": false,
            "fullname": "Ye",
            "user": "Owen777",
            "type": "user"
          },
          "name": "Tian Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:50:12.319Z",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a3",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a4",
          "name": "Xuancheng Yang",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a5",
          "name": "Jiantong Zhao",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a6",
          "name": "Hanzhong Guo",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a7",
          "name": "Terrance Wang",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a8",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a9",
          "name": "Zeke Xie",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321aa",
          "name": "Lei Zhu",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ab",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ac",
          "name": "Michael Lingelbach",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ad",
          "name": "Daquan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T23:21:11.000Z",
      "title": "말과 목소리로 무한한 이야기의 이미지를 생성합니다.",
      "summary": "MagicInfinity는 Difu-sion Transfomer (DiT) 프레임워크를 사용하여, 이전의 인물 애니메이션의 제한을 극복하고 다양한 캐릭터 타입에서 고품질의 결과를 제공합니다. 이는 현실적인 인간, 전신의 피지 urea, 스타일화된 애니메이션 캐릭터를 포함합니다. 이 기능은 변화하는 얼굴의 자세를 지원하며, 대면의 관점을 포함하고 있으며, 입력 마스크를 사용하여 다 캐릭터 시선에서의 결정자를 정밀하게 지정할 수 있습니다. 우리의 접근 방식은 3개의 혁신적인 구현으로 주요 문제들을 해결하고 있습니다. 1. 3D 전체 注意 기능과 스라이딩 윈도우 디노이징 스테라쳐를 사용하여 무한 길이의 비디오 생성을 가능하게 하고, 시퀀적 협조성과 다양한 캐릭터 스타일의 시각적 품질을 유지합니다. 2. 2단계의 클레클러 러닝 스키밍을 도입하여, 음성을 입술 동기화, 문장을 표현적인 동작, 참조 이미지를 정체성을 유지하는 데 사용함으로써, 긴 기간의 긴 시선에서의 다 모달 제어를 가능하게 합니다. 3. 영역별 마스크와 적응적인 손실 함수를 사용하여, 글로벌 문장 제어와 국소적인 음성 가이드를 균형을 맞추고, 특정 결정자의 애니메이션을 지원합니다. 효율화는 우리의 혁신적인 통합 스텝과 cfg 디스틸레이션 기술로, 기본 모델보다 20배의 추론 속도를 달성합니다. 8개의 H100 GPU를 사용하여 540x540p의 비디오를 10초에 생성하고, 720x720p의 비디오를 30초에 생성할 수 있습니다. 질량 손실이 없습니다. 새로운 벤치마크 평가는 음성 입술 동기화, 정체성 유지, 자연스러운 동작의 우수한 성능을 보여주며, 공개적으로 사용할 수 있습니다. https://www.hedra.com/, 예시: https://magicinfinite.github.io/",
      "upvotes": 19,
      "discussionId": "67d129e332b4bbfb938324a0",
      "projectPage": "https://magicinfinite.github.io/",
      "ai_keywords": [
        "diffusion Transformer (DiT)",
        "3D full-attention mechanisms",
        "sliding window denoising strategy",
        "infinite video generation",
        "temporal coherence",
        "two-stage curriculum learning scheme",
        "audio for lip sync",
        "text for expressive dynamics",
        "reference images for identity preservation",
        "region-specific masks",
        "adaptive loss functions",
        "unified step and cfg distillation techniques",
        "inference speed",
        "audio-lip synchronization",
        "identity preservation",
        "motion naturalness"
      ]
    },
    "publishedAt": "2025-03-07T18:21:11.000Z",
    "title": "MagicInfinite: Generating Infinite Talking Videos with Your Words and\n  Voice",
    "summary": "We present MagicInfinite, a novel diffusion Transformer (DiT) framework that\novercomes traditional portrait animation limitations, delivering high-fidelity\nresults across diverse character types-realistic humans, full-body figures, and\nstylized anime characters. It supports varied facial poses, including\nback-facing views, and animates single or multiple characters with input masks\nfor precise speaker designation in multi-character scenes. Our approach tackles\nkey challenges with three innovations: (1) 3D full-attention mechanisms with a\nsliding window denoising strategy, enabling infinite video generation with\ntemporal coherence and visual quality across diverse character styles; (2) a\ntwo-stage curriculum learning scheme, integrating audio for lip sync, text for\nexpressive dynamics, and reference images for identity preservation, enabling\nflexible multi-modal control over long sequences; and (3) region-specific masks\nwith adaptive loss functions to balance global textual control and local audio\nguidance, supporting speaker-specific animations. Efficiency is enhanced via\nour innovative unified step and cfg distillation techniques, achieving a 20x\ninference speed boost over the basemodel: generating a 10 second 540x540p video\nin 10 seconds or 720x720p in 30 seconds on 8 H100 GPUs, without quality loss.\nEvaluations on our new benchmark demonstrate MagicInfinite's superiority in\naudio-lip synchronization, identity preservation, and motion naturalness across\ndiverse scenarios. It is publicly available at https://www.hedra.com/, with\nexamples at https://magicinfinite.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05978.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08625",
      "authors": [
        {
          "_id": "67d0fd74f8595b656f921a48",
          "user": {
            "_id": "632179745fc60c44fd91fc33",
            "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
            "isPro": false,
            "fullname": "zhumuzhi",
            "user": "Z-MU-Z",
            "type": "user"
          },
          "name": "Muzhi Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:42.495Z",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a49",
          "name": "Yuzhuo Tian",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4a",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4b",
          "name": "Chunluan Zhou",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4c",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4d",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4e",
          "name": "Ming Yang",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4f",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:08:54.000Z",
      "title": "SegAgent: 멤티모렐 모델라인 모델의 픽셀 이해 능력을 탐색하기 위해, 인간 Annotationist의 경로를 모방합니다.",
      "summary": "マルチプリオンラインモデル（MLLMs）는 이미지 이해 능력을 보여주지만, 픽셀 수준의 이해에 어려움을 겪는 점에서 실질적인 응용에 제한되어 있습니다. 현재 평가 태스크와 같은 VQA, 시각적 조지닝은 픽셀 수준의 이해를 정확히 평가하기 위해 과도하게 평가되고 있습니다. 픽셀 수준 이해의 근간이 되는 분할이 있지만, 현재의 방법들은 MLLMs가 외부 픽셀 디코더로 해석되는 은닉 토큰을 생성해야 합니다. 이 접근법은 MLLM의 텍스트 출력 공간을 파괴하고 언어 능력이 잠재적으로 손실되고, 기능성과 확장성을 줄이고, 모델의 고유의 픽셀 수준 이해를 반영하지 못하는 경우가 있습니다.\n\n따라서, 우리는 인간의 마스크 Annotation 태스크 (HLMAT)을 소개합니다. 이는 MLLMs가 인간 Annotation자처럼, 인터랙티브 분할 도구를 사용하여 Annotation하는 새로운 패러다임입니다. 분할을 다단계 마르코프 결정 과정으로 모델링하고, HLMAT는 구조적 변경이나 은닉 토큰을 사용하지 않고, MLLMs가 반복적으로 텍스트 기반의 클릭 포인트를 생성하여 고품질의 마스크를 구현할 수 있습니다. 이 시스템에서, SegAgent를 개발했습니다. 이는 인간처럼 Annotation 프로젝트에 최종 훈련된 모델입니다. 이 모델은 최신 기술 (SOTA)의 성능과 같은 성능을 달성하고, 마스크의 최적화와 Annotation의 필터링 등 추가 태스크를 지원합니다.\n\nHLMAT는 MLLMs의 픽셀 수준 이해를 평가하는 프로토콜을 제공하며, 시각 기반의 다단계 결정 태스크를 도입하고, MLLMs의 시각적 추론 능력을 탐색하는 데 도움을 줍니다. 우리 정책 개선 법 StaR와 PRM 가이드의 나무 검색 개선은 복잡한 분할 태스크에서 모델의 강건성을 향상시키고, MLLMs의 정확한 시각 인식과 다단계 결정 시스템의 미래 발전을 기반으로 합니다.",
      "upvotes": 18,
      "discussionId": "67d0fd76f8595b656f921ae8",
      "projectPage": "https://aim-uofa.github.io/SegAgent/",
      "githubRepo": "https://github.com/aim-uofa/SegAgent",
      "ai_keywords": [
        "Human-Like Mask Annotation Task (HLMAT)",
        "Markov Decision Process",
        "multi-step decision-making",
        "click points",
        "masks",
        "policy improvement method StaR",
        "PRM-guided tree search",
        "mask refinement",
        "annotation filtering",
        "fine-grained pixel understanding",
        "vision-centric task",
        "visual reasoning abilities"
      ]
    },
    "publishedAt": "2025-03-11T13:08:54.000Z",
    "title": "SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by\n  Imitating Human Annotator Trajectories",
    "summary": "While MLLMs have demonstrated adequate image understanding capabilities, they\nstill struggle with pixel-level comprehension, limiting their practical\napplications. Current evaluation tasks like VQA and visual grounding remain too\ncoarse to assess fine-grained pixel comprehension accurately. Though\nsegmentation is foundational for pixel-level understanding, existing methods\noften require MLLMs to generate implicit tokens, decoded through external pixel\ndecoders. This approach disrupts the MLLM's text output space, potentially\ncompromising language capabilities and reducing flexibility and extensibility,\nwhile failing to reflect the model's intrinsic pixel-level understanding.\n  Thus, we introduce the Human-Like Mask Annotation Task (HLMAT), a new\nparadigm where MLLMs mimic human annotators using interactive segmentation\ntools. Modeling segmentation as a multi-step Markov Decision Process, HLMAT\nenables MLLMs to iteratively generate text-based click points, achieving\nhigh-quality masks without architectural changes or implicit tokens. Through\nthis setup, we develop SegAgent, a model fine-tuned on human-like annotation\ntrajectories, which achieves performance comparable to state-of-the-art (SOTA)\nmethods and supports additional tasks like mask refinement and annotation\nfiltering.\n  HLMAT provides a protocol for assessing fine-grained pixel understanding in\nMLLMs and introduces a vision-centric, multi-step decision-making task that\nfacilitates exploration of MLLMs' visual reasoning abilities. Our adaptations\nof policy improvement method StaR and PRM-guided tree search further enhance\nmodel robustness in complex segmentation tasks, laying a foundation for future\nadvancements in fine-grained visual perception and multi-step decision-making\nfor MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08625.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07604",
      "authors": [
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b5",
          "name": "Tianhe Lin",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b6",
          "user": {
            "_id": "62d65139667051e0a29bffe7",
            "avatarUrl": "/avatars/0252aa2bcd4cf1c8e4b87e5f164b6da5.svg",
            "isPro": false,
            "fullname": "Jian Xie",
            "user": "hsaest",
            "type": "user"
          },
          "name": "Jian Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:42:36.765Z",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b7",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b8",
          "name": "Deqing Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:31.000Z",
      "title": "Transformer의 인풋 리지니징은, 스로트 스에 의한 추론입니다.",
      "summary": "테스트 시의 계산은 언어 모델의 복잡한 다단계 추론 능력을 향상시키는 새로운 패러다임으로 나타났습니다. OpenAI의 o1, o3 및 DeepSeek의 R1의 성공으로 명확히 밝혀졌습니다. 테스트 시의 계산에서의 명시적 추론에 비해, 은닉된 추론은 추론 효율이 높고, 생성되는 토큰 수가 적습니다. 그러나, 발전적인 추론 능력이 은닉된 추론의 스타일로 나타나는 이유는 불분명합니다. 본 논문에서는, GPT-2를 캄리어렛된 다단계 수학 추론 데이터 세트로 쉼에서 훈련시키고, 은닉된 추론이 다단계 작업에서 어떻게 작동하는지를 조사하는 분석적인 실험을 수행합니다. 우리의 발견은 다음과 같습니다: 1) 언어 모델은 고정 패턴 데이터로 훈련되어서, 단계별 추론을 수행하여 영역 내 및 영역 외 테스트에서 높은 정확도를 달성할 수 있습니다. 그러나, 이 능력은 고정 패턴 데이터에서만 나타납니다. 2) 반면, 불변 패턴 데이터로 훈련된 은닉된 추론 능력은 특정 패턴에 과적합되어 발전적인 적용에 실패합니다. 특히, 이 제한은 가장 선진한 큰 규모의 언어 모델에도 볼 수 있습니다. 이러한 발견은 언어 모델은 단축 학습을 통해 은닉된 추론을 얻으며, 유사한 패턴의 작업에 강한 성능을 얻으면서, 일반화 능력을 잃는 것을 보여줍니다.",
      "upvotes": 14,
      "discussionId": "67cfa4edd8cb8688d7d6d908",
      "githubRepo": "https://github.com/TianheL/LM-Implicit-Reasoning",
      "ai_keywords": [
        "test-time compute",
        "multi-step reasoning",
        "OpenAI's o1",
        "OpenAI's o3",
        "DeepSeek's R1",
        "implicit reasoning",
        "inference-efficient",
        "generated tokens",
        "explicit reasoning",
        "GPT-2",
        "multi-step mathematical reasoning dataset",
        "step-by-step reasoning",
        "in-domain tests",
        "out-of-domain tests",
        "fixed-pattern data",
        "unfixed-pattern data",
        "overfit",
        "generalization",
        "shortcut learning"
      ]
    },
    "publishedAt": "2025-03-10T13:58:31.000Z",
    "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
    "summary": "Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07604.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08605",
      "authors": [
        {
          "_id": "67d0ed0877b0c8ac3f304ef1",
          "name": "Subin Kim",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef2",
          "name": "Seoung Wug Oh",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef3",
          "name": "Jui-Hsien Wang",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef4",
          "name": "Joon-Young Lee",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef5",
          "name": "Jinwoo Shin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:43:45.000Z",
      "title": "동기 코피 샘플링을 위한 다イベント장 비디오 생성의 자유튜닝\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "summary": "최근의 텍스트에서 비디오의 확산 모델의 발전으로, 한 가지 Prompt에서 고품질의 짧은 비디오의 생성이 가능해지지만, 제한된 데이터량과 높은 계산 비용으로, 한 번의 패스만으로 현실 세계의 긴 비디오를 생성하는 것은 어려움입니다. 이에 대처하여 여러 연구는 무튜닝 접근 방식을 제안하고, 이미 존재하는 모델을 긴 비디오 생성에 확장시키고, 여러 Prompt를 사용하여 동적으로 제어 가능한 콘텐츠 변화를 허용하는 데 성공했습니다. 그러나 이러한 방법들은 주로 인접한 프레임의mooth한 이동을 보장하는 데 중점을 두고, 긴 시퀀스에서는 의미적인 일관성의 점차적인 손실로 인해 콘텐츠 유출이 주요 문제를 해결하는 데에 집중되어 있습니다. 이러한 문제를 해결하기 위해, 우리는 Synchronized Coupled Sampling (SynCoS)를 제안합니다. SynCoS는 전체 비디오의 확산 패스를 동기화하고, 인접한 것뿐만 아니라 멀리 떨어져 있는 프레임도 거리가 먼 일관성을 보장하는 새로운 추론 프레임입니다. 우리의 접근 방식은 역방향 샘플링과 최적화 기반 샘플링의 두 가지 보간된 샘플링 전략을 조합하고, 각에서 인접한 프레임의mooth한 이동을 보장하고, 전체적인 일관성을 강제합니다. 그러나 이러한 샘플링을 직접 교환하는 것은 확산 타일를 비대칭으로 만들고, Prompt Guide를 파괴하고, 독립적으로 동작하는 비예상적인 콘텐츠 변화를 불러일으키게 됩니다. 이를 해결하기 위해, SynCoS는 기본 시간 스텝과 고정 기반 노이즈를 사용하여 샘플링을 동기화하고, 완전한 샘플링을 대응시키고, 확산 타일을 대응시킵니다. 확장된 실험은 SynCoS가 긴 비디오의 생성을 크게 개선하고,mooth한 이동과 높은 거리의 일관성을 실현하고, 이전의 접근 방식을 양상적으로 뛰어넘는 것을 양상적으로 보여주었습니다.",
      "upvotes": 13,
      "discussionId": "67d0ed0b77b0c8ac3f304f7c",
      "projectPage": "https://syncos2025.github.io/",
      "githubRepo": "https://github.com/subin-kim-cv/SynCoS",
      "ai_keywords": [
        "text-to-video diffusion models",
        "high-quality short video generation",
        "long video generation",
        "tuning-free approaches",
        "multiple prompts",
        "dynamic content changes",
        "smooth transitions",
        "content drift",
        "semantic coherence",
        "Synchronized Coupled Sampling (SynCoS)",
        "denoising paths",
        "reverse sampling",
        "optimization-based sampling",
        "seamless local transitions",
        "global coherence",
        "grounded timestep",
        "fixed baseline noise",
        "multi-event long video generation",
        "long-range consistency"
      ]
    },
    "publishedAt": "2025-03-11T12:43:45.000Z",
    "title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling",
    "summary": "While recent advancements in text-to-video diffusion models enable\nhigh-quality short video generation from a single prompt, generating real-world\nlong videos in a single pass remains challenging due to limited data and high\ncomputational costs. To address this, several works propose tuning-free\napproaches, i.e., extending existing models for long video generation,\nspecifically using multiple prompts to allow for dynamic and controlled content\nchanges. However, these methods primarily focus on ensuring smooth transitions\nbetween adjacent frames, often leading to content drift and a gradual loss of\nsemantic coherence over longer sequences. To tackle such an issue, we propose\nSynchronized Coupled Sampling (SynCoS), a novel inference framework that\nsynchronizes denoising paths across the entire video, ensuring long-range\nconsistency across both adjacent and distant frames. Our approach combines two\ncomplementary sampling strategies: reverse and optimization-based sampling,\nwhich ensure seamless local transitions and enforce global coherence,\nrespectively. However, directly alternating between these samplings misaligns\ndenoising trajectories, disrupting prompt guidance and introducing unintended\ncontent changes as they operate independently. To resolve this, SynCoS\nsynchronizes them through a grounded timestep and a fixed baseline noise,\nensuring fully coupled sampling with aligned denoising paths. Extensive\nexperiments show that SynCoS significantly improves multi-event long video\ngeneration, achieving smoother transitions and superior long-range coherence,\noutperforming previous approaches both quantitatively and qualitatively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08605.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07891",
      "authors": [
        {
          "_id": "67d108c56bd6c57bab0b6f07",
          "name": "Jinhyuk Lee",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f08",
          "name": "Feiyang Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f09",
          "name": "Sahil Dua",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0a",
          "name": "Daniel Cer",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0b",
          "name": "Madhuri Shanbhogue",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0c",
          "name": "Iftekhar Naim",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0d",
          "name": "Gustavo Hernández Ábrego",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0e",
          "name": "Zhe Li",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0f",
          "name": "Kaifeng Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f10",
          "name": "Henrique Schechter Vera",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f11",
          "name": "Xiaoqi Ren",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f12",
          "name": "Shanfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f13",
          "name": "Daniel Salz",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f14",
          "name": "Michael Boratko",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f15",
          "name": "Jay Han",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f16",
          "name": "Blair Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f17",
          "name": "Shuo Huang",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f18",
          "name": "Vikram Rao",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f19",
          "name": "Paul Suganthan",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1a",
          "name": "Feng Han",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1b",
          "name": "Andreas Doumanoglou",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1c",
          "name": "Nithi Gupta",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1d",
          "name": "Fedor Moiseev",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1e",
          "name": "Cathy Yip",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1f",
          "name": "Aashi Jain",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f20",
          "name": "Simon Baumgartner",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f21",
          "name": "Shahrokh Shahi",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f22",
          "name": "Frank Palma Gomez",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f23",
          "name": "Sandeep Mariserla",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f24",
          "name": "Min Choi",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f25",
          "name": "Parashar Shah",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f26",
          "name": "Sonam Goenka",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f27",
          "name": "Ke Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f28",
          "name": "Ye Xia",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f29",
          "name": "Koert Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2a",
          "name": "Sai Meher Karthik Duddu",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2b",
          "name": "Yichang Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2c",
          "name": "Trevor Walker",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2d",
          "name": "Wenlei Zhou",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2e",
          "name": "Rakesh Ghiya",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2f",
          "name": "Zach Gleicher",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f30",
          "name": "Karan Gill",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f31",
          "name": "Zhe Dong",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f32",
          "name": "Mojtaba Seyedhosseini",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f33",
          "name": "Yunhsuan Sung",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f34",
          "name": "Raphael Hoffmann",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f35",
          "name": "Tom Duerig",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T22:16:45.000Z",
      "title": "Gemini Embedding: ジェミニ의 확장성 벡터화\n\n(Note: The original text \"Gemini Embedding: ジェミニの拡張性エンベディング\" was not in English. The translation provided is for the English text \"Gemini Embedding: Gemini's Extensibility Embedding\".)",
      "summary": "이 보고서에서는 Google의 가장 강력한 대언어 모델인 Gemini를 활용한 가장 선진적인 내장 모델 \"Gemini Embedding\"을 소개합니다. Gemini의 고유의 다언어 이해와 코드 이해 능력을 활용하여, 다언어와 다양한 문맥의 텍스트에 대해 높은 수준의 일반화를 가능한 내장을 생성합니다. Gemini Embedding이 생성한 표현은 분류, 유사도, 클러스터링, 순위, 검색 등 다양한 다운 스트리밍 작업에도 적용할 수 있습니다. Massive Multilingual Text Embedding Benchmark (MMTEB)에서 평가되었으며, 이 벤치마크는 250 이상의 언어에 대한 100 이상의 태스크를 포함하므로, Gemini Embedding은 이전의 가장 선진한 모델보다 크게 우월하며, 내장의 품질에 있어 상당한 향상을 나타냅니다. MMTEB의 다언어, 영어, 코드 벤치마크에서 가장 선진적인 성능을 달성하고, 우리의 통합 모델은 광범위한 선택의 태스크에 강한 능력을 보여주며, 영역专用 모델을 초월합니다.",
      "upvotes": 12,
      "discussionId": "67d108c66bd6c57bab0b6f6e",
      "ai_keywords": [
        "Gemini Embedding",
        "large language model",
        "multilingual",
        "code understanding",
        "representations",
        "downstream tasks",
        "classification",
        "similarity",
        "clustering",
        "ranking",
        "retrieval",
        "Massive Multilingual Text Embedding Benchmark (MMTEB)",
        "embedding quality",
        "specialized domain-specific models"
      ]
    },
    "publishedAt": "2025-03-10T18:16:45.000Z",
    "title": "Gemini Embedding: Generalizable Embeddings from Gemini",
    "summary": "In this report, we introduce Gemini Embedding, a state-of-the-art embedding\nmodel leveraging the power of Gemini, Google's most capable large language\nmodel. Capitalizing on Gemini's inherent multilingual and code understanding\ncapabilities, Gemini Embedding produces highly generalizable embeddings for\ntext spanning numerous languages and textual modalities. The representations\ngenerated by Gemini Embedding can be precomputed and applied to a variety of\ndownstream tasks including classification, similarity, clustering, ranking, and\nretrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark\n(MMTEB), which includes over one hundred tasks across 250+ languages, Gemini\nEmbedding substantially outperforms prior state-of-the-art models,\ndemonstrating considerable improvements in embedding quality. Achieving\nstate-of-the-art performance across MMTEB's multilingual, English, and code\nbenchmarks, our unified model demonstrates strong capabilities across a broad\nselection of tasks and surpasses specialized domain-specific models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07891.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08619",
      "authors": [
        {
          "_id": "67d0eb9cec69694dca382208",
          "name": "Xianfeng Wu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382209",
          "name": "Yajing Bai",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220a",
          "name": "Haoze Zheng",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220b",
          "name": "Harold Haodong Chen",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220c",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220d",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220e",
          "name": "Xuran Ma",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220f",
          "name": "Wen-Jie Shu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382210",
          "name": "Xianzu Wu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382211",
          "name": "Harry Yang",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382212",
          "name": "Ser-Nam Lim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:58:02.000Z",
      "title": "LightGen: 지식전달과 직접적인 취향 최적화를 통한 효율적인 이미지 생성",
      "summary": "최근의 이미지 생성 텍스트에서 이미지로 변환하는 기술의 발전은 주로 광범위한 데이터 세트와 Parameter-rich한 아키텍처에 의해 추진되어 왔습니다. 이러한 요구 사항은 계산 자원이 부족한 연구자나 실무자에게 접근성을 엄격하게 제한하고 있습니다. 본 논문에서는, Knowledge Distillation (KD)와 Direct Preference Optimization (DPO)를 활용한 효율적인 훈련 패러다임에 의해, LightGen을 소개합니다. LightGen은 광범위하게 채택되는 Multi-Type Large Language Model (MLLM)에서 성공적으로 적용되는 데이터 KD 기술의 성공을 모델로 삼아, 가장 최신 (SOTA)의 텍스트에서 이미지로 변환 모델의 지식이 Parameter 수가 적은 단순한 Masked Autoregressive Model (MAR) 아키텍처에 흡수됩니다. 2M의 고품질 이미지로 구성된 단순한 합성 데이터 세트를 사용하며, 이러한 데이터의 다양성이 모델의 성능을 결정하는 데 데이터량이보다 중요하다는 것을 보여주었습니다. 이 전략은 계산 자원의 요구 사항을 크게 줄이고, 사전 학습 시간을 数千 GPU 일에 88 GPU 일로 제한할 수 있습니다. 또한, 합성 데이터의 고유한 결점을 해결하기 위해 DPO 기술을 통합하여 이미지의 fidelity와 위치 정확도를 개선합니다. 세부적인 실험은, LightGen은 계산 자원의 줄이임과 계산 자원 제한 환경에서의 접근성 확장을 동반하여, SOTA 모델과 동일한 이미지 생성 품질을 달성한 것을 확인합니다. 코드는 https://github.com/XianfengWu01/LightGen에 공개되어 있습니다.",
      "upvotes": 11,
      "discussionId": "67d0eba3ec69694dca3823a0",
      "ai_keywords": [
        "knowledge distillation (KD)",
        "Direct Preference Optimization (DPO)",
        "Multi-Modal Large Language Models (MLLMs)",
        "Masked Autoregressive (MAR)",
        "synthetic dataset",
        "data diversity",
        "data volume",
        "model performance",
        "computational demands",
        "pre-training time",
        "synthetic data",
        "high-frequency details",
        "spatial inaccuracies",
        "image fidelity",
        "positional accuracy"
      ]
    },
    "publishedAt": "2025-03-11T12:58:02.000Z",
    "title": "LightGen: Efficient Image Generation through Knowledge Distillation and\n  Direct Preference Optimization",
    "summary": "Recent advances in text-to-image generation have primarily relied on\nextensive datasets and parameter-heavy architectures. These requirements\nseverely limit accessibility for researchers and practitioners who lack\nsubstantial computational resources. In this paper, we introduce \\model, an\nefficient training paradigm for image generation models that uses knowledge\ndistillation (KD) and Direct Preference Optimization (DPO). Drawing inspiration\nfrom the success of data KD techniques widely adopted in Multi-Modal Large\nLanguage Models (MLLMs), LightGen distills knowledge from state-of-the-art\n(SOTA) text-to-image models into a compact Masked Autoregressive (MAR)\narchitecture with only 0.7B parameters. Using a compact synthetic dataset of\njust 2M high-quality images generated from varied captions, we demonstrate\nthat data diversity significantly outweighs data volume in determining model\nperformance. This strategy dramatically reduces computational demands and\nreduces pre-training time from potentially thousands of GPU-days to merely 88\nGPU-days. Furthermore, to address the inherent shortcomings of synthetic data,\nparticularly poor high-frequency details and spatial inaccuracies, we integrate\nthe DPO technique that refines image fidelity and positional accuracy.\nComprehensive experiments confirm that LightGen achieves image generation\nquality comparable to SOTA models while significantly reducing computational\nresources and expanding accessibility for resource-constrained environments.\nCode is available at https://github.com/XianfengWu01/LightGen",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08619.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08686",
      "authors": [
        {
          "_id": "67d0f892a189f3978638e154",
          "name": "Jialv Zou",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e155",
          "name": "Bencheng Liao",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e156",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e157",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e158",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:46.000Z",
      "title": "OmniMamba: 상태 공간 모델을 기반으로 한 효율적인 유닛 모델에서 다모봇 이해와 생성\n\n(注意：虽然要求不添加额外文本，但为了确保翻译的准确性和专业性，这里提供了一个更自然的表达方式。)",
      "summary": "최근의 통합모노모ーダル 이해 및 시각화 생성(または모노모ーダル 생성) 모델의 발전은 계산복잡도의 오버플로우와 대규모 훈련 데이터의 의존관계로 제한되어 있습니다. 우리는 선형 아키텍처를 기반으로 한 모노모ーダル 생성 모델 \"OmniMamba\"를 소개합니다. 이 모델은 단일의 다음 토큰 예측 패러다임을 통해 텍스트와 이미지를 동시에 생성합니다. 이 모델은 Mamba-2의 높은 계산 및 메모리 효율을 최대한 활용하여 텍스트 생성에서 모노모ーダル 생성으로 확장할 수 있도록 합니다. 현재 통합 모델의 데이터 효율 문제 해결을 위해, 우리는 두 가지 핵심 아이디어를 제안합니다: (1) 특정 모듈 생성을 분리된 비오카시피오디디닝 모델을 가이드하고, (2) 태스크 시퀀스의 LoRA를 파라미터 효율적인 적용에 대해. 또한, 두 가지 태스크의 데이터 불균형을 완화하기 위해, 우리는 분리된 두 단계의 훈련 전략을 도입합니다. 이러한 기술에 따라, OmniMamba는 JanusFlow와 비교하여 상대적으로 경쟁적인 성능을 보였으며, Show-o의 데이터량의 1/1000으로도 데이터량을 2M의 이미지 텍스트 페어로 훈련하여 Show-o를 초월했습니다. 특히, OmniMamba는 Transformer 기반의 컴페나넌트와 비교하여, 119.2배의 속도업과 63%의 GPU 메모리 절감으로 추론 효율이 뛰어납니다. 코드와 모델은 https://github.com/hustvl/OmniMamba에서 공개되어 있습니다.",
      "upvotes": 9,
      "discussionId": "67d0f894a189f3978638e1b7",
      "ai_keywords": [
        "OmniMamba",
        "linear-architecture-based multimodal generation model",
        "unified next-token prediction paradigm",
        "Mamba-2",
        "computational efficiency",
        "memory efficiency",
        "decoupled vocabularies",
        "task-specific LoRA",
        "parameter-efficient adaptation",
        "decoupled two-stage training strategy",
        "data imbalance",
        "Show-o",
        "benchmark",
        "inference efficiency",
        "Transformer-based counterparts"
      ]
    },
    "publishedAt": "2025-03-11T13:59:46.000Z",
    "title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation\n  via State Space Models",
    "summary": "Recent advancements in unified multimodal understanding and visual generation\n(or multimodal generation) models have been hindered by their quadratic\ncomputational complexity and dependence on large-scale training data. We\npresent OmniMamba, the first linear-architecture-based multimodal generation\nmodel that generates both text and images through a unified next-token\nprediction paradigm. The model fully leverages Mamba-2's high computational and\nmemory efficiency, extending its capabilities from text generation to\nmultimodal generation. To address the data inefficiency of existing unified\nmodels, we propose two key innovations: (1) decoupled vocabularies to guide\nmodality-specific generation, and (2) task-specific LoRA for\nparameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage\ntraining strategy to mitigate data imbalance between two tasks. Equipped with\nthese techniques, OmniMamba achieves competitive performance with JanusFlow\nwhile surpassing Show-o across benchmarks, despite being trained on merely 2M\nimage-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba\nstands out with outstanding inference efficiency, achieving up to a 119.2 times\nspeedup and 63% GPU memory reduction for long-sequence generation compared to\nTransformer-based counterparts. Code and models are released at\nhttps://github.com/hustvl/OmniMamba",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08686.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07860",
      "authors": [
        {
          "_id": "67d0e915d0038007e5a75178",
          "user": {
            "_id": "650871aeb44445e9b3625c7b",
            "avatarUrl": "/avatars/c35bd3e4a851389a4b6898a5a51e2219.svg",
            "isPro": false,
            "fullname": "James Burgess",
            "user": "jmhb",
            "type": "user"
          },
          "name": "James Burgess",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:41:30.547Z",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a75179",
          "user": {
            "_id": "65703fab7f50602340d23704",
            "avatarUrl": "/avatars/324c45f5fba9cd8c38a89b30427c06b4.svg",
            "isPro": false,
            "fullname": "Xiaohan Wang",
            "user": "nicholswang",
            "type": "user"
          },
          "name": "Xiaohan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:40:56.564Z",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517a",
          "name": "Yuhui Zhang",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517b",
          "name": "Anita Rau",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517c",
          "name": "Alejandro Lozano",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517d",
          "name": "Lisa Dunlap",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517e",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517f",
          "name": "Serena Yeung-Levy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T21:18:32.000Z",
      "title": "비디오 액션 디퍼시언스",
      "summary": "같은 액션의 영상에서 2명이 행동하는 경우 어떤 차이를 보이는지 확인하는 방법은 무엇일까요? 본 논문에서는 같은 액션의 영상의 微妙한 차이를 인식하는 새로운 태스크 'Video Action Differencing (VidDiff)'를 소개합니다. 이 태스크는 코칭이나 스킬 학습 등 다양한 분야에 많은 응용이 있습니다. 이 새로운 태스크의 개발을 가능하게 하기 위해 먼저 VidDiffBench라는 벤치마크 데이터 세트를 만들었습니다. 이 데이터 세트는 549개의 영상 쌍을 포함하며, 4,469개의 세부화된 액션의 차이와 2,075개의 차이가 발생하는 시간 슬라이스의 인간 설명을 포함합니다. 우리의 실험은 최신의 대형 다모달 모델(LMMs)과 같은 GPT-4o와 Qwen2-VL에 VidDiffBench가 큰 도전을 제시하는 것을 보여줍니다. LMMs가 VidDiffBench에서 실패하는 경우를 분석하여 이 태스크에 대한 두 가지 중요한 도전을 밝혀줍니다: 두 영상과 관련된 서브 액션의 위치化和 세부화의 프레임 비교. 이러한 문제를 해결하기 위해 VidDiff 메소드를 제안합니다. 이 방법은 액션의 차이의 제안, 키 프레임의 위치화, 프레임의 비교의 3단계로 구성되며, 각 단계에 특화된 기본 모델을 사용합니다. 이 새로운 태스크의 향후 연구를 촉진하기 위해 벤치마크는 https://huggingface.co/datasets/jmhb/VidDiffBench, 코드는 http://jmhb0.github.io/viddiff에 공개됩니다.",
      "upvotes": 9,
      "discussionId": "67d0e917d0038007e5a751e9",
      "projectPage": "https://jmhb0.github.io/viddiff/",
      "githubRepo": "https://github.com/jmhb0/viddiff",
      "ai_keywords": [
        "Video Action Differencing (VidDiff)",
        "VidDiffBench",
        "multimodal models (LMMs)",
        "GPT-4o",
        "Qwen2-VL",
        "action difference proposal",
        "keyframe localization",
        "frame differencing",
        "agentic workflow",
        "fine-grained action differences",
        "localization timestamps"
      ]
    },
    "publishedAt": "2025-03-10T17:18:32.000Z",
    "title": "Video Action Differencing",
    "summary": "How do two individuals differ when performing the same action? In this work,\nwe introduce Video Action Differencing (VidDiff), the novel task of identifying\nsubtle differences between videos of the same action, which has many\napplications, such as coaching and skill learning. To enable development on\nthis new task, we first create VidDiffBench, a benchmark dataset containing 549\nvideo pairs, with human annotations of 4,469 fine-grained action differences\nand 2,075 localization timestamps indicating where these differences occur. Our\nexperiments demonstrate that VidDiffBench poses a significant challenge for\nstate-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL.\nBy analyzing failure cases of LMMs on VidDiffBench, we highlight two key\nchallenges for this task: localizing relevant sub-actions over two videos and\nfine-grained frame comparison. To overcome these, we propose the VidDiff\nmethod, an agentic workflow that breaks the task into three stages: action\ndifference proposal, keyframe localization, and frame differencing, each stage\nutilizing specialized foundation models. To encourage future research in this\nnew task, we release the benchmark at\nhttps://huggingface.co/datasets/jmhb/VidDiffBench and code at\nhttp://jmhb0.github.io/viddiff.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07860.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07572",
      "authors": [
        {
          "_id": "67d0e38171b6b577dbb8c72c",
          "user": {
            "_id": "6500bbf5e102da55f9ed43fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6500bbf5e102da55f9ed43fc/QZ6EAFV2CStFsILmTJw5D.jpeg",
            "isPro": true,
            "fullname": "Yuxiao Qu",
            "user": "CohenQu",
            "type": "user"
          },
          "name": "Yuxiao Qu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:41:33.926Z",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72d",
          "name": "Matthew Y. R. Yang",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72e",
          "name": "Amrith Setlur",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72f",
          "name": "Lewis Tunstall",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c730",
          "name": "Edward Emanuel Beeching",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c731",
          "name": "Ruslan Salakhutdinov",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c732",
          "name": "Aviral Kumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:40:43.000Z",
      "title": "테스트 시의 계산을 최적화하기 위한 메타 재보상 미세 조정",
      "summary": "테스트 시의 계산량을 효율적으로 사용하는 모델의 훈련은 LLM의 논리 성능 향상에 중요합니다. 현재의 방법은 주로 검색 트래스에서의 미세 조정이나 0/1 결과를 갖는 RL을 수행하는 것으로 이루어져 있습니다만, 이러한 접근 방식이 테스트 시의 계산량을 효율적으로 사용하고 있는지, 관리가 개선될 때 확장할 수 있는지는 아직 명확하지 않습니다. 본 논문에서는 이러한 질문에 대답하고자 합니다. 테스트 시의 계산량을 최적화하는 문제를 메타 재귀 학습(RL) 문제로 형식화하고, 이는 테스트 시의 계산량을 원리적으로 관찰할 수 있는 방식입니다. 이 관점에서 LLM에서 생성된 긴 출력 스트리밍을 테스트 시 실행한 수차례의 에피소드로 구성하여, 출력 토큰의 누적 손실을 사용하여 테스트 시의 계산량의 효율성을 평가할 수 있습니다. RL 알고리즘이 학습 중 최적의 탐색과 활용의 균형을 찾는 데 도움을 줄 수 있습니다. 누적 손실의 최소화는 토큰 스트리밍의 탐색과 활용의 균형을 제공하며, 최신 모델이 손실을 최소화하지 않는 것을 보여주고, 0/1 보상의 RL과 밀집 보상 보너스를 결합하여 이점을 실현할 수 있습니다. 이 보너스는 출력 스트리밍의 각 후속 블록에서 \"진보\"를 표현하고, 최종적인 성공 확률의 변화에 대해 정량화됩니다. 이러한 관점을 사용하여, 테스트 시의 계산량을 최적화하기 위한 새로운 클래스의 미세 조정 방법인 메타 재귀 학습 미세 조정(MRT)을 개발합니다. MRT는 결과 보상의 RL과 비교하여, 성능에 2-3배의 상대적인 효과를 제공하며, 수학 논리 토큰의 효율성에 약 1.5배의 효과를 제공합니다.",
      "upvotes": 8,
      "discussionId": "67d0e38271b6b577dbb8c7b7",
      "projectPage": "https://cohenqu.github.io/mrt.github.io/",
      "ai_keywords": [
        "meta-reinforcement learning (RL)",
        "cumulative regret",
        "token stream",
        "exploration and exploitation",
        "dense reward bonus",
        "likelihood of eventual success",
        "Meta Reinforcement Fine-Tuning (MRT)"
      ]
    },
    "publishedAt": "2025-03-10T13:40:43.000Z",
    "title": "Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning",
    "summary": "Training models to effectively use test-time compute is crucial for improving\nthe reasoning performance of LLMs. Current methods mostly do so via fine-tuning\non search traces or running RL with 0/1 outcome reward, but do these approaches\nefficiently utilize test-time compute? Would these approaches continue to scale\nas the budget improves? In this paper, we try to answer these questions. We\nformalize the problem of optimizing test-time compute as a meta-reinforcement\nlearning (RL) problem, which provides a principled perspective on spending\ntest-time compute. This perspective enables us to view the long output stream\nfrom the LLM as consisting of several episodes run at test time and leads us to\nuse a notion of cumulative regret over output tokens as a way to measure the\nefficacy of test-time compute. Akin to how RL algorithms can best tradeoff\nexploration and exploitation over training, minimizing cumulative regret would\nalso provide the best balance between exploration and exploitation in the token\nstream. While we show that state-of-the-art models do not minimize regret, one\ncan do so by maximizing a dense reward bonus in conjunction with the outcome\n0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in\nthe output stream, quantified by the change in the likelihood of eventual\nsuccess. Using these insights, we develop Meta Reinforcement Fine-Tuning, or\nMRT, a new class of fine-tuning methods for optimizing test-time compute. MRT\nleads to a 2-3x relative gain in performance and roughly a 1.5x gain in token\nefficiency for math reasoning compared to outcome-reward RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07572.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08588",
      "authors": [
        {
          "_id": "67d125362da8f91f8ef0412f",
          "user": {
            "_id": "6190ab805ca89a28e9f66873",
            "avatarUrl": "/avatars/a677a8401360be473895494e5fb267bb.svg",
            "isPro": false,
            "fullname": "Xin Xu",
            "user": "XinXuNLPer",
            "type": "user"
          },
          "name": "Xin Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:07.130Z",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04130",
          "name": "Wei Xu",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04131",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04132",
          "name": "Julian McAuley",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:25:36.000Z",
      "title": "BiasEdit: 편견을 제거하기 위한 편견을 훈련된 언어 모델을 수정하는 방법",
      "summary": "이전의 연구에서 언어 모델이 스케어 타입의 바이ア스를 보여주는 것을 명확히 확인했습니다. 현재의 장치 바이ア스 전략에서는, 카ンフェクト 팩터 데이터를 사용한 모델의 재학습, 표현 프로젝션, 프로ンプ팅 등이 바이ア스의 효율적인 제거 또는 모델의 편향된 내부 표현의 직접적인 변경에 실패합니다. 이러한 문제를 대처하기 위해, 우리는 스케어 타입의 바이ア스를 제거하기 위한 효율적인 모델 편집 방법인 BiasEdit를 제안합니다. BiasEdit는 가벼운 네트워크를 사용하여 모델 편집을 수행하는 에디터 네트워크를 사용하며, 편향을 제거하기 위해 파라미터 업데이트를 생성합니다. BiasEdit는 장치 바이ア스 손실을 사용하여 언어 모델링 능력을 유지하면서, 언어 모델의 일부 파라미터를 국소적으로 편집합니다. StereoSet과 Crows-Pairs의 실험에서, 바이ア스의 제거의 효과성, 효율성과 강건성을 보여주며, 접점 장치 바이ア스 베이스 라인과 비교하여 언어 모델의 일반적인 능력을 거의 영향을 미치지 않습니다. 또한, 바이ア스 트레이너를 수행하여 각 모듈의 바이ア스를 탐색하고, 언어 모델의 서로 다른 컴포넌트에 대한 바이ア스 편집의 영향을 조사했습니다.",
      "upvotes": 5,
      "discussionId": "67d125382da8f91f8ef041d0",
      "githubRepo": "https://github.com/zjunlp/BiasEdit",
      "ai_keywords": [
        "language models",
        "counterfactual data",
        "representation projection",
        "prompting",
        "BiasEdit",
        "parameter updates",
        "debiasing loss",
        "retention loss",
        "StereoSet",
        "Crows-Pairs",
        "language modeling abilities",
        "bias tracing"
      ]
    },
    "publishedAt": "2025-03-11T12:25:36.000Z",
    "title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing",
    "summary": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08588.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08689",
      "authors": [
        {
          "_id": "67d0f759cb5bf46c22ac8af1",
          "name": "Yongdong Luo",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af2",
          "name": "Wang Chen",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af3",
          "name": "Xiawu Zheng",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af4",
          "name": "Weizhong Huang",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af5",
          "name": "Shukang Yin",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af6",
          "name": "Haojia Lin",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af7",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af8",
          "name": "Jinfa Huang",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af9",
          "name": "Jiayi Ji",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8afa",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8afb",
          "name": "Rongrong Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:57.000Z",
      "title": "クエリー를 위한 토큰 분배 구조를 긴 비디오 이해에 기반한 컨텍스트 형식화로 실현합니다.",
      "summary": "최근의 긴 비디오 이해의 발전은 시각 토큰의 줄이기로 시각적 불필요한 정보의 감소를 통해 이루어지고 있습니다. 그러나 현재의 방법들은 후방 논리적인 낮은 반응 토큰의 줄이기를 디코더 레이어에서 사용하지만, 시각 토큰과 명령(쿼리)의 입력 수준의 의미적 관련성을 놓치고 있습니다. 본 논문에서는, 쿼리에 대한 프레임 수준의 중요도 평가에 기반한 시각 토큰 할당을 통해 기존의 대형 비디오 언어 모델(LVLMs)을 확장하기 위한 전방 논리적인 훈련 제한 모듈 \"QuoTA\"를 제안합니다. 쿼리에 대한 토큰 선택은 중요하며, 특정 태스크의 요구에 맞게 시각 처리를 수행하여 토큰 버킷의 사용을 최적화하고 의미적으로 관련 있는 내용을 보존합니다. 특히, (i) QuoTA는 쿼리 관련성을 기반으로 프레임 수준의 중요도 점수를 전략적으로 할당하여 디코더 레이어의 크로스 모드 상호작용 전에 한 번의 시각 토큰 할당을 가능하게 합니다, (ii) 쿼리를 Chain-of-Thoughts 추론에 의해 분리하고 LVLM을 통해 프레임 중요도 점수를 더 정확하게 평가하도록 촉진합니다, (iii) QuoTA는 기존의 LVLM을 확장하기 위한 포트와 파이프 기능을 제공합니다. 확장된 실험 결과를 통해 LLaVA-Video-7B과 결합하여 6개의 벤치마크(Video-MME와 MLVU를 포함)에서 평균적인 성능 향상률이 3.2%가 되며, 기준과 같은 시각 토큰 버킷을 사용합니다. 코드는 https://github.com/MAC-AutoML/QuoTA에 공개되어 있습니다.",
      "upvotes": 4,
      "discussionId": "67d0f75bcb5bf46c22ac8b70",
      "githubRepo": "https://github.com/MAC-AutoML/QuoTA",
      "ai_keywords": [
        "QuoTA",
        "ante-hoc",
        "training-free",
        "modular",
        "long video understanding",
        "visual token pruning",
        "attention distribution",
        "decoder layers",
        "input-level semantic correlation",
        "visual tokens",
        "instructions",
        "query",
        "frame-level importance assessment",
        "task-specific requirements",
        "token budget utilization",
        "semantically relevant content",
        "Chain-of-Thoughts reasoning",
        "cross-modal interactions",
        "plug-and-play functionality",
        "LLaVA-Video-7B",
        "Video-MME",
        "MLVU"
      ]
    },
    "publishedAt": "2025-03-11T13:59:57.000Z",
    "title": "QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long\n  Video Comprehension",
    "summary": "Recent advances in long video understanding typically mitigate visual\nredundancy through visual token pruning based on attention distribution.\nHowever, while existing methods employ post-hoc low-response token pruning in\ndecoder layers, they overlook the input-level semantic correlation between\nvisual tokens and instructions (query). In this paper, we propose QuoTA, an\nante-hoc training-free modular that extends existing large video-language\nmodels (LVLMs) for visual token assignment based on query-oriented frame-level\nimportance assessment. The query-oriented token selection is crucial as it\naligns visual processing with task-specific requirements, optimizing token\nbudget utilization while preserving semantically relevant content.\nSpecifically, (i) QuoTA strategically allocates frame-level importance scores\nbased on query relevance, enabling one-time visual token assignment before\ncross-modal interactions in decoder layers, (ii) we decouple the query through\nChain-of-Thoughts reasoning to facilitate more precise LVLM-based frame\nimportance scoring, and (iii) QuoTA offers a plug-and-play functionality that\nextends to existing LVLMs. Extensive experimental results demonstrate that\nimplementing QuoTA with LLaVA-Video-7B yields an average performance\nimprovement of 3.2% across six benchmarks (including Video-MME and MLVU) while\noperating within an identical visual token budget as the baseline. Codes are\nopen-sourced at https://github.com/MAC-AutoML/QuoTA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08689.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08685",
      "authors": [
        {
          "_id": "67d0f7032eaba9be7bf76e0e",
          "user": {
            "_id": "63483629ac5172169929da0e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665676793089-noauth.jpeg",
            "isPro": false,
            "fullname": "Xin Wen",
            "user": "xwen99",
            "type": "user"
          },
          "name": "Xin Wen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:39:00.455Z",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e0f",
          "user": {
            "_id": "62dcd71075e9787ec5aa41ba",
            "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
            "isPro": true,
            "fullname": "Bingchen Zhao",
            "user": "tennant",
            "type": "user"
          },
          "name": "Bingchen Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:56.945Z",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e10",
          "name": "Ismail Elezi",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e11",
          "name": "Jiankang Deng",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e12",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:41.000Z",
      "title": "\"주성분 분석\"는 이미지의 새로운 언어를 가능하게 합니다.",
      "summary": "우리는 새로운 시각 토큰화 프레임워크를 소개합니다. 이 프레임워크는 증명 가능한 PCA와 유사한 구조를 잠재 토큰 공간에 삽입합니다. 기존 시각 토큰화는 주로 재구성 정확도를 최적화하지만, 잠재 공간의 구조적 특성을 자주 무시합니다 - 이는 해석성과 하류 작업에서 중요한 요소입니다. 우리의 방법은 이미지에 대해 1차원 causal 토큰 시퀀스를 생성합니다. 각 연속 토큰은 수학적으로 보장되는 감소하는 설명된 변동성을 가짐으로써 비겹치는 정보를 제공합니다. 이는 주성분 분석과 유사합니다. 이 구조적 제약은 토큰이 가장 salient 시각 특징을 먼저 추출하고, 각 다음 토큰은 점진적으로 감소하지만 보완적인 정보를 추가합니다. 또한, 우리는 확산 디코더를 사용하여 고수준의 семанти적 콘텐츠와 저수준의 스펙트럴 디테일이 토큰에서 불필요하게 혼잡되어 있는 세ман틱 스펙트럼 결합 효과에 대해 파악하고 해결했습니다. 실험은 우리의 접근法是 현재의 최고 수준의 재구성 성능을 달성하며, 인간 시각 시스템과 더 잘 일치하는 해석성을 향상시켰음을 보여주고 있습니다. 또한, 우리의 토큰 시퀀스를 사용하여 훈련 및 추론을 수행하는 자동 회귀 모델은 현재의 최고 수준의 성능을 달성하지만 더 적은 토큰을 필요로 합니다.",
      "upvotes": 4,
      "discussionId": "67d0f7052eaba9be7bf76eac",
      "projectPage": "https://visual-gen.github.io/semanticist/",
      "githubRepo": "https://github.com/visual-gen/semanticist",
      "ai_keywords": [
        "visual tokenization framework",
        "PCA-like structure",
        "latent token space",
        "reconstruction fidelity",
        "structural properties",
        "interpretabiliy",
        "1D causal token sequence",
        "explained variance",
        "principal component analysis",
        "salient visual features",
        "semantic-spectrum coupling effect",
        "diffusion decoder",
        "reconstruction performance",
        "human vision system",
        "auto-regressive models",
        "state-of-the-art methods"
      ]
    },
    "publishedAt": "2025-03-11T13:59:41.000Z",
    "title": "\"Principal Components\" Enable A New Language of Images",
    "summary": "We introduce a novel visual tokenization framework that embeds a provable\nPCA-like structure into the latent token space. While existing visual\ntokenizers primarily optimize for reconstruction fidelity, they often neglect\nthe structural properties of the latent space -- a critical factor for both\ninterpretability and downstream tasks. Our method generates a 1D causal token\nsequence for images, where each successive token contributes non-overlapping\ninformation with mathematically guaranteed decreasing explained variance,\nanalogous to principal component analysis. This structural constraint ensures\nthe tokenizer extracts the most salient visual features first, with each\nsubsequent token adding diminishing yet complementary information.\nAdditionally, we identified and resolved a semantic-spectrum coupling effect\nthat causes the unwanted entanglement of high-level semantic content and\nlow-level spectral details in the tokens by leveraging a diffusion decoder.\nExperiments demonstrate that our approach achieves state-of-the-art\nreconstruction performance and enables better interpretability to align with\nthe human vision system. Moreover, auto-regressive models trained on our token\nsequences achieve performance comparable to current state-of-the-art methods\nwhile requiring fewer tokens for training and inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08685.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07699",
      "authors": [
        {
          "_id": "67d114912264403cbf39d0ba",
          "name": "Huiyang Shao",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bb",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bc",
          "name": "Yuhong Yang",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bd",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0be",
          "name": "Xing Wang",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bf",
          "name": "Xuefeng Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:20:52.000Z",
      "title": "RayFlow: 인스턴스 정보를 인식하여 분산 처리를 가속화하는 적응적인 폼 트래픽 처리기법",
      "summary": "Diffusion 모델은 다양한 분야에서 놀라울 정도로 성공을 거뒀습니다. 그러나 생성 속도의 느린 점이 중요한 문제로 남아 있습니다. 현재의 가속화 방법들은 샘플의 품질과 제어성을 잃거나 훈련의 복잡성을 도입하게 됩니다. 이에 따라 RayFlow라는 새로운 Diffusion 프레임워크를 제안합니다. RayFlow는 이전의 방법과 달리 각 샘플을 인스턴스 고유의 목표 분포에 대한 다른 경로로 안내합니다. 이 방법은 생성 다양성과 안정성을 유지하면서 샘플링 단계를 최소화합니다. 또한 Time Sampler라는 중요한 샘플링 방법도 소개합니다. 이 방법은 훈련 효율성을 향상시키기 위해 중요한 시간 단계에 집중하는 데 목적이 있습니다. 확장된 실험은 RayFlow가 기존의 가속화 방법과 비교하여, 높은 품질의 이미지 생성을 위해 속도, 제어성, 훈련 효율성을 향상시키는 것을 보여주고 있습니다.",
      "upvotes": 3,
      "discussionId": "67d114922264403cbf39d0f8",
      "ai_keywords": [
        "diffusion models",
        "generation speed",
        "RayFlow",
        "instance-specific target distribution",
        "sampling steps",
        "generation diversity",
        "stability",
        "Time Sampler",
        "importance sampling",
        "training efficiency",
        "high-quality images"
      ]
    },
    "publishedAt": "2025-03-10T13:20:52.000Z",
    "title": "RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow\n  Trajectories",
    "summary": "Diffusion models have achieved remarkable success across various domains.\nHowever, their slow generation speed remains a critical challenge. Existing\nacceleration methods, while aiming to reduce steps, often compromise sample\nquality, controllability, or introduce training complexities. Therefore, we\npropose RayFlow, a novel diffusion framework that addresses these limitations.\nUnlike previous methods, RayFlow guides each sample along a unique path towards\nan instance-specific target distribution. This method minimizes sampling steps\nwhile preserving generation diversity and stability. Furthermore, we introduce\nTime Sampler, an importance sampling technique to enhance training efficiency\nby focusing on crucial timesteps. Extensive experiments demonstrate RayFlow's\nsuperiority in generating high-quality images with improved speed, control, and\ntraining efficiency compared to existing acceleration techniques.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07699.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18858",
      "authors": [
        {
          "_id": "67d1080b2264403cbf36b0ad",
          "name": "Jingtao Zhan",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0ae",
          "name": "Jiahao Zhao",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0af",
          "name": "Jiayu Li",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b0",
          "name": "Yiqun Liu",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b1",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b2",
          "name": "Qingyao Ai",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b3",
          "name": "Jiaxin Mao",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b4",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b5",
          "name": "Min Zhang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b6",
          "name": "Shaoping Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T05:59:45.000Z",
      "title": "시험과 오류를 통해 지능 평가",
      "summary": "知能는 종족을 제한한 오류를 반복하여 해결책을 찾는 중요한 특성입니다. 이 개념에 기반하여, 우리는 실패 횟수를 기반으로 知能를 평가하기 위한 쉼빌버버게임(Survival Game)을 제안합니다. 실패 횟수가 적다는 것은 높은 知能在을 나타냅니다. 실패 횟수의 기대값과 분산이 제한되어 있으며, 이는 새로운 문제를 지속적으로 해결할 수 있는 능력을 나타내며, 이를 자율레벨(Autonomous Level)로 정의합니다. 쉼빌버버게임을 사용하여, 우리는 현재의 AI 시스템을 구조적으로 평가합니다. 결과적으로, 간단한 작업에서 AI 시스템은 자율레벨을 달성하지만, 시각, 검색, 리카메다리오, 언어와 같은 복잡한 작업에서는 아직 멀리 떨어져 있습니다. 현재의 AI 기술의 확장은 도움이 될 수 있지만, 이는 천문학적인 비용과 함께 필요합니다. 일반적인 작업에서 자율레벨을 달성하는 예측은 10^26 파라미터를 필요로 합니다. 이 것을 보아, 이러한 큰 모델을 읽어들이는 데 H100 GPU의 수가 너무 많으며, 총 가격은 Apple Inc.의 시장가치의 10^7배에 달합니다. 므어의 법칙(Moore's Law)에서도, 이러한 파라미터 규모를 지원하는 데 70년이 걸립니다. 이러한 큰 비용은, 인간의 작업의 복잡성과 현재의 AI 기술의 부족함을 명확히 합니다. 이러한 현상을 한 단계 더 조사하기 위해, 우리는 쉼빌버버게임의 이론적 분석과 실험 결과를 수행합니다. 우리의 발견은, 인간의 작업은 키리티티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피티리티피",
      "upvotes": 3,
      "discussionId": "67d108112264403cbf36b1e9",
      "githubRepo": "https://github.com/jingtaozhan/IntelligenceTest"
    },
    "publishedAt": "2025-02-26T00:59:45.000Z",
    "title": "Evaluating Intelligence via Trial and Error",
    "summary": "Intelligence is a crucial trait for species to find solutions within a\nlimited number of trial-and-error attempts. Building on this idea, we introduce\nSurvival Game as a framework to evaluate intelligence based on the number of\nfailed attempts in a trial-and-error process. Fewer failures indicate higher\nintelligence. When the expectation and variance of failure counts are both\nfinite, it signals the ability to consistently find solutions to new\nchallenges, which we define as the Autonomous Level of intelligence. Using\nSurvival Game, we comprehensively evaluate existing AI systems. Our results\nshow that while AI systems achieve the Autonomous Level in simple tasks, they\nare still far from it in more complex tasks, such as vision, search,\nrecommendation, and language. While scaling current AI technologies might help,\nthis would come at an astronomical cost. Projections suggest that achieving the\nAutonomous Level for general tasks would require 10^{26} parameters. To put\nthis into perspective, loading such a massive model requires so many H100 GPUs\nthat their total value is 10^{7} times that of Apple Inc.'s market value.\nEven with Moore's Law, supporting such a parameter scale would take 70 years.\nThis staggering cost highlights the complexity of human tasks and the\ninadequacies of current AI technologies. To further investigate this\nphenomenon, we conduct a theoretical analysis of Survival Game and its\nexperimental results. Our findings suggest that human tasks possess a\ncriticality property. As a result, Autonomous Level requires a deep\nunderstanding of the task's underlying mechanisms. Current AI systems, however,\ndo not fully grasp these mechanisms and instead rely on superficial mimicry,\nmaking it difficult for them to reach an autonomous level. We believe Survival\nGame can not only guide the future development of AI but also offer profound\ninsights into human intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18858.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07639",
      "authors": [
        {
          "_id": "67d0e3ede3afecf451915d0a",
          "name": "Xingyi Yang",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0b",
          "name": "Constantin Venhoff",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0c",
          "name": "Ashkan Khakzar",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0d",
          "name": "Christian Schroeder de Witt",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0e",
          "name": "Puneet K. Dokania",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0f",
          "name": "Adel Bibi",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d10",
          "name": "Philip Torr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T17:40:54.000Z",
      "title": "Mixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable\n\nMixture of Experts Made Intrinsically Interpretable",
      "summary": "뉴로ン은 대규모의 언어 모델에서 다중의 의미를 나타내며, 무관한 개념을 동시에 인코딩하여 해석성을 숨기는 경우가 많습니다. 후처리 방식에 의존하지 않고, 우리는 고유의 해석성을 가지는 Mixture-of-Experts(MoE) 언어 모델을 제안합니다. 우리의 접근법은 언어 모델에서 희소한 활성을 가진 광범위한 네트워크가 해석적인 원인을 쉽게 감지하는 것을 관찰한 이후, 이를 기반으로 합니다. 그러나 이러한 큰 희소한 네트워크를 직접 훈련하는 것은 계산적으로 금지되어 있습니다. MoE 아키텍처는 주어진 입력에 대해 일부의 익스플에이터(Expert)를 활성화시켜 Scalable한 대체로 되어, 고유의 해석성의 목표와 일치합니다. MoE-X에서는 MoE 레이어를 등가한 희소한 큰 MLP로 대체하여 이 연계를 확립합니다. 이 접근법은 은닉 크기의 효율적인 스케일링을 가능하게 하며, 희소성을 유지할 수 있습니다. 또한 해석성을 향상시키기 위해, 각 익스플에이터(Expert) 내에서 희소한 활성을 강제하고, 루팅 구조를 재설계하고, 가장 높은 활성 희소성을 가진 익스플에이터(Expert)를 우선시하는 것입니다. 이는 처리가 이루어지는 것은 가장 명확한 특징들만입니다. MoE-X는 체스 및 자연어 처리 태스크에서 평가되었으며, 그 성능은 밀한 모델과 비교하여도相当하며, 해석성을 크게 향상시킵니다. MoE-X는 GPT-2보다 퍼뮤라티가 좋으며, 해석성은 희소한 자동 인코더(SAE) 기반의 접근법보다도 뛰어납니다.",
      "upvotes": 2,
      "discussionId": "67d0e3f0e3afecf451915dfa",
      "ai_keywords": [
        "polysemanticity",
        "Mixture-of-Experts (MoE)",
        "interpretable",
        "sparse activations",
        "sparsity",
        "sparse networks",
        "hidden size",
        "sparse activation",
        "routing mechanism",
        "salient features",
        "perplexity",
        "GPT-2",
        "sparse autoencoder (SAE)"
      ]
    },
    "publishedAt": "2025-03-05T12:40:54.000Z",
    "title": "Mixture of Experts Made Intrinsically Interpretable",
    "summary": "Neurons in large language models often exhibit polysemanticity,\nsimultaneously encoding multiple unrelated concepts and obscuring\ninterpretability. Instead of relying on post-hoc methods, we present\nMoE-X, a Mixture-of-Experts (MoE) language model designed to be\nintrinsically interpretable. Our approach is motivated by the\nobservation that, in language models, wider networks with sparse activations\nare more likely to capture interpretable factors. However, directly training\nsuch large sparse networks is computationally prohibitive. MoE architectures\noffer a scalable alternative by activating only a subset of experts for any\ngiven input, inherently aligning with interpretability objectives. In MoE-X, we\nestablish this connection by rewriting the MoE layer as an equivalent sparse,\nlarge MLP. This approach enables efficient scaling of the hidden size while\nmaintaining sparsity. To further enhance interpretability, we enforce sparse\nactivation within each expert and redesign the routing mechanism to prioritize\nexperts with the highest activation sparsity. These designs ensure that only\nthe most salient features are routed and processed by the experts. We evaluate\nMoE-X on chess and natural language tasks, showing that it achieves performance\ncomparable to dense models while significantly improving interpretability.\nMoE-X achieves a perplexity better than GPT-2, with interpretability surpassing\neven sparse autoencoder (SAE)-based approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07639.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08507",
      "authors": [
        {
          "_id": "67d15293e3afecf451aceab7",
          "name": "Qing Jiang",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceab8",
          "name": "Lin Wu",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceab9",
          "name": "Zhaoyang Zeng",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceaba",
          "name": "Tianhe Ren",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabb",
          "name": "Yuda Xiong",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabc",
          "name": "Yihao Chen",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabd",
          "name": "Qin Liu",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabe",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T14:57:14.000Z",
      "title": "「누군가 상관없이」",
      "summary": "인간은 컴퓨터 비전의 가장 중요한 참여자이며, 자연언어의 설명에서 특정 개인을 감지하는 능력으로 정의된 작업은 실용적인 가치를 지닌다. 그러나 현재의 모델은 일반적으로 현실적인 사용 가능성을 달성하지 못하고 있으며, 현재의 벤치마크는 일대일의 기준을 중심으로 발전을 방해하고 있다. 본 논문에서는 이 작업의 작업 정의, 데이터셋 설계, 모델 아키텍처의 3가지 중요한 측면에서 재평가를 수행했다. 먼저, 기준이 가능한 엔티티의 5가지 면과 이 작업의 3가지 특성을 특정하고, 다음으로 HumanRef라는 새로운 데이터셋을 통해 이러한 문제를 극복하고 현실적인 애플리케이션을 더 잘 모방하는 것을 목표로 했다. 모델 설계의 관점에서, 다 모델 대 언어 모델과 물체 감지 프레임워크를 통합하여 강력한 기준 모델인 RexSeek을 구축했다. 실험 결과를 통해 일반적인 벤치마크(RefCOCO/+/g)에서 잘表现하는 가장 先端 모델은 HumanRef에서 다수의 개인을 감지하지 못하여 어려움을 보였다. 반면, RexSeek은 인간의 기준에도 뛰어난 성능을 보인 반면, 일반적인 물체의 기준에도 효과적으로 확장할 수 있으며 다양한 관측 작업에 광범위하게 적용할 수 있다. 코드는 https://github.com/IDEA-Research/RexSeek에 공개되어 있다.",
      "upvotes": 1,
      "discussionId": "67d15294e3afecf451aceb29",
      "ai_keywords": [
        "referable entities",
        "multimodal large language model",
        "object detection framework",
        "HumanRef",
        "RexSeek",
        "RefCOCO/+/g"
      ]
    },
    "publishedAt": "2025-03-11T10:57:14.000Z",
    "title": "Referring to Any Person",
    "summary": "Humans are undoubtedly the most important participants in computer vision,\nand the ability to detect any individual given a natural language description,\na task we define as referring to any person, holds substantial practical value.\nHowever, we find that existing models generally fail to achieve real-world\nusability, and current benchmarks are limited by their focus on one-to-one\nreferring, that hinder progress in this area. In this work, we revisit this\ntask from three critical perspectives: task definition, dataset design, and\nmodel architecture. We first identify five aspects of referable entities and\nthree distinctive characteristics of this task. Next, we introduce HumanRef, a\nnovel dataset designed to tackle these challenges and better reflect real-world\napplications. From a model design perspective, we integrate a multimodal large\nlanguage model with an object detection framework, constructing a robust\nreferring model named RexSeek. Experimental results reveal that\nstate-of-the-art models, which perform well on commonly used benchmarks like\nRefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple\nindividuals. In contrast, RexSeek not only excels in human referring but also\ngeneralizes effectively to common object referring, making it broadly\napplicable across various perception tasks. Code is available at\nhttps://github.com/IDEA-Research/RexSeek",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08507.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08478",
      "authors": [
        {
          "_id": "67d12d0b44be28339053b965",
          "user": {
            "_id": "64a3eb280111d5ff6c4849fd",
            "avatarUrl": "/avatars/3a9000393b8d200418bae5fe7d902e4d.svg",
            "isPro": false,
            "fullname": "Han-Wei Kung",
            "user": "hkung",
            "type": "user"
          },
          "name": "Han-Wei Kung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:50:08.946Z",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b966",
          "name": "Tuomas Varanka",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b967",
          "name": "Terence Sim",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b968",
          "name": "Nicu Sebe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T14:29:37.000Z",
      "title": "NullFace: 훈련 없는 국부적인 얼굴의 비명화",
      "summary": "카메라의 수가 계속 증가함에 따라, 오늘날의 디지털 시대에 대한 프라이버시 걱정이 증가하고 있습니다. 기존의 비명화 방법들은 개인의 정보를 숨길 수 있지만, 이미지의 유용성을 유지하는 것은 어렵고, 이를 해결하기 위해 본 논문에서는 얼굴을 비명화하면서 관련없는 키의 속성을 유지하는 훈련없이 적용할 수 있는 방법을 소개합니다. 우리의 접근법은 최적화 및 훈련이 필요하지 않은 사전 학습된 텍스트로부터 이미지로 확장된 모델을 사용합니다. 처음으로, 입력된 이미지에 대해 역산하여 초기의 노이즈를 복원합니다. 그 후, 키의 속성을 유지하기 위해 은닉 노이즈를 축소하고, 은닉 노이즈를 변경한 얼굴의 은닉 노이즈를 사용하여 은닉 노이즈를 축소합니다. 우리의 접근법은 얼굴 영역을 선택적으로 비명화하는 것을 지원하며, 사용자가 얼굴 영역을 비명화하거나 유지할 수 있음을 제어할 수 있습니다. 최신의 방법과 비교하여, 우리의 접근법은 비명화, 키의 속성 유지, 이미지 품질에 뛰어납니다. 그 유연성, 강건성, 실용적인 특성에 따라, 현실적인 응용 분야에 적합합니다. 코드와 데이터는 https://github.com/hanweikung/nullface에 찾을 수 있습니다.",
      "upvotes": 1,
      "discussionId": "67d12d1044be28339053baab",
      "ai_keywords": [
        "text-to-image diffusion model",
        "identity-conditioned diffusion",
        "identity embeddings",
        "localized anonymization"
      ]
    },
    "publishedAt": "2025-03-11T10:29:37.000Z",
    "title": "NullFace: Training-Free Localized Face Anonymization",
    "summary": "Privacy concerns around ever increasing number of cameras are increasing in\ntoday's digital age. Although existing anonymization methods are able to\nobscure identity information, they often struggle to preserve the utility of\nthe images. In this work, we introduce a training-free method for face\nanonymization that preserves key non-identity-related attributes. Our approach\nutilizes a pre-trained text-to-image diffusion model without requiring\noptimization or training. It begins by inverting the input image to recover its\ninitial noise. The noise is then denoised through an identity-conditioned\ndiffusion process, where modified identity embeddings ensure the anonymized\nface is distinct from the original identity. Our approach also supports\nlocalized anonymization, giving users control over which facial regions are\nanonymized or kept intact. Comprehensive evaluations against state-of-the-art\nmethods show our approach excels in anonymization, attribute preservation, and\nimage quality. Its flexibility, robustness, and practicality make it\nwell-suited for real-world applications. Code and data can be found at\nhttps://github.com/hanweikung/nullface .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08478.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08307",
      "authors": [
        {
          "_id": "67d140378cb4592900a1a75e",
          "user": {
            "_id": "66895b3d41fcf83c026b5dca",
            "avatarUrl": "/avatars/f1104041ee3445024f05d5d0b4d1550b.svg",
            "isPro": false,
            "fullname": "Alex Ergasti",
            "user": "MaverickAlex",
            "type": "user"
          },
          "name": "Alex Ergasti",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:03.978Z",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a75f",
          "name": "Giuseppe Gabriele Tarollo",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a760",
          "name": "Filippo Botti",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a761",
          "name": "Tomaso Fontanini",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a762",
          "name": "Claudio Ferrari",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a763",
          "name": "Massimo Bertozzi",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a764",
          "name": "Andrea Prati",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T11:18:47.000Z",
      "title": "^RFLAV: 무한의 음성 비디오 생성에 대한 로링 플로우 매칭",
      "summary": "AV 생성은 생성 AI에서 여전히 중요한 문제로, 주로 세 가지 중요한 요구 사항에 의해 복잡해지고 있습니다: 생성된 샘플의 품질, 무관한 모티브 동기화와 시간적 일관성, 그리고 음성 트랙이 시각 데이터에 일치하거나 반대로도 일치하거나, 또는 긴 비디오 시간 동안 제한없이 일치해야 합니다. 본 논문에서는, 모든 AV 생성의 중요한 문제를 해결하기 위한 새로운 transformer 기반의 아키텍처를 소개합니다. 세 가지 다른 크로스 모듈 상호작용 모듈을 검토하고, 그 가벼운 시간적 기능 모듈이 음성과 시각 모듈의 배치에 가장 효과적이고 계산적으로 효율적인 접근으로 평가되어 있습니다. 실험 결과를 통해, 이는 현재의 가장 선진한 모델을 초과하는 다 모듈 AV 생성 태스크에서 가장 뛰어난 것입니다. 코드와 체크포인트는 https://github.com/ErgastiAlex/R-FLAV에서 사용 가능합니다.",
      "upvotes": 1,
      "discussionId": "67d1403b8cb4592900a1a868",
      "githubRepo": "https://github.com/ErgastiAlex/R-FLAV",
      "ai_keywords": [
        "transformer-based architecture",
        "cross modality interaction modules",
        "lightweight temporal fusion module",
        "audio and visual modalities",
        "multimodal AV generation tasks"
      ]
    },
    "publishedAt": "2025-03-11T07:18:47.000Z",
    "title": "^RFLAV: Rolling Flow matching for infinite Audio Video generation",
    "summary": "Joint audio-video (AV) generation is still a significant challenge in\ngenerative AI, primarily due to three critical requirements: quality of the\ngenerated samples, seamless multimodal synchronization and temporal coherence,\nwith audio tracks that match the visual data and vice versa, and limitless\nvideo duration. In this paper, we present , a novel transformer-based\narchitecture that addresses all the key challenges of AV generation. We explore\nthree distinct cross modality interaction modules, with our lightweight\ntemporal fusion module emerging as the most effective and computationally\nefficient approach for aligning audio and visual modalities. Our experimental\nresults demonstrate that  outperforms existing state-of-the-art models\nin multimodal AV generation tasks. Our code and checkpoints are available at\nhttps://github.com/ErgastiAlex/R-FLAV.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08307.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08102",
      "authors": [
        {
          "_id": "67d12c32428a3d8d5281f310",
          "name": "Jiale Wei",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f311",
          "name": "Xiang Ying",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f312",
          "name": "Tao Gao",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f313",
          "name": "Felix Tao",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f314",
          "name": "Jingbo Shang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T07:05:52.000Z",
      "title": "AI-native Memory 2.0: Second Me\n\nAI-native Memory 2.0: 두 번째 메",
      "summary": "인간은 외부 세계와 상호작용을 통해 개인적인 기억을 교환합니다. 이 상호작용은 다른 개인, 웹 사이트, 앱, 그리고 미래의 AI 에이전트와의 상호작용에 포함됩니다. 이러한 상호작용의 일부는 사용자가 다른 맥락에서 같은 정보를 반복적으로 제공해야 하며, 불필요한 정보로 인해 복잡해지는 현상입니다. 현재의 해결책으로는 브라우저에 저장된 인증 정보, 자동완성 구조, 또는 통합 인증 시스템이 사용자의 데이터를 저장하고 추출하는 기능을 제공하여 불필요한 정보를 줄이는 데 도움을 줍니다. 대규모 언어 모델(LLM)의 발전은 AI 본원 패러다임에서 메모리 관리를 재설정하는 기회를 제공하며, SECOND ME가 이러한 기회를 활용하여 메모리 관리에 더 체계적이고 지능적인 접근을 촉진합니다. SECOND ME는 지식을 유지, 정리하고 사용자의 지식에 대한 동적으로 활용하는 지능적이고 지속적인 메모리 로드 시스템입니다. 사용자의 상호작용의 중간자 역할을 하며, 적절한 컨텍스트에 맞는 응답을 자동으로 생성하고 필요한 정보를 미리 채우고, 외부 시스템과의 무간 통신을 촉진하며, 인지적 부담과 상호작용의 마찰을 크게 줄이는 데 도움을 줍니다. 전통적인 메모리 저장 해결책에 비해, SECOND ME는 LLM 기반의 메모리 파라미터화를 활용하여 정적 데이터 저장을 초과하여 구조적 정리, 컨텍스트 연결, 적응적인 지식 검색을 가능하게 하며, 메모리 관리에 있어 체계적이고 지능적인 접근을 촉진합니다. AI 주도의 개인 에이전트처럼 SECOND ME는 디지털 생태계에 더욱 통합되어 있으며, SECOND ME는 지속적이고 컨텍스트에 맞는, 자동 조정된 메모리 시스템을 사용하여 인간과 세계와의 상호작용을 강화하는 중요한 단계로 표현됩니다. 우리는 GitHub에서 완전하게 지역화 가능한 배치 시스템의 오픈 소스화를 진행 중입니다: https://github.com/Mindverse/Second-Me.",
      "upvotes": 1,
      "discussionId": "67d12c33428a3d8d5281f346",
      "ai_keywords": [
        "large language models (LLMs)",
        "intelligent, persistent memory offload system",
        "context-aware responses",
        "structured organization",
        "contextual reasoning",
        "adaptive knowledge retrieval",
        "self-optimizing memory systems"
      ]
    },
    "publishedAt": "2025-03-11T03:05:52.000Z",
    "title": "AI-native Memory 2.0: Second Me",
    "summary": "Human interaction with the external world fundamentally involves the exchange\nof personal memory, whether with other individuals, websites, applications, or,\nin the future, AI agents. A significant portion of this interaction is\nredundant, requiring users to repeatedly provide the same information across\ndifferent contexts. Existing solutions, such as browser-stored credentials,\nautofill mechanisms, and unified authentication systems, have aimed to mitigate\nthis redundancy by serving as intermediaries that store and retrieve commonly\nused user data. The advent of large language models (LLMs) presents an\nopportunity to redefine memory management through an AI-native paradigm: SECOND\nME. SECOND ME acts as an intelligent, persistent memory offload system that\nretains, organizes, and dynamically utilizes user-specific knowledge. By\nserving as an intermediary in user interactions, it can autonomously generate\ncontext-aware responses, prefill required information, and facilitate seamless\ncommunication with external systems, significantly reducing cognitive load and\ninteraction friction. Unlike traditional memory storage solutions, SECOND ME\nextends beyond static data retention by leveraging LLM-based memory\nparameterization. This enables structured organization, contextual reasoning,\nand adaptive knowledge retrieval, facilitating a more systematic and\nintelligent approach to memory management. As AI-driven personal agents like\nSECOND ME become increasingly integrated into digital ecosystems, SECOND ME\nfurther represents a critical step toward augmenting human-world interaction\nwith persistent, contextually aware, and self-optimizing memory systems. We\nhave open-sourced the fully localizable deployment system at GitHub:\nhttps://github.com/Mindverse/Second-Me.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08102.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06594",
      "authors": [
        {
          "_id": "67cfd77ff8ee57c14450221b",
          "user": {
            "_id": "6440b38d3e0374802e1acc5e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440b38d3e0374802e1acc5e/w-ZpW_9gCSHUeDKyGSeMt.jpeg",
            "isPro": false,
            "fullname": "luoyingfeng",
            "user": "luoyingfeng",
            "type": "user"
          },
          "name": "Yingfeng Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:42:33.649Z",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221c",
          "name": "Tong Zheng",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221d",
          "name": "Yongyu Mu",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221e",
          "name": "Bei Li",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221f",
          "name": "Qinghong Zhang",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502220",
          "name": "Yongqi Gao",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502221",
          "name": "Ziqiang Xu",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502222",
          "name": "Peinan Feng",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502223",
          "name": "Xiaoqian Liu",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502224",
          "name": "Tong Xiao",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502225",
          "name": "Jingbo Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T12:54:05.000Z",
      "title": "단순히 디코더가 아니라: 대규모 언어 모델은 기계 번역의 엔코더로도 잘 사용될 수 있습니다.",
      "summary": "신경 기계 번역(NMT) 분야는 대규모 언어 모델(LLM)의 등장으로 변화했습니다. 최근의 자연어 처리(NLP)의 주장은 단일의 사전 학습된 Transformer 모델을 사용하여 기계 번역 및 다양한 문제를 모델링하는 데 중점을 두었습니다. 지난 주의 NMT 모델에서는 표준의 인코더-디코더 아키텍처가 상대적으로 적은 주목을 받았습니다. 본 논문에서는 LLM과 NMT의 세계를 연결하여 일반적인, 효율적이고 쉽게 최적화할 수 있는 번역 모델을 검토합니다. LLM을 NMT 인코더에 적용하고 NMT 디코더는 변경하지 않습니다. 또한 LLM을 NMT 디코더와 더 잘 협업시키기 위한 방법도 개발합니다. 또한 다양한 태스크를 포함하는 새로운 데이터 세트를 구축하여 기계 번역 시스템이 다수의 태스크에서 확장성을 어떻게 보여주는지 평가합니다. WMT와 우리의 데이터 세트의 평가에 따라, 우리의 방법에 의한 결과를 번역 품질에 대한 기준과 비교하여 동일하거나 초과하지만, 추론 속도는 2.4배 ~ 6.5배의 속도업과 KV 캐시의 메모리 사용량의 75% 감소를 실현합니다. 또한 여러 번역 관련 태스크에 강한 확장성을 나타냅니다.",
      "upvotes": 1,
      "discussionId": "67cfd780f8ee57c144502268",
      "githubRepo": "https://github.com/NiuTrans/LaMaTE/",
      "ai_keywords": [
        "large language models (LLMs)",
        "neural machine translation (NMT)",
        "natural language processing (NLP)",
        "Transformer decoder",
        "encoder-decoder architectures",
        "pre-trained Transformer decoder",
        "LLMs",
        "NMT encoding",
        "NMT decoder",
        "KV cache"
      ]
    },
    "publishedAt": "2025-03-09T08:54:05.000Z",
    "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
    "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve 2.4 sim 6.5 times inference speedups and a 75% reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06594.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06492",
      "authors": [
        {
          "_id": "67cfe557ad91643b5cb7d2c6",
          "user": {
            "_id": "67cd327432668b04f4555270",
            "avatarUrl": "/avatars/15e2cef976cbe05c4c5858c88dccf4af.svg",
            "isPro": false,
            "fullname": "Yanling Wang",
            "user": "WYLing",
            "type": "user"
          },
          "name": "Yanling Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T16:09:28.071Z",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c7",
          "name": "Yihan Zhao",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c8",
          "name": "Xiaodong Chen",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c9",
          "name": "Shasha Guo",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2ca",
          "name": "Lixin Liu",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cb",
          "name": "Haoyang Li",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cc",
          "name": "Yong Xiao",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cd",
          "name": "Jing Zhang",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2ce",
          "name": "Qi Li",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cf",
          "name": "Ke Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T07:25:32.000Z",
      "title": "VisualSimpleQA: 대형 비전-언어 모델의 사실 탐색 질문에 대한 분리된 평가의 기준점\n\n(注意：翻译中的“大规模”被译为“대형”，以保持专业性和准确性。)",
      "summary": "대시각 언어 모델(LVLMs)는 놀라운 성과를 보였지만, 사실적인 답변의 생성은 사실 탐색 질문에대하여 아직 많은 문제가 있습니다. 현재의 다 모델 사실 탐색 벤치마크는 주로 모델 출력과 실제 답변과의 비교에 초점을 맞추기 때문에, 모델의 성능에 대한 구체적인 이해는 제한되어 있습니다. 이를 해결하기 위해, VisualSimpleQA라는 다 모델 사실 탐색 벤치마크를 소개합니다. 이 벤치마크는 두 가지 특징을 가지고 있습니다. 1. LVLMs의 시각과 언어 모델의 스트리밍 및 디코더 로드 평가가 가능하도록 합니다. 2. 난이도의 정의가 명확하고, 인간 Annotation을 가이드하며, VisualSimpleQA-hard라는 어려운 서브셋을 추출하도록 합니다. 15 모델의 실험에 따르면, GPT-4o 등 최신 모델도 VisualSimpleQA에서 다 모델 사실 탐색 QA의 정확도는 60% 이상, VisualSimpleQA-hard에서 30% 이상입니다. 또한, 이러한 모델의 디코더 로드 평가는 시각과 언어 모듈 모두에서 큰 개선의 가능성을 보여줍니다. 데이터셋은 https://huggingface.co/datasets/WYLing/VisualSimpleQA에서 이용할 수 있습니다.",
      "upvotes": 1,
      "discussionId": "67cfe55bad91643b5cb7d3fb",
      "ai_keywords": [
        "Large vision-language models",
        "fact-seeking question answering",
        "multimodal benchmarks",
        "visual modality",
        "linguistic modality",
        "VisualSimpleQA",
        "VisualSimpleQA-hard",
        "GPT-4",
        "multimodal fact-seeking QA",
        "decoupled evaluation"
      ]
    },
    "publishedAt": "2025-03-09T03:25:32.000Z",
    "title": "VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large\n  Vision-Language Models in Fact-Seeking Question Answering",
    "summary": "Large vision-language models (LVLMs) have demonstrated remarkable\nachievements, yet the generation of non-factual responses remains prevalent in\nfact-seeking question answering (QA). Current multimodal fact-seeking\nbenchmarks primarily focus on comparing model outputs to ground truth answers,\nproviding limited insights into the performance of modality-specific modules.\nTo bridge this gap, we introduce VisualSimpleQA, a multimodal fact-seeking\nbenchmark with two key features. First, it enables streamlined and decoupled\nevaluation of LVLMs in visual and linguistic modalities. Second, it\nincorporates well-defined difficulty criteria to guide human annotation and\nfacilitates the extraction of a challenging subset, VisualSimpleQA-hard.\nExperiments on 15 LVLMs show that even state-of-the-art models such as GPT-4o\nachieve merely 60%+ correctness in multimodal fact-seeking QA on VisualSimpleQA\nand 30%+ on VisualSimpleQA-hard. Furthermore, the decoupled evaluation across\nthese models highlights substantial opportunities for improvement in both\nvisual and linguistic modules. The dataset is available at\nhttps://huggingface.co/datasets/WYLing/VisualSimpleQA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06492.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05860",
      "authors": [
        {
          "_id": "67d0a239967ead9b5aff9883",
          "user": {
            "_id": "655a627aab0644b531a02eb1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/9rW6X1idfx1p5omky67D6.jpeg",
            "isPro": false,
            "fullname": "Roham Koohestani",
            "user": "RohamKoohestani",
            "type": "user"
          },
          "name": "Roham Koohestani",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:41:54.151Z",
          "hidden": false
        },
        {
          "_id": "67d0a239967ead9b5aff9884",
          "user": {
            "_id": "655213d1968a2554a5e8212a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3XM_b9imWk-pwoueJwAZB.jpeg",
            "isPro": false,
            "fullname": "Philippe de Bekker",
            "user": "philippedebekker",
            "type": "user"
          },
          "name": "Philippe de Bekker",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T20:51:06.669Z",
          "hidden": false
        },
        {
          "_id": "67d0a239967ead9b5aff9885",
          "name": "Maliheh Izadi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T18:44:32.000Z",
      "title": "소프트웨어공학에서 AI 모델의 벤치마크: 리뷰, 검색 도구와 확장 프로토콜",
      "summary": "베치마크는 일관성 있는 평가와 재현성 보장에 중요합니다. 인공지능(AI)를 소프트웨어 엔지니어링(SE)에 통합함으로써, 코드 생성이나 버그 수정 등 작업에 대한 베치마크가 매우 많아졌습니다. 그러나 이 증가는 다음과 같은 여러 문제와 함께 있습니다: 1. 각 작업에 대한 베치마크 지식의 분산성, 2. 관련성 있는 베치마크의 선택의 어려움, 3. 베치마크 개발의 통일 표준의 부족, 4. 현재 베치마크의 제한성. 본 논문에서는 173건의 연구를 검토하고, 204건의 AI4SE 베치마크를 식별했습니다. 이들 베치마크를 클래스 분할하고, 제한성을 분석하고, 실용적 결점을 밝혀줍니다. 본 논문의 기초상, 맥락의 자동 클러스터링을 활용한 관련 연구의 맥락을 검색하기 위한 семантиック 검색 도구인 \"BenchScout\"를 개발했습니다. 22명의 참가자와 함께 사용자 스테이지에서, BenchScout의可用성, 有効性, 直感性에 대해 평가하였으며, 5점 평균 점수는 각각 4.5, 4.0, 4.1이었습니다. 베치마크 표준을 향상시키기 위해, BenchFrame라는 독특한 방법을 제안했습니다. 사례 연구로, HumanEval 베치마크에 BenchFrame을 적용하여 주요한 제한을 해결했습니다. 이로 인해, HumanEvalNext는 다음과 같은 특징을 갖게 되었습니다: 1. 오류의 수정, 2. 언어 변환의 개선, 3. 테스트 커버리지의 확장, 4. 난이도의 향상. 그 후, HumanEval, HumanEvalPlus, HumanEvalNext에 10개의 가장 先端의 코드 언어 모델을 평가했습니다. HumanEvalNext에서, HumanEval과 HumanEvalPlus에 대해 각각 31.22%와 19.94%의 점수가 감소했습니다.",
      "upvotes": 1,
      "discussionId": "67d0a23a967ead9b5aff98da",
      "projectPage": "https://evalpro.online/",
      "githubRepo": "https://github.com/AISE-TUDelft/AI4SE-benchmarks",
      "ai_keywords": [
        "AI4SE (Artificial Intelligence in Software Engineering)",
        "BenchScout",
        "semantic search tool",
        "automated clustering",
        "BenchFrame",
        "HumanEval",
        "HumanEvalNext",
        "pass@1 score"
      ]
    },
    "publishedAt": "2025-03-07T13:44:32.000Z",
    "title": "Benchmarking AI Models in Software Engineering: A Review, Search Tool,\n  and Enhancement Protocol",
    "summary": "Benchmarks are essential for consistent evaluation and reproducibility. The\nintegration of Artificial Intelligence into Software Engineering (AI4SE) has\ngiven rise to numerous benchmarks for tasks such as code generation and bug\nfixing. However, this surge presents challenges: (1) scattered benchmark\nknowledge across tasks, (2) difficulty in selecting relevant benchmarks, (3)\nthe absence of a uniform standard for benchmark development, and (4)\nlimitations of existing benchmarks. In this paper, we review 173 studies and\nidentify 204 AI4SE benchmarks. We classify these benchmarks, analyze their\nlimitations, and expose gaps in practices. Based on our review, we created\nBenchScout, a semantic search tool to find relevant benchmarks, using automated\nclustering of the contexts from associated studies. We conducted a user study\nwith 22 participants to evaluate BenchScout's usability, effectiveness, and\nintuitiveness which resulted in average scores of 4.5, 4.0, and 4.1 out of 5.\nTo advance benchmarking standards, we propose BenchFrame, a unified method to\nenhance benchmark quality. As a case study, we applied BenchFrame to the\nHumanEval benchmark and addressed its main limitations. This led to\nHumanEvalNext, featuring (1) corrected errors, (2) improved language\nconversion, (3) expanded test coverage, and (4) increased difficulty. We then\nevaluated ten state-of-the-art code language models on HumanEval,\nHumanEvalPlus, and HumanEvalNext. On HumanEvalNext, models showed a pass@1\nscore reduction of 31.22% and 19.94% compared to HumanEval and HumanEvalPlus,\nrespectively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05860.png",
    "numComments": 1,
    "isAuthorParticipating": true
  }
]