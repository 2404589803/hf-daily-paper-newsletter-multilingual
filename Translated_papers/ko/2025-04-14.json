[
  {
    "paper": {
      "id": "2504.08685",
      "authors": [
        {
          "_id": "67fc6ffc59b22e7c34d64c2e",
          "name": "Team Seawead",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c2f",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c30",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c31",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c32",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c33",
          "name": "Zhibei Ma",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c34",
          "name": "Haoyuan Guo",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c35",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c36",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c37",
          "name": "Sen Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c38",
          "name": "Feng Cheng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c39",
          "name": "Feilong Zuo Xuejiao Zeng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3a",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3b",
          "name": "Fangyuan Kong",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3c",
          "name": "Zhiwu Qing",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3d",
          "name": "Fei Xiao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3e",
          "name": "Meng Wei",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3f",
          "name": "Tuyen Hoang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c40",
          "name": "Siyu Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c41",
          "name": "Peihao Zhu",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c42",
          "name": "Qi Zhao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c43",
          "name": "Jiangqiao Yan",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c44",
          "name": "Liangke Gui",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c45",
          "name": "Sheng Bi",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c46",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c47",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c48",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c49",
          "name": "Huixia Li",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4a",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4b",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4c",
          "user": {
            "_id": "636a4e4fa55bbbdf8c877667",
            "avatarUrl": "/avatars/efdb68c56a4a44fdac52750c07a6cc35.svg",
            "isPro": false,
            "fullname": "Ling Feng",
            "user": "lingff",
            "type": "user"
          },
          "name": "Feng Ling",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:31.964Z",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4d",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4e",
          "name": "Houmin Wei",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4f",
          "name": "Huafeng Kuang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c50",
          "name": "Jerry Duncan",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c51",
          "name": "Junda Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c52",
          "name": "Junru Zheng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c53",
          "name": "Li Sun",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c54",
          "name": "Manlin Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c55",
          "name": "Renfei Sun",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c56",
          "name": "Xiaobin Zhuang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c57",
          "name": "Xiaojie Li",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c58",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c59",
          "name": "Xuyan Chi",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5a",
          "name": "Yanghua Peng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5b",
          "name": "Yuping Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5c",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5d",
          "name": "Zhongkai Zhao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5e",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5f",
          "name": "Zuquan Song",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c60",
          "user": {
            "_id": "6421183b69a2c2933882d652",
            "avatarUrl": "/avatars/66813a8fa22915087cccd4dbfb945ca7.svg",
            "isPro": false,
            "fullname": "Zhenheng Yang",
            "user": "zhenheny",
            "type": "user"
          },
          "name": "Zhenheng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:34.053Z",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c61",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c62",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c63",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a5cba3bea0116f8f7187a7/Zl6Xq55_4dviWTCMwOptb.jpeg"
      ],
      "publishedAt": "2025-04-11T16:46:20.000Z",
      "submittedOnDailyAt": "2025-04-14T00:55:27.428Z",
      "title": "알고리즘-7B: 비디오 생성 기반 모델의 효율적인 훈련",
      "submittedOnDailyBy": {
        "_id": "64a5cba3bea0116f8f7187a7",
        "avatarUrl": "/avatars/97b3bad82e359c57d1ab7d6e3aa7748b.svg",
        "isPro": false,
        "fullname": "Lu Jiang",
        "user": "roadjiang",
        "type": "user"
      },
      "summary": "이 기술보고서에서는 비디오 생성 기반 모델의 훈련에 효율적인 전략을 제안합니다. 여기서는 약 7억 파라미터(7B)를 가진 내부 크기의 연구 모델 'Seaweed-7B'를 665,000 H100 GPU 시간 동안 단축된 방식으로 훈련시킵니다. 계산 자원이 중도일 때도, Seaweed-7B는 큰 비디오 생성 모델과 비교하여 높은 경쟁적 성능을 보여주며, 자원 제한이 있는 환경에서 설계 선택이 특히 중요합니다. 이 기술보고서에서는 내부 크기의 Difu-Jetion 모델의 성능 향상에 기여하는 핵심적인 설계 결정을 중점적으로 제시합니다. 실험적으로는 두 가지 관찰을 수행합니다: 1) Seaweed-7B는 크게 큰 GPU 자원을 사용하여 훈련된 큰 모델과 성능을 비교할 수 있으며, 그 이상을 초월할 수 있습니다. 2) 강한 일반화 능력을 가진 모델은 광범위한 하류 애플리케이션에 효과적으로 적응할 수 있습니다. 프로젝트 페이지는 https://seaweed.video/에서 참조하세요.",
      "upvotes": 57,
      "discussionId": "67fc700159b22e7c34d64d78",
      "projectPage": "https://seaweed.video/"
    },
    "publishedAt": "2025-04-11T12:46:20.000Z",
    "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
    "summary": "This technical report presents a cost-efficient strategy for training a video\ngeneration foundation model. We present a mid-sized research model with\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\nresources, Seaweed-7B demonstrates highly competitive performance compared to\ncontemporary video generation models of much larger size. Design choices are\nespecially crucial in a resource-constrained setting. This technical report\nhighlights the key design decisions that enhance the performance of the\nmedium-sized diffusion model. Empirically, we make two observations: (1)\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\ntrained on substantially greater GPU resources, and (2) our model, which\nexhibits strong generalization ability, can be effectively adapted across a\nwide range of downstream applications either by lightweight fine-tuning or\ncontinue training. See the project page at https://seaweed.video/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a5cba3bea0116f8f7187a7/Zl6Xq55_4dviWTCMwOptb.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08685.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "64a5cba3bea0116f8f7187a7",
      "avatarUrl": "/avatars/97b3bad82e359c57d1ab7d6e3aa7748b.svg",
      "fullname": "Lu Jiang",
      "name": "roadjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08736",
      "authors": [
        {
          "_id": "67fc8e37864dfcbd93d3b802",
          "user": {
            "_id": "668125557b50b433cda2a211",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
            "isPro": false,
            "fullname": "Tianwei Xiong",
            "user": "YuuTennYi",
            "type": "user"
          },
          "name": "Tianwei Xiong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:29.330Z",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b803",
          "name": "Jun Hao Liew",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b804",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b805",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b806",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:26.140Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T17:59:58.000Z",
      "submittedOnDailyAt": "2025-04-14T02:57:04.488Z",
      "title": "GigaTok: 30억 파라미터의 시각 토크나이저를 확장하여 자동 복원 이미지 생성에 적용",
      "submittedOnDailyBy": {
        "_id": "668125557b50b433cda2a211",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
        "isPro": false,
        "fullname": "Tianwei Xiong",
        "user": "YuuTennYi",
        "type": "user"
      },
      "summary": "自動회귀(AR) 이미지 생성에서, 시각토큰나이저는 이미지를 총괄적인 분산 잠재토큰으로 압축하여, 다음 토큰 예측에 의한 시각 생성을 위한 효율적인 학습을 가능하게 합니다. 시각토큰나이저의 확장으로 이미지 재구성 품질이 개선되는 반면, 하류의 생성 품질이 악화하는 경우가 많습니다. 이는 현재의 문헌에서 충분히 해결되지 않은 문제입니다. 이에 대처하여, 우리는 GigaTok라는 최초의 접근 방식을 제안하고, 시각토큰나이저의 확장으로 이미지 재구성, 생성, 표현 학습을 동시에 개선하는 것을 목표로 합니다. 잠재 공간의 확장으로 인한 복잡성의 증가는 재구성과 생성의 대립의 주요 원인으로 인식되었습니다. 이를 완화하기 위해, 우리는 전처리된 시각 인코더로부터 의미적으로 일치하는 특성을 토큰나이저의 특성과 일치시키기 위한 의미적 정규화를 제안합니다. 이 제약은 확장으로 인한 잠재 공간의 과도한 복잡성을 방지하고, 재구성과 하류의 자동회귀 생성 모두에 일관된 개선을 얻습니다. 의미적 정규화에 기반하여, 우리는 토큰나이저의 확장의 3가지 주요 전략을 조사합니다: (1) 1D 토큰나이저를 사용하여 더 좋은 스케일러빌리티, (2) 인코더와 디코더 모두를 확장할 때 디코더의 스케일링을 우선 순위로, (3) 베리 스케일의 토큰나이저의 훈련 안정화를 목표로 엔트로피 손실을 사용합니다. 300억 파라미터로 확장된 GigaTok은 재구성, 하류의 AR 생성, 하류의 AR 표현 품질에 있어 최상위 성능을 달성합니다.",
      "upvotes": 23,
      "discussionId": "67fc8e38864dfcbd93d3b836",
      "projectPage": "https://silentview.github.io/GigaTok/",
      "githubRepo": "https://github.com/SilentView/GigaTok"
    },
    "publishedAt": "2025-04-11T13:59:58.000Z",
    "title": "GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for\n  Autoregressive Image Generation",
    "summary": "In autoregressive (AR) image generation, visual tokenizers compress images\ninto compact discrete latent tokens, enabling efficient training of downstream\nautoregressive models for visual generation via next-token prediction. While\nscaling visual tokenizers improves image reconstruction quality, it often\ndegrades downstream generation quality -- a challenge not adequately addressed\nin existing literature. To address this, we introduce GigaTok, the first\napproach to simultaneously improve image reconstruction, generation, and\nrepresentation learning when scaling visual tokenizers. We identify the growing\ncomplexity of latent space as the key factor behind the reconstruction vs.\ngeneration dilemma. To mitigate this, we propose semantic regularization, which\naligns tokenizer features with semantically consistent features from a\npre-trained visual encoder. This constraint prevents excessive latent space\ncomplexity during scaling, yielding consistent improvements in both\nreconstruction and downstream autoregressive generation. Building on semantic\nregularization, we explore three key practices for scaling tokenizers:(1) using\n1D tokenizers for better scalability, (2) prioritizing decoder scaling when\nexpanding both encoder and decoder, and (3) employing entropy loss to stabilize\ntraining for billion-scale tokenizers. By scaling to 3 space billion\nparameters, GigaTok achieves state-of-the-art performance in reconstruction,\ndownstream AR generation, and downstream AR representation quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08736.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668125557b50b433cda2a211",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
      "fullname": "Tianwei Xiong",
      "name": "YuuTennYi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08388",
      "authors": [
        {
          "_id": "67fc7367df5f5d1e87c14c6a",
          "name": "Junliang Guo",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6b",
          "name": "Yang Ye",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6c",
          "name": "Tianyu He",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6d",
          "name": "Haoyu Wu",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6e",
          "name": "Yushu Jiang",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6f",
          "name": "Tim Pearce",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c70",
          "name": "Jiang Bian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T09:41:04.000Z",
      "submittedOnDailyAt": "2025-04-14T02:07:06.500Z",
      "title": "MineWorld: マイクラストーン 위의 시간 실타임에 개방 소스의 상호작용 세계 모델",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "ワールドモデリング은 컴퓨터 에이전트가 인간과 효과적으로 상호작용하고 동적인 환경에서 작동할 수 있도록 하는 중요한 임무입니다. 본 논문에서는, テスト ボックス로 활용되는 개방형 접근 방식의 오픈 엔드 プローチ를 통해 사용되고 있는 サンドボックス ゲーム 「Minecraft」에 대해, 시퀀스적으로 상호작용 가능한 ワールドモデル 「MineWorld」를 제안합니다. 「MineWorld」는 시각적 행동 자동 회귀 ツレンダー를 통해, 시각적 センス와 행동을 입력으로 받아, 이에 대응하는 행동으로 생성되는 다음 シーン을 생성하는 데 사용됩니다. 특히, 이미지 トーキナナ와 행동 トーキナナ를 사용하여, 시각적 センス와 행동을 이산 トーキング ID로 변환하여, 이 두 종류의 ID의 교차 결합으로 모델의 입력을 구성합니다. 이후, 다음 トーキング 예측을 사용하여, 게임 상태의 풍부한 표현과 상태와 행동 사이의 관계를 동시에 학습합니다. 추론 시에는, 각 프레임의 공간적 중복 トーキング를 동시에 예측하는 새로운 병렬 디코딩 알고리즘을 개발하여, 다양한 스케일의 모델이 4から7 프레임마다 생성되고, 게임 플레이어와 시퀀스적인 상호작용을 가능하게 합니다. 평가에서는 새로운 메트릭을 제안하고, 새로운 シーン의 생성 시 시각적 품질과 행동의 적응력을 평가하여, ワールドモデル의 중요성을 보여주었습니다. 세부적인 평가에 따라, 「MineWorld」의 효과성은 정렬된 드라이브 버전의 분기 기반의 ワールドモデル보다 크게 초과했습니다. 코드와 모델은 릴리즈되었습니다.",
      "upvotes": 12,
      "discussionId": "67fc7367df5f5d1e87c14ca6",
      "githubRepo": "https://github.com/microsoft/MineWorld"
    },
    "publishedAt": "2025-04-11T05:41:04.000Z",
    "title": "MineWorld: a Real-Time and Open-Source Interactive World Model on\n  Minecraft",
    "summary": "World modeling is a crucial task for enabling intelligent agents to\neffectively interact with humans and operate in dynamic environments. In this\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\nan open-ended sandbox game which has been utilized as a common testbed for\nworld modeling. MineWorld is driven by a visual-action autoregressive\nTransformer, which takes paired game scenes and corresponding actions as input,\nand generates consequent new scenes following the actions. Specifically, by\ntransforming visual game scenes and actions into discrete token ids with an\nimage tokenizer and an action tokenizer correspondingly, we consist the model\ninput with the concatenation of the two kinds of ids interleaved. The model is\nthen trained with next token prediction to learn rich representations of game\nstates as well as the conditions between states and actions simultaneously. In\ninference, we develop a novel parallel decoding algorithm that predicts the\nspatial redundant tokens in each frame at the same time, letting models in\ndifferent scales generate 4 to 7 frames per second and enabling real-time\ninteractions with game players. In evaluation, we propose new metrics to assess\nnot only visual quality but also the action following capacity when generating\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\nbased world models significantly. The code and model have been released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08388.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.07963",
      "authors": [
        {
          "_id": "67f86da6ac109135e18e150f",
          "name": "Shoufa Chen",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1510",
          "name": "Chongjian Ge",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1511",
          "name": "Shilong Zhang",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1512",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1513",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6412a33900634c4fe9873652/tQXTl59CIq5IxKj_5dk7f.jpeg"
      ],
      "publishedAt": "2025-04-10T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-14T04:05:17.975Z",
      "title": "PixelFlow: 흐름을 이용한 픽셀 공간에서의 생성 모델",
      "submittedOnDailyBy": {
        "_id": "6412a33900634c4fe9873652",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
        "isPro": false,
        "fullname": "Shoufa Chen",
        "user": "ShoufaChen",
        "type": "user"
      },
      "summary": "PixelFlow는 일반적인 잠재 공간 모델과 달리, 직접 실제 픽셀 공간에서 동작하는 이미지 생성 모델의 집합입니다. 이 접근 방식은 사전 학습된 Variational Autoencoder (VAE)의 필요성을 줄이고, 전체 모델이 끝에서 끝까지 학습 가능한 형태로 만들어줍니다. 효율적인 계층적 모델링을 통해, PixelFlow는 실제 픽셀 공간에서 계산 비용을 줄일 수 있습니다. 256x256의 ImageNet 클래스 조건 이미지 생성 벤치마크에서, FID 값은 1.98입니다. 질적인 텍스트로부터 이미지의 결과는, 이미지의 품질, 예술성, семанти적 제어에서 PixelFlow가 뛰어납니다. 우리는 이 새로운 패러다임이 다음 세대의 시각화 생성 모델에 새로운 기회를 열어, 영감을 주는 것을 기대합니다. 코드와 모델은, https://github.com/ShoufaChen/PixelFlow에서 액세스할 수 있습니다.",
      "upvotes": 8,
      "discussionId": "67f86da7ac109135e18e154b",
      "githubRepo": "https://github.com/ShoufaChen/PixelFlow"
    },
    "publishedAt": "2025-04-10T13:59:56.000Z",
    "title": "PixelFlow: Pixel-Space Generative Models with Flow",
    "summary": "We present PixelFlow, a family of image generation models that operate\ndirectly in the raw pixel space, in contrast to the predominant latent-space\nmodels. This approach simplifies the image generation process by eliminating\nthe need for a pre-trained Variational Autoencoder (VAE) and enabling the whole\nmodel end-to-end trainable. Through efficient cascade flow modeling, PixelFlow\nachieves affordable computation cost in pixel space. It achieves an FID of 1.98\non 256times256 ImageNet class-conditional image generation benchmark. The\nqualitative text-to-image results demonstrate that PixelFlow excels in image\nquality, artistry, and semantic control. We hope this new paradigm will inspire\nand open up new opportunities for next-generation visual generation models.\nCode and models are available at https://github.com/ShoufaChen/PixelFlow.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6412a33900634c4fe9873652/tQXTl59CIq5IxKj_5dk7f.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07963.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6412a33900634c4fe9873652",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
      "fullname": "Shoufa Chen",
      "name": "ShoufaChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08600",
      "authors": [
        {
          "_id": "67fc9e72b2383c63dc413dcb",
          "name": "Peixian Ma",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dcc",
          "user": {
            "_id": "6575a625b951d40e7a4d8685",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
            "isPro": false,
            "fullname": "zhuangxialie",
            "user": "ZhuangXialie",
            "type": "user"
          },
          "name": "Xialie Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:23.810Z",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dcd",
          "name": "Chengjin Xu",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dce",
          "name": "Xuhui Jiang",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dcf",
          "name": "Ran Chen",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dd0",
          "name": "Jian Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T15:01:30.000Z",
      "submittedOnDailyAt": "2025-04-14T04:07:17.501Z",
      "title": "SQL-R1: 강화학습에 의한 자연어로부터 SQL의 추론 모델의 훈련",
      "submittedOnDailyBy": {
        "_id": "6575a625b951d40e7a4d8685",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
        "isPro": false,
        "fullname": "zhuangxialie",
        "user": "ZhuangXialie",
        "type": "user"
      },
      "summary": "자연어로부터 SQL(NL2SQL)는 자연어 쿼리를 구조화된 SQL 문으로 변환하여 데이터베이스와 직관적인 인터랙션을 가능하게 합니다. 최근 데이터베이스 애플리케이션 내의 인간・컴퓨터 인터랙션의 향상에 대한 발전이 있습니다만, 복잡한 시나리오에서 다 테이블 조인 및 숨겨진 쿼리에 대한 추론 성능에 큰 문제점이 남아 있습니다. 현재의 방법은 주로 관습적 미세 조정 훈련(SFT)을 통해 NL2SQL 모델을 훈련합니다만, 새로운 환경(예: 금융, 건강 관련)에서의 적응성 및 해석성에 한계가 있습니다. 복잡한 상황에서 NL2SQL 모델의 추론 성능을 향상시키기 위해 SQL-R1이라는 새로운 NL2SQL 추론 모델을 도입합니다. 이 모델은 강화학습(RL) 알고리즘으로 훈련되었습니다. NL2SQL 작업에 특화된 RL 기반 보상 함수를 설계하고, 강화학습의 효율성에 대한 초기 효과의 영향을 논의했습니다. 또한 합성된 NL2SQL 데이터의 적은 양에서 강화학습으로 경쟁적인 정확도를 달성하고, 더 나은 데이터 엔지니어링을 탐구했습니다. 기존 실험에서 SQL-R1은 88.6%의 정확도를 VIEWER의 스ピダー 및 66.6%의 정확도를 BIRD에서 달성했습니다.",
      "upvotes": 6,
      "discussionId": "67fc9e73b2383c63dc413e19"
    },
    "publishedAt": "2025-04-11T11:01:30.000Z",
    "title": "SQL-R1: Training Natural Language to SQL Reasoning Model By\n  Reinforcement Learning",
    "summary": "Natural Language to SQL (NL2SQL) enables intuitive interactions with\ndatabases by transforming natural language queries into structured SQL\nstatements. Despite recent advancements in enhancing human-computer interaction\nwithin database applications, significant challenges persist, particularly\nregarding the inference performance in complex scenarios involving multi-table\njoins and nested queries. Current methodologies primarily utilize supervised\nfine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and\ninterpretability in new environments (e.g., finance and healthcare). In order\nto enhance the reasoning performance of the NL2SQL model in the above complex\nsituations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the\nreinforcement learning (RL) algorithms. We design a specialized RL-based reward\nfunction tailored for NL2SQL tasks and discussed the impact of cold start on\nthe effectiveness of intensive training. In addition, we achieve competitive\naccuracy using only a tiny amount of synthetic NL2SQL data for augmented\ntraining and further explore data engineering for RL. In existing experiments,\nSQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider\nand BIRD, respectively, only using the 7B base model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6575a625b951d40e7a4d8685",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
      "fullname": "zhuangxialie",
      "name": "ZhuangXialie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.07405",
      "authors": [
        {
          "_id": "67fa383909d06d0501a5e34e",
          "user": {
            "_id": "660d844462d63ad0009a9859",
            "avatarUrl": "/avatars/6822fc1a82c64dbfd40b88080a5fb1ae.svg",
            "isPro": false,
            "fullname": "Linyan Huang",
            "user": "DevLinyan",
            "type": "user"
          },
          "name": "Linyan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-13T19:24:47.680Z",
          "hidden": false
        },
        {
          "_id": "67fa383909d06d0501a5e34f",
          "name": "Haonan Lin",
          "hidden": false
        },
        {
          "_id": "67fa383909d06d0501a5e350",
          "name": "Yanning Zhou",
          "hidden": false
        },
        {
          "_id": "67fa383909d06d0501a5e351",
          "name": "Kaiwen Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T02:58:22.000Z",
      "submittedOnDailyAt": "2025-04-14T00:54:04.513Z",
      "title": "FlexIP: 프로시앵스(Prescription)와 인테리어(Personality)의 동적 제어를 통한 사용자 맞춤형 이미지 생성",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "2D 생성 모델의 급속한 발전에 따라, 주제 인식을 유지하면서 다양한 편집이 가능한 것이 중요한 연구 초점이 되고 있다. 현재의 방법들은 일반적으로 인식 보호와 인물 표현의 조절 사이에 고유한 트레이드오프를 가집니다. 우리는 Style-wise 연산을 위한 Personalization Adapter와 인식 유지를 위한 Preservation Adapter를 포함하는 새로운 프레임워크 FlexIP를 소개합니다. 이 프레임워크는 생성 모델에 직접 제어 구조를 주입하고 추론 시 가중치 어드밴터의 동적 조정을 통해 유연한 파라미터 제어를 가능하게 합니다. 실험 결과는 우리의 접근 방식이 가치관적인 방법의 성능의 한계를 초월하고 높은 수준의 인식 보호를 실현하고 더 다양한 인물 표현을 생성할 수 있는 능력의 지원을 제공할 수 있음을 보여줍니다 (프로젝트 페이지: https://flexip-tech.github.io/flexip/).",
      "upvotes": 6,
      "discussionId": "67fa383c09d06d0501a5e3ef",
      "projectPage": "https://flexip-tech.github.io/flexip"
    },
    "publishedAt": "2025-04-09T22:58:22.000Z",
    "title": "FlexIP: Dynamic Control of Preservation and Personality for Customized\n  Image Generation",
    "summary": "With the rapid advancement of 2D generative models, preserving subject\nidentity while enabling diverse editing has emerged as a critical research\nfocus. Existing methods typically face inherent trade-offs between identity\npreservation and personalized manipulation. We introduce FlexIP, a novel\nframework that decouples these objectives through two dedicated components: a\nPersonalization Adapter for stylistic manipulation and a Preservation Adapter\nfor identity maintenance. By explicitly injecting both control mechanisms into\nthe generative model, our framework enables flexible parameterized control\nduring inference through dynamic tuning of the weight adapter. Experimental\nresults demonstrate that our approach breaks through the performance\nlimitations of conventional methods, achieving superior identity preservation\nwhile supporting more diverse personalized generation capabilities (Project\nPage: https://flexip-tech.github.io/flexip/).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07405.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08366",
      "authors": [
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3b",
          "name": "Sauradip Nag",
          "hidden": false
        },
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3c",
          "name": "Daniel Cohen-Or",
          "hidden": false
        },
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3d",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3e",
          "name": "Ali Mahdavi-Amiri",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T09:01:09.000Z",
      "submittedOnDailyAt": "2025-04-14T00:36:54.457Z",
      "title": "In-2-4D: 2개의 단일 시각점 이미지에서 4차원 생성의 간접화",
      "submittedOnDailyBy": {
        "_id": "6399ab3296ce14c5dcf4ccbf",
        "avatarUrl": "/avatars/89aeedc96f73d76f7a6da96454f40fd2.svg",
        "isPro": false,
        "fullname": "Sauradip Nag",
        "user": "sauradip",
        "type": "user"
      },
      "summary": "우리는 최소한의 입력 설정에서 생성되는 4D(즉, 3D + 움직임) 간접화에 대한 새로운 문제 \"In-2-4D\"를 제안합니다: 두 개의 단일 뷰의 이미지에서 대상물의 두 가지 다른 움직임 상태를 촬영합니다. 두 개의 이미지가 움직임의 시작과 종료 상태를 나타내는 경우, 우리의 목표는 4D에서 움직임을 생성하고 재구성하는 것입니다. 비디오 인터페이스 모델을 사용하여 움직임을 예측하여 큰 프레임 간 움직임이 불확실한 해석에 이어지지 않도록 합니다. 이를 해결하기 위해, 우리는 시각적으로 입력 상태에 가까운 키 프레임을 특정하고 눈에 띄는 움직임을 찾아 그 사이에 순滑한 프레임을 생성하기 위한 계층적 접근을 사용합니다. 각 프레임에서, 키 프레임의 3D 표현은 가우시안 스폿팅을 사용하여 구축됩니다. 프레임의 시간적인 프레임은 움직임을 가우시안으로 동적으로 변형하는 것을 가능하게 합니다. 시간적 일관성과 3D 움직임의 정확도를 개선하기 위해, 각 시간 단계마다 다 뷰 디퓨저의 자기 어텐션을 확장하고 刚성 변형 정규화를 적용합니다. 마지막으로, 독립적으로 생성된 3D 움직임 세그먼트는 경계 변형 필드를 인터페이스로 하여 가이드 비디오와 일치시켜서, 순滑하고 연속적인 컨텍스트를 보장하여 이를 통합합니다. 세부적인 질적 및 양적인 실험 및 사용자 스테이지에서, 우리는 우리의 방법 및 구성 요소의 효과를 보여주겠습니다. 프로젝트 페이지는 https://in-2-4d.github.io/ 에 있습니다.",
      "upvotes": 4,
      "discussionId": "67fc6d9374c3c0f0d6f24e12",
      "projectPage": "https://in-2-4d.github.io/",
      "githubRepo": "https://github.com/sauradip/In-2-4D"
    },
    "publishedAt": "2025-04-11T05:01:09.000Z",
    "title": "In-2-4D: Inbetweening from Two Single-View Images to 4D Generation",
    "summary": "We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion)\ninbetweening from a minimalistic input setting: two single-view images\ncapturing an object in two distinct motion states. Given two images\nrepresenting the start and end states of an object in motion, our goal is to\ngenerate and reconstruct the motion in 4D. We utilize a video interpolation\nmodel to predict the motion, but large frame-to-frame motions can lead to\nambiguous interpretations. To overcome this, we employ a hierarchical approach\nto identify keyframes that are visually close to the input states and show\nsignificant motion, then generate smooth fragments between them. For each\nfragment, we construct the 3D representation of the keyframe using Gaussian\nSplatting. The temporal frames within the fragment guide the motion, enabling\ntheir transformation into dynamic Gaussians through a deformation field. To\nimprove temporal consistency and refine 3D motion, we expand the self-attention\nof multi-view diffusion across timesteps and apply rigid transformation\nregularization. Finally, we merge the independently generated 3D motion\nsegments by interpolating boundary deformation fields and optimizing them to\nalign with the guiding video, ensuring smooth and flicker-free transitions.\nThrough extensive qualitative and quantitiave experiments as well as a user\nstudy, we show the effectiveness of our method and its components. The project\npage is available at https://in-2-4d.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08366.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6399ab3296ce14c5dcf4ccbf",
      "avatarUrl": "/avatars/89aeedc96f73d76f7a6da96454f40fd2.svg",
      "fullname": "Sauradip Nag",
      "name": "sauradip",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08716",
      "authors": [
        {
          "_id": "67fca0ca05cd5b5035123b7e",
          "name": "Wissam Antoun",
          "hidden": false
        },
        {
          "_id": "67fca0ca05cd5b5035123b7f",
          "name": "Benoît Sagot",
          "hidden": false
        },
        {
          "_id": "67fca0ca05cd5b5035123b80",
          "name": "Djamé Seddah",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T17:29:35.000Z",
      "submittedOnDailyAt": "2025-04-14T04:20:01.034Z",
      "title": "ModernBERT나 DeBERTaV3? Transformer Encoder 모델의 성능에 대한 구조와 데이터의 영향에 대해 검토합니다.",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "予測 모델에서 효율성과 성능 개선을 목표로 구조적인 발전을 도입한 데벨타V3와 현대BERT 등 사전 학습 텐서 라이버러 인코더 모델이 있습니다. 현대BERT의 저자는 DeBERTaV3보다 다수의 벤치마크에서 개선된 성능을 보고하고 있지만, 공개되지 않은 훈련 데이터와 공유 데이터 세트에 대한 비교가 없기 때문에 이러한 효과가 구조적인 개선인지 훈련 데이터의 차이에 의한 것이라 판단하기가 어렵습니다. 본 연구에서는 케멘베타V2와 같은 데이터 세트를 사용하여 현대BERT를 사전 학습하고 모델 설계의 영향을 분리하여 제어적인 연구를 수행했습니다. 결과적으로, 이전 모델의 생성은 샘플 효과와 전체 벤치마크 성능에 있어서 뛰어납니다が, 현대BERT의 주요 장점은 빠른 훈련 및 추론 속도입니다. 그러나 제안된 새로운 모델은 BERT나 RoBERTa와 비교하여 의미 있는 구조적인 개선을 제공하고 있습니다. 또한, 고품질의 사전 학습 데이터는 수렴을 가속화하지만, 최종적인 성능에 대해서는 상당한 개선이 없습니다. 이는 벤치마크의 사태션 가능성을 보여줍니다. 이러한 발견은 사전 학습 데이터와 구조적인 혁신을 구분하는 중요성을トレンジャー 모델 평가에示しています.",
      "upvotes": 3,
      "discussionId": "67fca0ca05cd5b5035123ba6"
    },
    "publishedAt": "2025-04-11T13:29:35.000Z",
    "title": "ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\n  Transformer Encoder Models Performance",
    "summary": "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce\narchitectural advancements aimed at improving efficiency and performance.\nAlthough the authors of ModernBERT report improved performance over DeBERTaV3\non several benchmarks, the lack of disclosed training data and the absence of\ncomparisons using a shared dataset make it difficult to determine whether these\ngains are due to architectural improvements or differences in training data. In\nthis work, we conduct a controlled study by pretraining ModernBERT on the same\ndataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of\nmodel design. Our results show that the previous model generation remains\nsuperior in sample efficiency and overall benchmark performance, with\nModernBERT's primary advantage being faster training and inference speed.\nHowever, the new proposed model still provides meaningful architectural\nimprovements compared to earlier models such as BERT and RoBERTa. Additionally,\nwe observe that high-quality pre-training data accelerates convergence but does\nnot significantly improve final performance, suggesting potential benchmark\nsaturation. These findings show the importance of disentangling pretraining\ndata from architectural innovations when evaluating transformer models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08716.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2491
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08192",
      "authors": [
        {
          "_id": "67fcb3584a92187863e732d5",
          "name": "Aashiq Muhamed",
          "hidden": false
        },
        {
          "_id": "67fcb3584a92187863e732d6",
          "name": "Jacopo Bonato",
          "hidden": false
        },
        {
          "_id": "67fcb3584a92187863e732d7",
          "name": "Mona Diab",
          "hidden": false
        },
        {
          "_id": "67fcb3584a92187863e732d8",
          "name": "Virginia Smith",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T01:24:03.000Z",
      "submittedOnDailyAt": "2025-04-14T05:34:15.388Z",
      "title": "SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder\n  LLM의 정확도 학습의 보호 라인\n\n(注意：虽然要求不添加额外的文本，但为了确保翻译的准确性和专业性，我在翻译中保留了原文的标题结构和标点符号。)",
      "submittedOnDailyBy": {
        "_id": "64755a83e0b188d3cb2579d8",
        "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
        "isPro": false,
        "fullname": "Aashiq Muhamed",
        "user": "aashiqmuhamed",
        "type": "user"
      },
      "summary": "머신 유닛러닝은 모델에서 불만족스러운 지식 제거를 통해 LLM의 안전성을 향상시키는 잠재적인 방법입니다. 그러나 일반적으로 사용되고 있는 경사 기반의 유닛러닝 방법들은 높은 계산 비용, 초 파라미터 불안정, 순차적인 유닛러닝 능력의 저하, 재학습 공격의 취약성, 낮은 데이터 효과성, 해석성 부족 등 여러 문제에 직면하고 있습니다. 희소 아웃워더는 특정 활성화기 기반의 유닛러닝을 가능하게 하며 이러한 면에 개선하는 데 적합하지만, 기존의 접근 방식은 경사 기반의 방법보다 떨어집니다. 본 논문에서는 기존의 결과를 반대하고, SAE는 동적으로 사용될 때 유닛러닝을 크게 개선하는 것을 보여줍니다. Dynamic Denoising Autoencoder Guardrails (DSG)을 소개합니다. DSG은 기본적인 특성 선택과 동적인 클래스 피처러를 활용한 정확도 유닛러닝의 새로운 방법입니다. 실험 결과에 따르면, DSG는 주요 유닛러닝 방법들을 크게 초월하며, 잊고 도움을 주는 균형을 높입니다. DSG은 경사 기반의 접근 방식의 주요한 단점을 해결하고, 효율적인 계산 비용과 안정성, 순차적인 유닛러닝의 강력한 성능, 재학습 공격에 강한 저항, 데이터 효과성의 개선(0 shot set 포함), 유닛러닝의 해석성을 제공합니다.",
      "upvotes": 2,
      "discussionId": "67fcb3594a92187863e732fa"
    },
    "publishedAt": "2025-04-10T21:24:03.000Z",
    "title": "SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder\n  Guardrails for Precision Unlearning in LLMs",
    "summary": "Machine unlearning is a promising approach to improve LLM safety by removing\nunwanted knowledge from the model. However, prevailing gradient-based\nunlearning methods suffer from issues such as high computational costs,\nhyperparameter instability, poor sequential unlearning capability,\nvulnerability to relearning attacks, low data efficiency, and lack of\ninterpretability. While Sparse Autoencoders are well-suited to improve these\naspects by enabling targeted activation-based unlearning, prior approaches\nunderperform gradient-based methods. This work demonstrates that, contrary to\nthese earlier findings, SAEs can significantly improve unlearning when employed\ndynamically. We introduce Dynamic DAE Guardrails (DSG), a novel\nmethod for precision unlearning that leverages principled feature selection and\na dynamic classifier. Our experiments show DSG substantially outperforms\nleading unlearning methods, achieving superior forget-utility trade-offs. DSG\naddresses key drawbacks of gradient-based approaches for unlearning -- offering\nenhanced computational efficiency and stability, robust performance in\nsequential unlearning, stronger resistance to relearning attacks, better data\nefficiency including zero-shot settings, and more interpretable unlearning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08192.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64755a83e0b188d3cb2579d8",
      "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
      "fullname": "Aashiq Muhamed",
      "name": "aashiqmuhamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01883",
      "authors": [
        {
          "_id": "67fcb50ea69150c25fb4b645",
          "name": "Aashiq Muhamed",
          "hidden": false
        },
        {
          "_id": "67fcb50ea69150c25fb4b646",
          "name": "Mona Diab",
          "hidden": false
        },
        {
          "_id": "67fcb50ea69150c25fb4b647",
          "name": "Virginia Smith",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T16:40:43.000Z",
      "submittedOnDailyAt": "2025-04-14T05:41:34.796Z",
      "title": "CoRAG: 협업 시그니처 데이터 생성",
      "submittedOnDailyBy": {
        "_id": "64755a83e0b188d3cb2579d8",
        "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
        "isPro": false,
        "fullname": "Aashiq Muhamed",
        "user": "aashiqmuhamed",
        "type": "user"
      },
      "summary": "レビュアル・アウゲーション(RAG) 모델은 지식집중형 태스크에 특화된 모델이며, 특히 フィーチャーショット 학습의 제약 아래도 뛰어난 성능을 보입니다. 저희는 CoRAG를 소개합니다. CoRAG은 RAG를 확장한 프레임워크로, 사용자는 공유 패스옹 스トア를 공유하여 공유 모델을 공동 학습합니다. CoRAG의 평가에 대한 내용은 CRAB를 소개합니다. CRAB는 공동 학습의 동일성 개방 도메인 질문 응답 벤치마크입니다. 저희의 실험은 CoRAG가 낮은 리소스 스케일러로 파라미터적 공동 학습 방법 및 지역적으로 학습된 RAG 모델을 일치시키고 뛰어난 성능을 보입니다는 것을 보여주고 있습니다. 진행된 분석은 공유 스토어 내 관련성 있는 패스옹의 중요성, 관련성 없는 패스옹의 놀라운 베나스, 하드 부정의 성능에 부정적인 영향을 미치는 가능성을 밝혀줍니다. 이는 공동 RAG에서 새로운 고려를 도입합니다: 공동으로 풍부화된 지식 기반의 활용과 다른 사용자로부터 유해한 패스옹의 행동의 위험의 트레이드오프입니다. 저희의 발견은 CoRAG의 가능성에 대한 강조와 주요 설계 문제 및 미래 연구의 잠재적인 길을 밝혀줍니다.",
      "upvotes": 2,
      "discussionId": "67fcb510a69150c25fb4b6b1"
    },
    "publishedAt": "2025-04-02T12:40:43.000Z",
    "title": "CoRAG: Collaborative Retrieval-Augmented Generation",
    "summary": "Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive\ntasks, especially under few-shot learning constraints. We introduce CoRAG, a\nframework extending RAG to collaborative settings, where clients jointly train\na shared model using a collaborative passage store. To evaluate CoRAG, we\nintroduce CRAB, a benchmark for collaborative homogeneous open-domain question\nanswering. Our experiments demonstrate that CoRAG consistently outperforms both\nparametric collaborative learning methods and locally trained RAG models in\nlow-resource scenarios. Further analysis reveals the critical importance of\nrelevant passages within the shared store, the surprising benefits of\nincorporating irrelevant passages, and the potential for hard negatives to\nnegatively impact performance. This introduces a novel consideration in\ncollaborative RAG: the trade-off between leveraging a collectively enriched\nknowledge base and the potential risk of incorporating detrimental passages\nfrom other clients. Our findings underscore the viability of CoRAG, while also\nhighlighting key design challenges and promising avenues for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01883.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64755a83e0b188d3cb2579d8",
      "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
      "fullname": "Aashiq Muhamed",
      "name": "aashiqmuhamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.05303",
      "authors": [
        {
          "_id": "67fcbbe8daf0cf6803943949",
          "user": {
            "_id": "6492bf9681d93008eb33f167",
            "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
            "isPro": false,
            "fullname": "Sai Kumar Dwivedi",
            "user": "saidwivedi",
            "type": "user"
          },
          "name": "Sai Kumar Dwivedi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:12.532Z",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394a",
          "name": "Dimitrije Antić",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394b",
          "name": "Shashank Tripathi",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394c",
          "name": "Omid Taheri",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394d",
          "name": "Cordelia Schmid",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394e",
          "name": "Michael J. Black",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394f",
          "name": "Dimitrios Tzionas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:59:33.000Z",
      "submittedOnDailyAt": "2025-04-14T06:14:23.936Z",
      "title": "InteractVLM: 3D 상호작용 추론을 2D 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM: 3차원 상호작용 추론을 2차원 기초 모델에서\n\nInteractVLM:",
      "submittedOnDailyBy": {
        "_id": "6492bf9681d93008eb33f167",
        "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
        "isPro": false,
        "fullname": "Sai Kumar Dwivedi",
        "user": "saidwivedi",
        "type": "user"
      },
      "summary": "InteractVLM는 인간이나 물체의 3D 접촉점들을 단일의 자연환경 이미지에서 추론하는 새로운 방법입니다. 이로 인해 인간과 물체의 3D적인 공통구조를 정확히 재구성할 수 있습니다. 이는 가려짐, 깊이의 불확실성, 물체 모양의 극한 변화로 인해 복잡한 문제를 해결하는 데 어려움이 있습니다. 현재의 방법들은 비싼 동작 감지 시스템으로부터 3D 접촉 어노테이션이나冗長한 손동작 라벨링을 통해 얻습니다. 이로 인해 scalability와 일반화 능력이 제한되어 있습니다. 이러한 문제를 해결하기 위해, InteractVLM은 광범위한 시각 언어 모델(VLMs)의 시각적 지식을 활용하고, 제한된 3D 접촉 데이터에 대한 미세 조정을 통해 구현합니다. 그러나 이러한 모델을 직접 적용하는 것은 2D에서의 논리를 수행하는 것이 복잡합니다. 따라서, InteractVLM은 새로운 Render-Localize-Lift 모듈을 도입하고 있습니다. 이는 (1) 3D의 몸체와 물체의 표면들을 여러 점 촬영으로 2D 공간에 삽입, (2) 새로운 여러 점 촬영 위치 추정 모델(MV-Loc)을 학습하여 2D에서의 접촉을 추론, (3) 이들을 3D로 업로드하여 구현합니다. 또한, InteractVLM은 새로운 Semantic Human Contact estimation 과제를 제안하고 있습니다. 이로 인해 인간과의 접촉 예측은 물체의 세ман틱에 기반하여 수행되고, 풍부한 상호작용 모델링이 가능해집니다. InteractVLM은 접촉 추론과 3D 재구성에서 현재의 방법을 초월하고, 자연환경 이미지로부터의 3D 재구성을 지원합니다. 코드와 모델은 https://interactvlm.is.tue.mpg.de 에서 사용 가능합니다.",
      "upvotes": 0,
      "discussionId": "67fcbbeadaf0cf68039439b9",
      "projectPage": "https://interactvlm.is.tue.mpg.de/",
      "githubRepo": "https://github.com/saidwivedi/InteractVLM"
    },
    "publishedAt": "2025-04-07T13:59:33.000Z",
    "title": "InteractVLM: 3D Interaction Reasoning from 2D Foundational Models",
    "summary": "We introduce InteractVLM, a novel method to estimate 3D contact points on\nhuman bodies and objects from single in-the-wild images, enabling accurate\nhuman-object joint reconstruction in 3D. This is challenging due to occlusions,\ndepth ambiguities, and widely varying object shapes. Existing methods rely on\n3D contact annotations collected via expensive motion-capture systems or\ntedious manual labeling, limiting scalability and generalization. To overcome\nthis, InteractVLM harnesses the broad visual knowledge of large Vision-Language\nModels (VLMs), fine-tuned with limited 3D contact data. However, directly\napplying these models is non-trivial, as they reason only in 2D, while\nhuman-object contact is inherently 3D. Thus we introduce a novel\nRender-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D\nspace via multi-view rendering, (2) trains a novel multi-view localization\nmodel (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D.\nAdditionally, we propose a new task called Semantic Human Contact estimation,\nwhere human contact predictions are conditioned explicitly on object semantics,\nenabling richer interaction modeling. InteractVLM outperforms existing work on\ncontact estimation and also facilitates 3D reconstruction from an in-the wild\nimage. Code and models are available at https://interactvlm.is.tue.mpg.de.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05303.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6492bf9681d93008eb33f167",
      "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
      "fullname": "Sai Kumar Dwivedi",
      "name": "saidwivedi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05262",
      "authors": [
        {
          "_id": "67fcc9a980568c7ef6180dcb",
          "name": "Yang Yan",
          "hidden": false
        },
        {
          "_id": "67fcc9a980568c7ef6180dcc",
          "name": "Yu Lu",
          "hidden": false
        },
        {
          "_id": "67fcc9a980568c7ef6180dcd",
          "name": "Renjun Xu",
          "hidden": false
        },
        {
          "_id": "67fcc9a980568c7ef6180dce",
          "name": "Zhenzhong Lan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T16:57:10.000Z",
      "submittedOnDailyAt": "2025-04-14T07:11:56.706Z",
      "title": "디피덴스레벨의 LLMs는 사실상 기본적인 加算을 이해합니다가 아닙니다? 룰 학습과 메모리화의 비교",
      "submittedOnDailyBy": {
        "_id": "62ce6dd785cfd21c04c7e6f5",
        "avatarUrl": "/avatars/89837a5dea6e2d753d59caad142bed4a.svg",
        "isPro": false,
        "fullname": "ZhenzhongLan",
        "user": "DannyLan",
        "type": "user"
      },
      "summary": "베치마크 스코어가 높더라도, 대규모 언어 모델(LLMs)은 간단한 문제를 실패하고 중요한 질문이 제기되고 있습니다: LLMs는 수학의 원리를 배우는지, 아니면 패턴의 기억에만 의존하는 것인가? 최근의 연구보다 복잡한 베치마크를 설계하는 것이 아니라, 기본적인 두 개의 정수의 더하기(0부터 2^64)를 사용하여 두 가지 핵심적인 특성을 조사하고 있습니다: 교환성(A+B=B+A)과 구성적 일반화(동형식 표지 매핑을 사용하며, 예를 들어 7→y). 현재의 최상위 LLMs는 숫자 더하기에서 73.8-99.8%의 정확도를 달성하지만, 표지 매핑에서 성능이 7.5% 이하로 떨어짐으로써 학습된 규칙의 일반화에 실패하고 있습니다. 숫자의 개수에 대한 비선형적인 성능 스케일링과, 빈번한 교환성의 파괴(1,700 이상의 A+B≠B+A의 경우)이 추가적으로 설명하고 있습니다. 더하기 규칙의 명확한 제공은 평균 81.2%의 성능 저하를 초래하고, 자동 설명은 기준 정확도를 유지하며, LLM의 계산 처리가 인간이 정한 원리에 맞지 않는 것을 보여줍니다. 우리가 발견한 것은 현재의 LLMs는 기억 패턴을 기반으로 하여 실제 규칙 학습에 의존하고 있으며, 구조적 제한과 새로운 접근 방식의 필요성을 밝혀냅니다.",
      "upvotes": 0,
      "discussionId": "67fcc9aa80568c7ef6180e24"
    },
    "publishedAt": "2025-04-07T12:57:10.000Z",
    "title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\n  vs. Memorization in Large Language Models",
    "summary": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n(0 to 2^{64}), probing two core properties: commutativity (A+B=B+A) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., 7\nrightarrow y). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to leq7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of A+B neq B+A) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05262.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ce6dd785cfd21c04c7e6f5",
      "avatarUrl": "/avatars/89837a5dea6e2d753d59caad142bed4a.svg",
      "fullname": "ZhenzhongLan",
      "name": "DannyLan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]