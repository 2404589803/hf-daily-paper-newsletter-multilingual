[
  {
    "paper": {
      "id": "2507.13334",
      "authors": [
        {
          "_id": "6879aad021b37e676c8e406b",
          "user": {
            "_id": "63120517ae8896941da4c5da",
            "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
            "isPro": false,
            "fullname": "Lingrui Mei",
            "user": "Chevalier",
            "type": "user"
          },
          "name": "Lingrui Mei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:58.423Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406c",
          "user": {
            "_id": "671f9cd9ff056a1b49444f37",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/B3Z9oiFb79Gi-_YXKP13u.png",
            "isPro": false,
            "fullname": "duoduo yao",
            "user": "Theodyy",
            "type": "user"
          },
          "name": "Jiayu Yao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:48.326Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406d",
          "user": {
            "_id": "656ad93853703dd78f3de7b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/r6VB6ICND1td3wFOy8pnz.jpeg",
            "isPro": false,
            "fullname": "YuyaoGe",
            "user": "YuyaoGe",
            "type": "user"
          },
          "name": "Yuyao Ge",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:54.689Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406e",
          "name": "Yiwei Wang",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406f",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4070",
          "name": "Yujun Cai",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4071",
          "name": "Jiazhi Liu",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4072",
          "user": {
            "_id": "6720cf97a0396f933ec93ab8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NAdPIISe9-TYGGM0nAgsb.png",
            "isPro": false,
            "fullname": "Li Max",
            "user": "LImax72",
            "type": "user"
          },
          "name": "Mingyu Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:56.619Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4073",
          "name": "Zhong-Zhi Li",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4074",
          "user": {
            "_id": "662383a20edeabfe3b64a6a5",
            "avatarUrl": "/avatars/a76da726002d853dd08a51a6af6311d9.svg",
            "isPro": false,
            "fullname": "Duzhen Zhang",
            "user": "ShowerMaker",
            "type": "user"
          },
          "name": "Duzhen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:50.488Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4075",
          "user": {
            "_id": "669e27902dbf53ccd23ae47f",
            "avatarUrl": "/avatars/5c193b542dcfe2e64467fa5c686f3e20.svg",
            "isPro": false,
            "fullname": "chenlin",
            "user": "tvstfe",
            "type": "user"
          },
          "name": "Chenlin Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:52.410Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4076",
          "name": "Jiayi Mao",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4077",
          "name": "Tianze Xia",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4078",
          "name": "Jiafeng Guo",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4079",
          "name": "Shenghua Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:50:36.000Z",
      "submittedOnDailyAt": "2025-07-18T00:52:14.092Z",
      "title": "라르제트 언어 모델의 컨텍스트 학습 조사",
      "submittedOnDailyBy": {
        "_id": "63120517ae8896941da4c5da",
        "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
        "isPro": false,
        "fullname": "Lingrui Mei",
        "user": "Chevalier",
        "type": "user"
      },
      "summary": "LLM의 성능은 추론 시 제공되는 컨텍스트 정보에 기초하여 근본적으로 결정된다. 본 조사에서는 단순한 Prompt 설계를 초월하고 LLM의 정보부하의 체계적인 최적화를 통합하는 공식적인 학문인 \"컨텍스트 공학\"을 소개합니다. 컨텍스트 공학을 기초적인 요소와 복잡한 구현으로 분해한 상세한 분류를 제안합니다. 먼저 기초적인 요소에 대해 검토합니다: 컨텍스트 검색 및 생성, 컨텍스트 처리 및 관리. 다음으로, 이러한 요소가 구조적으로 통합되어 복잡한 시스템 구현을 생성하는 방법에 대해 조사합니다: 검색 어셈블리 보호 생성(RAG), 메모리 시스템 및 도구 통합 논리, 다중 어셈블리 시스템. 1300점 이상의 연구 논문을 체계적으로 분석한 후, 본 조사는 기술적인 프로그램을 구축하며, 컨텍스트 엔지니어링에 의한 발전된 모델이 복잡한 컨텍스트를 이해하기 위한 우수한 효율성을 보여주는 동시에 동일한 복잡한 긴 문장의 생성에 있어 명확한 한계가 있음을 밝혀줍니다. 이러한 한계를 해결하는 것은 미래 연구의 정의적인 우선 순위입니다. 최종적으로, 이 조사는 컨텍스트 정보를 읽는 AI의 발전을 지원하는 연구자 및 엔지니어에게 제공되는 통합적인 프레임워크입니다.",
      "upvotes": 65,
      "discussionId": "6879aad021b37e676c8e407a",
      "githubRepo": "https://github.com/Meirtz/Awesome-Context-Engineering",
      "ai_summary": "Context Engineering systematically optimizes information payloads for Large Language Models, addressing gaps in generating sophisticated, long-form outputs.",
      "ai_keywords": [
        "Context Engineering",
        "context retrieval",
        "context generation",
        "context processing",
        "context management",
        "retrieval-augmented generation",
        "memory systems",
        "tool-integrated reasoning",
        "multi-agent systems"
      ],
      "githubStars": 164
    },
    "publishedAt": "2025-07-17T13:50:36.000Z",
    "title": "A Survey of Context Engineering for Large Language Models",
    "summary": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1300 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13334.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63120517ae8896941da4c5da",
      "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
      "fullname": "Lingrui Mei",
      "name": "Chevalier",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.13348",
      "authors": [
        {
          "_id": "6879ba2021b37e676c8e40c9",
          "name": "Senqiao Yang",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40ca",
          "name": "Junyi Li",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40cb",
          "name": "Xin Lai",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40cc",
          "name": "Bei Yu",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40cd",
          "name": "Hengshuang Zhao",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40ce",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:59:55.000Z",
      "submittedOnDailyAt": "2025-07-18T01:47:35.200Z",
      "title": "VisionThink: 레イノルス 학에 의한 스마트하고 효율적인 시각 언어 모델\n\n(Note: The original text \"VisionThink: レイノルス学によるスマートなおよび効率的な視覚言語モデル\" is a mix of English and Japanese. The translation provided above is in Korean. If you intended for the entire text to be in Korean, please provide the correct English text for translation.)",
      "submittedOnDailyBy": {
        "_id": "6527b7280ae663e384eb8499",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b7280ae663e384eb8499/73yF3eu2cUx7jVZrhXnXx.jpeg",
        "isPro": false,
        "fullname": "Senqiao Yang",
        "user": "Senqiao",
        "type": "user"
      },
      "summary": "최근의 시각 언어 모델(VLMs)의 발전은 시각 토큰의 수를 늘리며, 그 토큰이 문맥 토큰보다 더 길어졌으며, 성능을 향상시키는 데에 기여하고 있습니다. 그러나 우리는 대부분의 실제적인 시나리오에서 이러한 긴 시각 토큰이 필요하지 않습니다. OCR 관련 작업의 일부에서 성능이 크게 떨어지지만, 일반적인 VQA 작업에서 1/4의 해상도에서도 정확한 동작을 합니다. 따라서, 우리는 다양한 해상도별로 동적으로 처리하는 새로운 패러다임과 시각 토큰의 압축의 새로운 방법을 제안하고 있습니다. 이 방법은 다운 샘플링된 이미지에서 시작하여 문제 해결에 충분한지 지능적으로 판단합니다. 그렇지 않은 경우, 모델은 고해상도 이미지를 요구하는 특수 토큰을 출력합니다. 기존의 효율적인 VLM 방법과 비교하여, VisionThink은 개별적으로 토큰을 압축할지 말지 자동으로 판단합니다. OCR 관련 작업에서 이러한 능력이 강하며, 간단한 작업에서는 시각 토큰을 크게 줄일 수 있습니다. 강화 학습을 사용하여, LLM-as-Judge 전략을 제안하고, 일반적인 VQA 작업에 실제적으로 적용했습니다. 또한, 안정적인 해상도의 리사이징 호출 비율을 달성하기 위해 보상 함수와 보상 구조를 엄격하게 설계했습니다. 확장 검증은 우리의 방법의 위상성, 효율성, 효과성을 보여주었습니다. 코드는 https://github.com/dvlab-research/VisionThink에서 액세스할 수 있습니다.",
      "upvotes": 38,
      "discussionId": "6879ba2121b37e676c8e40cf",
      "githubRepo": "https://github.com/dvlab-research/VisionThink",
      "ai_summary": "VisionThink dynamically adjusts image resolution and visual token processing for efficient and effective vision-language tasks, improving performance on OCR tasks while reducing token usage in simpler tasks.",
      "ai_keywords": [
        "vision-language models",
        "visual tokens",
        "text tokens",
        "downsampled image",
        "smart decision-making",
        "special token",
        "Efficient VLM",
        "token compression",
        "reinforcement learning",
        "LLM-as-Judge",
        "reward function",
        "penalty mechanism",
        "image resize call ratio"
      ],
      "githubStars": 25
    },
    "publishedAt": "2025-07-17T13:59:55.000Z",
    "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning",
    "summary": "Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13348.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6527b7280ae663e384eb8499",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b7280ae663e384eb8499/73yF3eu2cUx7jVZrhXnXx.jpeg",
      "fullname": "Senqiao Yang",
      "name": "Senqiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.13347",
      "authors": [
        {
          "_id": "6879b78a21b37e676c8e40b1",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b2",
          "name": "Jianjun Zhou",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b3",
          "name": "Haoyi Zhu",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b4",
          "name": "Wenzheng Chang",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b5",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b6",
          "name": "Zizun Li",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b7",
          "name": "Junyi Chen",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b8",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b9",
          "name": "Chunhua Shen",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40ba",
          "name": "Tong He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:59:53.000Z",
      "submittedOnDailyAt": "2025-07-18T01:26:57.410Z",
      "title": "π^3: 시각적 기하학적 학습의 순서와 값의 동등성 확인\n\n(Note: The original text contains a non-standard character \"える\" which is not commonly used in Korean. It has been translated as \"확인\" for clarity and accuracy.)",
      "submittedOnDailyBy": {
        "_id": "6747ede3a9c72aebe1322382",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/inILqQ05sESbYLdsEldJ_.png",
        "isPro": false,
        "fullname": "Tong He",
        "user": "tonghe90",
        "type": "user"
      },
      "summary": "pi^3는 결정적인 고정된 기준점으로부터 의존성을 깨고, 시각적인 기하학적 재구성에 새로운 접근 방식을 제공하는 전방 뉴럴 네트워크입니다. 기존의 방법들은 재구성이 특정한 시각에서 고정되어 있으며, 이 경우 최적의 기준이 아닐 때 불안정성 또는 실패를招く 도입 편향을 초래합니다. 반면, pi^3는 기준 프레임을 사용하지 않고, 완전한 교환대칭성을 가지는 아키텍처를 사용하여, Affine 불변성을 갖는 카메라의 자세와 Scale 불변성을 갖는 지역점맵을 예측합니다. 이 설계는 입력 순서에 대한 강한 강건성을 가지고 있으며, 높은 스케일성능을 갖습니다. 이러한 우수한 점들을 가지고 있기 때문에, 우리의 간단하고 편향이 없는 접근 방식은 촬영 자세의 추정, Monocular/Video의 Depth 측정, 그리고 밀집점맵의 재구성 등 광범위한 태스크에서 가장 선진적인 성능을 발휘합니다. 코드와 모델은 공개적으로 제공됩니다.",
      "upvotes": 30,
      "discussionId": "6879b78b21b37e676c8e40bb",
      "projectPage": "https://yyfz.github.io/pi3/",
      "githubRepo": "https://github.com/yyfz/Pi3",
      "ai_summary": "A permutation-equivariant neural network, $\\pi^3$, reconstructs visual geometry without a fixed reference view, achieving state-of-the-art performance in camera pose estimation, depth estimation, and point map reconstruction.",
      "ai_keywords": [
        "feed-forward neural network",
        "permutation-equivariant architecture",
        "affine-invariant",
        "scale-invariant",
        "camera pose estimation",
        "monocular depth estimation",
        "video depth estimation",
        "dense point map reconstruction"
      ],
      "githubStars": 104
    },
    "publishedAt": "2025-07-17T13:59:53.000Z",
    "title": "π^3: Scalable Permutation-Equivariant Visual Geometry Learning",
    "summary": "We introduce pi^3, a feed-forward neural network that offers a novel\napproach to visual geometry reconstruction, breaking the reliance on a\nconventional fixed reference view. Previous methods often anchor their\nreconstructions to a designated viewpoint, an inductive bias that can lead to\ninstability and failures if the reference is suboptimal. In contrast, pi^3\nemploys a fully permutation-equivariant architecture to predict\naffine-invariant camera poses and scale-invariant local point maps without any\nreference frames. This design makes our model inherently robust to input\nordering and highly scalable. These advantages enable our simple and bias-free\napproach to achieve state-of-the-art performance on a wide range of tasks,\nincluding camera pose estimation, monocular/video depth estimation, and dense\npoint map reconstruction. Code and models are publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13347.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6747ede3a9c72aebe1322382",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/inILqQ05sESbYLdsEldJ_.png",
      "fullname": "Tong He",
      "name": "tonghe90",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.13332",
      "authors": [
        {
          "_id": "6879b01621b37e676c8e40a8",
          "user": {
            "_id": "66214b4e4991d64ad0e28675",
            "avatarUrl": "/avatars/2574928aab7e45bc581c567d556a4cfd.svg",
            "isPro": false,
            "fullname": "Zhouqi Hua",
            "user": "ZhouqiHUA",
            "type": "user"
          },
          "name": "Zhouqi Hua",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:38.507Z",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40a9",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40aa",
          "name": "Chengqi Lyu",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ab",
          "user": {
            "_id": "6601196cc91ba4c08ad6e270",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
            "isPro": false,
            "fullname": "yuzhe gu",
            "user": "vanilla1116",
            "type": "user"
          },
          "name": "Yuzhe Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:36.203Z",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ac",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ad",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ae",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:50:07.000Z",
      "submittedOnDailyAt": "2025-07-18T00:59:46.053Z",
      "title": "「게임 제작：トーリング머신의 제작자는 길이에 일반화 가능합니다」",
      "submittedOnDailyBy": {
        "_id": "6601196cc91ba4c08ad6e270",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
        "isPro": false,
        "fullname": "yuzhe gu",
        "user": "vanilla1116",
        "type": "user"
      },
      "summary": "길이의 일반화 능력, 즉, 훈련기간에 발견되지 않는 긴 시퀀스의 문제를 해결하는 능력은 Transformer 기반의 대규모 언어 모델(LLM)에 있어 핵심적인 문제입니다. 기존의 연구는 주로 수치 계산이나 기호 연산의 데이터 주도 접근 방식을 중심으로 진행되어 있으며, 이러한 접근 방식은 특정 태스크에 특화되어 전체적인 성능이 제한되어 있습니다. 더 일반적인 해결책을 찾기 위해, 본 연구는 알고리즘이 해결할 수 있는 계산 가능한 이유의 문제를 초점을 두고 있습니다. 이 관점에서 본 연구에서는 토라イン 머신의 실행 프로세스를 컴퓨터 프로그램으로 모방하는 코지oning 데이터를 통합하여 토라イン 머신 임베디드 학습(TAIL)을 제안하여 LLM의 길이의 일반화 능력을 향상시키는 것을 목표로 합니다. TAIL은 코지oning을原子적인 상태에 선형적으로 확장하고, 짧은 학습을 줄이고, 동적인 긴 거리 데이터 접근의 어려움을 줄이기 위해 명시적인 메모리 피치 메커니즘을 도입합니다. TAIL의 신뢰성과 일반성을 증명하기 위해, 8가지 알고리즘과 18가지 태스크를 포함하는 어려운 합성 데이터 세트를 구축했습니다. 합성 데이터만 사용함으로써, TAIL은 Qwen2.5-7B의 길이의 일반화 능력과 다양한 태스크의 성능을 크게 향상시키고, 기존의 방법과 DeepSeek-R1을 초월했습니다. 실험 결과를 통해 명확히 드러난 것은 TAIL의 길이의 일반화에서 토라イン 머신의 기본 개념이 생각의 스타일보다 필수적임을 알 수 있습니다. 이 모델은 그 注意층에서 토라イン 머신의 특성에 일치하는 읽기 쓰기 행동을 보여줍니다. 본 연구는 LLM의 이유 학습에 대한 미래의 연구의 바람직한 방향을 제공합니다.",
      "upvotes": 30,
      "discussionId": "6879b01721b37e676c8e40af",
      "ai_summary": "TAIL, a method that imitates Turing Machine execution processes, enhances the length generalization and performance of LLMs by synthesizing chain-of-thought data and reducing shortcut learning.",
      "ai_keywords": [
        "Transformer-based large language models",
        "length generalization",
        "Turing MAchine Imitation Learning",
        "TAIL",
        "chain-of-thoughts",
        "Turing Machine",
        "synthetic dataset",
        "Qwen2.5-7B",
        "read-and-write behaviors",
        "attention layers"
      ]
    },
    "publishedAt": "2025-07-17T13:50:07.000Z",
    "title": "The Imitation Game: Turing Machine Imitator is Length Generalizable\n  Reasoner",
    "summary": "Length generalization, the ability to solve problems of longer sequences than\nthose observed during training, poses a core challenge of Transformer-based\nlarge language models (LLM). Although existing studies have predominantly\nfocused on data-driven approaches for arithmetic operations and symbolic\nmanipulation tasks, these approaches tend to be task-specific with limited\noverall performance. To pursue a more general solution, this paper focuses on a\nbroader case of reasoning problems that are computable, i.e., problems that\nalgorithms can solve, thus can be solved by the Turing Machine. From this\nperspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to\nimprove the length generalization ability of LLMs. TAIL synthesizes\nchain-of-thoughts (CoT) data that imitate the execution process of a Turing\nMachine by computer programs, which linearly expands the reasoning steps into\natomic states to alleviate shortcut learning and explicit memory fetch\nmechanism to reduce the difficulties of dynamic and long-range data access in\nelementary operations. To validate the reliability and universality of TAIL, we\nconstruct a challenging synthetic dataset covering 8 classes of algorithms and\n18 tasks. Without bells and whistles, TAIL significantly improves the length\ngeneralization ability as well as the performance of Qwen2.5-7B on various\ntasks using only synthetic data, surpassing previous methods and DeepSeek-R1.\nThe experimental results reveal that the key concepts in the Turing Machine,\ninstead of the thinking styles, are indispensable for TAIL for length\ngeneralization, through which the model exhibits read-and-write behaviors\nconsistent with the properties of the Turing Machine in their attention layers.\nThis work provides a promising direction for future research in the learning of\nLLM reasoning from synthetic data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13332.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12841",
      "authors": [
        {
          "_id": "6879bba621b37e676c8e4195",
          "name": "Yiming Ren",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4196",
          "name": "Zhiqiang Lin",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4197",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4198",
          "name": "Gao Meng",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4199",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419a",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419b",
          "name": "Zicheng Lin",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419c",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419d",
          "name": "Yujiu Yang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419e",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419f",
          "user": {
            "_id": "642e3bcb958faf258a40e89c",
            "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
            "isPro": false,
            "fullname": "Ruihang Chu",
            "user": "Ruihang",
            "type": "user"
          },
          "name": "Ruihang Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:30.538Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T07:04:05.000Z",
      "submittedOnDailyAt": "2025-07-18T02:04:48.913Z",
      "title": "AnyCap 프로젝트: 한 개의 통일된 프레임워크, 데이터셋, 벤치마크이지만, 제어 가능한 전방위 캡칭",
      "submittedOnDailyBy": {
        "_id": "642e3bcb958faf258a40e89c",
        "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
        "isPro": false,
        "fullname": "Ruihang Chu",
        "user": "Ruihang",
        "type": "user"
      },
      "summary": "제어 가능한 캡티닝은 정밀한 다 모델 어레이먼트와 지시에 따라 필요한 것이지만, 현재의 모델은 일반적으로 미세한 제어와 신뢰성 있는 평가 프로토콜이 부족합니다. 이를 채우기 위해, 우리는 AnyCap 프로젝트를 소개합니다. 이것은 모델, 데이터셋, 평가를 통합한 통합적인 해결책입니다. AnyCapModel (ACM)을 소개합니다. 이것은 가벼운 프레임워크이며, 현재의 기초 모델의 제어 가능성을 향상시키기 위해 재학습을 피할 수 있습니다. ACM은 기초 모델의 원의 캡티닝을 재활용하고, 사용자 지시와 모델 특성을 조합하여 개선된 캡티닝을 생성합니다. 제어 가능한 다 모델 캡티닝의 데이터 부족을 보완하기 위해, AnyCapDataset (ACD)을 구축했습니다. 이는 3 모델, 28 사용자 지시 타입, 300k의 고품질 데이터 입력을 기록하고 있습니다. 또한, AnyCapEval을 제안했습니다. 이는 콘텐츠의 정확성과 스타일의 충실성을 분리하여, 제어 가능한 캡티닝의 신뢰성 있는 평가 지표를 제공합니다. ACM은 AnyCapEval에서 다양한 기초 모델의 캡티닝 품질을 명확히 향상시킵니다. 특히, ACM-8B는 GPT-4o의 콘텐츠 스코어를 45% 이상, 스타일 스코어를 12% 이상, MIA-Bench와 VidCapBench 등 일반적인 벤치마크에서도 큰 효과를 줍니다.",
      "upvotes": 27,
      "discussionId": "6879bba721b37e676c8e41a0",
      "githubRepo": "https://github.com/qishisuren123/AnyCap",
      "ai_summary": "The AnyCap Project introduces a framework, dataset, and evaluation protocol to enhance controllability and reliability in multimodal captioning.",
      "ai_keywords": [
        "AnyCapModel",
        "ACM",
        "omni-modal captioning",
        "AnyCapDataset",
        "ACD",
        "AnyCapEval",
        "content accuracy",
        "stylistic fidelity",
        "MIA-Bench",
        "VidCapBench"
      ],
      "githubStars": 27
    },
    "publishedAt": "2025-07-17T03:04:05.000Z",
    "title": "AnyCap Project: A Unified Framework, Dataset, and Benchmark for\n  Controllable Omni-modal Captioning",
    "summary": "Controllable captioning is essential for precise multimodal alignment and\ninstruction following, yet existing models often lack fine-grained control and\nreliable evaluation protocols. To address this gap, we present the AnyCap\nProject, an integrated solution spanning model, dataset, and evaluation. We\nintroduce AnyCapModel (ACM), a lightweight plug-and-play framework that\nenhances the controllability of existing foundation models for omni-modal\ncaptioning without retraining the base model. ACM reuses the original captions\nfrom base models while incorporating user instructions and modality features to\ngenerate improved captions. To remedy the data scarcity in controllable\nmultimodal captioning, we build AnyCapDataset (ACD), covering three modalities,\n28 user-instruction types, and 300\\,k high-quality data entries. We further\npropose AnyCapEval, a new benchmark that provides more reliable evaluation\nmetrics for controllable captioning by decoupling content accuracy and\nstylistic fidelity. ACM markedly improves caption quality across a diverse set\nof base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores\nby 45\\% and style scores by 12\\%, and it also achieves substantial gains on\nwidely used benchmarks such as MIA-Bench and VidCapBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12841.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e3bcb958faf258a40e89c",
      "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
      "fullname": "Ruihang Chu",
      "name": "Ruihang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.13344",
      "authors": [
        {
          "_id": "6879f3aa21b37e676c8e4202",
          "user": {
            "_id": "649958942ca6f96c8b8c1076",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649958942ca6f96c8b8c1076/olfLIqNryaog1nAnQPwkN.jpeg",
            "isPro": false,
            "fullname": "Yudong Jin",
            "user": "krahets",
            "type": "user"
          },
          "name": "Yudong Jin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:22.520Z",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4203",
          "name": "Sida Peng",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4204",
          "name": "Xuan Wang",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4205",
          "name": "Tao Xie",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4206",
          "name": "Zhen Xu",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4207",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4208",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4209",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e420a",
          "name": "Xiaowei Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649958942ca6f96c8b8c1076/G5xpU1DSr8smrbqbom9hU.mp4"
      ],
      "publishedAt": "2025-07-17T17:59:17.000Z",
      "submittedOnDailyAt": "2025-07-18T06:34:43.522Z",
      "title": "Diffuman4D: 4D 일관된 인간 시각 합성을 희소한 뷰 비디오로\n스펙트럴 시간 분산 모델을 사용합니다.",
      "submittedOnDailyBy": {
        "_id": "649958942ca6f96c8b8c1076",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649958942ca6f96c8b8c1076/olfLIqNryaog1nAnQPwkN.jpeg",
        "isPro": false,
        "fullname": "Yudong Jin",
        "user": "krahets",
        "type": "user"
      },
      "summary": "이 논문은 희귀한 비디오를 입력으로 받아 인간이 고품질의 시각합성 문제를 제기하고 있습니다. 기존의 방법들은 4D Difussion 모델을 사용하여 새로운 시각에서 비디오를 생성하여 관찰 부족의 문제를 해결하지만, 이러한 모델이 생성한 비디오는 주로 스펙트럴 시간의 일관성을 잃어 시각합성의 품질이 떨어집니다. 본 논문에서는 4D Difussion 모델의 스펙트럴 시간의 일관성을 높일 수 있는 새로운 슬라이딩 Iterative 방식의 디노이즈 프로세스를 제안합니다. 구체적으로는 각 비디오, 카메라의 상태, 인간의 상태를 포함하는 잠재 그리드를 정의하고, 스펙트럴 시간의 양쪽에 슬라이딩 윈도우를 사용하여 디노이즈를 교환하고, 최종적으로 목표의 시각에서 비디오를 디코딩합니다. 반복적인 슬라이딩 방식에서, 정보가 잠재 그리드의 너비만큼 충분히 흐름, Difussion 모델이 큰 수용野를 얻을 수 있으며, 출력의 4D 일관성을 높일 수 있습니다. 동시에 GPU의 메모리 소비를 허용할 수 있습니다. DNA-Rendering 및 ActorsHQ 데이터셋의 실험은 우리의 방법론이 고품질의 일관된 새로운 시각 비디오를 합성할 수 있으며, 현재의 방법론을 크게 초월할 수 있음을 보여줍니다. 프로젝트 페이지에는 상호작용 데모와 비디오 결과를 볼 수 있습니다: https://diffuman4d.github.io/.",
      "upvotes": 18,
      "discussionId": "6879f3ab21b37e676c8e420b",
      "projectPage": "https://diffuman4d.github.io/",
      "githubRepo": "https://github.com/zju3dv/Diffuman4D",
      "ai_summary": "A sliding iterative denoising process is proposed to enhance spatio-temporal consistency in 4D diffusion models for high-fidelity view synthesis from sparse-view videos.",
      "ai_keywords": [
        "4D diffusion models",
        "sliding iterative denoising",
        "latent grid",
        "image",
        "camera pose",
        "human pose",
        "spatio-temporal consistency",
        "GPU memory consumption",
        "DNA-Rendering",
        "ActorsHQ"
      ],
      "githubStars": 48
    },
    "publishedAt": "2025-07-17T13:59:17.000Z",
    "title": "Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos\n  with Spatio-Temporal Diffusion Models",
    "summary": "This paper addresses the challenge of high-fidelity view synthesis of humans\nwith sparse-view videos as input. Previous methods solve the issue of\ninsufficient observation by leveraging 4D diffusion models to generate videos\nat novel viewpoints. However, the generated videos from these models often lack\nspatio-temporal consistency, thus degrading view synthesis quality. In this\npaper, we propose a novel sliding iterative denoising process to enhance the\nspatio-temporal consistency of the 4D diffusion model. Specifically, we define\na latent grid in which each latent encodes the image, camera pose, and human\npose for a certain viewpoint and timestamp, then alternately denoising the\nlatent grid along spatial and temporal dimensions with a sliding window, and\nfinally decode the videos at target viewpoints from the corresponding denoised\nlatents. Through the iterative sliding, information flows sufficiently across\nthe latent grid, allowing the diffusion model to obtain a large receptive field\nand thus enhance the 4D consistency of the output, while making the GPU memory\nconsumption affordable. The experiments on the DNA-Rendering and ActorsHQ\ndatasets demonstrate that our method is able to synthesize high-quality and\nconsistent novel-view videos and significantly outperforms the existing\napproaches. See our project page for interactive demos and video results:\nhttps://diffuman4d.github.io/ .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649958942ca6f96c8b8c1076/G5xpU1DSr8smrbqbom9hU.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649958942ca6f96c8b8c1076",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649958942ca6f96c8b8c1076/olfLIqNryaog1nAnQPwkN.jpeg",
      "fullname": "Yudong Jin",
      "name": "krahets",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12508",
      "authors": [
        {
          "_id": "6879af4f21b37e676c8e409b",
          "user": {
            "_id": "65e919332fd9300c7eb96556",
            "avatarUrl": "/avatars/a826f7e14acf34603aa68e4fc27f12af.svg",
            "isPro": false,
            "fullname": "Yuncong Yang",
            "user": "yyuncong",
            "type": "user"
          },
          "name": "Yuncong Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:42.036Z",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409c",
          "name": "Jiageng Liu",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409d",
          "name": "Zheyuan Zhang",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409e",
          "name": "Siyuan Zhou",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409f",
          "name": "Reuben Tan",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e40a0",
          "name": "Jianwei Yang",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e40a1",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e40a2",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65e919332fd9300c7eb96556/sRj5beNxspccadVwKC_vj.mp4"
      ],
      "publishedAt": "2025-07-16T17:59:36.000Z",
      "submittedOnDailyAt": "2025-07-18T00:56:28.122Z",
      "title": "뇌의 여정: 인공지능 모델을 이용한 검증 시 스케일링에 의한 공간 논리\n\n(注意：虽然要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，这里提供了一个更符合韩文表达习惯的版本。)",
      "submittedOnDailyBy": {
        "_id": "65e919332fd9300c7eb96556",
        "avatarUrl": "/avatars/a826f7e14acf34603aa68e4fc27f12af.svg",
        "isPro": false,
        "fullname": "Yuncong Yang",
        "user": "yyuncong",
        "type": "user"
      },
      "summary": "3D 공간의 공간적 추론은 인간의 인지의 중심적인 부분이며, 댄스 가능한 태스크처럼 지도와 조작에 필수적이다. 그러나 최尖端의 시각 언어 모델(VLMs)은 단순한 태스크도 어려워진다: 2D 이미지만 보고 3D 역학의 내부 모델을 갖지 않는다. 이에, 우리는 비디오 디퓨전 기반의 제어 가능한 세계 모델과 결합하여 VLMs에 부족한 능력을 제공하는 데 사용할 수 있는 테스트 타임 스케일링 프레임워크인 \"MindJourney\"를 제안한다. VLMs는 세계 모델이 각 단계에서 합성한 상대적인 시각을 반복적으로 그려내고, 그 사이에 상호 탐색으로 수집된 다점의 증거로 이유를 만들어낸다. 최종 훈련 없이, 우리의 MindJourney는 대표적인 공간적 추론 벤치마크 SAT에서 평균 8% 이상의 성능 향상을 달성하고, VLMs와 세계 모델의 조합으로 테스트 타임 스케일링을 수행하는 것을 간단하고 강력한 3D 추론을 가능하게 하는 프로그와 플레이인 루트으로 보여준다. 또한, 우리의 방법은 강화 학습으로 훈련된 테스트 타임 추론 VLMs를 개선하고, 세계 모델을 테스트 타임 스케일링에 활용하는 방법의 가능성을 보여준다.",
      "upvotes": 10,
      "discussionId": "6879af5021b37e676c8e40a3",
      "projectPage": "https://umass-embodied-agi.github.io/MindJourney/",
      "githubRepo": "https://github.com/UMass-Embodied-AGI/MindJourney",
      "ai_summary": "MindJourney enhances vision-language models with 3D reasoning by coupling them with a video diffusion-based world model, achieving improved performance on spatial reasoning tasks without fine-tuning.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "world model",
        "video diffusion",
        "camera trajectory",
        "multi-view evidence",
        "spatial reasoning",
        "SAT benchmark",
        "reinforcement learning"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-07-16T13:59:36.000Z",
    "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning",
    "summary": "Spatial reasoning in 3D space is central to human cognition and indispensable\nfor embodied tasks such as navigation and manipulation. However,\nstate-of-the-art vision-language models (VLMs) struggle frequently with tasks\nas simple as anticipating how a scene will look after an egocentric motion:\nthey perceive 2D images but lack an internal model of 3D dynamics. We therefore\npropose MindJourney, a test-time scaling framework that grants a VLM with this\nmissing capability by coupling it to a controllable world model based on video\ndiffusion. The VLM iteratively sketches a concise camera trajectory, while the\nworld model synthesizes the corresponding view at each step. The VLM then\nreasons over this multi-view evidence gathered during the interactive\nexploration. Without any fine-tuning, our MindJourney achieves over an average\n8% performance boost on the representative spatial reasoning benchmark SAT,\nshowing that pairing VLMs with world models for test-time scaling offers a\nsimple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also\nimproves upon the test-time inference VLMs trained through reinforcement\nlearning, which demonstrates the potential of our method that utilizes world\nmodels for test-time scaling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e919332fd9300c7eb96556/sRj5beNxspccadVwKC_vj.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e919332fd9300c7eb96556",
      "avatarUrl": "/avatars/a826f7e14acf34603aa68e4fc27f12af.svg",
      "fullname": "Yuncong Yang",
      "name": "yyuncong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.13300",
      "authors": [
        {
          "_id": "6879c27e21b37e676c8e41a9",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41aa",
          "name": "Weiyuan Chen",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ab",
          "name": "Zhijian Xu",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ac",
          "name": "Manasi Patwardhan",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ad",
          "name": "Yixin Liu",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ae",
          "name": "Chengye Wang",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41af",
          "name": "Lovekesh Vig",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41b0",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:09:22.000Z",
      "submittedOnDailyAt": "2025-07-18T02:12:09.802Z",
      "title": "AbGen: 과학연구의 제거시험 설계 및 평가에서 대규모 언어 모델 평가\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "이곳에서는 과학 연구의 소멸 조사 설계 능력 평가를 위한 첫 번째 벤치마크 \"AbGen\"을 소개합니다. AbGen은 807편의 NLP 논문에서 1,500건의 전문가가 평가한 예를 구성하고 있습니다. 이 벤치마크에서, 주어진 연구 맥락에 기반하여 특정 모듈 또는 프로세스의 소멸 조사 설계를 생성하는 것을 LLM의 과제로 제시합니다. DeepSeek-R1-0528와 o4-mini 등 선진적인 LLM의 평가에 따라, 소멸 조사 설계의 중요성, 충실성, 그리고 타당성에 대해 모델과 인간 전문가 사이에서 분명한 성능 차이가 존재함을 확인했습니다. 또한, 현재의 자동 평가 방법은 인간 평가와 비교하여 명백한 신뢰도가 부족합니다. 이러한 점을 더 자세히 조사하기 위해, 자동 평가 시스템의 신뢰도를 평가하기 위한 메타 평가 벤치마크 \"AbGen-Eval\"을 개발했습니다. AbGen-Eval에서, 복잡한 과학 작업의 LLM 성능을 측정하는 일반적인 자동 평가 시스템의 신뢰도를 평가하기 위해, 다양한 LLM-as-Judge 시스템을 조사하고, 향후 연구에서 효율적이고 신뢰성 있는 LLM 기반 평가 시스템의 개발에 도움이 될 수 있는 지침을 제공합니다.",
      "upvotes": 9,
      "discussionId": "6879c27f21b37e676c8e41b1",
      "githubRepo": "https://github.com/yale-nlp/AbGen",
      "ai_summary": "AbGen evaluates LLMs in designing ablation studies for scientific research, revealing performance gaps compared to human experts and highlighting the unreliability of current automated evaluation methods.",
      "ai_keywords": [
        "LLMs",
        "ablation studies",
        "NLP papers",
        "DeepSeek-R1-0528",
        "o4-mini",
        "AbGen-Eval",
        "LLM-as-Judge"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-07-17T13:09:22.000Z",
    "title": "AbGen: Evaluating Large Language Models in Ablation Study Design and\n  Evaluation for Scientific Research",
    "summary": "We introduce AbGen, the first benchmark designed to evaluate the capabilities\nof LLMs in designing ablation studies for scientific research. AbGen consists\nof 1,500 expert-annotated examples derived from 807 NLP papers. In this\nbenchmark, LLMs are tasked with generating detailed ablation study designs for\na specified module or process based on the given research context. Our\nevaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a\nsignificant performance gap between these models and human experts in terms of\nthe importance, faithfulness, and soundness of the ablation study designs.\nMoreover, we demonstrate that current automated evaluation methods are not\nreliable for our task, as they show a significant discrepancy when compared to\nhuman assessment. To better investigate this, we develop AbGen-Eval, a\nmeta-evaluation benchmark designed to assess the reliability of commonly used\nautomated evaluation systems in measuring LLM performance on our task. We\ninvestigate various LLM-as-Judge systems on AbGen-Eval, providing insights for\nfuture research on developing more effective and reliable LLM-based evaluation\nsystems for complex scientific tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13300.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.12956",
      "authors": [
        {
          "_id": "6879df6521b37e676c8e41cd",
          "user": {
            "_id": "653b195c5f1703225b2fd571",
            "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
            "isPro": false,
            "fullname": "wangqiang",
            "user": "wangqiang9",
            "type": "user"
          },
          "name": "Qiang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:24.746Z",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41ce",
          "name": "Mengchao Wang",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41cf",
          "name": "Fan Jiang",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41d0",
          "name": "Yaqi Fan",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41d1",
          "name": "Yonggang Qi",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41d2",
          "name": "Mu Xu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/653b195c5f1703225b2fd571/vkwYxJilAke5PetjLV8lm.mp4"
      ],
      "publishedAt": "2025-07-17T09:50:43.000Z",
      "submittedOnDailyAt": "2025-07-18T04:28:50.046Z",
      "title": "퀘스타 팝럿： 표현을 향상시킨 디퓨전 트랜스포머를 활용한 다 캐릭터 팝럿 애니메이션의 향상",
      "submittedOnDailyBy": {
        "_id": "653b195c5f1703225b2fd571",
        "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
        "isPro": false,
        "fullname": "wangqiang",
        "user": "wangqiang9",
        "type": "user"
      },
      "summary": "정지화에서 표현적인 얼굴의 애니메이션을 생성하는 것은 어려운 임무입니다. 기존의 방법은 명시적인 기하학적인 사전관념을 기반으로 하며, 재연출 시 Artifacts가 발생하며, 미묘한 감정을捉えやすくなりません。 또한, 현재의 접근법은 다 캐릭터 애니메이션의 지원을 하지 않습니다. 서로 다른 개인으로부터의 driver feature가 상호 간섭하여 문제를 복잡화합니다. 이러한 문제를 해결하기 위해, 우리는 FantasyPortrait를 제안합니다. 이것은 고품질의 감정 풍부한 애니메이션을 생성할 수 있는, 확산 채널 기반의 프레임 워크입니다. 우리 방식은 표현을 확장한 학습 전략을 도입하고, 은닉 표현을 활용하여 얼굴의 동작을捉え, 모델의 감정의 세부 표현 능력을 향상시킵니다. 다 캐릭터의 제어에서, 우리는 마스크付きク로스 어텐션 구조를 설계하여, 독립적으로 동적으로 표현을 생성하고, 특히 특징의 간섭을 방지합니다. 이 분야의 연구에 기여하기 위해, 우리는 Multi-Expr 데이터 세트와 ExprBench를 제안합니다. 이는 다 캐릭터의 포터레이트 애니메이션의 훈련과 평가에 특화된 데이터 세트와 벤치마크입니다. 확장된 실험은 FantasyPortrait가 최신의 방법보다 뛰어나고, 양적인 평가와 질적인 평가에서 특히 뛰어나며, 재연출과 다 캐릭터의 복잡한 컨텍스트에서 특히 뛰어나다는 것을 보여주고 있습니다. 우리의 프로젝트 페이지는 https://fantasy-amap.github.io/fantasy-portrait/입니다.",
      "upvotes": 9,
      "discussionId": "6879df6521b37e676c8e41d3",
      "projectPage": "https://fantasy-amap.github.io/fantasy-portrait/",
      "githubRepo": "https://github.com/Fantasy-AMAP/fantasy-portrait",
      "ai_summary": "FantasyPortrait, a diffusion transformer framework, generates high-fidelity and emotion-rich facial animations for single and multi-character scenarios using implicit representations and a masked cross-attention mechanism.",
      "ai_keywords": [
        "diffusion transformer",
        "expression-augmented learning",
        "implicit representations",
        "masked cross-attention mechanism",
        "Multi-Expr dataset",
        "ExprBench"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-07-17T05:50:43.000Z",
    "title": "FantasyPortrait: Enhancing Multi-Character Portrait Animation with\n  Expression-Augmented Diffusion Transformers",
    "summary": "Producing expressive facial animations from static images is a challenging\ntask. Prior methods relying on explicit geometric priors (e.g., facial\nlandmarks or 3DMM) often suffer from artifacts in cross reenactment and\nstruggle to capture subtle emotions. Furthermore, existing approaches lack\nsupport for multi-character animation, as driving features from different\nindividuals frequently interfere with one another, complicating the task. To\naddress these challenges, we propose FantasyPortrait, a diffusion transformer\nbased framework capable of generating high-fidelity and emotion-rich animations\nfor both single- and multi-character scenarios. Our method introduces an\nexpression-augmented learning strategy that utilizes implicit representations\nto capture identity-agnostic facial dynamics, enhancing the model's ability to\nrender fine-grained emotions. For multi-character control, we design a masked\ncross-attention mechanism that ensures independent yet coordinated expression\ngeneration, effectively preventing feature interference. To advance research in\nthis area, we propose the Multi-Expr dataset and ExprBench, which are\nspecifically designed datasets and benchmarks for training and evaluating\nmulti-character portrait animations. Extensive experiments demonstrate that\nFantasyPortrait significantly outperforms state-of-the-art methods in both\nquantitative metrics and qualitative evaluations, excelling particularly in\nchallenging cross reenactment and multi-character contexts. Our project page is\nhttps://fantasy-amap.github.io/fantasy-portrait/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/653b195c5f1703225b2fd571/vkwYxJilAke5PetjLV8lm.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12956.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b195c5f1703225b2fd571",
      "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
      "fullname": "wangqiang",
      "name": "wangqiang9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12990",
      "authors": [
        {
          "_id": "6879fdc821b37e676c8e422b",
          "name": "Nikita Koriagin",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422c",
          "name": "Yaroslav Aksenov",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422d",
          "user": {
            "_id": "634c5f8cfb80cc6bcaf42c03",
            "avatarUrl": "/avatars/1f37db0e70cbaf9707f4c8cbcee37ca0.svg",
            "isPro": false,
            "fullname": "Daniil Laptev",
            "user": "dlaptev",
            "type": "user"
          },
          "name": "Daniil Laptev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T08:32:16.310Z",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422e",
          "name": "Gleb Gerasimov",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422f",
          "user": {
            "_id": "60b364e7f88532cd79eaff7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
            "isPro": false,
            "fullname": "Nikita Balagansky",
            "user": "elephantmipt",
            "type": "user"
          },
          "name": "Nikita Balagansky",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T08:32:14.730Z",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e4230",
          "user": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "isPro": false,
            "fullname": "Daniil Gavrilov",
            "user": "kefirski",
            "type": "user"
          },
          "name": "Daniil Gavrilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T08:32:17.917Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60b364e7f88532cd79eaff7b/puU_3A7T0uk5E0fGkoc1k.png"
      ],
      "publishedAt": "2025-07-17T10:57:49.000Z",
      "submittedOnDailyAt": "2025-07-18T06:31:25.084Z",
      "title": "Teach Old SAEs New Domain Tricks with Boosting\n\n이 글은 \"Boosting\"을 사용하여 \"Old SAEs\"가 새로운 영역의 \"Tricks\"를 배우는 방법을 설명합니다. \"SAEs\"는 \"State-of-the-art Artificial Experts\"를 의미하며, \"Boosting\"은 머신러닝에서 성능을 향상시키는 기술입니다. 이 글은 \"Old SAEs\"가 새로운 영역에서 더 높은 성능을 달성하는 방법을 설명하고 있습니다.",
      "submittedOnDailyBy": {
        "_id": "60b364e7f88532cd79eaff7b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
        "isPro": false,
        "fullname": "Nikita Balagansky",
        "user": "elephantmipt",
        "type": "user"
      },
      "summary": "Sparse Autoencoders는 Large Language Models의 내부 표현을 해석하기 위한 강력한 도구로 등장했지만, 일반적으로 특징적인 특징을 추출하지 못하여 실패하는 경향이 있었다. 본 논문에서는 이러한 특징을 보완하기 위해 잔차 학습 접근법을 제안하고, 완전한 재훈련이 필요하지 않도록 하는 방법을 제안하였다. 특히 다음 SAE를 훈련시키는 것을 제안하였으며, 사전 학습된 SAE의 특징적인 문장을 재구성 오류로 모델링하고, 주요 모델이 보지 못한 특징을 효과적으로捉える 것이 가능하도록 하였다. 추론 시 두 모델의 출력을 합하여 LLM의 교차 엔트로피와 설명 가능한 변이 메트릭에 있어 상당한 개선을 나타내었다. 실험 결과를 통해, 이 방법은 기존의 SAE에 새로운 멤버스키를 효율적으로 통합하면서 일반적인 태스크에 대한 성능을 유지하는 것을 보여주고 있다. 이 접근법은 특정 멤버스키에 맞는 기계적 해석성을 선택적으로 향상시킬 수 있으며, LLM의 목표적 기계적 해석성의 새로운 가능성을 개척한다.",
      "upvotes": 4,
      "discussionId": "6879fdc821b37e676c8e4231",
      "ai_summary": "A residual learning approach enhances Sparse Autoencoders to capture domain-specific features without retraining, improving interpretability and performance on specialized domains.",
      "ai_keywords": [
        "Sparse Autoencoders",
        "residual learning",
        "reconstruction error",
        "cross-entropy",
        "explained variance",
        "targeted mechanistic interpretability",
        "Large Language Models"
      ]
    },
    "publishedAt": "2025-07-17T06:57:49.000Z",
    "title": "Teach Old SAEs New Domain Tricks with Boosting",
    "summary": "Sparse Autoencoders have emerged as powerful tools for interpreting the\ninternal representations of Large Language Models, yet they often fail to\ncapture domain-specific features not prevalent in their training corpora. This\npaper introduces a residual learning approach that addresses this feature\nblindness without requiring complete retraining. We propose training a\nsecondary SAE specifically to model the reconstruction error of a pretrained\nSAE on domain-specific texts, effectively capturing features missed by the\nprimary model. By summing the outputs of both models during inference, we\ndemonstrate significant improvements in both LLM cross-entropy and explained\nvariance metrics across multiple specialized domains. Our experiments show that\nthis method efficiently incorporates new domain knowledge into existing SAEs\nwhile maintaining their performance on general tasks. This approach enables\nresearchers to selectively enhance SAE interpretability for specific domains of\ninterest, opening new possibilities for targeted mechanistic interpretability\nof LLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60b364e7f88532cd79eaff7b/puU_3A7T0uk5E0fGkoc1k.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60b364e7f88532cd79eaff7b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
      "fullname": "Nikita Balagansky",
      "name": "elephantmipt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12720",
      "authors": [
        {
          "_id": "68799d0521b37e676c8e4060",
          "name": "Abraham Toluase Owodunni",
          "hidden": false
        },
        {
          "_id": "68799d0521b37e676c8e4061",
          "name": "Orevaoghene Ahia",
          "hidden": false
        },
        {
          "_id": "68799d0521b37e676c8e4062",
          "name": "Sachin Kumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T01:55:41.000Z",
      "submittedOnDailyAt": "2025-07-18T00:36:57.429Z",
      "title": "FLEXITOKENS: 변화하는 언어 모델을 위한 유연한 토큰화\n\n(Note: The translation is provided as requested, maintaining professionalism and accuracy. The term \"변환하는 언어 모델\" is used to convey the concept of \"evolving language model\" in a way that is both accurate and idiomatic in Korean.)",
      "submittedOnDailyBy": {
        "_id": "626d1e1e72169e781945bf44",
        "avatarUrl": "/avatars/6bf9f35042b6d939f2ab525816ad0423.svg",
        "isPro": false,
        "fullname": "Abraham  Owodunni",
        "user": "Owos",
        "type": "user"
      },
      "summary": "언어 모델(LMs)는 간단한 미세 조정으로 새로운 데이터 분포에 적응하는 것이 어렵습니다. 이는 그 하위 워드 토큰화기의 刚성 (강성) 때문입니다. 이 불변성은 일반적으로 적응 기간 동안 변하지 않기 때문에, 새로운 데이터 분포, 처음 본 언어, 또는 스크립트의 과도한 분할로 인해 무효한 토큰화 과정을招く 경우가 많습니다. 본 연구에서는 학습 가능한 토큰화기를 가진 바이트 수준의 LMs를 개발하여 토큰화 과정을 적응 가능한 상태로 만듭니다. 모델에는 입력 바이트 시퀀스의 경계를 예측하는 서브모듈이 포함되어 있으며, 이를 변장 세그먼트로 변환합니다. 현재 토큰화기 없는 방법은 토큰화기의 경계 예측을 보조 손실을 사용하여 훈련하고, 전체 훈련 코퍼스에서 고정된 압축률을 강제하여 새로운 종류의 刚성 (강성)을 불러일으키기 때문입니다. FLEXITOKENS를 제안합니다. FLEXITOKENS는 적응 기간 동안 크게 상승하는 유연성을 문제로 삼습니다. 다수의 다언어 벤치마크, 구조적으로 다양한 태스크, 데이터 영역을 검증하여, FLEXITOKENS는 서브 워드 토큰화기나 다른 경사 기반의 토큰화기와 비교하여, 토큰의 과도한 분할을 절감하며, 하류 태스크의 성능에 10% 정도의 향상을 실현합니다. 실험의 코드와 데이터는 https://github.com/owos/flexitokens 에서 릴리즈됩니다.",
      "upvotes": 4,
      "discussionId": "68799d0521b37e676c8e4063",
      "ai_summary": "FLEXITOKENS, a byte-level language model with a learnable tokenizer, reduces token over-fragmentation and improves performance across multilingual and morphologically diverse tasks.",
      "ai_keywords": [
        "byte-level LMs",
        "learnable tokenizers",
        "boundary predictor",
        "FLEXITOKENS",
        "token over-fragmentation",
        "subword tokenizers",
        "gradient-based tokenizers"
      ]
    },
    "publishedAt": "2025-07-16T21:55:41.000Z",
    "title": "FLEXITOKENS: Flexible Tokenization for Evolving Language Models",
    "summary": "Language models (LMs) are challenging to adapt to new data distributions by\nsimple finetuning. This is due to the rigidity of their subword tokenizers,\nwhich typically remain unchanged during adaptation. This inflexibility often\nleads to inefficient tokenization, causing overfragmentation of\nout-of-distribution domains, unseen languages, or scripts. In this work, we\ndevelop byte-level LMs with learnable tokenizers to make tokenization adaptive.\nOur models include a submodule that learns to predict boundaries between the\ninput byte sequence, encoding it into variable-length segments. Existing\ntokenizer-free methods train this boundary predictor using an auxiliary loss\nthat enforces a fixed compression rate across the training corpus, introducing\na new kind of rigidity. We propose FLEXITOKENS, a simplified training objective\nthat enables significantly greater flexibility during adaptation. Evaluating\nacross multiple multilingual benchmarks, morphologically diverse tasks, and\ndomains, we demonstrate that FLEXITOKENS consistently reduces token\nover-fragmentation and achieves up to 10\\% improvements on downstream task\nperformance compared to subword and other gradient-based tokenizers. Code and\ndata for our experiments will be released at\nhttps://github.com/owos/flexitokens",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626d1e1e72169e781945bf44",
      "avatarUrl": "/avatars/6bf9f35042b6d939f2ab525816ad0423.svg",
      "fullname": "Abraham  Owodunni",
      "name": "Owos",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04984",
      "authors": [
        {
          "_id": "68783633001546c83aa4f928",
          "user": {
            "_id": "67abb26debe64eaa3a624bd7",
            "avatarUrl": "/avatars/39259308f5aeda7f80a5489335554913.svg",
            "isPro": false,
            "fullname": "Zonglin Lyu",
            "user": "ucfzl",
            "type": "user"
          },
          "name": "Zonglin Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:35.041Z",
          "hidden": false
        },
        {
          "_id": "68783633001546c83aa4f929",
          "name": "Chen Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T13:25:32.000Z",
      "submittedOnDailyAt": "2025-07-18T01:45:16.974Z",
      "title": "TLB-VFI: 시간 인식 잠재 브로ун 브릿지 확산 법의 비디오 프레임 인터 프로시쳇",
      "submittedOnDailyBy": {
        "_id": "67abb26debe64eaa3a624bd7",
        "avatarUrl": "/avatars/39259308f5aeda7f80a5489335554913.svg",
        "isPro": false,
        "fullname": "Zonglin Lyu",
        "user": "ucfzl",
        "type": "user"
      },
      "summary": "비디오 프레임 인터퐁레이션(VFI)는 2개의 연속된 인접 프레임 I_0와 I_1를 기반으로 시간 t와 부호 부하를 피하기 위해 n을 사용하여 중간 프레임 I_n을 예측하는 것을 목표로 합니다. 최근의 접근 방식은 이 작업에서 이미지 기반 및 비디오 기반의 확산 모델을 적용하여 강력한 성능을 달성하고 있습니다. 그러나 이미지 기반의 확산 모델은 시간 정보를 추출할 수 없기 때문에 비확산 방식과 비교하여 상대적으로 낡은 것입니다. 비디오 기반의 확산 모델은 시간 정보를 추출할 수 있기 때문에 학습 스케일, 모델 크기, 추론 시간 등에 과도한 영향을 미칩니다. 이러한 문제를 완화하기 위해, 우리는 3D-wavelet gating와 시간 정보를 아는 자동 인코더를 사용하여 Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI)를 제안합니다. 이 방법은 비디오 입력으로부터 풍부한 시간 정보를 추출하고, 최근의 이미지 기반의 확산 모델의 SOTA와 비교하여 FID에 20%의 개선을 달성합니다. 또한, 풍부한 시간 정보를 통해 이 방법은 3배 적은 파라미터 수를 가진다면 강력한 성능을 달성할 수 있습니다. 이러한 파라미터 수의 감소는 2.3배의 속도 향상을 얻을 수 있습니다. 오픈 칼라 그레이디언트 라인을 사용함으로써, 이 방법은 9000배 적은 학습 데이터를 필요로 하며, 비디오 기반의 확산 모델과 비교하여 20배 적은 파라미터 수를 달성합니다. 코드와 결과를 아래 프로젝트 페이지에서 접근할 수 있습니다: https://zonglinl.github.io/tlbvfi_page.",
      "upvotes": 4,
      "discussionId": "68783634001546c83aa4f92a",
      "projectPage": "https://zonglinl.github.io/tlbvfi_page/",
      "githubRepo": "https://github.com/ZonglinL/TLBVFI",
      "ai_summary": "Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) improves video frame interpolation by efficiently extracting temporal information, reducing parameters, and requiring less training data compared to existing methods.",
      "ai_keywords": [
        "diffusion models",
        "video frame interpolation",
        "temporal information",
        "3D-wavelet gating",
        "temporal-aware autoencoder",
        "FID",
        "optical flow guidance"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-07-07T09:25:32.000Z",
    "title": "TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame\n  Interpolation",
    "summary": "Video Frame Interpolation (VFI) aims to predict the intermediate frame I_n\n(we use n to denote time in videos to avoid notation overload with the timestep\nt in diffusion models) based on two consecutive neighboring frames I_0 and\nI_1. Recent approaches apply diffusion models (both image-based and\nvideo-based) in this task and achieve strong performance. However, image-based\ndiffusion models are unable to extract temporal information and are relatively\ninefficient compared to non-diffusion methods. Video-based diffusion models can\nextract temporal information, but they are too large in terms of training\nscale, model size, and inference time. To mitigate the above issues, we propose\nTemporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation\n(TLB-VFI), an efficient video-based diffusion model. By extracting rich\ntemporal information from video inputs through our proposed 3D-wavelet gating\nand temporal-aware autoencoder, our method achieves 20% improvement in FID on\nthe most challenging datasets over recent SOTA of image-based diffusion models.\nMeanwhile, due to the existence of rich temporal information, our method\nachieves strong performance while having 3times fewer parameters. Such a\nparameter reduction results in 2.3x speed up. By incorporating optical flow\nguidance, our method requires 9000x less training data and achieves over 20x\nfewer parameters than video-based diffusion models. Codes and results are\navailable at our project page: https://zonglinl.github.io/tlbvfi_page.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67abb26debe64eaa3a624bd7",
      "avatarUrl": "/avatars/39259308f5aeda7f80a5489335554913.svg",
      "fullname": "Zonglin Lyu",
      "name": "ucfzl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.11589",
      "authors": [
        {
          "_id": "687902bccc15e42a72b01ad6",
          "name": "Sandeep Suresh Cranganore",
          "hidden": false
        },
        {
          "_id": "687902bccc15e42a72b01ad7",
          "user": {
            "_id": "66bdb0025bdd611f9a008bec",
            "avatarUrl": "/avatars/468ec3aebcf798bff601583adbc3bf88.svg",
            "isPro": false,
            "fullname": "Bodnar",
            "user": "AndreiB137",
            "type": "user"
          },
          "name": "Andrei Bodnar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T14:59:15.181Z",
          "hidden": false
        },
        {
          "_id": "687902bccc15e42a72b01ad8",
          "name": "Arturs Berzins",
          "hidden": false
        },
        {
          "_id": "687902bccc15e42a72b01ad9",
          "name": "Johannes Brandstetter",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-15T14:55:39.000Z",
      "submittedOnDailyAt": "2025-07-18T08:27:07.754Z",
      "title": "エインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニュー",
      "submittedOnDailyBy": {
        "_id": "66bdb0025bdd611f9a008bec",
        "avatarUrl": "/avatars/468ec3aebcf798bff601583adbc3bf88.svg",
        "isPro": false,
        "fullname": "Bodnar",
        "user": "AndreiB137",
        "type": "user"
      },
      "summary": "エインテインフィールドズ는 계산량의 효율적인 4차원 수치상대론 시뮬레이션을 조직적인 은닉 뉴럴네트워크 가중치에 압축하는 데 필요한 뉴럴 표현을 소개합니다. 일반상대론의 핵심 텐서 필드인 메트릭을 모델링함으로써, 물리량을 자동 미분에 의해 계산할 수 있습니다. 그러나 일반적인 뉴럴 필드(예: 부호付き 거리, 점유, 라디언스 필드)와 달리, エインテインフィールドズ는 일반상대론의 공간 시간 기하를 뉴럴 필드 표현으로 변환할 때 자연스럽게 동력학이 발생하는 뉴럴텐서 필드입니다. エインテインフィールドズ는 4차원 공간 시간의 연속 모델링, 메ッ시 무관성, 스トレージエフィシェンス, 미분의 정확도, 사용의 쉬운성에서 놀라울만한 가능성을 보여주고 있습니다. 일반상대론의 표준 테스트 침대에서 이러한 도전에 대처하고, JAX를 기반으로 하는 오픈소스 라이브러리를 릴리즈하여 수치상대론의 더 scalable 및 표현력 있는 접근 방식을 돋보입니다. 코드는 https://github.com/AndreiB137/EinFields 에서 사용 가능합니다.",
      "upvotes": 0,
      "discussionId": "687902bdcc15e42a72b01ada",
      "githubRepo": "https://github.com/AndreiB137/EinFields",
      "ai_summary": "Einstein Fields, a neural tensor field representation, compresses four-dimensional numerical relativity simulations into neural network weights, enabling automatic differentiation and natural emergence of dynamics.",
      "ai_keywords": [
        "Einstein Fields",
        "neural representation",
        "implicit neural network",
        "metric",
        "general relativity",
        "neural tensor fields",
        "spacetime geometry",
        "automatic differentiation",
        "numerical relativity",
        "JAX-based library"
      ],
      "githubStars": 23
    },
    "publishedAt": "2025-07-15T10:55:39.000Z",
    "title": "Einstein Fields: A Neural Perspective To Computational General\n  Relativity",
    "summary": "We introduce Einstein Fields, a neural representation that is designed to\ncompress computationally intensive four-dimensional numerical relativity\nsimulations into compact implicit neural network weights. By modeling the\nmetric, which is the core tensor field of general relativity, Einstein\nFields enable the derivation of physical quantities via automatic\ndifferentiation. However, unlike conventional neural fields (e.g., signed\ndistance, occupancy, or radiance fields), Einstein Fields are Neural\nTensor Fields with the key difference that when encoding the spacetime\ngeometry of general relativity into neural field representations, dynamics\nemerge naturally as a byproduct. Einstein Fields show remarkable potential,\nincluding continuum modeling of 4D spacetime, mesh-agnosticity, storage\nefficiency, derivative accuracy, and ease of use. We address these challenges\nacross several canonical test beds of general relativity and release an open\nsource JAX-based library, paving the way for more scalable and expressive\napproaches to numerical relativity. Code is made available at\nhttps://github.com/AndreiB137/EinFields",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66bdb0025bdd611f9a008bec",
      "avatarUrl": "/avatars/468ec3aebcf798bff601583adbc3bf88.svg",
      "fullname": "Bodnar",
      "name": "AndreiB137",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]