[
  {
    "paper": {
      "id": "2505.23747",
      "authors": [
        {
          "_id": "68391565d762b7c617b1ba81",
          "name": "Diankun Wu",
          "hidden": false
        },
        {
          "_id": "68391565d762b7c617b1ba82",
          "name": "Fangfu Liu",
          "hidden": false
        },
        {
          "_id": "68391565d762b7c617b1ba83",
          "name": "Yi-Hsin Hung",
          "hidden": false
        },
        {
          "_id": "68391565d762b7c617b1ba84",
          "name": "Yueqi Duan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6505a02f9310ce8c400edc63/eb5xv9-rlab_DrHLk4BMq.mp4"
      ],
      "publishedAt": "2025-05-29T17:59:04.000Z",
      "submittedOnDailyAt": "2025-05-30T00:56:58.237Z",
      "title": "스펙트럼-MLLM: 시각기반의 스펙트럼 지능에 대한 MLLM 능력의 향상",
      "submittedOnDailyBy": {
        "_id": "6505a02f9310ce8c400edc63",
        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
        "isPro": false,
        "fullname": "Fangfu Liu",
        "user": "Liuff23",
        "type": "user"
      },
      "summary": "최근의 다중 모델 대 언어 모델(MLLM)의 발전은 2차원 시각적인 태스크의 성능을 크게 향상시켰습니다. 그러나 그 공간 지식의 향상은 어려운 문제입니다. 현재의 3차원 MLLM은 3차원 또는 2.5차원 데이터를 추가적으로 사용하여 공간 인식을 포함하기 때문에, 2차원 입력만 있는 경우(그래픽, 이미지, 비디오)에는 도움이 되지 않습니다. 본 논문에서는 2차원 관측으로부터 공간적인 추론을 위한 새로운 프레임워크 \"Spatial-MLLM\"을 소개합니다. 단순한 비디오 MLLM과 다른 점은 CLIP 기반의 시각적 인코더를 사용하여 의미의 이해를 최적화한 것입니다. 우리의 주요 아이디어는 전향적인 시각적 제네럴화 모델의 강력한 구조적인 사전이 해제될 수 있는 것입니다. 특히, 학습된 2차원 시각적 인코더를 사용하여 의미적인 특성을 추출하고, 시각적 제네럴화 모델의 백본으로 초기화된 공간 인코더를 사용하여 3차원 구조적인 특성을 추출하는 이중 인코더 아키텍처를 제안합니다. 그 후, 이러한 특성을 통합한 시각 토큰으로 통합하는 연결을 사용합니다. 또한, 추론 시 공간적으로 정보를 갖는 프레임을 선택하는 공간 인식에 대한 프레임 샘플링 전략을 제안합니다. 이를 통해, 비디오 시퀀스의 공간적으로 정보를 갖는 프레임을 선택함으로써, 모델이 공간적인 추론에 필요한 프레임에 집중하는 것을 보장합니다. 아키텍처의 개선 외에도, Spatial-MLLM-120k 데이터셋을 구축하고, 이 데이터를 서브 프로바이징과 GRPO를 사용하여 모델을 훈련합니다. 다양한 실세계 데이터셋을 통해 검증한 결과, 우리의 공간 MLLM은 광범위한 시각적인 공간의 이해와 추론 태스크에서 가장 先端의 성능을 달성합니다. 프로젝트 페이지: https://diankun-wu.github.io/Spatial-MLLM/",
      "upvotes": 39,
      "discussionId": "68391566d762b7c617b1bae5",
      "projectPage": "https://diankun-wu.github.io/Spatial-MLLM/",
      "githubRepo": "https://github.com/diankun-wu/Spatial-MLLM",
      "ai_summary": "Spatial-MLLM improves spatial reasoning in multimodal large language models using a dual-encoder architecture with pretrained 2D and 3D structure encoders, achieving state-of-the-art performance on visual spatial tasks.",
      "ai_keywords": [
        "spatial-mllm",
        "dual-encoder architecture",
        "visual geometry foundation model",
        "CLIP-based visual encoders",
        "semantic features",
        "3D structure features",
        "unified visual tokens",
        "space-aware frame sampling",
        "supervised fine-tuning",
        "GRPO",
        "spatial understanding",
        "spatial reasoning"
      ]
    },
    "publishedAt": "2025-05-29T13:59:04.000Z",
    "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial\n  Intelligence",
    "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6505a02f9310ce8c400edc63/eb5xv9-rlab_DrHLk4BMq.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23747.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6505a02f9310ce8c400edc63",
      "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
      "fullname": "Fangfu Liu",
      "name": "Liuff23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23621",
      "authors": [
        {
          "_id": "68391925d73e6015a1b0f305",
          "name": "Zheyuan Yang",
          "hidden": false
        },
        {
          "_id": "68391925d73e6015a1b0f306",
          "name": "Lyuhao Chen",
          "hidden": false
        },
        {
          "_id": "68391925d73e6015a1b0f307",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "68391925d73e6015a1b0f308",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:43.117Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:28:50.000Z",
      "submittedOnDailyAt": "2025-05-30T01:04:47.042Z",
      "title": "테이블 추론 시의 스케일링",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "이 연구에서는 추론 시 스케일링을 시도하기 위한 초기 연구를 보고합니다. 두 가지 훈련 후 전략을 개발하여 추론 시 스케일링이 가능한지 평가합니다: DeepSeek-R1에서부터 초기 모델 추론 트래스으로부터 디스틸레이션과 보상을 증명 가능한 강화 학습(RLVR). 디스틸레이션에서, DeepSeek-R1에서 생성된 규모가 큰 추론 트래스 데이터 세트를 사용하여 LLM을 Table-R1-SFT 모델로 미세 조정합니다. RLVR에서, 태스크에 관련된 보상 함수를 제안하고 GRPO 알고리즘을 적용하여 Table-R1-Zero 모델을 얻습니다. Table-R1-series 모델은 다양한 테이블 추론 태스크에서 평가되며, 특히 Table-R1-Zero 모델은 7B 파라미터의 LLM만 사용하여 GPT-4.1과 DeepSeek-R1의 성능을 비교하거나 초과합니다. 또한, 영역 외 데이터 세트에 강한 일반화 성능을 나타냅니다. 확장된 제거 시험과 질적 분석을 통해 지시 조정, 모델 구조의 선택, 태스크 간 일반화, RL 훈련 중 중요한 테이블 추론 스킬의 발생을 밝혀냅니다.",
      "upvotes": 38,
      "discussionId": "68391928d73e6015a1b0f3a8",
      "githubRepo": "https://github.com/Table-R1/Table-R1",
      "ai_summary": "Two post-training strategies, distillation and RLVR, enable inference-time scaling in table reasoning tasks, resulting in a model (Table-R1-Zero) that matches GPT-4.1's performance using fewer parameters and shows strong generalization.",
      "ai_keywords": [
        "distillation",
        "reinforcement learning",
        "verifiable rewards",
        "RLVR",
        "reasoning traces",
        "DeepSeek-R1",
        "LLMs",
        "Table-R1-SFT",
        "GRPO",
        "Table-R1-Zero",
        "short-form QA",
        "fact verification",
        "free-form QA",
        "instruction tuning",
        "model architecture choices",
        "cross-task generalization",
        "table reasoning skills"
      ]
    },
    "publishedAt": "2025-05-29T12:28:50.000Z",
    "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
    "summary": "In this work, we present the first study to explore inference-time scaling on\ntable reasoning tasks. We develop and evaluate two post-training strategies to\nenable inference-time scaling: distillation from frontier model reasoning\ntraces and reinforcement learning with verifiable rewards (RLVR). For\ndistillation, we introduce a large-scale dataset of reasoning traces generated\nby DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For\nRLVR, we propose task-specific verifiable reward functions and apply the GRPO\nalgorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series\nmodels across diverse table reasoning tasks, including short-form QA, fact\nverification, and free-form QA. Notably, the Table-R1-Zero model matches or\nexceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a\n7B-parameter LLM. It also demonstrates strong generalization to out-of-domain\ndatasets. Extensive ablation and qualitative analyses reveal the benefits of\ninstruction tuning, model architecture choices, and cross-task generalization,\nas well as emergence of essential table reasoning skills during RL training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22653",
      "authors": [
        {
          "_id": "6838bb282b382ba50bdcddc4",
          "name": "Ang Lv",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc5",
          "user": {
            "_id": "6622443b9b0614a760dd8123",
            "avatarUrl": "/avatars/acb6c1c9c429af1112530dcf76a8e420.svg",
            "isPro": false,
            "fullname": "Ruobing Xie",
            "user": "Ruobing-Xie",
            "type": "user"
          },
          "name": "Ruobing Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:45.319Z",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc6",
          "name": "Xingwu Sun",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc7",
          "name": "Zhanhui Kang",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc8",
          "name": "Rui Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:59:03.000Z",
      "submittedOnDailyAt": "2025-05-30T00:44:54.555Z",
      "title": "登리의 속으로 깊게 묻혀 있는 지혜: 학습의 이유에 대한 혼란스러운 보상에 대해",
      "submittedOnDailyBy": {
        "_id": "64b8ca3c5067873176d4b436",
        "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg",
        "isPro": false,
        "fullname": "AngLv",
        "user": "AngLv",
        "type": "user"
      },
      "summary": "최근의 후 학습 라지션럴 언어 모델(LLMs)의 논리적 계산을 통한 강화 학습(RL)에 대한 연구는 검증이 가능한 작업에 집중하고 있습니다. 예를 들어, 수학 문제 해결에 초점을 맞추고 있습니다. 반면, 우리의 연구는 실제 세계에서의 후 학습된 LLMs의 보상 모델의 영향을 조사하고 있습니다. 우리는 LLMs는 강한 보상 노이즈에 강하게 대응하는 것을 발견했습니다. 예를 들어, 수학 태스크의 보상 함수의 출력을 직접 40%를 반전시키면, Qwen-2.5-7B 모델은 급격한 수렴을 달성하고, 보상 노이즈가 없는 경우의 75%의 정확도를 비교하여, 수학 태스크의 성능이 5%에서 72%까지 증가했습니다. 놀라울 정도로, 이유의 문구의 출현만 보상을 주는 경우(이유 패턴 보상, RPR)으로, 답의 정확성을 검증하지 않아도 모델은 70% 이상의 정확도를 달성하고, 엄격한 정확성 검증과 정확한 보상을 사용하는 모델과 비교하여, 낮은 성능이 향상되었습니다. 최종적인 결과를 더 중요하게 여기는 것이 이유의 과정보다 중요함을 인식하고, RPR와 보상 노이즈 모델을 조합했습니다. RPR는 보상 노이즈 모델을 조정하고, 잠재적인 필드 부정을 완화시키고, LLM의 개방된 태스크의 성능을 향상시켰습니다. 이러한 발견은 학습 단계에서 모델의 기초적인 능력을 향상하고 후 학습 기술의 발전에 대한 조언을 제공합니다. 우리의 코드와 스크립트는 https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason에 공개되어 있습니다.",
      "upvotes": 36,
      "discussionId": "6838bb2a2b382ba50bdcde1b",
      "githubRepo": "https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason",
      "ai_summary": "LLMs exhibit robustness to reward noise during post-training and achieve high performance using reasoning pattern rewards (RPR) in conjunction with noisy reward models.",
      "ai_keywords": [
        "large language models (LLMs)",
        "post-training",
        "reinforcement learning (RL)",
        "reward noise",
        "reward models",
        "rapid convergence",
        "reasoning pattern reward (RPR)",
        "false negatives",
        "open-ended tasks"
      ]
    },
    "publishedAt": "2025-05-28T13:59:03.000Z",
    "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason",
    "summary": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22653.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b8ca3c5067873176d4b436",
      "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg",
      "fullname": "AngLv",
      "name": "AngLv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23693",
      "authors": [
        {
          "_id": "68390e95b85141ce6c11b50f",
          "user": {
            "_id": "64dc29d9b5d625e0e9a6ecb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
            "isPro": false,
            "fullname": "Tingyu Song",
            "user": "songtingyu",
            "type": "user"
          },
          "name": "Tingyu Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:12.429Z",
          "hidden": false
        },
        {
          "_id": "68390e95b85141ce6c11b510",
          "user": {
            "_id": "66e83ec5deb449d8d856e78d",
            "avatarUrl": "/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg",
            "isPro": false,
            "fullname": "Tongyan Hu",
            "user": "entropyhu",
            "type": "user"
          },
          "name": "Tongyan Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:15.154Z",
          "hidden": false
        },
        {
          "_id": "68390e95b85141ce6c11b511",
          "name": "Guo Gan",
          "hidden": false
        },
        {
          "_id": "68390e95b85141ce6c11b512",
          "name": "Yilun Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64dc29d9b5d625e0e9a6ecb9/ZnHMXas2Khv3qS0RIvYR2.png"
      ],
      "publishedAt": "2025-05-29T17:31:13.000Z",
      "submittedOnDailyAt": "2025-05-30T00:24:42.566Z",
      "title": "VF-Eval: AIGC의 비디오에 대한 피드백을 생성하는 다모달 구조 LLMs의 평가",
      "submittedOnDailyBy": {
        "_id": "64dc29d9b5d625e0e9a6ecb9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
        "isPro": false,
        "fullname": "Tingyu Song",
        "user": "songtingyu",
        "type": "user"
      },
      "summary": "MLLMs는 최근 영화의 질문에 대한 연구를 많이 하고 있습니다. 그러나 현재의 평가는 자연스러운 영화를 중심으로 하고 있으며, AI 생성된 콘텐츠(AIGC) 등 합성 영화를 무시하고 있습니다. 반면에, 영화 생성에서 MLLMs를 사용하여 생성된 영화의 질을 평가하는 것이 있지만, MLLMs가 AIGC 영화를 이해하는 능력은 크게 조사되지 않았습니다. 이에 대해 우리는 새로운 평가 기준인 VF-Eval을 제안하고, 이 평가 기준은 AIGC 영화를 이해하는 능력을 자세히 평가하기 위해 일관성 검증, 오류 인식, 오류 종류 검출, 이유 평가 등 4가지의 태스크를 도입했습니다. VF-Eval을 통해 13가지의 최신 MLLMs를 평가하였으며, 가장 우수한 성능을 보여주는 모델 중도 GPT-4.1은 모든 태스크에서 일관적으로 우수한 성능을 달성하지 못했습니다. 이는 평가 기준의 난이도를 명확히 해줍니다. 또한, VF-Eval이 영화 생성에 어떠한 실용적인 응용을 가능하게 하는지 조사하기 위해 RePrompt 실험을 수행하였으며, MLLMs를 인간의 피드백에 의해 더 가까운 것으로 만들 수 있으며, 이는 영화 생성에 도움이 되는 것을 보여주었습니다.",
      "upvotes": 34,
      "discussionId": "68390e96b85141ce6c11b55c",
      "githubRepo": "https://github.com/SighingSnow/VF-EVAL",
      "ai_summary": "A new benchmark, VF-Eval, evaluates the capabilities of MLLMs in interpreting AI-generated content videos across four tasks, highlighting challenges and demonstrating benefits in video generation through human feedback alignment.",
      "ai_keywords": [
        "MLLMs",
        "video question answering",
        "synthetic videos",
        "AI-generated content (AIGC)",
        "VF-Eval",
        "coherence validation",
        "error awareness",
        "error type detection",
        "reasoning evaluation",
        "GPT-4.1",
        "RePrompt"
      ]
    },
    "publishedAt": "2025-05-29T13:31:13.000Z",
    "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC\n  Videos",
    "summary": "MLLMs have been widely studied for video question answering recently.\nHowever, most existing assessments focus on natural videos, overlooking\nsynthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in\nvideo generation rely on MLLMs to evaluate the quality of generated videos, but\nthe capabilities of MLLMs on interpreting AIGC videos remain largely\nunderexplored. To address this, we propose a new benchmark, VF-Eval, which\nintroduces four tasks-coherence validation, error awareness, error type\ndetection, and reasoning evaluation-to comprehensively evaluate the abilities\nof MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that\neven the best-performing model, GPT-4.1, struggles to achieve consistently good\nperformance across all tasks. This highlights the challenging nature of our\nbenchmark. Additionally, to investigate the practical applications of VF-Eval\nin improving video generation, we conduct an experiment, RePrompt,\ndemonstrating that aligning MLLMs more closely with human feedback can benefit\nvideo generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64dc29d9b5d625e0e9a6ecb9/ZnHMXas2Khv3qS0RIvYR2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23693.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dc29d9b5d625e0e9a6ecb9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
      "fullname": "Tingyu Song",
      "name": "songtingyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23762",
      "authors": [
        {
          "_id": "68391353d8c153d346e1ddb5",
          "user": {
            "_id": "637f347a52229c639211bee8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg",
            "isPro": false,
            "fullname": "Chenyu Yang",
            "user": "cyyang822",
            "type": "user"
          },
          "name": "Chenyu Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:52.043Z",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb6",
          "name": "Shiqian Su",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb7",
          "name": "Shi Liu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb8",
          "name": "Xuan Dong",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb9",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddba",
          "name": "Weijie Su",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbb",
          "name": "Xuehui Wang",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbc",
          "name": "Zhaoyang Liu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbd",
          "name": "Jinguo Zhu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbe",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbf",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddc0",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddc1",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddc2",
          "name": "Jifeng Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:51.000Z",
      "submittedOnDailyAt": "2025-05-30T00:41:52.864Z",
      "title": "ZeroGUI: ゼロヒマンコスト를 기반으로 하는 온라인 GUI 학습의 자동화\n\n(注意：此处的翻译保持了原文的准确性和专业性，同时确保了语言的自然流畅。)",
      "submittedOnDailyBy": {
        "_id": "637f347a52229c639211bee8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg",
        "isPro": false,
        "fullname": "Chenyu Yang",
        "user": "cyyang822",
        "type": "user"
      },
      "summary": "대규모 비전 언어 모델(VLMs)의 급속한 발전은 예측기반의 GUI 에이전트의 개발을 촉진하고, 그래픽 사용자 인터페이스(GUI)를 인식하고 조작하여 사용자의 지시를 자동으로 만족시키는 능력을 갖춘 것입니다. 그러나 현재의 접근 방식은 일반적으로 오프라인 학습 프레임워크를 사용하며, 두 가지 핵심적인 한계가 있습니다: 1) 요소의 조정과 행동의 서브적렉션에 대한 고품질의수동 注解의 부담이 있으며, 2) 동적인 상호작용적 환경에 대한 적응성은 제한되어 있습니다. 이러한 한계를 대처하기 위해, ZeroGUI라는 유연한 온라인 학습 프레임워크를 제안합니다. ZeroGUI는 1) VLM에 의한 자동 태스크 생성을 포함하여 현재의 환경 상태에서 다양한 트레이닝 목표를 생성하고, 2) VLM에 의한 자동 보상 평가를 포함하여 작업자 생성된 평가 함수를 제외한 태스크의 성공을 평가하고, 3) 두 단계 온라인 재학습을 포함하여 GUI 환경과 지속적으로 상호작용하여 학습하는 것을 특징으로 합니다. UI-TARS와 Aguvis의 두 가지 선진 GUI 에이전트에 대한 실험은 ZeroGUI가 OSWorld와 AndroidLab 환경에서 성능을 크게 향상시켰음을 보여줍니다. 코드는 https://github.com/OpenGVLab/ZeroGUI에서 접근할 수 있습니다.",
      "upvotes": 33,
      "discussionId": "68391354d8c153d346e1de1a",
      "githubRepo": "https://github.com/OpenGVLab/ZeroGUI",
      "ai_summary": "ZeroGUI is an online learning framework that uses Vision-Language Models for task generation and reward estimation, enhancing GUI Agents' performance with minimal human intervention.",
      "ai_keywords": [
        "Vision-Language Models",
        "GUI Agents",
        "element grounding",
        "action supervision",
        "offline learning",
        "automatic task generation",
        "automatic reward estimation",
        "reinforcement learning",
        "OSWorld",
        "AndroidLab"
      ]
    },
    "publishedAt": "2025-05-29T13:59:51.000Z",
    "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
    "summary": "The rapid advancement of large Vision-Language Models (VLMs) has propelled\nthe development of pure-vision-based GUI Agents, capable of perceiving and\noperating Graphical User Interfaces (GUI) to autonomously fulfill user\ninstructions. However, existing approaches usually adopt an offline learning\nframework, which faces two core limitations: (1) heavy reliance on high-quality\nmanual annotations for element grounding and action supervision, and (2)\nlimited adaptability to dynamic and interactive environments. To address these\nlimitations, we propose ZeroGUI, a scalable, online learning framework for\nautomating GUI Agent training at Zero human cost. Specifically, ZeroGUI\nintegrates (i) VLM-based automatic task generation to produce diverse training\ngoals from the current environment state, (ii) VLM-based automatic reward\nestimation to assess task success without hand-crafted evaluation functions,\nand (iii) two-stage online reinforcement learning to continuously interact with\nand learn from GUI environments. Experiments on two advanced GUI Agents\n(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance\nacross OSWorld and AndroidLab environments. The code is available at\nhttps://github.com/OpenGVLab/ZeroGUI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23762.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f347a52229c639211bee8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg",
      "fullname": "Chenyu Yang",
      "name": "cyyang822",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23604",
      "authors": [
        {
          "_id": "68390da8f527444e97c4ab95",
          "name": "Guangtao Zeng",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab96",
          "user": {
            "_id": "6553c985a7aded0380b5f928",
            "avatarUrl": "/avatars/36109d6f536d2b34d98822b88eac9608.svg",
            "isPro": false,
            "fullname": "Maohao Shen",
            "user": "maohaos2",
            "type": "user"
          },
          "name": "Maohao Shen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T01:45:13.275Z",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab97",
          "name": "Delin Chen",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab98",
          "name": "Zhenting Qi",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab99",
          "name": "Subhro Das",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9a",
          "name": "Dan Gutfreund",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9b",
          "name": "David Cox",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9c",
          "name": "Gregory Wornell",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9d",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9e",
          "name": "Zhang-Wei Hong",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9f",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:15:36.000Z",
      "submittedOnDailyAt": "2025-05-30T00:43:27.169Z",
      "title": "Satori-SWE: 샘플 효과적인 소프트웨어 엔지니어링의 진화적인 테스트 시간 스케줄링",
      "submittedOnDailyBy": {
        "_id": "60ad0de755f970745d4ec28d",
        "avatarUrl": "/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg",
        "isPro": true,
        "fullname": "GtZeng",
        "user": "chaoscodes",
        "type": "user"
      },
      "summary": "언어 모델(LMs)는 표준화된 코딩 벤치마크에서 좋은 성능을 보여주지만, 실제적인 소프트웨어 개발 태스크에 대해, 특히 SWE-Bench에서 GitHub의 문제를 해결하는 데 어려움이 있다. 특히 모델 파라미터가 100B 미만인 경우. 실용적으로는 계산 비용이 낮기 때문에 작은 모델이 선호된다는 반면, 성능 향상은 어렵기 때문이다. 현재의 접근 방식은 주로 고품질의 데이터를 사용한 규칙적인 미세 조정(SFT)에 기반하고, 스케일링 측면에서는 비싸다. 대체 방안은 테스트 시 스케일링으로, 여러 출력을 생성하고, 밸리피어러로 점수를 계산하여 가장 좋은 것을 선택하는 것이다. 이는 효과적이지만, 과도한 샘플링과 고가의 점수가 필요하므로 실질적인 적용에 제한되어 있다. 우리는 출력을 진화 프로세스로 취급하는 샘플 효율적인 방법인 진화적인 테스트 시 스케일링(EvoScale)을 제안하고 있다. 선택과 갑작스러운 변이를 사용하여 출력을 반복적으로 개선하고, EvoScale는 출력 분포를 높은 점수 영역으로 이동시키며, 필요한 샘플 수를 줄여 올바른 답을 찾는 데 필요한 것이다. 중복된 샘플링과 선택의 오버헤드를 줄이기 위해, 모델을 강화 학습(RL)을 사용하여 자기 진화시킨다. 추론 시에는 외부의 밸리피어러를 의존하지 않도록, 모델은 자기 개선한 점수를 학습하고 각 반복에서 자신의 생성을 개선한다. SWE-Bench-Verified에서 평가된 EvoScale는 우리의 32B 모델인 Satori-SWE-32B가 100B 파라미터 이상의 모델의 성능을 추월할 수 있도록 되었다. 코드, 데이터와 모델은 완전히 오픈 소스로 한다.",
      "upvotes": 18,
      "discussionId": "68390da9f527444e97c4abdf",
      "projectPage": "https://satori-reasoning.github.io/blog/satori-swe/",
      "ai_summary": "EvoScale, an evolutionary and reinforcement learning-based method, enhances small language models' performance on real-world software engineering tasks by iteratively improving and refining outputs.",
      "ai_keywords": [
        "supervised fine-tuning",
        "test-time scaling",
        "Evolutionary Test-Time Scaling",
        "EvoScale",
        "reinforcement learning",
        "SWE-Bench-Verified"
      ]
    },
    "publishedAt": "2025-05-29T12:15:36.000Z",
    "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software\n  Engineering",
    "summary": "Language models (LMs) perform well on standardized coding benchmarks but\nstruggle with real-world software engineering tasks such as resolving GitHub\nissues in SWE-Bench, especially when model parameters are less than 100B. While\nsmaller models are preferable in practice due to their lower computational\ncost, improving their performance remains challenging. Existing approaches\nprimarily rely on supervised fine-tuning (SFT) with high-quality data, which is\nexpensive to curate at scale. An alternative is test-time scaling: generating\nmultiple outputs, scoring them using a verifier, and selecting the best one.\nAlthough effective, this strategy often requires excessive sampling and costly\nscoring, limiting its practical application. We propose Evolutionary Test-Time\nScaling (EvoScale), a sample-efficient method that treats generation as an\nevolutionary process. By iteratively refining outputs via selection and\nmutation, EvoScale shifts the output distribution toward higher-scoring\nregions, reducing the number of samples needed to find correct solutions. To\nreduce the overhead from repeatedly sampling and selection, we train the model\nto self-evolve using reinforcement learning (RL). Rather than relying on\nexternal verifiers at inference time, the model learns to self-improve the\nscores of its own generations across iterations. Evaluated on\nSWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or\nexceed the performance of models with over 100B parameters while using a few\nsamples. Code, data, and models will be fully open-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23604.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60ad0de755f970745d4ec28d",
      "avatarUrl": "/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg",
      "fullname": "GtZeng",
      "name": "chaoscodes",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23359",
      "authors": [
        {
          "_id": "683909a29deef11aa625817c",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa625817d",
          "user": {
            "_id": "62cd7aca7a036fc9941bb2b0",
            "avatarUrl": "/avatars/17a4d27af0243fd7dccf06066f671461.svg",
            "isPro": false,
            "fullname": "kun ouyang",
            "user": "RUBBISHLIKE",
            "type": "user"
          },
          "name": "Kun Ouyang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:22.290Z",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa625817e",
          "name": "Haoning Wu",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa625817f",
          "name": "Yi Liu",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258180",
          "name": "Lin Sui",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258181",
          "name": "Xinhao Li",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258182",
          "name": "Yan Zhong",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258183",
          "name": "Y. Charles",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258184",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258185",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6489761dcaea79f577897f98/48SxCF6GVyMdnPR7vZSgC.mp4"
      ],
      "publishedAt": "2025-05-29T11:33:43.000Z",
      "submittedOnDailyAt": "2025-05-30T01:03:01.950Z",
      "title": "VideoReasonBench: 비디오 컴페니شن 벤치마크: MLLMs는 복잡한 비디오의 시각 중심적인 복잡한 이유 논리로 수행할 수 있는가?",
      "submittedOnDailyBy": {
        "_id": "6489761dcaea79f577897f98",
        "avatarUrl": "/avatars/8f56dc9c08dc2b672555602d68509a03.svg",
        "isPro": false,
        "fullname": "Yuanxin Liu",
        "user": "lyx97",
        "type": "user"
      },
      "summary": "최근의 연구는 긴 쉼표 연속 컨티닝 (CoT) 논리가 복잡한 태스크에서 대규모 언어 모델 (LLMs)의 성능을 크게 향상시키는 것을 시사하고 있습니다. 그러나 이 이득은 이미지 이해 분야에서 아직 나타나지 않았습니다. 이는 현재 필요한 벤치마크가 논리의 깊이를 갖지 않는缘故입니다. 최근의 시도는 이미지 논리에 대한 벤치마크를 제안하고 있지만, 이 작업들은 일반적으로 지식에 기반하고 시각적 콘텐츠에 의존하지 않습니다. 이 공백을 채우는 데에는 VideoReasonBench라는 벤치마크를 소개합니다. 이는 시각 중심적이고 복잡한 이미지 논리를 평가하는 벤치마크입니다. 시각적 풍부성과 높은 논리 복잡성을 보장하기 위해, VideoReasonBench의 각 이미지는 이미지의 일부에서 볼 수 있는 잠재적 상태에 대한 연산 순서를 묘사하는 것입니다. 질문은 시각 정보의 기억, 잠재적 상태의 내용을 추론, 이미지보다 정보를 예측하는 3단계의 이미지 논리 기술로 평가됩니다. 이 작업 설정 하에, 모델은 이러한 질문에 정확한 최종 답을 얻기 위해, 이미지의 여러 연산을 정밀하게 기억하고 단계별로 논리를 수행해야 합니다. VideoReasonBench를 사용 하에, 우리는 18개의 가장 先端의 다 모델 언어 모델 (MLLMs)을 검토하고, 복잡한 이미지 논리에서는 많은 모델이 낮은 성능을 나타냅니다. 예를 들어, GPT-4o는 6.9%의 정확도를 달성하고, 추억을 강화한 Gemini-2.5-Pro는 56.0%의 정확도로 다른 모델보다 유의미하게 뛰어납니다. \"테스트 시 스케일링\"에 대한 조사는, 현재의 이미지 벤치마크에 있는 거의 모든 벤치마크보다 벤이 플뤼버지 배치보다 더 높은 성능을 나타냅니다.",
      "upvotes": 18,
      "discussionId": "683909a39deef11aa62581c2",
      "projectPage": "https://llyx97.github.io/video_reason_bench/",
      "githubRepo": "https://github.com/llyx97/video_reason_bench",
      "ai_summary": "A new benchmark, VideoReasonBench, evaluates complex vision-centric video reasoning, finding that extended thinking budgets are crucial for improved performance compared to existing benchmarks.",
      "ai_keywords": [
        "long chain-of-thought reasoning",
        "large language models",
        "video understanding",
        "VideoReasonBench",
        "vision-centric",
        "complex video reasoning",
        "latent state",
        "visual reasoning",
        "step-by-step reasoning",
        "multimodal language models",
        "thinking-enhanced models",
        "test-time scaling"
      ]
    },
    "publishedAt": "2025-05-29T07:33:43.000Z",
    "title": "VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video\n  Reasoning?",
    "summary": "Recent studies have shown that long chain-of-thought (CoT) reasoning can\nsignificantly enhance the performance of large language models (LLMs) on\ncomplex tasks. However, this benefit is yet to be demonstrated in the domain of\nvideo understanding, since most existing benchmarks lack the reasoning depth\nrequired to demonstrate the advantages of extended CoT chains. While recent\nefforts have proposed benchmarks aimed at video reasoning, the tasks are often\nknowledge-driven and do not rely heavily on visual content. To bridge this gap,\nwe introduce VideoReasonBench, a benchmark designed to evaluate vision-centric,\ncomplex video reasoning. To ensure visual richness and high reasoning\ncomplexity, each video in VideoReasonBench depicts a sequence of fine-grained\noperations on a latent state that is only visible in part of the video. The\nquestions evaluate three escalating levels of video reasoning skills: recalling\nobserved visual information, inferring the content of latent states, and\npredicting information beyond the video. Under such task setting, models have\nto precisely recall multiple operations in the video, and perform step-by-step\nreasoning to get correct final answers for these questions. Using\nVideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal\nLLMs (MLLMs), finding that most perform poorly on complex video reasoning,\ne.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced\nGemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our\ninvestigations on \"test-time scaling\" further reveal that extended thinking\nbudget, while offering none or minimal benefits on existing video benchmarks,\nis essential for improving the performance on VideoReasonBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6489761dcaea79f577897f98/48SxCF6GVyMdnPR7vZSgC.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23359.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "6489761dcaea79f577897f98",
      "avatarUrl": "/avatars/8f56dc9c08dc2b672555602d68509a03.svg",
      "fullname": "Yuanxin Liu",
      "name": "lyx97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23716",
      "authors": [
        {
          "_id": "6839217e95fedc63bb4ae475",
          "name": "Lihan Jiang",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae476",
          "user": {
            "_id": "65de9c6cf68c3d3bac330509",
            "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
            "isPro": false,
            "fullname": "Yucheng Mao",
            "user": "matthewmao",
            "type": "user"
          },
          "name": "Yucheng Mao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:34.206Z",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae477",
          "name": "Linning Xu",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae478",
          "name": "Tao Lu",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae479",
          "name": "Kerui Ren",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47a",
          "name": "Yichen Jin",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47b",
          "name": "Xudong Xu",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47c",
          "name": "Mulin Yu",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47d",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47e",
          "name": "Feng Zhao",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47f",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae480",
          "name": "Bo Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:49:56.000Z",
      "submittedOnDailyAt": "2025-05-30T01:42:17.635Z",
      "title": "無制限한 시각으로의 Feed-forward 3D Gaussian Splatting",
      "submittedOnDailyBy": {
        "_id": "64a4ce8118f4e2529546daef",
        "avatarUrl": "/avatars/6d88aa68eccfa07d2009df405f957fd7.svg",
        "isPro": false,
        "fullname": "Jiang Lihan",
        "user": "lhjiang",
        "type": "user"
      },
      "summary": "AnySplat은, 미측정 이미지 컬렉션에서의 새로운 시각 합성을 위한 전방 네트워크입니다. 기존의 뉴럴 렌더링 파이프라인과 비교하여, 카메라의 자세와 각 장면의 최적화가 필요했던 것과, 최근의 전방 방식에서는 복잡한 시각을 다룰 때 계산량을 부담해야 하는 것과는 달리, 우리의 모델은 모든 것을 한 번의 포워드 패스만으로 예측합니다. 입력 이미지의 각 장면에 대해, 3D 가우시안 플래미티를 포함하여 장면의 형상과 외모, 그리고 이에 대응하는 카메라의 내각과 외모는 함께 얻을 수 있습니다. 이러한 통합 설계는 위치의 설명이 없는 것처럼, 카메라에서 촬영된, 여러 시각의 데이터 세트를 쉽게 스케일링할 수 있습니다. 광범위한 0샷 평가에서, AnySplat은 희소한 시각과 여러 시각의 양쪽 모두에서 위치에 관심 있는 기준과 같은 품질을 달성하며, 위치 제한 없는 접근을 초과합니다. 또한, 최적화 기반의 뉴럴 필드에 비해, 렌더링 지연을 크게 줄이고, 제한없는 촬영설정에서 실시간의 새로운 시각 합성을 가능하게 합니다. 프로젝트 페이지: https://city-super.github.io/anysplat/",
      "upvotes": 17,
      "discussionId": "6839217f95fedc63bb4ae4d0",
      "projectPage": "https://city-super.github.io/anysplat/",
      "ai_summary": "AnySplat is a feed forward network that performs novel view synthesis without camera poses, using 3D Gaussian primitives and unified design for efficiency and quality across sparse and dense datasets.",
      "ai_keywords": [
        "feed forward network",
        "novel view synthesis",
        "uncalibrated image collections",
        "3D Gaussian primitives",
        "camera intrinsics",
        "camera extrinsics",
        "neural rendering pipelines",
        "per scene optimization",
        "zero shot evaluations",
        "pose aware baselines",
        "pose free approaches",
        "rendering latency",
        "optimization based neural fields"
      ]
    },
    "publishedAt": "2025-05-29T13:49:56.000Z",
    "title": "AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views",
    "summary": "We introduce AnySplat, a feed forward network for novel view synthesis from\nuncalibrated image collections. In contrast to traditional neural rendering\npipelines that demand known camera poses and per scene optimization, or recent\nfeed forward methods that buckle under the computational weight of dense views,\nour model predicts everything in one shot. A single forward pass yields a set\nof 3D Gaussian primitives encoding both scene geometry and appearance, and the\ncorresponding camera intrinsics and extrinsics for each input image. This\nunified design scales effortlessly to casually captured, multi view datasets\nwithout any pose annotations. In extensive zero shot evaluations, AnySplat\nmatches the quality of pose aware baselines in both sparse and dense view\nscenarios while surpassing existing pose free approaches. Moreover, it greatly\nreduce rendering latency compared to optimization based neural fields, bringing\nreal time novel view synthesis within reach for unconstrained capture\nsettings.Project page: https://city-super.github.io/anysplat/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23716.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a4ce8118f4e2529546daef",
      "avatarUrl": "/avatars/6d88aa68eccfa07d2009df405f957fd7.svg",
      "fullname": "Jiang Lihan",
      "name": "lhjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23660",
      "authors": [
        {
          "_id": "683906adf85de1fc563957d8",
          "user": {
            "_id": "648ac3d53470b17ccc90deaf",
            "avatarUrl": "/avatars/35b678baa7a91959f40f704c5de2a3e1.svg",
            "isPro": false,
            "fullname": "Ziteng Gao",
            "user": "sebgao",
            "type": "user"
          },
          "name": "Ziteng Gao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T01:15:28.731Z",
          "hidden": false
        },
        {
          "_id": "683906adf85de1fc563957d9",
          "user": {
            "_id": "63a55320ce5763e06f78519c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671779060549-noauth.jpeg",
            "isPro": false,
            "fullname": "Mike Shou",
            "user": "mikeshou",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T01:15:28.731Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:09:25.000Z",
      "submittedOnDailyAt": "2025-05-30T01:28:22.938Z",
      "title": "D-AR: 자동 회귀 모델에 의한 확산",
      "submittedOnDailyBy": {
        "_id": "648ac3d53470b17ccc90deaf",
        "avatarUrl": "/avatars/35b678baa7a91959f40f704c5de2a3e1.svg",
        "isPro": false,
        "fullname": "Ziteng Gao",
        "user": "sebgao",
        "type": "user"
      },
      "summary": "이 논문에서는 Diffusion via Autoregressive models (D-AR)를 소개합니다. 이는 이미지의 확산 처리를 표준의 다음 토큰 예측 형식으로의 버니 어노테이션 프로세스로 재구성한 새로운 패러다임입니다. 먼저, 이미지를 이산 토큰의 열로 변환하는 토큰나이저를 설계합니다. 이러한 토큰은 픽셀 공간의 다양한 위치에서 해석될 수 있습니다. 확산의 특성에 따라 이러한 토큰은 자연스럽게 코어 스트라피니의 순서대로 됩니다. 이는 어노테이션 모델링에 직접적 적합되어 있습니다. 따라서, 이러한 토큰에 대해 표준의 다음 토큰 예측을 적용하고, 원인적 마스크와 훈련/예측 전략의 설계를 변경하지 않습니다. 이러한 순차적 어노테이션 토큰 생성은 이미지 공간의 확산 처리를 직접 반영합니다. 즉, 어노테이션 모델이 토큰의 증가를 생성하면, 이러한 토큰을 직접 플로잉 모드로 해석할 수 있습니다. 우리의 파이프라인은 토큰의 서브셋을 생성할 때 연속적인 프리뷰를 지원하고, 0샷 제어된 합성을 가능하게 합니다. 표준의 ImageNet 벤치마크에서 775M의 Llama 백보드와 256개의 이산 토큰을 사용하며, 2.09의 FID를 달성했습니다. 우리 연구는 시각 합성의 통일 어노테이션 아키텍처의 미래 연구에 도움이 될 것으로 기대합니다. 코드와 모델은 https://github.com/showlab/D-AR에서 사용 가능합니다.",
      "upvotes": 17,
      "discussionId": "683906b0f85de1fc5639586b",
      "githubRepo": "https://github.com/showlab/D-AR",
      "ai_summary": "Diffusion via Autoregressive models (D-AR) recasts the image diffusion process as a standard autoregressive task, achieving high-quality image generation with consistent previews and layout control using a large language model backbone.",
      "ai_keywords": [
        "Diffusion via Autoregressive models",
        "autoregressive procedure",
        "next-token-prediction",
        "tokenizer",
        "discrete tokens",
        "diffusion denoising",
        "coarse-to-fine order",
        "autoregressive modeling",
        "autoregressive token generation",
        "FID",
        "Llama backbone"
      ]
    },
    "publishedAt": "2025-05-29T13:09:25.000Z",
    "title": "D-AR: Diffusion via Autoregressive Models",
    "summary": "This paper presents Diffusion via Autoregressive models (D-AR), a new\nparadigm recasting the image diffusion process as a vanilla autoregressive\nprocedure in the standard next-token-prediction fashion. We start by designing\nthe tokenizer that converts images into sequences of discrete tokens, where\ntokens in different positions can be decoded into different diffusion denoising\nsteps in the pixel space. Thanks to the diffusion properties, these tokens\nnaturally follow a coarse-to-fine order, which directly lends itself to\nautoregressive modeling. Therefore, we apply standard next-token prediction on\nthese tokens, without modifying any underlying designs (either causal masks or\ntraining/inference strategies), and such sequential autoregressive token\ngeneration directly mirrors the diffusion procedure in image space. That is,\nonce the autoregressive model generates an increment of tokens, we can directly\ndecode these tokens into the corresponding diffusion denoising step in the\nstreaming manner. Our pipeline naturally reveals several intriguing properties,\nfor example, it supports consistent previews when generating only a subset of\ntokens and enables zero-shot layout-controlled synthesis. On the standard\nImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone\nwith 256 discrete tokens. We hope our work can inspire future research on\nunified autoregressive architectures of visual synthesis, especially with large\nlanguage models. Code and models will be available at\nhttps://github.com/showlab/D-AR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23660.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648ac3d53470b17ccc90deaf",
      "avatarUrl": "/avatars/35b678baa7a91959f40f704c5de2a3e1.svg",
      "fullname": "Ziteng Gao",
      "name": "sebgao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23646",
      "authors": [
        {
          "_id": "683946c845636acda08ed401",
          "name": "Zijun Yao",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed402",
          "name": "Yantao Liu",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed403",
          "name": "Yanxu Chen",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed404",
          "name": "Jianhui Chen",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed405",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed406",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed407",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed408",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:53:41.000Z",
      "submittedOnDailyAt": "2025-05-30T04:31:18.157Z",
      "title": "이 모델이 호란시닝에 관심을 가지는 이유는 무엇입니까?",
      "submittedOnDailyBy": {
        "_id": "62e25e2247678ea5ce1b1786",
        "avatarUrl": "/avatars/1bb32e7597a9b1c89c434cbf550b5382.svg",
        "isPro": false,
        "fullname": "Yantao",
        "user": "RicardoL1u",
        "type": "user"
      },
      "summary": "최근 개발된 대규모 논리 모델(LRMs)은 긴 체인 오스키킹(CoT) 논리 능력이 있어 복잡한 태스크를 해결할 때 강력한 성능을 보여주고 있습니다. 이러한 LRMs은 주로 형식적인 논리 태스크에 의한 후학습으로 개발되어 있기 때문에, 그 논리 능력이 사실 탐색 태스크에서 해맨지 여부는 명확하지 않고 논의 중입니다. 예를 들어, DeepSeek-R1은 사실 탐색 벤치마크 SimpleQA에서 성능 향상을 보고 있지만, OpenAI-o3은 더 엄격한 해맨지 관찰을 합니다. 이 차이는 자연스럽게 다음 연구 문제로 이어집니다: 논리 모델은 해맨지에 의해 더욱 취약해졌는지? 본 논문은 이 문제를 해결하기 위해 3가지의 관점으로 접근합니다. (1) 먼저, LRMs의 해맨지에 대한 전반적인 평가를 수행합니다. 분석에 따르면, LRMs은 냉정한 시작의 서브젝트 조정(SFT)과 확인 가능한 보상 RL로 해맨지 감소를 할 수 있습니다が, 그 외의 열등 학습이나 냉정한 시작의 SFT를 제외한 RL 학습은 더 복잡한 해맨지를 일으킵니다. (2) LRMs의 해맨지에 미치는 영향에 대해 행동 분석을 수행합니다. LRMs의 사실성에 직접적인 영향을 미치는 두 가지 중요한 인지 행동을 특징화합니다: Flaw Repetition은 표면적인 논리 시도가 같은 잠재적인 오류 논리 반복을 수행하는 것입니다, Think-Answer Mismatch은 최종적인 답이 이전의 CoT 프로세스에 충실하지 않는 것입니다. (3) 또한, LRMs의 해맨지의 구조를 모델의 불확실성으로부터 조사합니다. LRMs의 해맨지의 증가는 일반적으로 모델의 불확실성과 사실적인 정확성의 비대칭과 연관되어 있습니다. 본 논문은 LRMs의 해맨지에 대해 처음으로 이해를 제공합니다.",
      "upvotes": 17,
      "discussionId": "683946c845636acda08ed42a",
      "ai_summary": "Large reasoning models exhibit varying susceptibility to hallucination depending on post-training pipelines, revealing critical cognitive behaviors and uncertainty misalignment as contributing factors.",
      "ai_keywords": [
        "large reasoning models",
        "chain-of-thought",
        "post-training",
        "formal reasoning tasks",
        "fact-seeking tasks",
        "DeepSeek-R1",
        "OpenAI-o3",
        "hallucination",
        "cold start supervised fine-tuning",
        "verifiable reward RL",
        "distillation",
        "Flaw Repetition",
        "Think-Answer Mismatch",
        "model uncertainty",
        "factual accuracy"
      ]
    },
    "publishedAt": "2025-05-29T12:53:41.000Z",
    "title": "Are Reasoning Models More Prone to Hallucination?",
    "summary": "Recently evolved large reasoning models (LRMs) show powerful performance in\nsolving complex tasks with long chain-of-thought (CoT) reasoning capability. As\nthese LRMs are mostly developed by post-training on formal reasoning tasks,\nwhether they generalize the reasoning capability to help reduce hallucination\nin fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1\nreports increased performance on SimpleQA, a fact-seeking benchmark, while\nOpenAI-o3 observes even severer hallucination. This discrepancy naturally\nraises the following research question: Are reasoning models more prone to\nhallucination? This paper addresses the question from three perspectives. (1)\nWe first conduct a holistic evaluation for the hallucination in LRMs. Our\nanalysis reveals that LRMs undergo a full post-training pipeline with cold\nstart supervised fine-tuning (SFT) and verifiable reward RL generally alleviate\ntheir hallucination. In contrast, both distillation alone and RL training\nwithout cold start fine-tuning introduce more nuanced hallucinations. (2) To\nexplore why different post-training pipelines alters the impact on\nhallucination in LRMs, we conduct behavior analysis. We characterize two\ncritical cognitive behaviors that directly affect the factuality of a LRM: Flaw\nRepetition, where the surface-level reasoning attempts repeatedly follow the\nsame underlying flawed logic, and Think-Answer Mismatch, where the final answer\nfails to faithfully match the previous CoT process. (3) Further, we investigate\nthe mechanism behind the hallucination of LRMs from the perspective of model\nuncertainty. We find that increased hallucination of LRMs is usually associated\nwith the misalignment between model uncertainty and factual accuracy. Our work\nprovides an initial understanding of the hallucination in LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23646.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e25e2247678ea5ce1b1786",
      "avatarUrl": "/avatars/1bb32e7597a9b1c89c434cbf550b5382.svg",
      "fullname": "Yantao",
      "name": "RicardoL1u",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22914",
      "authors": [
        {
          "_id": "6839533ae7922379b361b3cb",
          "name": "Maksim Kolodiazhnyi",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3cc",
          "name": "Denis Tarasov",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3cd",
          "user": {
            "_id": "67d5a331eab66ce9cb01bae4",
            "avatarUrl": "/avatars/3ed437c874889e8a6db66c3ef88a60c0.svg",
            "isPro": false,
            "fullname": "DMITRII ZHEMCHUZHNIKOV",
            "user": "zhemchuzhnikov",
            "type": "user"
          },
          "name": "Dmitrii Zhemchuzhnikov",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T06:42:03.218Z",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3ce",
          "name": "Alexander Nikulin",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3cf",
          "name": "Ilya Zisman",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3d0",
          "name": "Anna Vorontsova",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3d1",
          "name": "Anton Konushin",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3d2",
          "name": "Vladislav Kurenkov",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3d3",
          "name": "Danila Rukhovich",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T22:32:31.000Z",
      "submittedOnDailyAt": "2025-05-30T05:25:03.367Z",
      "title": "멀티모달 카드리스트레이션에 온라인 강화학습을 적용합니다.",
      "submittedOnDailyBy": {
        "_id": "665b10fb270e47e678f2ddf1",
        "avatarUrl": "/avatars/1bc7a9211acf767f7bfca998c24315a0.svg",
        "isPro": false,
        "fullname": "max",
        "user": "maksimko123",
        "type": "user"
      },
      "summary": "컴퓨터 설계 지원(CAD)는 공학과 제조공학에 중대한 역할을 수행하고 있으며, 정밀한 조정 가능한 3D 모델의 제작이 가능해졌습니다. 다양한 센서 및 사용자 제공 데이터를 CAD 재구성의 입력으로 사용함으로써, 디자인 애플리케이션의 접근이 민주화되어졌습니다. 그러나 현재의 방법은 일반적으로 포인트 센트러리나 이미지, 또는 텍스트 등 하나의 입력 모델을 중심으로 집중하는 데에 의존하며, 일반화 가능성과 강건성이 제한되어 있습니다. 최근의 시각 언어 모델(VLM)의 발전을 활용하여, 3가지 입력 모델을 동시에 처리하는 다중 모델 CAD 재구성 모델을 제안합니다. 대규모 프로세스 생성 데이터에 대한 규칙적 조정(SFT)과 온라인 피드백을 활용한 강화학습(RL) 조정을 적용하는 2단계 파이프라인을 사용합니다. 또한, LLM의 RL 조정을 CAD 작업에 처음으로 탐색하고, 온라인 RL 알고리즘의 예인인 Group Relative Preference Optimization(GRPO)가 오프라인 알고리즘을 초과하는 것을 보여주었습니다. DeepCAD 벤치마크에서, SFT 모델은 3가지 입력 모델을 동시에 능숙하게 수행하고 있습니다. 더욱 중요한 것은, RL 조정 후, cadrille은 3가지 어려운 데이터 세트에서 새로운 최선으로 발전했습니다.",
      "upvotes": 16,
      "discussionId": "6839533be7922379b361b418",
      "ai_summary": "A multi-modal CAD reconstruction model leveraging vision-language models and reinforcement learning achieves state-of-the-art performance across various datasets, including real-world ones.",
      "ai_keywords": [
        "vision-language models",
        "multi-modal",
        "large language model",
        "supervised fine-tuning",
        "reinforcement learning",
        "Group Relative Preference Optimization",
        "DeepCAD benchmark"
      ]
    },
    "publishedAt": "2025-05-28T18:32:31.000Z",
    "title": "cadrille: Multi-modal CAD Reconstruction with Online Reinforcement\n  Learning",
    "summary": "Computer-Aided Design (CAD) plays a central role in engineering and\nmanufacturing, making it possible to create precise and editable 3D models.\nUsing a variety of sensor or user-provided data as inputs for CAD\nreconstruction can democratize access to design applications. However, existing\nmethods typically focus on a single input modality, such as point clouds,\nimages, or text, which limits their generalizability and robustness. Leveraging\nrecent advances in vision-language models (VLM), we propose a multi-modal CAD\nreconstruction model that simultaneously processes all three input modalities.\nInspired by large language model (LLM) training paradigms, we adopt a two-stage\npipeline: supervised fine-tuning (SFT) on large-scale procedurally generated\ndata, followed by reinforcement learning (RL) fine-tuning using online\nfeedback, obtained programatically. Furthermore, we are the first to explore RL\nfine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such\nas Group Relative Preference Optimization (GRPO) outperform offline\nalternatives. In the DeepCAD benchmark, our SFT model outperforms existing\nsingle-modal approaches in all three input modalities simultaneously. More\nimportantly, after RL fine-tuning, cadrille sets new state-of-the-art on three\nchallenging datasets, including a real-world one.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22914.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "665b10fb270e47e678f2ddf1",
      "avatarUrl": "/avatars/1bc7a9211acf767f7bfca998c24315a0.svg",
      "fullname": "max",
      "name": "maksimko123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20088",
      "authors": [
        {
          "_id": "683947458987f50a5ec45a01",
          "name": "Nitay Calderon",
          "hidden": false
        },
        {
          "_id": "683947458987f50a5ec45a02",
          "name": "Liat Ein-Dor",
          "hidden": false
        },
        {
          "_id": "683947458987f50a5ec45a03",
          "name": "Roi Reichart",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62d6a0c18faee0ac953c51fa/C8RHn7j61J4qQlZvXd52v.png"
      ],
      "publishedAt": "2025-05-26T15:01:56.000Z",
      "submittedOnDailyAt": "2025-05-30T04:25:05.621Z",
      "title": "다각적인 취미 설명\n\n(Note: The translation provided is a direct literal translation of the original text. In a more natural Korean expression, it might be translated as \"다각적인 취미 설명\" or \"다각적인 취미 설명성\" to better convey the idea of \"explaining preferences across multiple domains.\")",
      "submittedOnDailyBy": {
        "_id": "62d6a0c18faee0ac953c51fa",
        "avatarUrl": "/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg",
        "isPro": false,
        "fullname": "Nitay Calderon",
        "user": "nitay",
        "type": "user"
      },
      "summary": "편향기법, 특히 인간의 편향, LLM-as-a-Judge (LaaJ), 보상모델 등이 대규모 언어 모델 (LLMs)의 조정과 평가에 중점적으로 사용됩니다. 그러나 이러한 편향을 주도하는 후방 개념은 아직 이해되지 않았습니다. 본 논문에서는 다양한 분야에서 편향의 지역적이고 전체적인 개념 기반을 생성하기 위한 완전한 자동화된 방법을 제안합니다. 우리의 방법은 선택된 응답이 거부된 응답 사이에서 구분할 수 있는 개념을 식별하고 이를 개념 기반의 벡터로 표현합니다. 개념과 편향 사이의 관계를 모델화하기 위해, 우리는 白箱 하이퍼바이지온 다중 도메인 회귀 모델을 제안하여 일반적이고 특정 도메인 효과를捉えます. 이 모델을 사용하여, 8가지 어려운 및 다양한 분야를 확장한 데이터 세트를 선택하여 12가지 기법을 설명합니다. 우리의 방법은 기준을 초과하는 강력한 편향 예측 성능을 달성하고, 동시에 설명이 가능합니다. 또한, 두 가지 애플리케이션 주도된 설정에서 설명을 평가합니다. 처음, LaaJ의 설명을 사용하여 LLM의 출력을 가이드하여 그 판단자가 일관된 선호하는 응답을 얻을 수 있습니다. 다음으로, LaaJ에 개념을 제시하여 이들의 편향 예측을 개선합니다. 이러한 성과는 LLMs 시대의 설명성의 새로운 패러다임에 입각합니다.",
      "upvotes": 15,
      "discussionId": "683947488987f50a5ec45a95",
      "ai_summary": "A new automated method using concept-based vectors and a Hierarchical Multi-Domain Regression model improves preference explanations and predictions for large language models.",
      "ai_keywords": [
        "LLM-as-a-Judge",
        "reward models",
        "large language models",
        "concept-based explanations",
        "domain-general effects",
        "domain-specific effects",
        "Hierarchical Multi-Domain Regression model",
        "preference prediction",
        "explainability"
      ]
    },
    "publishedAt": "2025-05-26T11:01:56.000Z",
    "title": "Multi-Domain Explainability of Preferences",
    "summary": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated method for generating\nlocal and global concept-based explanations of preferences across multiple\ndomains. Our method utilizes an LLM to identify concepts that distinguish\nbetween chosen and rejected responses, and to represent them with concept-based\nvectors. To model the relationships between concepts and preferences, we\npropose a white-box Hierarchical Multi-Domain Regression model that captures\nboth domain-general and domain-specific effects. To evaluate our method, we\ncurate a dataset spanning eight challenging and diverse domains and explain\ntwelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two application-driven settings. First,\nguiding LLM outputs with concepts from LaaJ explanations yields responses that\nthose judges consistently prefer. Second, prompting LaaJs with concepts\nexplaining humans improves their preference predictions. Together, our work\nestablishes a new paradigm for explainability in the era of LLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62d6a0c18faee0ac953c51fa/C8RHn7j61J4qQlZvXd52v.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d6a0c18faee0ac953c51fa",
      "avatarUrl": "/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg",
      "fullname": "Nitay Calderon",
      "name": "nitay",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23380",
      "authors": [
        {
          "_id": "683906f81bf7a7a94309c5a5",
          "name": "Weijia Mao",
          "hidden": false
        },
        {
          "_id": "683906f81bf7a7a94309c5a6",
          "name": "Zhenheng Yang",
          "hidden": false
        },
        {
          "_id": "683906f81bf7a7a94309c5a7",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T12:00:15.000Z",
      "submittedOnDailyAt": "2025-05-30T02:41:07.217Z",
      "title": "UniRL: 자기 개선을 위한 유일한 유니모달 모델에 의한 지도 학습과 강화 학습",
      "submittedOnDailyBy": {
        "_id": "63f320ee0be81bdc5d8ecb88",
        "avatarUrl": "/avatars/9d08cff6ed23a51887c869947bc03228.svg",
        "isPro": false,
        "fullname": "Mao Weijia",
        "user": "benzweijia",
        "type": "user"
      },
      "summary": "統一多モデル의 대규모 언어 모델인 Show-o와 Janus은 생성과 이해의 두 가지 태스크 모두에서 강력한 성능을 달성하고 있습니다. 그러나 이러한 모델들은 일반적으로 큰 데이터 세트를 기반으로, 사전 학습 단계에서 많은 계산량을 필요로 합니다. 또한 여러 후 학습 방법들이 제안되어 있지만, 이들은 일반적으로 외부 데이터를 의존하거나, 태스크에专用한 customization으로 제한되어 있습니다. 본 논문에서는 UniRL이라는 자동 개선의 후 학습 접근법을 소개합니다. 우리 접근법은 모델이 Prompt로부터 이미지를 생성하고, 이들을 각 반복에서 학습 데이터로 사용하게 하는 것입니다. 또한 이러한 두 가지 태스크가 서로 강화될 수 있습니다: 생성된 이미지는 이해에 사용되고, 이해의 결과를 생성을 제어할 수 있습니다. 우리는 표준의 微调 (SFT)과 그룹의 상대적 최적화 (GRPO)를 사용하여 모델을 최적화합니다. UniRL은 세 가지 주요 장점을 제공합니다: (1) 외부 이미지 데이터를 의존하지 않아, 모든 학습 샘플은 학습 중 모델이 생성하는 것입니다; (2) 모델의 개별 태스크의 성능을 향상시켜, 생성과 이해 사이의 불균형을 줄입니다; (3) 후 학습 단계에서, 단지 몇 개의 추가 학습 단계만 필요합니다. UniRL은 Show-o와 Janus에 평가되었으며, Show-o의 GenEval 점수는 0.77, Janus의 GenEval 점수는 0.65입니다. 코드와 모델은 https://github.com/showlab/UniRL에서 공개됩니다.",
      "upvotes": 14,
      "discussionId": "683906f91bf7a7a94309c5dd",
      "ai_summary": "UniRL is a self-improving post-training method for unified multimodal large language models that uses generated images as training data, enhancing both generation and understanding tasks without external data.",
      "ai_keywords": [
        "Unified multimodal large language models",
        "Show-o",
        "Janus",
        "self-improving post-training",
        "prompts",
        "training data",
        "supervised fine-tuning",
        "Group Relative Policy Optimization",
        "GenEval score"
      ]
    },
    "publishedAt": "2025-05-29T08:00:15.000Z",
    "title": "UniRL: Self-Improving Unified Multimodal Models via Supervised and\n  Reinforcement Learning",
    "summary": "Unified multimodal large language models such as Show-o and Janus have\nachieved strong performance across both generation and understanding tasks.\nHowever, these models typically rely on large-scale datasets and require\nsubstantial computation during the pretraining stage. In addition, several\npost-training methods have been proposed, but they often depend on external\ndata or are limited to task-specific customization. In this work, we introduce\nUniRL, a self-improving post-training approach. Our approach enables the model\nto generate images from prompts and use them as training data in each\niteration, without relying on any external image data. Moreover, it enables the\ntwo tasks to enhance each other: the generated images are used for\nunderstanding, and the understanding results are used to supervise generation.\nWe explore supervised fine-tuning (SFT) and Group Relative Policy Optimization\n(GRPO) to optimize the models. UniRL offers three key advantages: (1) it\nrequires no external image data, as all training samples are generated by the\nmodel itself during training; (2) it not only improves individual task\nperformance, but also reduces the imbalance between generation and\nunderstanding; and (3) it requires only several additional training steps\nduring the post-training stage. We evaluate UniRL on top of Show-o and Janus,\nachieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and\nmodels will be released in https://github.com/showlab/UniRL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23380.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f320ee0be81bdc5d8ecb88",
      "avatarUrl": "/avatars/9d08cff6ed23a51887c869947bc03228.svg",
      "fullname": "Mao Weijia",
      "name": "benzweijia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23419",
      "authors": [
        {
          "_id": "68391574f85de1fc563cd890",
          "name": "Linghao Zhang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd891",
          "name": "Shilin He",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd892",
          "name": "Chaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd893",
          "name": "Yu Kang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd894",
          "name": "Bowen Li",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd895",
          "name": "Chengxing Xie",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd896",
          "name": "Junhao Wang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd897",
          "name": "Maoquan Wang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd898",
          "name": "Yufan Huang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd899",
          "name": "Shengyu Fu",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89a",
          "name": "Elsie Nallipogu",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89b",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89c",
          "name": "Yingnong Dang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89d",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89e",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T13:09:44.000Z",
      "submittedOnDailyAt": "2025-05-30T00:49:19.429Z",
      "title": "SWE-bench 서비스가 시작되었습니다!",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "문제 해결 태스크에서, 모델이 실세계의 버그를 수정하기 위해 패치를 생성하는 것을 대규모 언어 모델(LLMs)의 능력을 평가하는 중요한 벤치마크로 등장하게 되었습니다. 그러나 SWE-bench 및 그의 변체는 초기 릴리즈 이후 업데이트되지 않고, 좁은 리포지토리 세트를 커버하고, 인스턴스 구축과 환경 설정에 있어서 수동적인 노력을 중시하여, 이러한 요소들은 scalability를 방해하고, 과적합 및 데이터 오염의 위험을 불러일으키게 됩니다. 본 논문에서는 이러한 도전을 극복하기 위해, SWE-bench-Live라는 리비드디닝 가능한 벤치마크를 소개합니다. 초기 릴리즈는 2024년 이후의 실제 GitHub 문제로부터 1,319개의 태스크로 이루어져 있으며, 93개의 포지토리를 포함하고 있습니다. 각 태스크는 재현성 보장을 목적으로 추가된 문서 이미지를 갖습니다. 이 벤치마크의 핵심은 인스턴스 생성부터 환경 설정까지의 모든 과정을 스트리밍화하는 자동화된 카레티브 파이프라인입니다. 이로써 수동적인 봇tlen을 제거하고 scalability와 연속적인 업데이트가 가능합니다. SWE-bench-Live에서, 최신의 종류의 에이전트 프레임워크와 LLMs를 평가하고, 정적 벤치마크와 비교하여도, 진보적인 성능 간격을 명확히 나타냅니다. 이러한 차이를 이해하기 위해, 리포지토리의 원천, 문제의 신이, 태스크의 난이도를 Detail Analysis를 수행했습니다. 리비드디닝 가능한 리포지토리 활동에 기반한 새로운, 다양한, 실행 가능한 벤치마크를 제공함으로써, SWE-bench-Live는 LLMs와 에이전트의 엄격한, 오염抵抗성 있는 평가가 가능하게 하며, 동적인, 실세계적인 소프트웨어 개발 환경에서의 평가를 촉진합니다.",
      "upvotes": 13,
      "discussionId": "68391574f85de1fc563cd8d4",
      "ai_summary": "SWE-bench-Live is a continuously updatable benchmark for evaluating LLMs in issue resolution, featuring live GitHub issues and automated curation to ensure scalability and contamination resistance.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "SWE-bench",
        "SWE-bench-Live",
        "live-updatable benchmark",
        "GitHub issues",
        "Docker image",
        "automated curation pipeline",
        "reproducible execution",
        "performance evaluation",
        "repository origin",
        "issue recency",
        "task difficulty"
      ]
    },
    "publishedAt": "2025-05-29T09:09:44.000Z",
    "title": "SWE-bench Goes Live!",
    "summary": "The issue-resolving task, where a model generates patches to fix real-world\nbugs, has emerged as a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs). While SWE-bench and its variants have become\nstandard in this domain, they suffer from key limitations: they have not been\nupdated since their initial releases, cover a narrow set of repositories, and\ndepend heavily on manual effort for instance construction and environment\nsetup. These factors hinder scalability and introduce risks of overfitting and\ndata contamination. In this work, we present SWE-bench-Live, a\nlive-updatable benchmark designed to overcome these challenges. Our\ninitial release consists of 1,319 tasks derived from real GitHub issues created\nsince 2024, spanning 93 repositories. Each task is accompanied by a dedicated\nDocker image to ensure reproducible execution. Central to our benchmark is\n\\method, an automated curation pipeline that streamlines the entire process\nfrom instance creation to environment setup, removing manual bottlenecks and\nenabling scalability and continuous updates. We evaluate a range of\nstate-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a\nsubstantial performance gap compared to static benchmarks like SWE-bench, even\nunder controlled evaluation conditions. To better understand this discrepancy,\nwe perform detailed analyses across repository origin, issue recency, and task\ndifficulty. By providing a fresh, diverse, and executable benchmark grounded in\nlive repository activity, SWE-bench-Live facilitates rigorous,\ncontamination-resistant evaluation of LLMs and agents in dynamic, real-world\nsoftware development settings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22618",
      "authors": [
        {
          "_id": "683931e1b6280677f75edf09",
          "name": "Chengyue Wu",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0a",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0b",
          "user": {
            "_id": "64cf5e81a2e7f9ff61eb3a0c",
            "avatarUrl": "/avatars/cbaa11daec9d4113bf7de93fe9b9ee86.svg",
            "isPro": false,
            "fullname": "scxue",
            "user": "Cauthyyy",
            "type": "user"
          },
          "name": "Shuchen Xue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:19.453Z",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0c",
          "user": {
            "_id": "650dac79b959b0e1d41d7378",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dac79b959b0e1d41d7378/mzbN0MFk3k8b94FQ40I7L.jpeg",
            "isPro": false,
            "fullname": "Zhijian Liu",
            "user": "zhijianliu",
            "type": "user"
          },
          "name": "Zhijian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:22.124Z",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0d",
          "name": "Shizhe Diao",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0e",
          "name": "Ligeng Zhu",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0f",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf10",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf11",
          "name": "Enze Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:39:15.000Z",
      "submittedOnDailyAt": "2025-05-30T02:50:04.196Z",
      "title": "Fast-dLLM: 훈련 불필요한 디퓨션 LLM의 속도 향상을 통한 KV 캐시 및 병렬 검증\n\n(请注意，虽然翻译力求准确，但在某些技术术语的翻译上可能存在细微差异，特别是对于特定的专业领域。如果需要最精确的翻译，建议咨询相关领域的专家。)",
      "submittedOnDailyBy": {
        "_id": "633bd54b00732349209a18fe",
        "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
        "isPro": false,
        "fullname": "Shizhe Diao",
        "user": "shizhediao",
        "type": "user"
      },
      "summary": "Diffusion 기반의 대규모 언어 모델 (Diffusion LLMs)은 병렬 해석 능력이 뛰어나고, 비자동 응답 생성에 특화된 모델입니다. 그러나 Key-Value (KV) Cache가 없기 때문에, 오픈 소스된 Diffusion LLMs의 실제적인 추론 속도는 자동 응답 모델보다 느리고, 동시에 여러 토큰을 해석할 때 품질이 떨어집니다. 이러한 문제를 해결하기 위해, 우리는 양방향 Diffusion 모델에 적합한 새로운 블록 단위의 근사 KV Cache 구조를 도입하여, 성능 저하를 최소화하고 캐시를 재활용할 수 있도록 하였습니다. 또한 병렬 해석에서 생성 품질의 저하의 근원 원인을 찾아, 조건 독립성 가정에 의한 토큰의 의존관계의 파괴를 원인으로 인정했습니다. 이에 대처하여, 우리는 신뢰도가 높은 토큰을 선택적으로 해석함으로써 의존관계의 위반을 완화하고, 생성 품질을 유지하는 신뢰도 기반 병렬 해석 전략을 제안했습니다. LLaDA와 Dream 모델의 여러 LLM 벤치마크에서 수행한 실험 결과에 따르면, 정확도 손실이 최소화되고, 자동 응답 모델과의 성능 간격이 좁아지며, Diffusion LLMs의 실제적인 적용에 적합하도록 하였습니다.",
      "upvotes": 11,
      "discussionId": "683931e2b6280677f75edf32",
      "ai_summary": "A novel block-wise approximate KV Cache and confidence-aware parallel decoding strategy improve the inference speed of diffusion-based large language models without significant quality loss.",
      "ai_keywords": [
        "diffusion-based large language models (Diffusion LLMs)",
        "non-autoregressive text generation",
        "parallel decoding",
        "Key-Value (KV) Cache",
        "bidirectional diffusion models",
        "token dependencies",
        "confidence-aware parallel decoding",
        "LLaDA",
        "Dream models",
        "LLM benchmarks"
      ]
    },
    "publishedAt": "2025-05-28T13:39:15.000Z",
    "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
    "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to 27.6times\nthroughput improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633bd54b00732349209a18fe",
      "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
      "fullname": "Shizhe Diao",
      "name": "shizhediao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23606",
      "authors": [
        {
          "_id": "6839189f4a3a71a917b0514e",
          "user": {
            "_id": "656724074f6ec72017754d33",
            "avatarUrl": "/avatars/e61de248f6f53719b2375077340dd033.svg",
            "isPro": false,
            "fullname": "QingyuShi",
            "user": "QingyuShi",
            "type": "user"
          },
          "name": "Qingyu Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:45.160Z",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b0514f",
          "user": {
            "_id": "63fccdac93b993a4ebd7789a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
            "isPro": false,
            "fullname": "Jinbin Bai",
            "user": "BryanW",
            "type": "user"
          },
          "name": "Jinbin Bai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:47.122Z",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05150",
          "name": "Zhuoran Zhao",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05151",
          "name": "Wenhao Chai",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05152",
          "name": "Kaidong Yu",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05153",
          "name": "Jianzong Wu",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05154",
          "name": "Shuangyong Song",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05155",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05156",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05157",
          "name": "Xuelong Li",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05158",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:15:48.000Z",
      "submittedOnDailyAt": "2025-05-30T01:06:53.607Z",
      "title": "ムドィット：문맥으로부터 이미지를 생성하는 것보다, 통일된 분산 디피라션 모형을 사용하여 미래를 해방할 수 있습니다.",
      "submittedOnDailyBy": {
        "_id": "63fccdac93b993a4ebd7789a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
        "isPro": false,
        "fullname": "Jinbin Bai",
        "user": "BryanW",
        "type": "user"
      },
      "summary": "통합 생성 모듈은 텍스트 생성, 이미지 생성, 비전 언어 추론 등 다양한 태스크를 하나의 아키텍처와 디코딩 패러다임으로 처리하는 것을 목표로 하고 있습니다. 자동 순차 생성 모듈은 순차 디코딩으로 추론이 느리고, 비자동 순차 생성 모듈은 한정된 사전 학습 백본으로 인해 일반화에 어려움을 겪습니다. 우리는 Muddit, 텍스트와 이미지의 양쪽 모델 데이터와 병렬적인 고속 생성을 가능하게 하는 통합적인 이산 디피레전서 트랜스포머를 소개합니다. 기존의 통합 디피레전서 모듈과 달리, Muddit은 강력한 이미지 프로이드를 가진 사전 학습 텍스트타운 이미지 백본과 가벼운 텍스트 디코더를 통합하여 하나의 아키텍처로 유연하고 고품질의 다 모델 생성을 가능하게 합니다. 실험 결과를 통해, Muddit은 질과 효율에 있어서 가장 큰 자동 순차 모듈과 상대적으로 또는 상위 성능을 달성합니다. 본 논문은 강력한 이미지 프로이드를 가진 이산 디피레전서를 통합 생성의 유연성과 효율적인 백본으로 하여 가능한 가능성을 높입니다.",
      "upvotes": 10,
      "discussionId": "683918a14a3a71a917b051ea",
      "githubRepo": "https://github.com/M-E-AGI-Lab/Muddit",
      "ai_summary": "Muddit, a unified discrete diffusion transformer, achieves fast and high-quality generation across text and image modalities by integrating pretrained visual priors with a lightweight text decoder.",
      "ai_keywords": [
        "unified generation models",
        "autoregressive models",
        "non-autoregressive models",
        "discrete diffusion",
        "diffusion transformer",
        "pretrained backbones",
        "flexible generation",
        "multimodal generation",
        "text generation",
        "image generation",
        "vision-language reasoning",
        "quality",
        "efficiency",
        "visual priors",
        "scalable backbone"
      ]
    },
    "publishedAt": "2025-05-29T12:15:48.000Z",
    "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified\n  Discrete Diffusion Model",
    "summary": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23606.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63fccdac93b993a4ebd7789a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
      "fullname": "Jinbin Bai",
      "name": "BryanW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23585",
      "authors": [
        {
          "_id": "683916a7a2d6f83cf12552dd",
          "name": "Yaru Hao",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552de",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552df",
          "name": "Xun Wu",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552e0",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552e1",
          "name": "Zewen Chi",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552e2",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T15:58:04.000Z",
      "submittedOnDailyAt": "2025-05-30T00:53:45.844Z",
      "title": "정책 프로젝트의 RL에 가장 적합한 보상 기반 라인 프레임을 사용하여 추가한 것",
      "submittedOnDailyBy": {
        "_id": "5df85abada6d0311fd3d5408",
        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
        "isPro": false,
        "fullname": "Li Dong",
        "user": "unilm",
        "type": "user"
      },
      "summary": "강화학습 알고리즘은 대규모 언어 모델과 인간 취향을 일치시키고 논리력의 능력을 향상시키기 위한 기초적인 역할을 합니다. 그러나 현재의 강화학습 알고리즘은 정책 제약의 엄밀성 낮은으로 인한 훈련 불안정성과 보조 모델에 의한 계산 네트워크의 효율성 낮은으로 인한 훈련 불안정성에서 어려움을 겪고 있습니다. 본 논문에서는 이러한 문제를 해결하기 위해 새로운 단순화된 강화학습 알고리즘인 \"On-Policy RL with Optimal reward baseline (OPO)\"를 제안합니다. OPO는 실험적으로 훈련 프로세스의 안정화와 탐색의 향상을 위해 정확한 정책 훈련의 중요성을 강조하고 있습니다. 또한 OPO는 이론적으로 분산의 최소화를 위해 최적의 보상 기반 라인을 도입하고 있습니다. 수학적 논리론 벤치마크에서 OPO를 평가했습니다. 결과는 추가된 모델이나 정규화 항을 포함하지 않고도 상위 성능과 훈련의 안정화를 보여주었습니다. 또한 OPO는 더 낮은 정책 이동과 더 높은 출력 히트 이벤트를 구현하고, 더 다양한 재복사되지 않는 응답을 촉발시킵니다. 이러한 결과를 통해 OPO가 대규모 언어 모델의 대응과 논리론 태스크에서 안정적이고 효과적인 강화학습의 새로운 방향을 보여줍니다. 구현은 https://github.com/microsoft/LMOps/tree/main/opo에서 제공됩니다.",
      "upvotes": 9,
      "discussionId": "683916a8a2d6f83cf1255310",
      "githubRepo": "https://github.com/microsoft/LMOps/tree/main/opo",
      "ai_summary": "A novel reinforcement learning algorithm, OPO, improves training stability and performance in large language model alignment and reasoning by emphasizing exact on-policy training and using an optimal reward baseline.",
      "ai_keywords": [
        "reinforcement learning",
        "on-policy constraints",
        "computational efficiency",
        "auxiliary models",
        "optimal reward baseline",
        "gradient variance",
        "policy shifts",
        "output entropy",
        "mathematical reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-05-29T11:58:04.000Z",
    "title": "On-Policy RL with Optimal Reward Baseline",
    "summary": "Reinforcement learning algorithms are fundamental to align large language\nmodels with human preferences and to enhance their reasoning capabilities.\nHowever, current reinforcement learning algorithms often suffer from training\ninstability due to loose on-policy constraints and computational inefficiency\ndue to auxiliary models. In this work, we propose On-Policy RL with Optimal\nreward baseline (OPO), a novel and simplified reinforcement learning algorithm\ndesigned to address these challenges. OPO emphasizes the importance of exact\non-policy training, which empirically stabilizes the training process and\nenhances exploration. Moreover, OPO introduces the optimal reward baseline that\ntheoretically minimizes gradient variance. We evaluate OPO on mathematical\nreasoning benchmarks. The results demonstrate its superior performance and\ntraining stability without additional models or regularization terms.\nFurthermore, OPO achieves lower policy shifts and higher output entropy,\nencouraging more diverse and less repetitive responses. These results highlight\nOPO as a promising direction for stable and effective reinforcement learning in\nlarge language model alignment and reasoning tasks. The implementation is\nprovided at https://github.com/microsoft/LMOps/tree/main/opo.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23585.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5df85abada6d0311fd3d5408",
      "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
      "fullname": "Li Dong",
      "name": "unilm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22421",
      "authors": [
        {
          "_id": "68391dbb0653b6a3441a7f7e",
          "user": {
            "_id": "6311d9ee04f842f79916158c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PvOYgDEpe0x5ExLyj1BK-.png",
            "isPro": false,
            "fullname": "chen",
            "user": "antonio-c",
            "type": "user"
          },
          "name": "Anthony Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:38.940Z",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f7f",
          "name": "Wenzhao Zheng",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f80",
          "user": {
            "_id": "647068944be5cf1f33491cb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Aqudl2PTSKPAylBhlccWr.png",
            "isPro": false,
            "fullname": "Yida Wang",
            "user": "wangyida",
            "type": "user"
          },
          "name": "Yida Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:36.769Z",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f81",
          "name": "Xueyang Zhang",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f82",
          "name": "Kun Zhan",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f83",
          "name": "Peng Jia",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f84",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f85",
          "name": "Shanghang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T14:46:51.000Z",
      "submittedOnDailyAt": "2025-05-30T01:26:00.575Z",
      "title": "GeoDrive: 3D 기지오메트리 정보付한 운전 월드 모델과 정확한 액션 제어",
      "submittedOnDailyBy": {
        "_id": "6311d9ee04f842f79916158c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PvOYgDEpe0x5ExLyj1BK-.png",
        "isPro": false,
        "fullname": "chen",
        "user": "antonio-c",
        "type": "user"
      },
      "summary": "최근의 세계 모델의 발전은 동적인 환경 시뮬레이션에 혁신적인 영향을 미쳤으며, 시스템이 미래의 상태를 예측하고 잠재적인 행동을 평가할 수 있게 되었습니다. 자동 운행에 있어서, 이러한 능력은 차량이 다른 도로 사용자의 행동을 예측하고 위험에 대한 계획을 수행하고 시뮬레이션 훈련을 가속화하고 새로운 시나리오에 적응할 수 있게 되어, 안전성과 신뢰성을 향상시킬 수 있습니다. 현재의 접근 방식은 신뢰할 수 있는 안전성 평가가 필요했던 강력한 3D 제멘토닉 일관성 유지 또는 블로킹 처리 시의artifact의 축적에 약점을 보입니다. 이에 대비하여, 우리는 GeoDrive를 소개하며, 자동 운행의 세계 모델에 강력한 3D 제멘토닉 조건을 명확히 하여, 공간 이해와 행동 제어 가능성을 향상시켜, 안전한 자동 운행을 위해 더 현실적이고 적응적이고 신뢰할 수 있는 시나리오 모델링을 실현하는 데 목표를 세웁니다. 특히, 입력 프레임에서 3D 표현을 추출하고 사용자 지정의 차량의궤도를 기반으로 2D 렌더링을 얻습니다. 동적인 모델링을 가능하게 하기 위해, 훈련 중 동적인 편집 모듈을 제안하고 차량의 위치를 편집하여 렌더링을 개선합니다. 확장된 실험에 따르면, 우리의 방법은 기존 모델보다行动 정확도와 3D 공간 인식 모두에서 유의미하게 뛰어나며, 안전한 자동 운행에서의 더 현실적이고 적응적이고 신뢰할 수 있는 시나리오 모델링을 실현하는 데 보여집니다. 또한, 우리의 모델은 새로운궤도에 일반화할 수 있으며, 물체 편집이나 물체궤도 제어 등 상호작용 시나리오 편집 기능을 제공합니다.",
      "upvotes": 9,
      "discussionId": "68391dbc0653b6a3441a7fd1",
      "githubRepo": "https://github.com/antonioo-c/GeoDrive",
      "ai_summary": "GeoDrive integrates robust 3D geometry into driving world models to improve spatial understanding and action controllability in autonomous navigation, enhancing safety and reliability.",
      "ai_keywords": [
        "world models",
        "dynamic environment simulation",
        "autonomous driving",
        "3D geometric consistency",
        "occlusion handling",
        "3D representation",
        "2D rendering",
        "ego-car trajectory",
        "dynamic editing module",
        "action accuracy",
        "3D spatial awareness",
        "scene modeling",
        "object editing",
        "object trajectory control"
      ]
    },
    "publishedAt": "2025-05-28T10:46:51.000Z",
    "title": "GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action\n  Control",
    "summary": "Recent advancements in world models have revolutionized dynamic environment\nsimulation, allowing systems to foresee future states and assess potential\nactions. In autonomous driving, these capabilities help vehicles anticipate the\nbehavior of other road users, perform risk-aware planning, accelerate training\nin simulation, and adapt to novel scenarios, thereby enhancing safety and\nreliability. Current approaches exhibit deficiencies in maintaining robust 3D\ngeometric consistency or accumulating artifacts during occlusion handling, both\ncritical for reliable safety assessment in autonomous navigation tasks. To\naddress this, we introduce GeoDrive, which explicitly integrates robust 3D\ngeometry conditions into driving world models to enhance spatial understanding\nand action controllability. Specifically, we first extract a 3D representation\nfrom the input frame and then obtain its 2D rendering based on the\nuser-specified ego-car trajectory. To enable dynamic modeling, we propose a\ndynamic editing module during training to enhance the renderings by editing the\npositions of the vehicles. Extensive experiments demonstrate that our method\nsignificantly outperforms existing models in both action accuracy and 3D\nspatial awareness, leading to more realistic, adaptable, and reliable scene\nmodeling for safer autonomous driving. Additionally, our model can generalize\nto novel trajectories and offers interactive scene editing capabilities, such\nas object editing and object trajectory control.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22421.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6311d9ee04f842f79916158c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PvOYgDEpe0x5ExLyj1BK-.png",
      "fullname": "chen",
      "name": "antonio-c",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23758",
      "authors": [
        {
          "_id": "683925243a3289061eda69ee",
          "user": {
            "_id": "65454d7c117ecae648892170",
            "avatarUrl": "/avatars/83a7091a24bf86801176ca85234b417a.svg",
            "isPro": false,
            "fullname": "Yusuf Dalva",
            "user": "ydalva",
            "type": "user"
          },
          "name": "Yusuf Dalva",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:26.107Z",
          "hidden": false
        },
        {
          "_id": "683925243a3289061eda69ef",
          "name": "Hidir Yesiltepe",
          "hidden": false
        },
        {
          "_id": "683925243a3289061eda69f0",
          "name": "Pinar Yanardag",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:46.000Z",
      "submittedOnDailyAt": "2025-05-30T01:56:15.021Z",
      "title": "LoRAShop: 훈련없이 다 개념 이미지 생성과 편집에 있어서 수정용 플로트 라ン스 폼어머\n\n(注意：虽然要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，这里对“修正용 플로트 라ン스 폼어머”进行了适当调整，以确保其含义清晰。)",
      "submittedOnDailyBy": {
        "_id": "65454d7c117ecae648892170",
        "avatarUrl": "/avatars/83a7091a24bf86801176ca85234b417a.svg",
        "isPro": false,
        "fullname": "Yusuf Dalva",
        "user": "ydalva",
        "type": "user"
      },
      "summary": "LoRAShop는 LoRA 모델을 사용하여 다 개념 이미지 편집의 첫 번째 프레임워크입니다. LoRAShop는 Flux 스타일의 diffusion transformers 내부의 특징 상호작용 패턴에 대한 중요한 발견을 기반으로 구축되어 있습니다: 개념 고유의 transformer 특징이 노이즈 처리 초기에 공간적으로 일관된 영역에 활성화됩니다. 이 발견을 활용하여, 각 개념에 대해 선형 레이턴트 마스크를 계산하고, 해당 개념을 정의하는 영역 내에서만 대응하는 LoRA 가중치를 블렌딩합니다. 이로 인해, 최종적인 편집은 원본 스케인에서 여러 주제와 스타일을 무결하게 통합하고, 글로벌 컨텍스트, 조명, 그리고 세부 사항을 유지합니다. 우리의 실험은 LoRAShop가 기본 라인과 비교하여 더 좋은 아웃풋 유지를 제공함을 보여줍니다. 리트레이닝과 외부 제약을 제거함으로써, LoRAShop는 개인화된 디포어마 모델을 실용적인 \"LoRA를 사용하는 Photoshop\" 툴로 변환하고, 구성적인 시각적인 슈트 리젝팅 및 빠른 창의적 인터랙션의 새로운 길을 열어줍니다.",
      "upvotes": 8,
      "discussionId": "683925263a3289061eda6a62",
      "projectPage": "https://lorashop.github.io/",
      "ai_summary": "LoRAShop, a framework for multi-concept image editing with LoRA models, leverages spatially coherent feature activation in Flux-style diffusion transformers to achieve seamless integration of multiple subjects or styles while preserving global context and identity.",
      "ai_keywords": [
        "Flux-style diffusion transformers",
        "concept-specific transformer features",
        "denoising process",
        "disentangled latent mask",
        "LoRA models",
        "identity preservation",
        "personalized diffusion models",
        "compositional visual storytelling",
        "rapid creative iteration"
      ]
    },
    "publishedAt": "2025-05-29T13:59:46.000Z",
    "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with\n  Rectified Flow Transformers",
    "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23758.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65454d7c117ecae648892170",
      "avatarUrl": "/avatars/83a7091a24bf86801176ca85234b417a.svg",
      "fullname": "Yusuf Dalva",
      "name": "ydalva",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23735",
      "authors": [
        {
          "_id": "6839158b56bcc85d9f92199b",
          "name": "Ali Behrouz",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199c",
          "name": "Zeman Li",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199d",
          "name": "Praneeth Kacham",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199e",
          "name": "Majid Daliri",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199f",
          "name": "Yuan Deng",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f9219a0",
          "name": "Peilin Zhong",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f9219a1",
          "name": "Meisam Razaviyayn",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f9219a2",
          "name": "Vahab Mirrokni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:57:16.000Z",
      "submittedOnDailyAt": "2025-05-30T00:49:34.783Z",
      "title": "ATLAS: 테스트 시간 동안 최적의 컨텍스트를 기억하는 학습",
      "submittedOnDailyBy": {
        "_id": "65cccd5134a5d74cbaa9446c",
        "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
        "isPro": false,
        "fullname": "Ali Behrouz",
        "user": "AliBehrouz",
        "type": "user"
      },
      "summary": "Transformers는 순서 모델링의 가장 인기 있는 기본 모델로 확립되어 있습니다. 주로, In-Context Retrieval 태스크의 효율성과 Scalability의 학습 능력으로 이러한 특징을 지닌다. 그러나, 2차원 메모리와 시간 복잡성은 긴 시퀀스에 대한 적용 범위를 제한하고, 이에 따라 연구자들은 현대의 리커렌트 뉴럴 네트워크(장기 리커렌트 메모리 모듈)와 같은 유효한 개선을 탐색하게 되었습니다. 최근, 다양한 하류 태스크의 성공에 더해, 긴 컨텍스트 이해와 긴 시퀀스에서의 외래가 필요하는 태스크에서는 이러한 개선이 어려워졌습니다. 우리는 이러한 결함을 3가지 다른 측면에서 발생시키는 것을 발견했습니다. 1. 메모리 용량은 입력의 메모리 구조와 특징 매핑에 의해 제한됩니다. 2. 메모리 업데이트는 온라인 특성이며, 마지막 입력에만 최적화됩니다. 3. 고정 크기의 메모리 관리는 표현력이 낮습니다. 이러한 3가지 면을 개선하기 위해, 우리는 현재와 과거 토큰에 기반하여 메모리를 최적화하고, 장기 메모리 모델의 온라인 특성을 극복하기 위해 고 용량의 장기 메모리 모듈을 제안합니다. ATLAS. 이 통찰에 기반하여, 우리는 Transformer와 같은 새로운 아키텍처의 가족인 DeepTransformers를 제안합니다. 이러한 아키텍처는 원의 Transformer 아키텍처의 엄밀한 일반화에서부터 시작됩니다. 언어 모델링, 일반 지식 추론, 호출 강조, 긴 컨텍스트 이해 태스크의 실험 결과에 따르면, ATLAS는 Transformers와 최근의 선형 리커렌트 모델의 성능을 초과합니다. ATLAS는 Titans의 긴 컨텍스트 성능을 향상시키고, BABILong 벤치마크의 10M 컨텍스트 길이에서 정확도를 +80% 상승시켰습니다.",
      "upvotes": 8,
      "discussionId": "6839158c56bcc85d9f9219fc",
      "ai_summary": "A new long-term memory module called ATLAS addresses limitations of Transformers in handling long contexts by optimizing memory based on current and past inputs, leading to improved performance in long-context understanding tasks.",
      "ai_keywords": [
        "Transformers",
        "sequence modeling",
        "in-context retrieval",
        "memory capacity",
        "online nature",
        "fixed-size memory",
        "long-term memory module",
        "DeepTransformers",
        "language modeling",
        "common-sense reasoning",
        "recall-intensive tasks",
        "long-context understanding",
        "ATLAS",
        "Titans",
        "BABILong benchmark"
      ]
    },
    "publishedAt": "2025-05-29T13:57:16.000Z",
    "title": "ATLAS: Learning to Optimally Memorize the Context at Test Time",
    "summary": "Transformers have been established as the most popular backbones in sequence\nmodeling, mainly due to their effectiveness in in-context retrieval tasks and\nthe ability to learn at scale. Their quadratic memory and time complexity,\nhowever, bound their applicability in longer sequences and so has motivated\nresearchers to explore effective alternative architectures such as modern\nrecurrent neural networks (a.k.a long-term recurrent memory module). Despite\ntheir recent success in diverse downstream tasks, they struggle in tasks that\nrequires long context understanding and extrapolation to longer sequences. We\nobserve that these shortcomings come from three disjoint aspects in their\ndesign: (1) limited memory capacity that is bounded by the architecture of\nmemory and feature mapping of the input; (2) online nature of update, i.e.,\noptimizing the memory only with respect to the last input; and (3) less\nexpressive management of their fixed-size memory. To enhance all these three\naspects, we present ATLAS, a long-term memory module with high capacity that\nlearns to memorize the context by optimizing the memory based on the current\nand past tokens, overcoming the online nature of long-term memory models.\nBuilding on this insight, we present a new family of Transformer-like\narchitectures, called DeepTransformers, that are strict generalizations of the\noriginal Transformer architecture. Our experimental results on language\nmodeling, common-sense reasoning, recall-intensive, and long-context\nunderstanding tasks show that ATLAS surpasses the performance of Transformers\nand recent linear recurrent models. ATLAS further improves the long context\nperformance of Titans, achieving +80\\% accuracy in 10M context length of\nBABILong benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cccd5134a5d74cbaa9446c",
      "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
      "fullname": "Ali Behrouz",
      "name": "AliBehrouz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23416",
      "authors": [
        {
          "_id": "68393d075711daf8cc919c04",
          "user": {
            "_id": "62e21907eda17fc126a15210",
            "avatarUrl": "/avatars/097879cf1eafa5e77bde419c7172fac6.svg",
            "isPro": false,
            "fullname": "Jang-Hyun Kim",
            "user": "Jang-Hyun",
            "type": "user"
          },
          "name": "Jang-Hyun Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:14.736Z",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c05",
          "user": {
            "_id": "60eb9074cc720726777d22a2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60eb9074cc720726777d22a2/aI-pHJRmnYOfC5fH7fFzD.jpeg",
            "isPro": false,
            "fullname": "Jinuk Kim",
            "user": "jusjinuk",
            "type": "user"
          },
          "name": "Jinuk Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:17.350Z",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c06",
          "name": "Sangwoo Kwon",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c07",
          "name": "Jae W. Lee",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c08",
          "name": "Sangdoo Yun",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c09",
          "name": "Hyun Oh Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T13:05:47.000Z",
      "submittedOnDailyAt": "2025-05-30T03:37:32.573Z",
      "title": "KVzip: 쿼리 무지한 KV 캐시 압축에서 컨텍스트 재구성을 활용합니다.",
      "submittedOnDailyBy": {
        "_id": "62e21907eda17fc126a15210",
        "avatarUrl": "/avatars/097879cf1eafa5e77bde419c7172fac6.svg",
        "isPro": false,
        "fullname": "Jang-Hyun Kim",
        "user": "Jang-Hyun",
        "type": "user"
      },
      "summary": "Transformer 기반의 대규모 언어 모델(LLMs)은 추론 시 컨텍스트를 키 값(KV) 페어로 캐싱합니다. 컨텍스트의 길이가 증가하면 KV 캐싱 크기가 확장되어 메모리 오버헤드와 주의 지연이 증가합니다. 본 논문에서는 KVzip라는 쿼리 무관한 KV 캐싱의 제거 방법과 다양한 쿼리로 효과적으로 압축된 KV 캐싱의 재활용을 가능하게 합니다. KVzip은 압축된 KV 페어에서 원래의 컨텍스트를 재구성하기 위해 기본적인 LLM을 사용하며, 중요도를 평가하여 중요도가 낮은 페어를 제외합니다. 실험적 평가에 따르면 KVzip은 KV 캐싱 크기를 3~4배 줄이고, FlashAttention의 결정 지연을 약 2배 줄입니다. 문제 해결, 검색, 이유, 코드 이해의 문제를 해결합니다. 평가에는 LLaMA3.1-8B, Qwen2.5-14B, Gemma3-12B 등 다양한 모델을 포함하며, 컨텍스트 길이는 170K 토큰에 이르렀습니다. KVzip은 현재의 쿼리에 의존하는 KV 제거 방법에 비해서 90%의 캐싱 버킷 비율에서도 성능 저하를 받지 않도록, 다 쿼리 시나리오에서 현저하게 우수합니다.",
      "upvotes": 8,
      "discussionId": "68393d075711daf8cc919c2b",
      "githubRepo": "https://github.com/snu-mllab/KVzip",
      "ai_summary": "KVzip, a query-agnostic KV cache eviction method for transformer-based LLMs, reduces KV cache size and decoding latency while maintaining performance across various tasks and models.",
      "ai_keywords": [
        "Transformer-based large language models (LLMs)",
        "KV cache",
        "context length",
        "query-agnostic",
        "KVzip",
        "query-aware",
        "KV eviction",
        "attention latency",
        "FlashAttention",
        "question-answering",
        "retrieval",
        "reasoning",
        "code comprehension",
        "LLaMA3.1-8B",
        "Qwen2.5-14B",
        "Gemma3-12B"
      ]
    },
    "publishedAt": "2025-05-29T09:05:47.000Z",
    "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
    "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4times and FlashAttention decoding latency by approximately\n2times, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23416.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e21907eda17fc126a15210",
      "avatarUrl": "/avatars/097879cf1eafa5e77bde419c7172fac6.svg",
      "fullname": "Jang-Hyun Kim",
      "name": "Jang-Hyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23559",
      "authors": [
        {
          "_id": "68390eebc260f5fa36eaa27c",
          "user": {
            "_id": "66554507e6ea63012f35824c",
            "avatarUrl": "/avatars/b82de75bd60890e7bb524fc3754b131c.svg",
            "isPro": false,
            "fullname": "Kunlun_Zhu",
            "user": "Leozkl",
            "type": "user"
          },
          "name": "Kunlun Zhu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T01:50:37.811Z",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa27d",
          "name": "Jiaxun Zhang",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa27e",
          "name": "Ziheng Qi",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa27f",
          "name": "Nuoxing Shang",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa280",
          "user": {
            "_id": "65d188a4aa309d842e438ef1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
            "isPro": false,
            "fullname": "Zijia Liu",
            "user": "m-serious",
            "type": "user"
          },
          "name": "Zijia Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:08.570Z",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa281",
          "user": {
            "_id": "638d601b5e14c2f38678fb3a",
            "avatarUrl": "/avatars/98d6151ec4261741eb4bd406685c07b5.svg",
            "isPro": false,
            "fullname": "韩沛煊",
            "user": "HakHan",
            "type": "user"
          },
          "name": "Peixuan Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:06.300Z",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa282",
          "name": "Yue Su",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa283",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa284",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T15:35:58.000Z",
      "submittedOnDailyAt": "2025-05-30T00:27:19.661Z",
      "title": "SafeScientist: 과학의 위험 인식을 위한 LLM 에이전트에 의한 발견",
      "submittedOnDailyBy": {
        "_id": "64c090a9f613170e7be93d2f",
        "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
        "isPro": false,
        "fullname": "KunlunZhu",
        "user": "KunlunZhu",
        "type": "user"
      },
      "summary": "최근의 대규모 언어 모델(LLM) 에이전트의 발전은 과학의 발견 자동화를 크게 가속화했지만, 동시에 중요한 윤리적 및 안전성 우려를 불러일으켰습니다. 이러한 도전을 체계적으로 해결하기 위해, 우리는 과학을 AI가 주도하는 탐색에서 안전성과 윤리적 책임을 높이기 위해 특별히 설계된 혁신적인 AI 과학자 프레임워크 \"SafeScientist\"를 소개합니다. SafeScientist는 윤리적으로 적절하지 않거나 고위험의 작업에 대해 적극적으로 거부하고, 연구 프로세스 전반에서 엄격한 안전성을 강조합니다. 전체적인 안전성 감시를 실현하기 위해, 우리는 프롬프트 모니터링, 에이전트 협업 모니터링, 도구 사용 모니터링, 윤리적 평가자 구성 요소를 포함하는 여러 방어 구조를 통합하고 있습니다. SafeScientist의 보완으로, 우리는 과학의 맥락에서 AI의 안전성을 평가하기 위한 새로운 벤치마크 \"SciSafetyBench\"를 제안합니다. 이는 6개 분야에서 240개의 고위험 과학 작업, 특히 설계된 30개의 과학 도구, 그리고 120개의 도구 관련 위험 작업으로 구성된 벤치마크입니다. 확장된 실험은 전통적인 AI 과학자 프레임워크와 비교하여 안전성 성능을 35% 개선시키고, 과학의 출력 품질을 훼손시키지 않고 달성했습니다. 또한, 우리는 다양한 공격 방법의 안전 파이프라인의 강건성을 엄격하게 검증하고, 우리의 통합적 접근 방식의 효과를 더욱 확인하고 있습니다. 코드와 데이터는 https://github.com/ulab-uiuc/SafeScientist에서 사용 가능합니다. 경고: 이 논문에는 잘못된 것 또는 손해를招く 가능성 있는 샘플 데이터가 포함됩니다.",
      "upvotes": 7,
      "discussionId": "68390eedc260f5fa36eaa319",
      "ai_summary": "SafeScientist is an AI framework that enhances safety in AI-driven scientific research through multiple defensive mechanisms and is validated using the SciSafetyBench benchmark.",
      "ai_keywords": [
        "SafeScientist",
        "AI scientist framework",
        "ethical responsibility",
        "prompt monitoring",
        "agent-collaboration monitoring",
        "tool-use monitoring",
        "ethical reviewer component",
        "SciSafetyBench",
        "benchmark",
        "scientific tasks",
        "scientific tools",
        "safety performance",
        "adversarial attack methods"
      ]
    },
    "publishedAt": "2025-05-29T11:35:58.000Z",
    "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
    "summary": "Recent advancements in large language model (LLM) agents have significantly\naccelerated scientific discovery automation, yet concurrently raised critical\nethical and safety concerns. To systematically address these challenges, we\nintroduce SafeScientist, an innovative AI scientist framework\nexplicitly designed to enhance safety and ethical responsibility in AI-driven\nscientific exploration. SafeScientist proactively refuses ethically\ninappropriate or high-risk tasks and rigorously emphasizes safety throughout\nthe research process. To achieve comprehensive safety oversight, we integrate\nmultiple defensive mechanisms, including prompt monitoring, agent-collaboration\nmonitoring, tool-use monitoring, and an ethical reviewer component.\nComplementing SafeScientist, we propose SciSafetyBench, a novel\nbenchmark specifically designed to evaluate AI safety in scientific contexts,\ncomprising 240 high-risk scientific tasks across 6 domains, alongside 30\nspecially designed scientific tools and 120 tool-related risk tasks. Extensive\nexperiments demonstrate that SafeScientist significantly improves safety\nperformance by 35\\% compared to traditional AI scientist frameworks, without\ncompromising scientific output quality. Additionally, we rigorously validate\nthe robustness of our safety pipeline against diverse adversarial attack\nmethods, further confirming the effectiveness of our integrated approach. The\ncode and data will be available at https://github.com/ulab-uiuc/SafeScientist.\nred{Warning: this paper contains example data that may be offensive\nor harmful.}",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23559.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c090a9f613170e7be93d2f",
      "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
      "fullname": "KunlunZhu",
      "name": "KunlunZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22961",
      "authors": [
        {
          "_id": "68390b2ea26b4142b0578d05",
          "user": {
            "_id": "638d601b5e14c2f38678fb3a",
            "avatarUrl": "/avatars/98d6151ec4261741eb4bd406685c07b5.svg",
            "isPro": false,
            "fullname": "韩沛煊",
            "user": "HakHan",
            "type": "user"
          },
          "name": "Peixuan Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:17.916Z",
          "hidden": false
        },
        {
          "_id": "68390b2ea26b4142b0578d06",
          "user": {
            "_id": "65d188a4aa309d842e438ef1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
            "isPro": false,
            "fullname": "Zijia Liu",
            "user": "m-serious",
            "type": "user"
          },
          "name": "Zijia Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:20.240Z",
          "hidden": false
        },
        {
          "_id": "68390b2ea26b4142b0578d07",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T01:03:41.000Z",
      "submittedOnDailyAt": "2025-05-30T00:34:06.330Z",
      "title": "ToMAP: トライディング オペネント에 친한 LLM 说服者들을, 마음의 이론에 기반한",
      "submittedOnDailyBy": {
        "_id": "65d188a4aa309d842e438ef1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
        "isPro": false,
        "fullname": "Zijia Liu",
        "user": "m-serious",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)은 설득에서 기대되는 잠재력을 보여주고 있지만, 현재의 LLM 설득자의 훈련에 대한 연구는 아직 초기 단계에 불과하다. 특히, 인간은 상대방의 생각을 주동적으로 동적으로 모델화하는 능력이 있지만, 현재의 LLMs는 Theory of Mind(ToM)의 논리론을 이해하기 어렵고, 상대방의 다양성과 인식이 제한되어 있다. 이러한 제한을 해결하기 위해, 우리는 Theory of Mind Augmented Persuader(ToMAP)를 도입하고, 두 개의 Theory of Mind 모듈을 포함하여 설득자의 상대방의 마음의 상태를 인식하고 분석하는 새로운 접근 방식을 제안하고 있다. 특히, 우리는 설득자를 상대에 대한 중앙주언에 대한 가능한 반대 의견으로 유도하고, 그 반대 의견에 대한 상대의 현재의 입장을 예측하기 위해 텍스트 인코더와 학습된 MLP 분류기를 결합한다. 설득자가 학습할 수 있는 조정된 재학습 스키마를 설계하고, 상대와 관련된 정보를 분석하여 더 효과적인 논점을 생성하는 방법을 배울 수 있다. 실험은 ToMAP 설득자는 3B 파라미터를 가진다면, GPT-4o나 다른 큰 기반 라인을 초월하여 다수의 설득 대상 모델과 다양한 코퍼스에서 39.4%의 상대적인 이익을 보여주는 것을 보여준다. 특히, ToMAP은 복잡한 이유 연결을 보여주고, 훈련 중의 재현을 줄이고, 더 다양한 효과적인 논점을 생성할 수 있다. ToMAP의 상대 인식 기능도 긴 대화에 적절하며, 더 이성적이고 상대 인식 전략을 취할 수 있는 것이다. 이러한 결과를 통해, 우리의 방법의 효과성을 강조하고, 설득력 있는 언어 어셈블리의 개발 가능성을 밝혀준다. 코드는 아래 URL에서 사용 가능합니다: https://github.com/ulab-uiuc/ToMAP.",
      "upvotes": 7,
      "discussionId": "68390b30a26b4142b0578d58",
      "githubRepo": "https://github.com/ulab-uiuc/ToMAP",
      "ai_summary": "ToMAP enhances LLM persuaders with Theory of Mind modules, improving opponent awareness and argument quality.",
      "ai_keywords": [
        "large language models",
        "theory of mind (ToM)",
        "reinforcement learning",
        "text encoder",
        "MLP classifier",
        "opponent awareness",
        "reasoning chains",
        "effective arguments",
        "logical strategies"
      ]
    },
    "publishedAt": "2025-05-28T21:03:41.000Z",
    "title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind",
    "summary": "Large language models (LLMs) have shown promising potential in persuasion,\nbut existing works on training LLM persuaders are still preliminary. Notably,\nwhile humans are skilled in modeling their opponent's thoughts and opinions\nproactively and dynamically, current LLMs struggle with such Theory of Mind\n(ToM) reasoning, resulting in limited diversity and opponent awareness. To\naddress this limitation, we introduce Theory of Mind Augmented Persuader\n(ToMAP), a novel approach for building more flexible persuader agents by\nincorporating two theory of mind modules that enhance the persuader's awareness\nand analysis of the opponent's mental state. Specifically, we begin by\nprompting the persuader to consider possible objections to the target central\nclaim, and then use a text encoder paired with a trained MLP classifier to\npredict the opponent's current stance on these counterclaims. Our carefully\ndesigned reinforcement learning schema enables the persuader learns how to\nanalyze opponent-related information and utilize it to generate more effective\narguments. Experiments show that the ToMAP persuader, while containing only 3B\nparameters, outperforms much larger baselines, like GPT-4o, with a relative\ngain of 39.4% across multiple persuadee models and diverse corpora. Notably,\nToMAP exhibits complex reasoning chains and reduced repetition during training,\nwhich leads to more diverse and effective arguments. The opponent-aware feature\nof ToMAP also makes it suitable for long conversations and enables it to employ\nmore logical and opponent-aware strategies. These results underscore our\nmethod's effectiveness and highlight its potential for developing more\npersuasive language agents. Code is available at:\nhttps://github.com/ulab-uiuc/ToMAP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22961.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d188a4aa309d842e438ef1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
      "fullname": "Zijia Liu",
      "name": "m-serious",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20755",
      "authors": [
        {
          "_id": "6836a4401314d4ac39a526f3",
          "user": {
            "_id": "6838043c11b8b14a21f6ecd8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kzuNcUqFpRbM4rlHEfbrx.png",
            "isPro": false,
            "fullname": "Yifei Wang",
            "user": "smallAI",
            "type": "user"
          },
          "name": "Yifei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:43:51.221Z",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f4",
          "name": "Weimin Bai",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f5",
          "name": "Colin Zhang",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f6",
          "name": "Debing Zhang",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f7",
          "name": "Weijian Luo",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f8",
          "name": "He Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6838043c11b8b14a21f6ecd8/y_4g5XJdwhTEjDjAMTQaS.png"
      ],
      "publishedAt": "2025-05-27T05:55:45.000Z",
      "submittedOnDailyAt": "2025-05-30T02:37:16.654Z",
      "title": "Uni-Instruct: 통일된 Dif-Fachion 모델\nDif-Fachion 지시\n\n(Note: The translation provided is a direct translation of the given text. In a professional context, it would be important to ensure that the term \"Dif-Fachion\" is appropriately understood and used within the specific field or domain. If \"Dif-Fachion\" is a specific technical term, it may need to be replaced with the correct term in Korean, depending on the context.)",
      "submittedOnDailyBy": {
        "_id": "6838043c11b8b14a21f6ecd8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kzuNcUqFpRbM4rlHEfbrx.png",
        "isPro": false,
        "fullname": "Yifei Wang",
        "user": "smallAI",
        "type": "user"
      },
      "summary": "이 논문에서는 Diff-Instruct, DMD, SIM, SiD, f-distill 등 10개 이상의 기존 한 단계 차이분별 디스탠레이션 접근법을 이론적인 프레임워크인 \"Uni-Instruct\"으로 통합합니다. Uni-Instruct는 우리가 제안한 f-divergence 가족의 확장 이론에 기반하여 제안되었습니다. 다음으로, 확장된 f-divergence의 계산 어려움 문제를 극복하기 위해 중요한 이론을 소개하고, 확장된 f-divergence 가족을 최소화하는 데 의해 한 단계 차이분별 모델을 효과적으로 훈련할 수 있는 등호 및 계산 가능한 손실을 유도했습니다. Uni-Instruct에 의한 새로운 통합은, 기존 접근법을 고수준에서 이해하기 위한 새로운 이론적 기여를 제공하며, 가장 先端한 한 단계 차이분별 생성 성능을 실현합니다. CIFAR10 생성 벤치마크에서, 무조건 생성에서 1.46, 조건 생성에서 1.38의 새로운 레코드의 Frechet Inception Distance (FID) 값을 달성합니다. ImageNet-64×64 생성 벤치마크에서, 79 단계의 교사 차이분별을 크게 개선하고, 1.02의 새로운 가장 先端한 한 단계 생성 FID를 달성합니다. 또한, Uni-Instruct는 3D 생성 및 텍스트로부터 3D 생성 등 광범위한 태스크에 적용할 수 있습니다. 3D 생성에서 이전 방법과 비교하여 생성 품질과 다양성까지 약간의 우위를 나타냅니다. Uni-Instruct의 견고한 이론적 및 실험적 기여는 향후 한 단계 차이분별 디스탠레이션 및 차이분별 모델의 지식 전파에 잠재적으로 도움이 될 것으로 기대합니다.",
      "upvotes": 6,
      "discussionId": "6836a4441314d4ac39a52803",
      "ai_summary": "Uni-Instruct unifies and enhances one-step diffusion distillation methods through a novel diffusion expansion theory, achieving state-of-the-art performance in unconditional and conditional image generation and text-to-3D generation.",
      "ai_keywords": [
        "Uni-Instruct",
        "diffusion expansion theory",
        "f-divergence",
        "one-step diffusion models",
        "Frechet Inception Distance (FID)",
        "CIFAR10",
        "ImageNet-64x64",
        "text-to-3D generation",
        "SDS",
        "VSD"
      ]
    },
    "publishedAt": "2025-05-27T01:55:45.000Z",
    "title": "Uni-Instruct: One-step Diffusion Model through Unified Diffusion\n  Divergence Instruction",
    "summary": "In this paper, we unify more than 10 existing one-step diffusion distillation\napproaches, such as Diff-Instruct, DMD, SIM, SiD, f-distill, etc, inside a\ntheory-driven framework which we name the \\emph{Uni-Instruct}.\nUni-Instruct is motivated by our proposed diffusion expansion theory of the\nf-divergence family. Then we introduce key theories that overcome the\nintractability issue of the original expanded f-divergence, resulting in an\nequivalent yet tractable loss that effectively trains one-step diffusion models\nby minimizing the expanded f-divergence family. The novel unification\nintroduced by Uni-Instruct not only offers new theoretical contributions that\nhelp understand existing approaches from a high-level perspective but also\nleads to state-of-the-art one-step diffusion generation performances. On the\nCIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet\nInception Distance (FID) values of \\emph{1.46} for unconditional\ngeneration and \\emph{1.38} for conditional generation. On the\nImageNet-64times 64 generation benchmark, Uni-Instruct achieves a new SoTA\none-step generation FID of \\emph{1.02}, which outperforms its 79-step\nteacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35).\nWe also apply Uni-Instruct on broader tasks like text-to-3D generation. For\ntext-to-3D generation, Uni-Instruct gives decent results, which slightly\noutperforms previous methods, such as SDS and VSD, in terms of both generation\nquality and diversity. Both the solid theoretical and empirical contributions\nof Uni-Instruct will potentially help future studies on one-step diffusion\ndistillation and knowledge transferring of diffusion models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6838043c11b8b14a21f6ecd8/y_4g5XJdwhTEjDjAMTQaS.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20755.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6838043c11b8b14a21f6ecd8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kzuNcUqFpRbM4rlHEfbrx.png",
      "fullname": "Yifei Wang",
      "name": "smallAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23742",
      "authors": [
        {
          "_id": "683917d0ecf59de6ef345e76",
          "name": "Yufan Deng",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e77",
          "name": "Xun Guo",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e78",
          "name": "Yuanyang Yin",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e79",
          "name": "Jacob Zhiyuan Fang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7a",
          "name": "Yiding Yang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7b",
          "name": "Yizhi Wang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7c",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:49.399Z",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7d",
          "name": "Angtian Wang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7e",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7f",
          "name": "Haibin Huang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e80",
          "name": "Chongyang Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:58:15.000Z",
      "submittedOnDailyAt": "2025-05-30T00:58:48.386Z",
      "title": "MAGREF: 마스크付기 가이드 프레임에서 임의의 참조를 이용한 비디오 생성",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "ビデオ生成는 딥러닝 모델의 발전과 특히 분화기반의 접근법의 등장으로 크게 발전하고 있습니다. 그러나 여러 주주주를 기반으로 한 ビデオ生成는 다주주주의 일관성과 높은 생성 품질을 보장하는 데 큰 문제를 발견하고 있습니다. 본 논문에서는 다양한 참조 이미지와 텍스트 힌트에 기반하여 다주주주의 ビデオ 합성을 가능하게 하도록 마스크가 된 가이드를 포함하는 유닛 프레임워크 MAGREF를 제안합니다. 특히, (1) 영역에 대한 동적인 마스크 구조를 제안하고, 모델이 인간, 물체, 배경 등 다양한 주제의 추론을 유연하게 대응할 수 있도록 구조적인 변경이 필요하지 않습니다. 또한, (2) 픽셀별로 채널 결합 구조를 제안하고, 채널 차원상 더 높은 필터 특성을 유지하는 데 효과적으로 수행합니다. 우리의 모델은 단일 주주주의 훈련에서 복잡한 다주주주의 시나리오로 확장하여 가장 先端的な ビデオ 생성 품질을 제공하고, 현재의 오픈 소스나 상업적 기본라인을 초월합니다. 평가의ために, 여러 주주주의 ビデオ 벤치마크를 소개합니다. 분산 실험은 우리의 접근법의 효과성을 보여주고, 스케일러블, 제어 가능한 고품질의 다주주주의 ビデオ 합성을 위한 길을 열어줍니다. 코드와 모델은 다음 URL에서 찾을 수 있습니다: https://github.com/MAGREF-Video/MAGREF",
      "upvotes": 5,
      "discussionId": "683917d2ecf59de6ef345efd",
      "projectPage": "https://magref-video.github.io/magref.github.io/",
      "githubRepo": "https://github.com/MAGREF-Video/MAGREF",
      "ai_summary": "MAGREF is a unified framework for video generation that uses masked guidance and dynamic masking for coherent multi-subject synthesis from diverse references and text prompts.",
      "ai_keywords": [
        "diffusion-based approaches",
        "multi-subject consistency",
        "unified framework",
        "masked guidance",
        "region-aware dynamic masking",
        "pixel-wise channel concatenation",
        "appearance features",
        "multi-subject video benchmark",
        "scalable",
        "controllable",
        "high-fidelity video synthesis"
      ]
    },
    "publishedAt": "2025-05-29T13:58:15.000Z",
    "title": "MAGREF: Masked Guidance for Any-Reference Video Generation",
    "summary": "Video generation has made substantial strides with the emergence of deep\ngenerative models, especially diffusion-based approaches. However, video\ngeneration based on multiple reference subjects still faces significant\nchallenges in maintaining multi-subject consistency and ensuring high\ngeneration quality. In this paper, we propose MAGREF, a unified framework for\nany-reference video generation that introduces masked guidance to enable\ncoherent multi-subject video synthesis conditioned on diverse reference images\nand a textual prompt. Specifically, we propose (1) a region-aware dynamic\nmasking mechanism that enables a single model to flexibly handle various\nsubject inference, including humans, objects, and backgrounds, without\narchitectural changes, and (2) a pixel-wise channel concatenation mechanism\nthat operates on the channel dimension to better preserve appearance features.\nOur model delivers state-of-the-art video generation quality, generalizing from\nsingle-subject training to complex multi-subject scenarios with coherent\nsynthesis and precise control over individual subjects, outperforming existing\nopen-source and commercial baselines. To facilitate evaluation, we also\nintroduce a comprehensive multi-subject video benchmark. Extensive experiments\ndemonstrate the effectiveness of our approach, paving the way for scalable,\ncontrollable, and high-fidelity multi-subject video synthesis. Code and model\ncan be found at: https://github.com/MAGREF-Video/MAGREF",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23742.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 55
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23754",
      "authors": [
        {
          "_id": "68391c6dc4405ad056a9ff79",
          "name": "Ziyin Zhang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7a",
          "name": "Jiahao Xu",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7b",
          "name": "Zhiwei He",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7c",
          "name": "Tian Liang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7d",
          "name": "Qiuzhi Liu",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7e",
          "name": "Yansi Li",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7f",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff80",
          "name": "Zhengwen Liang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff81",
          "name": "Zhuosheng Zhang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff82",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff83",
          "name": "Zhaopeng Tu",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff84",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff85",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:39.000Z",
      "submittedOnDailyAt": "2025-05-30T01:18:32.068Z",
      "title": "DeepTheorem: 자연어와 강화학습을 통해 증명된 定理의 LLM 추론의 발전\n\n(Note: \"定理\" is typically translated as \"theorem\" in Korean, but I have used \"정의\" to maintain the original meaning of \"theorem\" as a mathematical concept. If you prefer \"theorem,\" please let me know.)",
      "submittedOnDailyBy": {
        "_id": "660399710f1fc2f16de18072",
        "avatarUrl": "/avatars/c22a749cc45db693c2d9ea877c7cace4.svg",
        "isPro": false,
        "fullname": "Jiahao Xu",
        "user": "Jiahao004",
        "type": "user"
      },
      "summary": "정리 증명은 대규모 언어 모델(LLMs)의 복잡한 추론 능력을 평가하는 중요한 테스트 박스로役立つ。しかし, 전통적인 자동화된 정리 증명(ATP) 접근 방식은 LLMs가 예측 기간에 자연어로 얻을 수 있는 비정식적인 지식의 강점을有形의 증명 시스템과 밀접히 일치하지 않는다. 본 논문에서는, 자연어를 활용하여 LLMs의 수리 이론 능력 향상을 위해 엄밀한 비정식적인 정리 증명 프레임워크 \"DeepTheorem\"를 제안합니다. DeepTheorem는 121K 건의 고품질의 IMO 수준의 비정식적인 정리와 증명을 포함하는 대규모 벤치마크 데이터 세트를 구축하고, 정확성, 난이도, 토픽 카테고리에 엄밀하게 注釈되어 있으며, 증명 가능한 정리의 변体形식도 체계적으로 구축되어 있습니다. 새로운 강화 학습 전략(RL-Zero)을 제안하고, 증명 가능한 정리의 변体形식을 활용하여 강력한 수리 추론을 촉발시킵니다. 또한, 증명의 정확성과 추론 단계의 질을 평가하는 상세한 결과를 제시합니다. 확장된 실험 분석을 통해, DeepTheorem는 기존의 데이터 세트와 지도 학습의 프로토콜과 비교하여, 최신 수준의 정확성과 추론 질을 달성합니다. 우리의 발견은 DeepTheorem가 자동화된 비정식적인 정리 증명과 수리 탐색에 기초한 기초적인 발전을 도모한다는 것을 명확히 합니다.",
      "upvotes": 4,
      "discussionId": "68391c6ec4405ad056a9ffb2",
      "ai_summary": "DeepTheorem enhances LLM theorem-proving through a large-scale natural language dataset and a tailored reinforcement learning strategy, achieving state-of-the-art results in informal theorem proving.",
      "ai_keywords": [
        "DeepTheorem",
        "automated theorem proving",
        "ATP",
        "natural language",
        "informal theorem-proving",
        "large-scale benchmark dataset",
        "IMO-level theorems",
        "reinforcement learning",
        "RL-Zero",
        "verified theorem variants",
        "proof correctness",
        "reasoning quality"
      ]
    },
    "publishedAt": "2025-05-29T13:59:39.000Z",
    "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement Learning",
    "summary": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23754.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "660399710f1fc2f16de18072",
      "avatarUrl": "/avatars/c22a749cc45db693c2d9ea877c7cace4.svg",
      "fullname": "Jiahao Xu",
      "name": "Jiahao004",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23387",
      "authors": [
        {
          "_id": "68397697a14020a996c30112",
          "name": "Mingzhe Du",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30113",
          "name": "Luu Tuan Tuan",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30114",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30115",
          "name": "Yuhao Qing",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30116",
          "name": "Dong Huang",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30117",
          "name": "Xinyi He",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30118",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30119",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c3011a",
          "name": "See-kiong Ng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T12:14:29.000Z",
      "submittedOnDailyAt": "2025-05-30T07:47:46.606Z",
      "title": "Afterburner: 강화학습에 의한 자동 개선 코드의 효율화 접근법",
      "submittedOnDailyBy": {
        "_id": "61711f02e0b1ddb56eb9b526",
        "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
        "isPro": true,
        "fullname": "Mingzhe Du",
        "user": "Elfsong",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 기능적으로 올바른 해결책을 생성하지만, 코드의 효율성에 문제가 있고, 실제 세계적인 기계 학습의 구현에서 중요한 막대 역할을 합니다. 본 논문에서는 이 문제를 해결하기 위해, 실행 Sandbox에서 실증적인 실행 성능의 피드백을 기반으로 반복적 최적화 프레임워크를 도입합니다. LLMs는 이 프레임워크에서 실행 Sandbox에서 실증적인 실행 성능의 피드백을 기반으로 코드를 반복적으로 수정합니다. 우리는 3가지의 훈련 전략을 검토합니다: Supervised Fine-Tuning(SFT), Direct Preference Optimization(DPO), Group Relative Policy Optimization(GRPO). 우리의 Venus 데이터셋과 APPS 벤치마크에서 실험 결과를 통해, SFT와 DPO는 효율성의 향상에 빠르게 도달합니다. 반면, GRPO는 강화 학습(RL)과 실행 피드백을 사용하여 코드의 효율성을 지속적으로 최적화하고, pass@1(47%에서 62%)과 효율성에 대한 인간의 제안보다 우수한 결과를 얻는 확률(31%에서 45%)을 크게 향상시킵니다. 우리의 연구는 테스트 시의 코드의 효율성 향상을 효과적으로 실현하고, 특히 LLMs를 코드의 효율성에 대해 스스로 개선하는 가능성에 대해 비판적으로 밝혀줍니다.",
      "upvotes": 4,
      "discussionId": "68397698a14020a996c30176",
      "ai_summary": "A novel test-time iterative optimization framework using reinforcement learning continuously enhances code efficiency generated by large language models.",
      "ai_keywords": [
        "Large Language Models",
        "iterative optimization",
        "closed-loop system",
        "empirical performance feedback",
        "Supervised Fine-Tuning",
        "Direct Preference Optimization",
        "Group Relative Policy Optimization",
        "reinforcement learning",
        "pass@1",
        "Venus dataset",
        "APPS benchmark"
      ]
    },
    "publishedAt": "2025-05-29T08:14:29.000Z",
    "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code\n  Efficiency Optimization",
    "summary": "Large Language Models (LLMs) generate functionally correct solutions but\noften fall short in code efficiency, a critical bottleneck for real-world\ndeployment. In this paper, we introduce a novel test-time iterative\noptimization framework to address this, employing a closed-loop system where\nLLMs iteratively refine code based on empirical performance feedback from an\nexecution sandbox. We explore three training strategies: Supervised Fine-Tuning\n(SFT), Direct Preference Optimization (DPO), and Group Relative Policy\nOptimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark\nshow that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO,\nusing reinforcement learning (RL) with execution feedback, continuously\noptimizes code performance, significantly boosting both pass@1 (from 47% to\n62%) and the likelihood of outperforming human submissions in efficiency (from\n31% to 45%). Our work demonstrates effective test-time code efficiency\nimprovement and critically reveals the power of RL in teaching LLMs to truly\nself-improve code efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61711f02e0b1ddb56eb9b526",
      "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
      "fullname": "Mingzhe Du",
      "name": "Elfsong",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21114",
      "authors": [
        {
          "_id": "68366f4410fa22cd420ae295",
          "user": {
            "_id": "66615c855fd9d736e670e0a9",
            "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
            "isPro": false,
            "fullname": "wangshuai",
            "user": "wangsssssss",
            "type": "user"
          },
          "name": "Shuai Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T08:58:12.608Z",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae296",
          "name": "Zexian Li",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae297",
          "name": "Qipeng zhang",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae298",
          "user": {
            "_id": "649e7693a83143427691769c",
            "avatarUrl": "/avatars/d04f7b3d417423abaa053375212da21f.svg",
            "isPro": false,
            "fullname": "Tianhui Song",
            "user": "sthuihui",
            "type": "user"
          },
          "name": "Tianhui Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T08:58:10.255Z",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae299",
          "name": "Xubin Li",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae29a",
          "name": "Tiezheng Ge",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae29b",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae29c",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T12:33:43.000Z",
      "submittedOnDailyAt": "2025-05-30T05:34:57.709Z",
      "title": "미분가능해법탐색에 의한 고속 분산 샘플링",
      "submittedOnDailyBy": {
        "_id": "66615c855fd9d736e670e0a9",
        "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
        "isPro": false,
        "fullname": "wangshuai",
        "user": "wangsssssss",
        "type": "user"
      },
      "summary": "Diffusion 모델은 거대한 생성 품질을 보여주지만, 그 대신 많은 함수 평가가 필요합니다. 최근, 역 디퓨옹 解법을 효율적으로 처리하기 위해, 발전된 ODE 기반의 솔버가 개발되었습니다. 그러나 이러한 솔버는 Adams 같은 다 스텝 방법을 모델로, t에 대한 Lagrangeinterpolator를 단순히 의존합니다. 우리는 t에 대한 Lagrangeinterpolator는 Diffusion 모델에 적합하지 않으며, 시간 스텝과 솔버 계수들로 구성된 좁은 탐색 공간을 나타냅니다. 우리의 분석에 기반하여, 새로운 미분 가능한 솔버 탐색 알고리즘을 제안하고, 더 최적화된 솔버를 지정합니다. 탐색된 솔버를 적용한 예로, SiT-XL/2와 FlowDCN-XL/2의 rectified-flow 모델은 ImageNet256에서 각각 10 단계로 FID 스코어 2.40과 2.35를 달성합니다. 반면, DDPM 모델의 DiT-XL/2는 10 단계로 FID 스코어 2.33를 달성합니다. 특히, 우리가 탐색한 솔버는 전통적인 솔버보다 크게 우세를示して 있습니다. 또한, 우리가 탐색한 솔버는 다양한 모델 구조, 해상도, 모델 크기에도 일반성을 나타냅니다.",
      "upvotes": 4,
      "discussionId": "68366f4a10fa22cd420ae43f",
      "githubRepo": "https://github.com/MCG-NJU/NeuralSolver",
      "ai_summary": "Researchers propose a novel differentiable solver search algorithm that optimizes the computational efficiency and quality of diffusion models for image generation tasks.",
      "ai_keywords": [
        "diffusion models",
        "ODE-based solvers",
        "Adams-like multistep methods",
        "t-related Lagrange interpolation",
        "differentiable solver search algorithm",
        "rectified-flow models",
        "SiT-XL/2",
        "FlowDCN-XL/2",
        "DDPM",
        "DiT-XL/2",
        "FID scores",
        "ImageNet256",
        "computational efficiency",
        "generality"
      ]
    },
    "publishedAt": "2025-05-27T08:33:43.000Z",
    "title": "Differentiable Solver Search for Fast Diffusion Sampling",
    "summary": "Diffusion models have demonstrated remarkable generation quality but at the\ncost of numerous function evaluations. Recently, advanced ODE-based solvers\nhave been developed to mitigate the substantial computational demands of\nreverse-diffusion solving under limited sampling steps. However, these solvers,\nheavily inspired by Adams-like multistep methods, rely solely on t-related\nLagrange interpolation. We show that t-related Lagrange interpolation is\nsuboptimal for diffusion model and reveal a compact search space comprised of\ntime steps and solver coefficients. Building on our analysis, we propose a\nnovel differentiable solver search algorithm to identify more optimal solver.\nEquipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and\nFlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256\nwith only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of\n2.33 with only 10 steps. Notably, our searched solver outperforms traditional\nsolvers by a significant margin. Moreover, our searched solver demonstrates\ngenerality across various model architectures, resolutions, and model sizes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21114.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66615c855fd9d736e670e0a9",
      "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
      "fullname": "wangshuai",
      "name": "wangsssssss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17818",
      "authors": [
        {
          "_id": "6837b52b1233747046cfa5df",
          "name": "Daeun Kyung",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e0",
          "name": "Hyunseung Chung",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e1",
          "name": "Seongsu Bae",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e2",
          "name": "Jiho Kim",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e3",
          "name": "Jae Ho Sohn",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e4",
          "name": "Taerim Kim",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e5",
          "name": "Soo Kyung Kim",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e6",
          "name": "Edward Choi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645cd00f5ebf379fd6d7a4c1/LP4mF9N1AGEXPzwV9jmbE.png"
      ],
      "publishedAt": "2025-05-23T12:34:48.000Z",
      "submittedOnDailyAt": "2025-05-30T01:25:38.142Z",
      "title": "PatientSim: 전문적인 의사와 환자의 실제 상호작용을 실현하는 프로파일러 Drove 시뮬레이터입니다.",
      "submittedOnDailyBy": {
        "_id": "645cd00f5ebf379fd6d7a4c1",
        "avatarUrl": "/avatars/be8375ed32ac234919e26a2450bf9d38.svg",
        "isPro": false,
        "fullname": "Daeun Kyung",
        "user": "dek924",
        "type": "user"
      },
      "summary": "의료사 환자 대화는 여러 차례의 맥락에 관련된 대화이며, 다양한 환자 포지션에 맞는 대화입니다. 이러한 설정에서 의료사 LLM의 훈련이나 평가를 수행하려면 실제 환자의 상호작용 시스템이 필요합니다. 그러나 현재 존재하는 시뮬레이터는 임상실践에서 보이는 전체 범위의 포지션을 반영하지 않습니다. 이에 대해 우리는 PatientSim을 소개합니다. PatientSim은 임상 스케나에 기반한 현실적이고 다양한 환자 포지션을 생성하고, 의료 지식을 기반으로 동작합니다. PatientSim은 다음과 같은 두 가지 방법을 사용하여 동작합니다: 1) 임상 프로파일, 증상 및 병력을 포함하는 실제 세계적인 데이터로 얻을 수 있는 MIMIC-ED와 MIMIC-IV 데이터셋에서 얻은 것입니다. 2) 성격, 언어 능력, 병력의 기억 수준, 인지 혼란 수준의 4가지 축으로 정의된 포지션으로, 37가지의 고유한 조합을 결과로 합니다. 우리는 8개의 LLM의 실제 정확도와 포지션의 일치성을 평가했습니다. 가장 우수한 오픈 소스 모델인 Llama 3.3은 4명의 의사가 프레임워크의 강건성을 확인했습니다. PatientSim은 오픈 소스로 커스터마이징 가능한 플랫폼이며, 특정 훈련의 필요성에 따라 커스터마이징 가능합니다. 개인정보를 고려한 환경 제공하여 다양한 환자의 표현을 통해 의료 전용 다이얼로그 시스템의 평가를 위한 강력한 테스트 벤더로役立ち, 건강 케어의 교육 도구로도 기대할 만한 성과를 나타냅니다.",
      "upvotes": 4,
      "discussionId": "6837b52c1233747046cfa614",
      "ai_summary": "PatientSim generates diverse and realistic patient personas using clinical data to evaluate LLMs in medical dialogue settings.",
      "ai_keywords": [
        "clinical profiles",
        "personas",
        "personality",
        "language proficiency",
        "medical history recall level",
        "cognitive confusion level",
        "patient simulator",
        "factual accuracy",
        "persona consistency",
        "privacy-compliant"
      ]
    },
    "publishedAt": "2025-05-23T08:34:48.000Z",
    "title": "PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient\n  Interactions",
    "summary": "Doctor-patient consultations require multi-turn, context-aware communication\ntailored to diverse patient personas. Training or evaluating doctor LLMs in\nsuch settings requires realistic patient interaction systems. However, existing\nsimulators often fail to reflect the full range of personas seen in clinical\npractice. To address this, we introduce PatientSim, a patient simulator that\ngenerates realistic and diverse patient personas for clinical scenarios,\ngrounded in medical expertise. PatientSim operates using: 1) clinical profiles,\nincluding symptoms and medical history, derived from real-world data in the\nMIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:\npersonality, language proficiency, medical history recall level, and cognitive\nconfusion level, resulting in 37 unique combinations. We evaluated eight LLMs\nfor factual accuracy and persona consistency. The top-performing open-source\nmodel, Llama 3.3, was validated by four clinicians to confirm the robustness of\nour framework. As an open-source, customizable platform, PatientSim provides a\nreproducible and scalable solution that can be customized for specific training\nneeds. Offering a privacy-compliant environment, it serves as a robust testbed\nfor evaluating medical dialogue systems across diverse patient presentations\nand shows promise as an educational tool for healthcare.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645cd00f5ebf379fd6d7a4c1/LP4mF9N1AGEXPzwV9jmbE.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17818.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645cd00f5ebf379fd6d7a4c1",
      "avatarUrl": "/avatars/be8375ed32ac234919e26a2450bf9d38.svg",
      "fullname": "Daeun Kyung",
      "name": "dek924",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23745",
      "authors": [
        {
          "_id": "683910d50df60182c0dd5b62",
          "user": {
            "_id": "6649fb62a460da1da20f66d0",
            "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
            "isPro": false,
            "fullname": "Hao Dong",
            "user": "hdong51",
            "type": "user"
          },
          "name": "Hao Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:56.719Z",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b63",
          "name": "Moru Liu",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b64",
          "name": "Jian Liang",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b65",
          "name": "Eleni Chatzi",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b66",
          "name": "Olga Fink",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:01.000Z",
      "submittedOnDailyAt": "2025-05-30T00:32:10.298Z",
      "title": "신뢰할 수 있는지 여부, 시각 언어 모델의 예측을 신뢰할 수 있는지 여부",
      "submittedOnDailyBy": {
        "_id": "6649fb62a460da1da20f66d0",
        "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
        "isPro": false,
        "fullname": "Hao Dong",
        "user": "hdong51",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs)는 시각과 텍스트를 정확하게 대응하는 강력한 능력을 보여주며, 다양한 애플리케이션에서 광범위하게 활용될 수 있습니다. 그러나, 0-shot 및 Transfer Learning의 경우, VLMs는 잘못된 분류를 쉽게 할 수 있고, 자신감이 있지만 잘못 예측하는 경우가 있습니다. 이러한 한계는 잘못된 예측이 심각한 결과를招く 안전한 영역에서 큰 위험을 나타냅니다. 이 연구에서는 TrustVLM이라는 훈련없이 적용할 수 있는 프레임워크를 소개하고, VLM의 예측이 신뢰할 수 있는지 평가하는 중요한 도전에 대한 해결책을 제안합니다. 모델 간의 오류를 감지하고, 특정 개념이 이미지 임베딩 공간에서 명확히 표현되는 것을 기반으로, 이 공간을 활용하여 새로운 신뢰도 스코어 함수를 제안합니다. 17가지의 다양한 데이터셋, 4가지의 아키텍처와 2가지의 VLMs를 사용하여 엄밀한 평가를 수행하였으며, 현재 기준과 비교하여 AURC에서 51.87%, AUROC에서 9.14%, FPR95에서 32.42%의 개선을 나타내며, 최신 성능을 달성했습니다. 훈련없이 모델의 신뢰도를 향상시키므로, TrustVLM은 VLM의 실제 세계의 애플리케이션에서 안전한 처리에 대한 길을 열어줍니다. 코드는 https://github.com/EPFL-IMOS/TrustVLM에서 사용할 수 있습니다.",
      "upvotes": 3,
      "discussionId": "683910d70df60182c0dd5bd3",
      "githubRepo": "https://github.com/EPFL-IMOS/TrustVLM",
      "ai_summary": "TrustVLM enhances the reliability of Vision-Language Models by estimating prediction trustworthiness without retraining, improving misclassification detection in multimodal tasks.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "modality gap",
        "image embedding space",
        "confidence-scoring function",
        "AURC",
        "AUROC",
        "FPR95"
      ]
    },
    "publishedAt": "2025-05-29T13:59:01.000Z",
    "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
    "summary": "Vision-Language Models (VLMs) have demonstrated strong capabilities in\naligning visual and textual modalities, enabling a wide range of applications\nin multimodal understanding and generation. While they excel in zero-shot and\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\noften yielding confident yet incorrect predictions. This limitation poses a\nsignificant risk in safety-critical domains, where erroneous predictions can\nlead to severe consequences. In this work, we introduce TrustVLM, a\ntraining-free framework designed to address the critical challenge of\nestimating when VLM's predictions can be trusted. Motivated by the observed\nmodality gap in VLMs and the insight that certain concepts are more distinctly\nrepresented in the image embedding space, we propose a novel confidence-scoring\nfunction that leverages this space to improve misclassification detection. We\nrigorously evaluate our approach across 17 diverse datasets, employing 4\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\ncompared to existing baselines. By improving the reliability of the model\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\nVLMs in real-world applications. The code will be available at\nhttps://github.com/EPFL-IMOS/TrustVLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6649fb62a460da1da20f66d0",
      "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
      "fullname": "Hao Dong",
      "name": "hdong51",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23253",
      "authors": [
        {
          "_id": "6839325ddbf608133c740556",
          "name": "Yixun Liang",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c740557",
          "name": "Kunming Luo",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c740558",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c740559",
          "name": "Rui Chen",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c74055a",
          "name": "Hongyu Yan",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c74055b",
          "name": "Weiyu Li",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c74055c",
          "name": "Jiarui Liu",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c74055d",
          "name": "Ping Tan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647d8f65becb41a272907e7a/xUWTQ8gs16XOP8nrS-KNI.jpeg"
      ],
      "publishedAt": "2025-05-29T08:58:41.000Z",
      "submittedOnDailyAt": "2025-05-30T03:02:50.614Z",
      "title": "UniTEX: 3차원 모양의 일반적인 고품질 생성 테크스처\n\n(请注意，虽然翻译保持了原文的专业性和准确性，但“3차원 모양의 일반적인 고품질 생성 테크스처”在韩语中可能更准确地表达为“3차원 모양의 일반적인 고품질 테크스처 생성”以保持与“生成”一词的一致性。)",
      "submittedOnDailyBy": {
        "_id": "647d8f65becb41a272907e7a",
        "avatarUrl": "/avatars/f891a1986a9361b23b7c83c23031cf26.svg",
        "isPro": false,
        "fullname": "yixunliang",
        "user": "lyxun",
        "type": "user"
      },
      "summary": "유닛텍스(UniTEX)는 새로운 2단계 3차원 테크스처 생성 프레임워크입니다. 이 프레임워크는 고품질의 일관된 테크스처를 3차원 자산에 생성하는 데 사용됩니다. 현재의 접근 방식은 재 projekci온된 생성된 다점 이미지를 3차원 모양에 UV 기반의 UV 인풋팅으로 테크스처를 보정하지만, 이는 토폴로지적인 불확실성에 대한 문제를 일으킨다. 이를 해결하기 위해, UV 매핑의 제한을 피하기 위해 통일된 3차원 함수 공간에서 처리하는 방법을 제안합니다. 특히, 먼저, 테크스처를 3차원 공간에 리스팅을 수행하는 테크스처 함수(TF)를 제안합니다. 이 함수는 테크스처 값을 매핑하기 위해, 짧은 프로럭에 기반한 연속적인 체적적인 표현입니다. 그리고, 이러한 TF를 이미지와 기오메트리 입력으로부터 직접 예측하기 위해, Transformer 기반의 대형 테크스처 모델(LTM)을 사용합니다. 또한, 테크스처의 품질을 향상시키고 강력한 2차원 프로이드를 활용하기 위해, DiTs의 효율적인 적용을 위한 고급 LoRA 기반의 전략을 개발합니다. 이는 첫 번째 단계에서 고품질의 다점 이미지 테크스처 합성을 수행합니다. 실험은 UniTEX가 현재의 접근 방식에 비해 고급적인 시각적 품질과 테크스처의 일관성을 구현하고, 자동화된 3차원 테크스처 생성의 일반화 가능한 scalable 해결책을 제공함을 보여줍니다. 코드는 다음과 같은 URL에서 사용 가능합니다: https://github.com/YixunLiang/UniTEX.",
      "upvotes": 3,
      "discussionId": "6839325fdbf608133c7405dc",
      "githubRepo": "https://github.com/YixunLiang/UniTEX",
      "ai_summary": "UniTEX generates high-quality, consistent 3D textures by using Texture Functions and adapting Diffusion Transformers directly from images and geometry without UV mapping.",
      "ai_keywords": [
        "Texture Functions",
        "transformer-based Large Texturing Model",
        "LoRA",
        "Diffusion Transformers"
      ]
    },
    "publishedAt": "2025-05-29T04:58:41.000Z",
    "title": "UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes",
    "summary": "We present UniTEX, a novel two-stage 3D texture generation framework to\ncreate high-quality, consistent textures for 3D assets. Existing approaches\npredominantly rely on UV-based inpainting to refine textures after reprojecting\nthe generated multi-view images onto the 3D shapes, which introduces challenges\nrelated to topological ambiguity. To address this, we propose to bypass the\nlimitations of UV mapping by operating directly in a unified 3D functional\nspace. Specifically, we first propose that lifts texture generation into 3D\nspace via Texture Functions (TFs)--a continuous, volumetric representation that\nmaps any 3D point to a texture value based solely on surface proximity,\nindependent of mesh topology. Then, we propose to predict these TFs directly\nfrom images and geometry inputs using a transformer-based Large Texturing Model\n(LTM). To further enhance texture quality and leverage powerful 2D priors, we\ndevelop an advanced LoRA-based strategy for efficiently adapting large-scale\nDiffusion Transformers (DiTs) for high-quality multi-view texture synthesis as\nour first stage. Extensive experiments demonstrate that UniTEX achieves\nsuperior visual quality and texture integrity compared to existing approaches,\noffering a generalizable and scalable solution for automated 3D texture\ngeneration. Code will available in: https://github.com/YixunLiang/UniTEX.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647d8f65becb41a272907e7a/xUWTQ8gs16XOP8nrS-KNI.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d8f65becb41a272907e7a",
      "avatarUrl": "/avatars/f891a1986a9361b23b7c83c23031cf26.svg",
      "fullname": "yixunliang",
      "name": "lyxun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18087",
      "authors": [
        {
          "_id": "68351b91037632c04211688d",
          "user": {
            "_id": "6837b9a63ed37b18326c7fff",
            "avatarUrl": "/avatars/7aaf5e1ff783aad45946c937ac36bdd8.svg",
            "isPro": false,
            "fullname": "Hyungyung Lee",
            "user": "ttumyche",
            "type": "user"
          },
          "name": "Hyungyung Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:44:18.221Z",
          "hidden": false
        },
        {
          "_id": "68351b91037632c04211688e",
          "name": "Geon Choi",
          "hidden": false
        },
        {
          "_id": "68351b91037632c04211688f",
          "name": "Jung-Oh Lee",
          "hidden": false
        },
        {
          "_id": "68351b91037632c042116890",
          "name": "Hangyul Yoon",
          "hidden": false
        },
        {
          "_id": "68351b91037632c042116891",
          "name": "Hyuk Gi Hong",
          "hidden": false
        },
        {
          "_id": "68351b91037632c042116892",
          "name": "Edward Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T16:44:21.000Z",
      "submittedOnDailyAt": "2025-05-30T00:18:09.180Z",
      "title": "CXReasonBench: CXReasonBench는 켤레 X射線의 구조적 진단이론 평가를 위한 벤치마크입니다.",
      "submittedOnDailyBy": {
        "_id": "6837b9a63ed37b18326c7fff",
        "avatarUrl": "/avatars/7aaf5e1ff783aad45946c937ac36bdd8.svg",
        "isPro": false,
        "fullname": "Hyungyung Lee",
        "user": "ttumyche",
        "type": "user"
      },
      "summary": "최근의 대형 시각 언어 모델(LVLMs)의 발전은 의료 작업에서 보고서 생성, 시각화 질문응답 등 기대되는 응용 분야를 가능하게 해왔습니다. 그러나 현재의 벤치마크는 주로 최종적인 진단 답변에 초점을 맞추고, 모델이 임상적으로 의미있는 추론을 수행하는지에 대한 제한된 견해를 제공하고 있습니다. 이에 대해 우리는 공개적으로 사용 가능한 MIMIC-CXR-JPG 데이터셋에 기반한 구조화된 파이프라인 및 벤치마크, CheXStruct와 CXReasonBench를 소개합니다. CheXStruct는 직접적으로 체부 X선 사진에서 중간적인 추론 단계를 자동적으로 추출합니다. 예를 들어, 解剖학적 영역의 분할, 解剖학적 표지 추출, 진단 측정, 진단 지수 계산, 임상 임계값 적용 등 수행합니다. CXReasonBench는 이 파이프라인을 활용하여 모델이 임상적으로 정당한 추론 단계를 수행하는지, 그리고 구조화된 가이드라인으로부터 어느 정도 학습되었는지 평가하고, 진단 추론의 각기 다른 세부점을 명확하게 평가할 수 있습니다. 벤치마크는 12개의 진단 작업과 1,200개의 사례로 18,988개의 QA 페어로 구성되며, 解剖학적 영역 선택에 의한 시각적 기준화와 진단 측정을 포함하는 다 경로, 다 단계 평가 지원을 제공합니다. 10개의 평가된 최강 LVLM도 구조화된 추론과 일반화, 추상적인 지식과 解剖학적으로 기반된 시각적 해석의 연결을 실패합니다. 코드는 https://github.com/ttumyche/CXReasonBench에 공개되어 있습니다.",
      "upvotes": 3,
      "discussionId": "68351b97037632c0421169d0",
      "githubRepo": "https://github.com/ttumyche/CXReasonBench",
      "ai_summary": "CheXStruct and CXReasonBench evaluate Large Vision-Language Models in clinical diagnosis by assessing structured reasoning, visual grounding, and generalization using the MIMIC-CXR-JPG dataset.",
      "ai_keywords": [
        "Large Vision-Language Models",
        "LVLMs",
        "CheXStruct",
        "CXReasonBench",
        "MIMIC-CXR-JPG",
        "structured reasoning",
        "visual question answering",
        "medical tasks",
        "report generation",
        "anatomical regions",
        "diagnostic measurements",
        "diagnostic indices",
        "clinical thresholds",
        "visual grounding",
        "diagnostic reasoning",
        "multi-path",
        "multi-stage evaluation"
      ]
    },
    "publishedAt": "2025-05-23T12:44:21.000Z",
    "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic\n  Reasoning in Chest X-rays",
    "summary": "Recent progress in Large Vision-Language Models (LVLMs) has enabled promising\napplications in medical tasks, such as report generation and visual question\nanswering. However, existing benchmarks focus mainly on the final diagnostic\nanswer, offering limited insight into whether models engage in clinically\nmeaningful reasoning. To address this, we present CheXStruct and CXReasonBench,\na structured pipeline and benchmark built on the publicly available\nMIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of\nintermediate reasoning steps directly from chest X-rays, such as segmenting\nanatomical regions, deriving anatomical landmarks and diagnostic measurements,\ncomputing diagnostic indices, and applying clinical thresholds. CXReasonBench\nleverages this pipeline to evaluate whether models can perform clinically valid\nreasoning steps and to what extent they can learn from structured guidance,\nenabling fine-grained and transparent assessment of diagnostic reasoning. The\nbenchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,\neach paired with up to 4 visual inputs, and supports multi-path, multi-stage\nevaluation including visual grounding via anatomical region selection and\ndiagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with\nstructured reasoning and generalization, often failing to link abstract\nknowledge with anatomically grounded visual interpretation. The code is\navailable at https://github.com/ttumyche/CXReasonBench",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18087.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6837b9a63ed37b18326c7fff",
      "avatarUrl": "/avatars/7aaf5e1ff783aad45946c937ac36bdd8.svg",
      "fullname": "Hyungyung Lee",
      "name": "ttumyche",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14321",
      "authors": [
        {
          "_id": "68396605ac00da416d094bbf",
          "name": "Bo Feng",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc0",
          "name": "Zhengfeng Lai",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc1",
          "name": "Shiyu Li",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc2",
          "name": "Zizhen Wang",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc3",
          "name": "Simon Wang",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc4",
          "name": "Ping Huang",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc5",
          "name": "Meng Cao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T13:07:55.000Z",
      "submittedOnDailyAt": "2025-05-30T06:32:58.958Z",
      "title": "视频LLM基准测试를 분해하는： 지식, 공간 인식, 또는 실제 시간계열 이해?",
      "submittedOnDailyBy": {
        "_id": "66b5295f83425904fa7a1a6a",
        "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
        "isPro": false,
        "fullname": "Zhengfeng Lai",
        "user": "jefflai",
        "type": "user"
      },
      "summary": "현재의 영상 이해 벤치마크는 지식 기반과 단순 영상 기반의 질문을 혼동시키고, 모델의 시간적 논리 능력에 대한 명확한 차이를 구분하지 못하여, 영상 이해의 다른 모델과의 차이를 명확히 보여주는 부분이 거의 보이지 않는다. 우리는 벤치마크가 모델이 영상의 동적인 내용을 이해하고 있는지를 명확히 판단할 수 없는 두 가지 주요 제한을 식별했다: 1) 강한 언어 우선관념, 모델이 영상을 볼 필요가 없으며 질문을 답할 수 있는 것, 2) 셔플 불변성, 영상의 프레임이 시간 순서가 셔플되어도 특정 질문에 대해 모델의 성능이 동일하게 유지되는 것. 이러한 문제를 해결하기 위해, 우리는 VBenchComp라는 자동화 파이프라인을 제안하고, 질문을 LLM-Answerable, Semantic, Temporal 등 서로 다른 도메인에 분류했다. 특히, LLM-Answerable의 질문은 영상을 볼 필요가 없으며, Semantic의 질문은 프레임이 셔플되어도 답할 수 있는 것, Temporal의 질문은 프레임의 정확한 시간 순서가 이해되어야 하는 것. 나머지 질문은 Others라는 레이블로 지정된다. 이로써, 영상 LLM의 다양한 능력을 명확하게 평가할 수 있게 된다. 우리의 분석은 전통적인 전체적인 점수에 의해 숨겨져 있던 모델의 微妙한 약점을 밝혀내고, 향후 벤치마크 설계에 대한 조언과 추천을 제공하며, 영상 LLM의 평가를 더욱 정확하게 할 수 있는 지침을 제공한다.",
      "upvotes": 3,
      "discussionId": "68396607ac00da416d094c6c",
      "ai_summary": "VBenchComp, an automated pipeline, categorizes video LLM questions into different domains to evaluate temporal reasoning and isolate model weaknesses beyond overall scores.",
      "ai_keywords": [
        "video understanding",
        "LLM-Answerable",
        "Semantic",
        "Temporal"
      ]
    },
    "publishedAt": "2025-05-20T09:07:55.000Z",
    "title": "Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or\n  True Temporal Understanding?",
    "summary": "Existing video understanding benchmarks often conflate knowledge-based and\npurely image-based questions, rather than clearly isolating a model's temporal\nreasoning ability, which is the key aspect that distinguishes video\nunderstanding from other modalities. We identify two major limitations that\nobscure whether higher scores truly indicate stronger understanding of the\ndynamic content in videos: (1) strong language priors, where models can answer\nquestions without watching the video; and (2) shuffling invariance, where\nmodels maintain similar performance on certain questions even when video frames\nare temporally shuffled. To alleviate these issues, we propose VBenchComp, an\nautomated pipeline that categorizes questions into different domains:\nLLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions\ncan be answered without viewing the video; Semantic questions remain answerable\neven when the video frames are shuffled; and Temporal questions require\nunderstanding the correct temporal order of frames. The rest of the questions\nare labeled as Others. This can enable fine-grained evaluation of different\ncapabilities of a video LLM. Our analysis reveals nuanced model weaknesses that\nare hidden by traditional overall scores, and we offer insights and\nrecommendations for designing future benchmarks that more accurately assess\nvideo LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14321.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b5295f83425904fa7a1a6a",
      "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
      "fullname": "Zhengfeng Lai",
      "name": "jefflai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23761",
      "authors": [
        {
          "_id": "6839107ed762b7c617b0731b",
          "user": {
            "_id": "67578bf874cf42cdedbf00df",
            "avatarUrl": "/avatars/5981300ee0025c876973f94c81a85b19.svg",
            "isPro": false,
            "fullname": "Yunjae Won",
            "user": "yunjae-won",
            "type": "user"
          },
          "name": "Yunjae Won",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:04.071Z",
          "hidden": false
        },
        {
          "_id": "6839107ed762b7c617b0731c",
          "name": "Hyunji Lee",
          "hidden": false
        },
        {
          "_id": "6839107ed762b7c617b0731d",
          "name": "Hyeonbin Hwang",
          "hidden": false
        },
        {
          "_id": "6839107ed762b7c617b0731e",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:50.000Z",
      "submittedOnDailyAt": "2025-05-30T05:38:24.883Z",
      "title": "차분정보: 정보이론의 관점에서의 선호 최적화",
      "submittedOnDailyBy": {
        "_id": "67578bf874cf42cdedbf00df",
        "avatarUrl": "/avatars/5981300ee0025c876973f94c81a85b19.svg",
        "isPro": false,
        "fullname": "Yunjae Won",
        "user": "yunjae-won",
        "type": "user"
      },
      "summary": "직접 선호 최적화(DPO)는 관측 학습의 방법 중 언어 모델을 인간의 취향에 맞게 조정하는 표준적인 방법으로 자리잡았습니다. 실험적인 성공에도 불구하고, 로그 비율 보상 파라미터화의 이론적인 설명은 완벽하지 않습니다. 본 연구에서는 이러한 결함을 보완하기 위해, 분산 정보 분포(DID)를 사용합니다: 이는 정책 업데이트 시 얻는 정보를 감지하는 토큰 시퀀스의 분포입니다. 우선, 선호 라벨이 DID를 포함하여 기준 정책을 목표 정책으로 변환하기 위해 필요한 경우, DPO에서 로그 비율 보상은 선호 최적화에 의해 목표 정책의 학습을 위해 가장 적절한 형식으로 나타납니다. 이 결과는 거부된 답변에 대한 최적의 샘플링 분포의 폐쇄형 표현을 자연스럽게 제공합니다. 다음으로, 선호가 DID를 포함하는 조건은 단조 보상 함수에 대한 인덱스 편향과 기본적으로 관련되어 있습니다. 이 인덱스 편향은 선호 최적화에서 광범위하게 사용되어 왔지만, 이전에 인식되지 않았습니다. 마지막으로, DID의 엔트로피를 분석하여 낮은 엔트로피의 분산 정보가 정책 분포를 강화하고 높은 엔트로피의 분산 정보가 소음 효과를 일으키는 것을 설명하고 로그 유사률의 이동 현상을 해석합니다. 합성적인 실험을 통해 이론적인 발견을 검증하고 현실적인 지시 데이터 세트에 확장했습니다. 결과는 일반적인 지시에 대해 높은 엔트로피의 분산 정보의 학습이 중요하며, 지식 관련 문제 해결에 대해 낮은 엔트로피의 분산 정보의 학습이 유리하다는 것을 보여주었습니다. 전체적으로, 본 연구는 DPO의 목표, 선호 데이터의 구조, 그 결과로 나타나는 정책 행동을 일관된 관점으로 제공합니다.",
      "upvotes": 2,
      "discussionId": "6839107fd762b7c617b07385",
      "ai_summary": "Theoretical analysis of Direct Preference Optimization (DPO) reveals that log-ratio reward parameterization is optimal for learning target policy via preference optimization, linked to log-margin ordered policies, and explains policy reinforcement and smoothing based on differential information entropy.",
      "ai_keywords": [
        "Direct Preference Optimization",
        "DPO",
        "log-ratio reward parameterization",
        "Differential Information Distribution",
        "DID",
        "token sequences",
        "policy updates",
        "preference labels",
        "target policy",
        "preference optimization",
        "log-margin ordered policies",
        "entropy",
        "differential information entropy",
        "log-likelihood displacement",
        "instruction-following datasets",
        "knowledge-intensive question answering"
      ]
    },
    "publishedAt": "2025-05-29T13:59:50.000Z",
    "title": "Differential Information: An Information-Theoretic Perspective on\n  Preference Optimization",
    "summary": "Direct Preference Optimization (DPO) has become a standard technique for\naligning language models with human preferences in a supervised manner. Despite\nits empirical success, the theoretical justification behind its log-ratio\nreward parameterization remains incomplete. In this work, we address this gap\nby utilizing the Differential Information Distribution (DID): a distribution\nover token sequences that captures the information gained during policy\nupdates. First, we show that when preference labels encode the differential\ninformation required to transform a reference policy into a target policy, the\nlog-ratio reward in DPO emerges as the uniquely optimal form for learning the\ntarget policy via preference optimization. This result naturally yields a\nclosed-form expression for the optimal sampling distribution over rejected\nresponses. Second, we find that the condition for preferences to encode\ndifferential information is fundamentally linked to an implicit assumption\nregarding log-margin ordered policies-an inductive bias widely used in\npreference optimization yet previously unrecognized. Finally, by analyzing the\nentropy of the DID, we characterize how learning low-entropy differential\ninformation reinforces the policy distribution, while high-entropy differential\ninformation induces a smoothing effect, which explains the log-likelihood\ndisplacement phenomenon. We validate our theoretical findings in synthetic\nexperiments and extend them to real-world instruction-following datasets. Our\nresults suggest that learning high-entropy differential information is crucial\nfor general instruction-following, while learning low-entropy differential\ninformation benefits knowledge-intensive question answering. Overall, our work\npresents a unifying perspective on the DPO objective, the structure of\npreference data, and resulting policy behaviors through the lens of\ndifferential information.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23761.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67578bf874cf42cdedbf00df",
      "avatarUrl": "/avatars/5981300ee0025c876973f94c81a85b19.svg",
      "fullname": "Yunjae Won",
      "name": "yunjae-won",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23625",
      "authors": [
        {
          "_id": "683924d1ac00da416df936dd",
          "name": "Chao Huang",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936de",
          "name": "Yuesheng Ma",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936df",
          "name": "Junxuan Huang",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e0",
          "name": "Susan Liang",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e1",
          "name": "Yunlong Tang",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e2",
          "name": "Jing Bi",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e3",
          "name": "Wenqiang Liu",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e4",
          "name": "Nima Mesgarani",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e5",
          "name": "Chenliang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:31:45.000Z",
      "submittedOnDailyAt": "2025-05-30T01:54:58.337Z",
      "title": "ZeroSep: 음성 내의 어떤 것도 학습하지 않고 분리할 수 있습니다.",
      "submittedOnDailyBy": {
        "_id": "67257ee0938e718957c9c100",
        "avatarUrl": "/avatars/db1f792ee1a5d860ea4e98c11a016a1b.svg",
        "isPro": false,
        "fullname": "Chao Huang",
        "user": "ChaoHuangCS",
        "type": "user"
      },
      "summary": "음원 분리는 기계가 복잡한 음향 환경에 대해 이해하기 위해 기본적인 기술이며, 여러 음향 애플리케이션의 기초로 되어 있습니다. 현재의 서브젝트付き 심층 학습 접근 방식은 강력한 반면, 광범위한, 태스크专用의 표준화된 데이터의 필요성으로 제한되어 있으며, 현실적인 음향 공간의 큰 변동성과 개방 세트의 특성에 대한 일반화할 수 없는 문제가 있습니다. 생성 Fundamental Model의 성공을 참고하여, 이러한 제한을 극복할 수 있는지 조사하고 있습니다. 그 결과, 어떤 태스크에도 적용할 수 있는 텍스트 가이드된 음향 분기 모델로 0 shot의 음원 분리가 가능한 것을 놀라워서 발견했습니다. 이 방법은 ZeroSep로 이름 붙이고, 혼합 음향을 분기 모델의 잠재 공간으로 역전시키고, 텍스트 조건을 통해 디노이즈 프로세스를 가이드하여 개별 소스를 복원하는 방식으로 작동합니다. 태스크专用의 훈련이나 미세 조정이 필요하지 않고, ZeroSep는 생성 분기 모델을 분류적인 분리 태스크에 재활용하며, 풍부한 텍스트 플레이어를 통해 개방 세트의 시나리오를 내부적으로 지원합니다. ZeroSep는 여러 텍스트 가이드된 음향 분기 기반 모델과 호환하며, 여러 분리 벤치마크에서 강력한 분리 성능을 제공하며, 서브젝트付き 방식을 초월할 수 있습니다.",
      "upvotes": 2,
      "discussionId": "683924d6ac00da416df937f6",
      "ai_summary": "ZeroSep, a text-guided audio diffusion model, achieves zero-shot source separation through pre-trained models and text conditioning, outperforming supervised methods on various benchmarks.",
      "ai_keywords": [
        "audio source separation",
        "supervised deep learning",
        "generative foundation models",
        "text-guided audio diffusion models",
        "zero-shot source separation",
        "latent space",
        "denoising process",
        "discriminative separation task",
        "textual priors",
        "open-set scenarios"
      ]
    },
    "publishedAt": "2025-05-29T12:31:45.000Z",
    "title": "ZeroSep: Separate Anything in Audio with Zero Training",
    "summary": "Audio source separation is fundamental for machines to understand complex\nacoustic environments and underpins numerous audio applications. Current\nsupervised deep learning approaches, while powerful, are limited by the need\nfor extensive, task-specific labeled data and struggle to generalize to the\nimmense variability and open-set nature of real-world acoustic scenes. Inspired\nby the success of generative foundation models, we investigate whether\npre-trained text-guided audio diffusion models can overcome these limitations.\nWe make a surprising discovery: zero-shot source separation can be achieved\npurely through a pre-trained text-guided audio diffusion model under the right\nconfiguration. Our method, named ZeroSep, works by inverting the mixed audio\ninto the diffusion model's latent space and then using text conditioning to\nguide the denoising process to recover individual sources. Without any\ntask-specific training or fine-tuning, ZeroSep repurposes the generative\ndiffusion model for a discriminative separation task and inherently supports\nopen-set scenarios through its rich textual priors. ZeroSep is compatible with\na variety of pre-trained text-guided audio diffusion backbones and delivers\nstrong separation performance on multiple separation benchmarks, surpassing\neven supervised methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67257ee0938e718957c9c100",
      "avatarUrl": "/avatars/db1f792ee1a5d860ea4e98c11a016a1b.svg",
      "fullname": "Chao Huang",
      "name": "ChaoHuangCS",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22918",
      "authors": [
        {
          "_id": "68393b9ef0458c8bcb652d78",
          "user": {
            "_id": "6814e98abbe5b8a5d92fc335",
            "avatarUrl": "/avatars/83c1963280bc94fdc7a2b21f6e0f1c59.svg",
            "isPro": false,
            "fullname": "Ruichen Chen",
            "user": "crc5577",
            "type": "user"
          },
          "name": "Ruichen Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T05:01:23.240Z",
          "hidden": false
        },
        {
          "_id": "68393b9ef0458c8bcb652d79",
          "user": {
            "_id": "661c391720b47b0daddfcc5a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RwvDJRtlEEPch27xId9Cb.png",
            "isPro": false,
            "fullname": "Keith G. Mills",
            "user": "kgmills",
            "type": "user"
          },
          "name": "Keith G. Mills",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T06:54:23.854Z",
          "hidden": false
        },
        {
          "_id": "68393b9ef0458c8bcb652d7a",
          "user": {
            "_id": "64ac49ccb7d86b40fd60a8dd",
            "avatarUrl": "/avatars/e9f5482cffdd1d5917523a496a3805f0.svg",
            "isPro": false,
            "fullname": "Liyao Jiang",
            "user": "LiyaoJiang",
            "type": "user"
          },
          "name": "Liyao Jiang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T05:01:59.348Z",
          "hidden": false
        },
        {
          "_id": "68393b9ef0458c8bcb652d7b",
          "name": "Chao Gao",
          "hidden": false
        },
        {
          "_id": "68393b9ef0458c8bcb652d7c",
          "name": "Di Niu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T22:39:12.000Z",
      "submittedOnDailyAt": "2025-05-30T06:23:07.403Z",
      "title": "Re-ttention: Ultrasparsity Visible Memory Generation for Attention Statistical Reshaping",
      "submittedOnDailyBy": {
        "_id": "6814e98abbe5b8a5d92fc335",
        "avatarUrl": "/avatars/83c1963280bc94fdc7a2b21f6e0f1c59.svg",
        "isPro": false,
        "fullname": "Ruichen Chen",
        "user": "crc5577",
        "type": "user"
      },
      "summary": "Diffusion Transformers (DiT)는 고품질의 시각적 콘텐츠(예: 영화와 이미지)의 생성에서 사실상의 표준 모델로 자리잡고 있습니다. 큰 후퇴를 이루는 것은 복잡도가 해상도와 영화의 길이에 대해 제곱적으로 증가하는 어텐션 구조입니다. 이 부담을 줄이기 위한 한 가지 합리적인 방법은 스패르스한 어텐션이지만, 현재의 기술은 매우 높은 스패르스 레벨에서 품질을 유지할 수 없거나 계산 오버헤드가 예상되는 경우가 있습니다. 이러한 문제를 해결하기 위해, 우리는 Re-ttention을 제안합니다. Re-ttention은 Diffusion Models의 시간적冗長성을 활용하여 어텐션 구조 내의 확률적 정규화 쉘을 극복하고 매우 높은 스패르스한 어텐션을 실현합니다. 구체적으로는, Re-ttention은 전제된 소프트맥스 분포의 역사를 기반으로 모든 제곱 어텐션의 품질을 유지하기 위해 어텐션 스코어를 리사이징합니다. T2V/T2I 모델의 실험 결과에서, CogVideoX와 PixArt DiTs 등에서 Re-ttention은 추론 시 3.1%의 토큰을 필요로하며, FastDiTAttn, Sparse VideoGen, MInference 등 현대적인 방법들을 초과합니다. 또한, 우리는 H100 GPU에서부터의 라틴쉘의 감소를 측정하고, 무시할 수 있는 비용으로 45% 이상의 라틴쉘과 92% 이상의 자동 어텐션 라틴쉘의 감소를 구현할 수 있음을 보여줍니다. 코드는 아래 링크에서 공개되어 있습니다.",
      "upvotes": 2,
      "discussionId": "68393ba3f0458c8bcb652e60",
      "ai_summary": "Re-ttention uses temporal redundancy in diffusion models to enable high sparse attention in visual generation, maintaining quality with minimal computational overhead.",
      "ai_keywords": [
        "Diffusion Transformers",
        "sparse attention",
        "visual generation models",
        "probabilistic normalization shift",
        "Re-ttention",
        "attention scores",
        "softmax distribution",
        "latency reduction",
        "H100 GPU"
      ]
    },
    "publishedAt": "2025-05-28T18:39:12.000Z",
    "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical\n  Reshape",
    "summary": "Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. % To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. % Experimental\nresults on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate\nthat Re-ttention requires as few as 3.1\\% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference. Further, we measure latency to show that our method can attain over\n45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU\nat negligible overhead cost.\n  Code available online here:\nhttps://github.com/cccrrrccc/Re-ttention{https://github.com/cccrrrccc/Re-ttention}",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22918.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6814e98abbe5b8a5d92fc335",
      "avatarUrl": "/avatars/83c1963280bc94fdc7a2b21f6e0f1c59.svg",
      "fullname": "Ruichen Chen",
      "name": "crc5577",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22765",
      "authors": [
        {
          "_id": "68394fb7f1e62a44f53ecf82",
          "user": {
            "_id": "658436f5c73f74776b19198a",
            "avatarUrl": "/avatars/3f1d76af6fc0405d663c9294318fe83e.svg",
            "isPro": false,
            "fullname": "Iddo Yosha",
            "user": "iyosha",
            "type": "user"
          },
          "name": "Iddo Yosha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:52:57.738Z",
          "hidden": false
        },
        {
          "_id": "68394fb7f1e62a44f53ecf83",
          "user": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "isPro": false,
            "fullname": "Gallil Maimon",
            "user": "gallilmaimon",
            "type": "user"
          },
          "name": "Gallil Maimon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:52:41.076Z",
          "hidden": false
        },
        {
          "_id": "68394fb7f1e62a44f53ecf84",
          "name": "Yossi Adi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T18:32:56.000Z",
      "submittedOnDailyAt": "2025-05-30T05:20:21.736Z",
      "title": "스트레스테스트：당신의 언어 모델은 스트레스를 받아들일 수 있나요？",
      "submittedOnDailyBy": {
        "_id": "66b9bc2dacdbc1d0b39c3b50",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
        "isPro": false,
        "fullname": "Gallil Maimon",
        "user": "gallilmaimon",
        "type": "user"
      },
      "summary": "문맥중심은 대화어어절의 특정 단어에 중점을 두고, 아이디어의 강조 또는 대비하거나 새로운 정보를 소개하는 데 사용되는 강조입니다. 이는 주로潜在적인 목적을 나타내는 경우가 많습니다. 최근의 음성청취 언어 모델(SLMs)의 발전은 음성 신호의 모든 풍부성을 활용하여 음성처리 태스크에 대응할 수 있게 되었습니다. 문맥중심은 문맥의 의미 형성과 대화자의 의도에 대한 중요한 역할을 하지만, 이러한 모델의 평가와 개발에는 주로 무시되어 있습니다. 본 연구에서는 이러한 격차를 해결하기 위해, 문맥중심 패턴에 기반하여 대화어어절의 해석을 구분하는 모델의 능력을 평가하기 위해 설계된 벤치마크인 StressTest를 도입했습니다. SLMs의 성능을 평가하고, 이러한 태스크에서 종합적인 능력을 비교하여 낮은 성능을 나타내는 것을 발견했습니다. 이러한 제한을 극복하기 위해, 새로운 합성 데이터 생성 프로세스를 제안하고, 음절의 변화로 의미의 변화를 시뮬레이션하는 훈련세트인 Stress17k를 만들었습니다. 이 합성 데이터세트를 사용하여 모델을 최적화함으로써, 실세계의 녹음과 일치하며, SLMs의 효과적인 미세 조정을 가능하게 하였습니다. 결과는 우리의 미세 조정 모델인 StressSLM이 현재의 모델과 비교하여 문맥중심의 이유와 검출 태스크에서 유의미하게 뛰어난 것을 보여주었습니다. 코드, 모델, 데이터, 음성 샘플 - pages.cs.huji.ac.il/adiyoss-lab/stresstest.",
      "upvotes": 2,
      "discussionId": "68394fb8f1e62a44f53ecfa4",
      "projectPage": "https://pages.cs.huji.ac.il/adiyoss-lab/stresstest/",
      "githubRepo": "https://github.com/slp-rl/StressTest",
      "ai_summary": "A StressTest benchmark and synthetic Stress17k dataset are introduced to improve speech-aware language models' ability to interpret sentence stress in spoken language.",
      "ai_keywords": [
        "speech-aware language models",
        "spoken question answering",
        "sentence stress",
        "benchmark",
        "synthetic data generation pipeline",
        "audio reasoning",
        "sentence stress reasoning and detection tasks"
      ]
    },
    "publishedAt": "2025-05-28T14:32:56.000Z",
    "title": "StressTest: Can YOUR Speech LM Handle the Stress?",
    "summary": "Sentence stress refers to emphasis, placed on specific words within a spoken\nutterance to highlight or contrast an idea, or to introduce new information. It\nis often used to imply an underlying intention that is not explicitly stated.\nRecent advances in speech-aware language models (SLMs) have enabled direct\nprocessing of audio, allowing models to bypass transcription and access the\nfull richness of the speech signal and perform audio reasoning tasks such as\nspoken question answering. Despite the crucial role of sentence stress in\nshaping meaning and speaker intent, it remains largely overlooked in evaluation\nand development of such models. In this work, we address this gap by\nintroducing StressTest, a benchmark specifically designed to evaluate a model's\nability to distinguish between interpretations of spoken sentences based on the\nstress pattern. We assess the performance of several leading SLMs and find\nthat, despite their overall capabilities, they perform poorly on such tasks. To\novercome this limitation, we propose a novel synthetic data generation\npipeline, and create Stress17k, a training set that simulates change of meaning\nimplied by stress variation. Then, we empirically show that optimizing models\nwith this synthetic dataset aligns well with real-world recordings and enables\neffective finetuning of SLMs. Results suggest, that our finetuned model,\nStresSLM, significantly outperforms existing models on both sentence stress\nreasoning and detection tasks. Code, models, data, and audio samples -\npages.cs.huji.ac.il/adiyoss-lab/stresstest.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22765.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b9bc2dacdbc1d0b39c3b50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
      "fullname": "Gallil Maimon",
      "name": "gallilmaimon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20282",
      "authors": [
        {
          "_id": "683889f78c8e3b72170f6412",
          "user": {
            "_id": "641ddac5be3bd3a5a06ed4a4",
            "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
            "isPro": false,
            "fullname": "zitian gao",
            "user": "zgao3186",
            "type": "user"
          },
          "name": "Zitian Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:47.238Z",
          "hidden": false
        },
        {
          "_id": "683889f78c8e3b72170f6413",
          "name": "Lynx Chen",
          "hidden": false
        },
        {
          "_id": "683889f78c8e3b72170f6414",
          "name": "Joey Zhou",
          "hidden": false
        },
        {
          "_id": "683889f78c8e3b72170f6415",
          "name": "Bryan Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T17:58:30.000Z",
      "submittedOnDailyAt": "2025-05-30T01:51:16.066Z",
      "title": "1ピック 엔트로피 최소화",
      "submittedOnDailyBy": {
        "_id": "641ddac5be3bd3a5a06ed4a4",
        "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
        "isPro": false,
        "fullname": "zitian gao",
        "user": "zgao3186",
        "type": "user"
      },
      "summary": "라르지언어 모델 13,440개를 훈련시키고, 엔트로피 최소화는 단일의 무라벨 데이터와 10 단계의 최적화로, 규칙기반의 재보상 효과 학습을 통해 수십 개의 데이터와 엄밀하게 설계된 보상을 사용하여 성능 향상을 달성할 수 있었습니다. 이 놀라운 결과를 통해, 대규모 언어 모델의 훈련 후 패러다임에 대한 재고찰을 촉발하고 있습니다. 코드는 https://github.com/zitian-gao/one-shot-em에 공개되어 있습니다.",
      "upvotes": 2,
      "discussionId": "683889f78c8e3b72170f643d",
      "projectPage": "https://www.notion.so/One-shot-Entropy-Minimization-202606db813b80639773f850f39246a5",
      "githubRepo": "https://github.com/zitian-gao/one-shot-em",
      "ai_summary": "Entropy minimization with one sample and minimal optimization achieves significant performance improvements for large language models.",
      "ai_keywords": [
        "large language models",
        "entropy minimization",
        "unlabeled data",
        "optimization",
        "rule-based reinforcement learning",
        "post-training paradigms"
      ]
    },
    "publishedAt": "2025-05-26T13:58:30.000Z",
    "title": "One-shot Entropy Minimization",
    "summary": "We trained 13,440 large language models and found that entropy minimization\nrequires only a single unlabeled data and 10 steps optimization to achieve\nperformance improvements comparable to or even greater than those obtained\nusing thousands of data and carefully designed rewards in rule-based\nreinforcement learning. This striking result may prompt a rethinking of\npost-training paradigms for large language models. Our code is avaliable at\nhttps://github.com/zitian-gao/one-shot-em.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20282.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641ddac5be3bd3a5a06ed4a4",
      "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
      "fullname": "zitian gao",
      "name": "zgao3186",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19360",
      "authors": [
        {
          "_id": "68392279896eb9ceb71fac39",
          "name": "Manan Suri",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3a",
          "user": {
            "_id": "65c16444d4c3b8dff2f0d78d",
            "avatarUrl": "/avatars/4ed764c1657bd260d2a12ba61c111062.svg",
            "isPro": false,
            "fullname": "Puneet Mathur",
            "user": "puneetm",
            "type": "user"
          },
          "name": "Puneet Mathur",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T03:14:02.414Z",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3b",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3c",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:28.558Z",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3d",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3e",
          "name": "Dinesh Manocha",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T23:17:32.000Z",
      "submittedOnDailyAt": "2025-05-30T01:44:05.816Z",
      "title": "ChartLens: 차트의 미세한 시각화 어트리뷰션",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "다모뢰 모델 대언어 모델(MLLMs)의 기능 개선은 그래프 이해 등 다양한 태스크를 개선하고 있습니다. 그러나 이러한 모델은 생성된 문장을 제공받은 시각화 데이터와 불일치하는 '호라이즈샹션'(hallucinations)에 어려움을 겪고 있습니다. 이를 해결하기 위해 우리는 '후시점 시각화 Attribution'를 도입합니다. 이는 주어진 그래프 관련 응답을 정당화하기 위해 미세한 그래프 요소를 특정합니다. 또한 우리는 새로운 그래프 책임 알고리즘 'ChartLens'를 제안합니다. 이는 분할 기반의 방법 사용으로 그래프 오브젝트를 특정하고 MLLMs를 사용하여 미세한 시각화 책임 수행합니다. 또한 우리는 합성 차트와 실세계 차트로 구성된 벤치마크 'ChartVA-Eval'을 제시합니다. 이는 재정, 정책, 경제 등 다양한 분야에서 포함되는 차트를 포함하며, 미세한 책임 기록을 특징으로 합니다. 우리 평가에 따르면 ChartLens는 미세한 책임 기록을 26-66% 향상시킵니다.",
      "upvotes": 2,
      "discussionId": "6839227a896eb9ceb71fac99",
      "ai_summary": "ChartLens enhances multimodal language models with fine-grained visual attributions, improving the accuracy of chart understanding by 26-66%.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "chart understanding",
        "hallucinations",
        "Post-Hoc Visual Attribution",
        "ChartLens",
        "segmentation-based techniques",
        "set-of-marks prompting",
        "ChartVA-Eval",
        "synthetic charts",
        "real-world charts",
        "fine-grained attribution annotations"
      ]
    },
    "publishedAt": "2025-05-25T19:17:32.000Z",
    "title": "ChartLens: Fine-grained Visual Attribution in Charts",
    "summary": "The growing capabilities of multimodal large language models (MLLMs) have\nadvanced tasks like chart understanding. However, these models often suffer\nfrom hallucinations, where generated text sequences conflict with the provided\nvisual data. To address this, we introduce Post-Hoc Visual Attribution for\nCharts, which identifies fine-grained chart elements that validate a given\nchart-associated response. We propose ChartLens, a novel chart attribution\nalgorithm that uses segmentation-based techniques to identify chart objects and\nemploys set-of-marks prompting with MLLMs for fine-grained visual attribution.\nAdditionally, we present ChartVA-Eval, a benchmark with synthetic and\nreal-world charts from diverse domains like finance, policy, and economics,\nfeaturing fine-grained attribution annotations. Our evaluations show that\nChartLens improves fine-grained attributions by 26-66%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19360.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19286",
      "authors": [
        {
          "_id": "6839220cf85de1fc56402b3d",
          "name": "Utkarsh Sahu",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b3e",
          "name": "Zhisheng Qi",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b3f",
          "name": "Yongjia Lei",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b40",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b41",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:31.804Z",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b42",
          "name": "Nesreen K. Ahmed",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b43",
          "user": {
            "_id": "637c6d95a8716d642050b50f",
            "avatarUrl": "/avatars/0955a10113807348f24db968c7bd7c7a.svg",
            "isPro": false,
            "fullname": "Mahantesh Halappanavar",
            "user": "mhalappa",
            "type": "user"
          },
          "name": "Mahantesh M Halappanavar",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T05:25:34.615Z",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b44",
          "name": "Yao Ma",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b45",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T19:34:15.000Z",
      "submittedOnDailyAt": "2025-05-30T01:42:17.692Z",
      "title": "그래프의 시각적 관점에서의 구조적 패턴 검증",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "대 언어 모델은 지식 액세스, 편집 가능성, 추론, 설명성에서 광범위하게 연구되어 있습니다지만, 구조적인 패턴에 초점을 맞추지 않고 연구되어 있는 것은 적습니다. 이러한 빈백에 의해 동력가져, 우리는 그래프의 관점에서 이러한 구조적인 패턴을 조사합니다. 우리는 LLM의 지식을 트립렛 수준과 엔티티 수준으로 양수화하고, 노드의 차수 등 그래프 구조적인 특성과 관계를 분석합니다. 또한 지식 호모피리를 발견합니다. 토폴로지적으로 가까운 엔티티는 유사한 지식 레벨을 나타냅니다. 이를 통해, 우리는 엔티티의 지식를 로컬의 인접 노드를 기반으로 추정하는 그래프 기계 학습 모델을 개발합니다. 이 모델은 LLM에 의해 알려지지 않은 트립렛을 선택하여 유익한 지식 검증을 수행할 수 있습니다. 실험 결과를 통해, 선택된 트립렛을 이용한 微调은 높은 성능을 나타냅니다.",
      "upvotes": 2,
      "discussionId": "6839220ef85de1fc56402ba7",
      "ai_summary": "The study explores the structural patterns of knowledge in large language models from a graph perspective, uncovering knowledge homophily and developing models for graph machine learning to estimate entity knowledge.",
      "ai_keywords": [
        "large language models",
        "graph perspective",
        "triplet",
        "entity",
        "node degree",
        "knowledge homophily",
        "graph machine learning",
        "knowledge checking",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-05-25T15:34:15.000Z",
    "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large\n  Language Models",
    "summary": "Large language models have been extensively studied as neural knowledge bases\nfor their knowledge access, editability, reasoning, and explainability.\nHowever, few works focus on the structural patterns of their knowledge.\nMotivated by this gap, we investigate these structural patterns from a graph\nperspective. We quantify the knowledge of LLMs at both the triplet and entity\nlevels, and analyze how it relates to graph structural properties such as node\ndegree. Furthermore, we uncover the knowledge homophily, where topologically\nclose entities exhibit similar levels of knowledgeability, which further\nmotivates us to develop graph machine learning models to estimate entity\nknowledge based on its local neighbors. This model further enables valuable\nknowledge checking by selecting triplets less known to LLMs. Empirical results\nshow that using selected triplets for fine-tuning leads to superior\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19286.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23183",
      "authors": [
        {
          "_id": "683962e4cba8ce4f5ebced9c",
          "user": {
            "_id": "5e7749883d77a72421292d07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
            "isPro": false,
            "fullname": "Gabriele Sarti",
            "user": "gsarti",
            "type": "user"
          },
          "name": "Gabriele Sarti",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T07:54:16.127Z",
          "hidden": false
        },
        {
          "_id": "683962e4cba8ce4f5ebced9d",
          "name": "Vilém Zouhar",
          "hidden": false
        },
        {
          "_id": "683962e4cba8ce4f5ebced9e",
          "name": "Malvina Nissim",
          "hidden": false
        },
        {
          "_id": "683962e4cba8ce4f5ebced9f",
          "name": "Arianna Bisazza",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/1j5bJLYubeP6s5cGuTNzs.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/I2PXFiD8jN4odhmi5vrG6.png"
      ],
      "publishedAt": "2025-05-29T07:20:36.000Z",
      "submittedOnDailyAt": "2025-05-30T06:23:03.812Z",
      "title": "無サブバイアス의 단어 수준의 품질 평가 도구의 기계 번역을 통해 (Dis)agreement의 라벨러들에게의 시각",
      "submittedOnDailyBy": {
        "_id": "5e7749883d77a72421292d07",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
        "isPro": false,
        "fullname": "Gabriele Sarti",
        "user": "gsarti",
        "type": "user"
      },
      "summary": "단어 수준의 품질 평가 (WQE)는 기계 번역의 출력 중 미세한 오류 스팬을 자동으로 특정하는 것을 목표로 합니다. 이를 통해 번역사 후 편집 지원 등 다양한 활용 사례가 많습니다. 현대의 WQE 기술은 대규모 언어 모델의 실행 및 많은 인간 라벨 데이터의 실질적인 훈련에 의해 비용이 높게 됩니다. 본 논문에서는 언어 모델의 설명성과 불확실성 평가의 최근 진보를 활용하여 효율적인 대체 방법이 검토되며, 번역 모델의 내부 기능으로부터 번역 오류를 특정하는 것을 목표로 합니다. 12방향 번역에서 14개 시장에서 평가되었으며, 여러 인간 라벨 세트를 사용하여 메트릭의 성능에 미치는 인간 라벨의 변화의 영향을 정량화합니다. 이 결과는 무 매뉴얼 메트릭의 개발 가능성, 매뉴얼 메트릭의 라벨 불확실성 문제, 두 번째 노테이터 평가의 취약성을 밝혀냅니다.",
      "upvotes": 1,
      "discussionId": "683962e6cba8ce4f5ebcedfc",
      "githubRepo": "https://github.com/gsarti/labl",
      "ai_summary": "Evaluation of word-level quality estimation techniques leverages model interpretability and uncertainty quantification to identify translation errors with a focus on the impact of label variation and the performance of supervised versus unsupervised metrics.",
      "ai_keywords": [
        "language model interpretability",
        "uncertainty quantification",
        "word-level quality estimation",
        "translation errors",
        "human label variation",
        "unsupervised metrics",
        "supervised methods",
        "single-annotator evaluation practices"
      ]
    },
    "publishedAt": "2025-05-29T03:20:36.000Z",
    "title": "Unsupervised Word-level Quality Estimation for Machine Translation\n  Through the Lens of Annotators (Dis)agreement",
    "summary": "Word-level quality estimation (WQE) aims to automatically identify\nfine-grained error spans in machine-translated outputs and has found many uses,\nincluding assisting translators during post-editing. Modern WQE techniques are\noften expensive, involving prompting of large language models or ad-hoc\ntraining on large amounts of human-labeled data. In this work, we investigate\nefficient alternatives exploiting recent advances in language model\ninterpretability and uncertainty quantification to identify translation errors\nfrom the inner workings of translation models. In our evaluation spanning 14\nmetrics across 12 translation directions, we quantify the impact of human label\nvariation on metric performance by using multiple sets of human labels. Our\nresults highlight the untapped potential of unsupervised metrics, the\nshortcomings of supervised methods when faced with label uncertainty, and the\nbrittleness of single-annotator evaluation practices.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/1j5bJLYubeP6s5cGuTNzs.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/I2PXFiD8jN4odhmi5vrG6.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23183.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e7749883d77a72421292d07",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
      "fullname": "Gabriele Sarti",
      "name": "gsarti",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 225
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22943",
      "authors": [
        {
          "_id": "683912ee831cc04b0a3c0c6c",
          "user": {
            "_id": "64bb081c01f1983a863654dc",
            "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
            "isPro": false,
            "fullname": "Jaewoo Ahn",
            "user": "ahnpersie",
            "type": "user"
          },
          "name": "Jaewoo Ahn",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:54.580Z",
          "hidden": false
        },
        {
          "_id": "683912ee831cc04b0a3c0c6d",
          "name": "Heeseung Yun",
          "hidden": false
        },
        {
          "_id": "683912ee831cc04b0a3c0c6e",
          "name": "Dayoon Ko",
          "hidden": false
        },
        {
          "_id": "683912ee831cc04b0a3c0c6f",
          "name": "Gunhee Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T23:45:55.000Z",
      "submittedOnDailyAt": "2025-05-30T00:40:41.829Z",
      "title": "LLMs는 CLIP를 속일 수 있을까？ 텍스트 업데이트에 의한 사전 학습된 다모둠 표현의 적대적인 구조성을 평가하기 위한 벤치마크입니다.",
      "submittedOnDailyBy": {
        "_id": "64bb081c01f1983a863654dc",
        "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
        "isPro": false,
        "fullname": "Jaewoo Ahn",
        "user": "ahnpersie",
        "type": "user"
      },
      "summary": "クリップなど의 사전 학습된 다중 모달 표현은 놀라운 능력을 보여주고 있지만, 구조적 취약성을 나타내고, 직관적인 판단에 반하는 경우가 많다. 우리는 MAC(Multimodal Adversarial Crafting)라는 기준을 소개하고, 대규모 언어 모델(LLMs)을 사용하여 이러한 취약성을 활용할 수 있는 가짜 텍스트 샘플을 생성하고, 각 샘플의 공격 성공률과 그룹별로 기반한 다양성을 평가한다. 0-shot 메소드를 개선하기 위해, 우리는 가짜 샘플의 다양성을 촉진하는 필터링을 수행하는 거부 샘플 학습을 통해 자기 학습 접근 방식을 제안하고, 공격 성공률과 샘플의 다양성을 양방향으로 향상시킨다. Llama-3.1-8B 등 소규모 언어 모델을 사용하여, 우리의 접근 방식은 이미지, 비디오, 음성 등 다양한 다중 모달 표현의 구조적 취약성을 명확히 보여주는 우수한 성능을 보였다.",
      "upvotes": 1,
      "discussionId": "683912ef831cc04b0a3c0cc1",
      "githubRepo": "https://github.com/ahnjaewoo/MAC",
      "ai_summary": "A benchmark using deceptive text samples to evaluate compositional vulnerabilities in multimodal representations is introduced, and a self-training approach improves zero-shot methods by enhancing attack success and sample diversity.",
      "ai_keywords": [
        "Multimodal Adversarial Compositionality (MAC)",
        "multimodal representations",
        "large language models (LLMs)",
        "rejection-sampling fine-tuning",
        "diversity-promoting filtering",
        "images",
        "videos",
        "audios"
      ]
    },
    "publishedAt": "2025-05-28T19:45:55.000Z",
    "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of\n  Pre-trained Multimodal Representation via Text Updates",
    "summary": "While pre-trained multimodal representations (e.g., CLIP) have shown\nimpressive capabilities, they exhibit significant compositional vulnerabilities\nleading to counterintuitive judgments. We introduce Multimodal Adversarial\nCompositionality (MAC), a benchmark that leverages large language models (LLMs)\nto generate deceptive text samples to exploit these vulnerabilities across\ndifferent modalities and evaluates them through both sample-wise attack success\nrate and group-wise entropy-based diversity. To improve zero-shot methods, we\npropose a self-training approach that leverages rejection-sampling fine-tuning\nwith diversity-promoting filtering, which enhances both attack success rate and\nsample diversity. Using smaller language models like Llama-3.1-8B, our approach\ndemonstrates superior performance in revealing compositional vulnerabilities\nacross various multimodal representations, including images, videos, and\naudios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22943.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64bb081c01f1983a863654dc",
      "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
      "fullname": "Jaewoo Ahn",
      "name": "ahnpersie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22810",
      "authors": [
        {
          "_id": "68395c867f983113faf29005",
          "name": "Zhoufaran Yang",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf29006",
          "name": "Yan Shu",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf29007",
          "name": "Zhifei Yang",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf29008",
          "name": "Yan Zhang",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf29009",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900a",
          "name": "Keyang Lu",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900b",
          "name": "Gangyan Zeng",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900c",
          "name": "Shaohui Liu",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900d",
          "name": "Yu Zhou",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900e",
          "name": "Nicu Sebe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T19:39:35.000Z",
      "submittedOnDailyAt": "2025-05-30T05:52:32.917Z",
      "title": "VidText: 동영상 텍스트 이해의 완전한 평가로의 방법",
      "submittedOnDailyBy": {
        "_id": "65c4f99b27736b5b86c2cbda",
        "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
        "isPro": false,
        "fullname": "Yan Shu",
        "user": "sy1998",
        "type": "user"
      },
      "summary": "埋め込められた 시각적인 텍스트는 풍부한 семанти적 정보를 가지고 있으며, 비디오 전체의 이해와 특정한 인간 행동의 세부적인 추론에 중요합니다. 그러나 현재의 비디오 이해 벤치마크는 주로 텍스트 정보를 무시하고 있으며, OCR专門 벤치마크는 정적 이미지에 제한되어 있으며, 텍스트와 동적인 시각적 컨텍스트의 상호작용을捉える 능력이 제한되어 있습니다. 이를 채워주기 위해, VidText라는 새로운 벤치마크를 제안합니다. VidText는 다음 점들을 제시합니다: 1) 현실적인 시나리오의 광범위한 범위를 커버하고, 다언어 콘텐츠를 지원하며, 자연스럽게 등장하는 다양한 비디오 텍스트를 포함합니다. 2) 비디오 수준, 클립 수준, 인스턴스 수준의 단계적 평가 프레임워크를 도입하여, 글로벌 요약과 지역적인 검색 능력에 대한 평가가 가능합니다. 3) 벤치마크는 시각적인 텍스트 인식으로부터 문자와 시각적 컨텍스트의 크로스 모드 추론 범위를 확장한 인식 추론 태스크를 도입합니다. 18개의 가장 先端의 대형 다모달 모델(LMMs)에 대한 확장된 실험은 현재의 모델이 많은 태스크에서苦戦하고, 큰 개선의 여지가 있음을 명확히 합니다. 진한 분석은 입력 해상도와 OCR 능력의 모델의 고유한 원인, 그리고 보조 정보의 사용과 Chain-of-Thought 추론 전략의 외부 원인의 영향에 주력합니다. VidText는 현재의 비디오 이해 벤치마크의 공간을 채워, 동적인 환경에서 비디오 텍스트를 사용한 다모달 추론의 미래의 연구의 기초로役立つことを望みます。",
      "upvotes": 1,
      "discussionId": "68395c897f983113faf290eb",
      "ai_summary": "VidText is a new benchmark that evaluates video text understanding across various tasks, covering global summarization and local retrieval, and highlights challenges for current multimodal models.",
      "ai_keywords": [
        "LMMs",
        "video text perception",
        "cross-modal reasoning",
        "Chain-of-Thought reasoning"
      ]
    },
    "publishedAt": "2025-05-28T15:39:35.000Z",
    "title": "VidText: Towards Comprehensive Evaluation for Video Text Understanding",
    "summary": "Visual texts embedded in videos carry rich semantic information, which is\ncrucial for both holistic video understanding and fine-grained reasoning about\nlocal human actions. However, existing video understanding benchmarks largely\noverlook textual information, while OCR-specific benchmarks are constrained to\nstatic images, limiting their ability to capture the interaction between text\nand dynamic visual contexts. To address this gap, we propose VidText, a new\nbenchmark designed for comprehensive and in-depth evaluation of video text\nunderstanding. VidText offers the following key features: 1) It covers a wide\nrange of real-world scenarios and supports multilingual content, encompassing\ndiverse settings where video text naturally appears. 2) It introduces a\nhierarchical evaluation framework with video-level, clip-level, and\ninstance-level tasks, enabling assessment of both global summarization and\nlocal retrieval capabilities. 3) The benchmark also introduces a set of paired\nperception reasoning tasks, ranging from visual text perception to cross-modal\nreasoning between textual and visual information. Extensive experiments on 18\nstate-of-the-art Large Multimodal Models (LMMs) reveal that current models\nstruggle across most tasks, with significant room for improvement. Further\nanalysis highlights the impact of both model-intrinsic factors, such as input\nresolution and OCR capability, and external factors, including the use of\nauxiliary information and Chain-of-Thought reasoning strategies. We hope\nVidText will fill the current gap in video understanding benchmarks and serve\nas a foundation for future research on multimodal reasoning with video text in\ndynamic environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22810.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c4f99b27736b5b86c2cbda",
      "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
      "fullname": "Yan Shu",
      "name": "sy1998",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22126",
      "authors": [
        {
          "_id": "683944793e5dd928f04e8431",
          "name": "Yifan Chang",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8432",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8433",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8434",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8435",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8436",
          "name": "S. Kevin Zhou",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8437",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T08:51:01.000Z",
      "submittedOnDailyAt": "2025-05-30T04:18:26.192Z",
      "title": "SridBench: 과학 연구 그림의 그리기 방법 모델의 벤치마크\n\n(Note: The original text was in Japanese, not English. The translation provided above is from Japanese to Korean. If you meant to request a translation from English to Korean, please provide the English text.)",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "최근 AI가 주도하는 이미지 생성 분야에서 급격한 발전을 보이고 있습니다. 초기의 Difusion 모델은 시각적 품질을 우선시하며, 새로운 다모달 모델들은 고수준의 논리 통합과 의미 이해 및 구조적 구성 개선을 통해 발전하고 있습니다. 과학 설명도 그래프의 생성이 이러한 발전의 예로, 일반적인 이미지 합성과 달리 기술 내용을 정확하게 해석하고 추상적인 아이디어를 명확하고 표준화된 시각화로 변환하는 것이 필요합니다. 이 작업은 많은 경우 지식과 노동이 필요하며, 수시간을 소요하며, 특화된 도구가 필요합니다. 이러한 작업을 제어적이고 지능적인 방법으로 자동화하는 것은 실질적인 가치를 제공하지만, 현재 이 분야의 AI 평가에는 기준이 존재하지 않습니다. 이를 채워내기 위해, 우리는 SridBench를 소개합니다. 이것은 과학 설명도 그래프의 생성에 대한 첫 번째 기준이며, 13 분야의 자연과학과 계산과학의 先進的な 과학 논문에서 선택된 1,120 샘플로 구성되어 있습니다. 각 샘플은 의미적 충실도와 구조적 정확성 등 6차원으로 평가됩니다. 실험 결과로부터, GPT-4o-image 등 최상급 모델은 문/이미지의 명확성과 과학적 정확성에서 인간 성능을 초월하는 것을 명확히 보여주었습니다. 이러한 발견은 진화적인 논리 통합을 위한 시각화 생성 능력의 필요성을 강조하고 있습니다.",
      "upvotes": 1,
      "discussionId": "6839447c3e5dd928f04e84c1",
      "ai_summary": "The introduction of SridBench, a benchmark for scientific figure generation, reveals that current top-tier models, such as GPT-4o-image, fall short in semantic and structural accuracy compared to human performance, underscoring the need for more advanced multimodal reasoning-driven visual generation.",
      "ai_keywords": [
        "diffusion models",
        "multimodal models",
        "GPT-4o-image",
        "semantic understanding",
        "structural composition",
        "scientific illustration generation",
        "SridBench",
        "semantic fidelity",
        "structural accuracy"
      ]
    },
    "publishedAt": "2025-05-28T04:51:01.000Z",
    "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of\n  Image Generation Model",
    "summary": "Recent years have seen rapid advances in AI-driven image generation. Early\ndiffusion models emphasized perceptual quality, while newer multimodal models\nlike GPT-4o-image integrate high-level reasoning, improving semantic\nunderstanding and structural composition. Scientific illustration generation\nexemplifies this evolution: unlike general image synthesis, it demands accurate\ninterpretation of technical content and transformation of abstract ideas into\nclear, standardized visuals. This task is significantly more\nknowledge-intensive and laborious, often requiring hours of manual work and\nspecialized tools. Automating it in a controllable, intelligent manner would\nprovide substantial practical value. Yet, no benchmark currently exists to\nevaluate AI on this front. To fill this gap, we introduce SridBench, the first\nbenchmark for scientific figure generation. It comprises 1,120 instances\ncurated from leading scientific papers across 13 natural and computer science\ndisciplines, collected via human experts and MLLMs. Each sample is evaluated\nalong six dimensions, including semantic fidelity and structural accuracy.\nExperimental results reveal that even top-tier models like GPT-4o-image lag\nbehind human performance, with common issues in text/visual clarity and\nscientific correctness. These findings highlight the need for more advanced\nreasoning-driven visual generation capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20199",
      "authors": [
        {
          "_id": "683919611186f2cbf3ed2267",
          "name": "Pengxiang Li",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed2268",
          "name": "Shilin Yan",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed2269",
          "name": "Joey Tsai",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226a",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226b",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226c",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226d",
          "name": "Xiaowei Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T16:40:22.000Z",
      "submittedOnDailyAt": "2025-05-30T01:05:23.884Z",
      "title": "Adaptive Class-Free Guiding for Dynamic Low-Reliability Masks",
      "submittedOnDailyBy": {
        "_id": "64245f2c089d5fae56b4549a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
        "isPro": false,
        "fullname": "Pengxiang Li",
        "user": "pengxiang",
        "type": "user"
      },
      "summary": "클래시퍼프리지드닝(CFG)는 생성 모형의 제어성을 크게 향상시키기 위해 조건付き와 비조건付き 예측을 인터프리션(인터페이스)에 사용되고 있습니다. 그러나 표준의 CFG는 반복적인 생성 프로세스에서 모형의 불확실성이 동적으로 변하기 때문에静的 비조건付き 입력을 사용하지만, 이는 최적이 아닙니다. 우리는 모형의 순간적인 예측 신뢰성을 활용하여 비조건付き 입력을 마스터드로 만드는 새로운 방법인 적응적 클래시퍼프리지드닝(A-CFG)을 소개합니다. 반복적인(마스크付き) 디피르션 언어 모형의 각 단계에서, A-CFG는 현재 생성된 시퀀스의 모형이 낮은 신뢰성을 나타내는 토큰을 특정합니다. 이러한 토큰은 동적으로, 지역적으로 비조건付き 입력을 생성하기 위해 일시적으로 재마스크됩니다. 이로 인해 CFG의 수정 영향은 불확실한 영역에 정확하게 집중되어, 더 효과적인 가이드를 제공합니다. 우리는 최신의 마스크付き 디피르션 언어 모형에 A-CFG를 통합하고, 그 효과성을 보여줍니다. 다양한 언어 생성 벤치마크의 실험 결과에 따르면, A-CFG는 표준의 CFG보다 큰 개선을 거뒀으며, 예를 들어 GPQA에서 3.9점의 이득을 얻었습니다. 우리의 연구는 반복적인 생성에서 모형의 불확실성에 대한 가이드 메카니즘을 동적으로 조정하는 것이 기반인 것을 밝혀냅니다.",
      "upvotes": 1,
      "discussionId": "683919611186f2cbf3ed2292",
      "ai_summary": "Adaptive Classifier-Free Guidance (A-CFG) dynamically adjusts the guidance in masked diffusion language models by focusing on areas of low model confidence, leading to significant improvements in language generation performance.",
      "ai_keywords": [
        "Classifier-Free Guidance (CFG)",
        "Adaptive Classifier-Free Guidance (A-CFG)",
        "masked diffusion language model",
        "predictive confidence",
        "GPQA"
      ]
    },
    "publishedAt": "2025-05-26T12:40:22.000Z",
    "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
    "summary": "Classifier-Free Guidance (CFG) significantly enhances controllability in\ngenerative models by interpolating conditional and unconditional predictions.\nHowever, standard CFG often employs a static unconditional input, which can be\nsuboptimal for iterative generation processes where model uncertainty varies\ndynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel\nmethod that tailors the unconditional input by leveraging the model's\ninstantaneous predictive confidence. At each step of an iterative (masked)\ndiffusion language model, A-CFG identifies tokens in the currently generated\nsequence for which the model exhibits low confidence. These tokens are\ntemporarily re-masked to create a dynamic, localized unconditional input. This\nfocuses CFG's corrective influence precisely on areas of ambiguity, leading to\nmore effective guidance. We integrate A-CFG into a state-of-the-art masked\ndiffusion language model and demonstrate its efficacy. Experiments on diverse\nlanguage generation benchmarks show that A-CFG yields substantial improvements\nover standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work\nhighlights the benefit of dynamically adapting guidance mechanisms to model\nuncertainty in iterative generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20199.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64245f2c089d5fae56b4549a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
      "fullname": "Pengxiang Li",
      "name": "pengxiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19236",
      "authors": [
        {
          "_id": "68392fc4b6280677f75e6194",
          "user": {
            "_id": "5fbdf878485ef14d9a960f4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634724388998-5fbdf878485ef14d9a960f4d.jpeg",
            "isPro": false,
            "fullname": "Qian Cao",
            "user": "Aman",
            "type": "user"
          },
          "name": "Qian Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:24.101Z",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6195",
          "name": "Xiting Wang",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6196",
          "name": "Yuzhuo Yuan",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6197",
          "name": "Yahui Liu",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6198",
          "name": "Fang Luo",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6199",
          "name": "Ruihua Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T17:25:23.000Z",
      "submittedOnDailyAt": "2025-05-30T02:42:27.745Z",
      "title": "다양한 분야에서 텍스트 창의성 평가: 데이터셋과 대규모 언어 모델 평가자",
      "submittedOnDailyBy": {
        "_id": "5fbdf878485ef14d9a960f4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634724388998-5fbdf878485ef14d9a960f4d.jpeg",
        "isPro": false,
        "fullname": "Qian Cao",
        "user": "Aman",
        "type": "user"
      },
      "summary": "창의성 평가는 대규모 언어 모델(LLMs)에서 매우 어려운 상황입니다. 현재의 평가는 무효율적이고 고비싼 인간 판단을 중시하고 있으며, 기계의 창의성 향상에 연결되지 않습니다. 자동화 방법은 심리 테스트부터 휴리스틱이나 프로ンプ팅 기반의 접근 방식까지 있으며, 일반화 가능한데, 인간 판단에 맞지 않습니다. 이러한 문제를 해결하기 위해, 이 논문에서는 공통의 컨텍스트 프레임에서 평가의 일치성을 개선하기 위한 새로운 두 비교 프레임워크를 제안합니다. CreataSet라는 대규모 데이터 세트를 소개합니다. 이 데이터 세트는 100K+의 인간 수준과 1M+의 합성적인 창의적인 응답 페어를 포함하며, 다양한 개방 영역 태스크에 확장됩니다. CreataSet에서의 훈련을 통해 LLM 기반의 평가자 CrEval을 개발합니다. CrEval은 현재의 방법보다 인간 판단에 맞는 놀라운 우수한 성능을 보여주며, 인간 생성 및 합성 데이터 모두를 포함하여 높은 강도의 평가자의 훈련의 필수적 중요성을 강조하고, CrEval의 실용적인 효용을 보여주었습니다. 곧 공개할 것입니다. 연구를 지원하는 데 도움이 됩니다.",
      "upvotes": 1,
      "discussionId": "68392fc5b6280677f75e61e6",
      "projectPage": "https://creval-creative-evaluation.github.io/",
      "ai_summary": "A novel pairwise-comparison framework using CreataSet dataset trains CrEval, an LLM-based evaluator that significantly improves the assessment of textual creativity aligned with human judgments.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "automated methods",
        "heuristic-based approaches",
        "prompting-based approaches",
        "pairwise-comparison framework",
        "shared contextual instructions",
        "CreataSet",
        "synthetic creative instruction-response pairs",
        "open-domain tasks",
        "human-level instructions",
        "CrEval",
        "human judgments",
        "robust evaluators",
        "creativity enhancement"
      ]
    },
    "publishedAt": "2025-05-25T13:25:23.000Z",
    "title": "Evaluating Text Creativity across Diverse Domains: A Dataset and Large\n  Language Model Evaluator",
    "summary": "Creativity evaluation remains a challenging frontier for large language\nmodels (LLMs). Current evaluations heavily rely on inefficient and costly human\njudgments, hindering progress in enhancing machine creativity. While automated\nmethods exist, ranging from psychological testing to heuristic- or\nprompting-based approaches, they often lack generalizability or alignment with\nhuman judgment. To address these issues, in this paper, we propose a novel\npairwise-comparison framework for assessing textual creativity, leveraging\nshared contextual instructions to improve evaluation consistency. We introduce\nCreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic\ncreative instruction-response pairs spanning diverse open-domain tasks. Through\ntraining on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval\ndemonstrates remarkable superiority over existing methods in alignment with\nhuman judgments. Experimental results underscore the indispensable significance\nof integrating both human-generated and synthetic data in training highly\nrobust evaluators, and showcase the practical utility of CrEval in boosting the\ncreativity of LLMs. We will release all data, code, and models publicly soon to\nsupport further research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19236.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fbdf878485ef14d9a960f4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634724388998-5fbdf878485ef14d9a960f4d.jpeg",
      "fullname": "Qian Cao",
      "name": "Aman",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  }
]