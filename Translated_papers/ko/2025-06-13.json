[
  {
    "paper": {
      "id": "2506.09513",
      "authors": [
        {
          "_id": "684b8dbd3b733ba33368701b",
          "user": {
            "_id": "6723079ad1306fe9c76a1d29",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
            "isPro": false,
            "fullname": "Yu Sun",
            "user": "YuSun-AI",
            "type": "user"
          },
          "name": "Yu Sun",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:32:30.652Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701c",
          "name": "Xingyu Qian",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701d",
          "name": "Weiwen Xu",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701e",
          "user": {
            "_id": "64b7cd74ff6d81ae297feded",
            "avatarUrl": "/avatars/880fbc96cc093f5e901ce84f32a1d21d.svg",
            "isPro": false,
            "fullname": "ZHANG HAO",
            "user": "26hzhang",
            "type": "user"
          },
          "name": "Hao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:43.056Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701f",
          "name": "Chenghao Xiao",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687020",
          "name": "Long Li",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687021",
          "user": {
            "_id": "642eecbf9b2484d7d8526781",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642eecbf9b2484d7d8526781/4IvGbd66s49Wx5pZyZGHA.png",
            "isPro": false,
            "fullname": "Yu Rong",
            "user": "Swrooy",
            "type": "user"
          },
          "name": "Yu Rong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:40.908Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687022",
          "name": "Wenbing Huang",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687023",
          "name": "Qifeng Bai",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687024",
          "name": "Tingyang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T08:36:55.000Z",
      "submittedOnDailyAt": "2025-06-13T01:06:46.741Z",
      "title": "ReasonMed: 370K 多 에이전트 생성 데이터 세트 의료논리의 발전에 대한 연구\n\n(注意：由于原文中的“多エージェント生成データセット”在韩文中没有直接对应的表达，这里将其翻译为“多 에이전트 생성 데이터 세트”，以保持原意。如果需要更精确的翻译，可以进一步讨论。)",
      "submittedOnDailyBy": {
        "_id": "6723079ad1306fe9c76a1d29",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
        "isPro": false,
        "fullname": "Yu Sun",
        "user": "YuSun-AI",
        "type": "user"
      },
      "summary": "리론기반의 대규모 언어 모델(LLMs)은 수학과 프로그래밍에서 뛰어난 성능을 보여주지만, 지식밀집형의 의료 질문에 대한 답변에서는 조사가 부족한 능력이 있습니다. 이에 대처하여, 우리는 리론메드(ReasonMed)라는 가장 큰 의료 이유 데이터 세트를 소개합니다. 리론메드는 170만 개의 초기 이유 패스를 추출하여 고품질의 예 37만 개로 구성됩니다. 리론메드는 오류 검출과 수정을 수행하는 검증자가 플래그한 오류의 위험 있는 단계를 특정하고, 이를 수정하여 이유 패스를 강화하기 위한 에러 리Finn을 설계하고, 효과적인 조사와 개선 프로세스를 통해 구축되어 있습니다. 리론메드를 활용하여, 의료 이유 모델의 최적의 훈련 프로세스를 체계적으로 조사하고, 세부적인 Chain-of-Thought(CoT) 이유와 간결한 답변의 요약을 결합한 최적의 微調(Fine-tuning) 프로세스를 발견했습니다. 이 프로세스에 기반하여, 리론메드-7B를 훈련시키고, 10B 모델의 새로운 벤치마크를 설정하였으며, 지난 주의 최고의 모델보다 4.17% 더 우수하며, PubMedQA에서 LLaMA3.1-70B보다 4.60% 더 우수합니다.",
      "upvotes": 46,
      "discussionId": "684b8dbe3b733ba333687025",
      "githubRepo": "https://github.com/YuSun-Work/ReasonMed",
      "ai_summary": "ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.",
      "ai_keywords": [
        "reasoning-based large language models",
        "LLMs",
        "medical question answering",
        "ReasonMed",
        "multi-agent verification",
        "Error Refiner",
        "Chain-of-Thought",
        "CoT reasoning",
        "ReasonMed-7B",
        "PubMedQA"
      ]
    },
    "publishedAt": "2025-06-11T04:36:55.000Z",
    "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning",
    "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a multi-agent\nverification and refinement process, where we design an Error Refiner\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09513.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6723079ad1306fe9c76a1d29",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
      "fullname": "Yu Sun",
      "name": "YuSun-AI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10954",
      "authors": [
        {
          "_id": "684b7ea83b733ba333686f8a",
          "name": "Lianghong Guo",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8b",
          "name": "Yanlin Wang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8c",
          "name": "Caihua Li",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8d",
          "name": "Pengyu Yang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8e",
          "name": "Jiachi Chen",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8f",
          "user": {
            "_id": "6355473d525beaee688b7ba1",
            "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
            "isPro": false,
            "fullname": "Wei Tao",
            "user": "itaowe",
            "type": "user"
          },
          "name": "Wei Tao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:58.091Z",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f90",
          "name": "Yingtian Zou",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f91",
          "name": "Duyu Tang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f92",
          "name": "Zibin Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:54:17.000Z",
      "submittedOnDailyAt": "2025-06-13T00:07:20.052Z",
      "title": "SWE-Factory: 문제 해결 데이터와 평가 벤치마크의 자동화 팩토리",
      "submittedOnDailyBy": {
        "_id": "6355473d525beaee688b7ba1",
        "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
        "isPro": false,
        "fullname": "Wei Tao",
        "user": "itaowe",
        "type": "user"
      },
      "summary": "GitHub 문제 해결 태스크를 위한 대규모 데이터 세트의 구축은 대규모 언어 모델(LLMs)의 소프트웨어 개발 능력의 훈련과 평가에 중요합니다. 그러나 이러한 벤치마크의 전통적인 생성 절차는 평가 환경의 설정, 테스트 결과를 평가, 태스크 인스턴스의 검증 등 단계에서 특히 어려워서 많은 노력을 필요로 합니다. 본 논문에서는 이러한 도전을 해결하기 위해 SWE-Factory라는 자동화 파이프라인을 제안합니다. 이 파이프라인은 3가지 핵심적인 자동화 컴포넌트를 조합하여 이러한 문제를 해결하기 위해 설계되었습니다. 먼저, SWE-Builder라는 다 에이전트 시스템을 통해 평가 환경을 자동화하고 4가지의 특수화 에이전트가 협력하여 환경 메모리 포어를 활용하여 효율화를 실현합니다. 다음으로, 표준화된, 확장 코드 기반의 평가 방법을 도입하고 사용자 정의 파서의 수동 작성을 생략합니다. 마지막으로, 이러한 신뢰도가 높은 확장 코드 신호를 사용하여 fail2pass의 검증 프로세스를 자동화합니다. 4가지 프로그래밍 언어의 671문제에 대한 실험 결과를 통해 우리의 파이프라인은 유효한 태스크 인스턴스의 구축에 효과적이라는 것을 보여주었습니다. 예를 들어, GPT-4.1-mini를 사용했을 때 269개의 유효한 인스턴스를 0.045 인스턴스당으로 구축하였고, Gemini-2.5-flash를 사용했을 때 가장 낮은 비용으로 0.024 인스턴스당으로 유사한 성능을 달성했습니다. 또한, 확장 코드 기반의 평가는 수동 검증과 비교하여 100%의 정확도를 달성하였고, 자동화 파일 2 패스의 검증은 정확도 0.92와 재현율 1.00을 달성했습니다. 우리는 이 자동화 파이프라인이 크기와 품질 높은 GitHub 문제 해결 데이터 세트의 수집을 가속화하고 그 데이터 세트의 훈련과 평가에 도움을 줄 수 있는 것을 기대하고 있습니다. 우리의 코드와 데이터 세트는 https://github.com/DeepSoftwareAnalytics/swe-factory에 공개되어 있습니다.",
      "upvotes": 28,
      "discussionId": "684b7ea83b733ba333686f93",
      "githubRepo": "https://github.com/DeepSoftwareAnalytics/swe-factory",
      "ai_summary": "A pipeline named SWE-Factory automates the creation and validation of GitHub issue resolution datasets for training and evaluating Large Language Models, using SWE-Builder for environment setup, exit-code-based grading, and automated fail2pass validation.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "SWE-Factory",
        "SWE-Builder",
        "multi-agent system",
        "environment memory pool",
        "exit-code-based grading",
        "automated fail2pass validation",
        "GPT-4.1-mini",
        "Gemini-2.5-flash",
        "precision",
        "recall"
      ]
    },
    "publishedAt": "2025-06-12T13:54:17.000Z",
    "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks",
    "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of 0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10954.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6355473d525beaee688b7ba1",
      "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
      "fullname": "Wei Tao",
      "name": "itaowe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09993",
      "authors": [
        {
          "_id": "684ae204dbd21a9cc27b0fba",
          "user": {
            "_id": "66012e9c9e1cf5eb41ee0c4c",
            "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
            "isPro": false,
            "fullname": "Jaewon Min",
            "user": "Min-Jaewon",
            "type": "user"
          },
          "name": "Jaewon Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:25.024Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbb",
          "user": {
            "_id": "65ec3449a69aaabb431db0da",
            "avatarUrl": "/avatars/d7b507be0175a61a8fc21176eea45001.svg",
            "isPro": false,
            "fullname": "Jin Hyeon Kim",
            "user": "jinlovespho",
            "type": "user"
          },
          "name": "Jin Hyeon Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:22.776Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbc",
          "user": {
            "_id": "6752b6315281c3cae4b0783f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xmcyVEl2xBhk3G5_7dmpz.png",
            "isPro": false,
            "fullname": "Paul Hyunbin Cho",
            "user": "paulcho98",
            "type": "user"
          },
          "name": "Paul Hyunbin Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:20.327Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbd",
          "name": "Jaeeun Lee",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbe",
          "name": "Jihye Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbf",
          "name": "Minkyu Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc0",
          "name": "Sangpil Kim",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc1",
          "name": "Hyunhee Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc2",
          "name": "Seungryong Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:59:46.000Z",
      "submittedOnDailyAt": "2025-06-13T00:32:01.285Z",
      "title": "디퓨션 모형을 이용한 관심 있는 이미지의 리스트링",
      "submittedOnDailyBy": {
        "_id": "66012e9c9e1cf5eb41ee0c4c",
        "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
        "isPro": false,
        "fullname": "Jaewon Min",
        "user": "Min-Jaewon",
        "type": "user"
      },
      "summary": "그림 리파일화의 목적은 저해된 이미지를 복원하는 것입니다. 그러나 현재 존재하는 확산 기반의 리파일화 방법들은 자연스러운 이미지의 리파일화에大成功를 거뒀지만, 저해된 이미지의 텍스트 영역의 정확한 재구성이 어려워졌습니다. 이러한 방법들은 텍스트와 같은 패턴을 생성하고, 이 현상을 \"텍스트 이미지의 퀘스\"라고 부르며, 잘못된 것으로 간주합니다. 본 논문에서는 텍스트를 관심持有的 이미지 리파일화(TAIR)을 소개합니다. 이는 시각적 콘텐츠와 텍스트의 정확성을 동시에 복원하는 새로운 리파일화 작업입니다. 이 작업을 해결하기 위해, SA-Text를 제안합니다. 이는 100K의 고품질 스케ن 이미지의 규모적인 벤치마크에서, 다양한 및 복잡한 텍스트 인스턴스를 밀집적으로 어노테이팅한 것입니다. 또한, TeReDiff라는 다 태스크 확산 프레임워크를 제안합니다. 이는 확산 모델의 내부적 특성을 텍스트 스팸 모듈에 통합하고, 두 컴포넌트가 공통 학습에서 이점을 얻도록 합니다. 이로써, 풍부한 텍스트 표현을 추출할 수 있으며, 그들은 후속의 디노이즈 단계에서 Prompt로 사용됩니다. 확장된 실험은 우리의 접근이 가장 先端의 리파일화 방법과 일관적으로 초월하고, 텍스트 인식 정확도에서 상당한 효과를 보였습니다. 프로젝트 페이지를 참조: https://cvlab-kaist.github.io/TAIR/",
      "upvotes": 28,
      "discussionId": "684ae204dbd21a9cc27b0fc5",
      "projectPage": "https://cvlab-kaist.github.io/TAIR/",
      "githubRepo": "https://github.com/cvlab-kaist/TAIR",
      "ai_summary": "The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.",
      "ai_keywords": [
        "diffusion-based restoration",
        "text-image hallucination",
        "Text-Aware Image Restoration (TAIR)",
        "SA-Text",
        "multi-task diffusion framework",
        "TeReDiff",
        "text-spotting module",
        "text recognition accuracy"
      ]
    },
    "publishedAt": "2025-06-11T13:59:46.000Z",
    "title": "Text-Aware Image Restoration with Diffusion Models",
    "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09993.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66012e9c9e1cf5eb41ee0c4c",
      "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
      "fullname": "Jaewon Min",
      "name": "Min-Jaewon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10857",
      "authors": [
        {
          "_id": "684b817e3b733ba333686f95",
          "user": {
            "_id": "64b89a14cf14c2fabe96664c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
            "isPro": false,
            "fullname": "Jiashuo Yu",
            "user": "awojustin",
            "type": "user"
          },
          "name": "Jiashuo Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:55.618Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f96",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f97",
          "name": "Meng Chu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f98",
          "name": "Zhifei Ren",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f99",
          "name": "Zizheng Huang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9a",
          "user": {
            "_id": "64c9beb2904317f42de06dd8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c9beb2904317f42de06dd8/he3rxfyzfwEd1vLuK6_o2.jpeg",
            "isPro": false,
            "fullname": "Pei Chu",
            "user": "chupei",
            "type": "user"
          },
          "name": "Pei Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:51.884Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9b",
          "name": "Ruijie Zhang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9c",
          "name": "Yinan He",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9d",
          "name": "Qirui Li",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9e",
          "user": {
            "_id": "64acbbd51aee69ece03c6c0c",
            "avatarUrl": "/avatars/604df1cabc5faeda55022ae4c1997e56.svg",
            "isPro": false,
            "fullname": "Songze Li",
            "user": "LarryLee",
            "type": "user"
          },
          "name": "Songze Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:53.776Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9f",
          "name": "Zhenxiang Li",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa0",
          "name": "Zhongying Tu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa1",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa2",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa3",
          "name": "Yali Wang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa4",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa5",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T16:17:17.000Z",
      "submittedOnDailyAt": "2025-06-13T00:10:47.082Z",
      "title": "VRBench: 긴 비디오의 여러 단계의 이유에 적합한 벤치마크",
      "submittedOnDailyBy": {
        "_id": "64b89a14cf14c2fabe96664c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
        "isPro": false,
        "fullname": "Jiashuo Yu",
        "user": "awojustin",
        "type": "user"
      },
      "summary": "VRBench는 대규모 모델의 다단계 추론 능력을 평가하기 위해 만든 최초의 긴 뉴럴레이팅 비디오 벤치마크입니다. 현재 평가에서 시간적 추론과 절차의 정당성에 대한 단점을 해결합니다. 이는 1,010개의 긴 비디오를 포함하며, 9,468건의 인간 레이블付여 다단계 질문응답 쌍과 30,292건의 이유론의 시간 스탬프를 가지고 있습니다. 이러한 비디오들은 plot의 일관성을 우선시하기 위해 다단계 필터링 프로세스에 의해 칫솔이 졌습니다. 인간과 AI의 협력 프레임워크를 개발하고, 7가지 유형의 이유론(예: 이벤트의 설명, 은닉 에피스토리지)을 포함하는 시간적으로 기반된 여러 단계를 필요로 하는 독특한 이유론 코스를 생성합니다. VRBench는 모델을 평가하기 위해 결과와 과정 모두를 평가하기 위한 다단계 평가 프로세스를 설계하고 있습니다. 최종 결과의 MCQ뿐만 아니라 이유론 코스의 질을 구성적으로 평가하기 위해 여러 차원으로 진행 단계 LLM 가이드라인 점수 메트릭을 제안합니다. VRBench에서 12가지의 LLM과 16가지의 VLM을 검증하고, 세부적인 분석을 수행하여 다단계 추론 분야에 대한 유효한 팁을 제공합니다.",
      "upvotes": 23,
      "discussionId": "684b817e3b733ba333686fa6",
      "projectPage": "https://vrbench.github.io/",
      "githubRepo": "https://github.com/OpenGVLab/VRBench",
      "ai_summary": "VRBench is a long narrative video benchmark designed to evaluate models' multi-step reasoning and procedural validity through human-labeled question-answering pairs and a human-AI collaborative framework with a multi-phase evaluation pipeline.",
      "ai_keywords": [
        "VRBench",
        "multi-step reasoning",
        "temporal reasoning",
        "procedural validity",
        "long videos",
        "human-labeled",
        "multi-step question-answering",
        "expert inter-rater reviewing",
        "coherent reasoning chains",
        "event attribution",
        "implicit inference",
        "multi-phase evaluation",
        "progress-level LLM-guided scoring metric",
        "LLMs",
        "VLMs"
      ]
    },
    "publishedAt": "2025-06-12T12:17:17.000Z",
    "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
    "summary": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10857.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b89a14cf14c2fabe96664c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
      "fullname": "Jiashuo Yu",
      "name": "awojustin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10540",
      "authors": [
        {
          "_id": "684bad683b733ba3336870b6",
          "user": {
            "_id": "652fb8bcc9dd2692a25ef2e3",
            "avatarUrl": "/avatars/461e6cc1c3441cde18192b080b0b8576.svg",
            "isPro": false,
            "fullname": "Haoyuan Shi",
            "user": "MrSunshy",
            "type": "user"
          },
          "name": "Haoyuan Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:52.713Z",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b7",
          "user": {
            "_id": "62fdb01bc1588e1d4c6c1a7c",
            "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
            "isPro": false,
            "fullname": "Yunxin Li",
            "user": "YunxinLi",
            "type": "user"
          },
          "name": "Yunxin Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T04:47:39.539Z",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b8",
          "name": "Xinyu Chen",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b9",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870ba",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870bb",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T10:06:21.000Z",
      "submittedOnDailyAt": "2025-06-13T03:26:17.710Z",
      "title": "AniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성\n\nAniMaker: MCTS Droid의 자동화된 멀티 에이전트 애니메이션 스토리텔링 및 클립 생성",
      "submittedOnDailyBy": {
        "_id": "62fdb01bc1588e1d4c6c1a7c",
        "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
        "isPro": false,
        "fullname": "Yunxin Li",
        "user": "YunxinLi",
        "type": "user"
      },
      "summary": "TV 생성 모델의 급격한 발전에도 불구하고, 많은 스케인과 캐릭터를 조합하여 연속적인 이야기를 생성하는 것은 어려움입니다. 현재의 방법들은 미리 생성된 키프레임을 고정 길이의 클립으로 변환하여 연속적인 뉴레트트와 페이지링 문제를 발생시킵니다. 또한, TV 생성 모델의 내부 불확실성은 한 가지 저품질의 클립도 전체 출력 애니메이션의 논리적 일관성과 시각적 연속성을 크게 저하시켰을 수 있습니다. 이러한 장애물을 극복하기 위해, 우리는 AniMaker를 소개합니다. AniMaker는 효율적으로 다수의 클립을 생성하고 이야기에 관련된 클립 선택을 가능하게 하는 다 에이전트 프레임워크입니다. 이 프레임워크는 스토리리보드 생성을 위한 디렉터 에이전트, 클립 생성을 위한 사진그래피 에이전트, 평가를 위한 리뷰 에이전트, 편집과 목소리 오버를 위한 포스트 프로덕션 에이전트로 구성됩니다. AniMaker의 접근 방식의 핵심은 사진그래피 에이전트의 MCTS-Gen과 리뷰 에이전트의 AniEval의 두 기술적 구성 요소입니다. MCTS-Gen은 효율적인 Monte Carlo 트리 탐색(MCTS) 风의 전략으로, 유능하게 후보 공간을 탐색하며 리소스 사용 최적화를 통해 고 에너지의 클립을 생성합니다. AniEval은 첫 번째 프레임워크로, 연속적인 애니메이션 평가에 특화된 프레임워크이며, 스토리 수준의 일관성, 액션의 완료, 애니메이션의 고유 기능 등 중요한 측면에서, 이전과 다음 클립의 컨텍스트를 고려하여 평가합니다. 실험은 VBench와 제안한 AniEval 프레임워크를 사용하여 선호 지표를 보여주고, 다수의 클립 생성 효율을 크게 향상시키고, AI 생성의 이야기 애니메이션을 생산 표준에 근접시켰음을 보여줍니다.",
      "upvotes": 23,
      "discussionId": "684bad683b733ba3336870bc",
      "ai_summary": "AniMaker, a multi-agent framework using MCTS-Gen and AniEval, generates coherent storytelling videos from text input, outperforming existing models with better quality and efficiency.",
      "ai_keywords": [
        "multi-agent framework",
        "Director Agent",
        "Photography Agent",
        "Reviewer Agent",
        "Post-Production Agent",
        "Monte Carlo Tree Search (MCTS)",
        "AniEval",
        "VBench",
        "action completion",
        "story-level consistency",
        "animation-specific features"
      ]
    },
    "publishedAt": "2025-06-12T06:06:21.000Z",
    "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation",
    "summary": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity. To overcome these\nobstacles, we introduce AniMaker, a multi-agent framework enabling efficient\nmulti-candidate clip generation and storytelling-aware clip selection, thus\ncreating globally consistent and story-coherent animation solely from text\ninput. The framework is structured around specialized agents, including the\nDirector Agent for storyboard generation, the Photography Agent for video clip\ngeneration, the Reviewer Agent for evaluation, and the Post-Production Agent\nfor editing and voiceover. Central to AniMaker's approach are two key technical\ncomponents: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search\n(MCTS)-inspired strategy that intelligently navigates the candidate space to\ngenerate high-potential clips while optimizing resource usage; and AniEval in\nReviewer Agent, the first framework specifically designed for multi-shot\nanimation evaluation, which assesses critical aspects such as story-level\nconsistency, action completion, and animation-specific features by considering\neach clip in the context of its preceding and succeeding clips. Experiments\ndemonstrate that AniMaker achieves superior quality as measured by popular\nmetrics including VBench and our proposed AniEval framework, while\nsignificantly improving the efficiency of multi-candidate generation, pushing\nAI-generated storytelling animation closer to production standards.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10540.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62fdb01bc1588e1d4c6c1a7c",
      "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
      "fullname": "Yunxin Li",
      "name": "YunxinLi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10952",
      "authors": [
        {
          "_id": "684b96403b733ba33368703a",
          "user": {
            "_id": "65e808ed7c10574cc3f8e363",
            "avatarUrl": "/avatars/ed10759d354e271bfc15afd946b66b4a.svg",
            "isPro": false,
            "fullname": "zhangmozhi",
            "user": "mzzhang",
            "type": "user"
          },
          "name": "Mozhi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:33.791Z",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703b",
          "user": {
            "_id": "6718fc605e14ff6b94a7109f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
            "isPro": false,
            "fullname": "Howe Tissue",
            "user": "Howe77",
            "type": "user"
          },
          "name": "Howe Tissue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:31.965Z",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703c",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703d",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:53:51.000Z",
      "submittedOnDailyAt": "2025-06-13T01:43:47.223Z",
      "title": "도메인 2 벡터: 훈련 없이 최적의 데이터 혼합을 찾는 데이터 세트의 벡터화",
      "submittedOnDailyBy": {
        "_id": "6718fc605e14ff6b94a7109f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
        "isPro": false,
        "fullname": "Howe Tissue",
        "user": "Howe77",
        "type": "user"
      },
      "summary": "Domain2Vec는 새로운 개념인 메타 도메인(meta-domains)의 선형 결합으로 데이터 세트를 분해하는 새로운 접근입니다. 이 접근은 데이터 세트의 숨겨진 특성을 파악하기 위해 설계되었습니다. Domain2Vec는 메타 도메인의 배열을 유지하며, 주어진 데이터 세트를 이 배열에 대응하는 도메인 벡터로 분해하는 클래스 분류기를 사용합니다. 이러한 도메인 벡터는 훈련이 필요하지 않고, 가장 최적의 데이터 혼합을 식별하고 언어 모델(LM)의 사전 학습 성능을 향상시키기 위해 분포 일치 가정(DA^2)에 기반하여 효과적으로 사용될 수 있습니다. 이 가정은 훈련 세트와 검증 세트의 데이터 분포가 더 잘 일치할수록 검증 손실이 낮아질 것을 보여주고 있습니다. 또한, Domain2Vec는 이전 연구와 무차별적으로 도메인 벡터와 LM의 성능 관계를 모델링할 수 있으며, 이전 방법의 효율성과 scalability를 크게 향상시킵니다. 확장된 실험은 Domain2Vec가 최소한의 계산 오버헤드로 하류 태스크의 성능을 향상시키는 데이터 혼합을 찾는 것을 보여줍니다. 특히, Pile-CC 데이터 세트의 원의 혼합으로 훈련하는 데 필요한 계산량을 51.5%만 사용하여 같은 검증 손실을 달성하고, 동일한 계산 오버헤드에서는 하류 태스크의 성능이 평균 2.83% 향상됩니다.",
      "upvotes": 14,
      "discussionId": "684b96413b733ba33368703e",
      "ai_summary": "Domain2Vec decomposes datasets into meta-domains to optimize language model pretraining and downstream performance with reduced computational cost.",
      "ai_keywords": [
        "Domain2Vec",
        "meta-domains",
        "domain vector",
        "distribution alignment assumption",
        "DA²",
        "language model",
        "pretraining",
        "downstream task performance",
        "Pile-CC",
        "The Pile dataset"
      ]
    },
    "publishedAt": "2025-06-12T13:53:51.000Z",
    "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training",
    "summary": "We introduce~Domain2Vec, a novel approach that decomposes any\ndataset into a linear combination of several meta-domains, a new concept\ndesigned to capture the key underlying features of datasets.\nDomain2Vec maintains a vocabulary of meta-domains and uses a\nclassifier to decompose any given dataset into a domain vector that corresponds\nto a distribution over this vocabulary. These domain vectors enable the\nidentification of the optimal data mixture for language model (LM) pretraining\nin a training-free manner under the \\textbf{Distribution\nAlignment Assumption} (DA^{2}), which suggests that when\nthe data distributions of the training set and the validation set are better\naligned, a lower validation loss is achieved. Moreover, Domain2vec can\nbe seamlessly integrated into previous works to model the relationship between\ndomain vectors and LM performance, greatly enhancing the efficiency and\nscalability of previous methods. Extensive experiments demonstrate that\nDomain2Vec helps find the data mixture that enhances downstream task\nperformance with minimal computational overhead. Specifically,\nDomain2Vec achieves the same validation loss on Pile-CC using only\n51.5% of the computation required when training on the original mixture of\nThe Pile dataset. Under equivalent compute budget, Domain2Vec improves\ndownstream performance by an average of 2.83%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6718fc605e14ff6b94a7109f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
      "fullname": "Howe Tissue",
      "name": "Howe77",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10357",
      "authors": [
        {
          "_id": "684b86bf3b733ba333686fbe",
          "name": "Zaijing Li",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fbf",
          "name": "Yuquan Xie",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc0",
          "name": "Rui Shao",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc1",
          "name": "Gongwei Chen",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc2",
          "name": "Weili Guan",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc3",
          "name": "Dongmei Jiang",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc4",
          "name": "Liqiang Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T05:29:40.000Z",
      "submittedOnDailyAt": "2025-06-13T00:37:48.793Z",
      "title": "Optimus-3: 스케일러블な 태스크 익스퍼를 채용한 일반적인 다 타입 매뉴얼 마이크로소프트 어셈블리 에이전트에 대한向け",
      "submittedOnDailyBy": {
        "_id": "66b45fe75d0ac130d7d82764",
        "avatarUrl": "/avatars/09253f41f82c533b36199f82620cd075.svg",
        "isPro": false,
        "fullname": "Zaijing Li",
        "user": "dawn0815",
        "type": "user"
      },
      "summary": "최근, 다 모델 대 언어 모델(MLLM) 기반의 에이전트는 다양한 분야에서 놀라운 진보를 달성했습니다. 그러나, 플레이스테이트나 Minecraft와 같은 개방형 밭 환경에서 시각적 인식, 계획, 행동, 기초화, 반성 등 일반적인 기능들을 가진 일반 에이전트의 구축은 어려운 문제입니다: 특정 분야의 데이터가 부족하고, 서로 다른 태스크 사이에 간섭이 있으며, 개방형 밭 설정의 시각적 다양성이 있습니다. 본 논문에서는 이러한 문제를 해결하기 위해 3가지 주요의 기여를 제안합니다. 1) 데이터 생성 파이프라인을 제안하여, 에이전트의 개발에 확장 가능한 고품질의 데이터를 제공합니다. 2) 서로 다른 태스크 사이의 간섭을 완화하기 위해, 태스크 레벨의 루팅을 사용한 Mixture-of-Experts(MoE) 아키텍처를 도입합니다. 3) Minecraft에서의 시각적 다양성에 대한 에이전트의 추론 능력을 높이기 위해, Multimodal Reasoning-Augmented Reinforcement Learning 접근법을 개발합니다. 이러한 혁신적인 기능을 기반으로, Optimus-3, Minecraft용 일반적인 에이전트를 소개합니다. 확장된 실험 결과를 통해, Optimus-3은 Minecraft 환경의 다양한 태스크에서 일반적인 다 모델 대 언어 모델과 현재의 최선 에이전트를 초과하는 것을 보여주었습니다. 프로젝트 페이지: https://cybertronagent.github.io/Optimus-3.github.io/",
      "upvotes": 12,
      "discussionId": "684b86bf3b733ba333686fc5",
      "projectPage": "https://cybertronagent.github.io/Optimus-3.github.io/",
      "githubRepo": "https://github.com/JiuTian-VL/Optimus-3",
      "ai_summary": "Optimus-3, a multimodal large language model agent, uses knowledge-enhanced data generation, a Mixture-of-Experts architecture, and multimodal reasoning-augmented reinforcement learning to achieve superior performance across various tasks in Minecraft.",
      "ai_keywords": [
        "multimodal large language models",
        "knowledge-enhanced data generation",
        "Mixture-of-Experts",
        "task-level routing",
        "multimodal reasoning-augmented reinforcement learning"
      ]
    },
    "publishedAt": "2025-06-12T01:29:40.000Z",
    "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts",
    "summary": "Recently, agents based on multimodal large language models (MLLMs) have\nachieved remarkable progress across various domains. However, building a\ngeneralist agent with capabilities such as perception, planning, action,\ngrounding, and reflection in open-world environments like Minecraft remains\nchallenges: insufficient domain-specific data, interference among heterogeneous\ntasks, and visual diversity in open-world settings. In this paper, we address\nthese challenges through three key contributions. 1) We propose a\nknowledge-enhanced data generation pipeline to provide scalable and\nhigh-quality training data for agent development. 2) To mitigate interference\namong heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture\nwith task-level routing. 3) We develop a Multimodal Reasoning-Augmented\nReinforcement Learning approach to enhance the agent's reasoning ability for\nvisual diversity in Minecraft. Built upon these innovations, we present\nOptimus-3, a general-purpose agent for Minecraft. Extensive experimental\nresults demonstrate that Optimus-3 surpasses both generalist multimodal large\nlanguage models and existing state-of-the-art agents across a wide range of\ntasks in the Minecraft environment. Project page:\nhttps://cybertronagent.github.io/Optimus-3.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10357.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b45fe75d0ac130d7d82764",
      "avatarUrl": "/avatars/09253f41f82c533b36199f82620cd075.svg",
      "fullname": "Zaijing Li",
      "name": "dawn0815",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10974",
      "authors": [
        {
          "_id": "684b8e193b733ba333687028",
          "user": {
            "_id": "6241749cf80bd930bd99f3dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669210243382-6241749cf80bd930bd99f3dd.jpeg",
            "isPro": false,
            "fullname": "Ou Yixin",
            "user": "OE-Heart",
            "type": "user"
          },
          "name": "Yixin Ou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:36.148Z",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba333687029",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702a",
          "name": "Jingsheng Zheng",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702b",
          "name": "Lanning Wei",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702c",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702d",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702e",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702f",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba333687030",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:37.984Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:59:32.000Z",
      "submittedOnDailyAt": "2025-06-13T03:44:21.173Z",
      "title": "AutoMind: 자동 데이터 과학을 위한 적응적인 지식형 에이전트",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "대 언어 모delo (LLM) 에이전트는 현실적인 데이터 사이언스 문제를 해결하기 위해 큰 잠재력을 보여주고 있습니다. LLM 주도 데이터 사이언스 에이전트는 모든 기계 학습 파이프라인을 자동화할 것을 약속하지만, 현실적인 효과성은 제한되어 있습니다. 현재의 프레임워크는 엄격한, 사전 정의된 작업 흐름과 불변한 코딩 전략에 의존하고 있으며, 상대적으로 간단한 고전적인 문제에서 뛰어난 성능을 보여주는 데만, 복잡한 혁신적인 태스크에 대한 인간 실증적 지식을 파악할 수 없습니다. 본 연구에서는, 3가지의 핵심 진전을 통해 이러한 단점을 극복한 자동화된 지식을 가지는 LLM 에이전트 프레임워크 \"AutoMind\"를 소개합니다. 이 진전은 (1) 전문가의 지식 기반, (2) 에이전트 지식을 가지는 트리 검색 알고리즘, (3) 자동 조정 코딩 전략입니다. 2가지의 자동 데이터 사이언스 벤치마크에서 평가에 의해, AutoMind는 가장 선진한 기본 라인과 비교하여 상위 성능을 보여주고 있습니다. 추가적인 분석은 유효성, 효율성, 질의적 해결책의 품질의 우수성을 확인하고, AutoMind는 완전한 자동화된 데이터 사이언스를 위한 적절하고 강력한 단계로서의 역할을 수행하고 있음을 명확히 합니다.",
      "upvotes": 10,
      "discussionId": "684b8e193b733ba333687031",
      "ai_summary": "AutoMind, a flexible and knowledgeable LLM-agent framework, improves automated data science through expert knowledge integration, strategic solution exploration, and adaptive coding, outperforming existing systems.",
      "ai_keywords": [
        "LLM",
        "data science agents",
        "machine learning pipeline",
        "expert knowledge base",
        "agentic knowledgeable tree search",
        "self-adaptive coding strategy"
      ]
    },
    "publishedAt": "2025-06-12T13:59:32.000Z",
    "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
    "summary": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10974.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10960",
      "authors": [
        {
          "_id": "684bb33a3b733ba3336870c5",
          "name": "Kangwei Liu",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c6",
          "name": "Siyuan Cheng",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c7",
          "name": "Bozhong Tian",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c8",
          "name": "Xiaozhuan Liang",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c9",
          "name": "Yuyang Yin",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870ca",
          "name": "Meng Han",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cb",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:50.549Z",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cc",
          "name": "Bryan Hooi",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cd",
          "user": {
            "_id": "635113fdcba4ff2e81cb236e",
            "avatarUrl": "/avatars/f80df906b722b4901debce9baa867073.svg",
            "isPro": false,
            "fullname": "chen",
            "user": "Jasonchen123",
            "type": "user"
          },
          "name": "Xi Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T05:12:27.526Z",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870ce",
          "name": "Shumin Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:57:05.000Z",
      "submittedOnDailyAt": "2025-06-13T03:43:12.055Z",
      "title": "중국 유해 콘텐츠 감지 벤치마크",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)은 유해한 콘텐츠 검출 작업에 자동화되어 있습니다. 이들은 모델러가 정책 위반을 인식하고 콘텐츠 리뷰의 전반적인 효율성과 정확도를 향상시키는 데 도움을 주며, 그러나 현재의 유해한 콘텐츠 검출 리소스는 주로 영어에 집중되어 있으며, 중국어 데이터셋은 희귀하고 범위가 제한되어 있습니다. 우리는 6가지 대표적인 카테고리를 수록하고, 모든 데이터는 현실적인 것들로 구축된 전문적으로 설명된 벤치마크를 제시합니다. 이 설명 프로세스는 LLMs가 중국어 유해한 콘텐츠 검출에 도움이 되는 명확한 전문 지식을 제공하는 지식 규칙 기반을 얻습니다. 또한, 우리는 인간 설명된 지식 규칙과 대 언어 모델에서 얻을 수 있는 잠재적 지식을 통합한 지식 부가 기반 라인을 제안하고, 이 방식에 의해 작은 모델이 가장 선진한 LLMs와 비교하여 높은 성능을 달성할 수 있습니다. 코드와 데이터는 https://github.com/zjunlp/ChineseHarm-bench 에 액세스할 수 있습니다.",
      "upvotes": 9,
      "discussionId": "684bb33a3b733ba3336870cf",
      "ai_summary": "A benchmark for Chinese harmful content detection is introduced, along with a knowledge-augmented model that enhances efficiency and accuracy using human-annotated rules and LLMs.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "harmful content detection",
        "knowledge-augmented baseline",
        "annotation process",
        "knowledge rule base",
        "Chinese datasets"
      ]
    },
    "publishedAt": "2025-06-12T13:57:05.000Z",
    "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark",
    "summary": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which covers six representative categories and\nis constructed entirely from real-world data. Our annotation process further\nyields a knowledge rule base that provides explicit expert knowledge to assist\nLLMs in Chinese harmful content detection. In addition, we propose a\nknowledge-augmented baseline that integrates both human-annotated knowledge\nrules and implicit knowledge from large language models, enabling smaller\nmodels to achieve performance comparable to state-of-the-art LLMs. Code and\ndata are available at https://github.com/zjunlp/ChineseHarm-bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10960.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10821",
      "authors": [
        {
          "_id": "684b91c73b733ba333687033",
          "name": "Huaying Yuan",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687034",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687035",
          "name": "Junjie Zhou",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687036",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687037",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T15:39:10.000Z",
      "submittedOnDailyAt": "2025-06-13T01:20:51.837Z",
      "title": "VideoDeepResearch: 장 비디오 이해에 대한 아웃 로카ル 도구의 사용",
      "submittedOnDailyBy": {
        "_id": "66d916a7b86f0d569aa19b60",
        "avatarUrl": "/avatars/2537cee66afecc2d999e05b01c78d319.svg",
        "isPro": false,
        "fullname": "huaying Yuan",
        "user": "avery00",
        "type": "user"
      },
      "summary": "장기 비디오 이해(LVU)는 현재의 다모달 대 언어 모델(MLLMs)에 대한 중요한 문제로, 이 문제를 해결하기 위해 일반적으로 확장된 컨텍스트 윈도우, 강력한 시각 인식 능력, 그리고 전문 지식이 있는 기초적인 MLLM이 필요합니다. 본 논문에서는 이러한 일반적인 믿음을 도전하고, VideoDeepResearch라는 새로운 에이전트 프레임워크를 소개합니다. 우리의 접근 방식은 단일 텍스트 기반의 대 언어 모델(LRM)과 모듈화된 다모달 툴킷을 조합하여 구성됩니다. 이 툴킷에는 다모달 리튬너와 시각 인식기가 포함되어 있으며, 실용적으로 쉽게 사용할 수 있습니다. LVU 태스크마다, 시스템은 논리론을 통해 문제 해결 전략을 구축하고, 필요한 비디오 콘텐츠에 선택적으로 접근하여 툴을 사용합니다. 인기 있는 LVU 벤치마크에서 광범위한 실험을 수행했습니다. MLVU(테스트), LVBench, LongVideoBench에서 VideoDeepResearch은 현재의 MLLM 기반 선에서 큰 개선을 거뒀으며, 각각 9.6%, 6.6%, 3.9%의 개선률을 달성했습니다. 이러한 결과를 통해 에이전트 시스템이 LVU 프로브의 중요한 문제를 극복할 가능성은 밝혀져 있습니다.",
      "upvotes": 9,
      "discussionId": "684b91c73b733ba333687038",
      "ai_summary": "VideoDeepResearch, a text-only reasoning model with modular tools, surpasses existing baselines in long video understanding tasks without extending context windows or enhancing visual perception capabilities.",
      "ai_keywords": [
        "long video understanding",
        "multi-modal large language models",
        "VideoDeepResearch",
        "text-only large reasoning model",
        "multimodal retrievers",
        "visual perceivers",
        "MLVU",
        "Video-MME",
        "LVBench",
        "LongVideoBench",
        "agentic systems"
      ]
    },
    "publishedAt": "2025-06-12T11:39:10.000Z",
    "title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using",
    "summary": "Long video understanding (LVU) presents a significant challenge for current\nmulti-modal large language models (MLLMs) due to the task's inherent complexity\nand context window constraint. It is widely assumed that addressing LVU tasks\nrequires foundation MLLMs with extended context windows, strong visual\nperception capabilities, and proficient domain expertise. In this work, we\nchallenge this common belief by introducing VideoDeepResearch, a novel agentic\nframework for long video understanding. Our approach relies solely on a\ntext-only large reasoning model (LRM) combined with a modular multi-modal\ntoolkit, including multimodal retrievers and visual perceivers, all of which\nare readily available in practice. For each LVU task, the system formulates a\nproblem-solving strategy through reasoning, while selectively accessing and\nutilizing essential video content via tool using. We conduct extensive\nexperiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.\nOur results demonstrate that VideoDeepResearch achieves substantial\nimprovements over existing MLLM baselines, surpassing the previous\nstate-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and\nLongVideoBench, respectively. These findings highlight the promise of agentic\nsystems in overcoming key challenges in LVU problems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d916a7b86f0d569aa19b60",
      "avatarUrl": "/avatars/2537cee66afecc2d999e05b01c78d319.svg",
      "fullname": "huaying Yuan",
      "name": "avery00",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10741",
      "authors": [
        {
          "_id": "684b881f3b733ba333686fd4",
          "user": {
            "_id": "64966691990b342dcc9fccb5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64966691990b342dcc9fccb5/tQSrE3MkBeakk5QYfgHSo.jpeg",
            "isPro": false,
            "fullname": "sixiang chen",
            "user": "Ephemeral182",
            "type": "user"
          },
          "name": "SiXiang Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:47.414Z",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd5",
          "name": "Jianyu Lai",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd6",
          "name": "Jialin Gao",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd7",
          "name": "Tian Ye",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd8",
          "name": "Haoyu Chen",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd9",
          "name": "Hengyu Shi",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fda",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdb",
          "name": "Yunlong Lin",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdc",
          "name": "Song Fei",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdd",
          "name": "Zhaohu Xing",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fde",
          "name": "Yeying Jin",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdf",
          "name": "Junfeng Luo",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fe0",
          "name": "Xiaoming Wei",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fe1",
          "name": "Lei Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T14:28:12.000Z",
      "submittedOnDailyAt": "2025-06-13T04:30:38.214Z",
      "title": "PosterCraft: 통일된 프레임워크에서 고품질 미술 포스터 생성에 대한 재고찰",
      "submittedOnDailyBy": {
        "_id": "66015e8aa4d296af07de538e",
        "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
        "isPro": false,
        "fullname": "Ye",
        "user": "Owen777",
        "type": "user"
      },
      "summary": "미술 海报의 생성은 단순한 디자인 이미지보다 더 어려워집니다: 그것은 텍스트의 정밀한 그림뿐만 아니라 추상적인 예술적인 내용을 무한히 잘 맞추는 엔지니어링, 매력적인 레이아웃, 그리고 전체적인 스타일의 조화성을 요구하는 데서부터 어려워집니다. 이를 대처하기 위해, 우리는 PosterCraft라는 통합 프레임워크를 제안하고 있습니다. 이는 이전의 모듈화된 폼 프로리프와 엄격한, 사전 정의된 레이아웃을 포기하고 모델이 자유롭게 일관된, 시각적으로 매력적인 구성을 탐색할 수 있도록 허용합니다. PosterCraft는 고품질의 미술 海报의 생성을 최적화하기 위해, 엄격한 작업 흐름을 구축하고 있습니다: (i) 새로 추가된 Text-Render-2M 데이터 세트에 대한 대규모 텍스트 그림 최적화; (ii) HQ-Poster100K에 대한 영역에 대한 규정된 微調校; (iii) 예술적인 텍스트의 强化学習을 수행하는 최고의 n의 취향 최적화; (iv) 공통의 시각 언어의 피드백의 리فا인먼트. 각 단계는 그 특정한 필요에 맞는 전산화 데이터 구축 폼 프로리프를 통해 지원되고, 복잡한 아키텍처의 변경을 제외한 강력한 훈련을 가능하게 합니다. 여러 실험으로 평가된 PosterCraft는 렌더링 정확도, 레이아웃의 일관성, 전체적인 시각적인 매력성에서 오픈 소스 기반 라이닝을 크게 초월하고, 가장 先端의 상업 시스템의 품질에 근접합니다. 우리의 코드, 모델, 데이터 세트는 프로젝트 페이지에 있습니다: https://ephemeral182.github.io/PosterCraft",
      "upvotes": 9,
      "discussionId": "684b881f3b733ba333686fe2",
      "projectPage": "https://ephemeral182.github.io/PosterCraft/",
      "githubRepo": "https://github.com/Ephemeral182/PosterCraft",
      "ai_summary": "PosterCraft improves aesthetic poster generation through a unified, modular pipeline with enhanced text rendering, region-aware fine-tuning, aesthetic reinforcement learning, and joint vision-language refinement.",
      "ai_keywords": [
        "text-rendering optimization",
        "Text-Render-2M",
        "region-aware supervised fine-tuning",
        "HQ-Poster100K",
        "aesthetic-text-reinforcement learning",
        "best-of-n preference optimization",
        "joint vision-language feedback refinement"
      ]
    },
    "publishedAt": "2025-06-12T10:28:12.000Z",
    "title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a\n  Unified Framework",
    "summary": "Generating aesthetic posters is more challenging than simple design images:\nit requires not only precise text rendering but also the seamless integration\nof abstract artistic content, striking layouts, and overall stylistic harmony.\nTo address this, we propose PosterCraft, a unified framework that abandons\nprior modular pipelines and rigid, predefined layouts, allowing the model to\nfreely explore coherent, visually compelling compositions. PosterCraft employs\na carefully designed, cascaded workflow to optimize the generation of\nhigh-aesthetic posters: (i) large-scale text-rendering optimization on our\nnewly introduced Text-Render-2M dataset; (ii) region-aware supervised\nfine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via\nbest-of-n preference optimization; and (iv) joint vision-language feedback\nrefinement. Each stage is supported by a fully automated data-construction\npipeline tailored to its specific needs, enabling robust training without\ncomplex architectural modifications. Evaluated on multiple experiments,\nPosterCraft significantly outperforms open-source baselines in rendering\naccuracy, layout coherence, and overall visual appeal-approaching the quality\nof SOTA commercial systems. Our code, models, and datasets can be found in the\nProject page: https://ephemeral182.github.io/PosterCraft",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10741.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66015e8aa4d296af07de538e",
      "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
      "fullname": "Ye",
      "name": "Owen777",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10890",
      "authors": [
        {
          "_id": "684b8b533b733ba333686fe4",
          "user": {
            "_id": "62bc1adacaf01b9bec398547",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
            "isPro": false,
            "fullname": "Zhao Zhang",
            "user": "zbrl",
            "type": "user"
          },
          "name": "Zhao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:45.153Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe5",
          "name": "Yutao Cheng",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe6",
          "user": {
            "_id": "6669a0cc9f28880b31d7c4ef",
            "avatarUrl": "/avatars/bd66a6f68a9af2bf7ee40510579e57fe.svg",
            "isPro": false,
            "fullname": "dexiang hong",
            "user": "hxxxl",
            "type": "user"
          },
          "name": "Dexiang Hong",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:22:14.202Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe7",
          "user": {
            "_id": "63fd7279ed9eead590fd02ed",
            "avatarUrl": "/avatars/4cf6f005069412ee87ed07cd81500f1e.svg",
            "isPro": false,
            "fullname": "YangMaoke",
            "user": "YangMaoke",
            "type": "user"
          },
          "name": "Maoke Yang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:22:14.202Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe8",
          "user": {
            "_id": "6436619ead9b9147de287a24",
            "avatarUrl": "/avatars/180c43c79e552dd345636a47db80e3e9.svg",
            "isPro": false,
            "fullname": "ShiLayne",
            "user": "ShiLayne",
            "type": "user"
          },
          "name": "Gonglei Shi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-13T02:23:52.345Z",
          "hidden": true
        },
        {
          "_id": "684b8b533b733ba333686fe9",
          "name": "Lei Ma",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fea",
          "name": "Hui Zhang",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686feb",
          "name": "Jie Shao",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fec",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T16:54:39.000Z",
      "submittedOnDailyAt": "2025-06-13T00:55:02.473Z",
      "title": "CreatiPoster: 멀티레이어 그래픽 디자인의 편집 가능한 및 제어 가능한 생성에 대한 도구입니다.",
      "submittedOnDailyBy": {
        "_id": "62bc1adacaf01b9bec398547",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
        "isPro": false,
        "fullname": "Zhao Zhang",
        "user": "zbrl",
        "type": "user"
      },
      "summary": "그래픽디자인은 상업적 및 개인적인 측면에서 모두 중요한 역할을 수행하지만, 고품질, 편집 가능한, 아름다운 그래픽 컴포지션의 제작은 특히 초보자에게 시간이 걸리고 높은 기술 요구가 있는 과제입니다. 현재의 AI툴은 작업흐름의 일부를 자동화하지만, 사용자가 제공하는 자산을 정확히 포함시키고 편집 가능성 유지하며, 전문적인 시각적 매력을 달성하는 것은 어렵습니다. 상업적 시스템에서, Canva Magic Design와 같은 도구는 큰 템플릿 라이브러리를 기반으로 하지만, 이를 재현하는 것은 실질적이지 않습니다. 본 논문에서는 CreatiPoster라는 프레임워크를 소개합니다. 이 프레임워크는 선택 가능한 자연어 지시 또는 자산으로부터 편집 가능한 다층 컴포지션을 생성합니다. 프로토콜 모델과 RGBA 규모의 다모달 모델은 텍스트 또는 자산의 각 레이어에 대해 정밀한 배치, 계층, 콘텐츠 및 스타일을 상세히 기록한 JSON 특수 파일을 생성합니다. 그리고 간단한 배경 프로ン탔토도 포함합니다. 다음으로, 이 레이어에 기반한 조건부 배경 모델은 이 그려진 후그라운드 레이어에 기반하여 일관된 배경을 합성합니다. 그래픽 디자인의 자동화 시장에서 벤치마크를 구축하고, CreatiPoster는 선진적인 오픈소스 접근법과 특허 가능한 비즈니스 시스템을 초과하는 것을 보여주며, 100,000개의 다층 디자인의 복사권 없는 코퍼스를 공개하여 피드백을 촉진합니다. CreatiPoster는 캔버스 편집, 텍스트 오버라이드, 리스폰시 리사이징, 다언어 대응, 애니메이션 포스터 등 다양한 애플리케이션을 지원하고, AI를 활용한 그래픽 디자인의 민주화를 촉진합니다. 프로젝트 홈 페이지: https://github.com/graphic-design-ai/creatiposter",
      "upvotes": 7,
      "discussionId": "684b8b533b733ba333686fed",
      "githubRepo": "https://github.com/graphic-design-ai/creatiposter",
      "ai_summary": "CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.",
      "ai_keywords": [
        "RGBA large multimodal model",
        "JSON specification",
        "conditional background model",
        "automated metrics",
        "graphic-design generation",
        "multi-layer designs",
        "AI-assisted graphic design"
      ]
    },
    "publishedAt": "2025-06-12T12:54:39.000Z",
    "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation",
    "summary": "Graphic design plays a crucial role in both commercial and personal contexts,\nyet creating high-quality, editable, and aesthetically pleasing graphic\ncompositions remains a time-consuming and skill-intensive task, especially for\nbeginners. Current AI tools automate parts of the workflow, but struggle to\naccurately incorporate user-supplied assets, maintain editability, and achieve\nprofessional visual appeal. Commercial systems, like Canva Magic Design, rely\non vast template libraries, which are impractical for replicate. In this paper,\nwe introduce CreatiPoster, a framework that generates editable, multi-layer\ncompositions from optional natural-language instructions or assets. A protocol\nmodel, an RGBA large multimodal model, first produces a JSON specification\ndetailing every layer (text or asset) with precise layout, hierarchy, content\nand style, plus a concise background prompt. A conditional background model\nthen synthesizes a coherent background conditioned on this rendered foreground\nlayers. We construct a benchmark with automated metrics for graphic-design\ngeneration and show that CreatiPoster surpasses leading open-source approaches\nand proprietary commercial systems. To catalyze further research, we release a\ncopyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports\ndiverse applications such as canvas editing, text overlay, responsive resizing,\nmultilingual adaptation, and animated posters, advancing the democratization of\nAI-assisted graphic design. Project homepage:\nhttps://github.com/graphic-design-ai/creatiposter",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10890.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bc1adacaf01b9bec398547",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
      "fullname": "Zhao Zhang",
      "name": "zbrl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09967",
      "authors": [
        {
          "_id": "684ae1eedbd21a9cc27b0f10",
          "user": {
            "_id": "67469d6a8407f929491dce06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
            "isPro": true,
            "fullname": "Shangshang Wang",
            "user": "upup-ashton-wang",
            "type": "user"
          },
          "name": "Shangshang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:31.479Z",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f11",
          "name": "Julian Asilis",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f12",
          "name": "Ömer Faruk Akgül",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f13",
          "name": "Enes Burak Bilgin",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f14",
          "name": "Ollie Liu",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f15",
          "user": {
            "_id": "63c8454e46421a2efe82709d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
            "isPro": true,
            "fullname": "Deqing Fu",
            "user": "deqing",
            "type": "user"
          },
          "name": "Deqing Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:29.284Z",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f16",
          "user": {
            "_id": "644bf65522d211df6444a7f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf65522d211df6444a7f4/k_ddZdQDg2fzhwjI1EXyx.jpeg",
            "isPro": false,
            "fullname": "Willie Neiswanger",
            "user": "willieneis",
            "type": "user"
          },
          "name": "Willie Neiswanger",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:27.301Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:44:01.000Z",
      "submittedOnDailyAt": "2025-06-13T02:39:37.215Z",
      "title": "레サ: 투명한 이유론리 모뎀을 SAEs를 통해 생성합니다.",
      "submittedOnDailyBy": {
        "_id": "67469d6a8407f929491dce06",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
        "isPro": true,
        "fullname": "Shangshang Wang",
        "user": "upup-ashton-wang",
        "type": "user"
      },
      "summary": "모델의 잠재적인 표현을 활용하여, 언어 모델에 강력한 논리론을 효율적으로 추출할 수 있는지 조사합니다. 이 질문에 답하는 것은 Resa입니다. Resa는 새로운 효율적인 희소한 자동 인코더 조정(SAE-Tuning) 절차를 사용하여, 150M 단위의 논리론 모델의 가족입니다. 이 방법은 먼저, SAE를 훈련하여 소스 모델에서 논리론 능력을 파악하고, 훈련된 SAE를 사용하여 표준적인 오버피팅 훈련 프로세스를 가이드하고, 논리론 능력을 목표 모델에 추출합니다. 이는 논리론 트래스 없는 검증된 질문 답변 데이터로 수행됩니다. 특히, 이 방법을 특정의 기초 모델에 적용하여 RL 후 학습을 수행하기 전에, SAE-Tuning은 RL 훈련 모델의 97% 이상의 논리론 성능을 유지하면서, 훈련 비용 2000배 이상 감소하고, 약 1달러와 약 20분의 훈련 시간을 줄입니다. 또한, 이 방법은 예를 들어 2개의 GPU에서 1시간 이내의 가벼운 RL 훈련 모델에 적용할 때, AIME24의 Pass@1이 43.33%, AMC23의 Pass@1이 90%의 논리론 성능을 거의 추가 비용 없이 달성할 수 있습니다. 놀라운 사실은 SAE에서 추출된 논리론 능력은 잠재적으로 일반화 가능한 모듈화 가능한 것입니다. 일반화는 추출된 능력이 더 큰 중복된 코퍼스에서도 성능을 향상시키는 것을 의미합니다. 모듈화는 Qwen이나 Qwen-Math에서 추출된 능력이 테스트 시 R1-Distill 모델에 추가되어, 리트레이닝 없이 상대적으로 큰 이익을 얻을 수 있다는 것을 의미합니다. 확장된 실험은 이러한 발견을 증명하며, 모든 Artifacts는 완전하게 오픈 소스로 제공됩니다.",
      "upvotes": 6,
      "discussionId": "684ae1eedbd21a9cc27b0f17",
      "projectPage": "https://shangshangwang.notion.site/resa",
      "githubRepo": "https://github.com/shangshang-wang/Resa",
      "ai_summary": "SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.",
      "ai_keywords": [
        "sparse autoencoder tuning",
        "SAE-Tuning",
        "reasoning models",
        "verification",
        "sparse autoencoders",
        "supervised fine-tuning",
        "RL post-training",
        "Pass@1",
        "AIME24",
        "AMC23",
        "generality",
        "modularity",
        "R1-Distill",
        "Qwen",
        "Qwen-Math"
      ]
    },
    "publishedAt": "2025-06-11T13:44:01.000Z",
    "title": "Resa: Transparent Reasoning Models via SAEs",
    "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround 1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09967.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67469d6a8407f929491dce06",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
      "fullname": "Shangshang Wang",
      "name": "upup-ashton-wang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10910",
      "authors": [
        {
          "_id": "684bbe273b733ba3336870ed",
          "name": "Mistral-AI",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870ef",
          "name": "Abhinav Rastogi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f0",
          "name": "Albert Q. Jiang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f1",
          "name": "Andy Lo",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f2",
          "name": "Gabrielle Berrada",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f3",
          "name": "Guillaume Lample",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f4",
          "name": "Jason Rute",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f5",
          "name": "Joep Barmentlo",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f6",
          "name": "Karmesh Yadav",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f7",
          "name": "Kartik Khandelwal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f8",
          "name": "Khyathi Raghavi Chandu",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f9",
          "name": "Léonard Blier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fa",
          "name": "Lucile Saulnier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fb",
          "name": "Matthieu Dinot",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fc",
          "name": "Maxime Darrin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fd",
          "name": "Neha Gupta",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fe",
          "name": "Roman Soletskyi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870ff",
          "name": "Sagar Vaze",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687100",
          "name": "Teven Le Scao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687101",
          "name": "Yihan Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687102",
          "name": "Adam Yang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687103",
          "name": "Alexander H. Liu",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687104",
          "name": "Alexandre Sablayrolles",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687105",
          "name": "Amélie Héliou",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687106",
          "name": "Amélie Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687107",
          "name": "Andy Ehrenberg",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687108",
          "name": "Anmol Agarwal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687109",
          "name": "Antoine Roux",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710a",
          "name": "Arthur Darcet",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710b",
          "name": "Arthur Mensch",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710c",
          "name": "Baptiste Bout",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710d",
          "name": "Baptiste Rozière",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710e",
          "name": "Baudouin De Monicault",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710f",
          "name": "Chris Bamford",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687110",
          "name": "Christian Wallenwein",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687111",
          "name": "Christophe Renaudin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687112",
          "name": "Clémence Lanfranchi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687113",
          "name": "Darius Dabert",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687114",
          "name": "Devon Mizelle",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687115",
          "name": "Diego de las Casas",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687116",
          "name": "Elliot Chane-Sane",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687117",
          "name": "Emilien Fugier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687118",
          "name": "Emma Bou Hanna",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687119",
          "name": "Gauthier Delerce",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711a",
          "name": "Gauthier Guinet",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711b",
          "name": "Georgii Novikov",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711c",
          "name": "Guillaume Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711d",
          "name": "Himanshu Jaju",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711e",
          "name": "Jan Ludziejewski",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711f",
          "name": "Jean-Hadrien Chabran",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687120",
          "name": "Jean-Malo Delignon",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687121",
          "name": "Joachim Studnia",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687122",
          "name": "Jonas Amar",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687123",
          "name": "Josselin Somerville Roberts",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687124",
          "name": "Julien Denize",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687125",
          "name": "Karan Saxena",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687126",
          "name": "Kush Jain",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687127",
          "name": "Lingxiao Zhao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687128",
          "name": "Louis Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687129",
          "name": "Luyu Gao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712a",
          "name": "Lélio Renard Lavaud",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712b",
          "name": "Marie Pellat",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712c",
          "name": "Mathilde Guillaumin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712d",
          "name": "Mathis Felardos",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712e",
          "name": "Maximilian Augustin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712f",
          "name": "Mickaël Seznec",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687130",
          "name": "Nikhil Raghuraman",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687131",
          "name": "Olivier Duchenne",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687132",
          "name": "Patricia Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687133",
          "name": "Patrick von Platen",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687134",
          "name": "Patryk Saffer",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687135",
          "name": "Paul Jacob",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687136",
          "name": "Paul Wambergue",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687137",
          "name": "Paula Kurylowicz",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687138",
          "name": "Pavankumar Reddy Muddireddy",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687139",
          "name": "Philomène Chagniot",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713a",
          "name": "Pierre Stock",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713b",
          "name": "Pravesh Agrawal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713c",
          "name": "Romain Sauvestre",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713d",
          "name": "Rémi Delacourt",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713e",
          "name": "Sanchit Gandhi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713f",
          "name": "Sandeep Subramanian",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687140",
          "name": "Shashwat Dalal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687141",
          "name": "Siddharth Gandhi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687142",
          "name": "Soham Ghosh",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687143",
          "name": "Srijan Mishra",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687144",
          "name": "Sumukh Aithal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687145",
          "name": "Szymon Antoniak",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687146",
          "name": "Thibault Schueller",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687147",
          "name": "Thibaut Lavril",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687148",
          "name": "Thomas Robert",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687149",
          "name": "Thomas Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714a",
          "name": "Timothée Lacroix",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714b",
          "name": "Valeriia Nemychnikova",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714c",
          "name": "Victor Paltz",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714d",
          "name": "Virgile Richard",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714e",
          "name": "Wen-Ding Li",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714f",
          "name": "William Marshall",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687150",
          "name": "Xuanyu Zhang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687151",
          "name": "Yunhao Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:22:37.000Z",
      "submittedOnDailyAt": "2025-06-13T04:29:39.974Z",
      "title": "마지스탈",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "마지스탈의 최초의 추론 모델과 자기 회사의 확장 가능한 강화 학습(RL) 파이프라인을 소개합니다. 기존 구현과 선행 모델로부터 얻은 RL 도구 체인에 의존하지 않고, 자기 회사의 모델과 인프라를만 의존하여 간단한 접근 방식을 채택합니다. 특히, 우리는 LLM의 단순한 RL 훈련의 한계를 탐색하기 위한 스택을 보여주고, 모델의 이유론을 强制的 간단한 방법을 보여주고, 문서 데이터만 기반으로의 RL은 초기 체크포인트의 능력을 크게 유지합니다. 문서 데이터에 대한 RL은 다양한 이해, 지시 따라기, 함수 호출을 유지하거나 향상시키기를 발견했습니다. Magistral Medium, Mistral Medium 3에 대한 이유론을 학습시킨 모델을 소개하고, Magistral Small(Apache 2.0)을 공개합니다. Magistral Small은 Magistral Medium으로부터 냉정하게 시작되는 데이터를 포함하여 더 강력한 모델이 됩니다.",
      "upvotes": 5,
      "discussionId": "684bbe283b733ba333687152",
      "ai_summary": "Magistral, a scalable reinforcement learning pipeline, demonstrates that RL can enhance multimodal understanding and instruction following in large language models without requiring existing RL traces.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "LLMs",
        "multimodal understanding",
        "instruction following",
        "function calling",
        "cold-start data"
      ]
    },
    "publishedAt": "2025-06-12T13:22:37.000Z",
    "title": "Magistral",
    "summary": "We introduce Magistral, Mistral's first reasoning model and our own scalable\nreinforcement learning (RL) pipeline. Instead of relying on existing\nimplementations and RL traces distilled from prior models, we follow a ground\nup approach, relying solely on our own models and infrastructure. Notably, we\ndemonstrate a stack that enabled us to explore the limits of pure RL training\nof LLMs, present a simple method to force the reasoning language of the model,\nand show that RL on text data alone maintains most of the initial checkpoint's\ncapabilities. We find that RL on text maintains or improves multimodal\nunderstanding, instruction following and function calling. We present Magistral\nMedium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we\nopen-source Magistral Small (Apache 2.0) which further includes cold-start data\nfrom Magistral Medium.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10910.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2746
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09344",
      "authors": [
        {
          "_id": "684ae277dbd21a9cc27b118d",
          "name": "Inclusion AI",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b118e",
          "name": "Biao Gong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b118f",
          "name": "Cheng Zou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1190",
          "name": "Chuanyang Zheng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1191",
          "name": "Chunluan Zhou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1192",
          "name": "Canxiang Yan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1193",
          "name": "Chunxiang Jin",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1194",
          "name": "Chunjie Shen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1195",
          "name": "Dandan Zheng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1196",
          "name": "Fudong Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1197",
          "name": "Furong Xu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1198",
          "name": "GuangMing Yao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1199",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119a",
          "name": "Jingdong Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119b",
          "name": "Jianxin Sun",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119c",
          "name": "Jiajia Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119d",
          "name": "Jianjiang Zhu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119e",
          "name": "Jun Peng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119f",
          "name": "Kaixiang Ji",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a0",
          "name": "Kaiyou Song",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a1",
          "name": "Kaimeng Ren",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a2",
          "name": "Libin Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a3",
          "name": "Lixiang Ru",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a4",
          "name": "Lele Xie",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a5",
          "name": "Longhua Tan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a6",
          "name": "Lyuxin Xue",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a7",
          "name": "Lan Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a8",
          "name": "Mochen Bai",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a9",
          "name": "Ning Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11aa",
          "name": "Pei Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ab",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ac",
          "name": "Qinglong Zhang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ad",
          "name": "Qiang Xu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ae",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11af",
          "name": "Ruijie Xiong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b0",
          "name": "Sirui Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b1",
          "name": "Tinghao Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b2",
          "name": "Taisong Li",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b3",
          "name": "Weilong Chai",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b4",
          "name": "Xinyu Xiao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b5",
          "name": "Xiaomei Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b6",
          "name": "Xiaoxue Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b7",
          "name": "Xiao Lu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b8",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b9",
          "name": "Xingning Dong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ba",
          "name": "Xuzheng Yu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bb",
          "name": "Yi Yuan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bc",
          "name": "Yuting Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bd",
          "name": "Yunxiao Sun",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11be",
          "name": "Yipeng Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bf",
          "name": "Yifei Wu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c0",
          "name": "Yongjie Lyu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c1",
          "name": "Ziping Ma",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c2",
          "name": "Zipeng Feng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c3",
          "name": "Zhijiang Fang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c4",
          "name": "Zhihao Qiu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c5",
          "name": "Ziyuan Huang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c6",
          "name": "Zhengyu He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T02:50:49.000Z",
      "submittedOnDailyAt": "2025-06-13T01:53:15.172Z",
      "title": "명-오ムニ：감각과 생성의 통합 모델",
      "submittedOnDailyBy": {
        "_id": "644fcbea4f7316588267dc80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
        "isPro": false,
        "fullname": "Biao Gong",
        "user": "BiaoGong",
        "type": "user"
      },
      "summary": "명-Omni는 이미지, 텍스트, 음성, 비디오를 처리할 수 있으며, 동시에 연설과 이미지 생성에 강력한 실력을 보여주는 통일된 모달 모델입니다. 명-Omni는 다른 모달티어로부터 토큰을 추출하기 위해专用한 인코더를 사용하며, 이 인코더는 새로운 모달티어专用 루터러를 연결하여 MoE 아키텍처의 Ling에 처리됩니다. 이 설계는 효율적으로 여러 모달 입력을 처리하고 융합할 수 있도록 통일된 프레임워크 내에서 작동하며, 서로 다른 태스크를 수행하는 데 별도의 모델, 태스크专用의 최종 훈련, 또는 구조적 재설계가 필요하지 않습니다. 중요한 점은, 명-Omni는 음성과 이미지 생성을 지원함으로써 가치를 높입니다. 이는 고급 음성 디코더와 명-Lite-Uni의 고품질 이미지 생성을 결합하여 자연스러운 음성을 생성하고, 관련된 컨텍스트에 대한 대화, 텍스트로부터 연설의 변환, 다양한 이미지 편집을 수행할 수 있습니다. 실험 결과를 통해 명-Omni는 모든 모달티어에서의 통합 포지션과 생성에 강력한 해결책을 제공합니다. 특히, 명-Omni는 GPT-4o와 같은 모달티어 지원을 구현하는 첫 번째 오픈 소스 모델이며, 모든 코드와 모델 가중치를 공개하여 커뮤니티의 진보를 촉진하는 것을 목표로 합니다.",
      "upvotes": 4,
      "discussionId": "684ae277dbd21a9cc27b11c7",
      "ai_summary": "Ming-Omni is a unified multimodal model with dedicated encoders and modality-specific routers that can process images, text, audio, and video, and performs tasks like speech and image generation, context-aware chatting, and versatile image editing.",
      "ai_keywords": [
        "multimodal model",
        "encoders",
        "tokens",
        "MoE architecture",
        "modality-specific routers",
        "audio decoder",
        "Ming-Lite-Uni",
        "context-aware chatting",
        "text-to-speech conversion",
        "image editing",
        "unified perception",
        "generation",
        "open-source"
      ]
    },
    "publishedAt": "2025-06-10T22:50:49.000Z",
    "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
    "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644fcbea4f7316588267dc80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
      "fullname": "Biao Gong",
      "name": "BiaoGong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08060",
      "authors": [
        {
          "_id": "6848e0b042e4f9106973f280",
          "user": {
            "_id": "62f32eab52ad88c930bb3f3b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
            "isPro": true,
            "fullname": "Asankhaya Sharma",
            "user": "codelion",
            "type": "user"
          },
          "name": "Asankhaya Sharma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:08.045Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T08:37:19.000Z",
      "submittedOnDailyAt": "2025-06-13T00:31:17.814Z",
      "title": "推論에 의한 Fine-Tuned Transformer의 능력 추출",
      "submittedOnDailyBy": {
        "_id": "62f32eab52ad88c930bb3f3b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
        "isPro": true,
        "fullname": "Asankhaya Sharma",
        "user": "codelion",
        "type": "user"
      },
      "summary": "대 언어 모델은 자연어 처리를 변경했습니다が, 규범적인 미세 조정 훈련(SFT)은 계산량이 많습니다. 본 논문에서는, 이상적인 가정을 만족하는 경우, 무한 계산 명령의 지원과 미세 조정 훈련 데이터 세트의 액세스를 포함하여, 추론 시의 방법론을 통해, 특히 컨텍스트 학습(ICL)을 활용하여, SFT에서 얻은 능력을 근사할 수 있는 것이 기본 모델에 근사할 수 있다는 것을 공식적으로 증명합니다. 이러한 결과를 실제적인 시나리오로 확장합니다. 고정된 출력 길이 l의 문자 생성 태스크에서, 에러율 ε로 m 컨텍스트 내의 미세 조정 훈련의 행동을 근사하기 위해, 단어 집합 크기 V와 실패 확률 δ를 포함하는 O(mVε² log m/δ) 또는, 유한한 컨텍스트의 경우, O(l log Vε² log 1/δ)의 데이터 세트가 충분합니다. 선형 분류에서는, O(dε) 또는, 고정된 컨텍스트의 경우, O(1/ε² log 1/δ)의 데이터 세트가 충분합니다. 이러한 결과를 토대로, 토링 완전성을 기반으로 한 모델의 이론적 기초를 제공하며, 대 언어 모델의 자원 효율적인 활용과 실제적인 방법론을 연결합니다.",
      "upvotes": 4,
      "discussionId": "6848e0b042e4f9106973f281",
      "githubRepo": "https://github.com/codelion/optillm",
      "ai_summary": "Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.",
      "ai_keywords": [
        "supervised fine-tuning",
        "in-context learning",
        "base transformer model",
        "Turing completeness",
        "retrieval-augmented generation",
        "text generation",
        "linear classification"
      ]
    },
    "publishedAt": "2025-06-09T04:37:19.000Z",
    "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time\n  Techniques",
    "summary": "Large language models have transformed natural language processing, yet\nsupervised fine-tuning (SFT) remains computationally intensive. This paper\nformally proves that capabilities acquired through SFT can be approximated by a\nbase transformer model using inference-time techniques, specifically in-context\nlearning (ICL), without altering model parameters, under idealized assumptions\nincluding unbounded computational resources and access to the fine-tuning\ndataset. We extend these results to practical scenarios with finite context\nlengths and partial dataset access. For text generation tasks with fixed output\nlength l, datasets of size Oleft( m V{varepsilon^2} log\nm{delta} right) or, with bounded context, Oleft( l\nlog V{varepsilon^2} log 1{delta} right) suffice to approximate\nfine-tuned behavior across m contexts within error varepsilon, where V\nis the vocabulary size and delta is the failure probability. For linear\nclassification, datasets of size Oleft( d{varepsilon}\nright) or, with fixed context, Oleft( 1{varepsilon^2} log\n1{delta} right) are sufficient, where d is the input dimension.\nGrounded in the Turing completeness of transformers, these results provide a\ntheoretical foundation for resource-efficient deployment of large language\nmodels, with practical techniques like retrieval-augmented generation bridging\ntheory to real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f32eab52ad88c930bb3f3b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
      "fullname": "Asankhaya Sharma",
      "name": "codelion",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 91
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09952",
      "authors": [
        {
          "_id": "684ae226dbd21a9cc27b107a",
          "user": {
            "_id": "63579b21a8e247a69d4e13de",
            "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
            "isPro": false,
            "fullname": "Ziyi Wang",
            "user": "LavenderLA",
            "type": "user"
          },
          "name": "Ziyi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:40:18.063Z",
          "hidden": false
        },
        {
          "_id": "684ae226dbd21a9cc27b107b",
          "user": {
            "_id": "661cfae9a853782abad2a495",
            "avatarUrl": "/avatars/39723a07bf9efed8278e009fe966d044.svg",
            "isPro": false,
            "fullname": "Yanran Zhang",
            "user": "Yanran21",
            "type": "user"
          },
          "name": "Yanran Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:09.847Z",
          "hidden": false
        },
        {
          "_id": "684ae226dbd21a9cc27b107c",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "684ae226dbd21a9cc27b107d",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:23:21.000Z",
      "submittedOnDailyAt": "2025-06-13T06:56:09.722Z",
      "title": "UniPre3D: 3D Point Cloud 모델의 통합 예측 학습과 크로스 모드 가우시안 스폿팅\n\n(注意：翻译中保留了原文的专有名词和技术术语，以保持专业性和准确性。)",
      "submittedOnDailyBy": {
        "_id": "63579b21a8e247a69d4e13de",
        "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
        "isPro": false,
        "fullname": "Ziyi Wang",
        "user": "LavenderLA",
        "type": "user"
      },
      "summary": "점 Cloud 데이터의 스케일 다양성은 3D 비전의 통일 표현 학습 방법의 개발에 중대한 문제를 가져오고 있습니다. 현재, 통일된 3D 모델은 적고, 대상 모델과 공간 모델 모두에 효과적인 사전 학습 방법은 존재하지 않습니다. 본 논문에서는, 객체 또는 공간 수준의 점 Cloud 데이터의 어떤 스케일이나 3D 모델의 어떤 아키텍처에 무간 적용 가능한 첫 번째 통일된 사전 학습 방법인 UniPre3D를 소개합니다. 우리의 접근법은, 사전 학습 태스크로 가우시안 프리미티브를 예측하고, 미분 가능한 가우시안 스플릿팅을 사용하여 이미지를 렌더링하고, 정확한 픽셀 수준의 서브 객체와 최종적인 최적화를 가능하게 합니다. 또한, 사전 학습 태스크의 복잡성을 조절하고, 모델의 초점을 기하 구조에 맞추기 위해, 사전 학습된 이미지 모델에서 2D 특징을 통합하고, 기존의 테크닉knowledge를 통합합니다. 우리 제안 방법의 일반적인 유효성을 증명하기 위해, 다양한 점 Cloud 모델을 백터로, 대상 모델과 공간 모델 모두의 다양한 태스크에서 광범위한 실험을 수행했습니다. 코드는, https://github.com/wangzy22/UniPre3D 에 접근할 수 있습니다.",
      "upvotes": 3,
      "discussionId": "684ae226dbd21a9cc27b107e",
      "ai_summary": "UniPre3D is a unified pre-training method for 3D point clouds and models of any scale, using Gaussian primitives and 2D feature integration for effective performance across object and scene tasks.",
      "ai_keywords": [
        "point cloud",
        "3D vision",
        "representation learning",
        "UniPre3D",
        "Gaussian primitives",
        "differentiable Gaussian splatting",
        "pixel-level supervision",
        "end-to-end optimization",
        "2D features",
        "pre-trained image models",
        "geometric structures"
      ]
    },
    "publishedAt": "2025-06-11T13:23:21.000Z",
    "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal\n  Gaussian Splatting",
    "summary": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63579b21a8e247a69d4e13de",
      "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
      "fullname": "Ziyi Wang",
      "name": "LavenderLA",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09942",
      "authors": [
        {
          "_id": "684ae26adbd21a9cc27b1177",
          "user": {
            "_id": "625a5446f1063e7085d5178a",
            "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
            "isPro": false,
            "fullname": "Hao Peng",
            "user": "Wesleythu",
            "type": "user"
          },
          "name": "Hao Peng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:40:06.456Z",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b1178",
          "name": "Yunjia Qi",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b1179",
          "name": "Xiaozhi Wang",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117a",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117b",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117c",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:10:36.000Z",
      "submittedOnDailyAt": "2025-06-13T00:15:19.828Z",
      "title": "VerIF: 검증공학에 의한 순수 인스톰션 준수의 강화학습\n\n(Note: The translation provided is a direct translation of the given text. If \"VerIF\" is a specific term or acronym, it is kept as is. The rest of the text is translated into Korean while maintaining the original meaning and structure.)",
      "submittedOnDailyBy": {
        "_id": "625a5446f1063e7085d5178a",
        "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
        "isPro": false,
        "fullname": "Hao Peng",
        "user": "Wesleythu",
        "type": "user"
      },
      "summary": "强化학습에서 신뢰 가능한 보상(RLVR)는 대규모 언어 모델(LLMs)의 기능을 향상시키기 위한 중요한 기술로 자리잡고 있으며, 증명공학이 중심적인 역할을 수행하고 있습니다. 그러나, 强化学習에서의 지시에 따른 최적화는 아직 조사가 shallow한 상태입니다. 본 연구에서는 지시에 따른 强化学習의 증명 문제를 조사하고, 규칙 기반의 코드 증명과 LLM에 의한 증명(예: QwQ-32B)을 결합한 증명 방법 VerIF를 제안합니다. 이를 위해, VerInstruct라는 고품질의 지시에 따른 데이터셋을 구축하고 약 22,000개의 인스턴스를 포함하도록 하였습니다. VerIF를 이용한 强化学習은 두 개의 모델에 적용되었으며, 대표적인 지시에 따른 벤치마크에서 훈련된 모델은 다양한 성능을 향상시키고, 상대적으로 작은 모델 가운데 가장 先進한 성능을 달성하며, 새로운 제약에도 광범위하게 대응할 수 있습니다. 또한, 그 일반적인 능력은 RLVR를 포함하지 않고도 영향을 받지만, 현재의 모델 성능을 향상시킬 수 있음을 보여줍니다. 본 연구에서는 데이터셋, 코드, 모델을 공개하고, 향후 연구를 촉진하기 위해 https://github.com/THU-KEG/VerIF에 접근해주세요.",
      "upvotes": 3,
      "discussionId": "684ae26adbd21a9cc27b117d",
      "githubRepo": "https://github.com/THU-KEG/VerIF",
      "ai_summary": "VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "RLVR",
        "large language models",
        "LLMs",
        "rule-based code verification",
        "QwQ-32B",
        "instruction-following",
        "VerInstruct",
        "RL training",
        "instruction-following benchmarks",
        "state-of-the-art performance",
        "existing RL recipes"
      ]
    },
    "publishedAt": "2025-06-11T13:10:36.000Z",
    "title": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09942.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625a5446f1063e7085d5178a",
      "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
      "fullname": "Hao Peng",
      "name": "Wesleythu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10953",
      "authors": [
        {
          "_id": "684b9fa13b733ba333687066",
          "user": {
            "_id": "5fa9ff3ea13e063b8b2b60cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
            "isPro": false,
            "fullname": "Xing Han Lù",
            "user": "xhluca",
            "type": "user"
          },
          "name": "Xing Han Lù",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:26.665Z",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687067",
          "name": "Gaurav Kamath",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687068",
          "name": "Marius Mosbach",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687069",
          "name": "Siva Reddy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:53:58.000Z",
      "submittedOnDailyAt": "2025-06-13T02:22:59.640Z",
      "title": "웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃풋을 웹에 준비하여 웹을 아웃풋 준비하고 아웃",
      "submittedOnDailyBy": {
        "_id": "5fa9ff3ea13e063b8b2b60cb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
        "isPro": false,
        "fullname": "Xing Han Lù",
        "user": "xhluca",
        "type": "user"
      },
      "summary": "최근의 대언어 모델(LLMs)와 다양화 모델의 발전으로 웹 에이전트(AI 시스템) 개발에 큰 관심을 가졌습니다. 이 시스템은 웹 환경 내에서 자동적으로 작업을 수행할 수 있습니다. 웹 인터페이스와 LLM의 능력의 근본적인 차이로 인해, 현재의 접근 방식은 복잡한 웹 인터랙션의 자동화에 가장 큰 문제를 가지고 있습니다. 현재의 방법은 DOM 트리의 큰 처리, 스크린샷에 추가 정보를 추가하거나, 사용자 인터페이스를 완전히 피하는 API 인터랙션으로 이루어져 있습니다. 이 논문은 웹 에이전트 연구의 패러다임 전환을 주장합니다: 웹 에이전트가 인간을 위한 인터페이스에 적응하지 않아도 괜찮습니다. 대신, 에이전트 능력에 최적화된 새로운 인터페이스 패러다임의 개발이 필요합니다. 여기에서는, 에이전트 중심의 웹 인터페이스(AWI) 개념을 소개하고, AWI 설계에서 안전성, 효율성, 표준화를 강조하며, 주요 스택홀더의 이익을 고려한 6가지의 지침 원칙을 제시합니다. 이 레퍼런스는 현재의 인터페이스의 기본적인 한계를 극복하고, 더 효율적이고 신뢰성 있는 투명한 웹 에이전트의 설계를 가능하게 합니다. 이는 더 넓은 ML 커뮤니티의 협력이 필요합니다.",
      "upvotes": 2,
      "discussionId": "684b9fa13b733ba33368706a",
      "ai_summary": "A new Agentic Web Interface (AWI) design paradigm is proposed to optimize web agents for navigating websites, focusing on safety, efficiency, and standardization to address fundamental interface mismatches.",
      "ai_keywords": [
        "Large Language Models",
        "multimodal",
        "web agents",
        "Agentic Web Interface",
        "AWI",
        "DOM trees",
        "screenshots",
        "API interactions"
      ]
    },
    "publishedAt": "2025-06-12T13:53:58.000Z",
    "title": "Build the web for agents, not agents for the web",
    "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10953.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fa9ff3ea13e063b8b2b60cb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
      "fullname": "Xing Han Lù",
      "name": "xhluca",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07795",
      "authors": [
        {
          "_id": "6848dca942e4f9106973f25c",
          "user": {
            "_id": "6659b410a69183808d04b22f",
            "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
            "isPro": false,
            "fullname": "Xiaotian Ye",
            "user": "Acruxos",
            "type": "user"
          },
          "name": "Xiaotian Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:10.086Z",
          "hidden": false
        },
        {
          "_id": "6848dca942e4f9106973f25d",
          "name": "Mengqi Zhang",
          "hidden": false
        },
        {
          "_id": "6848dca942e4f9106973f25e",
          "name": "Shu Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T14:21:25.000Z",
      "submittedOnDailyAt": "2025-06-13T05:51:30.395Z",
      "title": "LLM Unlearning는 형태에 의존하지 않아야 합니다.",
      "submittedOnDailyBy": {
        "_id": "6659b410a69183808d04b22f",
        "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
        "isPro": false,
        "fullname": "Xiaotian Ye",
        "user": "Acruxos",
        "type": "user"
      },
      "summary": "大型 언어 모델(LLM)의 잊음 기능은 부적절한 지식의 제거 또는 억제로, 유해한 정보나 숨겨진 정보의 제어를 가능하게 하고, 오류가 있는 사용을 방지할 수 있는 가능성을 제공합니다. 그러나 최근의 연구는 실제 세계의 스케일에서 효과가 제한되어 있으며, 실용적인 도입을 방해하고 있습니다. 본 연구에서는, 여러 하위 단계의 실패에 대한 일반적인 문제를 특정했습니다: 현재의 잊음 방법의 효과성은 학습 샘플의 형식에 강한 의존성을 가집니다, 같은 지식의 다른 표현에 대한 일반화가 잘 되지 않습니다. 이 문제를 공식적으로 Form-Dependent Bias로 특징화하고, 다양한 하위 태스크에서 구체적인 표현 패턴을 체계적으로 조사했습니다. 이 문제를 확산시키기 위해, 지식 표현의 변화에 대한 잊음 방법의 견고성을 평가하기 위해 새로운 벤치마크 \"ORT\"를 도입했습니다. 결과는 현재의 기술에서 Form-Dependent Bias가 광범위하고 심각하다는 것을 보여주고 있습니다.\n\nLLM의 잊음 기능은 실제 세계의 안전한 스케일에서 나타나는 수많은 하위 태스크의 형태에 의존하지 않도록 하여, 오류가 있는 사용을 방지할 수 있는 것이 필요합니다. 이를 위해, 새로운 무학습 기반의 방법 \"Rank-one Concept Redirection (ROCR)\"을 도입하여, 바람직한 해결책의 루트를 소개합니다. ROCR은 하위 태스크의 불변성을 특정하고, 특히 활성화 된 위험한 개념을 잊음으로써 잊음을 수행합니다. 이를 위해, 특정 잊음 목표 개념의 모델 파라미터를 초당으로 변경하여, 모델의 인식을 다른 무害한 개념으로 리다이렉트할 수 있습니다. 확장된 실험은 전통적인 방법과 비교하여, 잊음 효과성을 크게 향상시키고, 높은 자연스러운 출력을 생성하는 것을 보여줍니다.",
      "upvotes": 2,
      "discussionId": "6848dca942e4f9106973f25f",
      "ai_summary": "Form-Dependent Bias limits the effectiveness of LLM unlearning across different knowledge expressions, and Rank-one Concept Redirection (ROCR) is proposed as a form-independent solution that enhances unlearning efficacy.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "unlearning",
        "Form-Dependent Bias",
        "ORT",
        "Rank-one Concept Redirection (ROCR)",
        "downstream tasks",
        "unlearning methods",
        "concept redirection",
        "model parameters",
        "activated dangerous concepts"
      ]
    },
    "publishedAt": "2025-06-09T10:21:25.000Z",
    "title": "LLM Unlearning Should Be Form-Independent",
    "summary": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to generalize to alternate expressions of\nthe same knowledge. We formally characterize this problem as Form-Dependent\nBias and systematically investigate its specific manifestation patterns across\nvarious downstream tasks. To quantify its prevalence and support future\nresearch, we introduce ORT, a novel benchmark designed to evaluate the\nrobustness of unlearning methods against variations in knowledge expression.\nResults reveal that Form-Dependent Bias is both widespread and severe among\ncurrent techniques.\n  We argue that LLM unlearning should be form-independent to address the\nendless forms of downstream tasks encountered in real-world security-critical\nscenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),\na novel training-free method, as a promising solution path. ROCR performs\nunlearning by targeting the invariants in downstream tasks, specifically the\nactivated dangerous concepts. It is capable of modifying model parameters\nwithin seconds to redirect the model's perception of a specific unlearning\ntarget concept to another harmless concept. Extensive experiments demonstrate\nthat ROCR significantly improves unlearning effectiveness compared to\ntraditional methods while generating highly natural outputs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07795.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6659b410a69183808d04b22f",
      "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
      "fullname": "Xiaotian Ye",
      "name": "Acruxos",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06694",
      "authors": [
        {
          "_id": "684ba6bc3b733ba333687093",
          "name": "Yuan Yuan",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687094",
          "name": "Yukun Liu",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687095",
          "name": "Chonghua Han",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687096",
          "user": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "isPro": false,
            "fullname": "Jie Feng",
            "user": "JJ-TMT",
            "type": "user"
          },
          "name": "Jie Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:13.023Z",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687097",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/LfNEmKBa7cYZhNNRnZRIx.png"
      ],
      "publishedAt": "2025-06-07T07:19:11.000Z",
      "submittedOnDailyAt": "2025-06-13T02:53:49.430Z",
      "title": "데이터 시롤을 파괴하는 것: 개방적이고 스케일러블한 이동 기반을 목표로 하여 생성 학습을 통해 모델을 구축하는 것.",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "기초 모델은 자연언어 처리 및 컴퓨터 비전 분야를 혁신적으로 변화시켰습니다. 이러한 모델들은 다양한 태스크와 데이터셋에 대한 일반적인 학습을 가능하게 해 왔습니다. 그러나 인간 이동에 대한 기초 모델의 구축은 이동 데이터의 은닉성과 이에 따른 기관 간 데이터 시일론의 존재로 인해 어려워졌습니다. 이를 해결하기 위해, 우리는 생성적인 연속 학습을 통해 이동 Fundamental 모델의 훈련을 가능하게 하는 Scalable Privacy-Preserving Framework인 \"MoveGCL\"을 제안하고 있습니다. MoveGCL은 자유드 教师 모델로부터 생성된 합성 트래픽을 재현하여 분산적이고 진보적인 모델 진화를 가능하게 하며, 知識 저장을 강화하기 위해 カタストロフィック フォーゲッティング를 완화시키는 ティアリゼット 전략을 통해 데이터 공유를 방지합니다. 이동 패턴의 다양성을 처리하기 위해, MoveGCL은 이동에 대한 知識를 가진 エクスパートルーティング 기관을 가진 Mixture-of-Experts Transformer를 사용하며, 계층적인 진보적인 적응성에 의해 연속적인 업데이트를 안정화하고 있습니다. 6개의 현실적인 도시 데이터셋을 대상으로의 실험은 MoveGCL이 JOINT TRAINING과 같은 성능을 달성하고, FEDERATED LEARNING 기반 학습을 크게 초과하며, 강력한 프라이버시 보호를 제공함을 보여주었습니다. MoveGCL은 이동에 대한 기초 모델의 개발의 중요한 단계를 기록하고, 기초 모델 시대의 개방적이고 Scalable한 프라이버시 보존 모델 개발의 실질적인 계획을 제공하고 있습니다.",
      "upvotes": 2,
      "discussionId": "684ba6bc3b733ba333687098",
      "githubRepo": "https://github.com/ScottLiu2003/MoveGCL",
      "ai_summary": "MoveGCL is a privacy-preserving framework using generative continual learning and a Mixture-of-Experts Transformer for training mobility foundation models without sharing raw data.",
      "ai_keywords": [
        "generative continual learning",
        "privacy-preserving",
        "Mixture-of-Experts Transformer",
        "mobility-aware expert routing mechanism",
        "layer-wise progressive adaptation",
        "catastrophic forgetting",
        "federated learning"
      ]
    },
    "publishedAt": "2025-06-07T03:19:11.000Z",
    "title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation\n  Models via Generative Continual Learning",
    "summary": "Foundation models have revolutionized fields such as natural language\nprocessing and computer vision by enabling general-purpose learning across\ndiverse tasks and datasets. However, building analogous models for human\nmobility remains challenging due to the privacy-sensitive nature of mobility\ndata and the resulting data silos across institutions. To bridge this gap, we\npropose MoveGCL, a scalable and privacy-preserving framework for training\nmobility foundation models via generative continual learning. Without sharing\nraw data, MoveGCL enables decentralized and progressive model evolution by\nreplaying synthetic trajectories generated from a frozen teacher model, and\nreinforces knowledge retention through a tailored distillation strategy that\nmitigates catastrophic forgetting. To address the heterogeneity of mobility\npatterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a\nmobility-aware expert routing mechanism, and employs a layer-wise progressive\nadaptation strategy to stabilize continual updates. Experiments on six\nreal-world urban datasets demonstrate that MoveGCL achieves performance\ncomparable to joint training and significantly outperforms federated learning\nbaselines, while offering strong privacy protection. MoveGCL marks a crucial\nstep toward unlocking foundation models for mobility, offering a practical\nblueprint for open, scalable, and privacy-preserving model development in the\nera of foundation models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/LfNEmKBa7cYZhNNRnZRIx.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10036",
      "authors": [
        {
          "_id": "684bc8db3b733ba33368718c",
          "name": "Javad Rajabi",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718d",
          "name": "Soroush Mehraban",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718e",
          "user": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "isPro": false,
            "fullname": "Seyedmorteza Sadat",
            "user": "msadat97",
            "type": "user"
          },
          "name": "Seyedmorteza Sadat",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:32.584Z",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718f",
          "name": "Babak Taati",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T21:25:46.000Z",
      "submittedOnDailyAt": "2025-06-13T05:21:38.595Z",
      "title": "토큰 파버바버지언 GUIDFLIVE 모델",
      "submittedOnDailyBy": {
        "_id": "63b4b02a103617b0a5b0ee2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
        "isPro": false,
        "fullname": "Seyedmorteza Sadat",
        "user": "msadat97",
        "type": "user"
      },
      "summary": "클래시퍼프리드 Guidance (CFG)는 현대의 분화 모델의 중요한 구성 요소 중 하나이며, 생성 품질과 입력 조건과의 일치성을 향상시키기 위해 필수적이다. 그러나 CFG는 특정한 훈련 절차가 필요하며, 조건부 생성에 제한되어 있다. 이러한 제한을 해결하기 위해, 우리는 Token Perturbation Guidance (TPG)라는 새로운 방법을 제안하고 있습니다. TPG는 분화 네트워크 내부의 중간 토큰 표현에 직접적으로 퍼런스 행렬을 적용하고 있습니다. TPG는 정규 유지를 위한 셔플 연산을 사용하여 구조적인 변경이 있어도 생성 품질을 향상시키는 효과적이고 안정적인 가이드 신호를 제공합니다. 이와 같이, TPG는 훈련이 필요하지 않고 입력 조건에 의존하지 않는 것을 통해 조건부 생성과 비조건부 생성에 적용할 수 있습니다. 또한, TPG가 제공하는 가이드 정보를 한 단계 더 분석하여 현재 훈련이 필요하지 않는 가이드 메소드와 CFG 같은 영향을 보여주는 것을 보여주고 있습니다. SDXL과 Stable Diffusion 2.1에서 확장된 실험은 TPG는 SDXL 기반 라인과 비교하여 비조건부 생성의 FID에 근似 2배의 개선을 실현하고, CFG와 유사한 성능을 보여주는 Prompt의 일치성에도 나타냅니다. 이러한 결과를 통해, TPG는 일반적인 조건에 의존하지 않는 가이드 메소드이며, 분화 모델의 광범위한 범위에서 CFG와 같은 장점을 제공하는 것을 확립하고 있습니다. 코드는 아래 URL에서 제공됩니다.\nhttps://github.com/TaatiTeam/Token-Perturbation-Guidance",
      "upvotes": 1,
      "discussionId": "684bc8db3b733ba333687190",
      "githubRepo": "https://github.com/TaatiTeam/Token-Perturbation-Guidance",
      "ai_summary": "Token Perturbation Guidance (TPG) enhances diffusion models with condition-agnostic, training-free guidance, similar to classifier-free guidance (CFG), without requiring architectural changes.",
      "ai_keywords": [
        "classifier-free guidance (CFG)",
        "Token Perturbation Guidance (TPG)",
        "perturbation matrices",
        "intermediate token representations",
        "norm-preserving shuffling",
        "FID",
        "prompt alignment",
        "SDXL",
        "Stable Diffusion 2.1"
      ]
    },
    "publishedAt": "2025-06-10T17:25:46.000Z",
    "title": "Token Perturbation Guidance for Diffusion Models",
    "summary": "Classifier-free guidance (CFG) has become an essential component of modern\ndiffusion models to enhance both generation quality and alignment with input\nconditions. However, CFG requires specific training procedures and is limited\nto conditional generation. To address these limitations, we propose Token\nPerturbation Guidance (TPG), a novel method that applies perturbation matrices\ndirectly to intermediate token representations within the diffusion network.\nTPG employs a norm-preserving shuffling operation to provide effective and\nstable guidance signals that improve generation quality without architectural\nchanges. As a result, TPG is training-free and agnostic to input conditions,\nmaking it readily applicable to both conditional and unconditional generation.\nWe further analyze the guidance term provided by TPG and show that its effect\non sampling more closely resembles CFG compared to existing training-free\nguidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1\nshow that TPG achieves nearly a 2times improvement in FID for unconditional\ngeneration over the SDXL baseline, while closely matching CFG in prompt\nalignment. These results establish TPG as a general, condition-agnostic\nguidance method that brings CFG-like benefits to a broader class of diffusion\nmodels. The code is available at\nhttps://github.com/TaatiTeam/Token-Perturbation-Guidance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10036.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b4b02a103617b0a5b0ee2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
      "fullname": "Seyedmorteza Sadat",
      "name": "msadat97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08373",
      "authors": [
        {
          "_id": "684ae1f3dbd21a9cc27b0f32",
          "name": "Kevin Galim",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f33",
          "name": "Ethan Ewer",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f34",
          "name": "Wonjun Kang",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f35",
          "name": "Minjae Lee",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f36",
          "name": "Hyung Il Koo",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f37",
          "name": "Kangwook Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T02:37:46.000Z",
      "submittedOnDailyAt": "2025-06-13T00:35:36.456Z",
      "title": "드래프트 기반의 근사 추론을 이용한 LLM",
      "submittedOnDailyBy": {
        "_id": "630c90123dc31beba6e8f406",
        "avatarUrl": "/avatars/2188b41fff122d4f5683b46c529ed79d.svg",
        "isPro": false,
        "fullname": "Kevin Galim",
        "user": "kev95",
        "type": "user"
      },
      "summary": "장문맥락의 대규모 언어 모델(LLMs)의 추론을 최적화하는 것은 Transformer의 2차원 계산량과 선형 메모리 복잡성에 의해 중요하게 되었다. 현재의 근사 방법들은 토큰이나 KV 페어의 중요성을 대략적으로 예측하기 위한 주요 방법은 KV 캐시의 제거, 함수 매개변수의 희소화, 프로ン퓰트의 압축 등이다. 우리는 작은 드래프트 모델을 사용하여 토큰과 KV 페어의 중요성을 더욱 정확하게 예측할 수 있는 새로운 프레임워크를 제안합니다. 특히, 우리는 다음 두 구현을 제안합니다: (i) SpecKV는 드래프트 출력을 사용하여 각 KV 페어의 중요성을 정확하게 평가하여 더 효과적인 KV 캐시의 제거에 맞게 설계됩니다. (ii) SpecPC는 드래프트 모델의 注意活性화를 사용하여 중요한 프로ン퓰트 토큰을 특정하고 제거합니다. 우리의 지식에 따르면, 이것은 드래프트 모델을 사용하여 근사 LLM 추론 속도 향상에 적용하는 첫 번째 연구입니다. 이는 전통적인 무손실 추론 디코딩보다 드래프트 모델의 역할을 확장하는 것입니다. 이론적 및 실험적 분석을 사용하여 우리의 방법을 설명하고, 드래프트 모델과 목표 모델의 注意 패턴의 관련성을 보여주었습니다. 장문맥락 벤치마크에서 확장된 실험을 통해, 우리의 방법은 기존의 기준과 비교하여도 정확도가 높고 메모리 사용량, 라턴시, 사이클 수의 개선이 동일합니다. 우리의 코드는 https://github.com/furiosa-ai/draft-based-approx-llm에 공개되어 있습니다.",
      "upvotes": 1,
      "discussionId": "684ae1f3dbd21a9cc27b0f38",
      "ai_summary": "A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.",
      "ai_keywords": [
        "Large Language Models",
        "Transformers",
        "key-value cache dropping",
        "sparse attention",
        "prompt compression",
        "draft models",
        "SpecKV",
        "SpecPC",
        "attention activations",
        "long-context benchmarks"
      ]
    },
    "publishedAt": "2025-06-09T22:37:46.000Z",
    "title": "Draft-based Approximate Inference for LLMs",
    "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08373.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630c90123dc31beba6e8f406",
      "avatarUrl": "/avatars/2188b41fff122d4f5683b46c529ed79d.svg",
      "fullname": "Kevin Galim",
      "name": "kev95",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08234",
      "authors": [
        {
          "_id": "684ae1dddbd21a9cc27b0edc",
          "name": "Yu-Ang Lee",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0edd",
          "name": "Guan-Ting Yi",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0ede",
          "name": "Mei-Yi Liu",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0edf",
          "name": "Jui-Chao Lu",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0ee0",
          "name": "Guan-Bo Yang",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0ee1",
          "name": "Yun-Nung Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T21:04:14.000Z",
      "submittedOnDailyAt": "2025-06-13T07:22:29.081Z",
      "title": "합성물 AI 시스템 최적화: 방법, 문제점 및 미래 방향에 대한 개요",
      "submittedOnDailyBy": {
        "_id": "6615752da15c52fa7ab3e2f7",
        "avatarUrl": "/avatars/37e72cfb829a42630d229080ad8d60f3.svg",
        "isPro": false,
        "fullname": "Lee",
        "user": "Speeeed",
        "type": "user"
      },
      "summary": "최근의 대규모 언어 모델(LLMs)와 AI 시스템의 발전은 복잡한 AI 작업 프로세스의 설계와 최적화에 새로운 패러다임을 가져왔습니다. 복합 AI 시스템은 여러 구성 요소를 통합하여 복잡한 작업을 수행하게 되었습니다. 그러나 이러한 시스템이 복잡해지는 데 따라, 개별 구성 요소의 최적화뿐만 아니라 그 상호작용의 최적화에 새로운 문제를 발생시켰습니다. 텍스트 기반의 감독 학습(SFT)과 강화 학습(RL) 등 전통적인 최적화 방법은 여전히 기반으로 남아있지만, 자연어 피드백의 증가는 특히 미분 불가능한 시스템의 최적화에서 원하는 새로운 접근 방식을 이끌어 냈습니다. 본 논문은 복합 AI 시스템의 최적화에 대한 최근 진전을 체계적으로 조사하고, 수치적 및 언어 기반의 기술을 포함하는 것을 제공합니다. 복합 AI 시스템의 최적화 개념을 형식화하고, 기존의 방법을 수차례로 분류하여 이 빠르게 변화하는 분야의 개방 연구 과제와 미래 방향을 밝혀줍니다. 조사한 논문의 목록은 https://github.com/MiuLab/AISysOpt-Survey 에서 공개되어 있습니다.",
      "upvotes": 1,
      "discussionId": "684ae1dedbd21a9cc27b0ee2",
      "ai_summary": "Recent advancements in optimizing compound AI systems highlight challenges in integrating various components, with an emphasis on natural language feedback methods for non-differentiable systems.",
      "ai_keywords": [
        "large language models",
        "AI systems",
        "compound AI systems",
        "supervised fine-tuning",
        "reinforcement learning",
        "natural language feedback",
        "non-differentiable systems"
      ]
    },
    "publishedAt": "2025-06-09T17:04:14.000Z",
    "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and\n  Future Directions",
    "summary": "Recent advancements in large language models (LLMs) and AI systems have led\nto a paradigm shift in the design and optimization of complex AI workflows. By\nintegrating multiple components, compound AI systems have become increasingly\nadept at performing sophisticated tasks. However, as these systems grow in\ncomplexity, new challenges arise in optimizing not only individual components\nbut also their interactions. While traditional optimization methods such as\nsupervised fine-tuning (SFT) and reinforcement learning (RL) remain\nfoundational, the rise of natural language feedback introduces promising new\napproaches, especially for optimizing non-differentiable systems. This paper\nprovides a systematic review of recent progress in optimizing compound AI\nsystems, encompassing both numerical and language-based techniques. We\nformalize the notion of compound AI system optimization, classify existing\nmethods along several key dimensions, and highlight open research challenges\nand future directions in this rapidly evolving field. A list of surveyed papers\nis publicly available at https://github.com/MiuLab/AISysOpt-Survey.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08234.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6615752da15c52fa7ab3e2f7",
      "avatarUrl": "/avatars/37e72cfb829a42630d229080ad8d60f3.svg",
      "fullname": "Lee",
      "name": "Speeeed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06950",
      "authors": [
        {
          "_id": "684ae1fbdbd21a9cc27b0f51",
          "name": "Do Xuan Long",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f52",
          "name": "Duy Dinh",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f53",
          "name": "Ngoc-Hai Nguyen",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f54",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f55",
          "name": "Nancy F. Chen",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f56",
          "name": "Shafiq Joty",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f57",
          "name": "Min-Yen Kan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-07T23:19:27.000Z",
      "submittedOnDailyAt": "2025-06-13T03:14:57.042Z",
      "title": "어떤 요소가 좋은 자연어 프로ンプト를 만들 때 필요합니다?",
      "submittedOnDailyBy": {
        "_id": "63a9a0d13453852ef53c0b37",
        "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
        "isPro": false,
        "fullname": "Do Xuan Long",
        "user": "dxlong2000",
        "type": "user"
      },
      "summary": "LLM는 인간과 거의 같은 방식으로 커뮤니케이션에 대한 발전을 가집니다. プロンプト는 결정적인 요소로 등장하게 되었습니다. 그러나 자연어 プロンプト를 정량화하기 위한 구체적인 개념적인 공통 인식은 제한되어 있습니다. 2022年至2025년의 先進的な NLP と AI コンファレンス에서 150 개 이상의 プロンプト 관련 논문과 ブログ를 조사하여 이 문제를 해결하려고 합니다. 21 개의 プロパティ를 6 개의 ディメンション으로 분류한 プロパティ와 인간 중심적인 프레임워크를 제안します. 그리고 현재의 연구가 LLM에 어떤 영향을 미칠지 조사하고, 모델이나 태스크의 균형이 불균형で 있으며, 연구의 결함이 크다는 것을 밝혀냅니다. 또한, 고품질의 자연어 プロンプト의 プロパティ 간의 상관관계를 분석하고, プロンプト의 추천을 얻습니다. 그리고, 다 プロパティ プロンプト의 강화를 실험적으로 조사하고, 단일 プロパティ의 강화가 가장 큰 영향을 미칠 것을 발견합니다. 마지막으로, プロパティ를 확장한 プロンプト에 대한 훈련 チューニング에서 논리론 모델이 개선되는 것을 발견합니다. 우리의 발견은 プロパティ 중심적인 プロンプト 평가와 최적화의 기초를 마련하고, 인간과 AI의 커뮤니케이션 사이에 공차를 채우고, 새로운 プロンプト 연구의 방향을 개척합니다.",
      "upvotes": 1,
      "discussionId": "684ae1fbdbd21a9cc27b0f58",
      "ai_summary": "A framework for evaluating and optimizing natural language prompts in large language models is proposed, revealing correlations between prompt properties and their impact on reasoning tasks.",
      "ai_keywords": [
        "large language models",
        "prompting",
        "meta-analysis",
        "property-centric framework",
        "instruction-tuning",
        "reasoning tasks"
      ]
    },
    "publishedAt": "2025-06-07T19:19:27.000Z",
    "title": "What Makes a Good Natural Language Prompt?",
    "summary": "As large language models (LLMs) have progressed towards more human-like and\nhuman--AI communications have become prevalent, prompting has emerged as a\ndecisive component. However, there is limited conceptual consensus on what\nexactly quantifies natural language prompts. We attempt to address this\nquestion by conducting a meta-analysis surveying more than 150\nprompting-related papers from leading NLP and AI conferences from 2022 to 2025\nand blogs. We propose a property- and human-centric framework for evaluating\nprompt quality, encompassing 21 properties categorized into six dimensions. We\nthen examine how existing studies assess their impact on LLMs, revealing their\nimbalanced support across models and tasks, and substantial research gaps.\nFurther, we analyze correlations among properties in high-quality natural\nlanguage prompts, deriving prompting recommendations. We then empirically\nexplore multi-property prompt enhancements in reasoning tasks, observing that\nsingle-property enhancements often have the greatest impact. Finally, we\ndiscover that instruction-tuning on property-enhanced prompts can result in\nbetter reasoning models. Our findings establish a foundation for\nproperty-centric prompt evaluation and optimization, bridging the gaps between\nhuman--AI communication and opening new prompting research directions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06950.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a9a0d13453852ef53c0b37",
      "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
      "fullname": "Do Xuan Long",
      "name": "dxlong2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06561",
      "authors": [
        {
          "_id": "684b88113b733ba333686fc7",
          "name": "Ho Yin 'Sam' Ng",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fc8",
          "name": "Ting-Yao Hsu",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fc9",
          "name": "Aashish Anantha Ramakrishnan",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fca",
          "name": "Branislav Kveton",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcb",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcc",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:49.692Z",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcd",
          "name": "Dongwon Lee",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fce",
          "name": "Tong Yu",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcf",
          "name": "Sungchul Kim",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fd0",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fd1",
          "name": "Ting-Hao 'Kenneth' Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T22:16:16.000Z",
      "submittedOnDailyAt": "2025-06-13T00:38:33.715Z",
      "title": "LaMP-Cap: 다모달 피지유 프로파일에 의한 개인화 피지유 캡처 생성",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "그래프의 타이틀은 독자가 그래프의 주요 메시지를 이해하고 기억하기 위해 중요합니다. 많은 모델이 이러한 타이틀을 생성하기 위해 개발되었습니다. 이 모델들은 저자가 더 좋은 질의의 타이틀을 쉽게 생성할 수 있도록 돕습니다. 그러나 저자는 그래프의 작성 스타일과 분야의 스타일에 맞춰 표준적인 AI 타이틀을 편집하는 것이 필요합니다. 이로 인해 개인화의 필요성이 명확히 됩니다. 언어 모델의 개인화(LaMP)의 발전에도 불구하고, 이러한 기술은 거의 언어만 설정에 집중하고, 입력과 프로파일이 다형적인 경우를 거의 조사하지 않습니다. 본 논문에서는 LaMP-Cap을 소개합니다. LaMP-Cap은 다양한 그래프 프로파일 파일을 사용하여 개인화된 그래프 타이틀 생성 데이터 세트를 제공합니다. LaMP-Cap은 타겟 그래프에 대해 필요한 입력을 제공하면서, 같은 문서에서 다른 3개의 그래프의 이미지, 타이틀, 그래프를 언급한 문장을 포함하는 프로파일을 제공하여 кон텍스트를 특징화합니다. 4개의 LLM과의 실험은 프로파일 정보를 사용하여 타이틀을 생성하고, 원 저자가 쓴 것과 가까운 것을 보여주었습니다. 제거 시험은 프로파일의 이미지는 그래프를 언급한 문장보다 더 도움이 되고, 언어만 사용하는 것보다 다양한 프로파일을 사용하는 것이 더 좋습니다.",
      "upvotes": 1,
      "discussionId": "684b88123b733ba333686fd2",
      "ai_summary": "LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.",
      "ai_keywords": [
        "LaMP-Cap",
        "personalized figure caption generation",
        "multimodal figures",
        "figure profiles",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-06-06T18:16:16.000Z",
    "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles",
    "summary": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06561.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05982",
      "authors": [
        {
          "_id": "684b86913b733ba333686fb8",
          "name": "Zonglin Wu",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fb9",
          "name": "Yule Xue",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fba",
          "name": "Xin Wei",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fbb",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T11:02:01.000Z",
      "submittedOnDailyAt": "2025-06-13T00:33:34.648Z",
      "title": "MCA-Bench: VLM 기반 공격에 대한 CAPTCHA의 역력 평가를 위한 다양성 평가기준",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "자동화 공격 방법들이 급격히 발전하면서, CAPTCHA는 악의적인 봇을 막기 위한 중요한 방어 기구입니다. 그러나 현재의 CAPTCHA 스키밍은 다양한 모달리티를 포함하고, 정적인 비틀린 글자, 잡음으로 오염된 이미지, 교차 클릭, 슬라이딩 퍼즐, 로직 기반의 질문 등 광범위한 범위를 넓혀왔지만, 커뮤니티는 아직 하나의 통일된, 큰 규모의, 다양한 모달리티를 포함하는 벤치마크를 가지고 있지 않습니다. 이 한계점을 해결하기 위해, MCA-Bench라는, 다양한 CAPTCHA 타입을 포함하여 통합된 평가 프로토콜을 통해 재현 가능한 벤치마크 시스템에 대해 소개합니다. 공유된 Vision 언어 모델의 백트랙을 활용하여, 각 CAPTCHA 카테고리에 전문화된 크래싱 아그언트를 미세 조정하고, 일관된, 모달리티 교차 평가가 가능하게 합니다. 광범위한 실험을 통해, MCA-Bench는 현대의 CAPTCHA 디자인의 취약성 스펙트럼을 효과적으로 바ン드워드로 전환하고, 중요한 점으로, 도전의 복잡성, 상호작용의 깊이, 모델의 해결 가능성과의 상호관계를 처음으로 정량적인 분석을 제공합니다. 이러한 발견에 기반하여, 세 가지의 행동 가능한 설계 원칙을 제안하고, 주요 열린 문제를 특정하고, 시스템적인 CAPTCHA의 강화, 공정한 벤치마크, 그리고 더 넓은 커뮤니티의 협력의 기초를 구축합니다. 데이터셋과 코드는 온라인에서 제공됩니다.",
      "upvotes": 1,
      "discussionId": "684b86923b733ba333686fbc",
      "ai_summary": "MCA-Bench provides a unified benchmark for evaluating CAPTCHA security using a shared vision-language model and attackers specialized for each type of CAPTCHA.",
      "ai_keywords": [
        "vision-language model",
        "CAPTCHA",
        "benchmark",
        "evaluation protocol",
        "cracking agents",
        "vulnerability spectrum",
        "challenge complexity",
        "interaction depth",
        "model solvability"
      ]
    },
    "publishedAt": "2025-06-06T07:02:01.000Z",
    "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks",
    "summary": "As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking\nsuite that integrates heterogeneous CAPTCHA types into a single evaluation\nprotocol. Leveraging a shared vision-language model backbone, we fine-tune\nspecialized cracking agents for each CAPTCHA category, enabling consistent,\ncross-modal assessments. Extensive experiments reveal that MCA-Bench\neffectively maps the vulnerability spectrum of modern CAPTCHA designs under\nvaried attack settings, and crucially offers the first quantitative analysis of\nhow challenge complexity, interaction depth, and model solvability interrelate.\nBased on these findings, we propose three actionable design principles and\nidentify key open challenges, laying the groundwork for systematic CAPTCHA\nhardening, fair benchmarking, and broader community collaboration. Datasets and\ncode are available online.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05982.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10378",
      "authors": [
        {
          "_id": "684bb08e3b733ba3336870bf",
          "name": "Jikai Jin",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c0",
          "name": "Vasilis Syrgkanis",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c1",
          "name": "Sham Kakade",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c2",
          "name": "Hanlin Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T06:07:42.000Z",
      "submittedOnDailyAt": "2025-06-13T03:36:14.455Z",
      "title": "디스카바리닝 휘리루카셜 레텐트 캐파빌리티 포맷 모듈에 의한 카우슬레이 레지듀레이션 학습",
      "submittedOnDailyBy": {
        "_id": "624054bcc2c17da6a63eb539",
        "avatarUrl": "/avatars/bf52dc0683b4100733f8696a97696d0e.svg",
        "isPro": false,
        "fullname": "hlzhang109",
        "user": "hlzhang109",
        "type": "user"
      },
      "summary": "신뢰성 있는 언어 모델 능력 평가는 행동 가능한 통찰을 제공하기 위해 개발 과정에서 중요한 요소입니다. 그러나 이 분야에서는 복잡한 혼잡 효과와 여러 모델의 확장 훈련에 따른 계산 비용 등, 엄격한 원인적 평가에 대한 큰 방법학적 문제들이 존재합니다. 이러한 문제를 해결하기 위해, 우리는 관찰된 벤치마크 성능을 몇 개의 잠재적인 능력 계수의 선형 변환으로 모델화하는因果 표현 학습 프레임워크를 제안하고 있습니다. 중요한 점은 이러한 잠재적인 계수는 기본 모델을 공통의 혼잡제로 적절하게 제어한 후, 서로 원인적 관계가 있는 것을 인식할 수 있습니다. 이 접근 방식을 Open LLM Leaderboard에서 6개의 벤치마크를 선택하여 평가한 1500개 이상의 모델을 포함하는 세부적인 데이터 세트에 적용한 결과, 관찰된 성능의 변화를 신뢰성 있는 3개의 노드의 선형 원인적 구조로 설명할 수 있음을 밝혀줍니다. 이 원인적 구조의进一步的 해석은 단순한 숫자 순위보다 과학적인 통찰을 제공하며, 특히 일반적인 문제 해결 능력부터 시작하여 지시에 따른 숙련을 통해 수학적인 이유론 능력까지 명확한 원인적 방향을 보여주고 있습니다. 우리의 결과를 통해, 평가 중 기본 모델의 변화를 신중하게 제어하는 중요성을 강조하고, 잠재적인 모델 능력 사이의 잠재적인 원인적 관계를 정확히 밝혀내는 데 필수적인 단계임을 설명하고 있습니다.",
      "upvotes": 0,
      "discussionId": "684bb08e3b733ba3336870c3",
      "ai_summary": "A causal representation learning framework identifies a concise causal structure to explain performance variations in language models across benchmarks by controlling for base model variations.",
      "ai_keywords": [
        "causal representation learning",
        "latent capability factors",
        "causal interrelated",
        "base model confounder",
        "Open LLM Leaderboard",
        "linear causal structure",
        "general problem-solving capabilities",
        "instruction-following proficiency",
        "mathematical reasoning ability"
      ]
    },
    "publishedAt": "2025-06-12T02:07:42.000Z",
    "title": "Discovering Hierarchical Latent Capabilities of Language Models via\n  Causal Representation Learning",
    "summary": "Faithful evaluation of language model capabilities is crucial for deriving\nactionable insights that can inform model development. However, rigorous causal\nevaluations in this domain face significant methodological challenges,\nincluding complex confounding effects and prohibitive computational costs\nassociated with extensive retraining. To tackle these challenges, we propose a\ncausal representation learning framework wherein observed benchmark performance\nis modeled as a linear transformation of a few latent capability factors.\nCrucially, these latent factors are identified as causally interrelated after\nappropriately controlling for the base model as a common confounder. Applying\nthis approach to a comprehensive dataset encompassing over 1500 models\nevaluated across six benchmarks from the Open LLM Leaderboard, we identify a\nconcise three-node linear causal structure that reliably explains the observed\nperformance variations. Further interpretation of this causal structure\nprovides substantial scientific insights beyond simple numerical rankings:\nspecifically, we reveal a clear causal direction starting from general\nproblem-solving capabilities, advancing through instruction-following\nproficiency, and culminating in mathematical reasoning ability. Our results\nunderscore the essential role of carefully controlling base model variations\nduring evaluation, a step critical to accurately uncovering the underlying\ncausal relationships among latent model capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "624054bcc2c17da6a63eb539",
      "avatarUrl": "/avatars/bf52dc0683b4100733f8696a97696d0e.svg",
      "fullname": "hlzhang109",
      "name": "hlzhang109",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08862",
      "authors": [
        {
          "_id": "684ae26ddbd21a9cc27b117f",
          "name": "Zike Wu",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1180",
          "name": "Qi Yan",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1181",
          "name": "Xuanyu Yi",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1182",
          "name": "Lele Wang",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1183",
          "name": "Renjie Liao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T14:52:36.000Z",
      "submittedOnDailyAt": "2025-06-13T07:33:07.278Z",
      "title": "StreamSplat: 向けて、未校正のビデオストリームから의 온라인 다이나믹 3D 재구성\n\n(注意: 原始文本中的“向けて”在韩语翻译中没有直接对应的表达，因此这里直接翻译为“向けて”，但通常在韩语中可能会省略或根据上下文调整。)",
      "submittedOnDailyBy": {
        "_id": "648058ff8c6a3b8f11f77893",
        "avatarUrl": "/avatars/043c832314a8d6713af90d7c255fc2f2.svg",
        "isPro": false,
        "fullname": "Wu Zike",
        "user": "Nickwzk",
        "type": "user"
      },
      "summary": "동적 3D 공간에서 무 도전 텍스스의 비디오 스트리밍으로부터 시간 단위의 실시간 재구성은 여러 실제 세계의 애플리케이션에 중요합니다. 그러나 기존의 방법은 3가지 주요한 문제점을 동시에 해결하는 것이 어렵습니다: 1) 시간 단위로 무 도전 텍스스의 입력을 처리하기, 2) 동적인 공간의 진화를 정확하게 모델화하기, 3) 장기간의 안정성과 계산 효율성을 유지하기. 이에 대해 우리는 임의의 길이의 무 도전 텍스스의 비디오 스트리밍을 동적 3D 가우스 확산(3DGS) 표현으로 변환하기 위한 최초의 완전한 전파 프레임워크를 도입합니다. 이것은 시간적으로 가까운 관측으로부터 공간의 움직임을 복원할 수 있습니다. 우리는 두 가지 핵심적인 기술적인 혁신을 제안합니다:静的 인코더에서 3DGS 위치 예측의 확률적 샘플링 구조, 그리고 동적 디코더에서 바이 德里克斯希アル의 변형 필드, 이 둘은 강인하고 효율적인 동적 모델링에 적합합니다.静的 및 동적인 벤치마크의 확장된 실험에 따라, StreamSplat은 이전의 작업과 비교하여 재구성 품질과 동적인 공간 모델링에서 일관된 우수함을 보여주고, 특히 임의의 길이의 비디오 스트리밍의 온라인 재구성을 지원합니다. 코드와 모델은 https://github.com/nickwzk/StreamSplat에 제공됩니다.",
      "upvotes": 0,
      "discussionId": "684ae26ddbd21a9cc27b1184",
      "ai_summary": "StreamSplat, a fully feed-forward framework, addresses real-time 3D scene reconstruction from uncalibrated video with accurate dynamics and long-term stability.",
      "ai_keywords": [
        "3D Gaussian Splatting",
        "3DGS",
        "probabilistic sampling mechanism",
        "bidirectional deformation field",
        "online reconstruction"
      ]
    },
    "publishedAt": "2025-06-10T10:52:36.000Z",
    "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated\n  Video Streams",
    "summary": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams\nis crucial for numerous real-world applications. However, existing methods\nstruggle to jointly address three key challenges: 1) processing uncalibrated\ninputs in real time, 2) accurately modeling dynamic scene evolution, and 3)\nmaintaining long-term stability and computational efficiency. To this end, we\nintroduce StreamSplat, the first fully feed-forward framework that transforms\nuncalibrated video streams of arbitrary length into dynamic 3D Gaussian\nSplatting (3DGS) representations in an online manner, capable of recovering\nscene dynamics from temporally local observations. We propose two key technical\ninnovations: a probabilistic sampling mechanism in the static encoder for 3DGS\nposition prediction, and a bidirectional deformation field in the dynamic\ndecoder that enables robust and efficient dynamic modeling. Extensive\nexperiments on static and dynamic benchmarks demonstrate that StreamSplat\nconsistently outperforms prior works in both reconstruction quality and dynamic\nscene modeling, while uniquely supporting online reconstruction of arbitrarily\nlong video streams. Code and models are available at\nhttps://github.com/nickwzk/StreamSplat.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08862.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648058ff8c6a3b8f11f77893",
      "avatarUrl": "/avatars/043c832314a8d6713af90d7c255fc2f2.svg",
      "fullname": "Wu Zike",
      "name": "Nickwzk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]