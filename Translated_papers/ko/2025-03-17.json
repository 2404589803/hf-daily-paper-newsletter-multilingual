[
  {
    "paper": {
      "id": "2503.07677",
      "authors": [
        {
          "_id": "67d2ca0767366130cccad93d",
          "user": {
            "_id": "63973ee44e7b4959dc98028f",
            "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
            "isPro": false,
            "fullname": "Kwanyoung",
            "user": "kwanyoung",
            "type": "user"
          },
          "name": "Kwanyoung Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:58:03.528Z",
          "hidden": false
        },
        {
          "_id": "67d2ca0767366130cccad93e",
          "user": {
            "_id": "668377232d89090894bea7b4",
            "avatarUrl": "/avatars/1a74a08d645a352db4a460036b9fb6db.svg",
            "isPro": false,
            "fullname": "byeongsu sim",
            "user": "byeongsus",
            "type": "user"
          },
          "name": "Byeongsu Sim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:47:11.835Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T07:23:19.000Z",
      "submittedOnDailyAt": "2025-03-17T00:44:05.364Z",
      "title": "PLADIS:稀疏성을 활용하여 제한된 범위로 주목을 집중하여 계산 시간을 줄이는 데에 대한 발전 모델을 추천합니다.",
      "submittedOnDailyBy": {
        "_id": "63973ee44e7b4959dc98028f",
        "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
        "isPro": false,
        "fullname": "Kwanyoung",
        "user": "kwanyoung",
        "type": "user"
      },
      "summary": "Diffusion 모델은 Classifier-Free Guidance (CFG) 등 지침 방법의 사용으로 고품질의 조건付き 샘플을 생성하여 놀라운 결과를 보입니다. 그러나 현재의 방법들은 추가적인 훈련이나 뉴럴 함수 평가 (NFEs)를 필요로 하여 지침 디스틸 모델과의 대응이 어려워질 때도 많습니다. 또한 이러한 방법들은 목표의 층을 특정하기 위해 휴리스틱 접근 방식을 의존하고 있습니다. 본 연구에서는 새로운 효과적인 방법을 제안하고 있습니다. 이 방법은 PLADIS로 불리며, 사전 훈련된 모델 (U-Net/Transformer)을 희소 어텐션을 사용하여 강화합니다. 특히, 추론 중의 교차 어텐션 층에서 소프트맥스와 그 희소 버전을 사용하여 쿼리-키 관계를 추상화하고 추가적인 훈련이나 NFEs가 필요하지 않도록 합니다. 희소 어텐션의 노이즈 견디성을 활용하여 PLADIS는 텍스트로부터 이미지 생성 모델의 잠재적 가능성을 해방하고, 지금까지의 노력을 기울인 상황에서도 새로운 효과를 발휘합니다. PLADIS는 지침 방법과 그 디스틸 모델과의 잘 대응하며, 광범위한 실험에서 문맥의 일치성과 인간의 취향에 대한 뚜렷한 개선을 보여주고, 높은 효율성과 일반적인 적용 가능성에 대한 제공을 합니다.",
      "upvotes": 58,
      "discussionId": "67d2ca0b67366130cccada34",
      "ai_keywords": [
        "diffusion models",
        "Classifier-Free Guidance (CFG)",
        "neural function evaluations (NFEs)",
        "guidance-distilled models",
        "PLADIS",
        "pre-trained models (U-Net/Transformer)",
        "sparse attention",
        "query-key correlations",
        "softmax",
        "cross-attention layer",
        "noise robustness",
        "text-to-image diffusion models",
        "text alignment",
        "human preference"
      ]
    },
    "publishedAt": "2025-03-10T03:23:19.000Z",
    "title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity",
    "summary": "Diffusion models have shown impressive results in generating high-quality\nconditional samples using guidance techniques such as Classifier-Free Guidance\n(CFG). However, existing methods often require additional training or neural\nfunction evaluations (NFEs), making them incompatible with guidance-distilled\nmodels. Also, they rely on heuristic approaches that need identifying target\nlayers. In this work, we propose a novel and efficient method, termed PLADIS,\nwhich boosts pre-trained models (U-Net/Transformer) by leveraging sparse\nattention. Specifically, we extrapolate query-key correlations using softmax\nand its sparse counterpart in the cross-attention layer during inference,\nwithout requiring extra training or NFEs. By leveraging the noise robustness of\nsparse attention, our PLADIS unleashes the latent potential of text-to-image\ndiffusion models, enabling them to excel in areas where they once struggled\nwith newfound effectiveness. It integrates seamlessly with guidance techniques,\nincluding guidance-distilled models. Extensive experiments show notable\nimprovements in text alignment and human preference, offering a highly\nefficient and universally applicable solution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07677.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63973ee44e7b4959dc98028f",
      "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
      "fullname": "Kwanyoung",
      "name": "kwanyoung",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11647",
      "authors": [
        {
          "_id": "67d785fa473d4edd330edee1",
          "user": {
            "_id": "6530bf50f145530101ec03a2",
            "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg",
            "isPro": false,
            "fullname": "Jianhong Bai",
            "user": "jianhongbai",
            "type": "user"
          },
          "name": "Jianhong Bai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:47:22.245Z",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee2",
          "user": {
            "_id": "63401c89f81b9d101361f712",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665146415483-63401c89f81b9d101361f712.png",
            "isPro": false,
            "fullname": "Richard",
            "user": "menghanxia",
            "type": "user"
          },
          "name": "Menghan Xia",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:47:41.792Z",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee3",
          "name": "Xiao Fu",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee4",
          "user": {
            "_id": "60e272ca6c78a8c122b12127",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
            "isPro": false,
            "fullname": "Xintao Wang",
            "user": "Xintao",
            "type": "user"
          },
          "name": "Xintao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:47:51.145Z",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee5",
          "user": {
            "_id": "6672dd6d239ba86f129c5384",
            "avatarUrl": "/avatars/6209afb551995b12d5e0d4d95e495694.svg",
            "isPro": false,
            "fullname": "Lianrui Mu",
            "user": "Mu437",
            "type": "user"
          },
          "name": "Lianrui Mu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:47:58.483Z",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee6",
          "name": "Jinwen Cao",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee7",
          "user": {
            "_id": "6458b8d0990172cd1d703715",
            "avatarUrl": "/avatars/55f0695e3cb9933c3903fde5a8f740d5.svg",
            "isPro": false,
            "fullname": "Zuozhu Liu",
            "user": "Zuozhu",
            "type": "user"
          },
          "name": "Zuozhu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:48:17.243Z",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee8",
          "user": {
            "_id": "66c46129d67297a9b93e03c5",
            "avatarUrl": "/avatars/cffd8b07fa3655e240efc8e81f99d97d.svg",
            "isPro": false,
            "fullname": "Haoji Hu",
            "user": "garland1979",
            "type": "user"
          },
          "name": "Haoji Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:48:23.846Z",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee9",
          "user": {
            "_id": "641790e2f1e86908935d82a0",
            "avatarUrl": "/avatars/ced7a137c6344c74b7ac0d5c84833fc8.svg",
            "isPro": false,
            "fullname": "Xiang Bai",
            "user": "baixianger",
            "type": "user"
          },
          "name": "Xiang Bai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:48:29.804Z",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edeea",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edeeb",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:48:49.559Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/RzQL-WqDDCxBy_j4rJuMl.mp4"
      ],
      "publishedAt": "2025-03-14T17:59:31.000Z",
      "submittedOnDailyAt": "2025-03-17T00:50:10.251Z",
      "title": "ReCamMaster: 카메라 제어 제네레이터 렌더링（단일 비디오에서）",
      "submittedOnDailyBy": {
        "_id": "6530bf50f145530101ec03a2",
        "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg",
        "isPro": false,
        "fullname": "Jianhong Bai",
        "user": "jianhongbai",
        "type": "user"
      },
      "summary": "카메라 제어는 문서 또는 이미지에 기반한 비디오 생성 태스크에서 활발하게 연구되고 있습니다. 그러나 주어진 비디오의 카메라 트레이지리를 변경하는 것은 비디오 제작 분야에서 중요한 분야이지만 조사가 부족합니다. 이는 여러 프레임의 외관을 유지하면서 동적인 동기를 유지하기 위해 추가적인 제약으로 인해 복잡합니다. 이에 대해 우리는 ReCamMaster라는 카메라 제어를 포함하는 생성 비디오 재그리기 프레임워크를 제안합니다. 이 방법은 입력 비디오의 동적인 시나리오를 새로운 카메라 트레이지리로 재현합니다. 핵심의 혁신은 간단하고 강력한 비디오 조건 부여 구조를 통해 현재의 연구에서 자주 무시되는 생성 능력을 활용하는 것입니다. 품질 좋은 훈련 데이터의 부족을 극복하기 위해 우리는 Unreal Engine 5를 사용하여 실제 영화 촬영특징에 맞게 조정된 상세한 다 카메라 동기 비디오 데이터 세트를 구축했습니다. 이는 모델이 자연스러운 비디오에 일반화할 수 있게 됩니다. 마지막으로, 여러 입력에 대한 강건성을 향상시키기 위해 설계된 훈련 전략을 도입했습니다. 확장된 실험은 우리의 방법론이 현재의 최전단적인 접근과 강한 baseline에 비해 크게 우위를示している 것을 보여줍니다. 우리의 방법론은 비디오 안정성, 서브 레이프, 그리고 오픈 인플레이에 대한 유망한 응용을 보여줍니다. 프로젝트 페이지: https://jianhongbai.github.io/ReCamMaster/",
      "upvotes": 55,
      "discussionId": "67d785fb473d4edd330edf77",
      "ai_keywords": [
        "ReCamMaster",
        "text-to-video models",
        "video conditioning mechanism",
        "multi-camera synchronized video dataset",
        "Unreal Engine 5",
        "video stabilization",
        "super-resolution",
        "outpainting"
      ]
    },
    "publishedAt": "2025-03-14T13:59:31.000Z",
    "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
    "summary": "Camera control has been actively studied in text or image conditioned video\ngeneration tasks. However, altering camera trajectories of a given video\nremains under-explored, despite its importance in the field of video creation.\nIt is non-trivial due to the extra constraints of maintaining multiple-frame\nappearance and dynamic synchronization. To address this, we present\nReCamMaster, a camera-controlled generative video re-rendering framework that\nreproduces the dynamic scene of an input video at novel camera trajectories.\nThe core innovation lies in harnessing the generative capabilities of\npre-trained text-to-video models through a simple yet powerful video\nconditioning mechanism -- its capability often overlooked in current research.\nTo overcome the scarcity of qualified training data, we construct a\ncomprehensive multi-camera synchronized video dataset using Unreal Engine 5,\nwhich is carefully curated to follow real-world filming characteristics,\ncovering diverse scenes and camera movements. It helps the model generalize to\nin-the-wild videos. Lastly, we further improve the robustness to diverse inputs\nthrough a meticulously designed training strategy. Extensive experiments tell\nthat our method substantially outperforms existing state-of-the-art approaches\nand strong baselines. Our method also finds promising applications in video\nstabilization, super-resolution, and outpainting. Project page:\nhttps://jianhongbai.github.io/ReCamMaster/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/RzQL-WqDDCxBy_j4rJuMl.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11647.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6530bf50f145530101ec03a2",
      "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg",
      "fullname": "Jianhong Bai",
      "name": "jianhongbai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11646",
      "authors": [
        {
          "_id": "67d78c194fd0e3fa3a082f8d",
          "user": {
            "_id": "634e4120038b5879133552f5",
            "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
            "isPro": true,
            "fullname": "Siyuan",
            "user": "SiyuanH",
            "type": "user"
          },
          "name": "Siyuan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:44:21.620Z",
          "hidden": false
        },
        {
          "_id": "67d78c194fd0e3fa3a082f8e",
          "user": {
            "_id": "670f827bb94a3734d270f707",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/D6qCPBMJAUgozfG7YTwky.png",
            "isPro": false,
            "fullname": "Yue Liao",
            "user": "morninghaze",
            "type": "user"
          },
          "name": "Yue Liao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:49:27.927Z",
          "hidden": false
        },
        {
          "_id": "67d78c194fd0e3fa3a082f8f",
          "user": {
            "_id": "620326e962b2b0e46e79971b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620326e962b2b0e46e79971b/1FVPRpsWng5q3An4qbuYQ.jpeg",
            "isPro": false,
            "fullname": "Siyuan Feng",
            "user": "Eralien",
            "type": "user"
          },
          "name": "Siyuan Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:44:19.837Z",
          "hidden": false
        },
        {
          "_id": "67d78c194fd0e3fa3a082f90",
          "name": "Shu Jiang",
          "hidden": false
        },
        {
          "_id": "67d78c194fd0e3fa3a082f91",
          "name": "Si Liu",
          "hidden": false
        },
        {
          "_id": "67d78c194fd0e3fa3a082f92",
          "user": {
            "_id": "65c04e9c27a5fdca81abcbd9",
            "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
            "isPro": false,
            "fullname": "Hongsheng LI",
            "user": "hsli-cuhk",
            "type": "user"
          },
          "name": "Hongsheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:49:51.674Z",
          "hidden": false
        },
        {
          "_id": "67d78c194fd0e3fa3a082f93",
          "user": {
            "_id": "67739bfa64e8b7438ae68eb4",
            "avatarUrl": "/avatars/15193bfbce487b2de4ce8c86bd18885a.svg",
            "isPro": false,
            "fullname": "Maoqing Yao",
            "user": "AutobotZero",
            "type": "user"
          },
          "name": "Maoqing Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:49:59.798Z",
          "hidden": false
        },
        {
          "_id": "67d78c194fd0e3fa3a082f94",
          "user": {
            "_id": "646ec9b135f55eb49e405faa",
            "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
            "isPro": false,
            "fullname": "Guanghui Ren",
            "user": "sundrops",
            "type": "user"
          },
          "name": "Guanghui Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:50:05.432Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/2Cb7g14KRbbgg6yotocsP.mp4"
      ],
      "publishedAt": "2025-03-14T17:59:07.000Z",
      "submittedOnDailyAt": "2025-03-17T01:30:24.394Z",
      "title": "적대적인 데이터 수집: 인간과의 협업에 의한 효율적이고 강건한 로봇의 복제 학습을 통한 패버티징",
      "submittedOnDailyBy": {
        "_id": "634e4120038b5879133552f5",
        "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
        "isPro": true,
        "fullname": "Siyuan",
        "user": "SiyuanH",
        "type": "user"
      },
      "summary": "데이터의 효율화를 추구하고, 질이 양보다 중요하다는 인식은 실제적인 데이터 수집에 따른 고가비용을 가정하며, 로봇 조작의 기초로 자리잡았습니다. 단일의 지시의 정보밀도를 최대화하여, 대규모 데이터셋의 의존성을 크게 줄이고, 작업의 성능을 향상시킬 수 있는 방법을 제안합니다. 이를 위해, 적대적인 데이터 수집(Adversarial Data Collection, ADC)를 소개합니다. 이것은 시간적으로, 히마ン과 환경 사이의 양방향 상호작용을 통해, 히마ン-in-the-Loop(HiL) 프레임워크로 로봇의 데이터 획득을 새롭게 정의합니다. 기존의 파이프라인과 달리, ADC는 동적인 지시를 기록하는 것이 아니라, 협력적인 퍼바티브 패러다임(cooperative paradigm)을 채택합니다: 하나의 에피소드 내에서, 적대적인 운영자는 대상물의 상태, 환경 조건, 언어 명령을 동적으로 변경하며, 텔레오픈너는 이러한 진화적인 문제를 해결하기 위해 적절한 행동을 조정합니다. 이 과정에서, 다양한 실패 회복 행동, 구성적인 태스크의 변화, 그리고 환경의 적대적인 변화를 최소한의 지시로 압축합니다. 실험에 따르면, ADC에서 학습된 모델은未见过的 태스크 지시에 대한 구성적인 일반화, 시각적인 적대적인 변화에 대한 강건성, 그리고 발생되는 오류의 회복 능력이 향상됩니다. 특히, ADC에서 수집된 지시의 20%만 사용된 모델은 전체 데이터셋을 사용했던 기존 접근법과 비교하여, 상당한 성능을 보입니다. 이러한 발전은 데이터 중심의 학습 패러다임과 실용적인 로봇의 도입 사이에 빈틈을 메고, 전략적인 데이터 수집과 후처리는 scalable, 실용적인 로봇의 학습에서 중요하다는 것을 보여주고 있습니다. 또한, 적대적인 데이터 수집을 통해 실제적인 동작 태스크의 대규모 데이터셋을 구축하고 있습니다. 이 벤치마크는 로봇의 가볍은 학습의 발전을 촉진하기 위해 오픈소스로 공개됩니다.",
      "upvotes": 28,
      "discussionId": "67d78c1b4fd0e3fa3a08301c",
      "projectPage": " https://sites.google.com/view/adc-robot",
      "ai_keywords": [
        "Adversarial Data Collection",
        "Human-in-the-Loop (HiL)",
        "real-time, bidirectional human-environment interactions",
        "collaborative perturbation paradigm",
        "adversarial operator",
        "tele-operator",
        "compositional generalization",
        "perceptual perturbations",
        "error recovery capabilities",
        "ADC-trained models",
        "ADC-Robotics dataset",
        "robotic imitation learning"
      ]
    },
    "publishedAt": "2025-03-14T13:59:07.000Z",
    "title": "Adversarial Data Collection: Human-Collaborative Perturbations for\n  Efficient and Robust Robotic Imitation Learning",
    "summary": "The pursuit of data efficiency, where quality outweighs quantity, has emerged\nas a cornerstone in robotic manipulation, especially given the high costs\nassociated with real-world data collection. We propose that maximizing the\ninformational density of individual demonstrations can dramatically reduce\nreliance on large-scale datasets while improving task performance. To this end,\nwe introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework\nthat redefines robotic data acquisition through real-time, bidirectional\nhuman-environment interactions. Unlike conventional pipelines that passively\nrecord static demonstrations, ADC adopts a collaborative perturbation paradigm:\nduring a single episode, an adversarial operator dynamically alters object\nstates, environmental conditions, and linguistic commands, while the\ntele-operator adaptively adjusts actions to overcome these evolving challenges.\nThis process compresses diverse failure-recovery behaviors, compositional task\nvariations, and environmental perturbations into minimal demonstrations. Our\nexperiments demonstrate that ADC-trained models achieve superior compositional\ngeneralization to unseen task instructions, enhanced robustness to perceptual\nperturbations, and emergent error recovery capabilities. Strikingly, models\ntrained with merely 20% of the demonstration volume collected through ADC\nsignificantly outperform traditional approaches using full datasets. These\nadvances bridge the gap between data-centric learning paradigms and practical\nrobotic deployment, demonstrating that strategic data acquisition, not merely\npost-hoc processing, is critical for scalable, real-world robot learning.\nAdditionally, we are curating a large-scale ADC-Robotics dataset comprising\nreal-world manipulation tasks with adversarial perturbations. This benchmark\nwill be open-sourced to facilitate advancements in robotic imitation learning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/2Cb7g14KRbbgg6yotocsP.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11646.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4120038b5879133552f5",
      "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
      "fullname": "Siyuan",
      "name": "SiyuanH",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11224",
      "authors": [
        {
          "_id": "67d788b6ba098a0651e1e235",
          "user": {
            "_id": "663f07d029be04778ba97871",
            "avatarUrl": "/avatars/fb7c9d4a2c537d918a3267e7cbc03f04.svg",
            "isPro": false,
            "fullname": "Xingtai Lv",
            "user": "XingtaiHF",
            "type": "user"
          },
          "name": "Xingtai Lv",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:44:34.410Z",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e236",
          "user": {
            "_id": "679ce8c048ebd7903d76a832",
            "avatarUrl": "/avatars/5f3fecaacfee6e2d5a72dd19fe87055a.svg",
            "isPro": false,
            "fullname": "Youbang Sun",
            "user": "Youbang",
            "type": "user"
          },
          "name": "Youbang Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:50:17.568Z",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e237",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:44:26.057Z",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e238",
          "name": "Shang Qu",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e239",
          "user": {
            "_id": "647ffddeb82adfa7cc1a10d9",
            "avatarUrl": "/avatars/26aa168d6b2068298ebb16584aa52b6c.svg",
            "isPro": false,
            "fullname": "zhu",
            "user": "xuekai",
            "type": "user"
          },
          "name": "Xuekai Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:50:38.118Z",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e23a",
          "user": {
            "_id": "672c2d7816766a76a747b7b5",
            "avatarUrl": "/avatars/12c7b26d2b81721ccac3a5c71e32a1a1.svg",
            "isPro": false,
            "fullname": "Yuchen Fan",
            "user": "yuchenFan",
            "type": "user"
          },
          "name": "Yuchen Fan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:50:54.445Z",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e23b",
          "name": "Yi Wu",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e23c",
          "user": {
            "_id": "6445fa2ffc22e309d78bef3e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fa2ffc22e309d78bef3e/FQaINLd0PjgY9EnK_APRk.jpeg",
            "isPro": false,
            "fullname": "Messi Hua",
            "user": "Messi-Hua",
            "type": "user"
          },
          "name": "Ermo Hua",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:44:30.639Z",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e23d",
          "user": {
            "_id": "667e577139b49eba118d569f",
            "avatarUrl": "/avatars/1a26dd96b4b352b8968561750ecae9a7.svg",
            "isPro": false,
            "fullname": "Xinwei Long",
            "user": "xinwei666",
            "type": "user"
          },
          "name": "Xinwei Long",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:51:02.068Z",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e23e",
          "user": {
            "_id": "677b80e31ad30ab2c798e776",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/X8IFnIK3TDHOGKZCzLTe8.jpeg",
            "isPro": false,
            "fullname": "Ning Ding",
            "user": "BradPitt2025",
            "type": "user"
          },
          "name": "Ning Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:51:08.621Z",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e23f",
          "user": {
            "_id": "669f614b59adf5b56e05bce3",
            "avatarUrl": "/avatars/ffd4189efbceb0e63a03db273065a44b.svg",
            "isPro": false,
            "fullname": "BowenZhou",
            "user": "bowenZhou",
            "type": "user"
          },
          "name": "Bowen Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:51:15.825Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T09:20:31.000Z",
      "submittedOnDailyAt": "2025-03-17T01:26:02.931Z",
      "title": "기술의 효과성과 효율성에 대한 스테이트스페이스 모델의 개요",
      "submittedOnDailyBy": {
        "_id": "6445fa2ffc22e309d78bef3e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fa2ffc22e309d78bef3e/FQaINLd0PjgY9EnK_APRk.jpeg",
        "isPro": false,
        "fullname": "Messi Hua",
        "user": "Messi-Hua",
        "type": "user"
      },
      "summary": "ステートスペースモデル（SSMs）는, 적절한 변환기 기반 모델의 유망한 대체로 되고 있으며, 주목을 더 받고 있습니다. 변환기와 비교하여, SSMs은 순서적인 데이터나 긴 컨텍스트를 포함하는 태스크에서 뛰어난 성능을示し, 상대적인 성능과 관련없는 효율 향상을 보여주고 있습니다. 본 조사에서는, SSMs의 이론적 원인, 수학적 공식화, 기존 모델 클래스과의 비교, 다양한 애플리케이션에 대한 일관된 체계적인 개요를 제공합니다. SSM 시리즈를 원형 SSM, 구조화된 SSM의 S4, 선택적 SSM의 Manbaara로 3개의 주요 부문으로 나눌 때, 각 부문에 대해 상세한 소개를 진행합니다. 기술적인 측면을 중시하며, SSMs의 효율성과 효율성에 대한 다양한 중요한 기술에 주목합니다. 이 논문은, SSMs의 이론적 기초를 연구자들에게 소개하고자 합니다.",
      "upvotes": 18,
      "discussionId": "67d788b7ba098a0651e1e2a4",
      "ai_keywords": [
        "State Space Models (SSMs)",
        "transformer-based models",
        "sequential data",
        "theoretical motivations",
        "mathematical formulations",
        "comparison",
        "model classes",
        "original SSM",
        "structured SSM",
        "S4",
        "selective SSM",
        "Mamba",
        "effectiveness",
        "efficiency"
      ]
    },
    "publishedAt": "2025-03-14T05:20:31.000Z",
    "title": "Technologies on Effectiveness and Efficiency: A Survey of State Spaces\n  Models",
    "summary": "State Space Models (SSMs) have emerged as a promising alternative to the\npopular transformer-based models and have been increasingly gaining attention.\nCompared to transformers, SSMs excel at tasks with sequential data or longer\ncontexts, demonstrating comparable performances with significant efficiency\ngains. In this survey, we provide a coherent and systematic overview for SSMs,\nincluding their theoretical motivations, mathematical formulations, comparison\nwith existing model classes, and various applications. We divide the SSM series\ninto three main sections, providing a detailed introduction to the original\nSSM, the structured SSM represented by S4, and the selective SSM typified by\nMamba. We put an emphasis on technicality, and highlight the various key\ntechniques introduced to address the effectiveness and efficiency of SSMs. We\nhope this manuscript serves as an introduction for researchers to explore the\ntheoretical foundations of SSMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11224.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6445fa2ffc22e309d78bef3e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fa2ffc22e309d78bef3e/FQaINLd0PjgY9EnK_APRk.jpeg",
      "fullname": "Messi Hua",
      "name": "Messi-Hua",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11069",
      "authors": [
        {
          "_id": "67d785458678eaf139e3c594",
          "user": {
            "_id": "654dbac9938fbf1e696be8aa",
            "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
            "isPro": false,
            "fullname": "Chaoyun Zhang",
            "user": "vyokky",
            "type": "user"
          },
          "name": "Chaoyun Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:51:30.401Z",
          "hidden": false
        },
        {
          "_id": "67d785458678eaf139e3c595",
          "user": {
            "_id": "62c6df026a092eda1f1ab6e5",
            "avatarUrl": "/avatars/d58fff1a157b189ce2617889ef5f6e2f.svg",
            "isPro": false,
            "fullname": "Shilin He",
            "user": "shilhe",
            "type": "user"
          },
          "name": "Shilin He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:51:37.539Z",
          "hidden": false
        },
        {
          "_id": "67d785458678eaf139e3c596",
          "user": {
            "_id": "666933c97bf97e24f7b5266e",
            "avatarUrl": "/avatars/283961b37d463a386b08ad33dacca0f4.svg",
            "isPro": false,
            "fullname": "Liqun Li",
            "user": "liqul",
            "type": "user"
          },
          "name": "Liqun Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:51:57.886Z",
          "hidden": false
        },
        {
          "_id": "67d785458678eaf139e3c597",
          "user": {
            "_id": "67481846f47628abdd8c4397",
            "avatarUrl": "/avatars/b43f2988ac17bd2bb2369133934ce75d.svg",
            "isPro": false,
            "fullname": "Si Qin",
            "user": "SiQin88",
            "type": "user"
          },
          "name": "Si Qin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:52:05.644Z",
          "hidden": false
        },
        {
          "_id": "67d785458678eaf139e3c598",
          "name": "Yu Kang",
          "hidden": false
        },
        {
          "_id": "67d785458678eaf139e3c599",
          "user": {
            "_id": "652fc9f39bc50a6c0e435224",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fc9f39bc50a6c0e435224/70OBVDHHBsxG2giJ-E3_1.jpeg",
            "isPro": false,
            "fullname": "Lin Qingwei",
            "user": "Eliblo1969",
            "type": "user"
          },
          "name": "Qingwei Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:52:17.826Z",
          "hidden": false
        },
        {
          "_id": "67d785458678eaf139e3c59a",
          "user": {
            "_id": "66473d2c7abe6ad66e81a3dd",
            "avatarUrl": "/avatars/82f40244806c06ffeaa1c4265e9725ea.svg",
            "isPro": false,
            "fullname": "ZHANGDONGMEI",
            "user": "ZDM6426",
            "type": "user"
          },
          "name": "Dongmei Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:52:31.623Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T04:26:21.000Z",
      "submittedOnDailyAt": "2025-03-17T00:43:33.225Z",
      "title": "API 아그언츠와 GUI 아그언츠：분리와 통합",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 단순한 문장 생성을 넘어서, 자연어 명령을 직접 행동으로 변환하는 소프트웨어 에이전트에 힘을 쏘고 있습니다. API 기반의 LLM 에이전트는 강력한 자동화 능력과 프로그래밍 엔드 포인트와의无缝 통합으로, 처음으로 프리미엄 수준에 도달했습니다. 그러나 최근의 다모달 LLM 연구의 발전에 따라, GUI 기반의 LLM 에이전트가 인간처럼 그래픽적 사용자 인터페이스와 상호작용하는 방식으로 발전했습니다. 이러한 두 가지 패러다임은 LLM Drove Task 자동화에 대한 허용을 공유하지만, 구조적 복잡성, 개발 작업 흐름, 사용자 인터페이스 모델 등에서는 상당한 차이점이 있습니다.\n\n본 논문은 API 기반과 GUI 기반의 LLM 에이전트의 첫 번째 상세한 비교 연구를 제공하며, 그 차이점과 잠재적 수렴점을 체계적으로 분석합니다. 이러한 핵심 차원을 검토하고, 하이브리드 접근 방식을 활용할 수 있는 시나리오를 특징적으로 설명합니다. 결정 기준을 명확히 제시하고, 실질적인 사용 사례를 예시하여, 실무자와 연구자들에게 이러한 패러다임의 선택, 조합 또는 이동을 가이드하려고 합니다. 최종적으로, LLM 기반의 자동화의 발전은 API 도브러시와 GUI 도브러시의 에이전트의 경계를 섞으며, 광범위한 현실적인 애플리케이션에서 유연성과 적응성을 위해 더 광범위한 해결책을 제시합니다.",
      "upvotes": 16,
      "discussionId": "67d785468678eaf139e3c5ee"
    },
    "publishedAt": "2025-03-14T00:26:21.000Z",
    "title": "API Agents vs. GUI Agents: Divergence and Convergence",
    "summary": "Large language models (LLMs) have evolved beyond simple text generation to\npower software agents that directly translate natural language commands into\ntangible actions. While API-based LLM agents initially rose to prominence for\ntheir robust automation capabilities and seamless integration with programmatic\nendpoints, recent progress in multimodal LLM research has enabled GUI-based LLM\nagents that interact with graphical user interfaces in a human-like manner.\nAlthough these two paradigms share the goal of enabling LLM-driven task\nautomation, they diverge significantly in architectural complexity, development\nworkflows, and user interaction models.\n  This paper presents the first comprehensive comparative study of API-based\nand GUI-based LLM agents, systematically analyzing their divergence and\npotential convergence. We examine key dimensions and highlight scenarios in\nwhich hybrid approaches can harness their complementary strengths. By proposing\nclear decision criteria and illustrating practical use cases, we aim to guide\npractitioners and researchers in selecting, combining, or transitioning between\nthese paradigms. Ultimately, we indicate that continuing innovations in\nLLM-based automation are poised to blur the lines between API- and GUI-driven\nagents, paving the way for more flexible, adaptive solutions in a wide range of\nreal-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11069.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11514",
      "authors": [
        {
          "_id": "67d778325121a10e6fc650b3",
          "user": {
            "_id": "668f440894dfc0ed1a7006ed",
            "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
            "isPro": false,
            "fullname": "Pengxin Guo",
            "user": "gpx333",
            "type": "user"
          },
          "name": "Pengxin Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:44:42.855Z",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650b4",
          "user": {
            "_id": "65a28d30c0e637bd9cbddc15",
            "avatarUrl": "/avatars/50be3e38617a51f7f8c22fa219a4d10a.svg",
            "isPro": false,
            "fullname": "Runxi Wang",
            "user": "Rx-Wang",
            "type": "user"
          },
          "name": "Runxi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:44:39.377Z",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650b5",
          "user": {
            "_id": "66712ff09c609c2484ce4aa0",
            "avatarUrl": "/avatars/717b96ddef8a4c19ce07ea1fd9e9fd66.svg",
            "isPro": false,
            "fullname": "Shuang Zeng",
            "user": "stevezs",
            "type": "user"
          },
          "name": "Shuang Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:53:09.089Z",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650b6",
          "user": {
            "_id": "6479ea5effe1b559f5408453",
            "avatarUrl": "/avatars/6077dcc62fbd41dac92ee33b3133ceec.svg",
            "isPro": false,
            "fullname": "Zhu",
            "user": "Jinjing08",
            "type": "user"
          },
          "name": "Jinjing Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:53:22.627Z",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650b7",
          "user": {
            "_id": "66ff619fe48de0216cd43531",
            "avatarUrl": "/avatars/e4642e02b6475cfbd677c6e28640b5b0.svg",
            "isPro": false,
            "fullname": "HaoningJiang",
            "user": "haoning666",
            "type": "user"
          },
          "name": "Haoning Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:44:36.769Z",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650b8",
          "user": {
            "_id": "656f698c80ff527c44e3c33b",
            "avatarUrl": "/avatars/19ea552ed0bb36260ab0f6e41421f9b3.svg",
            "isPro": false,
            "fullname": "Yanran Wang",
            "user": "yanranw1",
            "type": "user"
          },
          "name": "Yanran Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:53:29.620Z",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650b9",
          "user": {
            "_id": "66c7fb4ce2c92fe5b132f314",
            "avatarUrl": "/avatars/22d915fa339a70803c5c748255250256.svg",
            "isPro": false,
            "fullname": "Yuyin Zhou",
            "user": "RitaCoding",
            "type": "user"
          },
          "name": "Yuyin Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:53:35.758Z",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650ba",
          "user": {
            "_id": "653d6970885338b011d283d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653d6970885338b011d283d8/FNViRXsMdrhrjOurBVBSf.jpeg",
            "isPro": false,
            "fullname": "Feifei Wang",
            "user": "feifeiwang",
            "type": "user"
          },
          "name": "Feifei Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:53:43.569Z",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650bb",
          "name": "Hui Xiong",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650bc",
          "user": {
            "_id": "663058bc2653ec94f4a6235f",
            "avatarUrl": "/avatars/f55b8c3c8100d6b6d65ba61abc4fb014.svg",
            "isPro": false,
            "fullname": "Liangqiong Qu",
            "user": "Liangqiong-QU",
            "type": "user"
          },
          "name": "Liangqiong Qu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:53:51.160Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T08:08:44.000Z",
      "submittedOnDailyAt": "2025-03-17T00:38:48.278Z",
      "title": "탐색 파데레드 러닝의 취약성: 제노미아 인버시온 공격의 깊은 연습이 필요합니다.",
      "submittedOnDailyBy": {
        "_id": "668f440894dfc0ed1a7006ed",
        "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
        "isPro": false,
        "fullname": "Pengxin Guo",
        "user": "gpx333",
        "type": "user"
      },
      "summary": "フェデレイド・ラーニング（FL）는 개인 정보를 보호하여 원의 데이터를 공유하지 않는 기여적인 협업 모델 훈련 패러다임으로 발전하여 왔습니다. 그러나 최근의 연구에 따르면, 공유된 경사 정보로 개인 정보가 누설하고, 경사 역전 공격（GIA）으로 공격받을 수 있다는 사실을 밝혀졌습니다. GIA의 방법들은 많은 종류가 있지만, 이들 방법에 대한 세부적인 분석, 평가 및 요약은 아직 부족합니다. 또한, 다양한 논문이 FL에서 기존의 개인 정보 공격을 요약하지만,GIA의 효과성과 관련된 제한 요인에 대한 광범위한 실험은 적고, 이 분야의 연구는 아직 부족합니다. 이를 채워기 위해, GIA의 체계적인 검토를 수행하고, 기존의 방법을 최적화 기반의 GIA（OP-GIA）、생성기반의 GIA（GEN-GIA）、분석기반의 GIA（ANA-GIA）의 3가지 카테고리로 분류했습니다. 다음으로, FL에서 3가지의 GIA를 상세히 분석하고, 성능, 실용성, 잠재적인 위험에 대한 영향을 제공했습니다. 우리의 발견은 OP-GIA는 성능이 만족되지만 가장 실용적인 공격 설정으로 나타났으며, GEN-GIA는 많은 의존관계를 가지고 있으며, ANA-GIA는 쉽게 감지될 수 있으므로, 둘 다 실용성이 낮다는 것을 보여주었습니다. 마지막으로, 개인 정보 보호를 우선시하는 FL 프레임워크와 프로토콜 설계에 3단계의 방어 파이프라인을 제공하고, 공격자와 방어자의 입장에서 미래의 연구 방향을 공유했습니다. 우리의 연구는 이러한 공격을 막기 위해 더 강인한 FL 프레임워크의 설계에 도움을 줄 수 있기를 바랍니다.",
      "upvotes": 13,
      "discussionId": "67d778395121a10e6fc652eb",
      "ai_keywords": [
        "Gradient Inversion Attacks (GIA)",
        "optimization-based GIA (OP-GIA)",
        "generation-based GIA (GEN-GIA)",
        "analytics-based GIA (ANA-GIA)"
      ]
    },
    "publishedAt": "2025-03-13T04:08:44.000Z",
    "title": "Exploring the Vulnerabilities of Federated Learning: A Deep Dive into\n  Gradient Inversion Attacks",
    "summary": "Federated Learning (FL) has emerged as a promising privacy-preserving\ncollaborative model training paradigm without sharing raw data. However, recent\nstudies have revealed that private information can still be leaked through\nshared gradient information and attacked by Gradient Inversion Attacks (GIA).\nWhile many GIA methods have been proposed, a detailed analysis, evaluation, and\nsummary of these methods are still lacking. Although various survey papers\nsummarize existing privacy attacks in FL, few studies have conducted extensive\nexperiments to unveil the effectiveness of GIA and their associated limiting\nfactors in this context. To fill this gap, we first undertake a systematic\nreview of GIA and categorize existing methods into three types, i.e.,\noptimization-based GIA (OP-GIA), generation-based GIA\n(GEN-GIA), and analytics-based GIA (ANA-GIA). Then, we comprehensively\nanalyze and evaluate the three types of GIA in FL, providing insights into the\nfactors that influence their performance, practicality, and potential threats.\nOur findings indicate that OP-GIA is the most practical attack setting despite\nits unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA\nis easily detectable, making them both impractical. Finally, we offer a\nthree-stage defense pipeline to users when designing FL frameworks and\nprotocols for better privacy protection and share some future research\ndirections from the perspectives of attackers and defenders that we believe\nshould be pursued. We hope that our study can help researchers design more\nrobust FL frameworks to defend against these attacks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11514.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668f440894dfc0ed1a7006ed",
      "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
      "fullname": "Pengxin Guo",
      "name": "gpx333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10970",
      "authors": [
        {
          "_id": "67d771335e9c4135a570f57f",
          "user": {
            "_id": "6350fc5ba8822aadf571304f",
            "avatarUrl": "/avatars/19686add3cbdaef5772b913152333f9b.svg",
            "isPro": false,
            "fullname": "gasvn",
            "user": "shgao",
            "type": "user"
          },
          "name": "Shanghua Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:44:49.587Z",
          "hidden": false
        },
        {
          "_id": "67d771335e9c4135a570f580",
          "name": "Richard Zhu",
          "hidden": false
        },
        {
          "_id": "67d771335e9c4135a570f581",
          "name": "Zhenglun Kong",
          "hidden": false
        },
        {
          "_id": "67d771335e9c4135a570f582",
          "user": {
            "_id": "643b2ce2c5f633a7fa82d507",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643b2ce2c5f633a7fa82d507/RFXzG5tiRqVYdF-bWbNl-.png",
            "isPro": false,
            "fullname": "Ayush",
            "user": "ayushnoori",
            "type": "user"
          },
          "name": "Ayush Noori",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:54:34.672Z",
          "hidden": false
        },
        {
          "_id": "67d771335e9c4135a570f583",
          "user": {
            "_id": "660aef84362a1d713aea88ec",
            "avatarUrl": "/avatars/7a16c54e1ee43d5366501d12e8087a7e.svg",
            "isPro": false,
            "fullname": "Xiaorui Su",
            "user": "Blair1213",
            "type": "user"
          },
          "name": "Xiaorui Su",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:54:42.342Z",
          "hidden": false
        },
        {
          "_id": "67d771335e9c4135a570f584",
          "name": "Curtis Ginder",
          "hidden": false
        },
        {
          "_id": "67d771335e9c4135a570f585",
          "name": "Theodoros Tsiligkaridis",
          "hidden": false
        },
        {
          "_id": "67d771335e9c4135a570f586",
          "user": {
            "_id": "636826f95bb06007ea0e911e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667770136112-636826f95bb06007ea0e911e.jpeg",
            "isPro": false,
            "fullname": "Marinka Zitnik",
            "user": "marinkaz",
            "type": "user"
          },
          "name": "Marinka Zitnik",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:55:18.525Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T00:28:15.000Z",
      "submittedOnDailyAt": "2025-03-17T02:04:58.876Z",
      "title": "TxAgent: 우주의 치료의 이유를 기능하는 AI 에이전트",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "精准치료제는 다양한 적응형 모델을 필요로 하며, 개별화된 치료 추천을 생성합니다. 우리는 TxAgent를 소개합니다. TxAgent는 211개의 도구를 사용하는 211개 도구 박스를 활용하여, 다단계 추론과 시간적 생물학적 의료 지식 검색을 활용하여, 약물 상호작용, 차이점, 환자별 치료 전략을 분석하는 AI 에이전트입니다. TxAgent는 약물 간의 분자적, 약학적, 임상 수준의 상호작용을 평가하고, 환자의 병과 병용 약물에 기반한 차이점을 특정하고, 환자의 개인적 특성에 맞는 치료 전략을 제작합니다. 여러 생물학적 의료 소스에서 증거를 검색하고 합성하여, 약물과 환자 상태 간의 상호작용을 평가하고, 반복적인 추론을 통해 치료 추천을 보정합니다. 치료제의 태스크 오브젝트를 기반으로 도구를 선택하고, 구조화된 함수 호출을 수행하여, 임상적인 추론과 크로스 소스 검증이 필요한 치료제의 태스크를 해결하는 것을 목표로 합니다. ToolUniverse는 신뢰할 수 있는 소스에서 211개의 도구를 수집하여, 1939년 이후 모든 US FDA 승인 약물과 Open Targets에서 검증된 임상적인 인젝트를 포함합니다. TxAgent는 5개의 새로운 벤치마크 (DrugPC, BrandPC, GenericPC, TreatmentPC, DescriptionPC)을 통해, 3,168개의 약물 추론 태스크와 456개의 환자별 치료 시나리오를 초과하여, 先진적인 LLMs, 도구 사용 모델, 이유 에이전트를 초과합니다. TxAgent는 개방 엔드 프로브 약물 추론 태스크에서 92.1%의 정확도를 달성하며, GPT-4o를 초과하고, DeepSeek-R1 (671B)을 초과합니다. TxAgent는 약물의 이름 변이와 설명을 일반화하고, 다단계 추론, 시간적 지식 게이팅, 도구 보조 결정을 통합하여, 치료 추천이 기존의 임상 가이드라인과 실세계의 증거에 맞으며, 부작용의 위험을 감소시키고, 치료 결정을 개선하는 것을 보장합니다.",
      "upvotes": 8,
      "discussionId": "67d771345e9c4135a570f5d0",
      "ai_keywords": [
        "AI agent",
        "multi-step reasoning",
        "biomedical knowledge retrieval",
        "drug interactions",
        "contraindications",
        "patient-specific treatment strategies",
        "molecular levels",
        "pharmacokinetic levels",
        "clinical levels",
        "patient comorbidities",
        "concurrent medications",
        "ToolUniverse",
        "FDA-approved drugs",
        "Open Targets",
        "DrugPC",
        "BrandPC",
        "GenericPC",
        "TreatmentPC",
        "DescriptionPC",
        "drug reasoning tasks",
        "personalized treatment scenarios",
        "multi-step inference",
        "knowledge grounding",
        "tool-assisted decision-making",
        "clinical guidelines",
        "real-world evidence",
        "adverse events",
        "therapeutic decision-making"
      ]
    },
    "publishedAt": "2025-03-13T20:28:15.000Z",
    "title": "TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools",
    "summary": "Precision therapeutics require multimodal adaptive models that generate\npersonalized treatment recommendations. We introduce TxAgent, an AI agent that\nleverages multi-step reasoning and real-time biomedical knowledge retrieval\nacross a toolbox of 211 tools to analyze drug interactions, contraindications,\nand patient-specific treatment strategies. TxAgent evaluates how drugs interact\nat molecular, pharmacokinetic, and clinical levels, identifies\ncontraindications based on patient comorbidities and concurrent medications,\nand tailors treatment strategies to individual patient characteristics. It\nretrieves and synthesizes evidence from multiple biomedical sources, assesses\ninteractions between drugs and patient conditions, and refines treatment\nrecommendations through iterative reasoning. It selects tools based on task\nobjectives and executes structured function calls to solve therapeutic tasks\nthat require clinical reasoning and cross-source validation. The ToolUniverse\nconsolidates 211 tools from trusted sources, including all US FDA-approved\ndrugs since 1939 and validated clinical insights from Open Targets. TxAgent\noutperforms leading LLMs, tool-use models, and reasoning agents across five new\nbenchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC,\ncovering 3,168 drug reasoning tasks and 456 personalized treatment scenarios.\nIt achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing\nGPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning.\nTxAgent generalizes across drug name variants and descriptions. By integrating\nmulti-step inference, real-time knowledge grounding, and tool-assisted\ndecision-making, TxAgent ensures that treatment recommendations align with\nestablished clinical guidelines and real-world evidence, reducing the risk of\nadverse events and improving therapeutic decision-making.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10970.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6382
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10781",
      "authors": [
        {
          "_id": "67d78ff6f789a7b68993ab6b",
          "user": {
            "_id": "62f38b19261bc5fb2e06652c",
            "avatarUrl": "/avatars/0a7f7d63e1096f5d52bcb3be8e236c87.svg",
            "isPro": false,
            "fullname": "Evangelos Kazakos",
            "user": "ekazakos",
            "type": "user"
          },
          "name": "Evangelos Kazakos",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:44:14.132Z",
          "hidden": false
        },
        {
          "_id": "67d78ff6f789a7b68993ab6c",
          "name": "Cordelia Schmid",
          "hidden": false
        },
        {
          "_id": "67d78ff6f789a7b68993ab6d",
          "name": "Josef Sivic",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T18:21:07.000Z",
      "submittedOnDailyAt": "2025-03-17T07:19:07.091Z",
      "title": "대규모의 기초 학습에 의한 실체화 비디오 캡처 생성",
      "submittedOnDailyBy": {
        "_id": "62f38b19261bc5fb2e06652c",
        "avatarUrl": "/avatars/0a7f7d63e1096f5d52bcb3be8e236c87.svg",
        "isPro": false,
        "fullname": "Evangelos Kazakos",
        "user": "ekazakos",
        "type": "user"
      },
      "summary": "우리는 비디오에서 캡션과 객체 고정을 위한 새로운 접근 방식을 제안합니다. 캡션에 있는 객체는 비디오의 시간적으로 밀집한 경계 박스로 고정됩니다. 우리는 다음과 같은 기여를 소개합니다. 첫째, 우리는 개별 프레임에서 경계 박스로 고정된 캡션을 모으고 시간적으로 밀집하고 일관된 경계 박스 어노테이션을 구축하는 대형 자동 어노테이션 방법을 제시합니다. 이 접근 방식은 HowTo100M 데이터셋에 적용되어 HowToGround1M(1M)의 대형 사전 훈련 데이터셋을 구축합니다. 또한 Grounded Video Caption Generation 모델, GROVE,를 소개하고 HowToGround1M에 사전 훈련합니다. 둘째, 우리는 3500개의 비디오로 구성된 새로운 데이터셋인 iGround을 소개합니다. 이 데이터셋에는 수동으로 어노테이된 캡션과 밀집한 공간-시간적으로 고정된 경계 박스가 포함됩니다. 이는 이 도전적인 문제를 평가하고 또한 작은 규모이지만 높은 품질의 데이터를 사용하여 모델을 미세 조정할 수 있게 합니다. 셋째, 우리는 제안된 iGround 데이터셋과 VidSTG, ActivityNet-Entities 데이터셋에 대한 여러 기준 모델과 비교하여 우리의 접근 방식이 최신 결과를 달성한다는 것을 보여주고 있습니다. 우리는 사전 훈련을 통해 HowToGround1M 데이터셋을 사용하는 중요성을 강조하고, 수동으로 어노테이된 iGround 데이터셋에 미세 조정한 후 모델의 주요 기술적 기여를 검증합니다.",
      "upvotes": 8,
      "discussionId": "67d78ffaf789a7b68993ac8f",
      "projectPage": "https://ekazakos.github.io/grounded_video_caption_generation/",
      "githubRepo": "https://github.com/ekazakos/grove",
      "ai_keywords": [
        "temporally dense bounding boxes",
        "automatic annotation",
        "pre-training dataset",
        "Grounded Video Caption Generation",
        "spatio-temporally grounded bounding boxes",
        "fine-tuning",
        "state-of-the-art results",
        "VidSTG",
        "ActivityNet-Entities",
        "ablations"
      ]
    },
    "publishedAt": "2025-03-13T14:21:07.000Z",
    "title": "Large-scale Pre-training for Grounded Video Caption Generation",
    "summary": "We propose a novel approach for captioning and object grounding in video,\nwhere the objects in the caption are grounded in the video via temporally dense\nbounding boxes. We introduce the following contributions. First, we present a\nlarge-scale automatic annotation method that aggregates captions grounded with\nbounding boxes across individual frames into temporally dense and consistent\nbounding box annotations. We apply this approach on the HowTo100M dataset to\nconstruct a large-scale pre-training dataset, named HowToGround1M. We also\nintroduce a Grounded Video Caption Generation model, dubbed GROVE, and\npre-train the model on HowToGround1M. Second, we introduce a new dataset,\ncalled iGround, of 3500 videos with manually annotated captions and dense\nspatio-temporally grounded bounding boxes. This allows us to measure progress\non this challenging problem, as well as to fine-tune our model on this\nsmall-scale but high-quality data. Third, we demonstrate that our approach\nachieves state-of-the-art results on the proposed iGround dataset compared to a\nnumber of baselines, as well as on the VidSTG and ActivityNet-Entities\ndatasets. We perform extensive ablations that demonstrate the importance of\npre-training using our automatically annotated HowToGround1M dataset followed\nby fine-tuning on the manually annotated iGround dataset and validate the key\ntechnical contributions of our model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10781.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f38b19261bc5fb2e06652c",
      "avatarUrl": "/avatars/0a7f7d63e1096f5d52bcb3be8e236c87.svg",
      "fullname": "Evangelos Kazakos",
      "name": "ekazakos",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10772",
      "authors": [
        {
          "_id": "67d78ce0b4d0fefa68385d7f",
          "user": {
            "_id": "661c9059bcd78151e5c06ea1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661c9059bcd78151e5c06ea1/27bfNo1LZeZQ77vWuAa10.png",
            "isPro": false,
            "fullname": "Ju He",
            "user": "turkeyju",
            "type": "user"
          },
          "name": "Ju He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:44:17.453Z",
          "hidden": false
        },
        {
          "_id": "67d78ce0b4d0fefa68385d80",
          "user": {
            "_id": "677b60e17279b5c57354108b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677b60e17279b5c57354108b/YOwDhVf9DkeRjOCOLErb6.png",
            "isPro": false,
            "fullname": "QihangYu",
            "user": "QihangYu",
            "type": "user"
          },
          "name": "Qihang Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:55:33.453Z",
          "hidden": false
        },
        {
          "_id": "67d78ce0b4d0fefa68385d81",
          "user": {
            "_id": "639f1e519f1f2baab2f00d22",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg",
            "isPro": true,
            "fullname": "Qihao Liu",
            "user": "QHL067",
            "type": "user"
          },
          "name": "Qihao Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:55:40.463Z",
          "hidden": false
        },
        {
          "_id": "67d78ce0b4d0fefa68385d82",
          "name": "Liang-Chieh Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T18:06:13.000Z",
      "submittedOnDailyAt": "2025-03-17T01:16:42.853Z",
      "title": "FlowTok: 텍스트와 이미지 토큰을 연속적으로 이동시키는 것\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "661c9059bcd78151e5c06ea1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661c9059bcd78151e5c06ea1/27bfNo1LZeZQ77vWuAa10.png",
        "isPro": false,
        "fullname": "Ju He",
        "user": "turkeyju",
        "type": "user"
      },
      "summary": "모델 간의 값을 연결하는 것은 Cross-Model Generation의 핵심 부분이다. 기존의 접근 방식에서는 텍스트 모델을 조건부 신호로 취급하여, 가우시안 노이즈로부터 목표 이미지 모델에 대한 노이즈 처리 과정을 점차적으로 가이드하는 방법을 시도하고 있다. 그러나, 이를 간단한 패러다임으로 시도하는 것이 좋습니다. 이는 텍스트와 이미지 모델 사이에 직접적인 변화를 일으키기 위해 Flow Matching을 사용한다. 이는 서로의 모델을 공통적인 잠재 공간에 투영하는 것이 필요하지만, 이는 각각의 고유 표현에 따라 매우 어려운 일이다. 텍스트는 고차원적으로 세ман틱적이고 1D 토큰으로 표현되어, 이미지는 공간적으로冗長하고 2D 잠재 표현이다. 이에 FlowTok를 도입합니다. FlowTok는 이미지를 단순한 1D 토큰 표현으로 인코딩하여, 텍스트와 이미지 사이에서 자연스럽게 흐름을 일으키는 최소한의 프레임워크입니다. 기존의 방법과 비교하여, 이 설계는 256의 이미지 해상도에서 잠재 공간의 크기를 3.3배 줄이고, 복잡한 조건부 구조나 노이즈 스케줄링의 필요성을 줄입니다. 또한, FlowTok는 동일한 계산식으로 이미지에서 텍스트의 생성을 자연스럽게 확장할 수 있습니다. 이 시스템은 간단한 1D 토큰을 중심으로 하는 스트리밍 아키텍처로, 높은 메모리 효율성과 적은 훈련 리소스로 샘플링 속도를 크게 향상시킵니다. 코드는 https://github.com/bytedance/1d-tokenizer에 제공됩니다.",
      "upvotes": 8,
      "discussionId": "67d78ce1b4d0fefa68385dc8",
      "projectPage": "https://tacju.github.io/projects/flowtok.html",
      "githubRepo": "https://github.com/bytedance/1d-tokenizer/",
      "ai_keywords": [
        "cross-modality generation",
        "flow matching",
        "latent space",
        "denoising process",
        "Gaussian noise",
        "semantic",
        "1D tokens",
        "2D latent embeddings",
        "FlowTok",
        "compact 1D token representation",
        "image-to-text generation",
        "memory-efficient",
        "sampling speeds",
        "state-of-the-art models"
      ]
    },
    "publishedAt": "2025-03-13T14:06:13.000Z",
    "title": "FlowTok: Flowing Seamlessly Across Text and Image Tokens",
    "summary": "Bridging different modalities lies at the heart of cross-modality generation.\nWhile conventional approaches treat the text modality as a conditioning signal\nthat gradually guides the denoising process from Gaussian noise to the target\nimage modality, we explore a much simpler paradigm-directly evolving between\ntext and image modalities through flow matching. This requires projecting both\nmodalities into a shared latent space, which poses a significant challenge due\nto their inherently different representations: text is highly semantic and\nencoded as 1D tokens, whereas images are spatially redundant and represented as\n2D latent embeddings. To address this, we introduce FlowTok, a minimal\nframework that seamlessly flows across text and images by encoding images into\na compact 1D token representation. Compared to prior methods, this design\nreduces the latent space size by 3.3x at an image resolution of 256,\neliminating the need for complex conditioning mechanisms or noise scheduling.\nMoreover, FlowTok naturally extends to image-to-text generation under the same\nformulation. With its streamlined architecture centered around compact 1D\ntokens, FlowTok is highly memory-efficient, requires significantly fewer\ntraining resources, and achieves much faster sampling speeds-all while\ndelivering performance comparable to state-of-the-art models. Code will be\navailable at https://github.com/bytedance/1d-tokenizer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10772.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661c9059bcd78151e5c06ea1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661c9059bcd78151e5c06ea1/27bfNo1LZeZQ77vWuAa10.png",
      "fullname": "Ju He",
      "name": "turkeyju",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10632",
      "authors": [
        {
          "_id": "67d4f1b1643653fd1cea5b5a",
          "user": {
            "_id": "66d5279130d7ea0b28d6d5d2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d5279130d7ea0b28d6d5d2/oGFmux--lARBF4PKSkHAu.png",
            "isPro": false,
            "fullname": "Subhajit Maity",
            "user": "maitysubhajit",
            "type": "user"
          },
          "name": "Subhajit Maity",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-15T03:19:40.103Z",
          "hidden": false
        },
        {
          "_id": "67d4f1b1643653fd1cea5b5b",
          "name": "Killian Hitsman",
          "hidden": false
        },
        {
          "_id": "67d4f1b1643653fd1cea5b5c",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "67d4f1b1643653fd1cea5b5d",
          "user": {
            "_id": "67d58d156db0e6f0c33c0f60",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d58d156db0e6f0c33c0f60/9KiFw0iEMZQHcHnm1k0ED.jpeg",
            "isPro": false,
            "fullname": "Aritra Dutta",
            "user": "aritradutta",
            "type": "user"
          },
          "name": "Aritra Dutta",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-15T14:35:55.373Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:52.000Z",
      "submittedOnDailyAt": "2025-03-17T01:09:07.184Z",
      "title": "コルモゴロフ-アーノルドアテンション：학습 가능한 어텐션은 시각 채널이나 비전 채널에 더 적합한가?",
      "submittedOnDailyBy": {
        "_id": "66d5279130d7ea0b28d6d5d2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d5279130d7ea0b28d6d5d2/oGFmux--lARBF4PKSkHAu.png",
        "isPro": false,
        "fullname": "Subhajit Maity",
        "user": "maitysubhajit",
        "type": "user"
      },
      "summary": "コルモゴロフ-アーノルドネットワーク（KANs）는 학습 가능한 활성함수를 구성하여 데이터로부터 더 복잡한 관계를捉えるための偉大的なイノベーションです。しかし、KANsは一様関数의 符号的表現の探索と継続的学習に有用であることはありますが、複雑な機械学習（ML）タスクでの効果性は疑問の問題です。現在、KANsは深いネットワークアーキテクチャでの多層パーセプトロン（MLPs）を置き換えて構築されています。この論文では、ベースの選択に依存しない一般的な学習可能なコルモゴロフ-アーノルドアテンション（KArAt）をベージョンのViTsに最初に設計します。しかし、それらの訓練の計算量とメモリコストにより、モジュール化したバージョンを提案し、特に学習可能なアテンションとしてフーリエ-KArAtを設計しました。フーリエ-KArAtおよびその変体は、CIFAR-10、CIFAR-100、ImageNet-1Kデータセット上でViTの対照バージョンを超えるものか、比較的性能を示します。これらのアーキテクチャの性能と一般化能力を分析し、損失のランドスケープ、重みの分布、最適化パス、アテンションの可視化、スペクトルの挙動を詳細に見ることで、ベージョンのViTsと比較します。この論文の目的は、パラメータと計算効率的なアテンションを生成することではありませんが、KANsとより先進的なアーキテクチャとの組み合わせを探索するよう、コミュニティに促しています。我々のオープンソースコードと実装詳細は以下のURLから利用できます：https://subhajitmaity.me/KArAt",
      "upvotes": 5,
      "discussionId": "67d4f1b6643653fd1cea5d20",
      "projectPage": "https://subhajitmaity.me/KArAt",
      "githubRepo": "https://github.com/MaitySubhajit/KArAt",
      "ai_keywords": [
        "Kolmogorov-Arnold networks (KANs)",
        "learnable activation functions",
        "multilayer perceptrons (MLPs)",
        "vision Transformers (ViTs)",
        "Kolmogorov-Arnold Attention (KArAt)",
        "Fourier-KArAt",
        "loss landscapes",
        "weight distributions",
        "optimizer path",
        "attention visualization",
        "spectral behavior"
      ]
    },
    "publishedAt": "2025-03-13T13:59:52.000Z",
    "title": "Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision\n  Transformers?",
    "summary": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of\nlearnable activation functions with the potential to capture more complex\nrelationships from data. Although KANs are useful in finding symbolic\nrepresentations and continual learning of one-dimensional functions, their\neffectiveness in diverse machine learning (ML) tasks, such as vision, remains\nquestionable. Presently, KANs are deployed by replacing multilayer perceptrons\n(MLPs) in deep network architectures, including advanced architectures such as\nvision Transformers (ViTs). In this paper, we are the first to design a general\nlearnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate\non any choice of basis. However, the computing and memory costs of training\nthem motivated us to propose a more modular version, and we designed particular\nlearnable attention, called Fourier-KArAt. Fourier-KArAt and its variants\neither outperform their ViT counterparts or show comparable performance on\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures'\nperformance and generalization capacity by analyzing their loss landscapes,\nweight distributions, optimizer path, attention visualization, and spectral\nbehavior, and contrast them with vanilla ViTs. The goal of this paper is not to\nproduce parameter- and compute-efficient attention, but to encourage the\ncommunity to explore KANs in conjunction with more advanced architectures that\nrequire a careful understanding of learnable activations. Our open-source code\nand implementation details are available on: https://subhajitmaity.me/KArAt",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10632.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d5279130d7ea0b28d6d5d2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d5279130d7ea0b28d6d5d2/oGFmux--lARBF4PKSkHAu.png",
      "fullname": "Subhajit Maity",
      "name": "maitysubhajit",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09279",
      "authors": [
        {
          "_id": "67d2bd340860f2d7ff10e3dc",
          "user": {
            "_id": "66a9b3533d417b0baa9220a6",
            "avatarUrl": "/avatars/adc372bd24df1d3bf43258833411e8af.svg",
            "isPro": false,
            "fullname": "Luozheng Qin",
            "user": "Fr0zencr4nE",
            "type": "user"
          },
          "name": "Luozheng Qin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:56:11.189Z",
          "hidden": false
        },
        {
          "_id": "67d2bd340860f2d7ff10e3dd",
          "name": "Zhiyu Tan",
          "hidden": false
        },
        {
          "_id": "67d2bd340860f2d7ff10e3de",
          "user": {
            "_id": "6304d630dae2eb7d084148c7",
            "avatarUrl": "/avatars/7d7a6ca99334bdae3ed1752ff40a8d94.svg",
            "isPro": false,
            "fullname": "mengping yang",
            "user": "Kobeshegu",
            "type": "user"
          },
          "name": "Mengping Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:56:25.200Z",
          "hidden": false
        },
        {
          "_id": "67d2bd340860f2d7ff10e3df",
          "user": {
            "_id": "658ea92268d0b7633176b4ed",
            "avatarUrl": "/avatars/40173c9126dccfe78bc46b12c6ced8c8.svg",
            "isPro": false,
            "fullname": "xiaomeng yang",
            "user": "xiaomengyang",
            "type": "user"
          },
          "name": "Xiaomeng Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:56:33.077Z",
          "hidden": false
        },
        {
          "_id": "67d2bd340860f2d7ff10e3e0",
          "name": "Hao Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T11:25:04.000Z",
      "submittedOnDailyAt": "2025-03-17T00:46:46.368Z",
      "title": "코ッカチェル: 합성 데이터와 인간의 취미에 기반한 훈련을 결합한 상세한 비디오 캡처",
      "submittedOnDailyBy": {
        "_id": "66a9b3533d417b0baa9220a6",
        "avatarUrl": "/avatars/adc372bd24df1d3bf43258833411e8af.svg",
        "isPro": false,
        "fullname": "Luozheng Qin",
        "user": "Fr0zencr4nE",
        "type": "user"
      },
      "summary": "Video Detailed Captioning (VDC)는 시각 언어를 연결하는 중요한 작업으로, 복잡한 비디오 콘텐츠에 대한 세부적인 설명이 가능합니다. 본 논문에서는 현재의 최전단적인 접근을 상세히 벤치마크하고, 두 가지 중요한 한계점을 체계적으로 식별했습니다: 특정 설명면에 대한 편향된 능력과, 인간의 취향과의 불일치. 이러한 결점을 해결하기 위해, 새로운 3단계의 훈련 파이프라인인 Cockatiel를 제안했습니다. 이 파이프라인은 합성 및 인간 조정된 훈련을 결합하여 VDC의 성능을 향상시키는 것을 목표로 합니다. 첫 번째 단계에서는 상세히 설명된 데이터 세트로부터 점수를 얻으며, 특정한 세부적인 비디오-설명의 대응과 인간 취향에 대한 높은 성능을 보여주는 합성 설명을 선택하여, 다른 설명을 무시합니다. 다음으로, 이 조정된 데이터 세트를 사용하여 Cockatiel-13B를 훈련시키고, 이 모델의 강점과 인간 취향을 통합합니다. 마지막으로, Cockatiel-8B는 Cockatiel-13B로부터 진화하여, 사용 편의성을 우선시합니다. 확장된 양적 및 질적 실험은 우리의 방법의 효과를 반영하며, VDCSCORE의 새로운 최전단적인 성능을 설정하고, 인간 취향에 큰 차이를 가져, 인간 평가 결과를 통해 선진적인 옵션을 초월합니다.",
      "upvotes": 4,
      "discussionId": "67d2bd370860f2d7ff10e4da",
      "ai_keywords": [
        "Cockatiel",
        "three-stage training pipeline",
        "synthetic and human-aligned training",
        "fine-grained video-caption alignment",
        "scorer",
        "meticulously annotated dataset",
        "curated dataset",
        "assembled model strengths",
        "human preferences",
        "VDCSCORE",
        "dimension-balanced way",
        "human evaluation results"
      ]
    },
    "publishedAt": "2025-03-12T07:25:04.000Z",
    "title": "Cockatiel: Ensembling Synthetic and Human Preferenced Training for\n  Detailed Video Caption",
    "summary": "Video Detailed Captioning (VDC) is a crucial task for vision-language\nbridging, enabling fine-grained descriptions of complex video content. In this\npaper, we first comprehensively benchmark current state-of-the-art approaches\nand systematically identified two critical limitations: biased capability\ntowards specific captioning aspect and misalignment with human preferences. To\naddress these deficiencies, we propose Cockatiel, a novel three-stage training\npipeline that ensembles synthetic and human-aligned training for improving VDC\nperformance. In the first stage, we derive a scorer from a meticulously\nannotated dataset to select synthetic captions high-performing on certain\nfine-grained video-caption alignment and human-preferred while disregarding\nothers. Then, we train Cockatiel-13B, using this curated dataset to infuse it\nwith assembled model strengths and human preferences. Finally, we further\ndistill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive\nquantitative and qualitative experiments reflect the effectiveness of our\nmethod, as we not only set new state-of-the-art performance on VDCSCORE in a\ndimension-balanced way but also surpass leading alternatives on human\npreference by a large margin as depicted by the human evaluation results.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a9b3533d417b0baa9220a6",
      "avatarUrl": "/avatars/adc372bd24df1d3bf43258833411e8af.svg",
      "fullname": "Luozheng Qin",
      "name": "Fr0zencr4nE",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10696",
      "authors": [
        {
          "_id": "67d7e9bd93b8599318993db2",
          "name": "Yefei He",
          "hidden": false
        },
        {
          "_id": "67d7e9bd93b8599318993db3",
          "name": "Yuanyu He",
          "hidden": false
        },
        {
          "_id": "67d7e9bd93b8599318993db4",
          "name": "Shaoxuan He",
          "hidden": false
        },
        {
          "_id": "67d7e9bd93b8599318993db5",
          "name": "Feng Chen",
          "hidden": false
        },
        {
          "_id": "67d7e9bd93b8599318993db6",
          "name": "Hong Zhou",
          "hidden": false
        },
        {
          "_id": "67d7e9bd93b8599318993db7",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67d7e9bd93b8599318993db8",
          "name": "Bohan Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T05:52:27.000Z",
      "submittedOnDailyAt": "2025-03-17T07:53:13.467Z",
      "title": "인접한 어노데크 회귀 모델링에 의한 효율적인 시각화 생성",
      "submittedOnDailyBy": {
        "_id": "65a88c3d26598b995531fff1",
        "avatarUrl": "/avatars/b1a524857d8572d0405476661b434160.svg",
        "isPro": false,
        "fullname": "Yefei He",
        "user": "yefly",
        "type": "user"
      },
      "summary": "Visual autoregressive models typically adhere to a raster-order ``next-token prediction\" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens compared to those that are distant. In this paper, we propose Neighboring Autoregressive Modeling (NAR), a novel paradigm that formulates autoregressive visual generation as a progressive outpainting procedure, following a near-to-far ``next-neighbor prediction\" mechanism. Starting from an initial token, the remaining tokens are decoded in ascending order of their Manhattan distance from the initial token in the spatial-temporal space, progressively expanding the boundary of the decoded region. To enable parallel prediction of multiple adjacent tokens in the spatial-temporal space, we introduce a set of dimension-oriented decoding heads, each predicting the next token along a mutually orthogonal dimension. During inference, all tokens adjacent to the decoded tokens are processed in parallel, substantially reducing the model forward steps for generation. Experiments on ImageNet256times 256 and UCF101 demonstrate that NAR achieves 2.4times and 8.6times higher throughput respectively, while obtaining superior FID/FVD scores for both image and video generation tasks compared to the PAR-4X approach. When evaluating on text-to-image generation benchmark GenEval, NAR with 0.8B parameters outperforms Chameleon-7B while using merely 0.4 of the training data. Code is available at https://github.com/ThisisBillhe/NAR.",
      "upvotes": 3,
      "discussionId": "67d7e9c093b8599318993e93",
      "projectPage": "https://yuanyu0.github.io/nar/",
      "githubRepo": "https://github.com/ThisisBillhe/NAR",
      "ai_keywords": [
        "Neighboring Autoregressive Modeling (NAR)",
        "raster-order",
        "next-token prediction",
        "spatial and temporal locality",
        "visual tokens",
        "spatially or temporally adjacent tokens",
        "outpainting procedure",
        "near-to-far prediction",
        "Manhattan distance",
        "spatial-temporal space",
        "dimension-oriented decoding heads",
        "FID (Fréchet Inception Distance)",
        "FVD (Fréchet Video Distance)",
        "ImageNet$256\\times 256$",
        "UCF101",
        "text-to-image generation benchmark GenEval",
        "Chameleon-7B",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-03-12T01:52:27.000Z",
    "title": "Neighboring Autoregressive Modeling for Efficient Visual Generation",
    "summary": "Visual autoregressive models typically adhere to a raster-order ``next-token\nprediction\" paradigm, which overlooks the spatial and temporal locality\ninherent in visual content. Specifically, visual tokens exhibit significantly\nstronger correlations with their spatially or temporally adjacent tokens\ncompared to those that are distant. In this paper, we propose Neighboring\nAutoregressive Modeling (NAR), a novel paradigm that formulates autoregressive\nvisual generation as a progressive outpainting procedure, following a\nnear-to-far ``next-neighbor prediction\" mechanism. Starting from an initial\ntoken, the remaining tokens are decoded in ascending order of their Manhattan\ndistance from the initial token in the spatial-temporal space, progressively\nexpanding the boundary of the decoded region. To enable parallel prediction of\nmultiple adjacent tokens in the spatial-temporal space, we introduce a set of\ndimension-oriented decoding heads, each predicting the next token along a\nmutually orthogonal dimension. During inference, all tokens adjacent to the\ndecoded tokens are processed in parallel, substantially reducing the model\nforward steps for generation. Experiments on ImageNet256times 256 and UCF101\ndemonstrate that NAR achieves 2.4times and 8.6times higher throughput\nrespectively, while obtaining superior FID/FVD scores for both image and video\ngeneration tasks compared to the PAR-4X approach. When evaluating on\ntext-to-image generation benchmark GenEval, NAR with 0.8B parameters\noutperforms Chameleon-7B while using merely 0.4 of the training data. Code is\navailable at https://github.com/ThisisBillhe/NAR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10696.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65a88c3d26598b995531fff1",
      "avatarUrl": "/avatars/b1a524857d8572d0405476661b434160.svg",
      "fullname": "Yefei He",
      "name": "yefly",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06674",
      "authors": [
        {
          "_id": "67d6881cf997964e21f90598",
          "user": {
            "_id": "65f7e6856bd4bac5b6a4ecc3",
            "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
            "isPro": false,
            "fullname": "Yihong Luo",
            "user": "Luo-Yihong",
            "type": "user"
          },
          "name": "Yihong Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:44:52.452Z",
          "hidden": false
        },
        {
          "_id": "67d6881cf997964e21f90599",
          "user": {
            "_id": "636a40faa6f948c4f0c62ae5",
            "avatarUrl": "/avatars/30c35b194ba84d6e274df30e91a8cc45.svg",
            "isPro": false,
            "fullname": "Tianyang Hu",
            "user": "whatlegequ",
            "type": "user"
          },
          "name": "Tianyang Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:56:48.283Z",
          "hidden": false
        },
        {
          "_id": "67d6881cf997964e21f9059a",
          "user": {
            "_id": "67b91a3c186bc4f8d83c94cf",
            "avatarUrl": "/avatars/a79538be4b5ed02cd54556458375e4af.svg",
            "isPro": false,
            "fullname": "Jiacheng Sun",
            "user": "JIACSUN96",
            "type": "user"
          },
          "name": "Jiacheng Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:56:55.114Z",
          "hidden": false
        },
        {
          "_id": "67d6881cf997964e21f9059b",
          "name": "Yujun Cai",
          "hidden": false
        },
        {
          "_id": "67d6881cf997964e21f9059c",
          "user": {
            "_id": "636d660056c0762cfd9dc8d5",
            "avatarUrl": "/avatars/50ea2100e00b67ef10adc57556477184.svg",
            "isPro": false,
            "fullname": "jing tang",
            "user": "jingtang",
            "type": "user"
          },
          "name": "Jing Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-17T08:57:09.362Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T15:53:49.000Z",
      "submittedOnDailyAt": "2025-03-17T02:34:51.976Z",
      "title": "학습 스텝이 적은 Diffusion 모델을轨道 분포의 매칭에 의한다.",
      "submittedOnDailyBy": {
        "_id": "65f7e6856bd4bac5b6a4ecc3",
        "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
        "isPro": false,
        "fullname": "Yihong Luo",
        "user": "Luo-Yihong",
        "type": "user"
      },
      "summary": "加速ディフュージョンモデル의 샘플링은 효율적인 AIGC의 도입에 중요합니다. ディスティルメント法是 統計分布을 비교하여 統計分布을 비교하여 統計分布を比較する手法で、複雑なタスク（例えば、文字から画像の生成）に対しては、1ステップで サンプリングを抑制することができますが、複雑なタスクに対しては不足します。少ないステップでの生成は、速さと品質のバランスを良く保つことができますが、現在のアプローチは、統計分布の比較による方法は多ステップの サンプリングに柔軟性がなく、軌道の比較による方法は画像の品質が低くなることが多いという持続的な調整が必要です。この間違いを補うために、私たちは、統計分布の比較と軌道の比較の強みを統合した新しい統合的な結果の統計分布を学習することを提案します。私たちの方法は、データ無しスコア結果の統計分布に対する教師の軌道を調整するオブジェクティブを導入し、さらに、異なるステップの学習ターゲットを分離し、より調整可能な サンプリングを可能にします。このアプローチは、確定的な サンプリングでの上品な画像の生成と柔軟な多ステップの適応を支援し、状況の最先端の性能を実現します。私たちのモデル、TDMは、SDXLやPixArt-alphaなどのバックボーンに対して現在の方法を超え、上品な質と大幅に減少された訓練コストを提供します。特に、私たちの方法は、PixArt-alphaを4ステップのジェネレータに結果の統計分布を統合し、教師に対して実際のユーザーの好みを超えるようにします。これは、1024解像度での1024回のイテレーションと2A800時間（教師の訓練コストの0.01%）で実現されます。また、私たちの提案のTDMは、文字から動画の生成を加速することも可能です。特に、VBenchで4NFEを使用して、教師モデル（CogVideoX-2B）を超えることができ、総合スコアは80.91から81.65に改善します。プロジェクトページ：https://tdm-t2x.github.io/",
      "upvotes": 3,
      "discussionId": "67d6881ef997964e21f90660",
      "projectPage": "https://tdm-t2x.github.io/",
      "githubRepo": "https://github.com/Luo-Yihong/TDM",
      "ai_keywords": [
        "diffusion model sampling",
        "diffusion distillation",
        "distribution matching",
        "trajectory matching",
        "few-step generation",
        "Trajectory Distribution Matching (TDM)",
        "data-free score distillation",
        "sampling-steps-aware objective",
        "deterministic sampling",
        "state-of-the-art performance",
        "SDXL",
        "PixArt-$\\alpha$",
        "TDM",
        "text-to-video diffusion",
        "CogVideoX-2B",
        "VBench",
        "NFE"
      ]
    },
    "publishedAt": "2025-03-09T11:53:49.000Z",
    "title": "Learning Few-Step Diffusion Models by Trajectory Distribution Matching",
    "summary": "Accelerating diffusion model sampling is crucial for efficient AIGC\ndeployment. While diffusion distillation methods -- based on distribution\nmatching and trajectory matching -- reduce sampling to as few as one step, they\nfall short on complex tasks like text-to-image generation. Few-step generation\noffers a better balance between speed and quality, but existing approaches face\na persistent trade-off: distribution matching lacks flexibility for multi-step\nsampling, while trajectory matching often yields suboptimal image quality. To\nbridge this gap, we propose learning few-step diffusion models by Trajectory\nDistribution Matching (TDM), a unified distillation paradigm that combines the\nstrengths of distribution and trajectory matching. Our method introduces a\ndata-free score distillation objective, aligning the student's trajectory with\nthe teacher's at the distribution level. Further, we develop a\nsampling-steps-aware objective that decouples learning targets across different\nsteps, enabling more adjustable sampling. This approach supports both\ndeterministic sampling for superior image quality and flexible multi-step\nadaptation, achieving state-of-the-art performance with remarkable efficiency.\nOur model, TDM, outperforms existing methods on various backbones, such as SDXL\nand PixArt-alpha, delivering superior quality and significantly reduced\ntraining costs. In particular, our method distills PixArt-alpha into a\n4-step generator that outperforms its teacher on real user preference at 1024\nresolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere\n0.01% of the teacher's training cost. In addition, our proposed TDM can be\nextended to accelerate text-to-video diffusion. Notably, TDM can outperform its\nteacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total\nscore from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06674.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65f7e6856bd4bac5b6a4ecc3",
      "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
      "fullname": "Yihong Luo",
      "name": "Luo-Yihong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06553",
      "authors": [
        {
          "_id": "67cfcf664dac6ed12db8b10a",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "67cfcf664dac6ed12db8b10b",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "67cfcf664dac6ed12db8b10c",
          "name": "Zhaopan Xu",
          "hidden": false
        },
        {
          "_id": "67cfcf664dac6ed12db8b10d",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67cfcf664dac6ed12db8b10e",
          "name": "Fanrui Zhang",
          "hidden": false
        },
        {
          "_id": "67cfcf664dac6ed12db8b10f",
          "name": "Zizhen Li",
          "hidden": false
        },
        {
          "_id": "67cfcf664dac6ed12db8b110",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "67cfcf664dac6ed12db8b111",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "67cfcf664dac6ed12db8b112",
          "name": "Baojin Huang",
          "hidden": false
        },
        {
          "_id": "67cfcf664dac6ed12db8b113",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "67cfcf664dac6ed12db8b114",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T10:55:51.000Z",
      "submittedOnDailyAt": "2025-03-17T07:32:38.224Z",
      "title": "ProJudge: 다모달 다학문野의 벤치마크와, MLLM 기반의 프로세스 판단관의 인스톰레이션 튜닝 데이터셋",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "다모뎔어대언어모델(MLLMs)가 과학 문제를 해결하는 과정에서 자주 오류를 보이므로, 그 이유론의 유효성을 평가하는 것은 신뢰도의 확보와 모델의 微妙한 약점을 밝혀내는 데 중요합니다. 인간 평가는 노동과 비용 때문에 복잡하므로, MLLMs를 자동화 프로세스 판단에 사용되어 일반화되어 왔습니다. 그러나 이러한 모델 기반의 판단자의 신뢰도는 불분명합니다. 이에대해, 우리는 MLLM 기반의 프로세스 판단자의 능력을 평가하기 위해 특별히 설계된 첫 번째 상세한 벤치마크인 ProJudgeBench를 소개합니다. ProJudgeBench는 4가지 과학 분야를 확장하고, 다양한 난이도 수준과 모델 내용을 포함하여 2,400개의 테스트 케이스와 50,118개의 단계 수준 레이블을 구성합니다. ProJudgeBench에서 각 단계는 인간 전문가에 의해 정확성, 오류의 종류, 설명에 따라 미세하게 기록되어 있으며, 판단자의 능력, 분류, 진단을 평가할 수 있습니다. ProJudgeBench에서의 평가는 오픈소스 모델과 상업 기관 모델 사이에 큰 성능 차이를 명확히 합니다. 이러한 차이를 보완하기 위해, 우리는 ProJudge-173k와 Dynamic Dual-Phase fine-tuning 전략을 제안합니다. 이 제안은 오픈소스 모델의 프로세스 평가 능력을 크게 향상시킵니다. 모든 리소스는 향후 신뢰성이 있는 다모뎔어프로세스 평가 연구를 위해 공개됩니다.",
      "upvotes": 3,
      "discussionId": "67cfcf684dac6ed12db8b185",
      "ai_keywords": [
        "ProJudgeBench",
        "multi-modal large language models (MLLMs)",
        "Reasoning processes",
        "Automated process judges",
        "Step-level labels",
        "Scientific disciplines",
        "Multi-modal content",
        "Error classification",
        "Dynamic Dual-Phase fine-tuning",
        "Instruction-tuning dataset",
        "Problem-solving reasoning"
      ]
    },
    "publishedAt": "2025-03-09T06:55:51.000Z",
    "title": "ProJudge: A Multi-Modal Multi-Discipline Benchmark and\n  Instruction-Tuning Dataset for MLLM-based Process Judges",
    "summary": "As multi-modal large language models (MLLMs) frequently exhibit errors when\nsolving scientific problems, evaluating the validity of their reasoning\nprocesses is critical for ensuring reliability and uncovering fine-grained\nmodel weaknesses. Since human evaluation is laborious and costly, prompting\nMLLMs as automated process judges has become a common practice. However, the\nreliability of these model-based judges remains uncertain. To address this, we\nintroduce ProJudgeBench, the first comprehensive benchmark specifically\ndesigned for evaluating abilities of MLLM-based process judges. ProJudgeBench\ncomprises 2,400 test cases and 50,118 step-level labels, spanning four\nscientific disciplines with diverse difficulty levels and multi-modal content.\nIn ProJudgeBench, each step is meticulously annotated by human experts for\ncorrectness, error type, and explanation, enabling a systematic evaluation of\njudges' capabilities to detect, classify and diagnose errors. Evaluation on\nProJudgeBench reveals a significant performance gap between open-source and\nproprietary models. To bridge this gap, we further propose ProJudge-173k, a\nlarge-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning\nstrategy that encourages models to explicitly reason through problem-solving\nbefore assessing solutions. Both contributions significantly enhance the\nprocess evaluation capabilities of open-source models. All the resources will\nbe released to foster future research of reliable multi-modal process\nevaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06553.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06542",
      "authors": [
        {
          "_id": "67d7e4ec1414fcb6196e79ba",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "67d7e4ec1414fcb6196e79bb",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "67d7e4ec1414fcb6196e79bc",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "67d7e4ec1414fcb6196e79bd",
          "name": "Fanrui Zhang",
          "hidden": false
        },
        {
          "_id": "67d7e4ec1414fcb6196e79be",
          "name": "Zizhen Li",
          "hidden": false
        },
        {
          "_id": "67d7e4ec1414fcb6196e79bf",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "67d7e4ec1414fcb6196e79c0",
          "name": "Sizhuo Zhou",
          "hidden": false
        },
        {
          "_id": "67d7e4ec1414fcb6196e79c1",
          "name": "Yu Dai",
          "hidden": false
        },
        {
          "_id": "67d7e4ec1414fcb6196e79c2",
          "name": "Shenglin Zhang",
          "hidden": false
        },
        {
          "_id": "67d7e4ec1414fcb6196e79c3",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T10:15:39.000Z",
      "submittedOnDailyAt": "2025-03-17T07:32:00.469Z",
      "title": "ARMOR v0.1: 아サミック 시나리오를 이용한 교차 모델 생성을 활용한 자동화된 다형성 이해 모델의 확장",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "Unifiedモデル（UniMs）는 시각과 언어 분야에서 최근에 많은 주목을 받고 있습니다. 현재의 UniMs는 양쪽의 여러 모델의 이해와 생성 능력을 동시에 학습하기 위해 많은 계산 자원을 소모하고 있으며, 텍스트와 이미지의 교차 생성에 어려움을 겪는 경우가 많습니다. 여기서는 리소스 효율적인, 단순한 자동단어연쇄 프레임워크인 ARMOR을 소개합니다. ARMOR은 기존의 여러 모델의 대형 언어 모델(MLLMs)을 미세 조정하여 이해와 생성을 동시에 실현합니다. 구체적으로는, ARMOR은 다음과 같은 3가지 점을 기존의 MLLMs에 확장합니다. 1. 모델 아키텍처에서, 대칭적인 인코더-디코더 아키텍처를 도입하고, 텍스트와 시각 모델을 통합하는 임베딩 공간을 생성하여 자연스러운 텍스트와 이미지의 교차 생성을 가능하게 합니다. 2. 훈련 데이터에서 고품질의 교차 데이터 세트를 정밀하게 선택하여 MLLMs의 미세 조정에 사용합니다. 3. 훈련 알고리즘에서, \"어떻게 생성할 것인가\" 알고리즘을 제안하여 기존의 MLLMs의 다 모델 생성 능력을 향상시키면서 이해 능력을 유지하는 것을 목표로 합니다. 실험 결과를 통해, ARMOR은 제한된 훈련 리소스를 사용하여 기존의 MLLMs를 UniMs에 업그레이드하여 기대되는 이미지 생성 능력을 보여주는 것으로 나타났습니다. 우리의 코드는 곧 https://armor.github.io에 공개됩니다.",
      "upvotes": 3,
      "discussionId": "67d7e4ee1414fcb6196e7a43",
      "ai_keywords": [
        "asymmetric encoder-decoder architecture",
        "forward-switching mechanism",
        "embedding space",
        "textual modality",
        "visual modality",
        "natural text-image interleaved generation",
        "computational overhead",
        "high-quality interleaved dataset",
        "multimodal large language models",
        "``what or how to generate\" algorithm",
        "progressive training stages",
        "image generation capabilities",
        "multimodal understanding capabilities"
      ]
    },
    "publishedAt": "2025-03-09T06:15:39.000Z",
    "title": "ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model\n  with Interleaved Multimodal Generation via Asymmetric Synergy",
    "summary": "Unified models (UniMs) for multimodal understanding and generation have\nrecently received much attention in the area of vision and language. Existing\nUniMs are designed to simultaneously learn both multimodal understanding and\ngeneration capabilities, demanding substantial computational resources, and\noften struggle to generate interleaved text-image. We present ARMOR, a\nresource-efficient and pure autoregressive framework that achieves both\nunderstanding and generation by fine-tuning existing multimodal large language\nmodels (MLLMs). Specifically, ARMOR extends existing MLLMs from three\nperspectives: (1) For model architecture, an asymmetric encoder-decoder\narchitecture with a forward-switching mechanism is introduced to unify\nembedding space integrating textual and visual modalities for enabling natural\ntext-image interleaved generation with minimal computational overhead. (2) For\ntraining data, a meticulously curated, high-quality interleaved dataset is\ncollected for fine-tuning MLLMs. (3) For the training algorithm, we propose a\n``what or how to generate\" algorithm to empower existing MLLMs with multimodal\ngeneration capabilities while preserving their multimodal understanding\ncapabilities, through three progressive training stages based on the collected\ndataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to\nUniMs with promising image generation capabilities, using limited training\nresources. Our code will be released soon at https://armor.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06542.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05689",
      "authors": [
        {
          "_id": "67d37c5c3b54e330517a545d",
          "user": {
            "_id": "665b2ac6e0e2374ca24ba000",
            "avatarUrl": "/avatars/d5218c9fa3dceae7b91df2e1d396bcf3.svg",
            "isPro": false,
            "fullname": "Zebin Xing",
            "user": "XXXXing",
            "type": "user"
          },
          "name": "Zebin Xing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:46.875Z",
          "hidden": false
        },
        {
          "_id": "67d37c5c3b54e330517a545e",
          "name": "Xingyu Zhang",
          "hidden": false
        },
        {
          "_id": "67d37c5c3b54e330517a545f",
          "name": "Yang Hu",
          "hidden": false
        },
        {
          "_id": "67d37c5c3b54e330517a5460",
          "name": "Bo Jiang",
          "hidden": false
        },
        {
          "_id": "67d37c5c3b54e330517a5461",
          "name": "Tong He",
          "hidden": false
        },
        {
          "_id": "67d37c5c3b54e330517a5462",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67d37c5c3b54e330517a5463",
          "name": "Xiaoxiao Long",
          "hidden": false
        },
        {
          "_id": "67d37c5c3b54e330517a5464",
          "user": {
            "_id": "654a2b1a83e7bfc4313a5cc7",
            "avatarUrl": "/avatars/dc3dfc3fcd26bb7350a9db0d075c5ea0.svg",
            "isPro": false,
            "fullname": "Wei Yin",
            "user": "WonderingWorld",
            "type": "user"
          },
          "name": "Wei Yin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:44:54.732Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T18:52:08.000Z",
      "submittedOnDailyAt": "2025-03-17T01:05:31.649Z",
      "title": "GoalFlow: 목표 주도의 다모달 트래지렉트의 플로우 매칭 및 종말에서 종말까지의 자동 운행",
      "submittedOnDailyBy": {
        "_id": "654a2b1a83e7bfc4313a5cc7",
        "avatarUrl": "/avatars/dc3dfc3fcd26bb7350a9db0d075c5ea0.svg",
        "isPro": false,
        "fullname": "Wei Yin",
        "user": "WonderingWorld",
        "type": "user"
      },
      "summary": "ゴールフロー、종단부터 종단으로의 자동주행을 위한 엔드투엔드 자동주행 메소드를 제안합니다. 자동주행 시나리오에서 적절한 모노모ー드 디지젝트는 적지 않습니다. 최근의 방법들은 다모ー드 디지젝트 분포를 모델링하는 데 중점을 두고 있습니다. 그러나 이러한 방법은 높은 디지젝트 분산과 가이드포더 및 공간정보의 불연속성으로 인해 디지젝트 품질의 저하와 트래픽선정의 복잡성에 직면하고 있습니다. 이러한 문제를 해결하기 위해, ゴールフロー, 새로운 방법을 소개합니다. 이 방법은 생성 프로세스를 효과적으로 제한하고 고품질의 다모ー드 디지젝트를 생성할 수 있습니다. 디퓨전 기반 방법의 트래픽 분산 문제를 해결하기 위해, ゴールフロー는 생성되는 트래픽을 목표점을 추가하여 제한합니다. ゴールフロー는 스케닝 정보에 기반하여 후보점에서 가장 적절한 목표점을 선택하는 새로운 점수 체계를 구축합니다. 또한, ゴールフロー는 효율적인 생성 메소드인 フロー 매칭을 사용하여 다모ー드 디지젝트를 생성하고 후보에서 가장 적절한 트래픽을 선택하는 개선된 점수 체계를 사용합니다. 우리의 실험 결과, NavsimDauner2024_navsim에서 검증된 것で, ゴールフロー는 가장 先端의 성능을 달성하고, 자동주행을 위한 견고한 다모ー드 디지젝트를 제공합니다. ゴールフロー는 PDMS 90.3를 달성하고 다른 방법보다 크게 초과했습니다. 디퓨전 정책 기반의 방법과 비교하여, 우리의 접근법은 우수한 성능을 얻기 위해 하나의 노이즈 디노이즈 스텝이 필요합니다. 코드는 https://github.com/YvanYin/GoalFlow에 공개되어 있습니다.",
      "upvotes": 2,
      "discussionId": "67d37c5d3b54e330517a54c7",
      "ai_keywords": [
        "GoalFlow",
        "multimodal trajectories",
        "trajectory selection complexity",
        "trajectory divergence",
        "diffusion-based methods",
        "goal point",
        "scoring mechanism",
        "Flow Matching",
        "Navsim",
        "PDMS",
        "diffusion-policy-based methods",
        "denoising step"
      ]
    },
    "publishedAt": "2025-03-07T13:52:08.000Z",
    "title": "GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving",
    "summary": "We propose GoalFlow, an end-to-end autonomous driving method for generating\nhigh-quality multimodal trajectories. In autonomous driving scenarios, there is\nrarely a single suitable trajectory. Recent methods have increasingly focused\non modeling multimodal trajectory distributions. However, they suffer from\ntrajectory selection complexity and reduced trajectory quality due to high\ntrajectory divergence and inconsistencies between guidance and scene\ninformation. To address these issues, we introduce GoalFlow, a novel method\nthat effectively constrains the generative process to produce high-quality,\nmultimodal trajectories. To resolve the trajectory divergence problem inherent\nin diffusion-based methods, GoalFlow constrains the generated trajectories by\nintroducing a goal point. GoalFlow establishes a novel scoring mechanism that\nselects the most appropriate goal point from the candidate points based on\nscene information. Furthermore, GoalFlow employs an efficient generative\nmethod, Flow Matching, to generate multimodal trajectories, and incorporates a\nrefined scoring mechanism to select the optimal trajectory from the candidates.\nOur experimental results, validated on the NavsimDauner2024_navsim,\ndemonstrate that GoalFlow achieves state-of-the-art performance, delivering\nrobust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS\nof 90.3, significantly surpassing other methods. Compared with other\ndiffusion-policy-based methods, our approach requires only a single denoising\nstep to obtain excellent performance. The code is available at\nhttps://github.com/YvanYin/GoalFlow.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05689.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654a2b1a83e7bfc4313a5cc7",
      "avatarUrl": "/avatars/dc3dfc3fcd26bb7350a9db0d075c5ea0.svg",
      "fullname": "Wei Yin",
      "name": "WonderingWorld",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10624",
      "authors": [
        {
          "_id": "67d5b604f58a6a411a5bb598",
          "user": {
            "_id": "65987383bf533e3c0dd1914b",
            "avatarUrl": "/avatars/4b3c0e791ef24a439a43371c4d92bc4b.svg",
            "isPro": false,
            "fullname": "Boqian Li",
            "user": "Boqian-Li",
            "type": "user"
          },
          "name": "Boqian Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-16T21:12:23.978Z",
          "hidden": false
        },
        {
          "_id": "67d5b604f58a6a411a5bb599",
          "name": "Haiwen Feng",
          "hidden": false
        },
        {
          "_id": "67d5b604f58a6a411a5bb59a",
          "name": "Zeyu Cai",
          "hidden": false
        },
        {
          "_id": "67d5b604f58a6a411a5bb59b",
          "name": "Michael J. Black",
          "hidden": false
        },
        {
          "_id": "67d5b604f58a6a411a5bb59c",
          "name": "Yuliang Xiu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/IP3jsn2w6NAUnlzaLby5W.png",
        "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/YFQHw4OpnXHm7SDXX14x3.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/vWNGuO2-4RWYzKS3X5kip.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/VPNP0CWFQW0VbiSXgf39v.mp4"
      ],
      "publishedAt": "2025-03-13T17:59:14.000Z",
      "submittedOnDailyAt": "2025-03-17T07:50:58.923Z",
      "title": "ETCH: 체적적 적응성을 모든 사람이 입을 수 있는 옷에 적용할 수 있는 엣지 기반의 가변성 밀집성",
      "submittedOnDailyBy": {
        "_id": "65987383bf533e3c0dd1914b",
        "avatarUrl": "/avatars/4b3c0e791ef24a439a43371c4d92bc4b.svg",
        "isPro": false,
        "fullname": "Boqian Li",
        "user": "Boqian-Li",
        "type": "user"
      },
      "summary": "3D 옷을 따라人体 포인트 그룹에 맞출 것은 보통이지만 어려운 작업입니다. 전통적인 최적화 기반의 접근 방식은 단계별 접근 방식이며, 초기 세팅에 민감하며, 최근의 학습 기반의 방법들은 다양한 자세와 옷의 종류에 대한 일반화에 어려움이 있습니다. 우리는 Equivariant Tightness Fitting for Clothed Humans, ETCH를 제안합니다. ETCH는 지역적으로 근사된 SE(3) 동변성을 사용하여 옷의 표면 매핑을 추정하고, 이 매핑에 따라 자세 불변한 신체 특징을 사용하여 희소한 신체 마커를 추정하여 옷을 내부 마커로 피팅하는 작업에 단순화합니다. CAPE와 4D-Dress에서 다양한 실험을 수행했으며, ETCH는 16.7% ~ 69.5%의 부드러운 옷의 신체 피팅 정확도와 평균 49.9%의 형상 정확도를 통해 가장 최신의 방법보다 크게 개선되었습니다. ETCH의 동변성 틱네스스 설계는 1샷(또는 분포 외) 설정에서 방향 오류를 67.2% ~ 89.8%까지 감소시킬 수 있습니다. 질적인 결과를 보여주는 것은 ETCH의 강력한 일반화 능력을 보여줍니다. 어려운 자세,未见의 형상, 부드러운 옷, 비정상 역학에 관계없이입니다. 우리는 https://boqian-li.github.io/ETCH/에서 연구를 위해 코드와 모델을 즉시 릴리즈합니다.",
      "upvotes": 1,
      "discussionId": "67d5b607f58a6a411a5bb680",
      "projectPage": "https://boqian-li.github.io/ETCH/",
      "githubRepo": "https://github.com/boqian-li/ETCH",
      "ai_keywords": [
        "equivariant",
        "tightness fitting",
        "SE(3) equivariance",
        "displacement vectors",
        "pose-invariant body features",
        "sparse body markers",
        "inner-body marker fitting task",
        "CAPE",
        "4D-Dress",
        "tightness-agnostic",
        "tightness-aware",
        "body fitting accuracy",
        "shape accuracy",
        "directional errors",
        "one-shot",
        "out-of-distribution",
        "generalization"
      ]
    },
    "publishedAt": "2025-03-13T13:59:14.000Z",
    "title": "ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant\n  Tightness",
    "summary": "Fitting a body to a 3D clothed human point cloud is a common yet challenging\ntask. Traditional optimization-based approaches use multi-stage pipelines that\nare sensitive to pose initialization, while recent learning-based methods often\nstruggle with generalization across diverse poses and garment types. We propose\nEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline\nthat estimates cloth-to-body surface mapping through locally approximate SE(3)\nequivariance, encoding tightness as displacement vectors from the cloth surface\nto the underlying body. Following this mapping, pose-invariant body features\nregress sparse body markers, simplifying clothed human fitting into an\ninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show\nthat ETCH significantly outperforms state-of-the-art methods -- both\ntightness-agnostic and tightness-aware -- in body fitting accuracy on loose\nclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant\ntightness design can even reduce directional errors by (67.2% ~ 89.8%) in\none-shot (or out-of-distribution) settings. Qualitative results demonstrate\nstrong generalization of ETCH, regardless of challenging poses, unseen shapes,\nloose clothing, and non-rigid dynamics. We will release the code and models\nsoon for research purposes at https://boqian-li.github.io/ETCH/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/IP3jsn2w6NAUnlzaLby5W.png",
      "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/YFQHw4OpnXHm7SDXX14x3.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/vWNGuO2-4RWYzKS3X5kip.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/VPNP0CWFQW0VbiSXgf39v.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10624.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65987383bf533e3c0dd1914b",
      "avatarUrl": "/avatars/4b3c0e791ef24a439a43371c4d92bc4b.svg",
      "fullname": "Boqian Li",
      "name": "Boqian-Li",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08111",
      "authors": [
        {
          "_id": "67d69afb060a3df28c886b2b",
          "name": "Jianhui Wang",
          "hidden": false
        },
        {
          "_id": "67d69afb060a3df28c886b2c",
          "user": {
            "_id": "6464c4ef92773d5eeb588525",
            "avatarUrl": "/avatars/65fc839453d892016640249cc1acf277.svg",
            "isPro": false,
            "fullname": "Zhifei Yang",
            "user": "yangzhifei",
            "type": "user"
          },
          "name": "Zhifei Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-16T21:12:12.522Z",
          "hidden": true
        },
        {
          "_id": "67d69afb060a3df28c886b2d",
          "name": "Yangfan He",
          "hidden": false
        },
        {
          "_id": "67d69afb060a3df28c886b2e",
          "name": "Huixiong Zhang",
          "hidden": false
        },
        {
          "_id": "67d69afb060a3df28c886b2f",
          "name": "Yuxuan Chen",
          "hidden": false
        },
        {
          "_id": "67d69afb060a3df28c886b30",
          "name": "Jingwei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T07:23:11.000Z",
      "submittedOnDailyAt": "2025-03-17T07:52:48.263Z",
      "title": "MaRI: 영역을 초월하는 데이터 검색 통합",
      "submittedOnDailyBy": {
        "_id": "6464c4ef92773d5eeb588525",
        "avatarUrl": "/avatars/65fc839453d892016640249cc1acf277.svg",
        "isPro": false,
        "fullname": "Zhifei Yang",
        "user": "yangzhifei",
        "type": "user"
      },
      "summary": "정확한 재료 검색은 리アル티스ト 3D 자산의 제작에 중요합니다. 현재의 방법들은 형상 불변성과 광원 다양성을 처리하는 데이터 세트를 기반으로 하지만, 이러한 데이터 세트는 희소하며, 다양성의 한계와 실세계의 일반화에 충분하지 않은 도전에 직면합니다. 많은 현재 접근법은 전통적인 이미지 검색 방법론을 채택하지만, 재료 공간의 고유한 특성을 이해하는 능력이 부족하여, 검색 태스크에서 최적의 성능을 보여주지 않습니다. 이러한 문제를 해결하기 위해, 우리는 합성 재료와 실세계 재료의 특성 공간 사이의 간극을 메우는 프레임워크 \"MaRI\"를 소개합니다. MaRI는 이미지와 재료 인코더를 함께 학습시켜, 대조적 학습 전략을 통해 시각적 특성과 재료의 속성을 조화롭게 통합된 공유 인코딩 공간으로 구축합니다. 유사한 재료와 이미지를 근접시키고, 특성 공간 내의 불일치한 쌍을 분리합니다. 이를 지원하기 위해, 고품질의 합성 재료 데이터 세트를 구축합니다. 이러한 재료는 제어된 형상 변화와 다양한 광원 조건을 동반하며, 또한 실세계 재료는 재료 전송 기술로 처리되어 표준화되어 있습니다. 확장된 실험은 MaRI가 다양한 복잡한 재료 검색 태스크에서 상위 성능, 정확성과 일반화 능력을 보여주는 것을 명확히 보여주었습니다.",
      "upvotes": 1,
      "discussionId": "67d69afd060a3df28c886c24",
      "projectPage": "https://jianhuiwemi.github.io/MaRI/",
      "ai_keywords": [
        "contrastive learning",
        "embedding space",
        "feature space",
        "image encoder",
        "material encoder",
        "material transfer techniques",
        "synthetic materials",
        "real-world materials",
        "shape variations",
        "lighting conditions",
        "material retrieval tasks"
      ]
    },
    "publishedAt": "2025-03-11T03:23:11.000Z",
    "title": "MaRI: Material Retrieval Integration across Domains",
    "summary": "Accurate material retrieval is critical for creating realistic 3D assets.\nExisting methods rely on datasets that capture shape-invariant and\nlighting-varied representations of materials, which are scarce and face\nchallenges due to limited diversity and inadequate real-world generalization.\nMost current approaches adopt traditional image search techniques. They fall\nshort in capturing the unique properties of material spaces, leading to\nsuboptimal performance in retrieval tasks. Addressing these challenges, we\nintroduce MaRI, a framework designed to bridge the feature space gap between\nsynthetic and real-world materials. MaRI constructs a shared embedding space\nthat harmonizes visual and material attributes through a contrastive learning\nstrategy by jointly training an image and a material encoder, bringing similar\nmaterials and images closer while separating dissimilar pairs within the\nfeature space. To support this, we construct a comprehensive dataset comprising\nhigh-quality synthetic materials rendered with controlled shape variations and\ndiverse lighting conditions, along with real-world materials processed and\nstandardized using material transfer techniques. Extensive experiments\ndemonstrate the superior performance, accuracy, and generalization capabilities\nof MaRI across diverse and complex material retrieval tasks, outperforming\nexisting methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08111.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6464c4ef92773d5eeb588525",
      "avatarUrl": "/avatars/65fc839453d892016640249cc1acf277.svg",
      "fullname": "Zhifei Yang",
      "name": "yangzhifei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]