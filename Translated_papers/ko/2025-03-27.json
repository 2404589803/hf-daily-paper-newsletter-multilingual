[
  {
    "paper": {
      "id": "2503.19757",
      "authors": [
        {
          "_id": "67e3e1e20706b07bfb2713d6",
          "user": {
            "_id": "643fa1c318afbc4d1f3e5e59",
            "avatarUrl": "/avatars/f8f35355902b4cda72e9c6d768322fae.svg",
            "isPro": false,
            "fullname": "Zhi Hou",
            "user": "zhihou",
            "type": "user"
          },
          "name": "Zhi Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:43:49.995Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713d7",
          "user": {
            "_id": "64c9e86a6a26cddbecd9bae2",
            "avatarUrl": "/avatars/61a84989dbbc1898ebcba3236dbed039.svg",
            "isPro": false,
            "fullname": "Tianyi Zhang",
            "user": "TianyiZhang0213",
            "type": "user"
          },
          "name": "Tianyi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:44:21.118Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713d8",
          "name": "Yuwen Xiong",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713d9",
          "user": {
            "_id": "66ab30dfd456f0408b93f27b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ab30dfd456f0408b93f27b/nps4Kni_eOExO5Z92RiiF.jpeg",
            "isPro": false,
            "fullname": "Haonan Duan",
            "user": "robot-haonan",
            "type": "user"
          },
          "name": "Haonan Duan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:04:53.498Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713da",
          "user": {
            "_id": "648a1e44fe11ebd7489c289c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WwMcD9PK0gxIu2I0n0QyD.jpeg",
            "isPro": false,
            "fullname": "Hengjun Pu",
            "user": "MIASANMIA",
            "type": "user"
          },
          "name": "Hengjun Pu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:01.725Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713db",
          "user": {
            "_id": "66b9a5bb32be421cd8538cd6",
            "avatarUrl": "/avatars/a1f7c0fe3ed4741017db713b4e6d47c8.svg",
            "isPro": false,
            "fullname": "Ronglei Tong",
            "user": "TTTTTony",
            "type": "user"
          },
          "name": "Ronglei Tong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:07.673Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713dc",
          "user": {
            "_id": "679165b9c7f527ef3619504e",
            "avatarUrl": "/avatars/f3e6ce5fc3d05c8632d8b208f55c2987.svg",
            "isPro": false,
            "fullname": "Chengyang Zhao",
            "user": "chengyzhao",
            "type": "user"
          },
          "name": "Chengyang Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:13.906Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713dd",
          "user": {
            "_id": "64ae2359179421d320b1694b",
            "avatarUrl": "/avatars/c387a75191005bcaa473091de5383a10.svg",
            "isPro": false,
            "fullname": "Xizhou Zhu",
            "user": "Einsiedler",
            "type": "user"
          },
          "name": "Xizhou Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:20.614Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713de",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713df",
          "user": {
            "_id": "64686f7172d9180d4ac8b4e4",
            "avatarUrl": "/avatars/db67dd6c4b2b41054ddcce5a18ade6f8.svg",
            "isPro": false,
            "fullname": "Jifeng Dai",
            "user": "daijifeng",
            "type": "user"
          },
          "name": "Jifeng Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:27.039Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713e0",
          "user": {
            "_id": "632dab84fdb35759ea6646a0",
            "avatarUrl": "/avatars/857b0b4d115aa5ab2f143e60b0e4edc6.svg",
            "isPro": false,
            "fullname": "Yuntao Chen",
            "user": "YuntaoChen",
            "type": "user"
          },
          "name": "Yuntao Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:40.200Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643fa1c318afbc4d1f3e5e59/T0twf6ibM7Htfq525gf3L.mp4"
      ],
      "publishedAt": "2025-03-25T15:19:56.000Z",
      "submittedOnDailyAt": "2025-03-27T01:27:55.746Z",
      "title": "Dita: 확대 디퓨젼 트랜지셔어 파시안의 一般主義적인 시각언어 액션 정책",
      "submittedOnDailyBy": {
        "_id": "643fa1c318afbc4d1f3e5e59",
        "avatarUrl": "/avatars/f8f35355902b4cda72e9c6d768322fae.svg",
        "isPro": false,
        "fullname": "Zhi Hou",
        "user": "zhihou",
        "type": "user"
      },
      "summary": "최근, 다양한 로봇 데이터셋을 사용하여 훈련된 시각 언어 액션 모델은 제한된 영역 데이터에서도 원하는 일반화 능력을 보여주고 있지만, 단순화된 액션 헤드를 사용하여, 이산화된 또는 연속된 액션을 예측함으로써, 다른 액션 공간에 적응할 수 없습니다. 여기서는, Transformer 아키텍처를 활용하여, 연속적인 액션 시퀀스를 직접 디노이징하는 scalable 프레임워크 Dita를 소개합니다. 이전의 방법과 달리, 얕은 네트워크로 결합된 인베딩에 기반한 디노이징을 조건으로 하지 않는 Dita는 인코텍스트 조건付き로 디노이징을 수행하고, 역사적인 관찰으로부터의 단순한 시각 토큰과 디노이징된 액션의 微妙한 대응을 가능하게 합니다. 이 설계는 액션의 차이와 환경의 微妙한 점을 명확히 모델화합니다. Transformer의 scalability와 함께, Dita는 촬영각도, 관찰 시뮬레이션, 태스크, 액션 공간의 다양성을 포함하는 다양한 기계의 데이터셋을 간접적으로 통합하고, 이 연계는 다양성 대응력을 강화하고, 장기 태스크의 성공적인 실행을 촉진합니다. 다양한 벤치마크에서 평가는, 시뮬레이션에서 가장 先端적인 성능이나 비교적인 성능을 나타냅니다. 특히, Dita는 10샷의 微調節에서, 그만第三人称의 카메라 입력을 사용하며, 환경의 변화와 복잡한 장기 태스크에 대한 실세계 적용을 실현합니다. 이 아키텍처는 일반적인 로봇 정책 학습을 위해 기능적, 가벼운, 오픈 소스의 기본 라인을 제공합니다. 프로젝트 페이지: https://robodita.github.io.",
      "upvotes": 32,
      "discussionId": "67e3e1e40706b07bfb2714cd",
      "projectPage": "https://robodita.github.io",
      "githubRepo": "https://github.com/RoboDita/Dita",
      "ai_keywords": [
        "Transformer architectures",
        "multimodal diffusion process",
        "in-context conditioning",
        "action deltas",
        "environmental nuances",
        "cross-embodiment datasets",
        "long-horizon tasks",
        "10-shot finetuning",
        "third-person camera inputs",
        "generalist robot policy learning"
      ]
    },
    "publishedAt": "2025-03-25T11:19:56.000Z",
    "title": "Dita: Scaling Diffusion Transformer for Generalist\n  Vision-Language-Action Policy",
    "summary": "While recent vision-language-action models trained on diverse robot datasets\nexhibit promising generalization capabilities with limited in-domain data,\ntheir reliance on compact action heads to predict discretized or continuous\nactions constrains adaptability to heterogeneous action spaces. We present\nDita, a scalable framework that leverages Transformer architectures to directly\ndenoise continuous action sequences through a unified multimodal diffusion\nprocess. Departing from prior methods that condition denoising on fused\nembeddings via shallow networks, Dita employs in-context conditioning --\nenabling fine-grained alignment between denoised actions and raw visual tokens\nfrom historical observations. This design explicitly models action deltas and\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\nTransformer's scalability, Dita effectively integrates cross-embodiment\ndatasets across diverse camera perspectives, observation scenes, tasks, and\naction spaces. Such synergy enhances robustness against various variances and\nfacilitates the successful execution of long-horizon tasks. Evaluations across\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\nsimulation. Notably, Dita achieves robust real-world adaptation to\nenvironmental variances and complex long-horizon tasks through 10-shot\nfinetuning, using only third-person camera inputs. The architecture establishes\na versatile, lightweight and open-source baseline for generalist robot policy\nlearning. Project Page: https://robodita.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643fa1c318afbc4d1f3e5e59/T0twf6ibM7Htfq525gf3L.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19757.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643fa1c318afbc4d1f3e5e59",
      "avatarUrl": "/avatars/f8f35355902b4cda72e9c6d768322fae.svg",
      "fullname": "Zhi Hou",
      "name": "zhihou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.19990",
      "authors": [
        {
          "_id": "67e4d3df7e97884ba4150ec0",
          "user": {
            "_id": "662516d72419feed62fb3a0a",
            "avatarUrl": "/avatars/24c4157829e70a4e346aa984885aa5ad.svg",
            "isPro": false,
            "fullname": "Dian",
            "user": "KexianTang",
            "type": "user"
          },
          "name": "Kexian Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:46:12.696Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec1",
          "user": {
            "_id": "64a6ae0e0437599198cf3a98",
            "avatarUrl": "/avatars/6635432cc0589ba12dc170cad6986d6d.svg",
            "isPro": false,
            "fullname": "Junyao Gao",
            "user": "favourisnotyou",
            "type": "user"
          },
          "name": "Junyao Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:15.061Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec2",
          "user": {
            "_id": "63d4b843df01ef426a0f79fb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676365795587-63d4b843df01ef426a0f79fb.jpeg",
            "isPro": false,
            "fullname": "Yanhong Zeng",
            "user": "zengyh1900",
            "type": "user"
          },
          "name": "Yanhong Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:46:24.775Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec3",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:00:33.733Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec4",
          "name": "Yanan Sun",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec5",
          "user": {
            "_id": "62fb2a9dc95d426ff8f74c8d",
            "avatarUrl": "/avatars/25c1a68ee7b7d0cc7e9f56bde37f4914.svg",
            "isPro": false,
            "fullname": "Zhening Xing",
            "user": "Leoxing",
            "type": "user"
          },
          "name": "Zhening Xing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:00:36.341Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec6",
          "user": {
            "_id": "6385f8598b5acae8d24caf16",
            "avatarUrl": "/avatars/9d261f95d24e882157b987b8827098be.svg",
            "isPro": false,
            "fullname": "liuwenran",
            "user": "lwrshi1965",
            "type": "user"
          },
          "name": "Wenran Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:46:45.866Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec7",
          "user": {
            "_id": "6414230a0fcefcf72e5085dd",
            "avatarUrl": "/avatars/3a38dc8c84b0f27af846184d1c19f6ef.svg",
            "isPro": false,
            "fullname": "Kaifeng Lyu",
            "user": "vfleaking",
            "type": "user"
          },
          "name": "Kaifeng Lyu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:46:52.341Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec8",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/kjq_V0to2uTR2RSR3TyfV.png"
      ],
      "publishedAt": "2025-03-25T18:21:07.000Z",
      "submittedOnDailyAt": "2025-03-27T03:00:01.698Z",
      "title": "LEGO-Puzzles: 멀티스텝 공간적인 논리에 대한 MLLM의 성능",
      "submittedOnDailyBy": {
        "_id": "63ee1379190ddd6214efd73a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
        "isPro": false,
        "fullname": "HAODONG DUAN",
        "user": "KennyUTC",
        "type": "user"
      },
      "summary": "다스텝 공간의 논리론은 여러 순서적인 스텝에 대한 공간 관계의 이해와 논리론을 포함하는 개념이며, 로봇 조작, 자동 네비게이션, 자동 조립 등 복잡한 현실 세계의 응용을 해결하기 위해 중요합니다. 현재의 Multimodal Large Language Models (MLLMs)이 이러한 기본적인 능력을 어느 정도 학습했는지 평가하기 위해, LEGO-Puzzles라는 scalable benchmark을 통해 MLLMs의 공간 이해와 순서적인 논리론을 평가하기 위한 LEGO 기반의 태스크를 설계했습니다. LEGO-Puzzles은 1,100점의 잘 선택된 시각적인 질문에 답 (VQA) 샘플을 포함하며, 11가지 종류의 다른 태스크를 범위로 넓혀, 기본적인 공간 이해부터 복잡한 다스텝 논리론까지 광범위하게 다루고 있습니다. LEGO-Puzzles에 기반하여, 가장 강력한 MLLMs를 평가하고, 공간 논리론 능력에 대한 중대한 제한을 밝혀 냈습니다: 가장 강력한 MLLM도 테스트 케이스의 절반을 답할 수 있는 정도만 한계입니다. 인간 참여자는 90% 이상의 정확도를 달성합니다. VQA 태스크 외에도, MLLMs가 조립 그림을 따라 LEGO 이미지 생성하는 능력을 평가했습니다. 실험 결과에 따르면, 다른 MLLM은 입력 이미지를 재현하거나 완전히 관련없는 출력을 생성하는 것을 확인했습니다. 전체적으로, LEGO-Puzzles은 현재의 MLLMs의 공간 이해와 순서적인 논리론 능력에 대한 중요한 부족점을 밝혀, 다스텝 공간 논리론의 발전의 필요성을 강조합니다.",
      "upvotes": 22,
      "discussionId": "67e4d3e07e97884ba4150f2b",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "LEGO-Puzzles",
        "visual question-answering (VQA)",
        "spatial understanding",
        "sequential reasoning",
        "Gemini-2.0-Flash",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-03-25T14:21:07.000Z",
    "title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
    "summary": "Multi-step spatial reasoning entails understanding and reasoning about\nspatial relationships across multiple sequential steps, which is crucial for\ntackling complex real-world applications, such as robotic manipulation,\nautonomous navigation, and automated assembly. To assess how well current\nMultimodal Large Language Models (MLLMs) have acquired this fundamental\ncapability, we introduce LEGO-Puzzles, a scalable benchmark designed\nto evaluate both spatial understanding and sequential\nreasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100\ncarefully curated visual question-answering (VQA) samples spanning 11 distinct\ntasks, ranging from basic spatial understanding to complex multi-step\nreasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of\nstate-of-the-art MLLMs and uncover significant limitations in their spatial\nreasoning capabilities: even the most powerful MLLMs can answer only about half\nof the test cases, whereas human participants achieve over 90\\% accuracy. In\naddition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images\nfollowing assembly illustrations. Our experiments show that only\nGemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these\ninstructions, while other MLLMs either replicate the input image or generate\ncompletely irrelevant outputs. Overall, LEGO-Puzzles exposes critical\ndeficiencies in existing MLLMs' spatial understanding and sequential reasoning\ncapabilities, and underscores the need for further advancements in multimodal\nspatial reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/kjq_V0to2uTR2RSR3TyfV.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ee1379190ddd6214efd73a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
      "fullname": "HAODONG DUAN",
      "name": "KennyUTC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20240",
      "authors": [
        {
          "_id": "67e4ae6787c92169aa3caa74",
          "user": {
            "_id": "66435efdc26b490acc85079b",
            "avatarUrl": "/avatars/16e0ee25734516d4295abe0fcc0e26a9.svg",
            "isPro": false,
            "fullname": "Prin Phunyaphibarn",
            "user": "prinphunya",
            "type": "user"
          },
          "name": "Prin Phunyaphibarn",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:34.055Z",
          "hidden": false
        },
        {
          "_id": "67e4ae6787c92169aa3caa75",
          "user": {
            "_id": "6342796a0875f2c99cfd313b",
            "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
            "isPro": false,
            "fullname": "Yuseung \"Phillip\" Lee",
            "user": "phillipinseoul",
            "type": "user"
          },
          "name": "Phillip Y. Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:32.144Z",
          "hidden": false
        },
        {
          "_id": "67e4ae6787c92169aa3caa76",
          "name": "Jaihoon Kim",
          "hidden": false
        },
        {
          "_id": "67e4ae6787c92169aa3caa77",
          "user": {
            "_id": "631f432b5ba8c026340a7890",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631f432b5ba8c026340a7890/9PK7A_TRMpugwYjCsNBf1.jpeg",
            "isPro": false,
            "fullname": "Minhyuk Sung",
            "user": "Minhyuk",
            "type": "user"
          },
          "name": "Minhyuk Sung",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:47:11.255Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T05:11:38.000Z",
      "submittedOnDailyAt": "2025-03-27T00:22:30.335Z",
      "title": "무조건 선두 모델이 중요합니다! 조정된 Difusion 모델의 조건부 생성을 개선하는 데 중요합니다.",
      "submittedOnDailyBy": {
        "_id": "6342796a0875f2c99cfd313b",
        "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
        "isPro": false,
        "fullname": "Yuseung \"Phillip\" Lee",
        "user": "phillipinseoul",
        "type": "user"
      },
      "summary": "クラスフレードガイド（CFG）는 조건付きディフフォーションモデル의 훈련에 기본적인 기술입니다. CFG에 기반한 훈련의 일반적인 실습은 조건付きおよび非条件付きノイズ予測를 학습하기 위해 하나의 네트워크를 사용하며, 조건付き 학습을 위해 작은 드롭아웃률을 사용합니다. 그러나우리는, 학습 중의 유한한 バンダイッド内で의非条件付きノイズと의 동시 학습을 통해, 非条件付きケース의 プロイアリズムが 悪くなることを見出しました. より重要なことに, これらの 悪い非条件付きノイズ予測が条件付き生成の品質を低下させる重大な原因となります.多数のCFGに基づく条件付きモデルが, ベースモデルの微調節で学習されていることをヒントに, 私たちは最初に, CFGでの非条件付きノイズを ベースモデルが予測したものに置き換えることで条件付き生成を大幅に向上させることができることを示します.また, 微調節されたモデルが学習されたディフフォーションモデル以外のモデルを非条件付きノイズ置換に使用することができることを示します. 이미지と映像生成の両方において, Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, InstructPix2Pixなどの様々なCFGに基づく条件付きモデルで実験的にこの主張を検証しました.",
      "upvotes": 17,
      "discussionId": "67e4ae6a87c92169aa3cabc2",
      "ai_keywords": [
        "Classifier-Free Guidance (CFG)",
        "conditional diffusion models",
        "noise prediction",
        "dropout rate",
        "priors",
        "fine-tuning",
        "base model",
        "unconditional generation",
        "variance scaling",
        "Zero-1-to-3",
        "Versatile Diffusion",
        "DiT",
        "DynamiCrafter",
        "InstructPix2Pix"
      ]
    },
    "publishedAt": "2025-03-26T01:11:38.000Z",
    "title": "Unconditional Priors Matter! Improving Conditional Generation of\n  Fine-Tuned Diffusion Models",
    "summary": "Classifier-Free Guidance (CFG) is a fundamental technique in training\nconditional diffusion models. The common practice for CFG-based training is to\nuse a single network to learn both conditional and unconditional noise\nprediction, with a small dropout rate for conditioning. However, we observe\nthat the joint learning of unconditional noise with limited bandwidth in\ntraining results in poor priors for the unconditional case. More importantly,\nthese poor unconditional noise predictions become a serious reason for\ndegrading the quality of conditional generation. Inspired by the fact that most\nCFG-based conditional models are trained by fine-tuning a base model with\nbetter unconditional generation, we first show that simply replacing the\nunconditional noise in CFG with that predicted by the base model can\nsignificantly improve conditional generation. Furthermore, we show that a\ndiffusion model other than the one the fine-tuned model was trained on can be\nused for unconditional noise replacement. We experimentally verify our claim\nwith a range of CFG-based conditional models for both image and video\ngeneration, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and\nInstructPix2Pix.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20240.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6342796a0875f2c99cfd313b",
      "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
      "fullname": "Yuseung \"Phillip\" Lee",
      "name": "phillipinseoul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20215",
      "authors": [
        {
          "_id": "67e4f2507e97884ba4205660",
          "name": "Jin Xu",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205661",
          "user": {
            "_id": "661e577cbac5d981f883b743",
            "avatarUrl": "/avatars/95e55e9707a6b55594c264081202d7f4.svg",
            "isPro": false,
            "fullname": "GuoZhifang",
            "user": "ZhifangGuo",
            "type": "user"
          },
          "name": "Zhifang Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:52:32.600Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205662",
          "user": {
            "_id": "6594f06ac04427eb38444bce",
            "avatarUrl": "/avatars/b13fbf589b25eff038deb3fa12d95871.svg",
            "isPro": false,
            "fullname": "Jinzheng He",
            "user": "jinzheng-he",
            "type": "user"
          },
          "name": "Jinzheng He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:52:48.707Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205663",
          "name": "Hangrui Hu",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205664",
          "name": "Ting He",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205665",
          "user": {
            "_id": "63451cf0a05b51f7ded25505",
            "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg",
            "isPro": false,
            "fullname": "shuai bai",
            "user": "bluelike",
            "type": "user"
          },
          "name": "Shuai Bai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:02.818Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205666",
          "user": {
            "_id": "6461d675681b2e19b6acb5a5",
            "avatarUrl": "/avatars/0d95d65d30f6672ec09dc92155324d7f.svg",
            "isPro": false,
            "fullname": "Keqin Chen",
            "user": "chenkq",
            "type": "user"
          },
          "name": "Keqin Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:19.770Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205667",
          "user": {
            "_id": "649a3ba9342f14148357c367",
            "avatarUrl": "/avatars/81a769fa38b7384f382ff3cc10d6d624.svg",
            "isPro": false,
            "fullname": "Jialin Wang",
            "user": "JialinWang",
            "type": "user"
          },
          "name": "Jialin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:26.467Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205668",
          "name": "Yang Fan",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205669",
          "user": {
            "_id": "6712930f0fac3235c56edf5b",
            "avatarUrl": "/avatars/cafe7cb56ce7c3b2572f5f2d0b89357a.svg",
            "isPro": false,
            "fullname": "kai dang",
            "user": "1vk5i",
            "type": "user"
          },
          "name": "Kai Dang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:33.105Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566a",
          "name": "Bin Zhang",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566b",
          "name": "Xiong Wang",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566c",
          "user": {
            "_id": "62c6a751a71b40cf26f359a8",
            "avatarUrl": "/avatars/49abd2e71946035452c316d703baaac6.svg",
            "isPro": false,
            "fullname": "Yunfei Chu",
            "user": "faychu",
            "type": "user"
          },
          "name": "Yunfei Chu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:51.503Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566d",
          "user": {
            "_id": "620760a26e3b7210c2ff1943",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
            "isPro": false,
            "fullname": "Junyang Lin",
            "user": "JustinLin610",
            "type": "user"
          },
          "name": "Junyang Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:44.277Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/659e513ea9bc1f60189ac148/3pDelIehF3CWGPS0jrZvN.png"
      ],
      "publishedAt": "2025-03-26T04:17:55.000Z",
      "submittedOnDailyAt": "2025-03-27T05:14:09.555Z",
      "title": "Qwen2.5-Omni 기술보고서\n\nQwen2.5-Omni 기술보고서는 Qwen2.5-Omni 모델의 기술적 특성과 성능을 자세히 설명하고 있으며, 이는 AI 모델의 발전과 혁신을 보여주는 중요한 보고서입니다. 보고서는 Qwen2.5-Omni 모델의 개발 배경, 기술적 원리, 성능 평가, 그리고 실제 응용 사례를 포함하여, AI 분야의 전문가와 관련자들에게 유용한 정보를 제공합니다.",
      "submittedOnDailyBy": {
        "_id": "659e513ea9bc1f60189ac148",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659e513ea9bc1f60189ac148/DBDDqjGTQ0SvjWyuYu7py.jpeg",
        "isPro": false,
        "fullname": "YuanjunLv",
        "user": "Bakerbunker",
        "type": "user"
      },
      "summary": "이 보고서에서는 Qwen2.5-Omni, 한 단말에서 단말까지 다양한 특성을 인식하는 모델을 소개합니다. 이 모델은 텍스트, 이미지, 음성, 영상 등 다양한 모델을 동시에 인식하고 스트리밍 모드에서 자연스러운 음성 응답을 생성할 수 있습니다. 다양한 정보 입력을 스트리밍으로 하려면, 음성 인코더와 시각 인코더는 블록별로 처리 방식을 사용합니다. 영상 입력의 시간 스탬프와 음성의 동기화를 구현하기 위해, 음성과 영상을 교차하여 순서대로 배치하고, TMRoPE(Time-aligned Multimodal RoPE)라는 새로운 위치 매핑 접근법을 제안합니다. 텍스트와 음성을 동시에 생성하고 모델 간섭을 피하기 위해, Thinker-Talker 아키텍처를 제안합니다. 이 프레임워크에서, Thinker는 대규모 언어 모델로 텍스트 생성을 담당하고, Talker는 듀얼 트랙 자동 증폭 모델이며, Thinker로부터의 은닉 표현을 직접 사용하여 음성 토큰을 출력할 수 있습니다. Thinker와 Talker 모델은 단말에서 단말까지의 학습과 추론을 가능하게 합니다. 스트리밍 모드에서 음성 토큰을 디코딩하기 위해, 슬라이딩 윈도우 DiT을 도입하고, 입력 영역을 제한하고 초기 패킷 지연을 줄이는 것을 목표로 합니다. Qwen2.5-Omni는 같은 크기의 Qwen2.5-VL과 비교하여 상대적으로 같은 성능을 보여주고, Qwen2-Audio의 성능을 초과합니다. 또한, Qwen2.5-Omni는 Omni-Bench 등 다양한 벤치마크에서 가장 先端의 성능을 달성합니다. 특히, MMLU와 GSM8K 등 벤치마크에 의한 증거로, Qwen2.5-Omni의 단말에서 단말까지의 음성 지시 수행 성능은 텍스트 입력과 같은 성능을 나타냅니다. 음성 생성에서, Qwen2.5-Omni의 스트리밍 Talker는 현재의 스트리밍 및 비 스트리밍 옵션에 비해 강력한 강건성과 자연성을 보여주며, 많은 것을 초과합니다.",
      "upvotes": 17,
      "discussionId": "67e4f2527e97884ba42056df",
      "projectPage": "https://qwenlm.github.io/blog/qwen2.5-omni/",
      "githubRepo": "https://github.com/QwenLM/Qwen2.5-Omni",
      "ai_keywords": [
        "multimodal model",
        "block-wise processing",
        "interleaved manner",
        "TMRoPE (Time-aligned Multimodal RoPE)",
        "position embedding",
        "Thinker-Talker architecture",
        "large language model",
        "dual-track autoregressive model",
        "end-to-end manner",
        "sliding-window DiT",
        "receptive field",
        "initial package delay",
        "Omni-Bench",
        "MMLU",
        "GSM8K",
        "end-to-end speech instruction following"
      ]
    },
    "publishedAt": "2025-03-26T00:17:55.000Z",
    "title": "Qwen2.5-Omni Technical Report",
    "summary": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model\ndesigned to perceive diverse modalities, including text, images, audio, and\nvideo, while simultaneously generating text and natural speech responses in a\nstreaming manner. To enable the streaming of multimodal information inputs,\nboth audio and visual encoders utilize a block-wise processing approach. To\nsynchronize the timestamps of video inputs with audio, we organize the audio\nand video sequentially in an interleaved manner and propose a novel position\nembedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently\ngenerate text and speech while avoiding interference between the two\nmodalities, we propose Thinker-Talker architecture. In this framework,\nThinker functions as a large language model tasked with text generation, while\nTalker is a dual-track autoregressive model that directly utilizes the hidden\nrepresentations from the Thinker to produce audio tokens as output. Both the\nThinker and Talker models are designed to be trained and inferred in an\nend-to-end manner. For decoding audio tokens in a streaming manner, we\nintroduce a sliding-window DiT that restricts the receptive field, aiming to\nreduce the initial package delay. Qwen2.5-Omni is comparable with the similarly\nsized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni\nachieves state-of-the-art performance on multimodal benchmarks like Omni-Bench.\nNotably, Qwen2.5-Omni's performance in end-to-end speech instruction following\nis comparable to its capabilities with text inputs, as evidenced by benchmarks\nsuch as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming\nTalker outperforms most existing streaming and non-streaming alternatives in\nrobustness and naturalness.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/659e513ea9bc1f60189ac148/3pDelIehF3CWGPS0jrZvN.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20215.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "659e513ea9bc1f60189ac148",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659e513ea9bc1f60189ac148/DBDDqjGTQ0SvjWyuYu7py.jpeg",
      "fullname": "YuanjunLv",
      "name": "Bakerbunker",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20314",
      "authors": [
        {
          "_id": "67e4b65a080a33e3955b340c",
          "name": "WanTeam",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b340e",
          "user": {
            "_id": "63f1f1727ddf724fbcbc9c7e",
            "avatarUrl": "/avatars/9e0516d9b1036c23c78f313c79872f55.svg",
            "isPro": false,
            "fullname": "Ang Wang",
            "user": "ang-annng",
            "type": "user"
          },
          "name": "Ang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:47:28.144Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b340f",
          "user": {
            "_id": "64755ff5a51711a3b59118af",
            "avatarUrl": "/avatars/2e899088902db94e785107c3ec2abe85.svg",
            "isPro": false,
            "fullname": "Baole Ai",
            "user": "baoleai",
            "type": "user"
          },
          "name": "Baole Ai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:47:49.260Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3410",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3411",
          "user": {
            "_id": "6458970cab9a44f42f620a80",
            "avatarUrl": "/avatars/f9779b0621c931f922440fec95342444.svg",
            "isPro": false,
            "fullname": "chaojie mao",
            "user": "chaojiemao",
            "type": "user"
          },
          "name": "Chaojie Mao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:02.730Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3412",
          "user": {
            "_id": "66592c72f4124d863fd55574",
            "avatarUrl": "/avatars/98f0d5e6ba3728e8a1164aa5188a3298.svg",
            "isPro": false,
            "fullname": "Chenwei Xie",
            "user": "chenweix7",
            "type": "user"
          },
          "name": "Chen-Wei Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:10.933Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3413",
          "name": "Di Chen",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3414",
          "name": "Feiwu Yu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3415",
          "user": {
            "_id": "67a73767282aa06f7bcaeeb1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/J28OVrPhD0xYulWMgICmW.png",
            "isPro": false,
            "fullname": "Haiming Zhao",
            "user": "HermanZ",
            "type": "user"
          },
          "name": "Haiming Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:26.135Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3416",
          "user": {
            "_id": "651441e92c5da979038df5ee",
            "avatarUrl": "/avatars/85cdafcccb522eced50dc9e4770b630a.svg",
            "isPro": false,
            "fullname": "Jianxiao Yang",
            "user": "Jianxiao0203",
            "type": "user"
          },
          "name": "Jianxiao Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:33.714Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3417",
          "user": {
            "_id": "6274b866f978441a764b30f6",
            "avatarUrl": "/avatars/953b1ff82f63e371a7358a85d68304cd.svg",
            "isPro": false,
            "fullname": "jianyuan.zengjy",
            "user": "filwsyl",
            "type": "user"
          },
          "name": "Jianyuan Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:40.108Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3418",
          "name": "Jiayu Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3419",
          "user": {
            "_id": "66f0e0262aee3cb7e981bbac",
            "avatarUrl": "/avatars/f8f1e70469b5e047dc6e0e9dec6c5bc1.svg",
            "isPro": false,
            "fullname": "Jingfeng Zhang",
            "user": "jingfengzhang",
            "type": "user"
          },
          "name": "Jingfeng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:58.316Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341a",
          "user": {
            "_id": "602f88f5e8149a962412a667",
            "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
            "isPro": false,
            "fullname": "Zhou",
            "user": "Jingren",
            "type": "user"
          },
          "name": "Jingren Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:49:09.146Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341b",
          "user": {
            "_id": "627c93b2bec91eb1720b8bad",
            "avatarUrl": "/avatars/89c31c71aa5027543ed5be0471fe1109.svg",
            "isPro": false,
            "fullname": "Jinkai Wang",
            "user": "zwsjink",
            "type": "user"
          },
          "name": "Jinkai Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:49:15.680Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341c",
          "user": {
            "_id": "6465941d0e6c7618f615675b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6465941d0e6c7618f615675b/W4EHqlCucz_bojFLFEeV_.jpeg",
            "isPro": false,
            "fullname": "Jixuan Chen",
            "user": "Mayome",
            "type": "user"
          },
          "name": "Jixuan Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:49:25.437Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341d",
          "name": "Kai Zhu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341e",
          "name": "Kang Zhao",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341f",
          "name": "Keyu Yan",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3420",
          "name": "Lianghua Huang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3421",
          "user": {
            "_id": "63b4ec15103617b0a5b3101e",
            "avatarUrl": "/avatars/e6faad833b31ad5d892faccf621e7a34.svg",
            "isPro": false,
            "fullname": "Mengyang Feng",
            "user": "archerfmy",
            "type": "user"
          },
          "name": "Mengyang Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:50:01.919Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3422",
          "user": {
            "_id": "66eae63f533fd44f8a8ca60b",
            "avatarUrl": "/avatars/38cecb4c80cc7a6e63028fcb572e3a22.svg",
            "isPro": false,
            "fullname": "Zhang Ningyi",
            "user": "ZhangNy",
            "type": "user"
          },
          "name": "Ningyi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:50:13.628Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3423",
          "name": "Pandeng Li",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3424",
          "user": {
            "_id": "64c5182771947b03ffee931c",
            "avatarUrl": "/avatars/478f4e06ac1bced092dde0f11963a975.svg",
            "isPro": false,
            "fullname": "Wupingyu",
            "user": "wpy1999",
            "type": "user"
          },
          "name": "Pingyu Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:50:38.625Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3425",
          "user": {
            "_id": "642e3bcb958faf258a40e89c",
            "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
            "isPro": false,
            "fullname": "Ruihang Chu",
            "user": "Ruihang",
            "type": "user"
          },
          "name": "Ruihang Chu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:50:46.771Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3426",
          "user": {
            "_id": "6790e2b74932687e24024b4a",
            "avatarUrl": "/avatars/951f55648490e1f520483a3e425621dd.svg",
            "isPro": false,
            "fullname": "Ruili",
            "user": "RuiliFeng",
            "type": "user"
          },
          "name": "Ruili Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:03.191Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3427",
          "name": "Shiwei Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3428",
          "user": {
            "_id": "62bbf42ac9633b01802a6d45",
            "avatarUrl": "/avatars/0fee1462d228f5e7f22d5c240900a3ad.svg",
            "isPro": false,
            "fullname": "Siyang Sun",
            "user": "sunsiyang",
            "type": "user"
          },
          "name": "Siyang Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:10.461Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3429",
          "name": "Tao Fang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342a",
          "name": "Tianxing Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342b",
          "name": "Tianyi Gui",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342c",
          "name": "Tingyu Weng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342d",
          "name": "Tong Shen",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342e",
          "name": "Wei Lin",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342f",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3430",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3431",
          "user": {
            "_id": "623c6253389748c9f72ca287",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654828369523-623c6253389748c9f72ca287.jpeg",
            "isPro": false,
            "fullname": "wenmeng zhou",
            "user": "wenmengzhou",
            "type": "user"
          },
          "name": "Wenmeng Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:38.310Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3432",
          "user": {
            "_id": "644240b1251730a7ee243ef3",
            "avatarUrl": "/avatars/c4ca99739e2b6f3d3d0ca83ecc54766a.svg",
            "isPro": false,
            "fullname": "wente.wang",
            "user": "shiftc",
            "type": "user"
          },
          "name": "Wente Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:46.041Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3433",
          "user": {
            "_id": "64af91eb5c17fe25cfcbebc3",
            "avatarUrl": "/avatars/ffc6e7b6a40300e05e66f544264dddbc.svg",
            "isPro": false,
            "fullname": "Wenting Shen",
            "user": "SeventeenSSS",
            "type": "user"
          },
          "name": "Wenting Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:53.298Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3434",
          "name": "Wenyuan Yu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3435",
          "user": {
            "_id": "642e19b26748dd4f8eea1321",
            "avatarUrl": "/avatars/a534e61c21d2fb3c7a4c4d4dba98fafb.svg",
            "isPro": false,
            "fullname": "Xianzhong Shi",
            "user": "itutor",
            "type": "user"
          },
          "name": "Xianzhong Shi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:19.514Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3436",
          "user": {
            "_id": "65105ab08c4b535a97052fe8",
            "avatarUrl": "/avatars/a97862045a26a74ca33d1a47b6a1f2b4.svg",
            "isPro": false,
            "fullname": "xiaominghuang",
            "user": "xiaominghuang",
            "type": "user"
          },
          "name": "Xiaoming Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:52:03.599Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3437",
          "name": "Xin Xu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3438",
          "name": "Yan Kou",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3439",
          "name": "Yangyu Lv",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343a",
          "name": "Yifei Li",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343b",
          "user": {
            "_id": "67d39e61943a965360fbbc0c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-JwILFmblPdd6Sv28c1J7.png",
            "isPro": false,
            "fullname": "yijing liu",
            "user": "86diphda",
            "type": "user"
          },
          "name": "Yijing Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:52:18.647Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343c",
          "name": "Yiming Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343d",
          "name": "Yingya Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343e",
          "name": "Yitong Huang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343f",
          "name": "Yong Li",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3440",
          "name": "You Wu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3441",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3442",
          "name": "Yulin Pan",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3443",
          "name": "Yun Zheng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3444",
          "name": "Yuntao Hong",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3445",
          "name": "Yupeng Shi",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3446",
          "name": "Yutong Feng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3447",
          "name": "Zeyinzi Jiang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3448",
          "name": "Zhen Han",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3449",
          "name": "Zhi-Fan Wu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b344a",
          "name": "Ziyu Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T08:25:43.000Z",
      "submittedOnDailyAt": "2025-03-27T00:52:37.426Z",
      "title": "1. \"Open and Advanced Large-Scale Video Generation Model\"",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "이 보고서에서 Wan이라는 유용하고 개방적인 비디오 기반 모델 시트를 소개합니다. 이 모델은 비디오 생성의 경계를 초월하기 위해 설계되었습니다. 주요 분산 트랜스포머 패러다임에 기반하여 구축되었으며, Wan은 새로운 VAE, scalable의 사전 학습 전략, 큰 데이터 수집, 마이너스틱 평가 지표의 자동 계산 등 시리즈의 혁신을 통해 생성 능력에 대한 중요한 진전을 달성했습니다. 이러한 기여는 모델의 성능과 다양성을 향상시킵니다. 특히, Wan은 다음과 같은 4가지 특징을 가지고 있습니다: \n\n- 리더십 성능: Wan의 14B 모델은 수조의 이미지와 비디오를 포함하는 큰 데이터 세트를 통해 훈련되었으며, 비디오 생성의 규모 법칙을 보여주고 있습니다. 내부와 외부의 여러 벤치마크에서 현재의 오픈 소스 모델과 가장 선진한 상업 솔루션을 초월하며, 명확하고 중요한 성능의 상위를 보여주고 있습니다.\n- 세부성: Wan은 두 가지 능력 있는 모델을 제공하며, 1.3B와 14B 파라미터 각각 효율성과 유효성을 보여주고 있습니다. 또한 이미지에서 비디오, 지시에 따른 비디오 편집, 비즈니스용 비디오 생성 등 다양한 하류 애플리케이션을 처리합니다.\n- 소비자 수준의 효율성: 1.3B 모델은 특별한 리소스 효율을 보여주며, 8.19GB VRAM만 필요합니다. 이는 광범위한 소비자 수준의 GPU에 대응합니다.\n- 개방성: Wan의 전체 시리즈를 오픈 소스화하며, 소스 코드와 모든 모델을 포함하며 비디오 생성 커뮤니티의 성장을 촉진하기 위해 목표를 가지고 있습니다. 이러한 개방성은 업계에서 비디오 제작의 창의적인 가능성을 크게 확장하고, 고품질의 비디오 기반 모델을 학계에 제공하기 위해 목표를 가지고 있습니다. 모든 코드와 모델은 https://github.com/Wan-Video/Wan2.1에서 사용 가능합니다.",
      "upvotes": 16,
      "discussionId": "67e4b663080a33e3955b371a",
      "ai_keywords": [
        "diffusion transformer",
        "VAE",
        "large-scale data curation",
        "automated evaluation metrics",
        "scaling laws",
        "image-to-video",
        "instruction-guided video editing",
        "personal video generation"
      ]
    },
    "publishedAt": "2025-03-26T04:25:43.000Z",
    "title": "Wan: Open and Advanced Large-Scale Video Generative Models",
    "summary": "This report presents Wan, a comprehensive and open suite of video foundation\nmodels designed to push the boundaries of video generation. Built upon the\nmainstream diffusion transformer paradigm, Wan achieves significant\nadvancements in generative capabilities through a series of innovations,\nincluding our novel VAE, scalable pre-training strategies, large-scale data\ncuration, and automated evaluation metrics. These contributions collectively\nenhance the model's performance and versatility. Specifically, Wan is\ncharacterized by four key features: Leading Performance: The 14B model of Wan,\ntrained on a vast dataset comprising billions of images and videos,\ndemonstrates the scaling laws of video generation with respect to both data and\nmodel size. It consistently outperforms the existing open-source models as well\nas state-of-the-art commercial solutions across multiple internal and external\nbenchmarks, demonstrating a clear and significant performance superiority.\nComprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B\nparameters, for efficiency and effectiveness respectively. It also covers\nmultiple downstream applications, including image-to-video, instruction-guided\nvideo editing, and personal video generation, encompassing up to eight tasks.\nConsumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource\nefficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range\nof consumer-grade GPUs. Openness: We open-source the entire series of Wan,\nincluding source code and all models, with the goal of fostering the growth of\nthe video generation community. This openness seeks to significantly expand the\ncreative possibilities of video production in the industry and provide academia\nwith high-quality video foundation models. All the code and models are\navailable at https://github.com/Wan-Video/Wan2.1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20314.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20201",
      "authors": [
        {
          "_id": "67e4b04c8c0347025bd0fe84",
          "user": {
            "_id": "6109bc89e84ad84682a69754",
            "avatarUrl": "/avatars/067aac8784320d4e8e875379dc4cc209.svg",
            "isPro": false,
            "fullname": "Salaheddin Alzubi",
            "user": "salzubi401",
            "type": "user"
          },
          "name": "Salaheddin Alzubi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:03.915Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe85",
          "user": {
            "_id": "673f945e6cd62dbd4b02790d",
            "avatarUrl": "/avatars/3742e4e6b88d4f8b78d5c5308f55773e.svg",
            "isPro": false,
            "fullname": "Creston Brooks",
            "user": "cabxyz",
            "type": "user"
          },
          "name": "Creston Brooks",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-27T01:56:28.853Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe86",
          "user": {
            "_id": "666619508a270cedd594e55e",
            "avatarUrl": "/avatars/79bb2b09a663cae555140ec9379f05d9.svg",
            "isPro": false,
            "fullname": "Purva Chiniya",
            "user": "pchiniya",
            "type": "user"
          },
          "name": "Purva Chiniya",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:10.184Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe87",
          "name": "Edoardo Contente",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe88",
          "name": "Chiara von Gerlach",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe89",
          "user": {
            "_id": "62296d3f2df798b7e951e475",
            "avatarUrl": "/avatars/661c23416c3d418e2996f9b9a024db82.svg",
            "isPro": false,
            "fullname": "Lucas Irwin",
            "user": "ljirwin",
            "type": "user"
          },
          "name": "Lucas Irwin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:26.927Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8a",
          "name": "Yihan Jiang",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8b",
          "user": {
            "_id": "67759bf644ceb61f96739324",
            "avatarUrl": "/avatars/44cb431a9cbc73e060aff7d90435c42d.svg",
            "isPro": false,
            "fullname": "Arda Kaz",
            "user": "speedyarda",
            "type": "user"
          },
          "name": "Arda Kaz",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:57.570Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8c",
          "user": {
            "_id": "64b98bcf842aa47891bc0f63",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/L-smrOCQ3MXtvnISJqmxJ.png",
            "isPro": false,
            "fullname": "Windsor Nguyen",
            "user": "windsornguyen",
            "type": "user"
          },
          "name": "Windsor Nguyen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:50.840Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8d",
          "user": {
            "_id": "6756dffd88428044e2ddbdd9",
            "avatarUrl": "/avatars/41dbb83c68b56546cdf8e34379faf6b3.svg",
            "isPro": false,
            "fullname": "Sewoong Oh",
            "user": "sewoong79",
            "type": "user"
          },
          "name": "Sewoong Oh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:55:20.604Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8e",
          "user": {
            "_id": "65f86cc77b704590d4a5439f",
            "avatarUrl": "/avatars/1a828cf755839f058241fb19ca83341f.svg",
            "isPro": false,
            "fullname": "Himanshu Tyagi",
            "user": "HimanshuTyagi",
            "type": "user"
          },
          "name": "Himanshu Tyagi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:55:27.139Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8f",
          "name": "Pramod Viswanath",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T03:51:32.000Z",
      "submittedOnDailyAt": "2025-03-27T00:26:59.804Z",
      "title": "Open Deep Search: 오픈소스의 이유 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과 효과",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "ODS를 소개합니다. ODS는 Perplexity의 Sonar Reasoning Pro와 OpenAI의 GPT-4o Search Preview과 같은 소유의 검색 AI 솔루션과 그 오픈 소스 버전 사이의 차이를 줄이는 것을 목표로 합니다. ODS에서, 최신의 오픈 소스 LLM의 추론 능력을 강화하기 위해 추론 에이전트를 사용하여 웹 검색 도구를 신중하게 사용하여 질문에 답하는 것이 주요한 혁신입니다. 구체적으로, ODS는 사용자가 선택한 베이스 LLM와 협력하여 작동하는 2개의 구성 요소로 이루어집니다: Open Search Tool와 Open Reasoning Agent. Open Reasoning Agent는 주어진 태스크를 이해하고 행렬을 작성하여 태스크를 완료합니다. 이 행렬에는 Open Search Tool를 포함하는 턴이 있습니다. Open Search Tool는 소유 버전의 소프트웨어보다 뛰어난 새로운 웹 검색 도구입니다. ODS는 DeepSeek-R1과 같은 강력한 오픈 소스 추론 LLM을 조합하여 SimpleQA와 FRAMES의 2개의 벤치마크에서 현재의 가장 先端의 베이스 라인과 가까운, 때로는 뛰어넘는 성능을 갖습니다. 예를 들어, FRAMES의 평가 벤치마크에서 ODS는 최근 발표된 GPT-4o Search Preview의 가장 좋은 베이스 라인을 9.7% 증가시키고 정확도를 향상시킵니다. ODS는 더 높은 성능을 달성하기 위해 어떤 LLM에도 세미 프레임 워크를 제공하고 검색 및 추론 능력을 추가합니다. 예를 들어, DeepSeek-R1은 SimpleQA에서 82.4%, FRAMES에서 30.1%의 성능을 달성하지만, ODS를 사용하면 SimpleQA에서 88.3%, FRAMES에서 75.3%의 가장 先端의 성능을 갖습니다.",
      "upvotes": 12,
      "discussionId": "67e4b04c8c0347025bd0fed2",
      "ai_keywords": [
        "LLMs",
        "reasoning agents",
        "web search tools",
        "Open Search Tool",
        "Open Reasoning Agent",
        "DeepSeek-R1",
        "SimpleQA",
        "FRAMES",
        "GPT-4o Search Preview"
      ]
    },
    "publishedAt": "2025-03-25T23:51:32.000Z",
    "title": "Open Deep Search: Democratizing Search with Open-source Reasoning Agents",
    "summary": "We introduce Open Deep Search (ODS) to close the increasing gap between the\nproprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and\nOpenAI's GPT-4o Search Preview, and their open-source counterparts. The main\ninnovation introduced in ODS is to augment the reasoning capabilities of the\nlatest open-source LLMs with reasoning agents that can judiciously use web\nsearch tools to answer queries. Concretely, ODS consists of two components that\nwork with a base LLM chosen by the user: Open Search Tool and Open Reasoning\nAgent. Open Reasoning Agent interprets the given task and completes it by\norchestrating a sequence of actions that includes calling tools, one of which\nis the Open Search Tool. Open Search Tool is a novel web search tool that\noutperforms proprietary counterparts. Together with powerful open-source\nreasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses\nthe existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES.\nFor example, on the FRAMES evaluation benchmark, ODS improves the best existing\nbaseline of the recently released GPT-4o Search Preview by 9.7% in accuracy.\nODS is a general framework for seamlessly augmenting any LLMs -- for example,\nDeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search\nand reasoning capabilities to achieve state-of-the-art performance: 88.3% on\nSimpleQA and 75.3% on FRAMES.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20201.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19480",
      "authors": [
        {
          "_id": "67e3693eebafaa1efbed08d2",
          "user": {
            "_id": "67d30d9ae45dc43004b31425",
            "avatarUrl": "/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg",
            "isPro": false,
            "fullname": "Shijie Ma",
            "user": "msj9817",
            "type": "user"
          },
          "name": "Shijie Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-26T20:44:37.274Z",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d3",
          "user": {
            "_id": "6455cc8f654d8bccae50e4d4",
            "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
            "isPro": false,
            "fullname": "Yuying Ge",
            "user": "tttoaster",
            "type": "user"
          },
          "name": "Yuying Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:55:42.975Z",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d4",
          "name": "Teng Wang",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d5",
          "user": {
            "_id": "67bc21106a3e748d80d11dc7",
            "avatarUrl": "/avatars/fbc7e76a3266a3c06d03e85db96a51cf.svg",
            "isPro": false,
            "fullname": "yuxin guo",
            "user": "aether25",
            "type": "user"
          },
          "name": "Yuxin Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:56:02.897Z",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d6",
          "user": {
            "_id": "640e9762b03f4cd29f58d982",
            "avatarUrl": "/avatars/81da37d628163fe3e094b247c7c3a3b5.svg",
            "isPro": false,
            "fullname": "Yixiao Ge",
            "user": "yxgeee",
            "type": "user"
          },
          "name": "Yixiao Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:56:08.653Z",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d7",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:56:14.275Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T09:15:34.000Z",
      "submittedOnDailyAt": "2025-03-27T01:26:37.430Z",
      "title": "GenHancer: 불완전한 생성 모형은 은닉적으로 강력한 것입니다\n  비전 시어 캐너스 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범 앨범",
      "submittedOnDailyBy": {
        "_id": "67d30d9ae45dc43004b31425",
        "avatarUrl": "/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg",
        "isPro": false,
        "fullname": "Shijie Ma",
        "user": "msj9817",
        "type": "user"
      },
      "summary": "생성 모형과 판별 모형의 융합이 주목을 받고 있습니다. 판별적 Contrastive Language-Image Pre-Training (CLIP)는 고수준의 언어적 의미를 뛰어넘지만, 미묘한 시각적 세부 사항에 대한 이해는 어려움이 있습니다. 일반적으로 표현을 향상시키기 위해 생성 모형은 CLIP의 시각적 특징을 재구성 조건으로 사용됩니다. 그러나 잠재적 원리는 조사가 부족합니다. 본 논문에서, 시각적으로 완전한 생성은 표현 향상에 항상 최적이 아님을 실험적으로 발견했습니다. 본질은 생성 모형에서 미묘한 지식을 효과적으로 추출하는 동시에 관련없는 정보를 억제하는 것입니다. 중요한 요인을 조사하기 위해 3가지 측면에서 검토했습니다: 1. 조건 기구: 소수 단위의 국부 토큰의 사용은 재구성의 어려움을 크게 줄였으며, 학습이 붕괴되는 것을 발견했습니다. 따라서, 전역적인 시각 토큰만 조건으로 삼는 것이 가장 효과적이라는 결론을 내렸습니다. 2. 디노이저의 설정: 끝말 정보를 포함하는 학습을 발견했습니다. 이에대해, 2단계 학습 전략을 제안하고, 유용한 시각적 지식을 학습하는 것을 우선시합니다. 또한, 가벼운 디노이저가 놀라운 개선을 보여주는 것을 확인했습니다. 3. 생성 패러다임: 연속적이고 분산적인 디노이저를 시도하고 전체적 적용성을 파악했습니다. 심도있는 검토를 통해 최적의 방법을 GenHancer로 제안하고, MMVP-VLM 벤치마크에서 기존 기술을 초월할 수 있었습니다 (예를 들어, OpenAI CLIP에서 6.0%의 개선). 강화된 CLIP은 다양한 언어 모델에 플러그인하여 시각적 시나리오의 성능을 향상시킬 수 있습니다. 모든 모델과 코드는 공개되어 있습니다.",
      "upvotes": 11,
      "discussionId": "67e36940ebafaa1efbed0951",
      "projectPage": "https://mashijie1028.github.io/GenHancer/",
      "githubRepo": "https://github.com/mashijie1028/GenHancer",
      "ai_keywords": [
        "generative",
        "discriminative",
        "Contrastive Language-Image Pre-Training (CLIP)",
        "visual features",
        "reconstruction",
        "local tokens",
        "global visual tokens",
        "Conditioning mechanisms",
        "end-to-end training",
        "denoising configurations",
        "two-stage training",
        "lightweight denoisers",
        "continuous denoisers",
        "discrete denoisers",
        "Generation paradigms",
        "GenHancer",
        "MMVP-VLM benchmark",
        "OpenAICLIP",
        "multimodal large language models",
        "vision-centric performance"
      ]
    },
    "publishedAt": "2025-03-25T05:15:34.000Z",
    "title": "GenHancer: Imperfect Generative Models are Secretly Strong\n  Vision-Centric Enhancers",
    "summary": "The synergy between generative and discriminative models receives growing\nattention. While discriminative Contrastive Language-Image Pre-Training (CLIP)\nexcels in high-level semantics, it struggles with perceiving fine-grained\nvisual details. Generally, to enhance representations, generative models take\nCLIP's visual features as conditions for reconstruction. However, the\nunderlying principle remains underexplored. In this work, we empirically found\nthat visually perfect generations are not always optimal for representation\nenhancement. The essence lies in effectively extracting fine-grained knowledge\nfrom generative models while mitigating irrelevant information. To explore\ncritical factors, we delve into three aspects: (1) Conditioning mechanisms: We\nfound that even a small number of local tokens can drastically reduce the\ndifficulty of reconstruction, leading to collapsed training. We thus conclude\nthat utilizing only global visual tokens as conditions is the most effective\nstrategy. (2) Denoising configurations: We observed that end-to-end training\nintroduces extraneous information. To address this, we propose a two-stage\ntraining strategy to prioritize learning useful visual knowledge. Additionally,\nwe demonstrate that lightweight denoisers can yield remarkable improvements.\n(3) Generation paradigms: We explore both continuous and discrete denoisers\nwith desirable outcomes, validating the versatility of our method. Through our\nin-depth explorations, we have finally arrived at an effective method, namely\nGenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark,\ne.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into\nmultimodal large language models for better vision-centric performance. All the\nmodels and codes are made publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19480.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67d30d9ae45dc43004b31425",
      "avatarUrl": "/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg",
      "fullname": "Shijie Ma",
      "name": "msj9817",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20672",
      "authors": [
        {
          "_id": "67e4b82c672b3d9c9cb42c70",
          "name": "Yuyang Peng",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c71",
          "name": "Shishi Xiao",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c72",
          "user": {
            "_id": "66bf00ca5b4e241fe266059d",
            "avatarUrl": "/avatars/f3eedfecf5baa8e2ac80d37abe42c63f.svg",
            "isPro": false,
            "fullname": "Keming Wu",
            "user": "wukeming11",
            "type": "user"
          },
          "name": "Keming Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:20.576Z",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c73",
          "user": {
            "_id": "672894ff1905afcdc9132fc6",
            "avatarUrl": "/avatars/78297eadad816c45d680aa70cea3b973.svg",
            "isPro": false,
            "fullname": "QISHENG LIAO",
            "user": "Marseclipse",
            "type": "user"
          },
          "name": "Qisheng Liao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:14.624Z",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c74",
          "user": {
            "_id": "64ba249e5c4deebf69aa17fd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/h7YdQe9wEr1TTJfP1ALhb.jpeg",
            "isPro": false,
            "fullname": "chen",
            "user": "bohanChen",
            "type": "user"
          },
          "name": "Bohan Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:07.459Z",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c75",
          "user": {
            "_id": "6298fd95b58e71e2ac9f3ad8",
            "avatarUrl": "/avatars/7d34644d537bc5c17cf1e4ce4095355c.svg",
            "isPro": false,
            "fullname": "Kevin Lin",
            "user": "kevinlin311tw",
            "type": "user"
          },
          "name": "Kevin Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:01.691Z",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c76",
          "name": "Danqing Huang",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c77",
          "name": "Ji Li",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c78",
          "user": {
            "_id": "631f108bb45367a05fe74260",
            "avatarUrl": "/avatars/c20da6800b643728062712d9a2771648.svg",
            "isPro": false,
            "fullname": "Researcher",
            "user": "YuanYuhui",
            "type": "user"
          },
          "name": "Yuhui Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:56:39.210Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T16:04:57.000Z",
      "submittedOnDailyAt": "2025-03-27T01:00:23.038Z",
      "title": "BizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽의 기사레벨의 시각화텍스트표시의 진보\n\nBizGen: 정보그래픽",
      "submittedOnDailyBy": {
        "_id": "62333a88fd7bb4a39b92d387",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
        "isPro": false,
        "fullname": "Alex Jinpeng Wang",
        "user": "Awiny",
        "type": "user"
      },
      "summary": "최근, 가장 先進的な텍스트에서 이미지 생성 모델과 같이 Flux, Ideogram 2.0은 문서 수준의 시각적인 텍스트 렌더링에 있어서 중요한 발전을 달성하고 있습니다. 본 논문에서는, 더 어려운 시나리오인 비즈니스 수준의 시각적인 텍스트 렌더링에 초점을 맞추고, 사용자가 제공하는 비즈니스 수준의 설명 프로ン푹과 초고밀도 레이아웃을 기반으로, 고품질의 비즈니스 콘텐츠(정보그래픽, 슬라이드 등)를 생성하는 새로운 태스크를 해결합니다. 기본적인 문제는 두 가지입니다: 긴 컨텍스트 길이와 고품질의 비즈니스 콘텐츠 데이터의 부족입니다.\n\n이러한 프로그램과 비교하여, 비즈니스 콘텐츠에 있어서, 다수의 서브 라이언스와 문서 수준의 프로ン푹을 초점을 맞추는 것이超高밀도 레이아웃을 정확하게 준수하는 것보다 더 어려울 것입니다. 우리는 두 가지 주요 기술적 기여를 수행합니다: 이는 Infographics-650K의 scalable한 고품질의 비즈니스 콘텐츠 데이터셋의 구축이며, 초고밀도 레이아웃과 프로ン푹을 준비하기 위해 레이어별로 검색 어쩌구 어쩌구 정보그래픽 생성 셈을 구현합니다. 그리고, 서브 라이언스 잠재 공간에 삽입하고, 추론 중에 서브 라이언스를 유연하게 정밀화하는 라이언스 가이드된 교차 注意 셈입니다.\n\n우리의 시스템의 강력한 결과를 비즈니스 수준의 프로ン푹 세트에서 SORT DARK 시스템(Flux, SD3)과 비교하여 보여줍니다. 또한, 각 컴포넌트의 효과를 확인하기 위해, Detailed removal experiments를 수행합니다. Infographics-650K와 BizEval을 구축하여 비즈니스 콘텐츠 생성의 발전을 촉진하는 것을 기대합니다.",
      "upvotes": 7,
      "discussionId": "67e4b831672b3d9c9cb42ebb",
      "ai_keywords": [
        "scalable, high-quality business content dataset",
        "Infographics-650K",
        "layer-wise retrieval-augmented infographic generation scheme",
        "layout-guided cross attention scheme",
        "cropped region latent space",
        "layout conditional CFG",
        "BizEval prompt set",
        "ablation experiments"
      ]
    },
    "publishedAt": "2025-03-26T12:04:57.000Z",
    "title": "BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation",
    "summary": "Recently, state-of-the-art text-to-image generation models, such as Flux and\nIdeogram 2.0, have made significant progress in sentence-level visual text\nrendering. In this paper, we focus on the more challenging scenarios of\narticle-level visual text rendering and address a novel task of generating\nhigh-quality business content, including infographics and slides, based on user\nprovided article-level descriptive prompts and ultra-dense layouts. The\nfundamental challenges are twofold: significantly longer context lengths and\nthe scarcity of high-quality business content data.\n  In contrast to most previous works that focus on a limited number of\nsub-regions and sentence-level prompts, ensuring precise adherence to\nultra-dense layouts with tens or even hundreds of sub-regions in business\ncontent is far more challenging. We make two key technical contributions: (i)\nthe construction of scalable, high-quality business content dataset, i.e.,\nInfographics-650K, equipped with ultra-dense layouts and prompts by\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\nand (ii) a layout-guided cross attention scheme, which injects tens of\nregion-wise prompts into a set of cropped region latent space according to the\nultra-dense layouts, and refine each sub-regions flexibly during inference\nusing a layout conditional CFG.\n  We demonstrate the strong results of our system compared to previous SOTA\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\nconduct thorough ablation experiments to verify the effectiveness of each\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\nthe broader community to advance the progress of business content generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20672.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20757",
      "authors": [
        {
          "_id": "67e4c08fd9b7021d4a600fa4",
          "user": {
            "_id": "662b4e3bc709a61df840fda1",
            "avatarUrl": "/avatars/fc73c63a4e1f8fbb084ec43ec9af0af0.svg",
            "isPro": false,
            "fullname": "Hu Yunhai",
            "user": "AlexCCtop",
            "type": "user"
          },
          "name": "Yunhai Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:51.052Z",
          "hidden": false
        },
        {
          "_id": "67e4c08fd9b7021d4a600fa5",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-27T03:05:54.275Z",
          "hidden": false
        },
        {
          "_id": "67e4c08fd9b7021d4a600fa6",
          "user": {
            "_id": "660103ec4ae78d4ded4633fc",
            "avatarUrl": "/avatars/efce106d70f5d092bf44d0638aa49984.svg",
            "isPro": false,
            "fullname": "CHEN Zhao",
            "user": "chenzhao",
            "type": "user"
          },
          "name": "Chen Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:04.608Z",
          "hidden": false
        },
        {
          "_id": "67e4c08fd9b7021d4a600fa7",
          "user": {
            "_id": "5f5ba21188f57f65f951f255",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1599840760465-noauth.png",
            "isPro": false,
            "fullname": "Arman Cohan",
            "user": "armanc",
            "type": "user"
          },
          "name": "Arman Cohan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:57.092Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:46:08.000Z",
      "submittedOnDailyAt": "2025-03-27T01:36:43.674Z",
      "title": "MCTS-RAG: Monte Carlo 트리를 이용한 검색 강화 생성\n\n(注意：虽然要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，我添加了“注意”以提醒读者翻译的准确性。)",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "MCTS-RAG는 작은 언어 모델의 추론 능력을 강화하는 새로운 접근법이다. 이 접근법은 RAG(Retrieval-Augmented Generation)를 사용하여 관련 컨텍스트를 제공하고, MCTS(Monte Carlo Tree Search)를 사용하여 추론 경로를 정밀화함으로써 지식밀도형 태스크에 대응한다. MCTS-RAG는 반복적인 의사결정 프로세스를 통해 검토와 추론을 동적으로 통합한다. 표준 RAG 방법과 달리, MCTS-RAG는 검토와 추론을 독립적으로 추출하여 지식이 최적적으로 통합되지 않는다. 또한 전통적인 MCTS 추론은 내부 모델의 지식만 의존하고 외부 사실에 포함되지 않기 때문에, MCTS-RAG는 구조화된 추론과 적응적인 검토를 결합한다. 이 통합 접근법은 의사결정을 강화하고 해로노미를 줄이고 사실적인 정확성과 응답의 일관성을 보장한다. ComplexWebQA, GPQA, FoolMeTwice 등 여러 추론과 지식밀도형 데이터셋의 실험 결과를 통해, 우리의 방법은 작은 LMs를 사용하여 GPT-4o와 같은 수준의 성능을 달성할 수 있으며, 작은 모델의 추론에서 새로운 기준을 세우고 있다.",
      "upvotes": 6,
      "discussionId": "67e4c092d9b7021d4a60108b",
      "ai_keywords": [
        "MCTS-RAG",
        "retrieval-augmented generation (RAG)",
        "Monte Carlo Tree Search (MCTS)",
        "reasoning paths",
        "iterative decision-making",
        "knowledge suboptimally",
        "structured reasoning",
        "adaptive retrieval",
        "decision-making",
        "hallucinations",
        "factual accuracy",
        "response consistency",
        "ComplexWebQA",
        "GPQA",
        "FoolMeTwice",
        "small-scale LMs",
        "frontier LLMs (GPT-4o)",
        "scaling inference-time compute"
      ]
    },
    "publishedAt": "2025-03-26T13:46:08.000Z",
    "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search",
    "summary": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20757.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20020",
      "authors": [
        {
          "_id": "67e4b288fa81c69f446da710",
          "name": "Gemini Robotics Team",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da711",
          "user": {
            "_id": "61fd7ac3fbafe89f48101d83",
            "avatarUrl": "/avatars/cb2c86a04574498e71d6c447c2b289c1.svg",
            "isPro": false,
            "fullname": "Saminda Abeyruwan",
            "user": "saminda",
            "type": "user"
          },
          "name": "Saminda Abeyruwan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:16.630Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da712",
          "name": "Joshua Ainslie",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da713",
          "user": {
            "_id": "6253ee39e4e98393660b5c35",
            "avatarUrl": "/avatars/9c032f6a0729bfe5c16b3affe190834d.svg",
            "isPro": false,
            "fullname": "Jean-Baptiste Alayrac",
            "user": "jalayrac",
            "type": "user"
          },
          "name": "Jean-Baptiste Alayrac",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:27.846Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da714",
          "user": {
            "_id": "63f47a88121f9894707465ed",
            "avatarUrl": "/avatars/d85d409d19068aea02a2532b587dd1ef.svg",
            "isPro": false,
            "fullname": "Montserrat Gonzalez Arenas",
            "user": "montse90",
            "type": "user"
          },
          "name": "Montserrat Gonzalez Arenas",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:33.087Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da715",
          "user": {
            "_id": "66140283cf3fef4fa812e92f",
            "avatarUrl": "/avatars/033586adc3931d6c85bf9e84220992b4.svg",
            "isPro": false,
            "fullname": "Travis Armstrong",
            "user": "TravisAStrong",
            "type": "user"
          },
          "name": "Travis Armstrong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:39.411Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da716",
          "user": {
            "_id": "653ad36e5f1703225b266b7b",
            "avatarUrl": "/avatars/170e6b54a9859c7fca0289a09654c47f.svg",
            "isPro": false,
            "fullname": "Ashwin Balakrishna",
            "user": "abalakrishna123",
            "type": "user"
          },
          "name": "Ashwin Balakrishna",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:46.130Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da717",
          "name": "Robert Baruch",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da718",
          "name": "Maria Bauza",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da719",
          "name": "Michiel Blokzijl",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71a",
          "name": "Steven Bohez",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71b",
          "name": "Konstantinos Bousmalis",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71c",
          "name": "Anthony Brohan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71d",
          "name": "Thomas Buschmann",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71e",
          "name": "Arunkumar Byravan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71f",
          "name": "Serkan Cabi",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da720",
          "name": "Ken Caluwaerts",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da721",
          "name": "Federico Casarini",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da722",
          "name": "Oscar Chang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da723",
          "name": "Jose Enrique Chen",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da724",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da725",
          "name": "Hao-Tien Lewis Chiang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da726",
          "name": "Krzysztof Choromanski",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da727",
          "name": "David D'Ambrosio",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da728",
          "name": "Sudeep Dasari",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da729",
          "name": "Todor Davchev",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72a",
          "name": "Coline Devin",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72b",
          "user": {
            "_id": "62dac377f014388f908974f4",
            "avatarUrl": "/avatars/39bf0ca206441575a45f577060cdd8bc.svg",
            "isPro": false,
            "fullname": "Norman Di Palo",
            "user": "normandipalo",
            "type": "user"
          },
          "name": "Norman Di Palo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:09.232Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72c",
          "user": {
            "_id": "64d89da3bab152b24713108e",
            "avatarUrl": "/avatars/22e10f9a13b0d18b3a3b1f5281c7124d.svg",
            "isPro": false,
            "fullname": "Tianli Ding",
            "user": "Tding",
            "type": "user"
          },
          "name": "Tianli Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:18.146Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72d",
          "user": {
            "_id": "63b7d36469b7bd7324f9f438",
            "avatarUrl": "/avatars/67b1864c378102b1cf2de571cce7bf9a.svg",
            "isPro": false,
            "fullname": "Adil Dostmohamed",
            "user": "adild",
            "type": "user"
          },
          "name": "Adil Dostmohamed",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:29.517Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72e",
          "user": {
            "_id": "67225875b46c703941fa7967",
            "avatarUrl": "/avatars/7c89fbdd9a135210209bcd0cbfe7988a.svg",
            "isPro": false,
            "fullname": "Danny Driess",
            "user": "dannydriess",
            "type": "user"
          },
          "name": "Danny Driess",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:37.044Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72f",
          "user": {
            "_id": "63c9bd445fdc575773c732fe",
            "avatarUrl": "/avatars/def472d1ab3fbf751225357c0932ae7e.svg",
            "isPro": false,
            "fullname": "Yilun Du",
            "user": "yilundu",
            "type": "user"
          },
          "name": "Yilun Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:43.648Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da730",
          "name": "Debidatta Dwibedi",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da731",
          "user": {
            "_id": "66f6e9a737473469e871cae8",
            "avatarUrl": "/avatars/8c247380cee5d879aac204299963d3a7.svg",
            "isPro": false,
            "fullname": "Michael Elabd",
            "user": "michaelelabd",
            "type": "user"
          },
          "name": "Michael Elabd",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:55.368Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da732",
          "name": "Claudio Fantacci",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da733",
          "name": "Cody Fong",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da734",
          "name": "Erik Frey",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da735",
          "name": "Chuyuan Fu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da736",
          "name": "Marissa Giustina",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da737",
          "name": "Keerthana Gopalakrishnan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da738",
          "name": "Laura Graesser",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da739",
          "name": "Leonard Hasenclever",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73a",
          "name": "Nicolas Heess",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73b",
          "name": "Brandon Hernaez",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73c",
          "name": "Alexander Herzog",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73d",
          "name": "R. Alex Hofer",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73e",
          "name": "Jan Humplik",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73f",
          "name": "Atil Iscen",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da740",
          "name": "Mithun George Jacob",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da741",
          "name": "Deepali Jain",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da742",
          "name": "Ryan Julian",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da743",
          "user": {
            "_id": "64b7bf04a5018e3c7ca2ecda",
            "avatarUrl": "/avatars/4a434c344f68f2915c6e823262e62946.svg",
            "isPro": false,
            "fullname": "Dmitry Kalashnikov",
            "user": "dmitry-kalashnikov",
            "type": "user"
          },
          "name": "Dmitry Kalashnikov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:01:27.098Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da744",
          "name": "M. Emre Karagozler",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da745",
          "name": "Stefani Karp",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da746",
          "name": "Chase Kew",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da747",
          "name": "Jerad Kirkland",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da748",
          "name": "Sean Kirmani",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da749",
          "name": "Yuheng Kuang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74a",
          "user": {
            "_id": "631fa7e9124782a19efd20f2",
            "avatarUrl": "/avatars/456ef70cdb2a6a1f79a078746e96034a.svg",
            "isPro": false,
            "fullname": "Thomas Lampe",
            "user": "tlampe",
            "type": "user"
          },
          "name": "Thomas Lampe",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:01:36.252Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74b",
          "name": "Antoine Laurens",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74c",
          "name": "Isabel Leal",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74d",
          "name": "Alex X. Lee",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74e",
          "name": "Tsang-Wei Edward Lee",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74f",
          "name": "Jacky Liang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da750",
          "name": "Yixin Lin",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da751",
          "name": "Sharath Maddineni",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da752",
          "name": "Anirudha Majumdar",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da753",
          "name": "Assaf Hurwitz Michaely",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da754",
          "name": "Robert Moreno",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da755",
          "name": "Michael Neunert",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da756",
          "name": "Francesco Nori",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da757",
          "name": "Carolina Parada",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da758",
          "name": "Emilio Parisotto",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da759",
          "name": "Peter Pastor",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75a",
          "name": "Acorn Pooley",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75b",
          "name": "Kanishka Rao",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75c",
          "name": "Krista Reymann",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75d",
          "name": "Dorsa Sadigh",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75e",
          "name": "Stefano Saliceti",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75f",
          "name": "Pannag Sanketi",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da760",
          "name": "Pierre Sermanet",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da761",
          "name": "Dhruv Shah",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da762",
          "name": "Mohit Sharma",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da763",
          "name": "Kathryn Shea",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da764",
          "name": "Charles Shu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da765",
          "name": "Vikas Sindhwani",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da766",
          "name": "Sumeet Singh",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da767",
          "name": "Radu Soricut",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da768",
          "name": "Jost Tobias Springenberg",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da769",
          "name": "Rachel Sterneck",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76a",
          "name": "Razvan Surdulescu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76b",
          "name": "Jie Tan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76c",
          "name": "Jonathan Tompson",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76d",
          "name": "Vincent Vanhoucke",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76e",
          "name": "Jake Varley",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76f",
          "name": "Grace Vesom",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da770",
          "name": "Giulia Vezzani",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da771",
          "name": "Oriol Vinyals",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da772",
          "name": "Ayzaan Wahid",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da773",
          "name": "Stefan Welker",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da774",
          "name": "Paul Wohlhart",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da775",
          "name": "Fei Xia",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da776",
          "name": "Ted Xiao",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da777",
          "name": "Annie Xie",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da778",
          "name": "Jinyu Xie",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da779",
          "name": "Peng Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77a",
          "name": "Sichun Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77b",
          "name": "Ying Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77c",
          "name": "Zhuo Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77d",
          "name": "Yuxiang Yang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77e",
          "name": "Rui Yao",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77f",
          "name": "Sergey Yaroshenko",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da780",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da781",
          "name": "Wentao Yuan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da782",
          "name": "Jingwei Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da783",
          "name": "Tingnan Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da784",
          "name": "Allan Zhou",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da785",
          "name": "Yuxiang Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T19:02:56.000Z",
      "submittedOnDailyAt": "2025-03-27T00:36:27.703Z",
      "title": "제미니 로보틱스: 물리 세계에 AI를 도입하기\n\n**注意**：이 번역은 단순히 텍스트를 번역한 결과로, 내용이나 맥락에 대한 추가 설명이나 설명은 제공되지 않습니다. 若要确保翻译的准确性和专业性，请根据具体需求进行校对。",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "최근의 대규모 다모달 모델의 발전은 디지털 영역에서 뛰어난 일반적인 능력을 발휘시켰지만, 로봇과 기타 물리적 에이전트에 대한 이 모델의 전파는 중대한 문제로 남아 있습니다. 본 보고서는 새로운 AI 모델을 로봇텍스특히 설계하여, Gemini 2.0의 기반으로 구축된 것을 소개합니다. 우리는 직접 로봇을 제어할 수 있는 높은 시각, 언어, 행동(VLA) 일반 모델인 Gemini Robotics를 소개합니다. Gemini Robotics는 다양한 복잡한 작업 태스크를 해결하기 위해 순滑한 반응적인 동작을 수행하고, 물체의 종류와 위치의 변화에 강건하며,未见한 환경을 처리할 수 있으며, 다양한 개방형 어휘를 따라할 수 있습니다. 또한 추가의 미세 조정을 통해, Gemini Robotics는 새로운 능력을 전문화할 수 있으며, 장기호라이존, 높은 디지엔드 태스크의 해결, 100개의 시술로부터 새로운 단기호라이존 태스크의 학습, 완전히 새로운 로봇의 구조를 익힐 수 있습니다. 이러한 것이 가능한 이유는 Gemini Robotics는 지금까지 소개된 Gemini Robotics-ER 모델의 기반에 구축되어 있기 때문입니다. Gemini Robotics-ER(Embodied Reasoning)는, Gemini의 다모달 모델 논리 능력을 물리적 세계에 확장하고, 공간 및 시간의 이해를 강화합니다. 이로써, 로봇텍스 관련 능력을 갖출 수 있습니다. 물체 검출, 지향,궤도 및 유리 예측, 다 뷰의 대응, 3D 바운딩 박스 예측 등. 이 새로운 조합은 로봇텍스의 다양한 애플리케이션을 지원합니다. 또한, 이 새로운 로봇텍스 기반 모델과 관련된 중요한 안전 고려 사항도 논의하고, 해결합니다. Gemini Robotics 가족은, AI의 실력을 물리적 세계에 실현하는 일반적인 로봇의 개발에 중요한 단계를 뛴 것입니다.",
      "upvotes": 6,
      "discussionId": "67e4b28cfa81c69f446da8c7",
      "ai_keywords": [
        "Vision-Language-Action (VLA) generalist model",
        "multimodal model",
        "multimodal reasoning capabilities",
        "fine-tuning",
        "long-horizon, highly dexterous tasks",
        "short-horizon tasks",
        "robot embodiments",
        "embodied reasoning",
        "object detection",
        "pointing",
        "trajectory prediction",
        "grasp prediction",
        "multi-view correspondence",
        "3D bounding box predictions",
        "robotics applications",
        "safety considerations",
        "robotics foundation models",
        "general-purpose robots"
      ]
    },
    "publishedAt": "2025-03-25T15:02:56.000Z",
    "title": "Gemini Robotics: Bringing AI into the Physical World",
    "summary": "Recent advancements in large multimodal models have led to the emergence of\nremarkable generalist capabilities in digital domains, yet their translation to\nphysical agents such as robots remains a significant challenge. This report\nintroduces a new family of AI models purposefully designed for robotics and\nbuilt upon the foundation of Gemini 2.0. We present Gemini Robotics, an\nadvanced Vision-Language-Action (VLA) generalist model capable of directly\ncontrolling robots. Gemini Robotics executes smooth and reactive movements to\ntackle a wide range of complex manipulation tasks while also being robust to\nvariations in object types and positions, handling unseen environments as well\nas following diverse, open vocabulary instructions. We show that with\nadditional fine-tuning, Gemini Robotics can be specialized to new capabilities\nincluding solving long-horizon, highly dexterous tasks, learning new\nshort-horizon tasks from as few as 100 demonstrations and adapting to\ncompletely novel robot embodiments. This is made possible because Gemini\nRobotics builds on top of the Gemini Robotics-ER model, the second model we\nintroduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends\nGemini's multimodal reasoning capabilities into the physical world, with\nenhanced spatial and temporal understanding. This enables capabilities relevant\nto robotics including object detection, pointing, trajectory and grasp\nprediction, as well as multi-view correspondence and 3D bounding box\npredictions. We show how this novel combination can support a variety of\nrobotics applications. We also discuss and address important safety\nconsiderations related to this new class of robotics foundation models. The\nGemini Robotics family marks a substantial step towards developing\ngeneral-purpose robots that realizes AI's potential in the physical world.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20020.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19950",
      "authors": [
        {
          "_id": "67e4b27cfe1f5acc68028de9",
          "user": {
            "_id": "6399c67bf78f75ae73146760",
            "avatarUrl": "/avatars/d1c3b0eff67b598a1ad58e297382957f.svg",
            "isPro": false,
            "fullname": "CHEN Han",
            "user": "Concyclics",
            "type": "user"
          },
          "name": "Han Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:29.747Z",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028dea",
          "user": {
            "_id": "650ccf6a36ac7eba06ea1cfa",
            "avatarUrl": "/avatars/50374fca4f6cc2cf7a6601cd8d3f725b.svg",
            "isPro": false,
            "fullname": "Zicong Jiang",
            "user": "Zicong99",
            "type": "user"
          },
          "name": "Zicong Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:02:00.596Z",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028deb",
          "user": {
            "_id": "64b781c5da8017900e7b8b25",
            "avatarUrl": "/avatars/0db9a83b0908cc6b9417360ed77fcc1a.svg",
            "isPro": false,
            "fullname": "zining zhang",
            "user": "deciding",
            "type": "user"
          },
          "name": "Zining Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:02:07.675Z",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028dec",
          "name": "Bingsheng He",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028ded",
          "name": "Pingyi Luo",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028dee",
          "name": "Mian Lu",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028def",
          "name": "Yuqiang Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T16:24:45.000Z",
      "submittedOnDailyAt": "2025-03-27T02:17:05.033Z",
      "title": "LogQuant: KV캐시의 로그 분포 2비트 쿼티레이션에 의한 정확도 유지의 상위 수준",
      "submittedOnDailyBy": {
        "_id": "6399c67bf78f75ae73146760",
        "avatarUrl": "/avatars/d1c3b0eff67b598a1ad58e297382957f.svg",
        "isPro": false,
        "fullname": "CHEN Han",
        "user": "Concyclics",
        "type": "user"
      },
      "summary": "LogQuant는 대규모 언어 모델(LLM) 추론의 KV Cache에 대한 축소화 기술입니다. 이는 메모리 지원량을 크게 줄일 수 있는 혁신적인 기술로, 우수한 성능을 유지하는 데 사용됩니다. 기존의 방법은 후순위 토큰이 중요하다고 가정하거나, 더 빠른 어텐션 패턴에 기반하여 중요 토큰을 예측하는 데 사용했습니다. 그러나 두 가지 접근 방식 모두 성능 저하 또는 예측 오류로 인한 문제가 발생할 수 있습니다.\n\nLogQuant는 다른 접근 방식을 사용합니다. 로그 기반의 필터링 구조를 적용하여, 전체 컨텍스트에서 선택적으로 KV Cache를 축소화하고, 기존 방법과 비교하여 같은 또는 감소된 메모리 사용량에서 더 우수한 성능을 구현합니다. 벤치마크 테스트에서 메모리 소비량을 증가시키지 않고, 트랜스포ف 로우를 25% 증가시키고, 배치 크기를 60% 증가시켰습니다. 수학이나 코드 완성 등 어려운 작업에서, 같은 컴퓨팅 비율에서 정확도를 40% ~ 200% 증가시키고, 비교 기술에 비해 뛰어납니다.\n\nLogQuant는 Python의 transformers 라이브러리와 같은 인기 있는 추론 프레임워크와 통합할 수 있습니다. 구현은 https://github.com/Concyclics/LogQuantKV에 제공됩니다.",
      "upvotes": 4,
      "discussionId": "67e4b27efe1f5acc68028e72",
      "githubRepo": "https://github.com/Concyclics/LogQuantKV",
      "ai_keywords": [
        "KV Cache",
        "large language model (LLM)",
        "token",
        "attention pattern",
        "log-based filtering mechanism",
        "throughput",
        "batch size",
        "accuracy",
        "Math Completion",
        "Code Completion",
        "compression ratio",
        "transformers library"
      ]
    },
    "publishedAt": "2025-03-25T12:24:45.000Z",
    "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
    "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19950.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6399c67bf78f75ae73146760",
      "avatarUrl": "/avatars/d1c3b0eff67b598a1ad58e297382957f.svg",
      "fullname": "CHEN Han",
      "name": "Concyclics",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.19462",
      "authors": [
        {
          "_id": "67e3641cd8da46951f860d84",
          "user": {
            "_id": "645b8bf6438d6cfbe1ae47ae",
            "avatarUrl": "/avatars/3e4dc0523ba8e314b298567cf8f0c0ae.svg",
            "isPro": false,
            "fullname": "Haiyu Zhang",
            "user": "aejion",
            "type": "user"
          },
          "name": "Haiyu Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:02:45.776Z",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d85",
          "user": {
            "_id": "643e943a70c6a27621eb1c89",
            "avatarUrl": "/avatars/73ec521ab5ba84cc7908c52c0acef6ef.svg",
            "isPro": false,
            "fullname": "Xinyuan Chen",
            "user": "AriaChen",
            "type": "user"
          },
          "name": "Xinyuan Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:02:58.635Z",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d86",
          "user": {
            "_id": "63201256c6b20f03c829c4b8",
            "avatarUrl": "/avatars/a42092119777d65e60b12eb5ba0e45f1.svg",
            "isPro": false,
            "fullname": "Yaohui Wang",
            "user": "YaohuiW",
            "type": "user"
          },
          "name": "Yaohui Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:03:18.769Z",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d87",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:03:26.468Z",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d88",
          "name": "Yunhong Wang",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d89",
          "name": "Yu Qiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T08:52:07.000Z",
      "submittedOnDailyAt": "2025-03-27T00:39:48.103Z",
      "title": "AccVideo: Acceleration of Video Diffusion Model Using Synthetic Datasets",
      "submittedOnDailyBy": {
        "_id": "645b8bf6438d6cfbe1ae47ae",
        "avatarUrl": "/avatars/3e4dc0523ba8e314b298567cf8f0c0ae.svg",
        "isPro": false,
        "fullname": "Haiyu Zhang",
        "user": "aejion",
        "type": "user"
      },
      "summary": "Diffusion 모델은 영화 생성 분야에서 놀라울 정도로 큰 발전을 이루었습니다. 그러나, 그 여러 반복의 노이즈 제거 특성으로 인해 영화를 생성하기 위해 많은 추론 단계가 필요하고, 이는 느리고 계산량이 많습니다. 본 논문에서는 현재 존재하는 Diffusion Distillation 방법의 문제에 대해 상세한 분석을 수행하고, 합성 데이터 세트를 사용하여 영화 노이즈 제거 모델을 가속화하기 위한 새로운 효율적인 방법인 AccVideo를 제안합니다. 훈련된 영화 노이즈 제거 모델을 사용하여, 여러 유효한 노이즈 제거 데이터 포인트를 생성하고, 이들을 합성 데이터 세트로 사용하며, 노이즈를 제거하는 과정에서 무용 데이터 포인트를 사용하지 않도록 합니다. 이 합성 데이터 세트에 기반하여, 노이즈로부터 영화로의 매핑을 학습하는 데 필요한 궤도 기반의 적은 단계의 가이드를 설계합니다. 또한, 합성 데이터 세트는 각 노이즈 제거 시간 단계의 데이터 분포를 감지하기 때문에, 학생 모델의 출력 분포를 우리의 합성 데이터 세트의 분포와 일치시키기 위해 adversarial 학습의 전략을 도입하고, 영화의 품질을 향상시킵니다. 확장된 실험에 따라, 우리의 모델은 지도 모델에 비해 8.5배의 생성 속도 향상을 달성하며, 상대적인 성능을 유지합니다. 이전의 가속 메소드와 비교하여, 우리의 접근 방식은 5초, 720x1280, 24fps의 고품질과 해상도를 가진 영화를 생성할 수 있습니다.",
      "upvotes": 4,
      "discussionId": "67e3641ed8da46951f860e12",
      "ai_keywords": [
        "diffusion models",
        "video generation",
        "iterative denoising",
        "inference steps",
        "diffusion distillation",
        "AccVideo",
        "synthetic dataset",
        "pretrained video diffusion model",
        "denoising trajectories",
        "trajectory-based few-step guidance",
        "noise-to-video mapping",
        "adversarial training strategy"
      ]
    },
    "publishedAt": "2025-03-25T04:52:07.000Z",
    "title": "AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset",
    "summary": "Diffusion models have achieved remarkable progress in the field of video\ngeneration. However, their iterative denoising nature requires a large number\nof inference steps to generate a video, which is slow and computationally\nexpensive. In this paper, we begin with a detailed analysis of the challenges\npresent in existing diffusion distillation methods and propose a novel\nefficient method, namely AccVideo, to reduce the inference steps for\naccelerating video diffusion models with synthetic dataset. We leverage the\npretrained video diffusion model to generate multiple valid denoising\ntrajectories as our synthetic dataset, which eliminates the use of useless data\npoints during distillation. Based on the synthetic dataset, we design a\ntrajectory-based few-step guidance that utilizes key data points from the\ndenoising trajectories to learn the noise-to-video mapping, enabling video\ngeneration in fewer steps. Furthermore, since the synthetic dataset captures\nthe data distribution at each diffusion timestep, we introduce an adversarial\ntraining strategy to align the output distribution of the student model with\nthat of our synthetic dataset, thereby enhancing the video quality. Extensive\nexperiments demonstrate that our model achieves 8.5x improvements in generation\nspeed compared to the teacher model while maintaining comparable performance.\nCompared to previous accelerating methods, our approach is capable of\ngenerating videos with higher quality and resolution, i.e., 5-seconds,\n720x1280, 24fps.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19462.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b8bf6438d6cfbe1ae47ae",
      "avatarUrl": "/avatars/3e4dc0523ba8e314b298567cf8f0c0ae.svg",
      "fullname": "Haiyu Zhang",
      "name": "aejion",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20756",
      "authors": [
        {
          "_id": "67e4b0b850ca38886d7e78d0",
          "name": "Chenxi Wang",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d1",
          "name": "Jizhan Fang",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d2",
          "name": "Xiang Chen",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d3",
          "name": "Bozhong Tian",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d4",
          "name": "Ziwen Xu",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d5",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d6",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:45:29.000Z",
      "submittedOnDailyAt": "2025-03-27T00:28:51.612Z",
      "title": "ADS-Edit: ADS-Edit는 자동주행 시스템에 대한 다양한 konfeface 캠퍼스 교육 데이터 세트입니다.",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "최근의 대형 다중 모델(LMMs)의 발전은 자율주행 시스템(ADS)에 원하는 효과를 보여주고 있습니다. 그러나 이들이 직접적인 ADS에 적용될 때, 교통 지식을 이해하는 오류, 복잡한 도로 상황, 차량의 다양한 상태 등 문제로 인해 제한되어 있습니다. 이러한 문제를 대처하기 위해 모델의 행동을 특정한 수정을 가능하게 하여 완전한 재훈련의 필요성을 없애는 방안을 제안하고 있습니다. 또한 ADS를 위한 지식 편집 데이터 세트「ADS-Edit」를 통해 현실적인 시나리오, 많은 데이터 타입, 상세한 평가 지표를 포함하는 것을 소개하고 있습니다. 구체적인 실험을 수행하여 많은 흥미로운 결론을 얻었습니다. 우리 연구는 자율주행 시스템 분야의 지식 편집 애플리케이션의 발전에 기여하고자 합니다. 코드와 데이터는 https://github.com/zjunlp/EasyEdit에 공개되어 있습니다.",
      "upvotes": 3,
      "discussionId": "67e4b0bb50ca38886d7e79d0",
      "ai_keywords": [
        "Knowledge Editing",
        "ADS-Edit",
        "multimodal knowledge editing dataset",
        "autonomous driving systems"
      ]
    },
    "publishedAt": "2025-03-26T13:45:29.000Z",
    "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\n  Systems",
    "summary": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20756.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20271",
      "authors": [
        {
          "_id": "67e4dc6a38e4d1444c71ce70",
          "name": "Haoqin Tu",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce71",
          "name": "Weitao Feng",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce72",
          "name": "Hardy Chen",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce73",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce74",
          "name": "Xianfeng Tang",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce75",
          "name": "Cihang Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T06:38:31.000Z",
      "submittedOnDailyAt": "2025-03-27T06:03:39.751Z",
      "title": "ViLBench: 시각 언어 처리 보상 모델링 시스템",
      "submittedOnDailyBy": {
        "_id": "604ae011caabafacfa48e3de",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1615519738679-noauth.jpeg",
        "isPro": false,
        "fullname": "Haoqin Tu",
        "user": "PahaII",
        "type": "user"
      },
      "summary": "프로세스 관측 보상 모델은 복잡한 작업의 이유궤도를 적절하게 선택하기 위한 미세한 기능을 가지고 있으며, 모델의 응답에 단계별로 상세한 피드백을 제공합니다. 그 우수성은 더불어, PRMs의 평가는 특히 많은 모델에서 부족하며, 이 공백을 채우는 데에, 이 논문은 처음으로 현재의 시각 언어 모델(VLLMs)을 2가지 보상 모델로 하여 여러 시각 언어 벤치마크에서 비교하고, 각각의 작업에서 일관된 우수성을 보이지 않는 것을 보여줍니다. 그리고 우수한 VLLMs은 반드시 보상의 성능이 좋아지는 것은 아니다를 보여주고 있습니다. 벤치마크의 발전을 위해, VilBench라는 시각 언어 벤치마크를 통해 엄밀한 프로세스 보상 신호를 필요로 하는 방식으로 설계하고, 이를 통해 현재의 VLLMs의 평가를 발전시킬 수 있음을 시도합니다. 특히, OpenAI의 GPT-4o는 Chain-of-Thought(CoT)을 사용하여도 27.3%의 정확도를 달성하는 것을 보여줍니다. 이는 현재의 VLLMs의 평가의 어려움을 보여주고 있습니다. 마지막으로, 일반적인 VLLMs과 보상 모델 사이에 공백을 채우는 가능성을 보여주기 위해, 확장된 나무 검색 알고리즘을 사용하여 73.6K의 시각 언어 프로세스 보상 데이터를 수집하고, 이를 통해 3B 모델은 평균적으로 CoT보다 3.3%의 개선을 달성하고, OpenAI o1의 생성을 선택할 때, 그 학습되지 않은 컴페나ン트보다 2.5%의 개선을 달성합니다. 구현은 https://ucsc-vlaa.github.io/VilBench에서 공개되어 있습니다.",
      "upvotes": 2,
      "discussionId": "67e4dc6b38e4d1444c71cee2",
      "ai_keywords": [
        "vision large language models (VLLMs)",
        "output reward models (ORMs)",
        "process reward models (PRMs)",
        "vision-language benchmarks",
        "Chain-of-Thought (CoT)",
        "vision-language process reward data",
        "tree-search algorithm",
        "ViLBench"
      ]
    },
    "publishedAt": "2025-03-26T02:38:31.000Z",
    "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling",
    "summary": "Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\nand data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20271.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "604ae011caabafacfa48e3de",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1615519738679-noauth.jpeg",
      "fullname": "Haoqin Tu",
      "name": "PahaII",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20198",
      "authors": [
        {
          "_id": "67e4b98039509b0149142daa",
          "name": "Alex Jinpeng Wang",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dab",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dac",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dad",
          "name": "Lijuan Wang",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dae",
          "name": "Min Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T03:44:25.000Z",
      "submittedOnDailyAt": "2025-03-27T01:06:03.239Z",
      "title": "WORD보다 멀리：다모달기 자동복원 모델에 의한 긴 이미지 생성의 발전",
      "submittedOnDailyBy": {
        "_id": "62333a88fd7bb4a39b92d387",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
        "isPro": false,
        "fullname": "Alex Jinpeng Wang",
        "user": "Awiny",
        "type": "user"
      },
      "summary": "최근의 자동복원 모델과 분기 모델의 발전은 짧은 문장 이미지 생성에 강력한 성능을 달성했습니다. 그러나 현재의 생성 모델에서 주요 문제로 남아있는 것은 이미지에 대한 복잡한, 긴 문장 생성（예: 슬라이드나 문서의 파그래프）입니다. 우리는 일반적인 문장 이미지 시스템에서 주로 짧은 문장나 단어만 처리하는 중요한 단점을 해결하기 위해, 긴 문장 이미지 생성에 특화된 첫 번째 연구를 소개합니다. 가장 先端의 자동복원 생성 모델의 상세한 분석을 통해, 이미지 토크나이저는 문장 생성의 질의 중요한 한계로 인식되었습니다. 이를 해결하기 위해, 문장 디바이스 뉴트라이너를 도입하고, 구체적인 장면 문장의 특징을 최적화하여捉えました. 이 토크나이저를 확장하여, 고품질의 긴 문장 이미지를 생성하기 위한 다형적인 자동복원 모델을 개발했습니다. 이 모델은 글꼴 스타일, 크기, 색상, 어레이 등 문장의 속성을 强力하게 제어하기 위해 강력한 제어성을 제공합니다. 확장된 실험은 DALL-E 3, SD3.5 Large, GPT4o와 비교하여, 긴 문장 생성에 대한 높은 정확도, 일관성, 유연성을 보여주었습니다. 이 모델은 문서와 슬라이드의 교차 생성 등 창의적인 응용 분야의 기회를 개척하고, 긴 문장 이미지 생성의 새로운 지평을 향해 있습니다.",
      "upvotes": 2,
      "discussionId": "67e4b98439509b0149142f3c",
      "ai_keywords": [
        "autoregressive models",
        "diffusion models",
        "image tokenizer",
        "binary tokenizer",
        "multimodal autoregressive model",
        "font style",
        "text properties",
        "size",
        "color",
        "alignment",
        "SD3.5 Large",
        "GPT4o",
        "DALL-E 3",
        "long-text image generation"
      ]
    },
    "publishedAt": "2025-03-25T23:44:25.000Z",
    "title": "Beyond Words: Advancing Long-Text Image Generation via Multimodal\n  Autoregressive Models",
    "summary": "Recent advancements in autoregressive and diffusion models have led to strong\nperformance in image generation with short scene text words. However,\ngenerating coherent, long-form text in images, such as paragraphs in slides or\ndocuments, remains a major challenge for current generative models. We present\nthe first work specifically focused on long text image generation, addressing a\ncritical gap in existing text-to-image systems that typically handle only brief\nphrases or single sentences. Through comprehensive analysis of state-of-the-art\nautoregressive generation models, we identify the image tokenizer as a critical\nbottleneck in text generating quality. To address this, we introduce a novel\ntext-focused, binary tokenizer optimized for capturing detailed scene text\nfeatures. Leveraging our tokenizer, we develop \\ModelName, a multimodal\nautoregressive model that excels in generating high-quality long-text images\nwith unprecedented fidelity. Our model offers robust controllability, enabling\ncustomization of text properties such as font style, size, color, and\nalignment. Extensive experiments demonstrate that \\ModelName~significantly\noutperforms SD3.5 Large~sd3 and GPT4o~gpt4o with DALL-E\n3~dalle3 in generating long text accurately, consistently, and flexibly.\nBeyond its technical achievements, \\ModelName~opens up exciting opportunities\nfor innovative applications like interleaved document and PowerPoint\ngeneration, establishing a new frontier in long-text image generating.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20198.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19846",
      "authors": [
        {
          "_id": "67e4c449672b3d9c9cb8aa4b",
          "user": {
            "_id": "67e4c444692ba54248a6b337",
            "avatarUrl": "/avatars/7695a30d185ccb2354129c72538e9180.svg",
            "isPro": false,
            "fullname": "Aaron Serianni",
            "user": "serianni",
            "type": "user"
          },
          "name": "Aaron Serianni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:27.573Z",
          "hidden": false
        },
        {
          "_id": "67e4c449672b3d9c9cb8aa4c",
          "name": "Tyler Zhu",
          "hidden": false
        },
        {
          "_id": "67e4c449672b3d9c9cb8aa4d",
          "name": "Olga Russakovsky",
          "hidden": false
        },
        {
          "_id": "67e4c449672b3d9c9cb8aa4e",
          "name": "Vikram V. Ramaswamy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:11:39.000Z",
      "submittedOnDailyAt": "2025-03-27T01:57:35.164Z",
      "title": "Attention IoU: 짧은 순위 IoU: 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위 짧은 순위",
      "submittedOnDailyBy": {
        "_id": "67e4c444692ba54248a6b337",
        "avatarUrl": "/avatars/7695a30d185ccb2354129c72538e9180.svg",
        "isPro": false,
        "fullname": "Aaron Serianni",
        "user": "serianni",
        "type": "user"
      },
      "summary": "컴퓨터 비전 모델은 광범위한 데이터 세트와 태스크에서 편향을 표현하고 확장되는 것을 볼 수 있습니다. 클래스 분류 모델에서 편향의 정량화의 기존 방법은 데이터 세트의 분포와 모델의 서브 그룹의 성능에 초점을 맞추지만, 모델의 내부 기능은 간과되어 있습니다. 우리는 Attention-IoU(Attention Intersection over Union) 메트릭과 관련된 점수를 소개하고, 이들은 어텐션 맵을 사용하여 모델의 내부 표현에서 편향을 명확히 하여, 편향을 유발할 가능성이 있는 이미지 특성을 특정하는 데 사용됩니다. 우선, Attention-IoU를 합성 데이터 세트「Waterbirds」에서 검증하고, 이 메트릭이 모델의 편향을 정확히 측정함을 보여줍니다. 다음으로, CelebA 데이터 세트를 분석하고, Attention-IoU가 정확도 차이보다 상관성을 명확히 합니다. 보호 속성「Male」을 통해 개별 속성을 조사하여 CelebA에서 편향의 표현의 특별한 방법을 조사합니다. 마지막으로, 훈련 세트를 부분 샘플링하여 속성 관련성을 변경함으로써, Attention-IoU가 데이터 세트 레이블에 포함되지 않은 잠재적인 혼잡 변수를 명확히 합니다.",
      "upvotes": 2,
      "discussionId": "67e4c44a672b3d9c9cb8aaae",
      "ai_keywords": [
        "Attention-IoU",
        "attention maps",
        "internal representations",
        "image features",
        "Waterbirds dataset",
        "CelebA dataset",
        "accuracy disparities",
        "protected attribute",
        "attribute correlations",
        "confounding variables"
      ]
    },
    "publishedAt": "2025-03-25T13:11:39.000Z",
    "title": "Attention IoU: Examining Biases in CelebA using Attention Maps",
    "summary": "Computer vision models have been shown to exhibit and amplify biases across a\nwide array of datasets and tasks. Existing methods for quantifying bias in\nclassification models primarily focus on dataset distribution and model\nperformance on subgroups, overlooking the internal workings of a model. We\nintroduce the Attention-IoU (Attention Intersection over Union) metric and\nrelated scores, which use attention maps to reveal biases within a model's\ninternal representations and identify image features potentially causing the\nbiases. First, we validate Attention-IoU on the synthetic Waterbirds dataset,\nshowing that the metric accurately measures model bias. We then analyze the\nCelebA dataset, finding that Attention-IoU uncovers correlations beyond\naccuracy disparities. Through an investigation of individual attributes through\nthe protected attribute of Male, we examine the distinct ways biases are\nrepresented in CelebA. Lastly, by subsampling the training set to change\nattribute correlations, we demonstrate that Attention-IoU reveals potential\nconfounding variables not present in dataset labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19846.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "67e4c444692ba54248a6b337",
      "avatarUrl": "/avatars/7695a30d185ccb2354129c72538e9180.svg",
      "fullname": "Aaron Serianni",
      "name": "serianni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20220",
      "authors": [
        {
          "_id": "67e4b035af7d0551dc377e13",
          "name": "Weijie Guo",
          "hidden": false
        },
        {
          "_id": "67e4b035af7d0551dc377e14",
          "name": "Guofeng Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b035af7d0551dc377e15",
          "name": "Wufei Ma",
          "hidden": false
        },
        {
          "_id": "67e4b035af7d0551dc377e16",
          "name": "Alan Yuille",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T04:23:53.000Z",
      "submittedOnDailyAt": "2025-03-27T00:26:48.304Z",
      "title": "DINeMo: 3D 注释 없이 뉴럴 메쉬 모델을 학습하는 방법",
      "submittedOnDailyBy": {
        "_id": "625f81afe1994410eef1c36a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650426282769-noauth.jpeg",
        "isPro": false,
        "fullname": "Wufei Ma",
        "user": "wufeim",
        "type": "user"
      },
      "summary": "카테고리 레벨 3D/6D 자세 추정은 3D 스케인 이해의 중요한 단계이며, 로보틱스 및 구체적인 AI 분야에서 광범위하게 적용될 수 있습니다. 최근의 연구는 2D 및 3D의 다양한 태스크를 다루는 뉴럴 메쉬 모델을 분석 기반 합성으로부터의 관점에서 조사했습니다. 부분적인 감막과 도메인 쉬프트에 대한 강건성은 크게 향상되었지만, 이러한 방법들은 부품 비교 학습에 의한 3D 라벨링의 의존성이 강하여, 제한적인 카테고리에 국한되어, 효율적인 확장을 방해하고 있습니다. 본 연구에서는, 큰 규모의 시각 기반 모델로부터 얻은 팩토리 로버드 컴피로ン 데ー지(FactorRobot Composition Stage)를 활용하여 3D 라벨링이 필요하지 않은 새로운 뉴럴 메쉬 모델인 DINeMo를 소개합니다. 양방향의 팩토리 로버트 컴피로ン 데어지 생성 방법을 채택하고, 지역적인 외관 특성과 전역적인 컨텍스트 정보를 모두 사용하여 팩토리 로버트 컴피로ン 데어지를 생성합니다. 자동차 데이터셋에서의 실험 결과는, 우리 연구의 DINeMo가 선행 연구의 0-shot 및 few-shot 3D 자세 추정을 크게 초과하고, 완전한 서브주버 방법과의 오류를 67.3%로 줄였습니다. DINeMo는 학습 중 추가되는 무 라벨 이미지 포함하여 효율적으로 확장할 수 있으며, 3D 라벨링에 의존하는 서브주버 학습 모델보다 우수한 성능을 나타냅니다. 프로젝트 페이지는 https://analysis-by-synthesis.github.io/DINeMo/ 로 접근할 수 있습니다.",
      "upvotes": 1,
      "discussionId": "67e4b038af7d0551dc377f07",
      "projectPage": "https://analysis-by-synthesis.github.io/DINeMo/",
      "ai_keywords": [
        "neural mesh models",
        "analysis-by-synthesis",
        "robustness",
        "partial occlusion",
        "domain shifts",
        "part-contrastive learning",
        "pseudo-correspondence",
        "visual foundation models",
        "bidirectional pseudo-correspondence generation",
        "local appearance features",
        "global context information",
        "zero-shot",
        "few-shot",
        "fully-supervised methods"
      ]
    },
    "publishedAt": "2025-03-26T00:23:53.000Z",
    "title": "DINeMo: Learning Neural Mesh Models with no 3D Annotations",
    "summary": "Category-level 3D/6D pose estimation is a crucial step towards comprehensive\n3D scene understanding, which would enable a broad range of applications in\nrobotics and embodied AI. Recent works explored neural mesh models that\napproach a range of 2D and 3D tasks from an analysis-by-synthesis perspective.\nDespite the largely enhanced robustness to partial occlusion and domain shifts,\nthese methods depended heavily on 3D annotations for part-contrastive learning,\nwhich confines them to a narrow set of categories and hinders efficient\nscaling. In this work, we present DINeMo, a novel neural mesh model that is\ntrained with no 3D annotations by leveraging pseudo-correspondence obtained\nfrom large visual foundation models. We adopt a bidirectional\npseudo-correspondence generation method, which produce pseudo correspondence\nutilize both local appearance features and global context information.\nExperimental results on car datasets demonstrate that our DINeMo outperforms\nprevious zero- and few-shot 3D pose estimation by a wide margin, narrowing the\ngap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively\nand efficiently when incorporating more unlabeled images during training, which\ndemonstrate the advantages over supervised learning methods that rely on 3D\nannotations. Our project page is available at\nhttps://analysis-by-synthesis.github.io/DINeMo/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20220.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625f81afe1994410eef1c36a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650426282769-noauth.jpeg",
      "fullname": "Wufei Ma",
      "name": "wufeim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19953",
      "authors": [
        {
          "_id": "67e4b46cc90e5edf25f581f8",
          "name": "Stefan Stojanov",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581f9",
          "name": "David Wendt",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fa",
          "name": "Seungwoo Kim",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fb",
          "name": "Rahul Venkatesh",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fc",
          "name": "Kevin Feigelis",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fd",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fe",
          "name": "Daniel LK Yamins",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:58:52.000Z",
      "submittedOnDailyAt": "2025-03-27T00:44:41.359Z",
      "title": "동작 개념의 자동 인식을 최적화하는 카운터파라메트릭 학습",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "ビデオ에서 움직이름을 추정하는 것은 여러 하위 애플리케이션을 포함하는 중요한 컴퓨터 비전 문제 중 하나입니다. 이 문제는 제어 가능한 비디오 생성이나 로봇공학 등 포함됩니다. 현재의 해결책은 주로 합성 데이터로 훈련되어 있거나, 상황에 맞는 휴리스틱을 조정하는 것이 필요합니다. 이는 모델의 능력을 현실 세계의 맥락에서 제한합니다. 최근, 비디오에서 큰 규모의 자동 인식 학습에 대한 발전이 있습니다; 그러나 이러한 표현을 움직이름 추정에 활용하는 것은 상대적으로 조사가 부족합니다. 본 연구에서는 이미 학습된 다음 프레임 예측 모델을 사용하여, 흐름과 가려를 추정하는 자동 인식 기법을 개발했습니다. Opt-CWM는 실제 비디오 입력을 사용하여 운동 정보를 추출하기 위해 기초 비디오 모델을 사용하며, 고정 휴리스틱의 필요성을 피합니다. 실제 비디오에서 움직이름을 추정하는데 가장 先端의 성능을 달성하고, 표준 데이터가 필요하지 않도록 합니다.",
      "upvotes": 1,
      "discussionId": "67e4b46ec90e5edf25f582db",
      "ai_keywords": [
        "self-supervised learning",
        "flow estimation",
        "occlusion estimation",
        "next-frame prediction model",
        "counterfactual probes",
        "motion estimation"
      ]
    },
    "publishedAt": "2025-03-25T13:58:52.000Z",
    "title": "Self-Supervised Learning of Motion Concepts by Optimizing\n  Counterfactuals",
    "summary": "Estimating motion in videos is an essential computer vision problem with many\ndownstream applications, including controllable video generation and robotics.\nCurrent solutions are primarily trained using synthetic data or require tuning\nof situation-specific heuristics, which inherently limits these models'\ncapabilities in real-world contexts. Despite recent developments in large-scale\nself-supervised learning from videos, leveraging such representations for\nmotion estimation remains relatively underexplored. In this work, we develop\nOpt-CWM, a self-supervised technique for flow and occlusion estimation from a\npre-trained next-frame prediction model. Opt-CWM works by learning to optimize\ncounterfactual probes that extract motion information from a base video model,\navoiding the need for fixed heuristics while training on unrestricted video\ninputs. We achieve state-of-the-art performance for motion estimation on\nreal-world videos while requiring no labeled data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19953.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17358",
      "authors": [
        {
          "_id": "67e3b41d0fa8f886db6b323a",
          "name": "Jerred Chen",
          "hidden": false
        },
        {
          "_id": "67e3b41d0fa8f886db6b323b",
          "name": "Ronald Clark",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/jIZXKaHvYvl_B_Tg_SAeY.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/cn0aTY48FezRlyiSMberj.mp4"
      ],
      "publishedAt": "2025-03-21T17:58:56.000Z",
      "submittedOnDailyAt": "2025-03-27T07:45:14.887Z",
      "title": "화상은 IMU로: 움직임 브러의 화상으로부터 카메라의 움직임을 추정합니다.",
      "submittedOnDailyBy": {
        "_id": "6305ee63d70693fdf1c7dbb8",
        "avatarUrl": "/avatars/0e81ed3757b4e65be82063b538c3fe49.svg",
        "isPro": false,
        "fullname": "Ronald Clark",
        "user": "r0nn13",
        "type": "user"
      },
      "summary": "로보틱스 및 VR/AR 애플리케이션에서 고속의 카메라 움직임으로 인해 높은 수준의 이동 브러를 발생시키고, 기존의 카메라 자세 측정 방법이 실패하는 경우가 있습니다. 본 논문에서는 이동 브러를 이동 측정의 풍부한 cut으로 기능시키며 실패하는 방법을 개선하기 위해 새로운 프레임워크를 제안합니다. 우리 접근법은 단일의 이동 브러 이미지에서 직접적으로 풍부한 이동 플로우 필드와 단목의 깊이 맵을 예측하는 방식으로 작동합니다. 다음으로, 작은 이동을 가정하여 선형 최소 제곱 문제를 해결하여 순간의 카메라 속도를 복원합니다. 본질적으로, 우리 방법은 IMU 마이너스 효과처럼 강건하게 고속과 공격적인 카메라 동작을 감지하는 측정을 제공합니다. 모델의 훈련에는 ScanNet++v2로부터 현실적인 합성 이동 브러를 사용하여 큰 데이터 세트를 구축하고, 완전히 미분 가능한 파이프라인을 사용하여 실 데이터에서 종말까지 훈련함으로써 모델을 발전시킵니다. 실세계의 벤치마크에서 확장된 평가에 따라, 우리 방법은 현재의 방법보다 가장 先端한 각속도와 이동 속도의 측정을 실현하고, MASt3R, COLMAP 등 방법을 초월함을 증명했습니다.",
      "upvotes": 1,
      "discussionId": "67e3b4200fa8f886db6b3328",
      "projectPage": "https://jerredchen.github.io/image-as-imu/",
      "ai_keywords": [
        "motion blur",
        "camera pose estimation",
        "motion flow field",
        "monocular depth map",
        "linear least squares problem",
        "small motion assumption",
        "IMU-like measurement",
        "ScanNet++v2",
        "fully differentiable pipeline",
        "real-world benchmarks",
        "angular velocity estimates",
        "translational velocity estimates"
      ]
    },
    "publishedAt": "2025-03-21T13:58:56.000Z",
    "title": "Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred\n  Image",
    "summary": "In many robotics and VR/AR applications, fast camera motions cause a high\nlevel of motion blur, causing existing camera pose estimation methods to fail.\nIn this work, we propose a novel framework that leverages motion blur as a rich\ncue for motion estimation rather than treating it as an unwanted artifact. Our\napproach works by predicting a dense motion flow field and a monocular depth\nmap directly from a single motion-blurred image. We then recover the\ninstantaneous camera velocity by solving a linear least squares problem under\nthe small motion assumption. In essence, our method produces an IMU-like\nmeasurement that robustly captures fast and aggressive camera movements. To\ntrain our model, we construct a large-scale dataset with realistic synthetic\nmotion blur derived from ScanNet++v2 and further refine our model by training\nend-to-end on real data using our fully differentiable pipeline. Extensive\nevaluations on real-world benchmarks demonstrate that our method achieves\nstate-of-the-art angular and translational velocity estimates, outperforming\ncurrent methods like MASt3R and COLMAP.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/jIZXKaHvYvl_B_Tg_SAeY.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/cn0aTY48FezRlyiSMberj.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17358.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6305ee63d70693fdf1c7dbb8",
      "avatarUrl": "/avatars/0e81ed3757b4e65be82063b538c3fe49.svg",
      "fullname": "Ronald Clark",
      "name": "r0nn13",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16870",
      "authors": [
        {
          "_id": "67e4c860136c8a867191e52e",
          "user": {
            "_id": "6627ff2a3b4fbc8420a416c3",
            "avatarUrl": "/avatars/de3aafdaf5563fe25edcdb92b394474f.svg",
            "isPro": false,
            "fullname": "AR",
            "user": "Anshumann",
            "type": "user"
          },
          "name": "Anshumann",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:03:45.930Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e52f",
          "user": {
            "_id": "61765fe0b0715831eab6d465",
            "avatarUrl": "/avatars/830406cebdd7ddf66799bf71f6bbff4b.svg",
            "isPro": false,
            "fullname": "Mohd Abbas Zaidi",
            "user": "ya-mehdi",
            "type": "user"
          },
          "name": "Mohd Abbas Zaidi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:03:57.163Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e530",
          "name": "Akhil Kedia",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e531",
          "user": {
            "_id": "65d61eebee922bc7a777d5a6",
            "avatarUrl": "/avatars/180efd503ef412b4dc728e6aa477c16e.svg",
            "isPro": false,
            "fullname": "Jinwoo Ahn",
            "user": "AndrewAhn",
            "type": "user"
          },
          "name": "Jinwoo Ahn",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:07.687Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e532",
          "user": {
            "_id": "629059cdb90dde28ef5cbb30",
            "avatarUrl": "/avatars/451b0e8999447b3a2f03378fe98c0661.svg",
            "isPro": false,
            "fullname": "Taehwak Kwon",
            "user": "Rock222",
            "type": "user"
          },
          "name": "Taehwak Kwon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:23.731Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e533",
          "user": {
            "_id": "6354137306d707b332451421",
            "avatarUrl": "/avatars/46770f32702e3ad08f91faeef9e4ea6e.svg",
            "isPro": false,
            "fullname": "Kangwook Lee",
            "user": "kw1jjang",
            "type": "user"
          },
          "name": "Kangwook Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:33.400Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e534",
          "user": {
            "_id": "653b6e7232bd4db35d981615",
            "avatarUrl": "/avatars/0fbf0c5d502b840bf26baf8c420c7593.svg",
            "isPro": false,
            "fullname": "Haejun Lee",
            "user": "haejunlee",
            "type": "user"
          },
          "name": "Haejun Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:39.155Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e535",
          "user": {
            "_id": "64b4be0665a7e15eac085878",
            "avatarUrl": "/avatars/dd19ec2987fb0735457c6492b53aacfe.svg",
            "isPro": false,
            "fullname": "Joo-hyung Lee",
            "user": "snrbs17",
            "type": "user"
          },
          "name": "Joohyung Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:46.702Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T05:58:18.000Z",
      "submittedOnDailyAt": "2025-03-27T02:09:27.795Z",
      "title": "Sparse Logit Sampling: LLM의 지식전환을 가속화하는 방법",
      "submittedOnDailyBy": {
        "_id": "61765fe0b0715831eab6d465",
        "avatarUrl": "/avatars/830406cebdd7ddf66799bf71f6bbff4b.svg",
        "isPro": false,
        "fullname": "Mohd Abbas Zaidi",
        "user": "ya-mehdi",
        "type": "user"
      },
      "summary": "知識전환은, 교사의 출력 로지스틱 함수가 사전 계산되어 캐시되어 있을 때, 대규모 언어 모델의 지식을 효율적으로 전환하는 방법입니다. 그러나, 이를 사전 학습에 성공하여 적용하기 위해 아직 크게 탐색되어 있지 않습니다. 본 연구에서는, 희소한 지식전환의 직관적인 접근법(예: Top-K 확률의 캐시)이 교사의 확률 분포를 학생에게 편향하여, 최적의 성능과 보완에 연결되지 않습니다. 우리는 무 편향 평가 제공, 기대값으로 경사를 유지하고, 에너지적으로 매우 희소한 로지스틱 함수를 저장하기 위해 중요도 샘플링 기반의 방법인 \"랜덤 샘플링 지식전환\"을 제안합니다. 이 방법은 교차 엔트로피 기반의 훈련에 비해 미세 조정(10% 미만)으로 학생 모델의 학습을 고속화할 수 있으며, 300M부터 3B의 다양한 모델 크기에 대해 완전한 지식전환과 경쟁적인 성능을 유지합니다.",
      "upvotes": 1,
      "discussionId": "67e4c861136c8a867191e58c",
      "ai_keywords": [
        "Knowledge distillation",
        "Large Language Models",
        "teacher output logits",
        "pre-computed",
        "cached",
        "sparse knowledge distillation",
        "Top-K probabilities",
        "biased estimates",
        "teacher probability distribution",
        "student",
        "suboptimal performance",
        "calibration",
        "importance-sampling-based method",
        "Random Sampling Knowledge Distillation",
        "unbiased estimates",
        "gradient in expectation",
        "storing significantly sparser logits",
        "cross-entropy based training",
        "competitive performance",
        "model sizes"
      ]
    },
    "publishedAt": "2025-03-21T01:58:18.000Z",
    "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
    "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61765fe0b0715831eab6d465",
      "avatarUrl": "/avatars/830406cebdd7ddf66799bf71f6bbff4b.svg",
      "fullname": "Mohd Abbas Zaidi",
      "name": "ya-mehdi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]