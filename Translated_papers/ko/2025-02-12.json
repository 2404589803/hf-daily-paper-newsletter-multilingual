[
  {
    "paper": {
      "id": "2502.06807",
      "authors": [
        {
          "_id": "67ac1b080686a1e0690741ce",
          "name": "OpenAI",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d0",
          "name": "Ahmed El-Kishky",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d1",
          "name": "Alexander Wei",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d2",
          "name": "Andre Saraiva",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d3",
          "name": "Borys Minaev",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d4",
          "name": "Daniel Selsam",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d5",
          "name": "David Dohan",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d6",
          "name": "Francis Song",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d7",
          "name": "Hunter Lightman",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d8",
          "name": "Ignasi Clavera",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d9",
          "name": "Jakub Pachocki",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741da",
          "name": "Jerry Tworek",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741db",
          "name": "Lorenz Kuhn",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741dc",
          "name": "Lukasz Kaiser",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741dd",
          "name": "Mark Chen",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741de",
          "name": "Max Schwarzer",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741df",
          "name": "Mostafa Rohaninejad",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e0",
          "name": "Nat McAleese",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e1",
          "name": "o3 contributors",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e2",
          "name": "Oleg Mürk",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e3",
          "name": "Rhythm Garg",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e4",
          "name": "Rui Shu",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e5",
          "name": "Szymon Sidor",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e6",
          "name": "Vineet Kosaraju",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e7",
          "name": "Wenda Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T23:00:15.000Z",
      "title": "コンペティションプログラミング와 대규모 논리 모델",
      "summary": "우리는 대규모 언어 모델(LLMs)에 적용한 강화 학습이 복잡한 코드와 추론 작업의 성능을 크게 향상시키는 것을 보여준다. 또한, 일반적인 추론 모델인 OpenAI o1과 o3의 초기 체크포인트를 비교하며, 2024년 국제 정보학 Olympiad(IOI)에서 경쟁하는 데 설계된 손으로 제작된 추론 전략을 사용하는 영역에 대한 시스템인 o1-ioi를 비교한다. 2024년 IOI에서 o1-ioi를 직접 경쟁하며, 손으로 만든 테스트 시간 전략을 사용하여 49번째 백분위로 자리를 차지하였다. 완화된 경쟁 제약 조건 하에서 o1-ioi는 금메달을 달성하였다. 그러나 나중에 모델을 평가할 때, o3 모델을 사용하면, 손으로 만든 영역에 대한 전략이나 완화된 제약 조건을 사용하지 않고도 금메달을 달성한다. 우리의 연구 결과는, Specialized Pipelines처럼 o1-ioi가 좋은 개선을 가져오는 반면, 확장된 일반적인 o3 모델은 손으로 만든 추론 휴리스틱을 사용하지 않고도 그 결과를 초월한다는 것을 보여준다. 특히, o3 모델은 2024년 IOI에서 금메달을 획득하고, Elite 인간 경쟁자로부터의 Codeforces 등급을 얻는다. 전반적으로, 이러한 결과는 일반적인 reinforcement learning을 확장하는 것이, 영역에 대한 특정 기술에 의존하지 않고, 추론 분야의 최신 AI 기술로 강대한 길을 제시한다는 것을 나타낸다.",
      "upvotes": 28,
      "discussionId": "67ac1b090686a1e069074208"
    },
    "publishedAt": "2025-02-11T22:53:19.310Z",
    "title": "Competitive Programming with Large Reasoning Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07316",
      "authors": [
        {
          "_id": "67ac0ab720e98bddc5c19fed",
          "name": "Junlong Li",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19fee",
          "name": "Daya Guo",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19fef",
          "name": "Dejian Yang",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff0",
          "name": "Runxin Xu",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff1",
          "name": "Yu Wu",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff2",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T07:26:50.000Z",
      "title": "CodeI/O: 코드의 입력 출력 예측에 의한 이유의 패턴의 압축",
      "summary": "이유는 대규모 언어 모델의 기본적인 능력입니다. 지난 주의 연구는 수학이나 코드 생성의 좁은 전문적인 능력을 향상시키기를 중심으로 했지만, 다양한 이유론 태스크의 성능 향상은 희귀하고 연속적인 훈련 데이터로 인해 어려워졌습니다. 이러한 문제를 해결하기 위해, 우리는 새로운 접근 방식을 제안합니다. 이 접근 방식은 코드의 맥락에 기반하여 고유의 이유론 패턴을 체계적으로 선택하는 것입니다. 이를 CodeI/O라고 합니다. 이 방법은 원래의 코드를 코드 입력 출력 예측 형식으로 변환하여 수행합니다. 코드와 테스트 케이스를 자연어로 제공된 상태에서 입력/출력을 예측하는 모델을 통해, Chain-of-Thought의 이유를 학습시키고, 일반적인 이유론의 기본 요소를 노출시킵니다. 이는 코드의 특정 문법을 제외하고 구조적인 이유론을 코드와 상관없이 유지하고, 절차의 정확성을 유지합니다. 실험 결과를 통해, CodeI/O는 기호적인, 과학적인, 로직, 수학과 숫자, 그리고 공통 지식의 이유론 태스크에서 일관된 향상을 나타냅니다. 기존의 사실과 일치하는 출력이나 예측된 입력으로 코드를 재실행하여, 각각의 예측을 확인하고, Chain-of-Thought를 여러 번 수정하여 CodeI/O++를 구현하고, 더 높은 성능을 얻을 수 있습니다. 데이터와 모델은 https://github.com/hkust-nlp/CodeIO에 공개되어 있습니다.",
      "upvotes": 14,
      "discussionId": "67ac0ab820e98bddc5c1a039"
    },
    "publishedAt": "2025-02-11T23:00:20.080Z",
    "title": "CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07316.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "621e40ac944c7e36aaec2369",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621e40ac944c7e36aaec2369/Yj-FJRWps3rvsS_B2bnKo.jpeg",
      "fullname": "Junlong Li",
      "name": "lockon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07701",
      "authors": [
        {
          "_id": "67ac23166def89f9aae56abd",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56abe",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56abf",
          "user": {
            "_id": "66015e8aa4d296af07de538e",
            "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
            "isPro": false,
            "fullname": "Ye",
            "user": "Owen777",
            "type": "user"
          },
          "name": "Tian Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:12.141Z",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac0",
          "name": "Jiantong Zhao",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac1",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac2",
          "name": "Michael Lingelbach",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac3",
          "name": "Li Yuan",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac4",
          "name": "Yonghong Tian",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac5",
          "name": "Enze Xie",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac6",
          "name": "Daquan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T16:58:15.000Z",
      "title": "Magic 1대1: 1분 이내로 1분의 비디오 클립을 생성합니다.",
      "summary": "이 기술보고서에는 효율적인 메모리 사용과 추론 시간 최적화를 위한 비디오 생성 모델 \"Magic 1-For-1 (Magic141)\"을 소개합니다. 주요 아이디어는 간단합니다: 텍스트로부터 비디오의 생성을 두 가지 다른 간단한 작업으로 분해하여, 서로 다른 학습 프로세스로 구현합니다. 텍스트로부터 이미지의 생성과 이미지로부터 비디오의 생성입니다. 동일한 최적화 알고리즘을 사용하므로, 이미지로부터 비디오의 생성은 텍스트로부터 비디오의 생성보다 간단하고, 수렴이 빠르다는 것을 확인했습니다. 또한, 이미지로부터 비디오의 생성 모델의 훈련 계산 비용의 3가지 면에서 감소시키기 위한 최적화 트릭을 조사했습니다. 1) 모델의 수렴 속도를 가속화하기 위한 다모달 선행 조건 Injection, 2) 추론 시간을 가속화하기 위한 adversarial steps의 학습, 3) 추론 때의 메모리 비용 최적화를 수행합니다. 이러한 기술에 활용하여, 3초 이내에 5초의 비디오 클랩을 생성할 수 있습니다. 테스트 시간의 슬라이딩 윈도우를 적용하여, 1분간의 비디오를 1분 이내内生成할 수 있으며, 시각적 품질인 화질과 동작이 크게 향상되고, 평균 1초의 비디오 클랩의 생성에 1초 이내의 시간이 소요될 수 있습니다. 이러한 기술을 활용하여, 계산 비용과 비디오의 품질의 최적의 조정을 찾아, 이는 오픈 소스 탐색의 가이드라인이 될 것으로 기대합니다. 코드와 모델 가중치는 https://github.com/DA-Group-PKU/Magic-1-For-1에서 사용 가능합니다.",
      "upvotes": 11,
      "discussionId": "67ac23186def89f9aae56b69"
    },
    "publishedAt": "2025-02-11T23:27:13.769Z",
    "title": "Magic 1-For-1: Generating One Minute Video Clips within One Minute",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07701.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03492",
      "authors": [
        {
          "_id": "67a5a8e595df68b0a167c298",
          "user": {
            "_id": "622f103fc78da4c7ebd7c887",
            "avatarUrl": "/avatars/b0c7cd29835d92c2cd584947fcd5d520.svg",
            "isPro": false,
            "fullname": "Xie",
            "user": "Zhihui",
            "type": "user"
          },
          "name": "Zhihui Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:17:02.682Z",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c299",
          "name": "Jie chen",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29a",
          "name": "Liyu Chen",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29b",
          "name": "Weichao Mao",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29c",
          "name": "Jingjing Xu",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29d",
          "name": "Lingpeng Kong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T02:18:46.000Z",
      "title": "「강화학습에 의한 언어 모델 평가 가이드」",
      "summary": "LLM의 비판과 개선을 배우는 것은 연속적으로 개선 가능한 시스템의 구축에 중요하지만, 정확한 판단과 행동 가능한 조언 제공 능력에 제한되어 있습니다. 본 연구에서는 LLM의 코드 생성을 위한 비판을 연구하고, CTRL(Reinforcement Learning을 통한 비판 학습) 프레임워크를 제안합니다. CTRL은 인간의 슈퍼바이저가 없는 상황에서 고정된 생성자 모델의 수정 성능을 최대화하기 위해 피드백을 생성하는 비판 모델을 훈련하는 것입니다. 우리의 결과를 통해, CTRL로 훈련된 비판이 기본적인 생성자 모델과 강화된 생성자 모델 모두의 통과율을 크게 향상시키고, 연속적인 오류를 억제할 수 있음을 나타냅니다. 또한, 이러한 비판 모델은 정확한 생성 보상 모델로 작동하며, 실험 시의 스케일링을 가능하게 하며, 어려운 코드 생성 벤치마크에서 최대 106.1%의 상대적인 향상을 달성합니다.",
      "upvotes": 10,
      "discussionId": "67a5a8e695df68b0a167c2c6"
    },
    "publishedAt": "2025-02-11T23:55:37.671Z",
    "title": "Teaching Language Models to Critique via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622f103fc78da4c7ebd7c887",
      "avatarUrl": "/avatars/b0c7cd29835d92c2cd584947fcd5d520.svg",
      "fullname": "Xie",
      "name": "Zhihui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06329",
      "authors": [
        {
          "_id": "67ab4174757d2eb190af0375",
          "user": {
            "_id": "621d6f532165dc431641e438",
            "avatarUrl": "/avatars/56ccef10a8426d7160ef3586a771bd63.svg",
            "isPro": false,
            "fullname": "Kiran Kamble",
            "user": "kiranr",
            "type": "user"
          },
          "name": "Kiran Kamble",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:55.367Z",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0376",
          "name": "Melisa Russak",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0377",
          "name": "Dmytro Mozolevskyi",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0378",
          "user": {
            "_id": "6320a906a023aad6a7670e99",
            "avatarUrl": "/avatars/48071559b0c7660bf6861cfe008b3006.svg",
            "isPro": false,
            "fullname": "Muayad Sayed Ali",
            "user": "muayad",
            "type": "user"
          },
          "name": "Muayad Ali",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:53.157Z",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0379",
          "name": "Mateusz Russak",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af037a",
          "name": "Waseem AlShikh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T10:29:28.000Z",
      "title": "예기할 수 없는 사건: Finance의 FailSafe 긴 문맥 기반 QA",
      "summary": "금융 분야의 LLM 벤치마크 \"FailSafeQA\"를 제안합니다. 이 벤치마크는 LLM 기반의クエリー 답변 시스템에서 6가지의 인간 인터페이스 상호작용에 대한 LLM의 강건성과 컨텍스트 인식을 측정하기 위해 설계되었습니다. 두 가지 사례를 중점적으로 다루고 있습니다:クエリー 파일과 컨텍스트 파일.クエリー 파일의 경우, 원래의クエリ를 변형하여 영역专門성, 완전성, 언어정확성 변화를 구현합니다. 컨텍스트 파일의 경우, 악화된, 관련없는, 빈 문서의 덮어씌기를 시뮬레이션합니다. Qwen2.5-72B-Instruct를 사용하여 LLM-as-a-Judge手法를 채택하고, 강건성, 컨텍스트 배경, 컴플라이언스 스코어를 정의하여 24개의 오프췸 모델에 대해 계산합니다. 결과는 특정 모델이 입력의 섭동을 억제하기 위한 우수한 성능을 보여주고, 이들은 강건한 응답과 환상을 피하는 능력의 균형을 유지하는 필요성을 보여주는 것입니다. 특히, 가장 컴플라이언스 높은 모델인 Palmyra-Fin-128k-Instruct는 기본 성능을 유지하지만, 17%의 테스트 케이스에서 강건한 예측을 유지하는 것이 어려웠습니다. 반면, 가장 강건한 모델인 OpenAI o3-mini는 41%의 테스트 케이스에서 정보를捏造했습니다. 이러한 결과를 통해, 고성능의 모델도 크게 개선의 여지가 있음을 보여주고, FailSafeQA는 금융 애플리케이션에서 신뢰성을 최적화하는 LLM 개발에 대한 도구로서의 역할을 나타냅니다. 데이터셋은 다음과 같은 URL에서 사용 가능합니다: https://huggingface.co/datasets/Writer/FailSafeQA",
      "upvotes": 8,
      "discussionId": "67ab4175757d2eb190af03ca"
    },
    "publishedAt": "2025-02-12T02:51:41.003Z",
    "title": "Expect the Unexpected: FailSafe Long Context QA for Finance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06329.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60e61b3969bd0df25c9375da",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
      "fullname": "Melisa Russak",
      "name": "melisa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07617",
      "authors": [
        {
          "_id": "67ac1d68c29356f92ed772c5",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c6",
          "name": "Ibrahim Alabdulmohsin",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c7",
          "name": "Daniel Salz",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c8",
          "name": "Zhe Li",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c9",
          "name": "Keran Rong",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772ca",
          "name": "Xiaohua Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T15:05:33.000Z",
      "title": "100억 데이터에 대한 시각 언어 모델의 스케일링 학습",
      "summary": "이 비즈니스의 잠재적인 가능성을 증거적으로 조사합니다: 1000억 예의 전례없는 규모의 Vision-Language 모델의 사전 학습. 이 규모에서는, 많은 일반적인 서양 중심적인 분류와 검색 벤치마크(예: COCO Captions)에서 모델의 성능이 일반적으로 포화됩니다. 그러나, 문화다양성의 태스크는 1000억 예의 웹 데이터의 폭으로 더 큰 효과를 얻습니다. 이는 긴 꼬리 개념의 폭에 의한 것입니다. 또한, 모델의 다언어성을 분석하고, 저 리소스 언어에서도 효과를 얻습니다. 또한, CLIP 등 품질 필터를 사용하여 예비 학습 데이터 세트의 크기를 줄이기로, 부정적인 영향을 주는 것을 발견했습니다. 이 데이터 크기는 실제로 포괄하는 문화다양성을 가진 다언어 시스템의 구축에 중요하다는 것을 명확히 합니다.",
      "upvotes": 8,
      "discussionId": "67ac1d6ac29356f92ed77354"
    },
    "publishedAt": "2025-02-11T23:03:08.578Z",
    "title": "Scaling Pre-training to One Hundred Billion Data for Vision Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07617.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07374",
      "authors": [
        {
          "_id": "67ac1c6436464325ebe3c6e3",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e4",
          "name": "Shiyi Cao",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e5",
          "name": "Tyler Griggs",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e6",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e7",
          "name": "Xiangxi Mo",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e8",
          "name": "Shishir G. Patil",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e9",
          "name": "Matei Zaharia",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6ea",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6eb",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T08:48:48.000Z",
      "title": "LLMs는 명령에 따라 이유를 이해할 수 있으며, 내용은 중요하지 않고 구조가 중요합니다！",
      "summary": "대논리 모형（LRMs）는 긴 연쇄 오프 사인 소스（Long CoT）를 추적하여 복잡한 논리 문제를 해결합니다. 이는 반성, 후퇴, 자동 증명 등을 포함하는 것입니다. 그러나 이러한 긴 연쇄 오프 사인 소스를 추출하기 위한 훈련 방법과 데이터 요구는 이해되지 않습니다. 본 연구에서는, 대언어 모형（LLM）이 데이터 효율적인 규범적 조정（SFT）과 파라미터 효율적인 저レン지 아다퍼터（LoRA）를 통해 긴 연쇄 오프 사인 소스 논리를 효과적으로 학습할 수 있음을 발견했습니다. 17k의 긴 연쇄 오프 사인 소스 훈련 샘플을 통해, Qwen2.5-32B-Instruct 모형는 광범위한 수학과 코딩 벤치마크에서 상당한 향상을 보였으며, AIME 2024에서 56.7%（+40.0%）, LiveCodeBench에서 57.0%（+8.1%）의 성적을 기록했습니다. 이 성적은 프로프라이드 모형의 o1-preview 모형의 점수（44.6%와 59.1%）와 경쟁적입니다. 더욱 중요한 점은, 긴 연쇄 오프 사인 소스의 구조는 학습 프로세스에 있어 중요하며, 개별 논리 단계의 내용은 영향을 미치지 않는 것을 발견했습니다. 내용 관련의 섭동（예를 들어, 부정적인 샘플을 통한 훈련이나 논리 키워드의 제거）은 성능에 약간의 영향을 미칩니다. 반면, 논리적인 일관성을 파괴하는 구조적 변경（예를 들어, 논리 단계의 셔플이나 제거）는 정확도를 크게 떨어뜨립니다. 예를 들어, 부정적인 답변을 포함하는 긴 연쇄 오프 사인 소스 샘플을 통해 훈련해도, 완전히 정확한 샘플을 통해 훈련한 것과 비교하여 정확도가 3.2% 정도 떨어집니다. 이러한 관점은 LLM의 논리 능력의 발휘 방법에 대한 이해를 깊게 하고, 다음 세대의 논리 모형의 효율적인 훈련을 위한 중요한 고려점을 명확히 합니다. 이 논문은 이전에 공개된 Sky-T1-32B-Preview 모형의 학술 논문입니다. 코드는 https://github.com/NovaSky-AI/SkyThought에 제공됩니다.",
      "upvotes": 8,
      "discussionId": "67ac1c6536464325ebe3c723"
    },
    "publishedAt": "2025-02-11T22:58:37.585Z",
    "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07374.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03997",
      "authors": [
        {
          "_id": "67ac206214d5fe7767e7ec4e",
          "name": "Yu Yuan",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec4f",
          "user": {
            "_id": "63eb00a191a1b8ec4fbba2a9",
            "avatarUrl": "/avatars/0cc7cf9b6d05337603f700e0d592edf5.svg",
            "isPro": false,
            "fullname": "ShizhaoSun",
            "user": "ShizhaoSun",
            "type": "user"
          },
          "name": "Shizhao Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:14.580Z",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec50",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec51",
          "name": "Jiang Bian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T11:57:14.000Z",
      "title": "CAD 편집기: 위치 지정하여 삽입 프레임 워크와 자동화된 훈련 데이터 합성을 사용하는 텍스트 기반의 CAD 편집",
      "summary": "컴퓨터 설계 지원(CAD)는 다양한 산업에서 필수적인 요소입니다. 맥락 기반의 CAD 편집은 텍스트 기반의 지시에 따라 CAD 모델의 변경을 자동화하는 데 큰 잠재력을 가지고 있습니다만, 아직 발견되지 않았습니다. 현재의 방법들은 주로 설계 변화 생성이나 텍스트 기반의 CAD 생성에 초점을 맞추고 있지만, 텍스트 기반의 제어 지원이 부족하거나, 기존의 CAD 모델을 제약으로 취급하고 있습니다. CAD-Editor를 소개합니다. 이것은 맥락 기반의 CAD 편집의 첫 번째 프레임워크입니다. 훈련에 필요한 데모克拉트 데이터의 엄격한 대응을 해결하기 위해, 자동화된 데이터 합성 파이프라인을 제안합니다. 이 파이프라인은 설계 변화 모델을 사용하여 원본과 편집된 CAD 모델의 쌍을 생성하고, 큰 규모의 비전 언어 모델(LVLMs)을 사용하여 그 차이를 편집 지시로 요약합니다. 맥락 기반의 CAD 편집의 복합적인 특성을 해결하기 위해, 로케이션하여 필터링의 프레임워크를 제안합니다. 이는 변경이 필요한 영역을 특정하고, 그 영역에 적절한 편집을 추가하는 두 가지의 초점을 둔 서브 태스크로 분해합니다. 대규모의 언어 모델(LLMs)은 두 서브 태스크를 기반으로 사용되며, 자연어 이해와 CAD 지식의 기능에 활용됩니다. 실험은 CAD-Editor는 양질의 성능을 달성합니다.",
      "upvotes": 6,
      "discussionId": "67ac206314d5fe7767e7ec98"
    },
    "publishedAt": "2025-02-11T23:16:28.213Z",
    "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03997.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63eb00a191a1b8ec4fbba2a9",
      "avatarUrl": "/avatars/0cc7cf9b6d05337603f700e0d592edf5.svg",
      "fullname": "ShizhaoSun",
      "name": "ShizhaoSun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07527",
      "authors": [
        {
          "_id": "67ac1eaac61306b0ac95d2c6",
          "name": "Yingce Xia",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c7",
          "name": "Peiran Jin",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c8",
          "name": "Shufang Xie",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c9",
          "name": "Liang He",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ca",
          "name": "Chuan Cao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cb",
          "name": "Renqian Luo",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cc",
          "name": "Guoqing Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cd",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ce",
          "name": "Zequn Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cf",
          "name": "Yuan-Jyue Chen",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d0",
          "name": "Zekun Guo",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d1",
          "name": "Yeqi Bai",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d2",
          "name": "Pan Deng",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d3",
          "name": "Yaosen Min",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d4",
          "name": "Ziheng Lu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d5",
          "name": "Hongxia Hao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d6",
          "name": "Han Yang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d7",
          "name": "Jielan Li",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d8",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d9",
          "name": "Jia Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2da",
          "name": "Jianwei Zhu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2db",
          "name": "Kehan Wu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2dc",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2dd",
          "name": "Kaiyuan Gao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2de",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2df",
          "name": "Qian Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e0",
          "name": "Xixian Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e1",
          "name": "Yanting Li",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e2",
          "name": "Houtian Zhu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e3",
          "name": "Yeqing Lu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e4",
          "name": "Mingqian Ma",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e5",
          "name": "Zun Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e6",
          "name": "Tian Xie",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e7",
          "name": "Krzysztof Maziarz",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e8",
          "name": "Marwin Segler",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e9",
          "name": "Zhao Yang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ea",
          "name": "Zilong Chen",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2eb",
          "name": "Yu Shi",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ec",
          "name": "Shuxin Zheng",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ed",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ee",
          "name": "Chen Hu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ef",
          "name": "Peggy Dai",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f0",
          "name": "Tie-Yan Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f1",
          "name": "Haiguang Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f2",
          "name": "Tao Qin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T13:08:03.000Z",
      "title": "자연의 언어를 해석하여 과학의 발견을 촉진합니다.",
      "summary": "기초 모델은 자연언어 처리와 인공지능에 혁신적인 영향을 미치며, 기계가 인간 언어를 이해하고 생성하는 방법을 크게 향상시켰습니다. 이러한 기초 모델의 성공에 힘입어, 연구자들은 분자, 재료, 단백질, DNA, RNA 등 개별 과학 분야에서 기초 모델을 개발했습니다. 그러나 이러한 모델들은 일반적으로 독립적으로 훈련되어 있으며, 과학 분야 간의 통합 능력이 부족한 경우가 많습니다. 이러한 분야의 모든 엔티티가 순서 표현으로 표현될 수 있으며, 이들은 \"자연의 언어\"로 결합되어 Nature Language Model (단축어: NatureLM)을 소개합니다. 이는 과학 발견을 위한 순서 기반의 과학 기초 모델로 설계되었습니다. 다수의 과학 분야의 데이터로 사전 학습되어, 다음과 같은 다양한 응용을 가능하게 합니다: (i) 문자 지시를 사용하여 분자, 단백질, RNA, 재료의 생성 및 최적화; (ii) 분야 간의 생성/설계, 그리고 (iii) SMILES-to-IUPAC 번역 및 USPTO-50k의 역 합성에서 가장 先端의 성능을 달성합니다. NatureLM는 약물발견 (시각 생성/최적화, ADMET 최적화, 합성), 신규 재료 설계, 치료 단백질 또는 핵레코이드 개발 등 다양한 과학 태스크에 대한 일반적인 접근 방식을 제공합니다. 다양한 크기 (1억, 8억, 467억 파라미터)의 NatureLM 모델을 개발하였으며, 모델 크기가 증가함에 따라 분명히 성능이 향상되었습니다.",
      "upvotes": 6,
      "discussionId": "67ac1eabc61306b0ac95d346"
    },
    "publishedAt": "2025-02-11T23:10:26.895Z",
    "title": "NatureLM: Deciphering the Language of Nature for Scientific Discovery",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06589",
      "authors": [
        {
          "_id": "67ac1d45e6f1e95ccf6de3b7",
          "user": {
            "_id": "6471bddd609ae9f56368f132",
            "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
            "isPro": true,
            "fullname": "Yuchen Zhuang",
            "user": "yczhuang",
            "type": "user"
          },
          "name": "Yuchen Zhuang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-12T04:02:14.866Z",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3b8",
          "name": "Jingfeng Yang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3b9",
          "name": "Haoming Jiang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3ba",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bb",
          "name": "Kewei Cheng",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bc",
          "name": "Sanket Lokegaonkar",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bd",
          "name": "Yifan Gao",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3be",
          "name": "Qing Ping",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bf",
          "name": "Tianyi Liu",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c0",
          "name": "Binxuan Huang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c1",
          "name": "Zheng Li",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c2",
          "name": "Zhengyang Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c3",
          "name": "Pei Chen",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c4",
          "name": "Ruijie Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c5",
          "name": "Rongzhi Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c6",
          "name": "Nasser Zalmout",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c7",
          "name": "Priyanka Nigam",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c8",
          "name": "Bing Yin",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c9",
          "name": "Chao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T15:54:34.000Z",
      "title": "헤フェアステウス: 대언어 모델의 기본적인 에이전트 능력을 지속적인 학습에 의해 지속적으로 향상시키기",
      "summary": "헤ファエスチュス・フォージュ는 LLM 에이전트의 기본적인 능력을 강화하기 위한 최초의 대규모 사전 데이터 코퍼스입니다. 이 코퍼스는 API 함수 호출, 내부적인 논리론 및 계획, 환경 피드백에 대한 적응성을 강화하는 것을 목표로 합니다. 헤ファエスチュス・フォージュ는 103B개의 에이전트 고유 데이터를 포함하고 있으며, 76,537개의 API를 기록하고 있습니다. 이러한 API에는 도구 문서가 포함되어 있으며, API 함수의 지식을 제공하고 함수 호출의 과정을 강화하고 있습니다. 또한 이 코퍼스로 인한 지속적인 사전 훈련을 수행함으로써, 헤ファエスチュス는 작은 중간 규모의 오픈 소스 LLM을 초과하고, 세 개의 에이전트 벤치마크에서 상업적 LLM과 같은 수준에 도달하며, LLM의 기본적인 에이전트 능력과 새로운 태스크 또는 환경에 대한 확장성 효과에 대해 보여주고 있습니다.",
      "upvotes": 6,
      "discussionId": "67ac1d46e6f1e95ccf6de419"
    },
    "publishedAt": "2025-02-11T23:04:08.153Z",
    "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6471bddd609ae9f56368f132",
      "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
      "fullname": "Yuchen Zhuang",
      "name": "yczhuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07508",
      "authors": [
        {
          "_id": "67ac2006a6b5a26040fc94f7",
          "name": "Yang Luo",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94f8",
          "name": "Xuanlei Zhao",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94f9",
          "name": "Mengzhao Chen",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fa",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fb",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fc",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fd",
          "name": "Zhangyang Wang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fe",
          "name": "Yang You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T12:22:35.000Z",
      "title": "언어 향상 아바이도오：무료로 더 좋은 생성 비디오를 제작하세요.",
      "summary": "DiT 기반의 비디오 생성은 놀라운 성과를 달성했습니다が, 기존 모델의 확장에 관한 연구는 상대적으로 찾기가 어려운 상태입니다.本研究에서는, DiT 기반의 생성 비디오의 연속성과 질을 향상시키기 위한 훈련 필요 없는 접근법을 소개합니다.이 접근법은, 비대각 시간 주의 분포에 기반하여 프레임 간 상호 관계를 향상시키는 핵심적인 개념을 가지고 있습니다.이 접근법은 간단한 설계를 특징으로 하고, 거의 모든 DiT 기반의 비디오 생성 프레임워크에 쉽게 적용할 수 있으며, 리트레이닝이나 미세 조정이 필요하지 않습니다.각 DiT 기반의 비디오 생성 모델에서, 본 접근법은 시간의 일관성과 시각적인 질의 향상에 대해 명백한 개선을 나타냅니다.우리는, 이 연구는 향후 비디오 생성 확장의 연구에 고무를 줄 수 있을 것으로 기대합니다.",
      "upvotes": 5,
      "discussionId": "67ac200ea6b5a26040fc9709"
    },
    "publishedAt": "2025-02-11T23:14:10.293Z",
    "title": "Enhance-A-Video: Better Generated Video for Free",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04223",
      "authors": [
        {
          "_id": "67ac5e0d653d273eeaf25e59",
          "name": "Ilia Karmanov",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5a",
          "user": {
            "_id": "67ac5d85a19e34140ea1013b",
            "avatarUrl": "/avatars/e5b7446787dbbd17553dc9e11b58a0b4.svg",
            "isPro": false,
            "fullname": "Amala Sanjay Deshmukh",
            "user": "amalad",
            "type": "user"
          },
          "name": "Amala Sanjay Deshmukh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:14:49.009Z",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5b",
          "name": "Lukas Voegtle",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5c",
          "name": "Philipp Fischer",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5d",
          "user": {
            "_id": "64c7a43e0d3d1b209df90b9c",
            "avatarUrl": "/avatars/1d0d2f129b799a72345b17fd5307aa5e.svg",
            "isPro": false,
            "fullname": "Kateryna Chumachenko",
            "user": "katerynaCh",
            "type": "user"
          },
          "name": "Kateryna Chumachenko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:14:47.025Z",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5e",
          "name": "Timo Roman",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5f",
          "user": {
            "_id": "60098ca06e8ac78787773f85",
            "avatarUrl": "/avatars/be6539e5706bf07c71e553254c1751b5.svg",
            "isPro": false,
            "fullname": "Jarno Seppänen",
            "user": "jseppanen",
            "type": "user"
          },
          "name": "Jarno Seppänen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:14:51.062Z",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e60",
          "name": "Jupinder Parmar",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e61",
          "name": "Joseph Jennings",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e62",
          "name": "Andrew Tao",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e63",
          "name": "Karan Sapra",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T17:07:22.000Z",
      "title": "エクレール -- 통합 읽기 순서로 된 내용과 순서의 추출\n문서 관련\n\n이 번역은 전문성과 정확성을 유지합니다.",
      "summary": "광학 문자 인식(OCR) 기술은 문서의 이미지에서 텍스트를 추출하기 위해 광범위하게 사용되고 있으며, 효율적인 디지털화와 데이터 검색을 촉진합니다. 그러나 복잡한 문서를 처리할 때, 텍스트만 추출하는 것은 충분하지 않습니다. 문서의 구조를 완전히 이해하기 위해서는, 형식, 공식, 테이블, 여러 페이지의 여러 블록과 열의 읽기 순서, 그리고 참고문과 이미지 캡처와 같은 요소의 검출에 대한 의미 정보를 필요로 합니다. 이러한 상세한 이해는 검색, 문서의 질문응답, 그리고 대규모 언어 모델(LLMs)와 시각 언어 모델(VLMs)의 훈련 데이터의 칫솔팅과 같은 하류 작업에 필요합니다. 이를 대처하기 위해, 우리는 여러 문서 타입을 처리하기 위해 특별히 설계된 일반적인 텍스트 추출 도구로 'Eclair'을 소개합니다. 이미지가 주어지면, 'Eclair'은 읽기 순서대로 형식화된 텍스트를 추출하고, 그 경계 박스와 대응하는 의미의 클래스를 함께 추출할 수 있습니다. 이러한 새로운 능력을 자세히 평가하기 위해, 우리는 문서 수준의 OCR와 의미 분류의 다양한 인간 데이터베이스를 소개합니다. 'Eclair'은 이 벤치마크에서 가장 先端의 정확도를 달성하며, 주요 구성 요소의 메트릭에서 다른 방법보다 뛰어납니다. 또한, 기존 벤치마크에서도 'Eclair'을 평가하고, 다양한 평가 기준에 대한 변容성과 강점을 보여주었습니다.",
      "upvotes": 3,
      "discussionId": "67ac5e0f653d273eeaf25eea"
    },
    "publishedAt": "2025-02-12T04:25:54.558Z",
    "title": "Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60098ca06e8ac78787773f85/BfZ57W-gCoY32J60tx7dN.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04223.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60098ca06e8ac78787773f85",
      "avatarUrl": "/avatars/be6539e5706bf07c71e553254c1751b5.svg",
      "fullname": "Jarno Seppänen",
      "name": "jseppanen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07445",
      "authors": [
        {
          "_id": "67ac216d602eb9ca8a517be6",
          "name": "Nurit Cohen-Inger",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be7",
          "name": "Yehonatan Elisha",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be8",
          "name": "Bracha Shapira",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be9",
          "name": "Lior Rokach",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517bea",
          "name": "Seffi Cohen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T10:43:36.000Z",
      "title": "チャメラン처럼 LLMs의 평가는 잊어봅시다.",
      "summary": "대 언어 모델(LLMs)는 공개 벤치마크에서 높은 점수를示す 경우가 많지만, 이러한 높은 점수는 실제 언어 이해에 의한 성능의 높음을 숨겨놓는 가능성이 있는 것이다. 우리는 C-BOD(Chameleon Benchmark Overfitting Detector)를 도입합니다. C-BOD는 파라미터 변환을 통해 벤치마크의 Prompt를 체계적으로 변형시키고, LLMs의 오버fitting을 감지하는 메타 평가 프레임워크입니다. 입력을 재구성하면서 의미적인 내용을 유지하고 라벨을 저장함으로써, C-BOD는 모델의 성능이 기억한 패턴으로 구동되는지 확인합니다. MMLU 벤치마크에서 26개의 발전된 LLMs를 사용하여 평가한 결과, 우리 방식은 약 2.15%의 평균적인 성능 저하율을 나타내며, 26개의 모델 중 20개의 모델이 통계적으로 유의한 차이를 나타냅니다. 특히, 기본 정확도가 높은 모델은 변형에 따른 성능 저하가 크며, 큰 LLMs는 재구성에 민감하며, 둘 다 고정된 Prompt 패턴에 의존하는 가능성이 높다는 것을 밝혀졌습니다. 대조적으로, Llama 가족과 기본 정확도가 낮은 모델은 통계적으로 유의한 저하는 보이지 않았으며, 표면적인 코드에 의존하는 가능성은 낮다는 것을 보여줍니다. 또한, C-BOD는 데이터셋과 모델에 의존하지 않는 설계로, 훈련 프로세스에 쉽게 통합할 수 있으며, 더 강력한 언어 이해를 촉진할 수 있습니다. 우리의 발견은, 순위 점수를 초월하여, LLM 평가의 놀라움과 확장성을 우선시하는 것을 커뮤니티에 요구하는 것을 보여줍니다.",
      "upvotes": 3,
      "discussionId": "67ac216e602eb9ca8a517c1d"
    },
    "publishedAt": "2025-02-11T23:22:50.454Z",
    "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07445.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6731e56a07cf693a1104d2cb",
      "avatarUrl": "/avatars/46a3269a19c7e6bfb7004a5da9701459.svg",
      "fullname": "Seffi Cohen",
      "name": "seffico",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04465",
      "authors": [
        {
          "_id": "67a953844ea315a67e02461d",
          "user": {
            "_id": "63195d0582e7eec0eac040e3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
            "isPro": false,
            "fullname": "Luca Della Libera",
            "user": "lucadellalib",
            "type": "user"
          },
          "name": "Luca Della Libera",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T10:03:10.257Z",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e02461e",
          "name": "Francesco Paissan",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e02461f",
          "name": "Cem Subakan",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e024620",
          "name": "Mirco Ravanelli",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T19:24:50.000Z",
      "title": "FocalCodec: 낮은 비트레이트 음성 코딩을 위한 초점 변조 네트워크 사용\n\n(Note: The original text \"FocalCodec: 焦点調制ネットワークを用いた低ビットレート音声コーディング\" is a mix of Chinese and Japanese. The translation provided above is in Korean. If you intended to translate from English to Korean, the correct translation would be \"FocalCodec: 초점 변조 네트워크를 사용한 낮은 비트레이트 음성 코딩\".)",
      "summary": "대 언어 모델은 큰 데이터 세트에 대해 자기규칙을 통해 사전 학습을 통해 자연어 처리를 혁신적으로 발전시켰습니다. 이 성공에 힘입어 연구자들은 뉴럴 어도이오 코더를 사용하여 연속적인 음성을 토큰화하고 이러한 방법을 음성에 적용하여 탐색하는 방법을 연구했습니다. 그러나 현재의 접근 방식은 고 비트레이트, 세ман틱 정보나 음향 정보의 손실을 최소화하기 위해 다 코드북 설계를 의존하고, 이는 하류 태스크의 아키텍처 복잡성을 증가시키고 여러 제한이 있습니다. 이러한 문제를 해결하기 위해 우리는 효율적인 저 비트레이트 코드북을 소개합니다. 이는 포커 코더에 기반하여 1개의 비네티어 코드북을 사용하여 0.16에서 0.65 kbps 범위의 음성을 압축합니다. 포커 코더는 현재의 최단 기술보다 낮은 비트레이트로 음성 재 합성과 음성 변환에 경쟁적 성능을 제공하며, 다 언어 음성과 노이즈 환경에서도 효과적으로 처리됩니다. 하류 태스크의 평가에 따르면 포커 코더는 충분한 세ман틱 정보와 음향 정보를 보존하며 동시에 생성 모델링에 적합합니다. 데모 샘플, 코드 및 체크포인트는 https://lucadellalib.github.io/focalcodec-web/에서 사용 가능합니다.",
      "upvotes": 2,
      "discussionId": "67a953854ea315a67e024659"
    },
    "publishedAt": "2025-02-12T01:31:44.368Z",
    "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04465.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63195d0582e7eec0eac040e3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
      "fullname": "Luca Della Libera",
      "name": "lucadellalib",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07531",
      "authors": [
        {
          "_id": "67ac21acaa680a0f8782d273",
          "name": "Sixiao Zheng",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d274",
          "name": "Zimian Peng",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d275",
          "name": "Yanpeng Zhou",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d276",
          "name": "Yi Zhu",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d277",
          "name": "Hang Xu",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d278",
          "name": "Xiangru Huang",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d279",
          "name": "Yanwei Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T13:11:59.000Z",
      "title": "VidCRAFT3: 카메라, 물체, 광원을 제어하는 이미지에서 동영상의 생성",
      "summary": "최근의 이미지에서 동영상 생성 기술은 카메라의 이동이나 물체의 움직임 등 시각 요소 중 하나 또는 두 개를 제어할 수 있음을 보여주었다が, 데이터와 네트워크의 효율에 따라 여러 시각 요소를 제어할 수 있는 것은 존재하지 않는다. 본 논문에서는 VidCRAFT3라는 새로운 프레임워크를 소개하고, 카메라의 이동, 물체의 움직임, 빛의 방향을 동시에 제어하여 정밀한 이미지로부터 동영상 생성을 가능하게 하는 것이다. 각 시각 요소의 제어를 더욱 분리하기 위해, 스펙트럴 트립ル 어텐션 트랜스포머를 제안하고, 빛의 방향, 텍스트, 이미지를 대칭적으로 통합하는 것을 목표로 한다. 다수의 현실적인 동영상 데이터 셋에는 빛의 방향의 Annotation이 없기 때문에, 고품질의 합성 동영상 데이터 셋을 구축하여 VideoLightingDirection (VLD) 데이터 셋으로 공개하였다. 이 데이터 셋은 빛의 방향의 Annotation과 다양한 외관 물체를 포함하고 있으며, VidCRAFT3가 강력한 빛의 전달과 반사 효과를 효과적으로 대응할 수 있도록 한다. 또한, VidCRAFT3가 동시에 시각 요소(카메라의 이동, 물체의 움직임, 빛의 방향)를 Annotation된 훈련 데이터의 필요성을 제거하기 위해, 3단계의 훈련 전략을 제안하였다. 벤치마크 데이터 셋에서의 검증은 VidCRAFT3가 고품질의 동영상 콘텐츠 생성이 가능하고, 제어의 정밀도와 시각적 일관성 측면에서 현재의 최상위 기술에 초월함을 보여주었다. 모든 코드와 데이터는 공개적으로 이용이 가능합니다. 프로젝트 페이지: https://sixiaozheng.github.io/VidCRAFT3/。",
      "upvotes": 2,
      "discussionId": "67ac21b2aa680a0f8782d3bd"
    },
    "publishedAt": "2025-02-11T23:21:13.452Z",
    "title": "VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07531.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07776",
      "authors": [
        {
          "_id": "67ac1f7851c7f3b53ffc4def",
          "name": "Chenchen Gu",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df0",
          "name": "Xiang Lisa Li",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df1",
          "name": "Rohith Kuditipudi",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df2",
          "name": "Percy Liang",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df3",
          "user": {
            "_id": "661595d1b3d0b21da55cde7d",
            "avatarUrl": "/avatars/ba3fa065536518637d21a5c46cee5dd1.svg",
            "isPro": false,
            "fullname": "Tatsu Hashimoto",
            "user": "thashim",
            "type": "user"
          },
          "name": "Tatsunori Hashimoto",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-12T04:11:36.912Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:58:04.000Z",
      "title": "언어 모델 API의 Prompt 캐쉬 검토",
      "summary": "プロンプト캐시링은 대규모 언어 모델(LLMs)에서 데이터 의존성의 시간 변화를 일으키는 주요 요소로, 캐시된 프로ンプト는 캐시되지 않은 프로ンプト보다 빠르게 처리된다. 이러한 시간 차이는 측면 채널 공격의 위험을 불러일으킨다. 예를 들어, 캐시가 사용자 간 공유되어 있을 때, 공격자는 고속의 API 응답 시간으로부터 캐시된 프로ンプト를 특정하고, 다른 사용자의 프로ンプト에 대한 정보를 배울 수 있다. 프로ンプト캐시링이 개인 정보 누설의 원인으로 여겨지는 만큼, API 제공사의 캐시 정책에 대한 투명성이 중요하다. 이에 따라, 우리는 실제 세계의 LLM API 제공사에 대한 프로ンプト캐시링의 검출을 목표로 통계적 평가를 개발하고 수행하였다. 우리는 7개의 API 제공사에서 사용자 간 글로벌 캐시 공유를 검출하고, 사용자의 프로ンプト에 대한 잠재적인 개인 정보 누설을 확인하였다. 프로ンプト캐시링에 의한 시간 변화는 모델 아키텍처에 대한 정보 누설도 불러일으킨다. 특히, OpenAI의 내장 모델이 이전에 공개되지 않은 디코더만 가진 Transformer임을 증명하였다.",
      "upvotes": 2,
      "discussionId": "67ac1f7851c7f3b53ffc4e1b"
    },
    "publishedAt": "2025-02-11T23:11:49.993Z",
    "title": "Auditing Prompt Caching in Language Model APIs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05932",
      "authors": [
        {
          "_id": "67ac4356401012b81050022a",
          "user": {
            "_id": "67ac430c4ab9207cc227d23f",
            "avatarUrl": "/avatars/59c499cc191e28a66ae917963c28ffb3.svg",
            "isPro": false,
            "fullname": "Tenglong Liu",
            "user": "LTL07",
            "type": "user"
          },
          "name": "Tenglong Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:15:09.627Z",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022b",
          "name": "Jianxiong Li",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022c",
          "name": "Yinan Zheng",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022d",
          "name": "Haoyi Niu",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022e",
          "name": "Yixing Lan",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022f",
          "name": "Xin Xu",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b810500230",
          "name": "Xianyuan Zhan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T15:22:38.000Z",
      "title": "파라미터 공간에서 기술 확장과 조합",
      "summary": "인간은, 미리 알고 있는 능력(capability)을 재활용하여 새로운 문제를 해결하고, 문제 해결 과정에서 새로운 기술을 개발하는 것을 능숙하게 합니다. 이 패러다임은, 자동 전이 에이전트의 개발에서, 새로운 문제를 자동으로 진화시키는 시스템의 개발으로 인해, 광범위하게 사용되고 있습니다. 그러나 이전의 방법들은, 새로운 기술을 확장할 때 훈련 효율이 제한되어, 미리 알고 있는 능력이 완전히 활용되어 새로운 태스크의 학습을 지원할 수 없었습니다. 본 논문에서는, 파라미터 스킬 확장과 결합(PSEC)이라는 새로운 프레임워크를 제안합니다. 이 프레임워크는, 기술 라이브러리를 관리할 수 있게 하여, 새로운 문제를 효율적으로 해결하고, 에이전트의 능력을 진화시킬 목적으로 설계되었습니다. 이 라이브러리는, 파라미터 효율적인 미세 조정으로, 기술의 기본 요소를 진보적으로 포트폴리오로 구성할 수 있습니다. 이 구조는, LoRA 모듈을 결합하여, 서로 다른 기술을 직접적으로 조합할 수 있게 하며, 기술 간의 공유 정보를 활용하여 새로운 기술을 효과적으로 프로그래밍할 수 있습니다. 이를 기반으로, 맥락에 관련된 모듈을 제안하고, 새로운 태스크를 공동으로 처리하기 위해, 동적으로 다양한 기술을 활성화할 수 있습니다. 다목적의 조합, 동적hift, 그리고 지속적인 정책hift 등 다양한 애플리케이션을 지원함으로써, D4RL, DSRL 벤치마크, 그리고 DeepMind Control Suite에서 얻은 결과를 통해, PSEC는 미리 알고 있는 능력을 효율적으로 활용하여 새로운 문제를 해결하는 능력을 보여주고, 기술 라이브러리를 확장하여 능력을 진화시킬 수 있음을 입증합니다. 프로젝트 웹 사이트: https://ltlhuuu.github.io/PSEC/",
      "upvotes": 0,
      "discussionId": "67ac435b401012b8105003dc"
    },
    "publishedAt": "2025-02-12T04:53:50.325Z",
    "title": "Skill Expansion and Composition in Parameter Space",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05932.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ac430c4ab9207cc227d23f",
      "avatarUrl": "/avatars/59c499cc191e28a66ae917963c28ffb3.svg",
      "fullname": "Tenglong Liu",
      "name": "LTL07",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]