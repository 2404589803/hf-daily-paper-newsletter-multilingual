[
  {
    "paper": {
      "id": "2502.12900",
      "authors": [
        {
          "_id": "67b54851b986e35c41e063da",
          "user": {
            "_id": "66975b9f8031bf92b428e138",
            "avatarUrl": "/avatars/3254281a7bac1c8ddde1d6bc7e518b2f.svg",
            "isPro": false,
            "fullname": "Yuhao Zhang",
            "user": "Yoohao",
            "type": "user"
          },
          "name": "Yuhao Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-19T02:56:18.848Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063db",
          "user": {
            "_id": "66597f2cf769c3c443b7cf41",
            "avatarUrl": "/avatars/735cc8aa430748d20ca7312c72b1eaf1.svg",
            "isPro": false,
            "fullname": "Chihang Lau",
            "user": "puccho",
            "type": "user"
          },
          "name": "Zhiheng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:05.678Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063dc",
          "user": {
            "_id": "668e7f46c243a12604035758",
            "avatarUrl": "/avatars/35bd20032fafb7d7603266cf9a72d1e0.svg",
            "isPro": false,
            "fullname": "Fan Bu",
            "user": "FanBuCUHK",
            "type": "user"
          },
          "name": "Fan Bu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:42:08.544Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063dd",
          "user": {
            "_id": "67b587c8882e49771f610b51",
            "avatarUrl": "/avatars/aecfb38b44141b8284416fc261692909.svg",
            "isPro": false,
            "fullname": "Ruiyu Zhang",
            "user": "PhoenixAxis",
            "type": "user"
          },
          "name": "Ruiyu Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:42:14.866Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063de",
          "user": {
            "_id": "637c6703ca8542a0ba900ccb",
            "avatarUrl": "/avatars/288ed63a1efa566c3f01e850c6ba5dd5.svg",
            "isPro": false,
            "fullname": "Wang",
            "user": "Benyou",
            "type": "user"
          },
          "name": "Benyou Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:42:23.845Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063df",
          "name": "Haizhou Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T14:36:39.000Z",
      "title": "Soundwave: 펄파는 적은 쪽이 많습니다.",
      "summary": "현재의 엔드포인트에서 엔드포인트까지의 언어 대 언어 모델(LLMs)은 일반적으로 큰 규모의 라벨링 데이터로 훈련되어 있으며, 데이터 효율적인 훈련은 심도있게 논의되어 있지 않습니다. 우리는 음성과 문자 사이의 두 가지 기본적인 문제를 중점적으로 다루고 있습니다: 표현 공간의 간격과 순서 길이의 불합성. 우리는 새로운 아키텍처인 Soundwave와 효율적인 훈련 전략을 사용하여 이러한 문제를 해결하는 방법을 제안합니다. 결과적으로, Soundwave는 음성 번역 및 AIR-Bench 음성 태스크에서 가장 先端의 Qwen2-Audio보다 뛰어난 성능을 보입니다. 또한 훈련 데이터의 1/50을 사용합니다. 진행된 분석에 따르면, Soundwave는 변환 과정에서도 지능을 유지하고 있음을 알 수 있습니다. 이 프로젝트는 https://github.com/FreedomIntelligence/Soundwave에 액세스할 수 있습니다.",
      "upvotes": 47,
      "discussionId": "67b54852b986e35c41e06426"
    },
    "publishedAt": "2025-02-19T00:22:36.628Z",
    "title": "Soundwave: Less is More for Speech-Text Alignment in LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66975b9f8031bf92b428e138",
      "avatarUrl": "/avatars/3254281a7bac1c8ddde1d6bc7e518b2f.svg",
      "fullname": "Yuhao Zhang",
      "name": "Yoohao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11564",
      "authors": [
        {
          "_id": "67b40f93aba9e111862052ab",
          "user": {
            "_id": "65e5bd4568234ef5d6decadc",
            "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
            "isPro": false,
            "fullname": "Jaehyeong Jo",
            "user": "harryjo97",
            "type": "user"
          },
          "name": "Jaehyeong Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:27.544Z",
          "hidden": false
        },
        {
          "_id": "67b40f93aba9e111862052ac",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T08:54:29.000Z",
      "title": "연속확산 모형의 언어 모델링",
      "summary": "ディフュージョンモデル은, 이산 분류 데이터의 모델링에 있어서 자동 회귀 모델의 유망한 대체로 등장했습니다. 그러나, 직접 이산 데이터 공간에서 작동하는 ディフュージョンモデル은, 이산 상태 사이에서 신호가 손실되어, 연속적인 개선의 힘에 완전히 활용되지 않습니다. 현재의 이산 데이터에 대한 연속 ディフュージョンモデル는, 이산 접근 방식과 비교하여 성능이 제한되어, 그 사이에 불확실한 연결은 이산 데이터에 대한 ディフュージョンモデル의 개발에 제약을 가집니다. 본 연구에서는, 이산 분류 분포의 구조를 통합하는 연속 ディフュージョンモデル를 제안합니다. 이산 ディフュージョン과 통계 다양체 위의 연속 흐름 사이의 연결을 확립하고, 그 유사성에 기반하여, 이전의 이산 ディフュージョンモデル을 일반화하는 간단한 ディフュージョン 프로세스의 설계를 제안합니다. 또한, 방사형 대칭성을 기반으로 하는 시뮬레이션 제한없는 훈련 프레임워크와 고차원 다양체의 문제를 해결하는 간단한 방법도 제안합니다. 언어 모델링 벤치마크와 다른 모델에 대한 상세한 실험은, 우리 방법が 현재의 이산 ディフュージョンモデル을 초월하고, 자동 회귀 모델의 성능에 가까운 것을 보여줍니다. 코드는, https://github.com/harryjo97/RDLM{https://github.com/harryjo97/RDLM}에 공개되어 있습니다.",
      "upvotes": 30,
      "discussionId": "67b40f94aba9e111862052d5"
    },
    "publishedAt": "2025-02-18T22:43:02.567Z",
    "title": "Continuous Diffusion Model for Language Modeling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11564.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65e5bd4568234ef5d6decadc",
      "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
      "fullname": "Jaehyeong Jo",
      "name": "harryjo97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11079",
      "authors": [
        {
          "_id": "67b40141ad717fe02e188c1a",
          "user": {
            "_id": "63a950ac3453852ef5394178",
            "avatarUrl": "/avatars/48a5e537b10e2247a17e63502e3201a6.svg",
            "isPro": false,
            "fullname": "Lijie Liu",
            "user": "liulj13",
            "type": "user"
          },
          "name": "Lijie Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:42.570Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1b",
          "user": {
            "_id": "657ab4705e1c941f4c2f7877",
            "avatarUrl": "/avatars/c450f81f83dd0436ae120ab15616c4f7.svg",
            "isPro": false,
            "fullname": "Tianxiang Ma",
            "user": "Grayson111",
            "type": "user"
          },
          "name": "Tianxiang Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:45:00.117Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1c",
          "user": {
            "_id": "63b415037af2e415f2599c18",
            "avatarUrl": "/avatars/4afbe7d6d05a702f1beeed9c53e78153.svg",
            "isPro": false,
            "fullname": "Bingchuan Li",
            "user": "lbc402",
            "type": "user"
          },
          "name": "Bingchuan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:47:57.441Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1d",
          "user": {
            "_id": "6304e2dabad6ce7fc0287d57",
            "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
            "isPro": false,
            "fullname": "Zhuowei_Chen",
            "user": "ZhuoweiChen",
            "type": "user"
          },
          "name": "Zhuowei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:47:50.995Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1e",
          "name": "Jiawei Liu",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1f",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c20",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T11:02:50.000Z",
      "title": "Fantom: 주제 일치의 비디오 생성에 의한 크로스 모달 어레이메이션",
      "summary": "연속적인 비디오 생성의 기초 모델은 다양한 애플리케이션을 위해 발전하고 있지만, 주제 일치의 비디오 생성은 아직 탐색 단계에 남아 있습니다. 이를 \"Subject-to-Video\"라고 부르며, 참조 이미지에서 주제 요소를 추출하고 맥락 지시에 따라 주제 일치의 비디오를 생성하는 것을 의미합니다. 우리는 Subject-to-Video의 본질은 맥락과 이미지의 이중모드 프로리밍의 균형을 조정하고, 두 가지 맥락과 시각적 콘텐츠를 깊은 수준에서 동시에 대응시키는 것이라고 믿습니다. 따라서, 우리는 Phantom이라는 통합적인 비디오 생성 프레임워크를 제안합니다. 기존의 맥락으로부터 비디오의 아키텍처를 기반으로, 맥락 이미지注入 모델을 재설계하고, 맥락 이미지 비디오 트라이플트 데이터에 대해 더 깊은 학습을 수행하여, 맥락과 이미지의 크로스 모드 대응을 달성합니다. 특히, 인간 생성에서의 주제 일치를 강조하고, 기존의 ID 유지의 비디오 생성을 포함하면서 기능 개선을 수행합니다. 이 프로젝트의 홈 페이지는 https://phantom-video.github.io/Phantom/에 있습니다.",
      "upvotes": 27,
      "discussionId": "67b40144ad717fe02e188cb2"
    },
    "publishedAt": "2025-02-18T21:56:39.407Z",
    "title": "Phantom: Subject-consistent video generation via cross-modal alignment",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63a950ac3453852ef5394178/HuVZ5d9xTlI4R1onRv_F5.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a950ac3453852ef5394178",
      "avatarUrl": "/avatars/48a5e537b10e2247a17e63502e3201a6.svg",
      "fullname": "Lijie Liu",
      "name": "liulj13",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12464",
      "authors": [
        {
          "_id": "67b55b2cc92c4aa82c13562d",
          "user": {
            "_id": "64ad5f59b7e4b2c1ce47eb43",
            "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg",
            "isPro": false,
            "fullname": "Seanie Lee",
            "user": "Seanie-lee",
            "type": "user"
          },
          "name": "Seanie Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:53.341Z",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c13562e",
          "name": "Dong Bok Lee",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c13562f",
          "name": "Dominik Wagner",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135630",
          "name": "Minki Kang",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135631",
          "user": {
            "_id": "63a9379e2e05ca32e352d93b",
            "avatarUrl": "/avatars/6cda37befc873a92ed6d5dcba507954a.svg",
            "isPro": false,
            "fullname": "Haebin Seong",
            "user": "hbseong",
            "type": "user"
          },
          "name": "Haebin Seong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:51:37.783Z",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135632",
          "name": "Tobias Bocklet",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135633",
          "name": "Juho Lee",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135634",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T02:51:17.000Z",
      "title": "SafeRoute: 효율적이고 정확한 안전성 위해 적응적인 모델 선택\n대규모 언어 모델의 안전성 가이드라인",
      "summary": "리アルウォールディスプレイズ에서 대규모 언어 모델(LLMs)의 도입은 강력한 안전 ゲード モデル이 필요합니다. 이들은 유해한 사용자 프롬프트를 감지하고 차단하기 위해 사용됩니다. 그러나 큰 안전 ゲード モデル는 강력한 성능을 달성하지만, 계산 비용이 커집니다. 이를 완화하기 위해 작은 ジュース モデル이 사용됩니다. 그러나 \"난해한\" 예외 사례에서는 큰 モデル이 정확한 예측을 제공하기 위해 사용하는 モデル과 비교하여 낮은 성능을 보입니다. 우리는 많은 입력이 작은 モデルで 신뢰적으로 처리할 수 있고, 그 작은 부분만 큰 モデル의 계산 능력이 필요함을 발견했습니다. 이를 토대로 SafeRoute라는 이진 로터를 제안합니다. 이 방법은 ローター가 어려운 데이터에 한해서 큰 안전 ゲード モデル을 선택적으로 적용하고, 이러한 데이터에 대한 효율화를 도모하면서, 이러한 데이터에 대한 정확도를 유지합니다. 여러 벤치마크 데이터 세트에서의 실험 결과를 통해, 우리의 적응적 모델 선택은 계산 비용과 안전성 성능의 트레이드오프를 크게 개선하고, 관련된 베이스라인을 초과하는 것을 보여줍니다.",
      "upvotes": 24,
      "discussionId": "67b55b2dc92c4aa82c13568b"
    },
    "publishedAt": "2025-02-18T23:23:34.214Z",
    "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ad5f59b7e4b2c1ce47eb43/ZEq_vSLjsXuPX3O-TWIpE.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12464.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ad5f59b7e4b2c1ce47eb43",
      "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg",
      "fullname": "Seanie Lee",
      "name": "Seanie-lee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13131",
      "authors": [
        {
          "_id": "67b5461d29cc269e5a4eb823",
          "name": "Feng Luo",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb824",
          "user": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "isPro": false,
            "fullname": "Rui Yang",
            "user": "Ray2333",
            "type": "user"
          },
          "name": "Rui Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:23.095Z",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb825",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb826",
          "user": {
            "_id": "634b9914dcf125e4da02498b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634b9914dcf125e4da02498b/crRgFroWq5U6XWtvlTXSZ.jpeg",
            "isPro": false,
            "fullname": "Chunyuan Deng",
            "user": "CharlesDDDD",
            "type": "user"
          },
          "name": "Chunyuan Deng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:56:33.053Z",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb827",
          "name": "Jiarui Yao",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb828",
          "name": "Jingyan Shen",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb829",
          "user": {
            "_id": "6719d581a6cad13741b8bc7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719d581a6cad13741b8bc7f/w4EttqfXRgWZJc6HpYOS9.jpeg",
            "isPro": false,
            "fullname": "Huan Zhang",
            "user": "huanzhang12",
            "type": "user"
          },
          "name": "Huan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:52:47.329Z",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb82a",
          "name": "Hanjie Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:55:26.000Z",
      "title": "주성분 분석에 의한 다양한 인간 취향 학습의 재검토",
      "summary": "인간의 취미를 이해하는 것은 기초 모델의 향상과 성격을 지닌 AI 시스템의 구축에 중요합니다. 그러나 취미는 본질적으로 다양하고 복잡하며, 전통적인 보상 모델이 그 전체 범위를 파악하는 것이 어렵습니다. 또한 세부적인 취미 데이터는 도움이 됩니다만, 그 수집은 비싼 반면 확장이 어렵습니다. 본 논문에서는, 다양한 인간 취미를 추출하기 위한 새로운 접근 방식인 분해된 보상 모델(DRMs)을 사용합니다. 우리의 주요 전망은 인간 취미를 벡터로 표현하고 주성분 분석(PCA)을 사용하여 분석하는 것입니다. 취미와 거부된 응답의 은닉 벡터의 차이를 데이터셋으로 구축함으로써, DRMs은 취미의 다른 면을 감지하는 직교 기저 벡터를 특정합니다. 이러한 분해된 보상은 적절한 사용자의 요구에 따라 유연하게 조합할 수 있으며, 설명 가능한 확장 가능한 전통적인 보상 모델의 대체자로 제공됩니다. DRMs은 의미 있는 취미의 차원(예: 효과성, 안전성, 훌륭성)을 효과적으로 추출하고, 새로운 사용자에 대해 추가 훈련을 하지 않도록 변경할 수 있음을 보여줍니다. 이러한 결과를 통해, DRMs은 성격을 지닌 LLM의 설명 가능한 alignment의 강력한 프레임워크로서의 역할을 밝혀줍니다.",
      "upvotes": 23,
      "discussionId": "67b5461f29cc269e5a4eb8bc"
    },
    "publishedAt": "2025-02-18T21:59:45.466Z",
    "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13131.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d45451c34a346181b130dd",
      "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
      "fullname": "Rui Yang",
      "name": "Ray2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13143",
      "authors": [
        {
          "_id": "67b546c0d8a1eac02c605f6a",
          "user": {
            "_id": "63c3e8abc7d7f4c63a515a02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
            "isPro": false,
            "fullname": "Zekun Qi",
            "user": "qizekun",
            "type": "user"
          },
          "name": "Zekun Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:21.001Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6b",
          "user": {
            "_id": "65f9533b136fb8ddbd14e1fa",
            "avatarUrl": "/avatars/d88f75da0448093ccd1babba2a37d73f.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "WenyaoZhang",
            "type": "user"
          },
          "name": "Wenyao Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T10:08:31.789Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6c",
          "user": {
            "_id": "66bde456198f9d79f2be2d17",
            "avatarUrl": "/avatars/8c349aecb8a3a7cd7ef9d69e94eca8bd.svg",
            "isPro": false,
            "fullname": "Yufei Ding",
            "user": "YufeiD",
            "type": "user"
          },
          "name": "Yufei Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T10:08:57.294Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6d",
          "user": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "isPro": false,
            "fullname": "Runpei Dong",
            "user": "RunpeiDong",
            "type": "user"
          },
          "name": "Runpei Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:18.622Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6e",
          "name": "Xinqiang Yu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6f",
          "name": "Jingwen Li",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f70",
          "name": "Lingyun Xu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f71",
          "name": "Baoyu Li",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f72",
          "name": "Xialin He",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f73",
          "name": "Guofan Fan",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f74",
          "name": "Jiazhao Zhang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f75",
          "name": "Jiawei He",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f76",
          "name": "Jiayuan Gu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f77",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f78",
          "name": "Kaisheng Ma",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f79",
          "name": "Zhizheng Zhang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f7a",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f7b",
          "name": "Li Yi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:02.000Z",
      "title": "이제까지: 언어 기반의 방향성 다리는 공간적 이유론과 대상의 조작을 연결하고 있습니다.",
      "summary": "공간知能在具象적인 AI의 중요한 구성 요소 중 하나이며, 로봇이 환경과 이해하고 상호작용하는 데 도움을 줍니다. 최근의 발전은 VLM이 물체의 위치를 인식하고 위치 관계를 이해하는 능력을 향상시키지만, 물체의 방향을 결정하는 능력은 아직 존재하지 않습니다. 이 결점을 해결하기 위해, 기하학적인 추론이 필요하며, 직감적이고 표현력이 있는 방향의 표현 방법이 필요합니다. 이 맥락에서, 자연어는 표준 프레임보다 유연한 표현 공간 제공하며, 명령을 듣는 로봇 시스템에 특히 적합하다는 점을 제안합니다. 이 논문에서는, 문맥적 방향의 개념을 통해 자연어로 물체의 방향을 정의하는 방법을 소개합니다 (예를 들어, USB의 'PLUG IN' 방향이나 칼의 'HANDLE' 방향). 이를 지원하기 위해, 3D 모델의 규모가 큰 데이터 세트인 OrientText300K를 구축합니다. 이 데이터 세트는 기능적인 세ман틱과 기하학적인 이해를 결합하는 것을 목표로 합니다. VLM 시스템에 문맥적 방향을 포함하여, 로봇이 위치와 방향의 두 가지 제약을 가진 동작을 생성할 수 있게 합니다. 시뮬레이션과 실세계에서의 확장 실험은, 예를 들어 Open6DOR에서 48.7%의 정확도, SIMPLER에서 74.9%의 정확도를 보여주며, 로봇의 동작 능력의 크게 향상을 보여주었습니다.",
      "upvotes": 22,
      "discussionId": "67b546c5d8a1eac02c606090"
    },
    "publishedAt": "2025-02-18T21:51:33.957Z",
    "title": "SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c3e8abc7d7f4c63a515a02",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
      "fullname": "Zekun Qi",
      "name": "qizekun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13145",
      "authors": [
        {
          "_id": "67b54b04bd51b4e46e39d287",
          "user": {
            "_id": "6577073fc2bf55b1f6bafb49",
            "avatarUrl": "/avatars/58803398b1a918b7570db17893e65122.svg",
            "isPro": false,
            "fullname": "liao",
            "user": "LegendBC",
            "type": "user"
          },
          "name": "Bencheng Liao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:00.934Z",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d288",
          "name": "Hongyuan Tao",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d289",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28a",
          "user": {
            "_id": "646b3db131968a60a01e4cf5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
            "isPro": false,
            "fullname": "Tianheng Cheng",
            "user": "wondervictor",
            "type": "user"
          },
          "name": "Tianheng Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:58.351Z",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28b",
          "name": "Yingyue Li",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28c",
          "name": "Haoran Yin",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28d",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28e",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:57.000Z",
      "title": "마르치모더럴 마바： 디코더만의 마르치모더럴 상태 스펙트럼 모델에 의한 2차원에서 선형의 열처리",
      "summary": "최근의 다모달 대언어 모델(MLLM)은 놀라운 성능을 달성하지만, 계산복잡성과 키-밸류 캐시 요구가 증가하며, 시각적 인코더의 의존관계로 인해 구현에 문제가 있습니다. 우리는 현재의 MLLM에서 발전적인 쏘트에 의한 중간 계산복잡도를 갖는 다모달 상태 공간 모델을 개발하는 프레임워크인 mmMamba를 제안합니다. 이 접근법은 학습된 디코더만 가진 MLLM을 직접적으로, RNN 기반의 LLM이나 시각적 인코더를 필요로 하지 않는 선형 계산복잡도의 아키텍처로 변환할 수 있게 합니다. 또한 Transformer로부터 Mamba를 만들 때의 엔드포인트 전략을 제안하고, Transformer로부터 Mamba로의 효과적인 지식전달을 수행하는 세 단계의 쏘트 레시피도 제안합니다. 이 방법은 Transformer와 Mamba의 레이어의 조합을 통해 유연한 하이브리드 아키텍처를 지원하고, 연속성 효과와 성능 조정을 가능하게 합니다. Transformer 기반의 디코더만 가진 HoVLE에서 쏘트된 mmMamba-linear는 현재의 선형과 오더 계산복잡도를 갖는 VLMs과 비교하여 경쟁적인 성능을 달성하고, mmMamba-hybrid는 더 높은 성능을 달성하며, HoVLE의 능력에 가까워집니다. 103K 토큰에서는 mmMamba-linear는 HoVLE보다 20.6배의 속도업과 75.8%의 GPU 메모리 줄임, mmMamba-hybrid는 13.5배의 속도업과 60.2%의 메모리 사용량을 달성합니다. 코드와 모델은 https://github.com/hustvl/mmMamba에 공개되어 있습니다.",
      "upvotes": 18,
      "discussionId": "67b54b05bd51b4e46e39d2bb"
    },
    "publishedAt": "2025-02-18T22:08:27.750Z",
    "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13145.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6577073fc2bf55b1f6bafb49",
      "avatarUrl": "/avatars/58803398b1a918b7570db17893e65122.svg",
      "fullname": "liao",
      "name": "LegendBC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11433",
      "authors": [
        {
          "_id": "67b54a644508bd0617598c21",
          "name": "Guojun Xiong",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c22",
          "name": "Zhiyang Deng",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c23",
          "name": "Keyi Wang",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c24",
          "name": "Yupeng Cao",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c25",
          "name": "Haohang Li",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c26",
          "name": "Yangyang Yu",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c27",
          "name": "Xueqing Peng",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c28",
          "name": "Mingquan Lin",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c29",
          "name": "Kaleb E Smith",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2a",
          "name": "Xiao-Yang Liu",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2b",
          "user": {
            "_id": "63b58ed5889aa6707f0bb0f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
            "isPro": true,
            "fullname": "Jimin Huang",
            "user": "jiminHuang",
            "type": "user"
          },
          "name": "Jimin Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:03.181Z",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2c",
          "name": "Sophia Ananiadou",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2d",
          "name": "Qianqian Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T04:45:53.000Z",
      "title": "FLAG-Trader: 경사 기반의 강화학습을 활용한 Fusion LLM-Agent 금융투자에 대한 기술",
      "summary": "대 언어 모델(LLMs)를 여러 종류의 금융 데이터에 미세 조정한 것은 다양한 금융 업무에서 놀라울만한 성능을 보여주고 있습니다. 그러나, 상호작용하는 금융 시장에서의 단계적, 목표지향적인 시나리오에 대해서는 복잡한 아웃풋 접근이 필요하며, 결산책을 개선하기 위해 어려움이 많습니다. 이를 대처하기 위해 FLAG-Trader를 제안합니다. FLAG-Trader는 언어 처리(LLMs를 통해)와 경사 기반의 강화 학습(RL)의 정책 최적화를 통합한 일련의 아키텍처입니다. 미세 조정된 LLM은 기존의 지식을 활용하면서, 금융 데이터에 대해 파라미터 효율적인 미세 조정을 통해 정책 네트워크로 작동합니다. 트레이딩 보상에 기반한 정책 경사 최적화를 통해, 우리의 프레임워크는 트레이딩에서 LLM의 성능을 향상시키고, 다른 금융 데이터베이스 작업의 결과를 개선할 수 있습니다. 이러한 향상을 증명하기 위해 확장된 실험 데이터를 제공합니다.",
      "upvotes": 18,
      "discussionId": "67b54a654508bd0617598c7e"
    },
    "publishedAt": "2025-02-18T22:06:19.200Z",
    "title": "FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/2C9mhT-1Qz14hik7sxjf2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11433.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b58ed5889aa6707f0bb0f4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
      "fullname": "Jimin Huang",
      "name": "jiminHuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.09245",
      "authors": [
        {
          "_id": "67b57a993d4f319f1fa9424b",
          "name": "Gleb Gerasimov",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424c",
          "user": {
            "_id": "63ed5676684767daecac6f8a",
            "avatarUrl": "/avatars/d0e4a715f9c3fb6d74c183bab751ec35.svg",
            "isPro": false,
            "fullname": "Yaroslav Aksenov",
            "user": "yaraksen",
            "type": "user"
          },
          "name": "Yaroslav Aksenov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:41.123Z",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424d",
          "user": {
            "_id": "60b364e7f88532cd79eaff7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
            "isPro": false,
            "fullname": "Nikita Balagansky",
            "user": "elephantmipt",
            "type": "user"
          },
          "name": "Nikita Balagansky",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:33:26.858Z",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424e",
          "name": "Viacheslav Sinii",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424f",
          "user": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "isPro": false,
            "fullname": "Daniil Gavrilov",
            "user": "kefirski",
            "type": "user"
          },
          "name": "Daniil Gavrilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:43.143Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-13T12:00:50.000Z",
      "title": "당신은 Transformer의 표현력을 완벽하게 활용하고 있지 않습니다.",
      "summary": "RNN와 달리 Transformers는 모든 이전 토큰을 직접적으로 처리할 수 있습니다. 그러나 표준의 Transformers는 이전 레이어의 표현을 유일하게 사용합니다. 본 논문에서는 이러한 설계 선택이 표현 파괴를 일으키고, 최적의 성능을 얻을 수 없게 되는 것을 보여줍니다. 이 문제를 해결하기 위해 Layer-Integrated Memory (LIMe)를 소개합니다. LIMe는 모델의 전체적인 메모리 Fit을 유지하면서, 이전 레이어의 은닉 상태에 접근할 수 있도록 하여 표현력을 확장합니다. 다양한 아키텍처와 다른 검색 구조를 조합한 확장된 실험을 통해, LIMe는 광범위한 태스크에서 통일적인 성능 향상을 나타냅니다. 또한, 학습된 표현 다이나믹스의 분석과 깊이 방향의 회로 탐색으로부터, LIMe가 레이어 간 정보를 통합하는 것을 보여주고, 향후 연구를 유도하는 지침적인 방향을 제시합니다.",
      "upvotes": 12,
      "discussionId": "67b57a9a3d4f319f1fa94274"
    },
    "publishedAt": "2025-02-19T03:03:51.930Z",
    "title": "You Do Not Fully Utilize Transformer's Representation Capacity",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63ed5676684767daecac6f8a/tZDsnW0gjHoYCpbZ-wwJi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09245.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ed5676684767daecac6f8a",
      "avatarUrl": "/avatars/d0e4a715f9c3fb6d74c183bab751ec35.svg",
      "fullname": "Yaroslav Aksenov",
      "name": "yaraksen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13130",
      "authors": [
        {
          "_id": "67b5625fb27eb6046b2ceec5",
          "name": "Jianwei Yang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec6",
          "name": "Reuben Tan",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec7",
          "name": "Qianhui Wu",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec8",
          "name": "Ruijie Zheng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec9",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceeca",
          "name": "Yongyuan Liang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecb",
          "name": "Yu Gu",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecc",
          "name": "Mu Cai",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecd",
          "name": "Seonghyeon Ye",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceece",
          "name": "Joel Jang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecf",
          "name": "Yuquan Deng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceed0",
          "name": "Lars Liden",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceed1",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:55:21.000Z",
      "title": "Magma: 다모뎀 AI 에이전트의 기본 모델\n\n(Note: The original text \"多モデルAIアガントの基礎モデル\" was translated to \"다모뎀 AI 에이전트의 기본 모델\" to maintain the closest equivalent in Korean, considering the context and the specific terminology used in AI and machine learning fields.)",
      "summary": "マグマ는, 시각언어(VL) 모델의 중요한 확장으로, 그 시각언어 이해 능력(어조 인지 능력)을 유지하면서도 시각스페이스(스페이스 타임 인지 능력)에서 계획과 행동의 능력을 갖습니다. 또한, UI 네비게이션부터 로봇 조작까지의 아웃 리지언셜 태스크를 처리할 수 있습니다. 이러한 아웃 리지언셜 능력에 대한 습득을 위해, マグマ는 이미지, 비디오부터 로봇 데이터까지 다양한 데이터 세트를 대규모로 학습합니다. 이미지 중 행동 가능한 시각적 객체(예: GUI의 클릭 가능한 버튼)을 Set-of-Mark(SoM)으로 레이블링하고, 비디오 중 물체의 이동(예: 사람 손 또는 로봇 팔의 흔적)을 Trace-of-Mark(ToM)으로 레이블링합니다. 확장된 실험은 SoM과 ToM이 좋은 조화를奏で, マグマ 모델의 공간 타임 인지 능력의 습득에 도움을 줍니다. 이는, 그림 1에 나타난 것처럼, 광범위한 범위의 태스크의 기초를 이루는 것입니다. 특히, マグマ는 이러한 태스크에 특화된 선행 모델을 초월하고, UI 네비게이션 및 로봇 조작 태스크에 새로운 최尖端 결과를 얻습니다. 이미지 및 비디오 관련 다양한 태스크에서도, マグマ는 인기 있는 대규모 다양한 모델에 비하여, 이러한 데이터 세트를 통해 훈련된人也 좋습니다. 우리 모델과 코드를 공개하고, 중복 가능한 실험을 수행하기 위해, 다음 URL을 참조하세요. https://microsoft.github.io/Magma",
      "upvotes": 11,
      "discussionId": "67b56265b27eb6046b2cf08f"
    },
    "publishedAt": "2025-02-18T23:51:36.910Z",
    "title": "Magma: A Foundation Model for Multimodal AI Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13130.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6142
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12513",
      "authors": [
        {
          "_id": "67b545fd88527668fa8bcc14",
          "name": "Tiancheng Gu",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc15",
          "name": "Kaicheng Yang",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc16",
          "name": "Chaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc17",
          "name": "Yin Xie",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc18",
          "name": "Xiang An",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc19",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1a",
          "name": "Dongnan Liu",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1b",
          "name": "Weidong Cai",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1c",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T03:58:38.000Z",
      "title": "RealSyn: 효과적이고 확장 가능한 다모달 인터레이트 문서 변환 패러다임",
      "summary": "広範な 이미지 텍스트 페어에 pretraining 된 후, Contrastive Language-Image Pre-training (CLIP)은 다양한 벤치마크에서 기대되는 성능을 보여주며, 그러나 다양한 비 페어 데이터의 대규모 양을 시각 언어 표현 학습에서 활용不足하고 있습니다. 이러한 비 페어 문서를 최대한 활용하기 위해, 먼저, Real-World Data Extraction Pipeline을 구축하고, 고품질의 이미지와 텍스트를 추출합니다. 다음으로, 각 이미지와 문법적으로 연관된 실제 텍스트를 효율적으로 연관 짓기 위한 계층적 검색 방법을 설계합니다. 또한, 미세한 시각 정보를 발전시키기 위해, 합성 텍스트의 생성을 위한 이미지 문법적 확장 생성 모듈을 제안합니다. 또한, 데이터셋의 다양성을 향상시키고, 긴 꼬리 개념의 학습을 개선하기 위해, 문법적 균형 샘플링 전략을 도입합니다. 이러한 혁신적인 기능에 기반하여, RealSyn라는 실제적이고 합성적인 텍스트를 조합한 데이터셋을 구축하고, 15M, 30M, 100M의 3가지 스케일로 제공됩니다. 확장 검증은 RealSyn가 시각 언어 표현 학습을 촉진하고, 강한 scalability를 보여주는 것을 명확히 합니다. RealSyn에서 pretraining 된 모델은 여러 다음 세대 태스크에서 가장 선진적인 성능을 달성합니다. 향후 연구를 촉진하기 위해, RealSyn 데이터셋과 pretraining 모델의 가중치는 https://github.com/deepglint/RealSyn에 공개되어 있습니다.",
      "upvotes": 10,
      "discussionId": "67b545fe88527668fa8bcc65"
    },
    "publishedAt": "2025-02-18T21:52:22.326Z",
    "title": "RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12513.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12859",
      "authors": [
        {
          "_id": "67b576aa489d68b981e086ad",
          "name": "Chenxing Wei",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086ae",
          "name": "Yao Shu",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086af",
          "name": "Mingwen Ou",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086b0",
          "name": "Ying Tiffany He",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086b1",
          "name": "Fei Richard Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T13:46:47.000Z",
      "title": "PAFT: 無관한 プロンプト의 미세 조정",
      "summary": "LLMs는 미세 조정 후 하류 태스크에 적응하고 있지만, 이 적응성은 Prompt의 미소적인 변화도 성능을 크게 저하시키는 경우가 있으며, Prompt의 강건성을 해칠 수 있습니다. 이를 대처하기 위해 Prompt-Agnostic Fine-Tuning(PAFT)을 제안합니다. PAFT는 간단하고 효과적인 접근법으로, 미세 조정 중 Prompt를 동적으로 조정합니다. 이로써 모델은 문제를 기반으로하는 원리를 학습하고 특정 Prompt의 표현에 과적합하지 않도록 합니다. PAFT는 2단계로 동작합니다. 첫째, 의미있는 합성적인 후보 Prompt의 다양한 세트를 구축합니다. 둘째, 이 세트에서 Prompt를 랜덤하게 샘플링하여 동적인 학습 입력을 생성합니다. 다양한 데이터 세트와 LLMs를 구성한 광범위한 실험에서, PAFT로 학습된 모델은 광범위한 Prompt, 특히未见 Prompt에 대한 강한 강건성과 일반화 능력을 보여주었습니다. 이 강건성의 향상은 모델의 성능과 추론 속도를 향상시키며, 동시에 훈련 효율을 유지합니다. 소멸 조사는 PAFT의 효과성을 더욱 확실히 확인합니다.",
      "upvotes": 8,
      "discussionId": "67b576aa489d68b981e08708"
    },
    "publishedAt": "2025-02-19T01:21:54.836Z",
    "title": "PAFT: Prompt-Agnostic Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12859.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ed3051492a7f35db21fea2",
      "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
      "fullname": "Chenxing Wei",
      "name": "kittttttt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12170",
      "authors": [
        {
          "_id": "67b5434f2b2ec6908fffe75e",
          "name": "Da Xiao",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe75f",
          "name": "Qingye Meng",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe760",
          "name": "Shengping Li",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe761",
          "name": "Xingyuan Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-13T10:26:27.000Z",
      "title": "MUDDFormer: Transformer의 잔차 볼트네크를 파괴하기 위한 다방향적인 동적 밀접 결합",
      "summary": "우리는 MUltiway Dynamic Dense (MUDD) 연결을 제안하여, 잔류 연결의 한계점을 해결하고 Transformer의 교차 계층 정보 흐름을 향상시키는 간단하고 효과적인 방법을 제시합니다. 기존의 밀집 연결 접근 방식과 달리, MUDD는 Transformer 블록의 각 분리된 입력 스트림 (쿼리, 키, 값, 또는 잔류)의 각 시퀀스 위치에 따라 동적으로 연결 가중치를 생성합니다. MUDD 연결은 Transformer 아키텍처의 어느 곳에서도 쉽게 통합될 수 있으며, MUDDFormer를 생성할 수 있습니다. 광범위한 실험은 MUDDFormer가 다양한 모델 아키텍처와 규모의 언어 모델링에서 Transformer를 크게 초과하는 성능을 달성한다는 것을 보여주며, Transformer가 1.8X-2.4X 계산을 사용하여 훈련한 성능을 달성했습니다. 특히, MUDDPythia-2.8B는 pretraining ppl과 하단 작업에서 Pythia-6.9B와 일치하며, 5-shot 설정에서 Pythia-12B와 경쟁하며, 0.23%의 파라미터와 0.4%의 계산을 추가합니다. JAX와 PyTorch에서 코드와 사전 훈련된 모델은 https://github.com/Caiyun-AI/MUDDFormer에서 제공됩니다.",
      "upvotes": 6,
      "discussionId": "67b543502b2ec6908fffe788"
    },
    "publishedAt": "2025-02-18T22:59:16.530Z",
    "title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12170.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d77440bad37ef354028365",
      "avatarUrl": "/avatars/df0dea879e06fa814867e9aad03d1e68.svg",
      "fullname": "Da Xiao",
      "name": "xiaoda99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12215",
      "authors": [
        {
          "_id": "67b56007fa141a55e51d9d78",
          "name": "Zhiyuan Zeng",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d79",
          "name": "Qinyuan Cheng",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7a",
          "name": "Zhangyue Yin",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7b",
          "name": "Yunhua Zhou",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7c",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T07:21:11.000Z",
      "title": "다시 조사 o1라시인 모댈의 테스트 타임 스케일링: 그들은 실제로 테스트 타임 스케일링 능력을 가지고 있는지?",
      "summary": "대 언어 모형(LLMs)에서 테스트 시 스케일링의 등장, OpenAI의 o1 시리즈를 예로 들면, 추론 시 계산 컴퓨팅 리소스의 할당을 스케일링하여 논리론의 능력을 발전시킬 수 있습니다. QwQ, Deepseek-R1(R1), LIMITな릍 후속 모형을 보면 이러한 진보를 재현하지만, 이들 모형이 본질적으로 테스트 시 스케일링 능력을 가지고 있는지 아직 조사가 부족합니다. 본 연구에서는 이러한 o1 같은 모형의 긴 CoTs(논리론)가 정확도를 일관하여 향상시키는 것은 아닙니다. 실제로 같은 문제를 대면할 때 올바른 답이 잘못된 답보다 짧게 나올 때가 많습니다. 이를 나아가, 이 현상은 모형의 자동 보정 능력과 밀접하게 관련되어 나타났습니다. 긴 CoTs에는 더 많은 자동 보정이 포함되어 있으며, 이는 일반적으로 성능 저하를 초래합니다. QwQ, R1, LIMIT에 대해 순열이나 병렬 스케일링 스냅을 비교하여 병렬 스케일링이 더 좋은 커버율과 스케일러 비리성을 달성하는 것을 명확히 알 수 있었습니다. 이러한 통찰에 기반하여 병렬 스케일링 스냅과 CoTs의 길이의 특징을 조합한 방법인 \"최단 다수 투표\"를 제안하고, 전통적인 다수 투표 접근법과 비교하여 테스트 시 스케일링 능력을 크게 향상시킬 수 있음을 보여주었습니다.",
      "upvotes": 5,
      "discussionId": "67b56007fa141a55e51d9da7"
    },
    "publishedAt": "2025-02-18T23:37:46.756Z",
    "title": "Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12215.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6142
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12501",
      "authors": [
        {
          "_id": "67b547ffc9071a3e97139532",
          "user": {
            "_id": "62a42f22c683d02f5b63320c",
            "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
            "isPro": false,
            "fullname": "Qiyuan Zhang",
            "user": "DonJoey",
            "type": "user"
          },
          "name": "Qiyuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:10.215Z",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139533",
          "name": "Yufei Wang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139534",
          "user": {
            "_id": "63c20105726f62e411fbe882",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c20105726f62e411fbe882/2UsU9O2psbDjJzz-sAmGH.jpeg",
            "isPro": false,
            "fullname": "Yuxin Jiang",
            "user": "YuxinJiang",
            "type": "user"
          },
          "name": "Yuxin Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:08.101Z",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139535",
          "name": "Liangyou Li",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139536",
          "name": "Chuhan Wu",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139537",
          "name": "Yasheng Wang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139538",
          "name": "Xin Jiang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139539",
          "name": "Lifeng Shang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953a",
          "name": "Ruiming Tang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953b",
          "name": "Fuyuan Lyu",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953c",
          "name": "Chen Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T03:31:06.000Z",
      "title": "클러스터 비교의 이유: LLM-as-a-Judge의 종합 평가 이해하기",
      "summary": "LLM-as-a-Judge는 chain-of-thought (CoT) 판단을 생성하여 광범위하게 적용되는 자동 평가 방법 중 하나입니다. 그러나 이 방법의 신뢰성은 CoT 추론이 상세한 내용을 완전히 이해할 수 없기 때문에, 상세한 내용을 이해하지 못하여 불완전한 결과를 생성하게 됩니다. 현재의 방법은 주로 다수결 또는 평가기준 확장에 의존하며, CoT의 한계를 해결할 수 없습니다. 우리는 Crowd-based Comparative Evaluation을 제안하고 있습니다. 이는 후보 답변과 비교하기 위해 추가한 코드의 응답을 통해 후보 답변 중 상세한 내용을 밝혀냅니다. 이 과정은 LLM-as-a-Judge가 상세한 CoT 판단을 제공하도록 촉진합니다. 광범위한 실험에 따라 우리의 접근法是 평가의 신뢰성을 향상시키고 5개의 벤치마크에서 평균 6.7%의 정확도 향상을 달성했습니다. 또한 우리의 방법은 판단의 경험을 활용하여 고품질의 CoT를 생성하고 훈련 피드백 (SFT)의 수렴 샘플링에서 상위 성능을 나타내며, 이를 \"코드의 수렴 샘플링\"으로 불리며, SFT의 효율적인 실행을 가능하게 합니다. 분석에 따르면 우리가 생성한 CoT은 상세하고 고품질이며, 평가 정확도는 추론의 규모 업그레이드에 따라 향상됩니다.",
      "upvotes": 5,
      "discussionId": "67b54800c9071a3e9713956c"
    },
    "publishedAt": "2025-02-18T21:55:26.822Z",
    "title": "Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12501.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a42f22c683d02f5b63320c",
      "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
      "fullname": "Qiyuan Zhang",
      "name": "DonJoey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11271",
      "authors": [
        {
          "_id": "67b4322c217ec18a40587bec",
          "user": {
            "_id": "60f5f68fa7fd83d025749234",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
            "isPro": false,
            "fullname": "Pan Lu",
            "user": "lupantech",
            "type": "user"
          },
          "name": "Pan Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:04:43.677Z",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bed",
          "name": "Bowen Chen",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bee",
          "name": "Sheng Liu",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bef",
          "name": "Rahul Thapa",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bf0",
          "name": "Joseph Boen",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bf1",
          "name": "James Zou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T21:18:47.000Z",
      "title": "OctoTools: 확장 가능한 도구를 갖춘 아그네틱 프레임워크で 복잡한 논리론리",
      "summary": "복잡한 논리적 태스크의 해결에는 시각적 이해, 영역 지식의 검색, 숫자 계산, 다단계 논리 등 다양한 요소가 포함되어 있습니다. 현재의 방법들은 외부 도구를 사용하여 대규모 언어 모델(LLMs)를 강화하고 있지만, 특정 영역, 도구 종류의 제한 및 추가적인 훈련 데이터의 필요성이 존재합니다. 본 논문에서는, 훈련 필요하지 않음, 사용자 친화적이고 쉽게 확장 가능한 오픈 소스 에이전트 프레임워크인 OctoTools를 소개합니다. 이 프레임워크는 다양한 영역에서 복잡한 논리적 태스크를 해결하기 위해 설계되었습니다. OctoTools는 표준화된 도구 카드를 사용하여 도구 기능들을 포함하고, 고수준과 저수준 계획 모두를 위한 계획기, 도구 사용을 수행하는 실행 기능을 갖추고 있습니다. OctoTools는 MathVista, MMLU-Pro, MedQA, GAIA-Text 등 16가지 다양한 태스크에서 일반성을 입증하고, GPT-4o를 기준으로 평균 정확도를 크게 증가시켰으며(9.3%) 또한 동일한 도구 세트를 제공하면 AutoGen, GPT-Functions, LangChain을 초과하여 10.6%의 최대값을 기록했습니다. 세부적인 분석과 제거 테스트를 통해 OctoTools는 태스크 계획, 유효한 도구 사용, 다단계 문제 해결에 있어서 우수한 성능을 보여주고 있습니다.",
      "upvotes": 4,
      "discussionId": "67b4322d217ec18a40587c27"
    },
    "publishedAt": "2025-02-19T02:27:36.940Z",
    "title": "OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11271.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f5f68fa7fd83d025749234",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
      "fullname": "Pan Lu",
      "name": "lupantech",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.09838",
      "authors": [
        {
          "_id": "67b55078a64445f58c771d84",
          "name": "Tianwei Lin",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d85",
          "name": "Wenqiao Zhang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d86",
          "name": "Sijing Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d87",
          "name": "Yuqian Yuan",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d88",
          "name": "Binhe Yu",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d89",
          "name": "Haoyuan Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8a",
          "name": "Wanggui He",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8b",
          "name": "Hao Jiang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8c",
          "name": "Mengze Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8d",
          "name": "Xiaohui Song",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8e",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8f",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d90",
          "name": "Hui Lin",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d91",
          "name": "Yueting Zhuang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d92",
          "name": "Beng Chin Ooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-14T00:42:36.000Z",
      "title": "건강 GPT: 의료용 대형 시각 언어 모델의 통합\n  이해와 생성을 통해 서로 다른 지식의 적응에 의한 통합",
      "summary": "ヘルスGPT, 강력한 의료용 대형 시각 언어 모델(Med-LVLM)를 소개합니다. 이것은 통일된 자동 복원 패러다임 내에서 의료적인 시각을 이해하고 생성할 수 있는 능력이 통합된 것입니다. 우리의 시작 철학은 다른 컴퓨팅의 이해와 생성에 대한 지식이 단계적으로 적용되고, 사전 학습된 대형 언어 모델(LLMs)에 적용하는 것입니다. 이는 새로운 H-LoRA手法에 의해 실현되어, 제조된 단계적인 시각 인식 접근법과 세 단계 학습 전략에 의해 보완됩니다. ヘルスGPT의 효과적인 학습을 위해, VL-Health라는 의료 분야에 특화된 세부적인 이해와 생성 데이터 세트를 설계했습니다. 실험 결과를 통해, ヘルスGPT는 의료적인 시각적인 통합 작업에서 뛰어난 성능과 scalability를 보여줍니다. 이 프로젝트는 https://github.com/DCDmllm/HealthGPT에서 접근할 수 있습니다.",
      "upvotes": 4,
      "discussionId": "67b5507aa64445f58c771df9"
    },
    "publishedAt": "2025-02-18T22:35:23.066Z",
    "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09838.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fc18edfb66882aba4d548e",
      "avatarUrl": "/avatars/f70d47fe4aba98b5a5cd64f7e002dfd2.svg",
      "fullname": "wenqiao",
      "name": "wannature",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12574",
      "authors": [
        {
          "_id": "67b547f555d0424a31b9c384",
          "user": {
            "_id": "64cb48f7667f4f808535107e",
            "avatarUrl": "/avatars/8f77f378ad665b246e1ea3aaba2153ae.svg",
            "isPro": false,
            "fullname": "chengluo",
            "user": "wdlctc",
            "type": "user"
          },
          "name": "Cheng Luo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:40:25.130Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c385",
          "user": {
            "_id": "64b15284372d4340772a3dca",
            "avatarUrl": "/avatars/417d5f1bc1bcb5e4d5de6169673c2cf7.svg",
            "isPro": false,
            "fullname": "Zefan Cai",
            "user": "ZefanCai",
            "type": "user"
          },
          "name": "Zefan Cai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:40:47.077Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c386",
          "name": "Hanshi Sun",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c387",
          "user": {
            "_id": "64c15c5bea792b1950e302e4",
            "avatarUrl": "/avatars/51f84365cc08a1dcd5da70968389aed2.svg",
            "isPro": false,
            "fullname": "Jinqi Xiao",
            "user": "jinqixiao",
            "type": "user"
          },
          "name": "Jinqi Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:41:01.931Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c388",
          "name": "Bo Yuan",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c389",
          "name": "Wen Xiao",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38a",
          "user": {
            "_id": "675f8271a63fff7b5bcbc478",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/9tJn7NyzLMreCJVH4wRho.png",
            "isPro": false,
            "fullname": "Junjie Hu",
            "user": "junjiehu",
            "type": "user"
          },
          "name": "Junjie Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:41:18.304Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38b",
          "name": "Jiawei Zhao",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38c",
          "user": {
            "_id": "64b732f832403871593e082c",
            "avatarUrl": "/avatars/dd21932b0c167131ee7545a622c46c3c.svg",
            "isPro": false,
            "fullname": "Beidi Chen",
            "user": "beidic",
            "type": "user"
          },
          "name": "Beidi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:39:20.563Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38d",
          "user": {
            "_id": "6532920b3e385cfc6002938d",
            "avatarUrl": "/avatars/cb9cc6d2733031582c83f56dc6cd1dd5.svg",
            "isPro": false,
            "fullname": "Anima Anandkumar",
            "user": "animakumar",
            "type": "user"
          },
          "name": "Anima Anandkumar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:39:15.091Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T06:26:05.000Z",
      "title": "HeadInfer: 메모리 효율적인 LLM 추론을 딥러닝 모델의 각 층별로 분할하여 실행하는 방법",
      "summary": "Transformer 기반의 대규모 언어 모델(LLMs)은 긴 문맥 생성에 우수한 성능을 나타냅니다. 문맥 길이의 확장으로, 추론 시 LLMs의 메모리 퀄리티를 키워드 캐시(KV cache)에 불균형하게 옮겼습니다. 본 논문에서는 HEADINFER를 제안합니다. HEADINFER는 모든 transformer 레이어의 KV cache를 GPU에서 완전히 저장하는 것을 피하기 위해 CPU RAM에 KV cache를 오버로드합니다. HEADINFER는 선택적 注意 헤드의 KV cache를 GPU에 남겨두면서, 注意 출력을 동적으로 계산합니다. 로프라인 분석을 통해, HEADINFER는 계산 효율성을 유지하면서, 메모리 퀄리티를 크게 줄일 수 있음을 보여주었습니다. Llama-3-8B 모델에 대해 1,000,000 토큰의 시퀀스를 사용하여 HEADINFER를 평가했습니다. KV cache의 GPU 메모리 퀄리티는 128GB에서 1GB로 줄였으며, 총합적인 GPU 메모리 사용량은 207GB에서 17GB로 줄였으며, BF16 기반 선형 추론에 대해 92%의 감소를 달성했습니다. 특히, HEADINFER는 24GB 메모리의 소비자 GPU(예: NVIDIA RTX 4090)에서 8B 모델에서 4,000,000 토큰의 추론을 수행할 수 있습니다. 또한, 근사 방법 사용을 피하고 있습니다.",
      "upvotes": 3,
      "discussionId": "67b547f755d0424a31b9c3e5"
    },
    "publishedAt": "2025-02-18T21:57:00.289Z",
    "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12574.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb48f7667f4f808535107e",
      "avatarUrl": "/avatars/8f77f378ad665b246e1ea3aaba2153ae.svg",
      "fullname": "chengluo",
      "name": "wdlctc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13063",
      "authors": [
        {
          "_id": "67b5a7896f72266cb765e744",
          "user": {
            "_id": "618b9540682ec1c38327e586",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618b9540682ec1c38327e586/v_ZBkfh8O9Zh6C2YQpuBX.jpeg",
            "isPro": false,
            "fullname": "Yury Kuratov",
            "user": "yurakuratov",
            "type": "user"
          },
          "name": "Yuri Kuratov",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-19T09:42:34.422Z",
          "hidden": false
        },
        {
          "_id": "67b5a7896f72266cb765e745",
          "name": "Mikhail Arkhipov",
          "hidden": false
        },
        {
          "_id": "67b5a7896f72266cb765e746",
          "name": "Aydar Bulatov",
          "hidden": false
        },
        {
          "_id": "67b5a7896f72266cb765e747",
          "user": {
            "_id": "639c6e978a34ed9a404c6a7b",
            "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
            "isPro": false,
            "fullname": "MIKHAIL BURTSEV",
            "user": "mbur",
            "type": "user"
          },
          "name": "Mikhail Burtsev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:56:59.080Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T17:08:45.000Z",
      "title": "クラミング 1568 토큰을 하나의 벡터에 포함시키고 다시 돌려보냄: 임베딩 공간의 용량의 한계를 조사합니다.",
      "summary": "최근의 다양한 연구들은 토큰의 순열을 실수값의 벡터의 짧은 순열로 압축하고 토큰埋めコード나 키値キャッシュ를 대신하여 사용하는 것을 목표로하고 있습니다. 이러한 접근法是 현재의 언어 모델의 계산량을 줄일 수 있습니다. 그러나 강력한 모델을 인코더로 사용하게 되면, 무손실 압축비율은 일반적으로 x10 이하로 나옵니다. 이 사실은 매우 흥미롭습니다. 이론적으로는 큰 실수값 벡터의 최대 정보 용량은 16비트 정밀도와 가벼운 벡터 크기로도, 현재의 속도보다 훨씬 멀리 있습니다. 본 논문에서는 인코더를 샘플별로 최적화 절차로 대체하여 압축의 한계를 조사하고 있습니다. 압축비가 x1500까지 존재하는 벡터를 보여줍니다. 그리고 기존의 해결책과 실용적인 해결책 사이의 제곱 간격을 명확히 합니다. 또한 실험적으로는 압축의 한계는 입력의 길이가 아니라, 줄일 불확실성의 양으로 결정됩니다. 즉, 이 순열의 교차 엔트로피 손실입니다. 얻은 한계는 입력埋めコード의 이론적 용량과 실용적인 사용 사이에서 매우 큰 차이를 명확히 하고, 모델 설계에서 큰 최적화 여지가 있음을 보여줍니다.",
      "upvotes": 1,
      "discussionId": "67b5a78a6f72266cb765e779"
    },
    "publishedAt": "2025-02-19T04:43:42.973Z",
    "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13063.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "639c6e978a34ed9a404c6a7b",
      "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
      "fullname": "MIKHAIL BURTSEV",
      "name": "mbur",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.10708",
      "authors": [
        {
          "_id": "67b58e32e972a2806a9a0451",
          "user": {
            "_id": "65407ba7a38390065750233f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
            "isPro": false,
            "fullname": "Zirui Song",
            "user": "Ziruibest",
            "type": "user"
          },
          "name": "Zirui Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:38.943Z",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0452",
          "name": "Bin Yan",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0453",
          "name": "Yuhan Liu",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0454",
          "name": "Miao Fang",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0455",
          "name": "Mingzhe Li",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0456",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0457",
          "name": "Xiuying Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-15T07:43:43.000Z",
      "title": "특정 분야의 지식을 대규모 언어 모델에 주입하는 방법: 일관된 조사",
      "summary": "대 언어 모델(LLMs)은 자연어 이해, 텍스트 요약, 기계 번역 등 다양한 태스크에서 놀라운 성공을 보여주고 있습니다. 그러나 일반적인 용도에 대한 특성은 건강 케어, 화학, 법규 분석 등 특정화된 지식이 필요한 분야로의 효과성을 제한하고 있습니다. 이에 연구자들은 LLMs에 특정화된 지식을 통합하는 방법을 찾아 이를 강화하고 있습니다. 본 조사에서는 이러한 방법을 동적 지식 주입, 정적 지식 삽입, 모듈화 디レイ터, 프로ン퓰트 최적화의 4가지 주요 접근으로 분류하고, 각 접근이 LLMs에 분야专門의 지식을 제공하며, 유연성, scalability, 효율성의 trade-off를 조화시키는 것을 설명합니다. 이러한 방법을 통해 LLMs가 특정화된 태스크에 접근할 수 있게 되고, 그 장점과 단점을 비교하고, 특정화된 LLMs와 일반적인 LLMs를 비교하며, 이新兴 분야의 문제와 기회를 명확히 합니다. 관심 있는 분들에게 추천하지만, 이 분야에 더 깊은 관심을 갖으려면, 일반적으로 사용되고 있는 데이터셋과 벤치마크를 정리해놓았습니다. https://github.com/abilliyb/Knowledge_Injection_Survey_Papers에서 전문적인 LLMs 연구를 기록하고 있습니다.",
      "upvotes": 1,
      "discussionId": "67b58e33e972a2806a9a04b8"
    },
    "publishedAt": "2025-02-19T02:56:09.510Z",
    "title": "Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10708.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65407ba7a38390065750233f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
      "fullname": "Zirui Song",
      "name": "Ziruibest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12669",
      "authors": [
        {
          "_id": "67b58c806e53744c2a373351",
          "user": {
            "_id": "63024676056ec3a2a8714b24",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
            "isPro": false,
            "fullname": "Xiang Liu",
            "user": "Dominic789654",
            "type": "user"
          },
          "name": "Xiang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:34:03.429Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373352",
          "user": {
            "_id": "64eded5fdfe0a679d840bc98",
            "avatarUrl": "/avatars/4d4c67c13e547a4d296a301e8694e79e.svg",
            "isPro": false,
            "fullname": "sunpenglei",
            "user": "sunpenglei",
            "type": "user"
          },
          "name": "Penglei Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:34:15.889Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373353",
          "name": "Shuyan Chen",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373354",
          "name": "Longhan Zhang",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373355",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373356",
          "name": "Huajie You",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373357",
          "user": {
            "_id": "64473221dcbe1333b64b2db2",
            "avatarUrl": "/avatars/5e4495d3581ad3e6ea3c47650f20b993.svg",
            "isPro": false,
            "fullname": "yongqi zhang",
            "user": "yongqi2023",
            "type": "user"
          },
          "name": "Yongqi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:35:12.059Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373358",
          "name": "Chang Yan",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373359",
          "user": {
            "_id": "6676935fcd0b89a0115174b0",
            "avatarUrl": "/avatars/4caca1b672d29e787814f9a30bf20bcc.svg",
            "isPro": false,
            "fullname": "Xiaowen Chu",
            "user": "wenxinsiju",
            "type": "user"
          },
          "name": "Xiaowen Chu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:35:20.611Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a37335a",
          "name": "Tong-yi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T09:19:24.000Z",
      "title": "Perovskite-LLM: 페르브이티에 특화된 지식증가형 대언어 모델",
      "summary": "ポロビティ 太陽光電池 (PSCs)의 급속한 발전은 연구 논문의 지수적인 증가로 이 분야에서 효율적인 지식 관리와 논리적 분석 시스템의 급한 필요를 제기했습니다. 우리는 PSCs의 전문 지식 그래프를 구축한 지식 증대 시스템에 소개합니다. 이 시스템은 3가지 주요 구성 요소를 통합하고 있습니다. 먼저, 1,517개의 연구 논문으로부터 구축된 전문 지식 그래프를 개발합니다. 이 그래프는 23,789개의 엔티티와 22,272개의 관계를 포함합니다. 다음으로, 2개의 보간 데이터 세트를 생성합니다. PSC샹은 새로운 다 에이전트 프레임워크에 의해 생성된 고품질의 질문 대답 쌍 (55,101건)를 포함하며, PSC랩핑은 2,217건의 신중하게 선택된 재료 과학의 문제를 포함합니다. 마지막으로, 2개의 전문 규모 언어 모델을 사용합니다. PSC샹 LLM은 전문 지식 지원을 제공하며, PSC랩핑 LLM은 과학의 논리적 분석 태스크를 처리합니다. 실험 결과를 통해 이 시스템은 현재의 모델과 비교하여 전문 지식의 검색과 과학의 논리적 분석 태스크에서 상당한 성능을 보였으며, 연구자에게 논문 검색, 실험 설계, 복잡한 문제 해결에 효과적인 도구를 제공함을 보여주고 있습니다.",
      "upvotes": 1,
      "discussionId": "67b58c826e53744c2a3733c2"
    },
    "publishedAt": "2025-02-19T02:47:33.654Z",
    "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.10990",
      "authors": [
        {
          "_id": "67b3ee6c1e80a69e79c3155a",
          "user": {
            "_id": "647d834618274bce03013cc2",
            "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
            "isPro": true,
            "fullname": "yixuan",
            "user": "yixuantt",
            "type": "user"
          },
          "name": "Yixuan Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:04:50.969Z",
          "hidden": false
        },
        {
          "_id": "67b3ee6c1e80a69e79c3155b",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T04:23:52.000Z",
      "title": "FinMTEB: 재무 마스터텍스트 엔드 피딩 벤치마크",
      "summary": "Embedding モデル은 다양한 NLP 애플리케이션에서 정보 표현과 검색에 중요한 역할을 수행하고 있습니다. 최근의 대규모 언어 모델(LLMs)의 발전은 Embedding モデル의 성능을 향상시켰습니다. 이러한 모델은 일반적으로 일반적인 데이터 세트에서 벤치마크가 됩니다만, 실제 세계적인 애플리케이션은 도민스피처의 평가에 요구합니다. 본 논문에서는 FinMTEB(Finance Massive Text Embedding Benchmark)을 소개합니다. FinMTEB는 MTEB의 특화된 컨테이너로, 금융 분야를 특화하고 있습니다. FinMTEB는 7개의 태스크로 구성되어 있으며, 중국어와 영어의 다양한 문형들을 커버하고 있습니다. 예를 들어, 금융 뉴스 기사, 회사 연간보고서, ESG 보고서, 규제 신청서, 회계 회의의 발표요지 등이 있습니다. 또한 FinPersona-E5라는 금융 적응 모델을 개발했습니다. FinPersona-E5는 포터레이트 기반의 데이터 합성 방법을 사용하며, 다양한 금융 エンベディング 태스크를 커버하고 있습니다. 15개의 エンベディング モデル의 확장 평가를 통해 3개의 주요 Findings를 제시했습니다. 이는 (1) 일반적인 벤치마크에서의 성능은 금융 도민스피처의 태스크에 제한된 관련성이 있습니다, (2) 도민스피처에 대한 적응 모델은 일반적인 컨테이너 모델을 확실히 초과합니다, (3) 간단한 Bag-of-Words(BoW) 접근법은 금융의 Semantic Textual Similarity(STS) 태스크에서 복잡한 デンス エンベディング보다 초과하고, 현재의 デンス エンベディング 기술의 한계를 밝혀줍니다. 본 논문은 금융의 NLP 애플리케이션에 대한 강력한 평가 프레임워크를 구축하고, 도민스피처 특화된 エンベディング モデル의 개발에 중요한 인삿점을 제공합니다.",
      "upvotes": 0,
      "discussionId": "67b3ee6d1e80a69e79c3158f"
    },
    "publishedAt": "2025-02-19T04:54:27.788Z",
    "title": "FinMTEB: Finance Massive Text Embedding Benchmark",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d834618274bce03013cc2",
      "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
      "fullname": "yixuan",
      "name": "yixuantt",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13142",
      "authors": [
        {
          "_id": "67b5790132be608036ee94e5",
          "user": {
            "_id": "65c3fdf79d062be813813e45",
            "avatarUrl": "/avatars/52528a61abe5bbbef4a4a431944973cd.svg",
            "isPro": false,
            "fullname": "Dantong Niu",
            "user": "NdtSoCool",
            "type": "user"
          },
          "name": "Dantong Niu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:12:28.457Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e6",
          "user": {
            "_id": "65406e82deee4716f1c29271",
            "avatarUrl": "/avatars/25331a773f8125f9ad1c3d6ac3375586.svg",
            "isPro": false,
            "fullname": "Yuvan Sharma",
            "user": "yuvansharma",
            "type": "user"
          },
          "name": "Yuvan Sharma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:12:35.531Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e7",
          "name": "Haoru Xue",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e8",
          "user": {
            "_id": "650bd36a7c99ca283e58e973",
            "avatarUrl": "/avatars/606d24b2dac190ebcbb4b2a2e4671380.svg",
            "isPro": false,
            "fullname": "Giscard Biamby",
            "user": "gbiamby",
            "type": "user"
          },
          "name": "Giscard Biamby",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:12:49.219Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e9",
          "name": "Junyi Zhang",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94ea",
          "user": {
            "_id": "66a09aec369dd38cf2113070",
            "avatarUrl": "/avatars/cc13bdd3dc1271d33b083b61e12f1a05.svg",
            "isPro": false,
            "fullname": "Ziteng Ji",
            "user": "zitengj0618",
            "type": "user"
          },
          "name": "Ziteng Ji",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:13:13.907Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94eb",
          "user": {
            "_id": "64cbdf02f103036e23d1c7f3",
            "avatarUrl": "/avatars/496069463900dea20929b57381182d39.svg",
            "isPro": false,
            "fullname": "Trevor Darrell",
            "user": "trevordarrell",
            "type": "user"
          },
          "name": "Trevor Darrell",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:13:20.379Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94ec",
          "user": {
            "_id": "667c5764186b27ef806636d3",
            "avatarUrl": "/avatars/5c08f0109bc0e350624112c0aff544f6.svg",
            "isPro": false,
            "fullname": "Roei Herzig",
            "user": "roeiherz",
            "type": "user"
          },
          "name": "Roei Herzig",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:13:26.134Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:01.000Z",
      "title": "4D 표현을 활용한 사전 학습 자동 회귀형 로봇 모델의 훈련",
      "summary": "fundamentāl mōdelu, ŏi jūi naru label dā'itā sutītōn de sakain sūrīn sareta mono de, kon'esutā suto ni tōkon'i hōyō ni kaikaku-teki eiyō o age, kyōi-teki ichigai kōka o shijishin suru, sakain sūrīn no jūyō-sei o shōmei shite imāsu. shika, rōbutikuku ni oite, kono yōna seikō o tachiru koto wa kōnan de, tōgō no tōgō no dā'itā no rekō to wa, fysik-u kai-sen o yōgo ni arau hyōgen no kōraku ni yotte seigen sarete imasu. kono ronbun de, jinjin no bi-deo-dā'itā kara sūrīn sareta rō-ru-bīn rē-bī-rō-mō-dēl o kōyō shite, rōbuto no mō-dēl o kaihatsu suru tame no jidō-hō-eki-teki rōbuto mō-dēl o tōji shite, ARM4R to iu mono o shōryū shimasu. tokubetsu, jikan ni wa tachite mono-karamu depusutō-seido tōkei o yōgo suru 2D hyōgen o 3D kō-sen ni hiku-ageta 3D pīn tō-chō hyōgen o riyū shimasu. konra no 4D hyōgen o, pīn to rōbuto no jōtai hyōgen no kanjō o kōgyō suru, senritsu henkan ni yotte kōnan naru yōni kaizen sarete iru tame, jinnin no bi-deo-dā'itā kara rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tō",
      "upvotes": 0,
      "discussionId": "67b5790832be608036ee9638"
    },
    "publishedAt": "2025-02-19T01:24:26.365Z",
    "title": "Pre-training Auto-regressive Robotic Models with 4D Representations",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667c5764186b27ef806636d3",
      "avatarUrl": "/avatars/5c08f0109bc0e350624112c0aff544f6.svg",
      "fullname": "Roei Herzig",
      "name": "roeiherz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.10852",
      "authors": [
        {
          "_id": "67b55321f703732d151de666",
          "name": "Zeli Su",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de667",
          "name": "Ziyin Zhang",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de668",
          "name": "Guixian Xu",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de669",
          "name": "Jianing Liu",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66a",
          "name": "XU Han",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66b",
          "name": "Ting Zhang",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66c",
          "name": "Yushuang Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-15T16:53:10.000Z",
      "title": "다언어 인코더는 실제로 생각하지 않았던 대로 알고 있다고: 가중치의 공유\n저자원 언어의 매우 적은 자원을 사용하여 사전 학습",
      "summary": "다언어 모델의 예로 XLM-R가 NLP의 다언어화를 촉진하고 있는 반면, 가장 언어자원이 적은 언어에서의 성능이 좋지 않음. 이러한 상황은 현대의 LLMs(예: LLaMA와 Qwen)가 XLM-R보다 적은 언어를 지원하고 있기에 악화되어 있는 것으로 보임. 이러한 도전에 대응하기 위해, 우리는 매우 적은 언어의 문장 생성을 위한 새로운 프레임워크를 제안합니다. 이 프레임워크는 인코더와 디코더의 가중치를 재사용함으로써 학습된 인코더의 의미 공간을 활용할 수 있으며, 낮은 언어자원 언어에서의 효율적인 학습과 효과적인 일반화에 도움을 줍니다. 이 프레임워크를 4가지의 중국의 소수민족 언어에 적용하여, XLM-SWCM를 소개하고, 그 우수한 성능을 보여주었습니다.",
      "upvotes": 0,
      "discussionId": "67b55322f703732d151de69d"
    },
    "publishedAt": "2025-02-18T22:46:16.586Z",
    "title": "Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10852.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6430bdd8cd31d174a9f900fb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
      "fullname": "Ziyin Zhang",
      "name": "Geralt-Targaryen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]