[
  {
    "paper": {
      "id": "2506.18095",
      "authors": [
        {
          "_id": "685a0ac20e4ad7e2197584ea",
          "name": "Junying Chen",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584eb",
          "name": "Zhenyang Cai",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ec",
          "user": {
            "_id": "675130185d873b8ed24d964a",
            "avatarUrl": "/avatars/30ee8ce21f95423db8ced7db4df3112b.svg",
            "isPro": false,
            "fullname": "Pengcheng Chen",
            "user": "cppppppc",
            "type": "user"
          },
          "name": "Pengcheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:33:07.093Z",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ed",
          "name": "Shunian Chen",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ee",
          "name": "Ke Ji",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ef",
          "name": "Xidong Wang",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584f0",
          "name": "Yunjin Yang",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584f1",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-22T16:51:09.000Z",
      "submittedOnDailyAt": "2025-06-26T02:58:42.859Z",
      "title": "ShareGPT-4o-Image: GPT-4o 수준의 이미지 생성에 의한 다 모델의 어레이멘트",
      "submittedOnDailyBy": {
        "_id": "64097dd1b6a334f53e2b3e4c",
        "avatarUrl": "/avatars/18d036aab5e096054a8706bc78027126.svg",
        "isPro": false,
        "fullname": "Junying Chen",
        "user": "jymcc",
        "type": "user"
      },
      "summary": "최근의 다 모델 생성 모델의 발전은 현실적인, 명령에 대응하는 이미지 생성이 가능해졌지만, GPT-4o-Image 등 최신 시스템은 프로퍼티와 접근이 제한되어 있습니다. 이러한 능력의 민주화를 위해, ShareGPT-4o-Image를 소개합니다. 이는 GPT-4o의 이미지 생성 능력을 사용하여 합성된 45K개의 문장으로부터 이미지로, 46K개의 문장과 이미지로부터 이미지로의 데이터의 첫 번째 데이터 세트입니다. 이 데이터 세트를 활용하여 Janus-4o를 개발했습니다. 이는 문장부터 이미지로, 문장과 이미지로부터 이미지로의 두 가지 생성을 가능하게 하는 다 모델 대 언어 모델입니다. Janus-4o는 이전의 Janus-Pro보다 문장부터 이미지로의 생성을 크게 개선하고, 새로운 문장과 이미지로부터 이미지로의 생성을 지원합니다. 특히, 문장과 이미지로부터 이미지로의 생성의 스크래치에서 우수한 성능을 달성했으며, 8A800-GPU 기계에서 91K개의 합성 샘플과 6시간의 훈련을 통해 실현되었습니다. ShareGPT-4o-Image와 Janus-4o의 공개로, 현실적인, 명령에 대응하는 이미지 생성의 개방적 연구를 촉진하고자 합니다.",
      "upvotes": 43,
      "discussionId": "685a0ac30e4ad7e2197584f2",
      "githubRepo": "https://github.com/FreedomIntelligence/ShareGPT-4o-Image",
      "ai_summary": "ShareGPT-4o-Image and Janus-4o enable open research in photorealistic, instruction-aligned image generation through a large dataset and multimodal model.",
      "ai_keywords": [
        "multimodal generative models",
        "text-to-image",
        "text-and-image-to-image",
        "photorealistic",
        "instruction-aligned",
        "dataset",
        "large language model",
        "synthetic samples"
      ],
      "githubStars": 63
    },
    "publishedAt": "2025-06-22T12:51:09.000Z",
    "title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image\n  Generation",
    "summary": "Recent advances in multimodal generative models have unlocked photorealistic,\ninstruction-aligned image generation, yet leading systems like GPT-4o-Image\nremain proprietary and inaccessible. To democratize these capabilities, we\npresent ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and\n46K text-and-image-to-image data, all synthesized using GPT-4o's image\ngeneration capabilities for distilling its advanced image generation abilities.\nLeveraging this dataset, we develop Janus-4o, a multimodal large language model\ncapable of both text-to-image and text-and-image-to-image generation. Janus-4o\nnot only significantly improves text-to-image generation over its predecessor,\nJanus-Pro, but also newly supports text-and-image-to-image generation. Notably,\nit achieves impressive performance in text-and-image-to-image generation from\nscratch, using only 91K synthetic samples and 6 hours of training on an 8\nA800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will\nfoster open research in photorealistic, instruction-aligned image generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18095.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64097dd1b6a334f53e2b3e4c",
      "avatarUrl": "/avatars/18d036aab5e096054a8706bc78027126.svg",
      "fullname": "Junying Chen",
      "name": "jymcc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19697",
      "authors": [
        {
          "_id": "685c1546df8a0d6c70bbf94e",
          "user": {
            "_id": "60f8435644e75317cc02ed51",
            "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
            "isPro": false,
            "fullname": "Jungwoo Park",
            "user": "affjljoo3581",
            "type": "user"
          },
          "name": "Jungwoo Park",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:32:38.469Z",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf94f",
          "name": "Taewhoo Lee",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf950",
          "name": "Chanwoong Yoon",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf951",
          "name": "Hyeon Hwang",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf952",
          "name": "Jaewoo Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T15:03:57.000Z",
      "submittedOnDailyAt": "2025-06-26T02:17:13.263Z",
      "title": "외부면접 연습을 활용한 강력한 4비트 고정소수점 표현의 대규모 언어 모델\n\n(Note: The original text \"アウトライアーサフェースの予ち練習を用いた大規模な言語モデルの強固な4ビット定数化\" was translated to Korean as \"외부면접 연습을 활용한 강력한 4비트 고정소수점 표현의 대규모 언어 모델\". This translation maintains the professional and accurate nature of the original text.)",
      "submittedOnDailyBy": {
        "_id": "60f8435644e75317cc02ed51",
        "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
        "isPro": false,
        "fullname": "Jungwoo Park",
        "user": "affjljoo3581",
        "type": "user"
      },
      "summary": "극단적 활성화의 아웃라이어는 대규모 언어 모델(LLMs)의 양화 성능을 크게 저하시키고, 기계에서 효율적인 기능화를 방해하고 있습니다. 채널별 조작과 적응적 경사 스케일링은 원인으로 인정되어 있지만, 실용적인 대책은 어렵습니다. 우리는 아웃라이어에 안전한 사전 훈련(OSP)을 통해 극단적 활성화를 적극적으로 방지하는 실용적인 가이드라인을 소개합니다. OSP는 3가지의 핵심 혁신을 조합하여 구성되어 있습니다. 1. 몬 원 가이드안더서(Muon optimizer)는 훈련 효율을 유지하는 동시에 특권적인 기초를 제거합니다. 2. Single-Scale RMSNorm은 채널별 접근을 방지하기 위해 목적입니다. 3. 학습 가능한 임베딩 프로젝션은 임베딩 행렬에서 활성화 매그니투드를 재배분합니다. OSP의 효과를 증명하기 위해, 100억 토큰을 사용하여 14억 파라미터의 모델을 훈련했습니다. 4비트의 심한 양화를 수행하면, OSP 모델의 평균 스코어가 35.7으로, Adam 모델의 26.5를 초과합니다. 또한, 훈련 오버헤드는 2%만입니다. 특히, OSP 모델은 극단적인 값(1818.56)과 비교하여, 근사한 0에克斯스카트켄(0.04)를 나타냅니다. 극단적 활성화의 아웃라이어는 LLMs에 고유하지 아니하고, 훈련 전략의 결과입니다. 기계에서 효율적인 LLM 기능화를 위한 길을 열었습니다. 소스 코드와 학습 완료 체크포인트는 https://github.com/dmis-lab/Outlier-Safe-Pre-Training에서 사용 가능합니다.",
      "upvotes": 29,
      "discussionId": "685c1546df8a0d6c70bbf953",
      "githubRepo": "https://github.com/dmis-lab/Outlier-Safe-Pre-Training",
      "ai_summary": "Outlier-Safe Pre-Training improves large language model quantization performance by preventing extreme activation outliers through innovative training techniques.",
      "ai_keywords": [
        "Muon optimizer",
        "Single-Scale RMSNorm",
        "learnable embedding projection",
        "outlier formation",
        "quantization performance",
        "LLM deployment",
        "excess kurtosis"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-06-24T11:03:57.000Z",
    "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models",
    "summary": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19697.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f8435644e75317cc02ed51",
      "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
      "fullname": "Jungwoo Park",
      "name": "affjljoo3581",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16012",
      "authors": [
        {
          "_id": "685cf7c0696820ba1f28f2ea",
          "name": "Boyu Li",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2eb",
          "name": "Siyuan He",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ec",
          "name": "Hang Xu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ed",
          "name": "Haoqi Yuan",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ee",
          "name": "Yu Zang",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ef",
          "name": "Liwei Hu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f0",
          "name": "Junpeng Yue",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f1",
          "name": "Zhenxiong Jiang",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f2",
          "name": "Pengbo Hu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f3",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:24.327Z",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f4",
          "user": {
            "_id": "63e5e3807f9730f523655c5d",
            "avatarUrl": "/avatars/3ded710049790d025e862861039d9df2.svg",
            "isPro": false,
            "fullname": "YehuiTang",
            "user": "WizardTY",
            "type": "user"
          },
          "name": "Yehui Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:21.125Z",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f5",
          "name": "Zongqing Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T04:13:36.000Z",
      "submittedOnDailyAt": "2025-06-26T06:05:04.111Z",
      "title": "DualTHOR: コンテキスト에 관심 있는 계획에 대한 이중アームヒューマノイド 시뮬레이션 플랫폼",
      "submittedOnDailyBy": {
        "_id": "61e52be53d6dbb1da842316a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
        "isPro": false,
        "fullname": "Börje Karlsson",
        "user": "tellarin",
        "type": "user"
      },
      "summary": "연동 AI 모델이 현실 세계의 시나리오에서 복잡한 상호작용 태스크를 수행할 수 있는 능력의 개발은 연동 AI의 기본적인 문제 중 하나입니다. 연동 비전 언어 모델(VLM)의 훈련 과정에서 최근의 시뮬레이션 플랫폼의 발전은 태스크의 다양성을 크게 높였지만, 대부분의 플랫폼은 단순화된 로봇의 모피와 낮은 수준의 확률적 특성을 피하고, 현실 세계의 로봇에 대한 전달성을 제한하고 있습니다. 이러한 문제를 대처하기 위해, 복잡한 양팔协同인형 로봇을 기반으로 하는 물리 기반의 시뮬레이션 플랫폼인 DualTHOR를 소개합니다. 이 시뮬레이션은 현실 세계의 로봇 자산, 양팔 협업의 태스크 시트,协同인형 로봇을 위한 역연쇄 솔루터, 물리 기반의 낮은 수준의 실행에 의한 잠재적인 실패를 포함한 콘텐츠 구조를 도입하고 있습니다. 이 시뮬레이션은 VLM의 강건성과 일반화 능력을 더욱 더 평가할 수 있게 해줍니다. 분산 평가에 따라 현재의 VLM은 양팔의 협조성에 어려움을 겪고, 콘텐츠가 포함된 현실적인 환경에서의 강건성은 제한되어 있습니다. 우리의 시뮬레이션을 통해, 연동 태스크에 의한 능력 높은 VLM의 개발의 중요성을 강조합니다. 코드는 https://github.com/ds199895/DualTHOR.git에 액세스 가능합니다.",
      "upvotes": 16,
      "discussionId": "685cf7c1696820ba1f28f2f6",
      "ai_summary": "A simulator named DualTHOR for training dual-arm humanoid robots integrates real-world assets and physics to enhance the robustness and generalization of Vision Language Models.",
      "ai_keywords": [
        "embodied AI",
        "Vision Language Models",
        "VLMs",
        "physics-based simulation",
        "DualTHOR",
        "AI2-THOR",
        "dual-arm robots",
        "real-world robot assets",
        "task suite",
        "inverse kinematics solvers",
        "contingency mechanism",
        "physics-based low-level execution"
      ]
    },
    "publishedAt": "2025-06-19T00:13:36.000Z",
    "title": "DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware\n  Planning",
    "summary": "Developing embodied agents capable of performing complex interactive tasks in\nreal-world scenarios remains a fundamental challenge in embodied AI. Although\nrecent advances in simulation platforms have greatly enhanced task diversity to\ntrain embodied Vision Language Models (VLMs), most platforms rely on simplified\nrobot morphologies and bypass the stochastic nature of low-level execution,\nwhich limits their transferability to real-world robots. To address these\nissues, we present a physics-based simulation platform DualTHOR for complex\ndual-arm humanoid robots, built upon an extended version of AI2-THOR. Our\nsimulator includes real-world robot assets, a task suite for dual-arm\ncollaboration, and inverse kinematics solvers for humanoid robots. We also\nintroduce a contingency mechanism that incorporates potential failures through\nphysics-based low-level execution, bridging the gap to real-world scenarios.\nOur simulator enables a more comprehensive evaluation of the robustness and\ngeneralization of VLMs in household environments. Extensive evaluations reveal\nthat current VLMs struggle with dual-arm coordination and exhibit limited\nrobustness in realistic environments with contingencies, highlighting the\nimportance of using our simulator to develop more capable VLMs for embodied\ntasks. The code is available at https://github.com/ds199895/DualTHOR.git.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e52be53d6dbb1da842316a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
      "fullname": "Börje Karlsson",
      "name": "tellarin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18315",
      "authors": [
        {
          "_id": "685cb786696820ba1f28f286",
          "name": "Lehan He",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f287",
          "user": {
            "_id": "66c44c6826efa38bc783b07a",
            "avatarUrl": "/avatars/7a09179a2c91adc97f8db851fca37eea.svg",
            "isPro": false,
            "fullname": "Zeren Chen",
            "user": "zx55",
            "type": "user"
          },
          "name": "Zeren Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:31.152Z",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f288",
          "name": "Zhe Zhang",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f289",
          "name": "Jing Shao",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f28a",
          "name": "Xiang Gao",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f28b",
          "user": {
            "_id": "65b722dbe02a17f0f8d1cc6b",
            "avatarUrl": "/avatars/65f20601ef9b8ebfdddadd737f9153d6.svg",
            "isPro": false,
            "fullname": "Lu Sheng",
            "user": "lsheng2024",
            "type": "user"
          },
          "name": "Lu Sheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:29.105Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T06:01:12.000Z",
      "submittedOnDailyAt": "2025-06-26T06:27:20.139Z",
      "title": "프로퍼티 기반 테스트를 사용하여 LLM 코드 생성 및 검증을 결합합니다.",
      "submittedOnDailyBy": {
        "_id": "64a96a375a69e2ca889abdff",
        "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
        "isPro": false,
        "fullname": "fanhongxing",
        "user": "fanhongxing",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 코드 생성에 뛰어나지만, 특히 복잡한 프로그래밍 태스크에서 출력을 기능적으로 올바르게 확인하는 것은 장기적인 문제입니다. 테스트 주도 개발(TDD)은 코드의 정화의 경로를 제공하지만, LLMs에 대해是高品質의 테스트 케이스의 부족과 자동 테스트 생성의 문제로 효과가 저하됩니다. 테스트의 편향과 부정확한 출력 예측이 수정 프로세스를 잘못 할 수 있습니다. 본 논문에서는, 특성 기반 테스트(PBT)를 활용한 새로운 프레임워크인 Property-Generated Solver를 소개합니다. 이 프레임워크는 구체적인 입력 출력 예에 기반한 테스트를 의존하지 않기 때문에, 높은 수준의 프로그램의 특성이나 이벤트를 확인함으로써 코드의 정확성을 확인합니다. 이러한 특성은 완전히 완벽한 테스트 오라클을 예측하는 것이 아니라, 정의와 확인이 간단하며, 테스트와 코드 사이에서 「자벌레의 순환」을 파괴합니다. Property-Generated Solver는 코드 생성 및 반복적인 정화를 전문으로 하는 Generator와, PBT의 라이프 사이클 관리 및 특성의 위반으로부터 풍부한 의미적인 피드백을 제공하는 Tester를 두 개의 LLM 기반의 에이전트를 사용합니다. 이러한 에이전트는 기존의 TDD 방법보다 크게 패스@1의 개선을 실현하고, 23.1%에서 37.3%의 상대적인 효과를 나타냅니다. 이 프레임워크는 특성 기반 테스트를 핵심 검증 엔진으로, LLMs를 올바르게 일반화 가능한 코드로 향하도록 강력한 구조를 제공합니다. 여러 코드 생성 벤치마크에서 분산된 실험 결과를 통해, Property-Generated Solver가 효과적인 피드백을 제공하고, Generator의 정화 효과에 영향을 미칩니다.",
      "upvotes": 8,
      "discussionId": "685cb786696820ba1f28f28c",
      "githubRepo": "https://github.com/HeLeHanPrivate/PBTwithCodeGen",
      "ai_summary": "A novel framework using Property-Based Testing and collaborative LLM-based agents improves code generation correctness and generalization.",
      "ai_keywords": [
        "Large Language Models",
        "code generation",
        "Test-Driven Development",
        "Property-Based Testing",
        "PBT",
        "iterative refinement",
        "property violations",
        "pass@1 improvements"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-06-23T02:01:12.000Z",
    "title": "Use Property-Based Testing to Bridge LLM Code Generation and Validation",
    "summary": "Large Language Models (LLMs) excel at code generation, but ensuring their\noutputs to be functionally correct, especially in complex programming tasks, is\na persistent challenge. While traditional Test-Driven Development (TDD) offers\na path for code refinement, its efficacy with LLMs is often undermined by the\nscarcity of high-quality test cases or the pitfalls of automated test\ngeneration, including biased tests or inaccurate output predictions that can\nmisdirect the correction process. This paper introduces Property-Generated\nSolver, a novel framework that leverages Property-Based Testing (PBT) to\nvalidate high-level program properties or invariants, instead of relying on\nspecific input-output examples. These properties are often simpler to define\nand verify than directly predicting exhaustive test oracles, breaking the\n\"cycle of self-deception\" where tests might share flaws with the code they are\nmeant to validate. Property-Generated Solver employs two collaborative\nLLM-based agents: a Generator dedicated to code generation and iterative\nrefinement, and a Tester that manages the PBT life-cycle and formulate\nsemantically rich feedback from property violations. The resulting\ncomprehensive and actionable feedback then guides the Generator in its\nrefinement efforts. By establishing PBT as the core validation engine within\nthis iterative, closed-loop paradigm, Property-Generated Solver provides a\nrobust mechanism for steering LLMs towards more correct and generalizable code.\nExtensive experimental results on multiple code generation benchmarks\ndemonstrate that Property-Generated Solver achieves substantial pass@1\nimprovements, ranging from 23.1% to 37.3% relative gains over established TDD\nmethods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a96a375a69e2ca889abdff",
      "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
      "fullname": "fanhongxing",
      "name": "fanhongxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18088",
      "authors": [
        {
          "_id": "685ae8e8d2ee4fac76521d03",
          "user": {
            "_id": "65b37a9b06d8b55123ef8921",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
            "isPro": false,
            "fullname": "Tianxing Chen",
            "user": "TianxingChen",
            "type": "user"
          },
          "name": "Tianxing Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T18:13:30.491Z",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d04",
          "name": "Zanxin Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d05",
          "name": "Baijun Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d06",
          "name": "Zijian Cai",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d07",
          "name": "Yibin Liu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d08",
          "name": "Qiwei Liang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d09",
          "name": "Zixuan Li",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0a",
          "name": "Xianliang Lin",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0b",
          "name": "Yiheng Ge",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0c",
          "name": "Zhenyu Gu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0d",
          "name": "Weiliang Deng",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0e",
          "name": "Yubin Guo",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0f",
          "name": "Tian Nian",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d10",
          "name": "Xuanbing Xie",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d11",
          "name": "Qiangyu Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d12",
          "name": "Kailun Su",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d13",
          "name": "Tianling Xu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d14",
          "name": "Guodong Liu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d15",
          "name": "Mengkang Hu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d16",
          "name": "Huan-ang Gao",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d17",
          "name": "Kaixuan Wang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d18",
          "name": "Zhixuan Liang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d19",
          "name": "Yusen Qin",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1a",
          "name": "Xiaokang Yang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1b",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1c",
          "name": "Yao Mu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-22T16:26:53.000Z",
      "submittedOnDailyAt": "2025-06-26T06:13:35.593Z",
      "title": "RoboTwin 2.0: Scalable Data Generator와 Strong Domain Randomization을 활용한 Robust Two-Handed Robot Manipulation Design with Benchmarks",
      "submittedOnDailyBy": {
        "_id": "65b37a9b06d8b55123ef8921",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
        "isPro": false,
        "fullname": "Tianxing Chen",
        "user": "TianxingChen",
        "type": "user"
      },
      "summary": "シミュレーションベースデータ 합성은 실세계의 로봇 조작을 강화하기 위한 강력한 패러다임으로 등장했습니다. 그러나 현재의 합성 데이터 세트는 2가지 도전으로 강건한 양손 조작에 충분하지 않습니다: (1) 새로운 태스크에 대한 효율적인 스케일러블な 데이터 생성 방법의 부족과 (2) 복잡한 실세계의 복잡성을 감지하지 않고 단순화된 시뮬레이션 환경. 우리는 RoboTwin 2.0, 하나의 스케일러블 시뮬레이션 프레임워크를 소개합니다. 이 프레임워크는 자동화된, 대규모의 다양성과 현실적인 데이터를 생성할 수 있으며, 양손 조작의 통일된 평가 프로토콜을 제공합니다. 먼저, RoboTwin-OD, 147개 카테고리의 731개의 인스턴스를 포함하는 대규모 객체 라이브러리를 구축합니다. 각 인스턴스는 의미적 라벨과 동작 관련 라벨로 설명되어 있습니다. 이 기반으로, 멀티모달 대 언어 모델 (MLLMs)과 시뮬레이션 인라인의 정밀화를 결합한 전문적인 데이터 합성 파이프라인을 개발합니다. 시뮬레이션에서 실세계로의 태ン스 이동을 개선하기 위해, RoboTwin 2.0는 5가지 축인 크레지, 조명, 배경, 테이블의 높이, 언어 지시에 대한 구조화된 도메인 랜덤화를 적용하여 데이터의 다양성과 정책의 강건성을 향상시킵니다. 이 프레임워크는 50개의 쌍 손 태스크를 범위로, 5개의 로봇의 신체 이미지를 넘어 범위에서 구현되어 있으며, 100,000 이상의 도메인 랜덤화된 엔지니어링 트래지토리를 사전적으로 수집합니다. 실험 결과를 통해, 코드 생성의 성공률이 10.9% 증가했으며, 새로운 실세계의 스케너에 대한 일반화가 개선되었습니다. 데이터 세트에 대한 VLA 모델의 미세 조정은 새로운 스케너의 실세계 태스크에 대해 367%의 상대적인 개선 (42.0% 대 9.0%)을 얻으며, 실세계의 슈퍼바이저를 제외한 합성 데이터에만 훈련하여 228%의 상대적인 이익을 얻으며, 실세계의 슈퍼바이저를 제외한 강한 일반화를 보여주었습니다. 데이터 생성자, 벤치마크, 데이터 세트, 코드를 릴리즈하고, 강건한 양손 조작의 스케일러블 연구에 지원을 제공합니다.",
      "upvotes": 7,
      "discussionId": "685ae8e8d2ee4fac76521d1d",
      "projectPage": "https://robotwin-platform.github.io/",
      "githubRepo": "https://github.com/robotwin-Platform/RoboTwin",
      "ai_summary": "RoboTwin 2.0 is a scalable simulation framework for bimanual robotic manipulation that uses expert data synthesis and structured domain randomization to generate diverse and realistic synthetic data, improving sim-to-real transfer and generalization.",
      "ai_keywords": [
        "multimodal large language models",
        "simulation-in-the-loop",
        "structured domain randomization",
        "bimanual manipulation",
        "task-level execution code",
        "sim-to-real transfer",
        "zero-shot learning"
      ],
      "githubStars": 1129
    },
    "publishedAt": "2025-06-22T12:26:53.000Z",
    "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain\n  Randomization for Robust Bimanual Robotic Manipulation",
    "summary": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b37a9b06d8b55123ef8921",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
      "fullname": "Tianxing Chen",
      "name": "TianxingChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18674",
      "authors": [
        {
          "_id": "685cd8fc696820ba1f28f2aa",
          "name": "Raquel Ferrando",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ab",
          "name": "Javier Conde",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ac",
          "name": "Gonzalo Martínez",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ad",
          "name": "Pedro Reviriego",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T14:18:46.000Z",
      "submittedOnDailyAt": "2025-06-26T03:52:31.234Z",
      "title": "로봇 텍스트 모델은 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을 사용하여, 로봇 텍스트 모델을",
      "submittedOnDailyBy": {
        "_id": "64f31365ed48e3bb9c487d5d",
        "avatarUrl": "/avatars/979c1979eadbd4529c95b925bbb58d78.svg",
        "isPro": false,
        "fullname": "Gonzalo",
        "user": "gonzmart",
        "type": "user"
      },
      "summary": "LLM의 계산 비용과 에너지 비용은 모델 크기의 확대와 수백만 명의 사용자의 대규모 도입으로 지수적으로 상승하고 있습니다. LLM의 단위 비용은 토큰 계산입니다. 따라서, 토큰화는 모델의 효율성에 중요한 역할을 하고 있으며, 이는 학습 코퍼스의 텍스트의 토큰 수를 최소限に抑えるために 조정되어 있습니다. LLM의 가장 인기 있는 애플리케이션 중 하나는 사용자와 상호작용하는 챗봇입니다. 그 관찰의 핵심은, 챗봇에서 중요한 것은 사용자 텍스트의 입력과 챗봇의 답변에 대한 토큰화의 성능입니다. 이는 학습 코퍼스의 텍스트와 다릅니다. 따라서, 토큰화를 챗봇의 대화에 최적화하는 데 있어 어떤 잠재적인 이익이 있는지는 곧바로 떠오르고 있습니다. 본 논문에서는, 챗봇의 대화를 위한 토큰화의 최적화 가능성을 조사하기 위해, 공개적으로 사용할 수 있는 챗봇의 대화 코퍼스를 사용하며, 벡토라이핑을 재설계하고, 그 성능을 이 분야에서 평가합니다. 결과는, 대화 최적화된 토큰화가 챗봇의 다이アロ그에서 토큰 수를 일관적으로 줄일 수 있으며, 이는 5%에서 10%의 범위에서 의미적인 에너지 절감을 실현할 수 있으며, 원래 학습 코퍼스의 토큰화 효과에 최소한으로 또는 더 큰 긍정적인 영향을 미치는 것을 보여줍니다.",
      "upvotes": 5,
      "discussionId": "685cd8fc696820ba1f28f2ae",
      "ai_summary": "Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.",
      "ai_keywords": [
        "Large Language Models",
        "token",
        "tokenizer",
        "chatbots",
        "conversation-optimized tokenizers"
      ]
    },
    "publishedAt": "2025-06-23T10:18:46.000Z",
    "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?",
    "summary": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f31365ed48e3bb9c487d5d",
      "avatarUrl": "/avatars/979c1979eadbd4529c95b925bbb58d78.svg",
      "fullname": "Gonzalo",
      "name": "gonzmart",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20544",
      "authors": [
        {
          "_id": "685cdd71696820ba1f28f2b8",
          "user": {
            "_id": "677cfa6cac2db4c2265edb26",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Kbi96ndfY-CIuJNd2TRZt.jpeg",
            "isPro": false,
            "fullname": "Ammar Khairi",
            "user": "ammar-cohere",
            "type": "user"
          },
          "name": "Ammar Khairi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:26.999Z",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2b9",
          "name": "Daniel D'souza",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2ba",
          "name": "Ye Shen",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2bb",
          "name": "Julia Kreutzer",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2bc",
          "name": "Sara Hooker",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T15:37:53.000Z",
      "submittedOnDailyAt": "2025-06-26T04:17:49.221Z",
      "title": "당신의 삶이 샘플을 준다면: 멀티언어 LLM의 추론을 확장하는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을 향상시키는 이점\n\nCompute를 확장하여 멀티언어 LLM의 추론을",
      "submittedOnDailyBy": {
        "_id": "6544e43b12da508864c38f96",
        "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
        "isPro": false,
        "fullname": "Julia Kreutzer",
        "user": "JuliaKreutzerCohere",
        "type": "user"
      },
      "summary": "최근의 대 언어 모델(LLMs)의 발전은 추론 시의 계산량을 확장하고 모델의 재학습을 피하여 성능을 향상시키는 데 중점을 두고 있습니다. 일반적인 접근 방식에는 여러 출력을 병렬로 샘플링하여 그 중 최종적인 출력을 선택하는 방법 등이 있습니다. 그러나 현재의 연구는 영어와 수학이나 코드 등 몇 가지 분야에만 집중되어 있습니다. 대조적으로, 우리는 개방된 태스크, 공식적으로 검증 가능한 태스크, 그리고 언어 간 일반화가 가능한 기술에 관심을 가지고 있습니다. 이 연구에서는, 여러 언어와 여러 태스크 설정에서 추론 시의 계산량을 강하게 확장하는 개방된 생성 태스크에 대해 조사하고 있습니다.\n\n우리가 발견한 것은 온도 변화에 기반한 샘플링 전략과 선택 전략이 다양한 분야와 다양한 언어 설정에 대응하는 것이 필요함을 보여줍니다. 기존의 선택 방법들을 평가하고, 영어에서 효과적인 전략이 언어 간 일반화할 수 없게 되는 것을 밝혀냅니다. 우리는 여러 언어와 여러 태스크의 추론 시나리오에 특화된 새로운 샘플링과 선택 전략을 제안하고, 이들은 언어와 태스크에 대해 뚜렷한 효과를 보입니다. 특히, 우리의 조합된 샘플링과 선택 방법은 m-ArenaHard-v2.0 프로ン퓰트에서 우리 8B 모델의 승률에 평균 +6.8의 상승을 나타내며, ゲミニ 등 프로프라이 모델에 대한 효과도 있습니다. 더 큰 규모에서는, Command-A(111B 모델)에 우리 방법을 적용한 것은 같은 벤치마크에서 승률에 +9.0의 개선을 나타내며, 단일 샘플의 검증에 대해 5개의 샘플로 큰 향상을 나타내며 최소한의 비용으로 효과적인 증가를 나타냅니다. 우리의 결과를 통해, 추론 시의 계산량에 대한 언어와 태스크에 대한 관심의 필요성을 강조하고, 대표적인 언어에서 성능을 향상시키기 위한 민주화를 목표로 합니다.",
      "upvotes": 4,
      "discussionId": "685cdd71696820ba1f28f2bd",
      "ai_summary": "The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.",
      "ai_keywords": [
        "sampling strategy",
        "selection strategy",
        "temperature variation",
        "open-ended generative tasks",
        "multilingual",
        "multi-task",
        "m-ArenaHard-v2.0",
        "win-rates",
        "Command-A",
        "single-sample decoding",
        "language-aware",
        "task-aware"
      ]
    },
    "publishedAt": "2025-06-25T11:37:53.000Z",
    "title": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs",
    "summary": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20544.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6544e43b12da508864c38f96",
      "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
      "fullname": "Julia Kreutzer",
      "name": "JuliaKreutzerCohere",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20495",
      "authors": [
        {
          "_id": "685d0223696820ba1f28f322",
          "name": "Haoze Wu",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f323",
          "name": "Yunzhi Yao",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f324",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f325",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f326",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T14:41:13.000Z",
      "submittedOnDailyAt": "2025-06-26T06:48:06.720Z",
      "title": "ReCode: 강화학습을 이용한 코드 API의 지식 업데이트",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "대규모 언어 모델(LLMs)는 코드 생성 능력을 보여주지만, 외부 라이브러리 API의 빈번한 업데이트에 적응하지 못하고 실패합니다. 이 중요한 한계는 훈련 데이터에서 오래된 API 캐럿 지식에 기반하여 있으며, 현재의 문서도 활용할 수 있기 때문에 동적인 환경에서 신뢰할 수 있는 코드 생성을 방해합니다. 이러한 문제를 해결하기 위해, 우리는 개발 프로그래머가 API의 변경에 적응하도록 만든 규칙 기반의 강화학습을 활용한 새로운 프레임워크인 ReCode를 제안합니다. 특히, LLMs를 코드의 버전 이동에 대해 업데이트 정보를 기반으로 훈련하기 위해 약 2,000건의 데이터 엔트리를 포함하는 데이터 세트를 구축합니다. 다음으로, 강화학습의 보상으로 코드 평가의 개선된 문자열 유사성 메트릭을 사용합니다. 실험 결과, ReCode는 동적인 API 스케너에서 LLMs의 코드 생성 성능을 크게 향상시키고, 특히 CodeUpdateArena 태스크에 대해 효과가 명확히 보입니다. 중요하게도, 관찰 제어의 미세 조정에 비해, ReCode는 LLMs의 일반적인 코드 생성 능력에 영향을 미치지 않습니다. ReCode는 다양한 LLMs와 강화 학습 알고리즘(GRPO와 DAPO)에 적용되어, 모든 경우에 일치하는 개선을 얻었습니다. 특히, 훈련 후, Qwen2.5-Coder-7B은 동일한 아키텍처의 코드 구조조정 모델과 논리 모델을 초과했습니다. 코드는 https://github.com/zjunlp/ReCode에 접근할 수 있습니다.",
      "upvotes": 4,
      "discussionId": "685d0224696820ba1f28f327",
      "githubRepo": "https://github.com/zjunlp/ReCode",
      "ai_summary": "ReCode, a rule-based reinforcement learning framework, enhances large language models' adaptation to API updates without compromising their general code generation capabilities.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "code generation",
        "API updates",
        "dataset",
        "version migration",
        "string similarity metric",
        "reinforcement learning",
        "rule-based",
        "Qwen2.5-Coder-7B",
        "CodeUpdateArena",
        "GRPO",
        "DAPO"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-06-25T10:41:13.000Z",
    "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
    "summary": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20495.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 25
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20452",
      "authors": [
        {
          "_id": "685cfd21696820ba1f28f30a",
          "name": "Tobias Vontobel",
          "hidden": false
        },
        {
          "_id": "685cfd21696820ba1f28f30b",
          "user": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "isPro": false,
            "fullname": "Seyedmorteza Sadat",
            "user": "msadat97",
            "type": "user"
          },
          "name": "Seyedmorteza Sadat",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:18.462Z",
          "hidden": false
        },
        {
          "_id": "685cfd21696820ba1f28f30c",
          "name": "Farnood Salehi",
          "hidden": false
        },
        {
          "_id": "685cfd21696820ba1f28f30d",
          "user": {
            "_id": "630f7646197cd3f24e7f8e9f",
            "avatarUrl": "/avatars/59bbd4ed38277b313051aac78f6808ac.svg",
            "isPro": false,
            "fullname": "Romann Weber",
            "user": "RMW",
            "type": "user"
          },
          "name": "Romann M. Weber",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:16.404Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T13:58:37.000Z",
      "submittedOnDailyAt": "2025-06-26T06:29:51.626Z",
      "title": "히와브: ワーブレット 기반 디퓨저 샘플링에 의한 훈련없이 고해상도 이미지 생성",
      "submittedOnDailyBy": {
        "_id": "63b4b02a103617b0a5b0ee2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
        "isPro": false,
        "fullname": "Seyedmorteza Sadat",
        "user": "msadat97",
        "type": "user"
      },
      "summary": "디퓨전 모듈은 이미지 합성의 첨단 접근 방식으로 등장하며, 뛰어난 현실성과 다양성을 보여주고 있습니다. 그러나 고해상도 디퓨전 모듈의 훈련은 계산적으로 어려워, 현재의 트라이ア우트 생성 기술은 훈련해상도보다 높은 이미지 합성에서 객체의 중복과 공간적 불연속적인artifacts를 많이 포함하는 경향이 있습니다. 본 논문에서는, 훈련없이 0샷 접근 방식을 통해, HiWave를 통해 사전 학습된 디퓨전 모듈을 사용하여 초고해상도 이미지 합성의 시각적 신뢰도와 구조적 일관성을 크게 향상시키기 위한 방법을 제시합니다. 우리 방법은 2단계 파이프라인을 사용합니다: 사전 학습 모듈에서 베이스 이미지를 생성하고, 패치 위즈 DDIM 역전원 스텝과 새로운 웨이브렛 기반의 세부 확대 모듈을 이어줍니다. 특히, 우리는 처음에 역전원手法를 사용하여, 베이스 이미지에서 글로벌 일관성을 유지하는 초기 노이즈 벡터를 얻습니다. 그 후, 샘플링 때, 우리 웨이브렛 영역의 세부 확대 모듈은 베이스 이미지에서 저주파 성분을 유지하며, 구조적 일관성을 보장하고, 선택적으로 고주파 성분을 가이드하여 세부적인 디테일과 테크스쳐를 향상시킵니다. Stable Diffusion XL를 사용하여 확장 평가에 따라, HiWave는 기존 방법의 일반적인 시각적artifacts를 효과적으로 줄이고, 상위 시각적 품질을 달성합니다. 사용자 스테이지에서, HiWave의 성능이 확인되어, 상태의 최선 대비 80% 이상의 성능을 비교하여 우수하게 평가되고, 고품질의 초고해상도 이미지 합성의 효과성을 강조합니다.",
      "upvotes": 4,
      "discussionId": "685cfd22696820ba1f28f30e",
      "ai_summary": "HiWave enhances ultra-high-resolution image synthesis using pretrained diffusion models through a two-stage pipeline involving DDIM inversion and wavelet-based detail enhancement, improving visual fidelity and reducing artifacts.",
      "ai_keywords": [
        "diffusion models",
        "image synthesis",
        "photorealism",
        "high resolutions",
        "zero-shot generation",
        "artifacts",
        "object duplication",
        "spatial incoherence",
        "pretrained diffusion models",
        "two-stage pipeline",
        "DDIM inversion",
        "wavelet-based detail enhancer",
        "structural consistency",
        "fine details",
        "textures",
        "perceptual quality",
        "user study"
      ]
    },
    "publishedAt": "2025-06-25T09:58:37.000Z",
    "title": "HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based\n  Diffusion Sampling",
    "summary": "Diffusion models have emerged as the leading approach for image synthesis,\ndemonstrating exceptional photorealism and diversity. However, training\ndiffusion models at high resolutions remains computationally prohibitive, and\nexisting zero-shot generation techniques for synthesizing images beyond\ntraining resolutions often produce artifacts, including object duplication and\nspatial incoherence. In this paper, we introduce HiWave, a training-free,\nzero-shot approach that substantially enhances visual fidelity and structural\ncoherence in ultra-high-resolution image synthesis using pretrained diffusion\nmodels. Our method employs a two-stage pipeline: generating a base image from\nthe pretrained model followed by a patch-wise DDIM inversion step and a novel\nwavelet-based detail enhancer module. Specifically, we first utilize inversion\nmethods to derive initial noise vectors that preserve global coherence from the\nbase image. Subsequently, during sampling, our wavelet-domain detail enhancer\nretains low-frequency components from the base image to ensure structural\nconsistency, while selectively guiding high-frequency components to enrich fine\ndetails and textures. Extensive evaluations using Stable Diffusion XL\ndemonstrate that HiWave effectively mitigates common visual artifacts seen in\nprior methods, achieving superior perceptual quality. A user study confirmed\nHiWave's performance, where it was preferred over the state-of-the-art\nalternative in more than 80% of comparisons, highlighting its effectiveness for\nhigh-quality, ultra-high-resolution image synthesis without requiring\nretraining or architectural modifications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20452.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b4b02a103617b0a5b0ee2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
      "fullname": "Seyedmorteza Sadat",
      "name": "msadat97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18403",
      "authors": [
        {
          "_id": "685a555f0e4ad7e2197586b1",
          "user": {
            "_id": "65eef9ce7443c09267513796",
            "avatarUrl": "/avatars/62547f99130557f54093b2ff4d6c9c24.svg",
            "isPro": false,
            "fullname": "Muntasir Adnan",
            "user": "adnaan525",
            "type": "user"
          },
          "name": "Muntasir Adnan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:33:02.852Z",
          "hidden": false
        },
        {
          "_id": "685a555f0e4ad7e2197586b2",
          "name": "Carlos C. N. Kuhn",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T08:40:45.000Z",
      "submittedOnDailyAt": "2025-06-26T01:42:45.705Z",
      "title": "디버깅 다운셋 인덱스：코드 LLM의 디버깅 전략을 재검토する",
      "submittedOnDailyBy": {
        "_id": "65eef9ce7443c09267513796",
        "avatarUrl": "/avatars/62547f99130557f54093b2ff4d6c9c24.svg",
        "isPro": false,
        "fullname": "Muntasir Adnan",
        "user": "adnaan525",
        "type": "user"
      },
      "summary": "AI 디버그의 효과성은 예측 가능한 지수 함수의 감쇠 패턴을 인정하고, 대부분의 모델은 2-3번의 시도에서 60-80%의 디버그 능력이 손실되며, 여러 번의 시도를 반복하는 것이 실질적인 코드 생성 시스템의 중요한 능력임을 인정받는다. 우리는 디버그가 유효하지 않을 때를 정량화하고, 수학적 프레임워크인 디버그 데카인 지수(DDI)를 제시하여 디버그의 효율성을 예측할 수 있는 접근점을 예측하는 데 사용한다. 우리의 전략적인 새로운 시작 접근법은 과거의 도입에 대한 전략적인 측면에서 디버그 프로세스를 탐색하고, 그 때의 적절한 접근이 디버그의 효율성을 회복할 수 있음을 보여준다. DDI는 현재의 AI 디버그의 근본적인 한계를 명확히 하고, 여러 번의 시도를 반복하는 코드 생성 전략을 최적화하기 위해 처음으로 정량적인 프레임워크를 제공하여 이점을 보낸다.",
      "upvotes": 2,
      "discussionId": "685a555f0e4ad7e2197586b3",
      "ai_summary": "The Debugging Decay Index (DDI) quantifies and optimizes the effectiveness of iterative AI debugging by predicting intervention points to revive and enhance debugging capability.",
      "ai_keywords": [
        "AI debugging",
        "Debugging Decay Index (DDI)",
        "iterative debugging",
        "code generation",
        "effectiveness",
        "intervention points"
      ]
    },
    "publishedAt": "2025-06-23T04:40:45.000Z",
    "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs",
    "summary": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18403.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65eef9ce7443c09267513796",
      "avatarUrl": "/avatars/62547f99130557f54093b2ff4d6c9c24.svg",
      "fullname": "Muntasir Adnan",
      "name": "adnaan525",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.20512",
      "authors": [
        {
          "_id": "685cb8d7696820ba1f28f296",
          "name": "Zengzhi Wang",
          "hidden": false
        },
        {
          "_id": "685cb8d7696820ba1f28f297",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "685cb8d7696820ba1f28f298",
          "name": "Xuefeng Li",
          "hidden": false
        },
        {
          "_id": "685cb8d7696820ba1f28f299",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62cbeb2d72dfd24b86bdf977/cZoks9vqpXBnkpA9PgVRV.png"
      ],
      "publishedAt": "2025-06-25T14:58:13.000Z",
      "submittedOnDailyAt": "2025-06-26T07:21:37.577Z",
      "title": "옥토티하이너：중간 학습에서 강화 학습의 스케일링을 촉진합니다.",
      "submittedOnDailyBy": {
        "_id": "62cbeb2d72dfd24b86bdf977",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62cbeb2d72dfd24b86bdf977/UcGYYSBNrCvPM5K9v-sro.png",
        "isPro": false,
        "fullname": "Zengzhi Wang",
        "user": "SinclairWang",
        "type": "user"
      },
      "summary": "異なる 기초 언어 모델의 가족, 예를 들어 Llama와 Qwen은, 강화 학습(RL) 후 수정 단계에서 서로 다른 행동을 보여줍니다. 기초 언어 모델이 강화 학습에 적합한 이유는 무엇일까요? 이러한 문제를 깊은 이해를 얻는 것이 다음 세대의 RL 스케일러블 기초 모델의 개발에 중요합니다. 본 연구에서는, 중반 학습 전략이 RL의 동적으로 어떻게 영향을 미칠지 조사하고, 대표적인 두 가지 모델의 가족인 Qwen과 Llama를 중심으로 진행합니다. 본 연구는 다음과 같은 점에 대한 결과를 제시합니다: (1) 고품질의 수학 코퍼스, MegaMath-Web-Pro 등은 기초 모델과 RL의 성능을 크게 향상시키고, 현재의 대체 코퍼스(예: FineMath-4plus)가 이점을 얻지 못하는 것을 보여줍니다; (2) QA 스타일의 데이터를 추가하고, 특히 긴 Chain-of-Thought(CoT) 추론 예를 포함하는 데이터를 추가함으로써 RL의 결과를 향상시키고, 지시 데이터가 이러한 효과를 내는 것을 보여줍니다; (3) 긴 CoT는 추론의 깊이를 향상시키지만, 동시에 모델의 응답 길이와 RL 훈련의 불안정성을 불러오고, 데이터의 형식의 중요성을 강조합니다; (4) 중반 학습의 스케일링은 차차 강한 하위 RL 성능을 보여주는 것을 보여줍니다. 이러한 통찰에 기반하여, 2단계의 중반 학습 전략인 Stable-then-Decay를 도입하고, 기초 모델은 200B 토큰으로 고정 학습률로 학습되고, 그 후, 3개의 CoT 포커스 브랜치에서 20B 토큰을 학습시키고 학습률의 감소를 수행합니다. 이로 인해, OctoThinker 모델의 가족은 강력한 RL의 호환성을 보여주고, 더 RL에 적합한 모델의 가족과의 성능 간격을 좁히는 것을 가능하게 합니다. 본 연구는, RL 시대의 기초 모델의 사전 학습 전략을 형성하는 것을 희망합니다. 더 나은 연구를 위해, 본 연구에서는 오픈 소스 모델과 700억 토큰 이상의 수리 이론력을 가진 코퍼스(MegaMath-Web-Pro-Max)를 공개합니다.",
      "upvotes": 1,
      "discussionId": "685cb8d7696820ba1f28f29a",
      "githubRepo": "https://github.com/GAIR-NLP/OctoThinker",
      "ai_summary": "Investigating mid-training strategies reveals that high-quality mathematical corpora and well-formatted chain-of-thought reasoning examples enhance reinforcement learning performance in language models, leading to the development of OctoThinker.",
      "ai_keywords": [
        "reinforcement learning",
        "base language model",
        "mid-training strategy",
        "MegaMath-Web-Pro",
        "QA-style data",
        "chain-of-thought (CoT) reasoning",
        "data formatting",
        "learning rate decay",
        "OctoThinker",
        "MegaMath-Web-Pro-Max"
      ],
      "githubStars": 66
    },
    "publishedAt": "2025-06-25T10:58:13.000Z",
    "title": "OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling",
    "summary": "Different base language model families, such as Llama and Qwen, exhibit\ndivergent behaviors during post-training with reinforcement learning (RL),\nespecially on reasoning-intensive tasks. What makes a base language model\nsuitable for reinforcement learning? Gaining deeper insight into this question\nis essential for developing RL-scalable foundation models of the next\ngeneration. In this work, we investigate how mid-training strategies shape RL\ndynamics, focusing on two representative model families: Qwen and Llama. Our\nstudy reveals that (1) high-quality mathematical corpora, such as\nMegaMath-Web-Pro, significantly improve both base model and RL performance,\nwhile existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further\nadding QA-style data, particularly long chain-of-thought (CoT) reasoning\nexamples, enhances RL outcomes, and instruction data further unlocks this\neffect; (3) while long-CoT improves reasoning depth, it can also induce\nverbosity of model responses and unstability of RL training, underscoring the\nimportance of data formatting; (4) scaling mid-training consistently leads to\nstronger downstream RL performance. Building on these insights, we introduce a\ntwo-stage mid-training strategy, Stable-then-Decay, in which base models are\nfirst trained on 200B tokens with a constant learning rate, followed by 20B\ntokens across three CoT-focused branches with learning rate decay. This yields\nOctoThinker, a family of models demonstrating strong RL compatibility and\nclosing the performance gap with more RL-friendly model families, i.e., Qwen.\nWe hope our work will help shape pre-training strategies for foundation models\nin the RL era. To support further research, we release our open-source models\nalong with a curated math reasoning-intensive corpus of over 70 billion tokens\n(i.e., MegaMath-Web-Pro-Max).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62cbeb2d72dfd24b86bdf977/cZoks9vqpXBnkpA9PgVRV.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20512.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62cbeb2d72dfd24b86bdf977",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62cbeb2d72dfd24b86bdf977/UcGYYSBNrCvPM5K9v-sro.png",
      "fullname": "Zengzhi Wang",
      "name": "SinclairWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19502",
      "authors": [
        {
          "_id": "685c02aadf8a0d6c70bbf918",
          "user": {
            "_id": "674ed490fc7f50ef61c3a7bd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
            "isPro": false,
            "fullname": "Aleksandr Algazinov",
            "user": "AleksandrAlgazinov",
            "type": "user"
          },
          "name": "Aleksandr Algazinov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T20:59:09.301Z",
          "hidden": false
        },
        {
          "_id": "685c02aadf8a0d6c70bbf919",
          "name": "Matt Laing",
          "hidden": false
        },
        {
          "_id": "685c02aadf8a0d6c70bbf91a",
          "name": "Paul Laban",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T10:40:23.000Z",
      "submittedOnDailyAt": "2025-06-26T01:26:07.991Z",
      "title": "MATE: 모바일 로봇 엔지니어링 테르미날 프로그래밍 엔비로온 포어 엑세스비리티 앱케이션",
      "submittedOnDailyBy": {
        "_id": "674ed490fc7f50ef61c3a7bd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
        "isPro": false,
        "fullname": "Aleksandr Algazinov",
        "user": "AleksandrAlgazinov",
        "type": "user"
      },
      "summary": "アクセシビリティ는 현대 사회에서 중요한 문제로, 많은 기술은 완전한 사용자의 필요를 충족하지 못하여 발생한다. 현재 존재하는 다 에이전트 시스템(MAS)은 클로즈드 소스 설계로 인한 사용자定制 부족으로, 필요한 사용자에게 완전히 도움이 되지 않는 경우가 많다. 이로 인해, 기능 장애자들은 디지털 환경과의 인터페이스를 다루는 데 큰 벽을 마주할 때가 있다. 우리는 사용자의 필요에 기반한 모델 변환을 수행하기 위한 다 모델 아키텍처의 다 모델 접근성 MAS를 소개합니다. 이 시스템은 데이터를 이해할 수 있는 형식으로 변환하고, 기능 장애자를 지원할 수 있습니다. 예를 들어, 사용자가 시각이 저하된 경우, 시스템은 이미지를 음성 설명으로 변환합니다. MATE는 건강 케어 등 광범위한 분야에서도 적용 가능하며, 많은 사용자 그룹에 도움이 되는 보조자가 될 수 있습니다. 이 시스템은 LLM API의 호출부터 사용자定制 머신 학습(ML) 챗어터즈의 사용까지 다양한 모델을 지원하고 있습니다. 이러한 유연성은 시스템이 다양한 요구에 대응할 수 있고, 다양한 하드웨어와 호환성을 유지하는 것을 보장합니다. 시스템은 로컬에서 실행될 것으로 가정되어 있으며, 중요한 정보의 프라이버시와 보안을 보장합니다. 또한, 프레임워크는 기관의 기술(예: 디지털 하드 케어 서비스)과 효과적으로 통합할 수 있으며, 실시간 사용자 보조 서비스를 제공할 수 있습니다. 그리고 ModCon-Task-Identifier를 소개합니다. 이 모델은 사용자의 입력으로부터 정확한 모델 변환 태스크를 추출할 수 있는 것입니다. 여러 실험은 ModCon-Task-Identifier가 우리의 사용자定制 데이터에 대해 다른 LLMs나 통계 모델보다 일관된 성능을 보여주는 것을 보여줍니다. 우리의 코드 및 데이터는 https://github.com/AlgazinovAleksandr/Multi-Agent-MATE에서 공개되어 있습니다.",
      "upvotes": 1,
      "discussionId": "685c02abdf8a0d6c70bbf91b",
      "ai_summary": "MATE, a multimodal accessibility multi-agent system, converts data into understandable formats based on user needs, supporting various disabilities and integrating with institutional technologies.",
      "ai_keywords": [
        "MULTIAGENT SYSTEMS",
        "MAS",
        "MODALITY CONVERSIONS",
        "LLM API",
        "CUSTOM MACHINE LEARNING CLASSIFIERS",
        "MODCON-TASK-IDENTIFIER",
        "LLM",
        "STATISTICAL MODELS"
      ]
    },
    "publishedAt": "2025-06-24T06:40:23.000Z",
    "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications",
    "summary": "Accessibility remains a critical concern in today's society, as many\ntechnologies are not developed to support the full range of user needs.\nExisting multi-agent systems (MAS) often cannot provide comprehensive\nassistance for users in need due to the lack of customization stemming from\nclosed-source designs. Consequently, individuals with disabilities frequently\nencounter significant barriers when attempting to interact with digital\nenvironments. We introduce MATE, a multimodal accessibility MAS, which performs\nthe modality conversions based on the user's needs. The system is useful for\nassisting people with disabilities by ensuring that data will be converted to\nan understandable format. For instance, if the user cannot see well and\nreceives an image, the system converts this image to its audio description.\nMATE can be applied to a wide range of domains, industries, and areas, such as\nhealthcare, and can become a useful assistant for various groups of users. The\nsystem supports multiple types of models, ranging from LLM API calling to using\ncustom machine learning (ML) classifiers. This flexibility ensures that the\nsystem can be adapted to various needs and is compatible with a wide variety of\nhardware. Since the system is expected to run locally, it ensures the privacy\nand security of sensitive information. In addition, the framework can be\neffectively integrated with institutional technologies (e.g., digital\nhealthcare service) for real-time user assistance. Furthermore, we introduce\nModCon-Task-Identifier, a model that is capable of extracting the precise\nmodality conversion task from the user input. Numerous experiments show that\nModCon-Task-Identifier consistently outperforms other LLMs and statistical\nmodels on our custom data. Our code and data are publicly available at\nhttps://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674ed490fc7f50ef61c3a7bd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
      "fullname": "Aleksandr Algazinov",
      "name": "AleksandrAlgazinov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.20331",
      "authors": [
        {
          "_id": "685d0b5d696820ba1f28f349",
          "user": {
            "_id": "62a9b0acf6708cb85014f9dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9b0acf6708cb85014f9dc/Sem1qcBt1lJjFEPK-xz4_.jpeg",
            "isPro": false,
            "fullname": "Rian Touchent",
            "user": "rntc",
            "type": "user"
          },
          "name": "Rian Touchent",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:08.325Z",
          "hidden": false
        },
        {
          "_id": "685d0b5d696820ba1f28f34a",
          "name": "Nathan Godey",
          "hidden": false
        },
        {
          "_id": "685d0b5d696820ba1f28f34b",
          "name": "Eric de la Clergerie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T11:30:25.000Z",
      "submittedOnDailyAt": "2025-06-26T07:44:08.382Z",
      "title": "바이오메디 엔리치도：LLM를 사용하여 사전학습과 희귀하거나 은닉된 내용을 추출하기 위한 바이오메디 데이터 세트의 확장",
      "submittedOnDailyBy": {
        "_id": "62a9b0acf6708cb85014f9dc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9b0acf6708cb85014f9dc/Sem1qcBt1lJjFEPK-xz4_.jpeg",
        "isPro": false,
        "fullname": "Rian Touchent",
        "user": "rntc",
        "type": "user"
      },
      "summary": "ビオメド・エニッチュード、PubMedから構築されたバイオメディカルテキストデータセットを紹介します。このデータセットは2段階のアノテーションプロセスを通じて構築されています。1段階目では、大規模な言語モデルがPubMedの科学論文から400Kページをアノテートし、それぞれの種類（レビュー、研究、臨床事例、その他）、領域（臨床、バイオメディカル、その他）、教育質量に対するスコアを割り当てます。教育質量スコア（1から5に評価される）は、そのページが大学レベルの学習にどのように役立つかを評価します。これらのアノテーションは、小規模な言語モデルの微調校に使用され、PMC-OAコーパス全体にラベルを伝播させます。このようになったメタデータは、臨床事例の精選サブセットを抽出し、商用利用許可の論文から450K件以上の高品質のサブセットを構築することができます。また、品質フィルタリングと領域の上昇サンプリングを通じて、数々のバージョンを構築します。\n\n臨床テキストは、隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠",
      "upvotes": 0,
      "discussionId": "685d0b5d696820ba1f28f34c",
      "ai_summary": "A biomedical text dataset, constructed from PubMed, uses a two-stage annotation process involving large and small language models to fine-tune and extract subsets for clinical NLP, improving pretraining efficiency and performance.",
      "ai_keywords": [
        "Biomed-Enriched",
        "PubMed",
        "large language model",
        "small language model",
        "fine-tuning",
        "PMC-OA corpus",
        "educational quality",
        "clinical cases",
        "biomedical NLP",
        "continual-pretraining",
        "OLMo2",
        "MMLU ProfMed",
        "MedQA",
        "MedMCQA",
        "training tokens"
      ]
    },
    "publishedAt": "2025-06-25T07:30:25.000Z",
    "title": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining\n  and Extracting Rare and Hidden Content",
    "summary": "We introduce Biomed-Enriched, a biomedical text dataset constructed from\nPubMed via a two-stage annotation process. In the first stage, a large language\nmodel annotates 400K paragraphs from PubMed scientific articles, assigning\nscores for their type (review, study, clinical case, other), domain (clinical,\nbiomedical, other), and educational quality. The educational quality score\n(rated 1 to 5) estimates how useful a paragraph is for college-level learning.\nThese annotations are then used to fine-tune a small language model, which\npropagates the labels across the full PMC-OA corpus. The resulting metadata\nallows us to extract refined subsets, including 2M clinical case paragraphs\nwith over 450K high-quality ones from articles with commercial-use licenses,\nand to construct several variants via quality filtering and domain upsampling.\nClinical text is typically difficult to access due to privacy constraints, as\nhospital records cannot be publicly shared. Hence, our dataset provides an\nalternative large-scale, openly available collection of clinical cases from\nPubMed, making it a valuable resource for biomedical and clinical NLP.\nPreliminary continual-pretraining experiments with OLMo2 suggest these curated\nsubsets enable targeted improvements, with clinical upsampling boosting\nperformance by ~5% on MMLU ProfMed and educational quality filtering improving\nMedQA and MedMCQA by ~1%. Combinations of these techniques led to faster\nconvergence, reaching same performance with a third of training tokens,\nindicating potential for more efficient and effective biomedical pretraining\nstrategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20331.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a9b0acf6708cb85014f9dc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9b0acf6708cb85014f9dc/Sem1qcBt1lJjFEPK-xz4_.jpeg",
      "fullname": "Rian Touchent",
      "name": "rntc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": true
  }
]