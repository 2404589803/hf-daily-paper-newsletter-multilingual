[
  {
    "paper": {
      "id": "2505.16938",
      "authors": [
        {
          "_id": "682fe3a565bac3ec3556fc6c",
          "name": "NovelSeek Team",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc6d",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc6e",
          "name": "Shiyang Feng",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc6f",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc70",
          "name": "Jiakang Yuan",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc71",
          "name": "Zhiyin Yu",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc72",
          "name": "Xiaohan He",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc73",
          "name": "Songtao Huang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc74",
          "name": "Shaowei Hou",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc75",
          "name": "Zheng Nie",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc76",
          "name": "Zhilong Wang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc77",
          "name": "Jinyao Liu",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc78",
          "name": "Runmin Ma",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc79",
          "name": "Tianshuo Peng",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7a",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7b",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7c",
          "name": "Shufei Zhang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7d",
          "name": "Xiaosong Wang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7e",
          "name": "Yilan Zhang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7f",
          "name": "Meng Li",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc80",
          "name": "Zhongying Tu",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc81",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc82",
          "name": "Wangli Ouyang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc83",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc84",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:27:43.000Z",
      "submittedOnDailyAt": "2025-05-23T01:25:34.477Z",
      "title": "뉴벨셸: 과학자로 나눌 에이전트 -- 가설부터 증명까지의 폐쇄 시스템의 구축",
      "submittedOnDailyBy": {
        "_id": "643dfd235aafbdca3a5792c0",
        "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg",
        "isPro": false,
        "fullname": "Bo Zhang",
        "user": "BoZhang",
        "type": "user"
      },
      "summary": "인공 지능(AI)은 과학 연구의 패러다임의 변화를 가속화하고 연구의 효율화를 강화하면서 혁신을 주도하고 있습니다. 뉴럴 시크를 소개합니다. 뉴럴 시크는 다양한 과학 연구 분야에서 자동 번역을 수행하는 통합된 폐쇄 루프 멀티 에이전트 프레임워크입니다. 이로 인해 연구자들은 이러한 분야에서 전례없는 속도와 정확도로 복잡한 문제를 해결할 수 있습니다. 뉴럴 시크는 3가지의 주요 장점을 특징으로 합니다: 1) scalability: 뉴럴 시크는 12가지의 과학 연구 태스크에 대해 다양성을 보여주고, baseline 코드의 성능 향상을 위한 혁신을 생성할 수 있습니다. 2) interactivity: 뉴럴 시크는 자동화된 단말에서부터 인간 전문가의 피드백과 멀티 에이전트의 상호작용을 제공하여, 대화형 전문가의 지식의 무간 통합을 가능하게 합니다. 3) efficiency: 뉴럴 시크는 인간의 노력을 대대적으로 줄일 때도, 다양한 과학 분야에서 놀라운 성능 이득을 얻고 있습니다. 예를 들어, 반응 생산 예측에서 12시간 동안 27.6%에서 35.4%로, 엔하너 활성 예측에서 4시간 동안 0.52에서 0.79로, 2D 세ман틱 분할에서 30시간 동안 78.8%에서 81.0%로 향상되었습니다.",
      "upvotes": 70,
      "discussionId": "682fe3a865bac3ec3556fd21"
    },
    "publishedAt": "2025-05-22T13:27:43.000Z",
    "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification",
    "summary": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16938.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "643dfd235aafbdca3a5792c0",
      "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg",
      "fullname": "Bo Zhang",
      "name": "BoZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16410",
      "authors": [
        {
          "_id": "682fd6045e83dc325675312b",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312c",
          "name": "Yifei Chen",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312d",
          "name": "Xiaoxi Li",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312e",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312f",
          "name": "Hongjin Qian",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753130",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753131",
          "name": "Hangyu Mao",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753132",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753133",
          "name": "Zhicheng Dou",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753134",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T09:00:19.000Z",
      "submittedOnDailyAt": "2025-05-23T00:31:41.669Z",
      "title": "Tool-Star: 강화학습에 의한 LLM 블라인드 다기능 체크기 지원",
      "submittedOnDailyBy": {
        "_id": "61cd4b833dd34ba1985e0753",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
        "isPro": false,
        "fullname": "KABI",
        "user": "dongguanting",
        "type": "user"
      },
      "summary": "최근, Large Language Models (LLMs)는 큰 규모의 Reinforcement Learning (RL)를 통해 놀라울 수 있는 논리론 능력을 보여주고 있습니다. 그러나, RL 알고리즘을 활용하여 LLMs에 의한 효과적인 여러 도구의 협업과 논리론을 강화하는 것은 개방적인 도전입니다. 본 논문에서는, RL 기반의 프레임워크인 Tool-Star을 소개하고, LLMs가 단계별로 논리론을 할 때 자동으로 여러 외부 도구를 호출할 수 있는 것을 목표로 합니다. Tool-Star은 6가지의 도구를 통합하고, 데이터의 합성과 훈련의 체계적인 설계를 채택합니다. 도구 사용 데이터의 부족을 해결하기 위해, 일반적인 도구 통합된 논리론 데이터의 합성 파이프라인을 제안하고, 도구 통합된 Prompt와 Hint 기반의 샘플링을 결합하여 자동적으로 scalable하게 도구 사용 과정을 생성합니다. 다음으로, 품질의 정규화와 난이도에 대한 클래스 분류 프로세스를 통해 저품질의 샘플을 제거하고, 데이터셋을 어려움에서부터 간단한 방향으로 배치합니다. 또한, 두 단계의 훈련 프레임워크를 제안하고, 여러 도구의 협업과 논리론을 강화하기 위해, (1) 냉정한 시작의 微調節과 (2) 휴리스틱한 보상 설계를 적용한 여러 도구의 자기 평가 로지스틱 로바스 알고리즘을 제안합니다. 10 이상의 어려운 논리론 벤치마크에 대한 실험 분석은 Tool-Star의 효과와 효율성을 밝혀줍니다. 코드는, https://github.com/dongguanting/Tool-Star에 공개되어 있습니다.",
      "upvotes": 37,
      "discussionId": "682fd6055e83dc3256753187",
      "projectPage": "https://github.com/dongguanting/Tool-Star/",
      "githubRepo": "https://github.com/dongguanting/Tool-Star/",
      "ai_summary": "Tool-Star, an RL-based framework, enables LLMs to autonomously use multiple tools for stepwise reasoning, leveraging data synthesis and hierarchical reward design.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "large-scale reinforcement learning",
        "RL",
        "multi-tool collaborative reasoning",
        "tool-use data",
        "tool-integrated reasoning",
        "tool-invocation feedback",
        "multi-tool self-critic",
        "hierarchical reward design"
      ]
    },
    "publishedAt": "2025-05-22T05:00:19.000Z",
    "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement\n  Learning",
    "summary": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61cd4b833dd34ba1985e0753",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
      "fullname": "KABI",
      "name": "dongguanting",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14810",
      "authors": [
        {
          "_id": "682ea2b450671dc82688b8ad",
          "user": {
            "_id": "640ad17a1ee054d66a74783e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640ad17a1ee054d66a74783e/u0PjIkyC-9HkGzEyUQ7JN.jpeg",
            "isPro": false,
            "fullname": "Tingchen Fu",
            "user": "TingchenFu",
            "type": "user"
          },
          "name": "Tingchen Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:44.217Z",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8ae",
          "name": "Jiawei Gu",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8af",
          "user": {
            "_id": "63f3502a520c14618925825a",
            "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
            "isPro": false,
            "fullname": "Yafu Li",
            "user": "yaful",
            "type": "user"
          },
          "name": "Yafu Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-22T04:06:13.396Z",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8b0",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8b1",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T18:18:01.000Z",
      "submittedOnDailyAt": "2025-05-23T00:49:30.349Z",
      "title": "스케일링 레지온, 제어의 손실: 대규모 논리 모형에서 지시에 따른 평가",
      "submittedOnDailyBy": {
        "_id": "64cb54da1af278541d663708",
        "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
        "isPro": false,
        "fullname": "Xiaoye Qu",
        "user": "Xiaoye08",
        "type": "user"
      },
      "summary": "명령어 따라기는 대규모 언어 모델(LLMs)와 사용자의 의도를 맞추기 위해 중요합니다. 최근의 이유에 대한 모델은 복잡한 수학 문제를 뛰어넘는 성능을 보여주지만, 자연어의 지시에 따라하는 능력은 아직 조사가 부족합니다. 본 논문에서는 수학적인 이유 작업에서 명령어 따라기를 평가하기 위한 전문 벤치마크인 \"MathIF\"를 소개합니다. 실험적 분석에서, 이유의 능력이 향상된 때에 제어성(Controllability)을 유지하는 것은 어려워졌음을 명확히 확인했습니다. 이유 능력이 높은 모델은 사용자의 지시에 따라하기가 어려워졌음을 알게 되었습니다. 모델을 긴 시퀀스 상태로 제한하거나, 이유에 대한 강화학습으로 훈련시킨 것은 특히 생성문장 길이가 길어질 때 명령어 따라기의 성능이 떨어집니다. 또한, 간단한 간섭으로 명령어 따라기의 회복이 부분적으로 가능하지만, 이는 이유 성능을 대신하여 비용이 들며, 이러한 발견은 현재의 LLM 훈련 패러다임에서 기본적인 대립을 밝혀, 명령어에 대한 이유 모델의 필요성을 강조합니다. 코드와 데이터는 https://github.com/TingchenFu/MathIF에서 릴리즈되어 있습니다.",
      "upvotes": 37,
      "discussionId": "682ea2b550671dc82688b8e2",
      "githubRepo": "https://github.com/TingchenFu/MathIF",
      "ai_summary": "An empirical analysis of MathIF identifies a tension between enhancing reasoning capacity and maintaining instruction adherence in large language models.",
      "ai_keywords": [
        "instruction-following",
        "reasoning-oriented models",
        "benchmarks",
        "chains-of-thought",
        "reinforcement learning",
        "instruction adherence"
      ]
    },
    "publishedAt": "2025-05-20T14:18:01.000Z",
    "title": "Scaling Reasoning, Losing Control: Evaluating Instruction Following in\n  Large Reasoning Models",
    "summary": "Instruction-following is essential for aligning large language models (LLMs)\nwith user intent. While recent reasoning-oriented models exhibit impressive\nperformance on complex mathematical problems, their ability to adhere to\nnatural language instructions remains underexplored. In this work, we introduce\nMathIF, a dedicated benchmark for evaluating instruction-following in\nmathematical reasoning tasks. Our empirical analysis reveals a consistent\ntension between scaling up reasoning capacity and maintaining controllability,\nas models that reason more effectively often struggle to comply with user\ndirectives. We find that models tuned on distilled long chains-of-thought or\ntrained with reasoning-oriented reinforcement learning often degrade in\ninstruction adherence, especially when generation length increases.\nFurthermore, we show that even simple interventions can partially recover\nobedience, though at the cost of reasoning performance. These findings\nhighlight a fundamental tension in current LLM training paradigms and motivate\nthe need for more instruction-aware reasoning models. We release the code and\ndata at https://github.com/TingchenFu/MathIF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14810.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb54da1af278541d663708",
      "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
      "fullname": "Xiaoye Qu",
      "name": "Xiaoye08",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16707",
      "authors": [
        {
          "_id": "682fdd77e3102e71872d9b00",
          "name": "Yongliang Wu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b01",
          "name": "Zonghui Li",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b02",
          "name": "Xinting Hu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b03",
          "name": "Xinyu Ye",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b04",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b05",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b06",
          "name": "Wenbo Zhu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b07",
          "name": "Bernt Schiele",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b08",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b09",
          "name": "Xu Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T14:08:59.000Z",
      "submittedOnDailyAt": "2025-05-23T00:59:23.402Z",
      "title": "KRIS-Bench: 미래의 지능형 이미지 편집 모델의 벤치마크",
      "submittedOnDailyBy": {
        "_id": "66f6bc97980d52c75c300511",
        "avatarUrl": "/avatars/f7c23c4b09701580b533212ec9b6e306.svg",
        "isPro": false,
        "fullname": "Yongliang",
        "user": "Liang0223",
        "type": "user"
      },
      "summary": "최근의 다 모델 생성 모델의 발전은 지시 기준 이미지 편집에 있어 상당한 진전을 실현했습니다. 그러나 이러한 모델은 시각적으로 적절한 출력을 생성하는 데만 충분하지 않습니다. 지식 기준의 논리적인 편집 작업의 기능에 대한 자세한 조사를 하지 않았습니다. 본 논문에서는 지식 기준의 논리로 모델을 평가하기 위한 진단 벤치마크인 KRIS-Bench(이미지 편집 시스템에서 지식 기준의 논리의 벤치마크)를 소개합니다. 교육 이론을 이어받아 KRIS-Bench는 편집 작업의 사실적, 개념적, 프로세스적 3가지 기본적인 지식 유형으로 분류하고 있습니다. 이 기술적 명칭에 기반하여 7가지의 논리적인 차원을 범위로 22가지의 대표적인 작업을 설계하고, 1,267건의 고품질의 레이블된 편집 인스턴스를 릴리즈했습니다. 미세한 평가를 지원하기 위해 지식의 합리성을 평가하는 새로운 메트릭을 제안하고, 지식의 조언을 추가하여 인간 연구에 의해 조정했습니다. 10개의 가장 선진 모델에 대한 실험 결과를 통해 논리적인 성능의 큰 차이를 명확히 한 것으로, 지식 중심적인 벤치마크의 필요성을 강조하고, 뇌근 영상 편집 시스템의 개발에 대한 발전을 촉진하는 것을 요구합니다.",
      "upvotes": 33,
      "discussionId": "682fdd79e3102e71872d9b79",
      "projectPage": "https://yongliang-wu.github.io/kris_bench_project_page/",
      "githubRepo": "https://github.com/mercurystraw/Kris_Bench",
      "ai_summary": "KRIS-Bench assesses generative models' knowledge-based reasoning in image editing through a taxonomy of editing tasks and a Knowledge Plausibility metric.",
      "ai_keywords": [
        "multi-modal generative models",
        "instruction-based image editing",
        "knowledge-based reasoning",
        "KRIS-Bench",
        "cognitive assessment",
        "foundational knowledge types",
        "Factual",
        "Conceptual",
        "Procedural",
        "reasoning dimensions",
        "Knowledge Plausibility metric"
      ]
    },
    "publishedAt": "2025-05-22T10:08:59.000Z",
    "title": "KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models",
    "summary": "Recent advances in multi-modal generative models have enabled significant\nprogress in instruction-based image editing. However, while these models\nproduce visually plausible outputs, their capacity for knowledge-based\nreasoning editing tasks remains under-explored. In this paper, we introduce\nKRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), a\ndiagnostic benchmark designed to assess models through a cognitively informed\nlens. Drawing from educational theory, KRIS-Bench categorizes editing tasks\nacross three foundational knowledge types: Factual, Conceptual, and Procedural.\nBased on this taxonomy, we design 22 representative tasks spanning 7 reasoning\ndimensions and release 1,267 high-quality annotated editing instances. To\nsupport fine-grained evaluation, we propose a comprehensive protocol that\nincorporates a novel Knowledge Plausibility metric, enhanced by knowledge hints\nand calibrated through human studies. Empirical results on 10 state-of-the-art\nmodels reveal significant gaps in reasoning performance, highlighting the need\nfor knowledge-centric benchmarks to advance the development of intelligent\nimage editing systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16707.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66f6bc97980d52c75c300511",
      "avatarUrl": "/avatars/f7c23c4b09701580b533212ec9b6e306.svg",
      "fullname": "Yongliang",
      "name": "Liang0223",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15966",
      "authors": [
        {
          "_id": "682fe6bd5f80e910085b5116",
          "name": "Alex Su",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b5117",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b5118",
          "name": "Weimin Ren",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b5119",
          "name": "Fangzhen Lin",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b511a",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:08:46.964Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/pPkAUFC7KKDS7Q9fhZKmM.png"
      ],
      "publishedAt": "2025-05-21T19:35:08.000Z",
      "submittedOnDailyAt": "2025-05-23T01:39:51.922Z",
      "title": "Pixel Reasoner: カイカルドリブレインドリニューロン학을 이용한 픽셀 공간의 이유론을 지지합니다.",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "鏈式思考推理는 다양한 분야의 대 언어 모델(LLMs)의 성능에 상당한 향상을 가져왔다. 그러나 이러한 추론 과정은 텍스트 공간에서만 이루어지므로, 시각적 밀도 任무의 유효성을 제한하고 있다. 이러한 한계를 해결하기 위해, 우리는 픽셀 공간에서 추론하는 개념을 제안했다. 이 새로운 프레임워크에서, 시각 언어 모델(VLMs)은 확대와 프레임 선택 등 시각적 추론 연산들을 갖게 된다. 이러한 연산들은 VLMs가 직접 시각적 증거를 확인하고 질문하고 추론할 수 있게 해, 시각적 任무의 추론의 진실도를 높일 수 있게 한다. VLMs에서 이러한 픽셀 공간 추론 능력을 갖추는 것은 초기의 불균형한 모델 능력과 새로운 픽셀 공간 연산에 대한 저항 등 明显한挑戰를 faces 한다. 우리는 이러한挑戰를 해결하기 위해 두 단계의 훈련 방법을 사용했다. 첫 번째 단계는 합성 추론 트래일로 guided adjustment을 통해 모델이 새로운 시각 연산에 익숙해지게 한다. 그 다음, 强化 학습(RL) 단계는 curiousity-driven reward plan을 사용하여 픽셀 공간 추론과 텍스트 추론 사이의 탐험을 균형을 이루게 한다. 이러한 시각 연산들을 통해, VLMs는 복잡한 시각 입력과 상호작용할 수 있으며, 예를 들어 정보 풍부한 이미지나 비디오에서 필요한 정보를 主動적으로 수집할 수 있게 된다. 우리는 이러한 방법을 다양한 시각 추론 테스트 기준에서 성능을 크게 향상시켰다는 것을 보여준다. 우리의 70억 매개변수 모델, \\model,은 V* 테스트 기준에서 84%, TallyQA-Complex에서 74%, InfographicsVQA에서 84%의 정확도를 달성했으며, 이는 지금까지의 가장 높은 정확도를 가진 开源 모델이다. 이러한 결과를 통해, 픽셀 공간 추론의 중요성과 우리의 프레임워크의 유효성을 강조한다.",
      "upvotes": 25,
      "discussionId": "682fe6bf5f80e910085b51ae",
      "ai_summary": "Introducing pixel-space reasoning in Vision-Language Models (VLMs) through visual operations like zoom-in and select-frame enhances their performance on visual tasks.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "pixel-space reasoning",
        "visual reasoning operations",
        "zoom-in",
        "select-frame",
        "reinforcement learning",
        "RL",
        "curiosity-driven reward scheme",
        "V* bench",
        "TallyQA-Complex",
        "InfographicsVQA"
      ]
    },
    "publishedAt": "2025-05-21T15:35:08.000Z",
    "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with\n  Curiosity-Driven Reinforcement Learning",
    "summary": "Chain-of-thought reasoning has significantly improved the performance of\nLarge Language Models (LLMs) across various domains. However, this reasoning\nprocess has been confined exclusively to textual space, limiting its\neffectiveness in visually intensive tasks. To address this limitation, we\nintroduce the concept of reasoning in the pixel-space. Within this novel\nframework, Vision-Language Models (VLMs) are equipped with a suite of visual\nreasoning operations, such as zoom-in and select-frame. These operations enable\nVLMs to directly inspect, interrogate, and infer from visual evidences, thereby\nenhancing reasoning fidelity for visual tasks. Cultivating such pixel-space\nreasoning capabilities in VLMs presents notable challenges, including the\nmodel's initially imbalanced competence and its reluctance to adopt the newly\nintroduced pixel-space operations. We address these challenges through a\ntwo-phase training approach. The first phase employs instruction tuning on\nsynthesized reasoning traces to familiarize the model with the novel visual\noperations. Following this, a reinforcement learning (RL) phase leverages a\ncuriosity-driven reward scheme to balance exploration between pixel-space\nreasoning and textual reasoning. With these visual operations, VLMs can\ninteract with complex visual inputs, such as information-rich images or videos\nto proactively gather necessary information. We demonstrate that this approach\nsignificantly improves VLM performance across diverse visual reasoning\nbenchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on\nTallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy\nachieved by any open-source model to date. These results highlight the\nimportance of pixel-space reasoning and the effectiveness of our framework.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/pPkAUFC7KKDS7Q9fhZKmM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15966.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 38
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16175",
      "authors": [
        {
          "_id": "682fd91a1ffb93faf139d288",
          "name": "Benjamin Schneider",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d289",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d28a",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d28b",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d28c",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:26:50.000Z",
      "submittedOnDailyAt": "2025-05-23T00:43:01.292Z",
      "title": "クイックビデオ：시스템 알고리즘에 의한 시간대 긴 비디오의 실시간 이해\n코드 쉐이프",
      "submittedOnDailyBy": {
        "_id": "62567c86d444a9b5a0ec51c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
        "isPro": false,
        "fullname": "Dongfu Jiang",
        "user": "DongfuJiang",
        "type": "user"
      },
      "summary": "장 비디오 이해는 현실적인 애플리케이션에서 중요한 능력으로 등장하여 왔습니다. 예를 들어, 비디오 시비어, 회의 요약, 교육 강의 분석, 스포츠 방송 등 다양한 분야에서 활용됩니다. 그러나 비디오 LLM에서, 두 가지 주요 문제로 계산량이 높고 실행이 불가능하게 됩니다. 1) 순차적인 비디오 디코딩, 비디오의 바이트 스트리م을 RGB 프레임으로 변환하는 과정에서, 1시간의 비디오 입력에 대해 약 1분 정도 소요될 수 있습니다. 2) LLM 추론을 위해, GPU 메모리를 10만 개의 토큰을 미리 읽어들이는 데, 라틴시와 메모리 사용이 높아집니다. 이러한 문제를 해결하기 위해, QuickVideo라는 시스템 알고리즘을 제안하고 있습니다. 이는 장 비디오 이해를 크게 가속화하고 실시간 하류 애플리케이션을 지원하는 것을 목표로 합니다. QuickVideo는 3가지의 핵심 인нова션으로 구성됩니다. QuickDecoder는 키 프레임에 대응하는 간격으로 비디오를 분할하고, 병렬화된 CPU 디코더로 2-3배의 속도 향상을 달성합니다. QuickPrefill은 KV 캐시 줄이기를 활용한 메모리 효율적인 사전 읽기 방법, GPU 메모리를 적게 사용하여 더 많은 프레임을 지원할 수 있습니다. 또한, CPU 비디오 디코딩과 GPU 추론을 오버랩 시냅스으로 결합합니다. 이러한 구성 요소는 장 비디오 입력에 대한 추론 시간을 약 1분까지 줄이고, 제한된 하드웨어에서도 Scalable한 고품질 비디오 이해가 가능하게 합니다. 실험에 따르면, QuickVideo는 시간과 샘플링 레이트의 확장성을 보여주고, 실용적인 장 비디오 처리가 가능한 것으로 나타납니다.",
      "upvotes": 24,
      "discussionId": "682fd91b1ffb93faf139d2d0",
      "ai_summary": "QuickVideo accelerates long-video understanding by combining a parallelized video decoder, memory-efficient prefilling, and overlapping video decoding with inference, enabling real-time performance.",
      "ai_keywords": [
        "QuickDecoder",
        "parallelized CPU-based video decoder",
        "keyframe-aligned intervals",
        "QuickPrefill",
        "memory-efficient prefilling",
        "KV-cache pruning",
        "overlapping scheme"
      ]
    },
    "publishedAt": "2025-05-21T23:26:50.000Z",
    "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
    "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16175.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62567c86d444a9b5a0ec51c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
      "fullname": "Dongfu Jiang",
      "name": "DongfuJiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17022",
      "authors": [
        {
          "_id": "682ffa9a6e906040a3bb7160",
          "name": "Chengqi Duan",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7161",
          "name": "Rongyao Fang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7162",
          "name": "Yuqing Wang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7163",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7164",
          "name": "Linjiang Huang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7165",
          "name": "Xingyu Zeng",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7166",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7167",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:58.000Z",
      "submittedOnDailyAt": "2025-05-23T03:06:29.578Z",
      "title": "GoT-R1: 시각 생성을 위한 MLLM의 이유론 능력의 해방은 강화 학습을 통해 실현됩니다.",
      "submittedOnDailyBy": {
        "_id": "64a2b496e2e19de17db7de65",
        "avatarUrl": "/avatars/241448ca487833d6cc5d57bb1fdb6ee5.svg",
        "isPro": false,
        "fullname": "Duan Chengqi",
        "user": "gogoduan",
        "type": "user"
      },
      "summary": "ビジュアル生成モデル은 텍스트 プロンプトから현실적인 이미지를 생성하기 위해 놀라운 발전을 거쳤으나, 여러 물체를 특정하고 정밀한 공간관계와 속성을 지정하는 복잡한 プロンプト에 대해서는 어려움을 겪고 있습니다. 이러한 プロンプト의 유효한 처리에는 문학적인 내용과 공간 배치의 명확한 논리가 필요합니다. 우리는 시각적 생성에 대한 논리화를 강화 학습을 사용하여 개선하는 프레임워크를 소개합니다. 이는 Generation Chain-of-Thought 접근 방식을 기반으로, 복잡한 プロンプト에 대한 문학적인 및 공간적인 논리화를 가능하게 하는 것입니다. 이를 달성하기 위해, 우리는 MLLM을 활용하여 논리화의 과정과 최종적인 출력을 평가하는 두 단계 다중 보상 프레임워크를 제안합니다. 이를 통해, 전체 생성 파이프라인에서 유효한 지원이 가능합니다. 보상 시스템은 문학적인 일치성, 공간적 정확성, 시각적 품질을 일괄적으로 평가합니다. 실험 결과를 따르면 T2I-CompBench 벤치마크에서, 특히 정밀한 공간관계와 속성의 결합을 포함하는 구성 태스크에 대해 상당한 향상이 관찰되었습니다. GoT-R1은 복잡한 논리화 능력에 성공하여 시각적 생성 디렉토리로 이동하고, 이미지 생성의 최상위 수준을 도모합니다. 향후 연구를 촉진하기 위해, 우리는 https://github.com/gogoduan/GoT-R1에서 코드와 사전 학습 모델을 공개합니다.",
      "upvotes": 21,
      "discussionId": "682ffa9b6e906040a3bb71ba",
      "githubRepo": "https://github.com/gogoduan/GoT-R1",
      "ai_summary": "GoT-R1 enhances visual generation by using reinforcement learning to improve semantic-spatial reasoning, outperforming existing models on complex compositional tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "Generation Chain-of-Thought",
        "MLLMs",
        "dual-stage multi-dimensional reward framework",
        "semantic alignment",
        "spatial accuracy",
        "visual quality",
        "T2I-CompBench"
      ]
    },
    "publishedAt": "2025-05-22T13:59:58.000Z",
    "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation\n  with Reinforcement Learning",
    "summary": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a2b496e2e19de17db7de65",
      "avatarUrl": "/avatars/241448ca487833d6cc5d57bb1fdb6ee5.svg",
      "fullname": "Duan Chengqi",
      "name": "gogoduan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16933",
      "authors": [
        {
          "_id": "682fe37bb998c9f79463b563",
          "name": "Zebin You",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b564",
          "name": "Shen Nie",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b565",
          "name": "Xiaolu Zhang",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b566",
          "name": "Jun Hu",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b567",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b568",
          "name": "Zhiwu Lu",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b569",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b56a",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:23:26.000Z",
      "submittedOnDailyAt": "2025-05-23T01:26:20.847Z",
      "title": "LLaDA-V: 대규모 언어 확산 모델과 시각 인스탠트 훈련\n\n(Note: The original text \"大规模言語拡散モデルと視覚インストラクションチューニング\" has been translated as \"대규모 언어 확산 모델과 시각 인스탠트 훈련\" to maintain the original meaning and context. The term \"LLaDA-V\" is kept in English as it is a specific acronym and may not have a direct Korean equivalent.)",
      "submittedOnDailyBy": {
        "_id": "624f909eac5dd186b01ac3f5",
        "avatarUrl": "/avatars/0aafdb1cbb492fda52a0303031cc6c14.svg",
        "isPro": false,
        "fullname": "Zebin You",
        "user": "yyyou",
        "type": "user"
      },
      "summary": "이 연구에서는 LLaDA-V, 즉 완전한 분산기반의 다중 모델 대 언어 모델(MLLM)을 소개합니다. 이는 현재의 다중 모델 접근 방식에서 주도하는 자동 회귀 패러다임에서 분리하고, 시각적 인스트럭션 튜닝과 마스크 분산 모델을 통합하여 있습니다. LLaDA(대표적인 대 언어 분산 모델)에 기반하여 구축된 LLaDA-V는 시각적 인코더와 MLP 연결자를 포함하며, 시각적 특징을 언어 임베딩 공간에 투영하여 효과적인 다중 모델 aligment를 가능하게 합니다. 실험적 조사에 따라 다음과 같은 흥미로운 결과를 얻었습니다. 첫째, LLaDA-V는 언어 모델이 단순한 문자 작업에서 LLaMA3-8B나 Qwen2-7B와 비교하여 약하지만, 다중 모델 성능에 대해 바람직한 결과를 나타냅니다. 같은 인스트럭션 데이터로 훈련된 경우, LLaDA-V는 동일한 데이터에서 다중 모델 작업에서 LLaMA3-V와 높은 경쟁력을 가지고 있으며, 데이터 스케일러리도 좋습니다. 또한 Qwen2-VL과의 성능 간격을 좁혀 이 구조의 효율성을 보여주고 있습니다. 둘째, 현재의 조합 자동 회귀 분산과 완전한 분산기반의 MLLM을 비교하여, 다중 모델 이해의 최고 성능을 달성하고 있습니다. 우리의 발견은 대 언어 분산 모델은 다중 모델 컨텍스트에서 바람직한 것을 보여주고, 향후 연구에서 추진되어야 하는 것을 보여줍니다. 프로젝트 페이지와 코드는 아래 URL에 있습니다.",
      "upvotes": 20,
      "discussionId": "682fe37cb998c9f79463b5ae",
      "ai_summary": "A diffusion-based Multimodal Large Language Model (LLaDA-V) with integrated visual instruction tuning performs competitively on multimodal tasks and outperforms existing models in multimodal understanding.",
      "ai_keywords": [
        "diffusion-based",
        "Multimodal Large Language Model (MLLM)",
        "visual instruction tuning",
        "masked diffusion models",
        "autoregressive paradigms",
        "vision encoder",
        "MLP connector",
        "language embedding space",
        "multimodal performance",
        "LLaDA",
        "LLaMA3-8B",
        "Qwen2-7B",
        "LLaMA3-V",
        "Qwen2-VL",
        "multimodal understanding",
        "hybrid autoregressive-diffusion"
      ]
    },
    "publishedAt": "2025-05-22T13:23:26.000Z",
    "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning",
    "summary": "In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large\nLanguage Model (MLLM) that integrates visual instruction tuning with masked\ndiffusion models, representing a departure from the autoregressive paradigms\ndominant in current multimodal approaches. Built upon LLaDA, a representative\nlarge language diffusion model, LLaDA-V incorporates a vision encoder and MLP\nconnector that projects visual features into the language embedding space,\nenabling effective multimodal alignment. Our empirical investigation reveals\nseveral intriguing results: First, LLaDA-V demonstrates promising multimodal\nperformance despite its language model being weaker on purely textual tasks\nthan counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same\ninstruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal\ntasks with better data scalability. It also narrows the performance gap to\nQwen2-VL, suggesting the effectiveness of its architecture for multimodal\ntasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal\nunderstanding compared to existing hybrid autoregressive-diffusion and purely\ndiffusion-based MLLMs. Our findings suggest that large language diffusion\nmodels show promise in multimodal contexts and warrant further investigation in\nfuture research. Project page and codes:\nhttps://ml-gsai.github.io/LLaDA-V-demo/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16933.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "624f909eac5dd186b01ac3f5",
      "avatarUrl": "/avatars/0aafdb1cbb492fda52a0303031cc6c14.svg",
      "fullname": "Zebin You",
      "name": "yyyou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16925",
      "authors": [
        {
          "_id": "68302d01b85e3ed6a61e6476",
          "user": {
            "_id": "67a33f0e36fccbd55d6e8f7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a33f0e36fccbd55d6e8f7f/o_2UU5bxtnPmPdKo-jg7x.png",
            "isPro": false,
            "fullname": "Igor Udovichenko",
            "user": "i-udovichenko",
            "type": "user"
          },
          "name": "Igor Udovichenko",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T08:16:34.263Z",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e6477",
          "name": "Olivier Croissant",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e6478",
          "name": "Anita Toleutaeva",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e6479",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e647a",
          "name": "Alexander Korotin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:18:07.000Z",
      "submittedOnDailyAt": "2025-05-23T06:41:43.892Z",
      "title": "리스크 회피형의 강화 학습에 대한 이타크라-사이토스코프 손실\n\n(注意：原文中的\"イタクラ-サイトスコープ損失\"在韩语中直接翻译为\"이타크라-사이토스코프 손실\"，但根据上下文，可能需要进一步确认术语的准确性。如果\"イタクラ\"和\"サイトスコープ\"有特定的含义，可能需要根据实际情况进行调整。)",
      "submittedOnDailyBy": {
        "_id": "67a33f0e36fccbd55d6e8f7f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a33f0e36fccbd55d6e8f7f/o_2UU5bxtnPmPdKo-jg7x.png",
        "isPro": false,
        "fullname": "Igor Udovichenko",
        "user": "i-udovichenko",
        "type": "user"
      },
      "summary": "리스크 회피형 강화학습은 많은 고위험 영역에서 적용되고 있습니다. 고전적인 강화학습과 달리, 기대값을 최대화하는 것이 아니라, 리스크를 최소화하기 위해 정책 선택을 수행하며, 때로는 期待値를 일시적으로 허용할 수 있습니다. 이러한 경향은 유틸리티 이론에 의해 구성될 수 있습니다. 우리는 지수형 유틸리티 함수의 특별한 경우를 주목하여, Bellman 방정식을 얻을 수 있으며, 최소한의 변경 없이 강화학습 알고리즘을 적용할 수 있습니다. 그러나 이러한 방법들은 전체 과정에서 지수 계산이 필요하여 수치 불안정성을 겪습니다. 이를 해결하기 위해, 상태 가치와 행동 가치 함수의 학습에서 수치적으로 안정적이고 수학적으로 정당한 손실 함수를 Itakura-Saito divergence에 기반하여 도입했습니다. 제안된 손실 함수는 이론적으로와 실험적으로 기존의 대체와 비교하여 평가되었습니다. 실험 섹션에서는 여러 금융 시나리오를 검토했으며, 일부는 분석적 해가 이미 알려진 경우를 포함하여, 제안된 손실 함수가 대체보다 우수함을 보여주었습니다.",
      "upvotes": 16,
      "discussionId": "68302d02b85e3ed6a61e64db",
      "ai_summary": "Proposed Itakura-Saito divergence-based loss function enhances numerical stability in risk-averse reinforcement learning using exponential utility functions.",
      "ai_keywords": [
        "reinforcement learning",
        "risk-averse",
        "utility theory",
        "exponential utility function",
        "Bellman equations",
        "numerical instability",
        "Itakura-Saito divergence",
        "state-value functions",
        "action-value functions"
      ]
    },
    "publishedAt": "2025-05-22T13:18:07.000Z",
    "title": "Risk-Averse Reinforcement Learning with Itakura-Saito Loss",
    "summary": "Risk-averse reinforcement learning finds application in various high-stakes\nfields. Unlike classical reinforcement learning, which aims to maximize\nexpected returns, risk-averse agents choose policies that minimize risk,\noccasionally sacrificing expected value. These preferences can be framed\nthrough utility theory. We focus on the specific case of the exponential\nutility function, where we can derive the Bellman equations and employ various\nreinforcement learning algorithms with few modifications. However, these\nmethods suffer from numerical instability due to the need for exponent\ncomputation throughout the process. To address this, we introduce a numerically\nstable and mathematically sound loss function based on the Itakura-Saito\ndivergence for learning state-value and action-value functions. We evaluate our\nproposed loss function against established alternatives, both theoretically and\nempirically. In the experimental section, we explore multiple financial\nscenarios, some with known analytical solutions, and show that our loss\nfunction outperforms the alternatives.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a33f0e36fccbd55d6e8f7f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a33f0e36fccbd55d6e8f7f/o_2UU5bxtnPmPdKo-jg7x.png",
      "fullname": "Igor Udovichenko",
      "name": "i-udovichenko",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15270",
      "authors": [
        {
          "_id": "682e907a24b2bb08885b94dc",
          "user": {
            "_id": "682e8e6d007cd8c2f2cd0afd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/az0B5USOPlLR3vq872JeN.png",
            "isPro": false,
            "fullname": "Chenyu Zheng",
            "user": "ChenyuZheng",
            "type": "user"
          },
          "name": "Chenyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:10.939Z",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94dd",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94de",
          "name": "Rongzhen Wang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94df",
          "name": "Wei Huang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e0",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e1",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e2",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e3",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T08:49:03.000Z",
      "submittedOnDailyAt": "2025-05-23T00:59:19.168Z",
      "title": "μP를 사용하여 효율적으로 확장하는 Diffusion Transformers",
      "submittedOnDailyBy": {
        "_id": "682e8e6d007cd8c2f2cd0afd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/az0B5USOPlLR3vq872JeN.png",
        "isPro": false,
        "fullname": "Chenyu Zheng",
        "user": "ChenyuZheng",
        "type": "user"
      },
      "summary": "Diffusion Transformers는 시각 생성 모델의 기본으로 등장하지만, 큰 규모의 파라미터 조정의 고가의 비용으로 스케일러리비티가 제한되어 있습니다. 최근, Maximal Update Parametrization (muP)는 베지엣프랑젼마이너스의 경우 제안되었으며, 작은 모델부터 큰 언어 모델까지 안정적인 파라미터 조정을 가능하게 하며, 조정 비용의 대폭 감소를 가능하게 하는 데 사용되었습니다. 그러나 muP의 베지엣프랑젼마이너스 버전이 확산 트랜스포머마이너스에도 적용할 수 있는지는 불확실합니다. 본 논문에서는 muP를 확산 트랜스포머마이너스로 일반화하고, 큰 규모의 실험을 통해 그 효과성을 증명합니다. 우선,主流의 확산 트랜스포머마이너스의 muP (DiT, U-ViT, PixArt-alpha, MMDiT)가 베지엣프랑젼마이너스의 muP와 일치함을 엄밀하게 증명하고, 기존의 muP手法를 직접 적용할 수 있음을 보여줍니다. 이 결과를 활용하여 DiT-muP의 파라미터 조정의 강력한 적용 가능성에 체계적으로 입증합니다. 특히, 학습률을 적용한 DiT-XL-2-muP는 원래의 DiT-XL-2보다 2.9배 빠르도록 수렴하는 것을 증명합니다. 마지막으로, PixArt-alpha와 MMDiT의 텍스트로부터 이미지 생성에 대한 muP의 효과성을 검증합니다. PixArt-alpha는 0.04B에서 0.61B, MMDiT는 0.18B에서 18B로 스케일합니다. 둘 다 muP 모델은 작은 조정 비용으로 기준 모델을 초월합니다. PixArt-alpha의 한 번의 학습 비용의 5.5%만 필요하며, MMDiT-18B의 전문가의 소비의 3%만 필요하며 실현될 수 있습니다. 이러한 결과를 통해 muP는 확산 트랜스포머마이너스의 스케일링에 대한 원리적이고 효율적인 프레임워크로 확립됩니다.",
      "upvotes": 16,
      "discussionId": "682e907b24b2bb08885b952c",
      "projectPage": "https://github.com/ML-GSAI/Scaling-Diffusion-Transformers-muP",
      "githubRepo": "https://github.com/ML-GSAI/Scaling-Diffusion-Transformers-muP",
      "ai_summary": "Maximal Update Parametrization (μP) is extended to diffusion Transformers, demonstrating efficient hyperparameter transferability and reduced tuning costs across various models and tasks.",
      "ai_keywords": [
        "Diffusion Transformers",
        "Maximal Update Parametrization",
        "μP",
        "hyperparameter tuning",
        "DiT",
        "U-ViT",
        "PixArt-α",
        "MMDiT",
        "text-to-image generation",
        "transfer learning",
        "convergence",
        "training run"
      ]
    },
    "publishedAt": "2025-05-21T04:49:03.000Z",
    "title": "Scaling Diffusion Transformers Efficiently via μP",
    "summary": "Diffusion Transformers have emerged as the foundation for vision generative\nmodels, but their scalability is limited by the high cost of hyperparameter\n(HP) tuning at large scales. Recently, Maximal Update Parametrization (muP)\nwas proposed for vanilla Transformers, which enables stable HP transfer from\nsmall to large language models, and dramatically reduces tuning costs. However,\nit remains unclear whether muP of vanilla Transformers extends to diffusion\nTransformers, which differ architecturally and objectively. In this work, we\ngeneralize standard muP to diffusion Transformers and validate its\neffectiveness through large-scale experiments. First, we rigorously prove that\nmuP of mainstream diffusion Transformers, including DiT, U-ViT,\nPixArt-alpha, and MMDiT, aligns with that of the vanilla Transformer,\nenabling the direct application of existing muP methodologies. Leveraging\nthis result, we systematically demonstrate that DiT-muP enjoys robust HP\ntransferability. Notably, DiT-XL-2-muP with transferred learning rate\nachieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we\nvalidate the effectiveness of muP on text-to-image generation by scaling\nPixArt-alpha from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,\nmodels under muP outperform their respective baselines while requiring small\ntuning cost, only 5.5% of one training run for PixArt-alpha and 3% of\nconsumption by human experts for MMDiT-18B. These results establish muP as a\nprincipled and efficient framework for scaling diffusion Transformers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "682e8e6d007cd8c2f2cd0afd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/az0B5USOPlLR3vq872JeN.png",
      "fullname": "Chenyu Zheng",
      "name": "ChenyuZheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14604",
      "authors": [
        {
          "_id": "682f34b52b9fdc24ae9de371",
          "name": "Haoran Zhao",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de372",
          "name": "Yuchen Yan",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de373",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de374",
          "name": "Haolei Xu",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de375",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de376",
          "name": "Kaitao Song",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de377",
          "name": "Jian Shao",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de378",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de379",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de37a",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T16:53:40.000Z",
      "submittedOnDailyAt": "2025-05-23T03:36:02.584Z",
      "title": "LLMs는 자기 자신의 한계로 인해 과도한 생각들을 억압하여 자유롭게 동작할 수 있도록 제안합니다.",
      "submittedOnDailyBy": {
        "_id": "5e1058e9fcf41d740b69966d",
        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
        "isPro": false,
        "fullname": "Yongliang Shen",
        "user": "tricktreat",
        "type": "user"
      },
      "summary": "대논리 모형(LRMs)의 예로 OpenAI o1과 DeepSeek-R1な릅니다. 이들은 생각의 길이를 늘려 다양한 태스크에서 최상위 평가에 성공하고, 이유 능력을 크게 향상시켰습니다. 그러나 이러한 성능의 향상은 생성 프로세스 중冗長한 이유의 크게 증가와 고의 계산 오버헤드 및 과도한 추측 문제에 따라 악화되고 있습니다. 현재 다수의 접근 방식은 과도한 추측 문제를 해결하기 위해 외부의 간섭에 의존하고 있습니다. 그러나 이러한 접근 방식은 일반적으로 외부의 제어 기구에 의존합니다. 본 논문에서는 모델이 자체적으로 이유 프로세스를 조절할 수 있는 새로운 프레임워크를 제안하고 있습니다. 이는 외부의 제어 기구에 의존하지 않고, 과도한 추측 문제를 해결하기 위해 모델이 자체적으로 이유 프로세스를 조절할 수 있도록 합니다. 표준의 답에 기반한 과도한 추측을 식별하는 측정 기준을 사용하여,冗長한 이유를 식별하는 체계적인 방법을 설계하고, 이유의 추적 내의 불필요한 단계를 정확하게 식별하고 학습을 위한 훈련 신호를 생성합니다. 이러한 기초에 의해, 적응적인 이유의 길이를 가진 데이터의 구축의 완전한 전략을 개발하고, 모델이 적절한 시점에서 이유를 종료하는 시간을 자연스럽게 학습하도록 혁신적인 브레이크 프로ン트 구조를 도입합니다. 수학 벤치마크(AIME, AMC, MATH500, GSM8K)의 실험은, 이 방법은 약 60% 이상의 토큰 소비를 줄일 수 있으며, 제약없이 모델과 비교적 정확도를 유지하는 것을 보여줍니다.",
      "upvotes": 16,
      "discussionId": "682f34b62b9fdc24ae9de3be",
      "ai_summary": "A novel Self-Braking Tuning framework reduces overthinking and unnecessary computational overhead in large reasoning models by enabling the model to self-regulate its reasoning process.",
      "ai_keywords": [
        "large reasoning models",
        "self-braking tuning",
        "overthinking",
        "reasoning capabilities",
        "mathematical benchmarks",
        "adaptive reasoning lengths",
        "braking prompt mechanism"
      ]
    },
    "publishedAt": "2025-05-20T12:53:40.000Z",
    "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning",
    "summary": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14604.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e1058e9fcf41d740b69966d",
      "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
      "fullname": "Yongliang Shen",
      "name": "tricktreat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16916",
      "authors": [
        {
          "_id": "682fdcfc2c98b5e99660561e",
          "name": "Xuankun Rong",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e99660561f",
          "name": "Wenke Huang",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605620",
          "name": "Jian Liang",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605621",
          "name": "Jinhe Bi",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605622",
          "name": "Xun Xiao",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605623",
          "name": "Yiming Li",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605624",
          "name": "Bo Du",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605625",
          "name": "Mang Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:11:58.000Z",
      "submittedOnDailyAt": "2025-05-23T00:58:25.177Z",
      "title": "외부 가이드라인 없이 필요한 멀티 모델 피치팅 후처리",
      "submittedOnDailyBy": {
        "_id": "66c014820836dd7a55be3fde",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OuqAxEFq1Ny2CHbl9HOm.jpeg",
        "isPro": false,
        "fullname": "Xuankun Rong",
        "user": "XuankunRong",
        "type": "user"
      },
      "summary": "다모달 대언어 모댈(MLLMs)는 사용자가 제공하는 데이터 세트를 사용하여 일반적인 모델을 하류 태스크에 적용하는 FINCH(Fine-Tuning as a Service) 설정에서 일일히 적용되고 있습니다. 이 유연성은 악의적인 FINCH가 MLLM에 백드로를 삽입하기 위해 최소한의 노력을 요구하는 데 의해 엄격한 보안 위험을 불러일으키는 데 기여합니다. 본 논문에서는 백드로 Trigger가 시스템적으로 비어미의 영역에 대한 비정상적인 Attention 집중을 일으키고, 크로스 모달 처리를 파괴하는 것을 관찰했습니다. 이 관찰에 기반하여, 우리는 Attention 엔트로피 패턴을 자동 얕은 신호로 활용하여 백드로 샘플을 식별하고 필터링하는 데이터 필터링 프레임워크 \"Believe Your Eyes (BYE)\"를 제안합니다. BYE는 다음 3 단계의 파이프라인을 통해 작동합니다: 1) FINCH 모델을 사용하여 Attention Map을 추출, 2) 바이 모우드 분리로 엔트로피 스코어를 계산하고 과민층을 프로파일링, 3) 비 서브주디션 클러스터링을 수행하고 의심 샘플을 제거합니다. 기존의 방어와 달리, BYE는 청결 서브주디션, 보조 라벨, 또는 모델의 수정이 필요하지 않습니다. 다양한 데이터 세트, 모델, 드레프트 종류의 다양한 실험에서 BYE의 효과성을 검증했습니다: 그것은 청결 태스크 성능을 유지하는 동시에 근사 0의 공격 성공률을 달성하고, MLLM에 대한 백드로 리스크에 대한 강력한 일반화 가능한 해결책을 제공합니다.",
      "upvotes": 14,
      "discussionId": "682fdcfd2c98b5e99660567b",
      "ai_summary": "A novel defense framework, Believe Your Eyes (BYE), identifies and filters backdoor samples in fine-tuned multimodal large language models by analyzing attention entropy patterns, preventing trigger activation without requiring additional labels or model changes.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "fine-tuning-as-a-service",
        "FTaaS",
        "backdoors",
        "attention collapse",
        "attention entropy",
        "self-supervised",
        "bimodal separation",
        "unsupervised clustering",
        "clean-task performance"
      ]
    },
    "publishedAt": "2025-05-22T13:11:58.000Z",
    "title": "Backdoor Cleaning without External Guidance in MLLM Fine-tuning",
    "summary": "Multimodal Large Language Models (MLLMs) are increasingly deployed in\nfine-tuning-as-a-service (FTaaS) settings, where user-submitted datasets adapt\ngeneral-purpose models to downstream tasks. This flexibility, however,\nintroduces serious security risks, as malicious fine-tuning can implant\nbackdoors into MLLMs with minimal effort. In this paper, we observe that\nbackdoor triggers systematically disrupt cross-modal processing by causing\nabnormal attention concentration on non-semantic regions--a phenomenon we term\nattention collapse. Based on this insight, we propose Believe Your Eyes (BYE),\na data filtering framework that leverages attention entropy patterns as\nself-supervised signals to identify and filter backdoor samples. BYE operates\nvia a three-stage pipeline: (1) extracting attention maps using the fine-tuned\nmodel, (2) computing entropy scores and profiling sensitive layers via bimodal\nseparation, and (3) performing unsupervised clustering to remove suspicious\nsamples. Unlike prior defenses, BYE equires no clean supervision, auxiliary\nlabels, or model modifications. Extensive experiments across various datasets,\nmodels, and diverse trigger types validate BYE's effectiveness: it achieves\nnear-zero attack success rates while maintaining clean-task performance,\noffering a robust and generalizable solution against backdoor threats in MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16916.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c014820836dd7a55be3fde",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OuqAxEFq1Ny2CHbl9HOm.jpeg",
      "fullname": "Xuankun Rong",
      "name": "XuankunRong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14684",
      "authors": [
        {
          "_id": "682f474c9ee0bb0cc953b885",
          "name": "Haolei Xu",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b886",
          "name": "Yuchen Yan",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b887",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b888",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b889",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88a",
          "name": "Shengpei Jiang",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88b",
          "name": "Kaitao Song",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88c",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88d",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88e",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:59:31.000Z",
      "submittedOnDailyAt": "2025-05-23T03:43:43.765Z",
      "title": "간격을 닫아서: 생각의 점프를 연결하여 Chain-of-Thought Tuning를 향상시키세요.",
      "submittedOnDailyBy": {
        "_id": "64098738342c26884c792c93",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
        "isPro": false,
        "fullname": "Yuchen Yan",
        "user": "yanyc",
        "type": "user"
      },
      "summary": "대 언어 모형(LLMs)는 Chain-of-Thought(CoT) 논리로 수학 태스크에서 놀라운 진전을 달성했습니다. 그러나 현재의 수학 CoT 데이터 세트는 전문가가 중간 단계를 생략하여 Thought Leaps로 싸워가고 있습니다. 이는 모델의 학습과 일반화에 부정적인 영향을 미칩니다. 우리는 CoT Thought Leap Bridge Task를 제안하고, 자동적으로 Leaps를 감지하고 부족한 중간 논리 단계를 생성하여 CoT의 완전성과 일관성을 회복하는 것을 목표로 합니다. 이를 촉진하기 위해 구조화된 ScaleQuestMath 데이터 세트를 기반으로 특별한 훈련 데이터 세트를 구축하고 CoT-Bridge를 훈련했습니다. 수학 논리 벤치마크에서 상세한 실험을 통해, 조정된 데이터 세트로 미세 조정된 모델은 원래 데이터 세트로 훈련된 모델을 초월하며, NuminaMath에서 +5.87%의 개선을 나타냅니다. 우리의 접근 방식은 결합된 데이터의 최적화(+3.02%)를 효과적으로 강화하고, 강화 학습의 더 좋은 시작점을 제공하며, 기존의 최적화 방법과 함께 플러그인 및 플레이인 모듈로 작동합니다. 또한 CoT-Bridge는 외란 영역의 논리 논리 태스크의 일반화에 개선하고, 이유론의 완전성을 강화하는 데 광범위한 이익을 제공함을 확인했습니다.",
      "upvotes": 14,
      "discussionId": "682f474d9ee0bb0cc953b8c7",
      "projectPage": "https://zju-real.github.io/CoT-Bridge/",
      "githubRepo": "https://github.com/ZJU-REAL/Mind-the-Gap",
      "ai_summary": "A model for detecting and generating missing intermediate steps in mathematical Chain-of-Thought reasoning improves performance and generalization on mathematical and logical reasoning tasks.",
      "ai_keywords": [
        "Chain-of-Thought",
        "reasoning",
        "thought leaps",
        "ScaleQM+",
        "ScaleQuestMath",
        "NuminaMath",
        "distilled data",
        "reinforcement learning",
        "generalization",
        "logical reasoning tasks"
      ]
    },
    "publishedAt": "2025-05-20T13:59:31.000Z",
    "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
    "summary": "Large language models (LLMs) have achieved remarkable progress on\nmathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14684.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64098738342c26884c792c93",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
      "fullname": "Yuchen Yan",
      "name": "yanyc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16990",
      "authors": [
        {
          "_id": "682fdd034640a9db4d1cc04d",
          "name": "Runpeng Yu",
          "hidden": false
        },
        {
          "_id": "682fdd034640a9db4d1cc04e",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "682fdd034640a9db4d1cc04f",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:55:04.000Z",
      "submittedOnDailyAt": "2025-05-23T01:04:28.755Z",
      "title": "DINPL: 분산 디피러션 다모렐 모델 대 언어 모델 병렬 확인",
      "submittedOnDailyBy": {
        "_id": "635364b3c41f548fe39db945",
        "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
        "isPro": false,
        "fullname": "Runpeng Yu",
        "user": "rp-yu",
        "type": "user"
      },
      "summary": "이 연구에서, 최초의 분산 디피션 다모뎅 언어 모델(DMLLM)인 Dimple를 제안합니다. 분산 디피션의 단일 접근 방식의 훈련은 큰 훈련 불안정, 최적의 성능, 길이 바이어스 문제로 인해 엄격한 문제를 동반합니다. 이러한 문제를 대처하기 위해, 초기의 자동 회귀 단계와 후속의 디피션 단계를 조합한 새로운 훈련 패러다임을 설계했습니다. 이 접근 방식에 따라 Dimple-7B 모델을 구축하고, LLaVA-NEXT와 같은 데이터 세트와 유사한 훈련 파이프라인을 사용하여 훈련되었습니다. Dimple-7B는 LLaVA-NEXT의 성능을 3.9% 초과로, DMLLM이 자동 회귀 모델과 동일한 성능을 달성할 수 있음을 보여주었습니다. 추론 효율화를 촉진하기 위해, 자신감 디코딩이라는 이름의 디코딩 스테레타지를 제안하고, 각 단계에서 생성되는 토큰 수를 동적으로 조정하여 생성 반복 수를 크게 줄입니다. 자동 회귀 모델에서, 생성 시의 양방향 반복 수는 응답의 길이와 같지만, 자신감 디코딩에서 Dimple의 필요한 반복 수는 응답 길이의 3배를 줄일 수 있습니다. 또한, 자동 회귀 모델의 예측 기법을 재구현하고, 버전 평가의 많은 테스트에서 성능에 큰 영향을 미치지 않도록 하였으며, 1.5배에서 7배의 속도 향상을 제공했습니다. 또한, Dimple의 응답의 정밀한 제어를 가능하게 하는 구조 프로파일을 사용하며, 이 프로파일은 명령 기반이나 체인 오프쇼 프로닝과 다른 구조화된 응답을 생성할 수 있으며, 응답의 형식과 길이의 미세한 제어를 가능하게 합니다. 전체적으로, 이 연구는 DMLLM의 가능성과 우수한 점을 증명하고, 추론 효율화와 제어 가능성의 향상을 달성했습니다. 코드와 모델은 https://github.com/yu-rp/Dimple에서 사용 가능합니다.",
      "upvotes": 12,
      "discussionId": "682fdd044640a9db4d1cc0a1",
      "githubRepo": "https://github.com/yu-rp/Dimple",
      "ai_summary": "Dimple, a Discrete Diffusion Multimodal Large Language Model, achieves performance comparable to autoregressive models through a hybrid training approach and enhances inference efficiency with confident decoding and structure priors.",
      "ai_keywords": [
        "Discrete Diffusion Multimodal Large Language Model",
        "DMLLM",
        "autoregressive phase",
        "diffusion phase",
        "confident decoding",
        "prefilling technique",
        "structure priors"
      ]
    },
    "publishedAt": "2025-05-22T13:55:04.000Z",
    "title": "Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel\n  Decoding",
    "summary": "In this work, we propose Dimple, the first Discrete Diffusion Multimodal\nLarge Language Model (DMLLM). We observe that training with a purely discrete\ndiffusion approach leads to significant training instability, suboptimal\nperformance, and severe length bias issues. To address these challenges, we\ndesign a novel training paradigm that combines an initial autoregressive phase\nwith a subsequent diffusion phase. This approach yields the Dimple-7B model,\ntrained on the same dataset and using a similar training pipeline as\nLLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%,\ndemonstrating that DMLLM can achieve performance comparable to that of\nautoregressive models. To improve inference efficiency, we propose a decoding\nstrategy termed confident decoding, which dynamically adjusts the number of\ntokens generated at each step, significantly reducing the number of generation\niterations. In autoregressive models, the number of forward iterations during\ngeneration equals the response length. With confident decoding, however, the\nnumber of iterations needed by Dimple is even only text{response\nlength}{3}. We also re-implement the prefilling technique in autoregressive\nmodels and demonstrate that it does not significantly impact performance on\nmost benchmark evaluations, while offering a speedup of 1.5x to 7x.\nAdditionally, we explore Dimple's capability to precisely control its response\nusing structure priors. These priors enable structured responses in a manner\ndistinct from instruction-based or chain-of-thought prompting, and allow\nfine-grained control over response format and length, which is difficult to\nachieve in autoregressive models. Overall, this work validates the feasibility\nand advantages of DMLLM and enhances its inference efficiency and\ncontrollability. Code and models are available at\nhttps://github.com/yu-rp/Dimple.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635364b3c41f548fe39db945",
      "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
      "fullname": "Runpeng Yu",
      "name": "rp-yu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16864",
      "authors": [
        {
          "_id": "682fe14abafb480b9595da32",
          "name": "Yuechen Zhang",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da33",
          "name": "Jinbo Xing",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da34",
          "name": "Bin Xia",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da35",
          "name": "Shaoteng Liu",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da36",
          "name": "Bohao Peng",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da37",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da38",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da39",
          "name": "Eric Lo",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da3a",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T16:21:32.000Z",
      "submittedOnDailyAt": "2025-05-23T01:22:45.264Z",
      "title": "무트레이닝의 효율적인 비디오 생성 방법: 동적 토큰 분리 방법",
      "submittedOnDailyBy": {
        "_id": "6418554a0956be7233a1023e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
        "isPro": false,
        "fullname": "zhang yuechen",
        "user": "julianjuaner",
        "type": "user"
      },
      "summary": "비디오 디퓨전 트랜스포머(DiT) 모델의 놀라운 생성 품질과 마찬가지로, 실용적인 처리에 따른 구현은 엄격한 계산 요구에 의해 엄격하게 제한되어 있습니다. 이러한 불합리성은 두 가지 주요 문제에 기반합니다: 토큰 길이에 대한 자동 注意의 2차원 복잡성과, 디퓨전 모델의 다단계적인 특성입니다. 이러한 제한을 해결하기 위해, 독특한 추론 파이프라인 「제네기」를 제안합니다. 이 파이프라인은 동적 注意 분리와 진화적인 해상도 생성을 조합합니다. 우리의 접근 방식은 두 가지 주요 힌트를 기반합니다: (1) 초기의 디노이즈 단계에서 고해상도 잠재 변수는 필요하지 않습니다, (2) 후속 단계에서는 밀집한 注意가 필요하지 않습니다. 제네기는 3D 공간 몰입 곡선을 사용하여 연관된 토큰 상호작용을 동적으로 선택하는 블록별로의 注意 구조를 도입하고, 생성 중에 잠재 해상도를 진화적으로 증가시킵니다. 실험 결과를 통해, 제네기는 많은 최신 비디오 디퓨전 모델에서 상당한 속도 향상을 실현하고, 상대적인 생성 품질을 유지합니다(VBench에서 8.83배의 속도 향상과 0.01%의 성능 떨어짐). 푸드와 빵처럼 해결책으로, 제네기는 모델 재학습이 필요하지 않는 모델의 추론 시간을 쉽게 이해할 수 있는 초에 억제할 수 있으며, 현대 하드웨어에서 실용적인 고품질 비디오 생성을 가능하게 합니다. 코드: https://github.com/dvlab-research/Jenga",
      "upvotes": 11,
      "discussionId": "682fe14ebafb480b9595db1c",
      "projectPage": "https://julianjuaner.github.io/projects/jenga/",
      "githubRepo": "https://github.com/dvlab-research/Jenga",
      "ai_summary": "Jenga, a novel inference pipeline for video Diffusion Transformer models, combines dynamic attention carving and progressive resolution generation to significantly speed up video generation while maintaining high quality.",
      "ai_keywords": [
        "video Diffusion Transformer (DiT)",
        "self-attention",
        "diffusion models",
        "dynamic attention carving",
        "progressive resolution generation",
        "block-wise attention",
        "3D space-filling curves"
      ]
    },
    "publishedAt": "2025-05-22T12:21:32.000Z",
    "title": "Training-Free Efficient Video Generation via Dynamic Token Carving",
    "summary": "Despite the remarkable generation quality of video Diffusion Transformer\n(DiT) models, their practical deployment is severely hindered by extensive\ncomputational requirements. This inefficiency stems from two key challenges:\nthe quadratic complexity of self-attention with respect to token length and the\nmulti-step nature of diffusion models. To address these limitations, we present\nJenga, a novel inference pipeline that combines dynamic attention carving with\nprogressive resolution generation. Our approach leverages two key insights: (1)\nearly denoising steps do not require high-resolution latents, and (2) later\nsteps do not require dense attention. Jenga introduces a block-wise attention\nmechanism that dynamically selects relevant token interactions using 3D\nspace-filling curves, alongside a progressive resolution strategy that\ngradually increases latent resolution during generation. Experimental results\ndemonstrate that Jenga achieves substantial speedups across multiple\nstate-of-the-art video diffusion models while maintaining comparable generation\nquality (8.83times speedup with 0.01\\% performance drop on VBench). As a\nplug-and-play solution, Jenga enables practical, high-quality video generation\non modern hardware by reducing inference time from minutes to seconds --\nwithout requiring model retraining. Code:\nhttps://github.com/dvlab-research/Jenga",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6418554a0956be7233a1023e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
      "fullname": "zhang yuechen",
      "name": "julianjuaner",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16181",
      "authors": [
        {
          "_id": "682fddbd2b4a4d1ce53c5afb",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Reza Taesiri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T03:18:33.876Z",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5afc",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Brandon Collins",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T02:32:10.307Z",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5afd",
          "user": {
            "_id": "668c8e8c142f9b26a49f03cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668c8e8c142f9b26a49f03cc/YNmPCrlsi6iwSeNfh1iID.png",
            "isPro": false,
            "fullname": "Logan Bolton",
            "user": "loganbolton",
            "type": "user"
          },
          "name": "Logan Bolton",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T02:33:14.731Z",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5afe",
          "name": "Viet Dac Lai",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5aff",
          "name": "Franck Dernoncourt",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5b00",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5b01",
          "name": "Anh Totti Nguyen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:35:15.000Z",
      "submittedOnDailyAt": "2025-05-23T01:00:45.248Z",
      "title": "일상 이미지 편집 작업에서 생성형 AI의 능력을 이해하는 방법\n\n(注意：虽然要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，这里提供了一个更加详细的翻译版本，以适应韩国读者的阅读习惯。)",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "생성된 AI（GenAI）는 일상적인 이미지 편집 작업의 자동화에 큰 잠재력을 발휘하고 있으며, 특히 2025년 3월 25일 GPT-4o가 공개된 이후 더욱 강조되어 왔다. 그러나 가장 자주 원하는 편집 主题은 무엇일까? 그들은 어떤 종류의 편집 작업(예: 主题을 제거하거나 스타일화)을 원할까? 사람들은 정확한, 예측 가능한 편집을 선호하는 것이나, 고차원의 창의적인 편집을 선호하는 것이 있을까? 현실 세계의 요청의 특성을 이해하고, 자유 직업 사진 편집 고手指가 수행한 대응적인 편집을 통해,我们是否 AI 기반의 편집기를 개선할 수 있는 교훈을 제공하고, 현재 어떤 종류의 요청이 AI 편집기에 성공적으로 처리될 수 있는지 결정할 수 있을까? 本文는 지난 12년(2013-2025년) 동안 Reddit 커뮤니티에서 수집된 83k 요청과 305k PSR-wizard 편집을 분석하여 이러한 문제를 독특하게 연구하였다. 인간 평가에 따르면, 가장 우수한 AI 편집기(GPT-4o, Gemini-2.0-Flash, SeedEdit 포함)이 처리할 수 있는 요청은 약 33% 정도이다. 흥미롭게도, AI 편집기는 낮은 창의적인 요청에 대한 정확한 편집이 필요할 때보다 개방형 태스크에서 더 잘 수행된다. 그들은 인물과 동물의 정체성을 유지하기 어려워서, 자주 요청하지 않은 수정을 수행한다. 반면, VLM 판사(예: o1)는 인간 판사과는 다르게, AI 편집에 더 선호하는 경향이 있어 보인다. 코드와 질적 예제는 https://psrdataset.github.io에서 찾을 수 있다.",
      "upvotes": 11,
      "discussionId": "682fddc32b4a4d1ce53c5c60",
      "projectPage": "https://psrdataset.github.io",
      "ai_summary": "Analysis of 83k image editing requests reveals that AI editors, including GPT-4o, struggle with low-creativity tasks and precise editing, while performing better on open-ended tasks, and human and VLM judges differ in their preferences for AI versus human edits.",
      "ai_keywords": [
        "GPT-4o",
        "Gemini-2.0-Flash",
        "SeedEdit",
        "VLM judges"
      ]
    },
    "publishedAt": "2025-05-21T23:35:15.000Z",
    "title": "Understanding Generative AI Capabilities in Everyday Image Editing Tasks",
    "summary": "Generative AI (GenAI) holds significant promise for automating everyday image\nediting tasks, especially following the recent release of GPT-4o on March 25,\n2025. However, what subjects do people most often want edited? What kinds of\nediting actions do they want to perform (e.g., removing or stylizing the\nsubject)? Do people prefer precise edits with predictable outcomes or highly\ncreative ones? By understanding the characteristics of real-world requests and\nthe corresponding edits made by freelance photo-editing wizards, can we draw\nlessons for improving AI-based editors and determine which types of requests\ncan currently be handled successfully by AI editors? In this paper, we present\na unique study addressing these questions by analyzing 83k requests from the\npast 12 years (2013-2025) on the Reddit community, which collected 305k\nPSR-wizard edits. According to human ratings, approximately only 33% of\nrequests can be fulfilled by the best AI editors (including GPT-4o,\nGemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on\nlow-creativity requests that require precise editing than on more open-ended\ntasks. They often struggle to preserve the identity of people and animals, and\nfrequently make non-requested touch-ups. On the other side of the table, VLM\njudges (e.g., o1) perform differently from human judges and may prefer AI edits\nmore than human edits. Code and qualitative examples are available at:\nhttps://psrdataset.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16181.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15952",
      "authors": [
        {
          "_id": "682fe833f39f561d1d8cd5d1",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Reza Taesiri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T03:18:36.072Z",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d2",
          "name": "Abhijay Ghildyal",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d3",
          "name": "Saman Zadtootaghaj",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d4",
          "name": "Nabajeet Barman",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d5",
          "user": {
            "_id": "644feede17b6189cda58575d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WVo1Ah7xmHEeBOUQpkYgS.png",
            "isPro": false,
            "fullname": "Cor-Paul",
            "user": "corpaul",
            "type": "user"
          },
          "name": "Cor-Paul Bezemer",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:15:06.040Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T19:08:38.000Z",
      "submittedOnDailyAt": "2025-05-23T01:45:28.315Z",
      "title": "VideoGameQA-Bench: 게임의 시각 언어 모델의 품질 보장 평가",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "현재 비디오 게임은 엔터테인먼트 산업에서 가장 높은 수입을 발생시키고 있으며, 게임 개발 프로세스의 최적화는 산업의 지속적인 성장에 매우 중요합니다. 최근의 비전 언어 모델(VLMs)의 발전은 게임 개발의 다양한 영역에서 자동화 및 강화가 가능한 것을 보여주고 있으며, 특히 게임의 품질 보장(QA) 분야에서, 현재의 자동화에 대한 기회가 제한되어 있는 것처럼, 산업에서 가장 노동력 풍부한 프로세스 중 하나입니다. VLMs가 게임 QA 태스크의 성능을 정확하게 평가하고, 현실적인 시나리오를 처리하는 효율성을 평가하기 위해, 표준화된 벤치마크가 필요합니다. 현재의 벤치마크는 이 분야의 특정 요구에 대응하지 못하기 때문에, 이를 채워 VideoGameQA-Bench라는 게임 QA 활동의 광범위한 범위를 커버하는 상세한 벤치마크를 소개합니다. 이 벤치마크는 시각적 단위 테스트, 시각적인 리셋 테스트, 하이스택의 노드 태스크, 깜빡 감지, 각 게임의 이미지 및 영상의 버그 보고 등 다양한 게임 QA 활동들을 포함합니다. 코드와 데이터는 다음 URL에서 사용 가능합니다: https://asgaardlab.github.io/videogameqa-bench/",
      "upvotes": 11,
      "discussionId": "682fe83af39f561d1d8cd7e5",
      "projectPage": "https://asgaardlab.github.io/videogameqa-bench/",
      "ai_summary": "A benchmark called VideoGameQA-Bench is introduced to assess Vision-Language Models in video game quality assurance tasks.",
      "ai_keywords": [
        "Vision-Language Models",
        "VideoGameQA-Bench",
        "visual unit testing",
        "visual regression testing",
        "needle-in-a-haystack tasks",
        "glitch detection",
        "bug report generation"
      ]
    },
    "publishedAt": "2025-05-21T15:08:38.000Z",
    "title": "VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game\n  Quality Assurance",
    "summary": "With video games now generating the highest revenues in the entertainment\nindustry, optimizing game development workflows has become essential for the\nsector's sustained growth. Recent advancements in Vision-Language Models (VLMs)\noffer considerable potential to automate and enhance various aspects of game\ndevelopment, particularly Quality Assurance (QA), which remains one of the\nindustry's most labor-intensive processes with limited automation options. To\naccurately evaluate the performance of VLMs in video game QA tasks and\ndetermine their effectiveness in handling real-world scenarios, there is a\nclear need for standardized benchmarks, as existing benchmarks are insufficient\nto address the specific requirements of this domain. To bridge this gap, we\nintroduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array\nof game QA activities, including visual unit testing, visual regression\ntesting, needle-in-a-haystack tasks, glitch detection, and bug report\ngeneration for both images and videos of various games. Code and data are\navailable at: https://asgaardlab.github.io/videogameqa-bench/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 83
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17018",
      "authors": [
        {
          "_id": "682fe2b865bac3ec3556c016",
          "name": "Kaixuan Fan",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c017",
          "name": "Kaituo Feng",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c018",
          "name": "Haoming Lyu",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c019",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c01a",
          "name": "Xiangyu Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:53.000Z",
      "submittedOnDailyAt": "2025-05-23T01:28:23.338Z",
      "title": "SophiaVL-R1: 텍스트 이해 모델의 계산 능력을 강화하기 위한 보상의 고려 사항",
      "submittedOnDailyBy": {
        "_id": "67079840a9bcb7459b8d2a46",
        "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
        "isPro": false,
        "fullname": "Kaituo Feng",
        "user": "KaituoFeng",
        "type": "user"
      },
      "summary": "최근의 진보는 규칙 기반의 강화 학습(RL)을 통해 다모뎀 대 언어 모델(MLLMs)에서 강력한 논리론 능력을 발휘하는 성공을 보여주고 있습니다. 그러나 이 패러다임은 최종적인 결과를 도출하는 사고 과정의 관찰에서 부족하여 모델이 최적의 논리론 전략을 학습할 수 있는 가능성이 있습니다. 이러한 점을 보완하기 위해 우리는 SophiaVL-R1을 제안하고, 이 패러다임에서 사고 과정에 보상 신호를 추가하는 시도를 합니다. 이를 달성하기 위해 먼저 사고 보상 모델을 훈련시키고 전체 모델의 사고 과정의 질을 평가하는 것입니다. 사고 보상은 특정 샘플에 대한 신뢰도가 낮은 가능성이 있으므로 우리는 Trust-GRPO 메소드를 제안하고 훈련 중에 사고 보상에 신뢰도 가중치를 할당하는 것입니다. 이 가중치는 정답의 답과 틀린 답을 도출하는 사고 보상의 비교에 기반하여 계산되어 잠재적으로 불신뢰한 사고 보상의 영향을 완화하는 데 도움을 줍니다. 또한 우리는 시간이 지남에 따라 사고 보상을 점차 줄이는 완화 훈련 전략을 설계하고 모델이 후반의 훈련 단계에서 정확한 규칙 기반의 결과 보상에 의해 많은 의존하도록 합니다. 실험 결과는 우리의 SophiaVL-R1은 MathVisita, MMMU 등 다양한 벤치마크에서 논리론 모델의 시리즈를 초월하고 강력한 논리론과 일반화 능력을 보여주고 있습니다. 특히 우리의 SophiaVL-R1-7B은 후者是 10배의 파라미터를 가진 LLaVA-OneVision-72B를 초월하고 있습니다. 모든 코드, 모델, 데이터 세트는 https://github.com/kxfan2002/SophiaVL-R1에 공개되어 있습니다.",
      "upvotes": 10,
      "discussionId": "682fe2b965bac3ec3556c066",
      "projectPage": "https://github.com/kxfan2002/SophiaVL-R1",
      "githubRepo": "https://github.com/kxfan2002/SophiaVL-R1",
      "ai_summary": "An enhanced multimodal language model incorporates thinking process rewards to improve reasoning and generalization, achieving superior performance on benchmarks compared to larger models.",
      "ai_keywords": [
        "multimodal large language models",
        "rule-based reinforcement learning",
        "reward signals",
        "thinking reward model",
        "Trust-GRPO method",
        "thinking reward comparison",
        "annealing training strategy",
        "reasoning MLLMs",
        "MathVisita",
        "MMMU"
      ]
    },
    "publishedAt": "2025-05-22T13:59:53.000Z",
    "title": "SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward",
    "summary": "Recent advances have shown success in eliciting strong reasoning abilities in\nmultimodal large language models (MLLMs) through rule-based reinforcement\nlearning (RL) with outcome rewards. However, this paradigm typically lacks\nsupervision over the thinking process leading to the final outcome.As a result,\nthe model may learn sub-optimal reasoning strategies, which can hinder its\ngeneralization ability. In light of this, we propose SophiaVL-R1, as an attempt\nto add reward signals for the thinking process in this paradigm. To achieve\nthis, we first train a thinking reward model that evaluates the quality of the\nentire thinking process. Given that the thinking reward may be unreliable for\ncertain samples due to reward hacking, we propose the Trust-GRPO method, which\nassigns a trustworthiness weight to the thinking reward during training. This\nweight is computed based on the thinking reward comparison of responses leading\nto correct answers versus incorrect answers, helping to mitigate the impact of\npotentially unreliable thinking rewards. Moreover, we design an annealing\ntraining strategy that gradually reduces the thinking reward over time,\nallowing the model to rely more on the accurate rule-based outcome reward in\nlater training stages. Experiments show that our SophiaVL-R1 surpasses a series\nof reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU),\ndemonstrating strong reasoning and generalization capabilities. Notably, our\nSophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite\nthe latter having 10 times more parameters. All code, models, and datasets are\nmade publicly available at https://github.com/kxfan2002/SophiaVL-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17018.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67079840a9bcb7459b8d2a46",
      "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
      "fullname": "Kaituo Feng",
      "name": "KaituoFeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17012",
      "authors": [
        {
          "_id": "682fde942f8f73559fcbc5da",
          "name": "Haoning Wu",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5db",
          "name": "Xiao Huang",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5dc",
          "name": "Yaohui Chen",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5dd",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5de",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5df",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632c7a0d1d303f5f9acf01b8/3GQcp-q-BBj5bX_DRmB6S.jpeg"
      ],
      "publishedAt": "2025-05-22T17:59:03.000Z",
      "submittedOnDailyAt": "2025-05-23T01:12:41.090Z",
      "title": "스펙트럼 스코어: 다모달 스펙트럼 이해의 통일된 평가로 가기 위한 방법\n\n(注意: \"스펙트럼 스코어\"는 \"Spectral Score\"의 번역이며, \"스펙트럼\"은 \"Spectrum\"의 번역입니다.)",
      "submittedOnDailyBy": {
        "_id": "632c7a0d1d303f5f9acf01b8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
        "isPro": false,
        "fullname": "Haoning Wu",
        "user": "haoningwu",
        "type": "user"
      },
      "summary": "다모달 대언어 모델(MLLMs)는 질문응답 태스크에서 인상적인 성공을 거두었지만, 공간 이해 능력을 조사하는 것은 거의 이루어지지 않았다. 본 논문은 주요한 문제를 연구하였다: 현재의 MLLMs가 3D 공간 인식과 이해 능력을 가지고 있는지를 묻는 것이다. 구체적으로, 본 논문은 다음과 같은 기여를 하였다: (i) 우리는 VGBench를 도입하였는데, 이는 MLLMs의 시각 기하학적 인식을 평가하기 위해 특별히 설계된 기준이며, 카메라 자세와 운동 추정 등 다양한 항목을 포함하고 있다; (ii) 우리는 SpatialScore를 제안하였는데, 이는 지금까지 가장 포괄적이고 다样적인 다모달 공간 이해 기준이며, VGBench와 다른 11개의 기존 데이터셋의 관련 데이터를 통합한 것이다. 이 기준은 28,000개의 샘플을 포함하며, 다양한 공간 이해 태스크, 모달과 QA 형식을 포함하고 있으며, SpatialScore-Hard라는 도전적인 서브셋을 포함하고 있다; (iii) 우리는 SpatialAgent를 개발하였는데, 이는 9개의 전문 도구를 결합한 새로운 다모달 다 에이전트 시스템이며, Plan-Execute와 ReAct 추론 패러다임을 지원한다; (iv) 우리는 광범위한 평가를 수행하였으며, 공간 추론에서 지속적으로 존재하는 도전을 밝혀 SpatialAgent의 유효성을 보여준다. SpatialScore가 MLLMs의 다음 진화의 엄격한 기준이 될 것으로 믿는다고 생각한다.",
      "upvotes": 9,
      "discussionId": "682fde952f8f73559fcbc616",
      "projectPage": "https://haoningwu3639.github.io/SpatialScore/",
      "githubRepo": "https://github.com/haoningwu3639/SpatialScore/",
      "ai_summary": "SpatialScore benchmarks multimodal large language models for 3D spatial understanding, revealing challenges and showcasing the effectiveness of SpatialAgent with specialized tools.",
      "ai_keywords": [
        "Multimodal large language models",
        "MLLMs",
        "VGBench",
        "SpatialScore",
        "spatial understanding",
        "visual geometry perception",
        "camera pose",
        "motion estimation",
        "multi-agent system",
        "SpatialAgent",
        "Plan-Execute",
        "ReAct reasoning paradigms"
      ]
    },
    "publishedAt": "2025-05-22T13:59:03.000Z",
    "title": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial\n  Understanding",
    "summary": "Multimodal large language models (MLLMs) have achieved impressive success in\nquestion-answering tasks, yet their capabilities for spatial understanding are\nless explored. This work investigates a critical question: do existing MLLMs\npossess 3D spatial perception and understanding abilities? Concretely, we make\nthe following contributions in this paper: (i) we introduce VGBench, a\nbenchmark specifically designed to assess MLLMs for visual geometry perception,\ne.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most\ncomprehensive and diverse multimodal spatial understanding benchmark to date,\nintegrating VGBench with relevant data from the other 11 existing datasets.\nThis benchmark comprises 28K samples across various spatial understanding\ntasks, modalities, and QA formats, along with a carefully curated challenging\nsubset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent\nsystem incorporating 9 specialized tools for spatial understanding, supporting\nboth Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive\nevaluations to reveal persistent challenges in spatial reasoning while\ndemonstrating the effectiveness of SpatialAgent. We believe SpatialScore will\noffer valuable insights and serve as a rigorous benchmark for the next\nevolution of MLLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632c7a0d1d303f5f9acf01b8/3GQcp-q-BBj5bX_DRmB6S.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632c7a0d1d303f5f9acf01b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
      "fullname": "Haoning Wu",
      "name": "haoningwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16839",
      "authors": [
        {
          "_id": "682fd5758d2fd6fc7cd5c9f7",
          "name": "Shufan Li",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9f8",
          "name": "Konstantinos Kallidromitis",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9f9",
          "name": "Hritik Bansal",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fa",
          "name": "Akash Gokul",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fb",
          "name": "Yusuke Kato",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fc",
          "name": "Kazuki Kozuka",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fd",
          "name": "Jason Kuen",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fe",
          "name": "Zhe Lin",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9ff",
          "name": "Kai-Wei Chang",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5ca00",
          "name": "Aditya Grover",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6310531914aa81e1044363ed/85JcNXnpZ6f0XvO4vZ5_s.gif"
      ],
      "publishedAt": "2025-05-22T16:07:12.000Z",
      "submittedOnDailyAt": "2025-05-23T00:30:19.437Z",
      "title": "LaViDa: 다중모달 이해를 위한 대형 Diffusion 언어 모델",
      "submittedOnDailyBy": {
        "_id": "6310531914aa81e1044363ed",
        "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
        "isPro": false,
        "fullname": "Shufan Li",
        "user": "jacklishufan",
        "type": "user"
      },
      "summary": "현대의 비슷언어 모델(VLMs)은 여러 가지 태스크를 해결할 수 있습니다. 실제 세계적인 시나리오에서, VLMs가 원하는 특성으로 고속의 추론과 생성의 제어(예: 출력을 특정 형식에 맞추도록 제한하는 등)이 필요합니다. 그러나 현재의 자동복원(AR) VLMs의 예인 LLaVA는 이러한 측면에서 어려움을 겪습니다. 이산적인 디피션 모델(DMs)은 병렬적인 해석을 가능하게 하여 고속의 추론을 실현하고, 문맥을 역방향으로 활용하여 생성의 제어를 가능하게 합니다. DMs는 언어만 있는 설정에서 효과적이지만, 다형 태스크의 가능성은 조사가 부족합니다. 우리는 DMs에 기반한 VLMs의 가족을 소개합니다. 우리는 DMs에 시각 엔코더를 추가하고 결합된 부분을 다형의 명령 순환을 모두 함께 미세 조정하여 LaViDa를 구축했습니다. 이러한 문제를 대처하기 위해, LaViDa는 interpolation mask, prefix KV 캐시, 시간 단계 쉬프트 등 새로운 기술들을 도입합니다. 실험은 LaViDa는 AR VLMs과 비교하여 MMMU 등 다형 태스크 벤치마크에서 우수한 성능을 보여주고, DMs의 고유한 우수한 성능을 제공합니다. 코드와 모델은 카메라 라인 버전으로 공개됩니다.",
      "upvotes": 9,
      "discussionId": "682fd5768d2fd6fc7cd5ca3c",
      "projectPage": " https://homepage.jackli.org/projects/lavida/index.html",
      "githubRepo": "https://github.com/jacklishufan/LaViDa",
      "ai_summary": "LaViDa, a family of vision-language models built on discrete diffusion models, offers competitive performance on multimodal benchmarks with advantages in speed, controllability, and bidirectional reasoning.",
      "ai_keywords": [
        "autoregressive (AR) VLMs",
        "discrete diffusion models (DMs)",
        "parallel decoding",
        "bidirectional context",
        "text-infilling",
        "multimodal instruction following",
        "complementary masking",
        "prefix KV cache",
        "timestep shifting",
        "MMMU",
        "COCO captioning",
        "Constrained Poem Completion",
        "Open-LLaVa-Next-8B",
        "CIDEr"
      ]
    },
    "publishedAt": "2025-05-22T12:07:12.000Z",
    "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
    "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6310531914aa81e1044363ed/85JcNXnpZ6f0XvO4vZ5_s.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16839.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310531914aa81e1044363ed",
      "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
      "fullname": "Shufan Li",
      "name": "jacklishufan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14625",
      "authors": [
        {
          "_id": "682fdb318df2d5446a1cf30b",
          "name": "Zhangchen Xu",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30c",
          "name": "Yuetai Li",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30d",
          "name": "Fengqing Jiang",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30e",
          "name": "Bhaskar Ramasubramanian",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30f",
          "name": "Luyao Niu",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf310",
          "name": "Bill Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf311",
          "name": "Radha Poovendran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:16:44.000Z",
      "submittedOnDailyAt": "2025-05-23T00:50:16.785Z",
      "title": "TinyV: 검증에서 과소漏捨い의 감소가 LLM의 RL 향상에 기여합니다.",
      "submittedOnDailyBy": {
        "_id": "653df1323479e9ebbe3eb6cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
        "isPro": true,
        "fullname": "Zhangchen Xu",
        "user": "zhangchenxu",
        "type": "user"
      },
      "summary": "강화학습(RL)은 보상 신호를 사용하여 대규모 언어 모델(LLMs)의 논리 능력을 향상시키는 강력한 도구로 자리잡고 있습니다. 그러나 RL의 성공은 보상의 신뢰성에 의존하며, 이는 증명자들이 제공합니다. 본 논문에서는 증명자들이 올바른 모델 출력을 잘못 거부하는 \"오류의 부정\" (false negatives)의 광범위한 문제를 노출하고 분석합니다. Big-Math-RL-Verified 데이터 세트에 대한 세부적인 연구에 따라, 38% 이상의 모델 생성의 응답이 증명자가 올바른 답을 인식할 수 없는 \"오류의 부정\"에 직면해 있다는 사실을 밝혀졌습니다. 실험적으로 및 이론적으로도 나타내며, 이러한 \"오류의 부정\"은 정보적인 경사 신호를 모델에서 빼앗으며, RL의 훈련을 악화시키며, 수렴을 늦추는 것을 보여줍니다. 이를 완화하기 위해, 간단한 LLM 기반의 증명자 tinyV를 제안하고 있습니다. 현재의 규칙 기반의 방법을 강화하고, 잠재적인 \"오류의 부정\"을 동적으로 인식하고, 올바른 응답을 복원하여, 더 정확한 보상의 추정을 수행합니다. 여러 수학 논리 벤치마크에서 TinyV의 통합으로, 합격률이 10% 이상 상승하며, 기준과 비교하여 수렴을 가속화합니다. 우리가 발견한 것은 증명자의 \"오류의 부정\"을 해결하는 중요성을 강조하고, LLMs의 RL 기반의 미세 조정을 개선하는 유용한 접근 방식을 제공합니다. 우리의 코드는 https://github.com/uw-nsl/TinyV에 공개되어 있습니다.",
      "upvotes": 9,
      "discussionId": "682fdb328df2d5446a1cf377",
      "githubRepo": "https://github.com/uw-nsl/TinyV",
      "ai_summary": "TinyV, a lightweight LLM-based verifier, improves RL training of large language models by addressing false negatives from existing rule-based verifiers, enhancing reward accuracy and convergence speed.",
      "ai_keywords": [
        "Reinforcement Learning",
        "large language models",
        "policies",
        "reward signals",
        "verifiers",
        "false negatives",
        "Big-Math-RL-Verified dataset",
        "gradient signals",
        "convergence",
        "TinyV",
        "rule-based methods",
        "math-reasoning benchmarks",
        "pass rates"
      ]
    },
    "publishedAt": "2025-05-20T13:16:44.000Z",
    "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning",
    "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "zhangchenxu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16151",
      "authors": [
        {
          "_id": "682fd61601208348fffaa62e",
          "name": "Hongchen Wei",
          "hidden": false
        },
        {
          "_id": "682fd61601208348fffaa62f",
          "name": "Zhenzhong Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T02:51:12.000Z",
      "submittedOnDailyAt": "2025-05-23T00:34:39.659Z",
      "title": "무훈련의 이유론과 반성기능의 MLLM",
      "submittedOnDailyBy": {
        "_id": "63f96e99ade090bc87bc2f81",
        "avatarUrl": "/avatars/0dd0807e5b2cec011e97c8d6a3c61bae.svg",
        "isPro": false,
        "fullname": "hcwei",
        "user": "hcwei",
        "type": "user"
      },
      "summary": "최근의 Reasoning LLMs（예：DeepSeek-R1와 OpenAI-o1）의 발전은 强化学習에 의한 魅力的な 추론 능력을 보여주고 있습니다. 그러나 이러한 능력을 Multimodal LLMs（MLLMs）에 확장하는 것은 재훈련의 고가 비용과 고품질의 확인 가능한 다형적인 추론 데이터셋의 부족으로 어려워졌습니다. 본 논문에서는 FRANK Model을 소개합니다. 이는 ∇ 업데이트와 추가적인 스テラバイオン을 필요로 하지 않고, 推理와 반성 능력을 제공하여 훈련 비용이 없는 ANd r1-like MLLM입니다. 우리의 주요 아이디어는 MLLM의 디코더 레이어에서 추론과 이유를 분리하는 것입니다. 특히, 깊은 디코더 레이어와 비교하여, 얕은 디코더 레이어는 시각 토큰에 많은 어텐션을 분배하고, 깊은 디코더 레이어는 문학적 의미에 집중하는 것을 관찰했습니다. 이 관찰은 시각 플렛 라인 된 MLLM과 이유에 특화된 LLM의 가중치 결합을 촉진하는 단계적 접근입니다. 이로 인해 깊은 디코더 레이어에 이유의 능력이 통합되고, 얕은 디코더 레이어에서 시각 지평을 유지하기 위해 레이어별로 タイヤードerived 닫힌 형태의 융합 구조를 제안합니다. 어려운 다형적인 추론 벤치마크에서 확장된 실험은 우리의 접근 방식의 효과를 보여주고 있습니다. MMMU 벤치마크에서, 우리의 모델인 FRANK-38B는 69.2의 정확도를 달성하며, 강력한 베이스라인 모델인 InternVL2.5-38B를 +5.3 초과하며, 더불어 프로필리어 GPT-4o 모델을 초과했습니다. 우리의 프로젝트 홈 페이지는 아래 URL을 참조하세요：http://iip.whu.edu.cn/frank/index.html",
      "upvotes": 6,
      "discussionId": "682fd61701208348fffaa654",
      "ai_summary": "The FRANK Model enhances multimodal LLMs with reasoning and reflection abilities without retraining, using a hierarchical weight merging approach that merges visual-pretrained and reasoning-specialized models.",
      "ai_keywords": [
        "Reasoning LLMs",
        "Multimodal LLMs (MLLMs)",
        "FRANK Model",
        "reinforcement learning",
        "multimodal reasoning datasets",
        "hierarchical weight merging",
        "Taylor-derived closed-form fusion mechanism",
        "MMMU benchmark",
        "visual tokens",
        "textual semantics",
        "deep decoder layers",
        "shallow decoder layers"
      ]
    },
    "publishedAt": "2025-05-21T22:51:12.000Z",
    "title": "Training-Free Reasoning and Reflection in MLLMs",
    "summary": "Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have\nshowcased impressive reasoning capabilities via reinforcement learning.\nHowever, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by\nthe prohibitive costs of retraining and the scarcity of high-quality,\nverifiable multimodal reasoning datasets. This paper introduces FRANK Model, a\ntraining-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning\nand reflection abilities, without any gradient updates or extra supervision.\nOur key insight is to decouple perception and reasoning across MLLM decoder\nlayers. Specifically, we observe that compared to the deeper decoder layers,\nthe shallow decoder layers allocate more attention to visual tokens, while the\ndeeper decoder layers concentrate on textual semantics. This observation\nmotivates a hierarchical weight merging approach that combines a\nvisual-pretrained MLLM with a reasoning-specialized LLM. To this end, we\npropose a layer-wise, Taylor-derived closed-form fusion mechanism that\nintegrates reasoning capacity into deep decoder layers while preserving visual\ngrounding in shallow decoder layers. Extensive experiments on challenging\nmultimodal reasoning benchmarks demonstrate the effectiveness of our approach.\nOn the MMMU benchmark, our model FRANK-38B achieves an accuracy of 69.2,\noutperforming the strongest baseline InternVL2.5-38B by +5.3, and even\nsurpasses the proprietary GPT-4o model. Our project homepage is at:\nhttp://iip.whu.edu.cn/frank/index.html",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16151.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63f96e99ade090bc87bc2f81",
      "avatarUrl": "/avatars/0dd0807e5b2cec011e97c8d6a3c61bae.svg",
      "fullname": "hcwei",
      "name": "hcwei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15879",
      "authors": [
        {
          "_id": "682fecc3fd3719dbe6fbb84b",
          "name": "Yue Fan",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84c",
          "name": "Xuehai He",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84d",
          "name": "Diji Yang",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84e",
          "name": "Kaizhi Zheng",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84f",
          "name": "Ching-Chen Kuo",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb850",
          "name": "Yuting Zheng",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb851",
          "name": "Sravana Jyothi Narayanaraju",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb852",
          "name": "Xinze Guan",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb853",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:54:49.000Z",
      "submittedOnDailyAt": "2025-05-23T02:05:02.804Z",
      "title": "GRIT: 이미지로 생각하는 MLLM의 지도 방법",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "최근의 연구는, 강화 학습(RL)을 사용하여 이유를 설명하는 모델을 효과적으로 구축하는 데 효과성을 보여주고 있습니다. 그러나 시각 언어 태스크의 이유를 가능하게 하는 발전이 진행 중이며, 현재의 오픈 소스의 시각 이유 모델은 일반적으로 이유의 내용을 단순한 자연어로 생성하고, 명시적인 시각 정보의 통합을 부족합니다. 이는 명확한 이유의 연속을 생성하는 능력을 제한하고 있습니다. 이러한 점에 대해, 우리는 이미지와 텍스트를 사용하는 기초적인 이유(GRIT)을 제안합니다. GRIT은 이미지를 사용하여 생각하는 MLLM의 훈련의 새로운 방법입니다. GRIT은 기초적인 이유 패러다임을 도입하고, 모델이 자연어와 명시적인 Bounding Box 좌표가 교차한 이유의 연속을 생성하도록 합니다. 이 좌표는 모델이 이유의 과정에서 참조하는 입력 이미지의 영역을 지정합니다. 또한, GRIT은 GRPO 알고리즘에 기반한 GRPO-GR의 강화 학습 접근 방식을 사용합니다. GRPO-GR은 최종적인 답의 정확성과 기초적인 이유의 출력 형식에 초점을 맞추어 강력한 보상을 사용합니다. 이는 이유의 연속을 증명하고 명시적인 Bounding Box 라벨의 데이터를 필요로 하지 않습니다. 이러한 기반에, GRIT은 특별한 데이터 효율을 달성하며, 현재의 데이터 세트에서 20개의 이미지-문제-답의 튜플을 필요로 합니다. 세부적인 평가는, GRIT이 MLLM을 이유의 연속을 생성하는 데 효과적으로 훈련하고, 이유와 기초적인 이유의 능력의 성공적인 통합을 보여줍니다.",
      "upvotes": 6,
      "discussionId": "682fecc4fd3719dbe6fbb8ac",
      "projectPage": "https://grounded-reasoning.github.io",
      "githubRepo": "https://github.com/eric-ai-lab/GRIT",
      "ai_summary": "A novel method called GRIT enhances visual reasoning in MLLMs by generating reasoning chains that integrate both natural language and bounding box coordinates, guided by a reinforcement learning approach for high data efficiency.",
      "ai_keywords": [
        "Reinforcement Learning",
        "RL",
        "MLLMs",
        "reasoning chains",
        "interleave natural language",
        "bounding box coordinates",
        "GRPO-GR",
        "GRPO",
        "grounded reasoning output",
        "reasoning and grounding abilities"
      ]
    },
    "publishedAt": "2025-05-21T13:54:49.000Z",
    "title": "GRIT: Teaching MLLMs to Think with Images",
    "summary": "Recent studies have demonstrated the efficacy of using Reinforcement Learning\n(RL) in building reasoning models that articulate chains of thoughts prior to\nproducing final answers. However, despite ongoing advances that aim at enabling\nreasoning for vision-language tasks, existing open-source visual reasoning\nmodels typically generate reasoning content with pure natural language, lacking\nexplicit integration of visual information. This limits their ability to\nproduce clearly articulated and visually grounded reasoning chains. To this\nend, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method\nfor training MLLMs to think with images. GRIT introduces a grounded reasoning\nparadigm, in which models generate reasoning chains that interleave natural\nlanguage and explicit bounding box coordinates. These coordinates point to\nregions of the input image that the model consults during its reasoning\nprocess. Additionally, GRIT is equipped with a reinforcement learning approach,\nGRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused\non the final answer accuracy and format of the grounded reasoning output, which\neliminates the need for data with reasoning chain annotations or explicit\nbounding box labels. As a result, GRIT achieves exceptional data efficiency,\nrequiring as few as 20 image-question-answer triplets from existing datasets.\nComprehensive evaluations demonstrate that GRIT effectively trains MLLMs to\nproduce coherent and visually grounded reasoning chains, showing a successful\nunification of reasoning and grounding abilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16944",
      "authors": [
        {
          "_id": "683023848d2fd6fc7ce9b99a",
          "name": "Yunjia Qi",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99b",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99c",
          "name": "Xiaozhi Wang",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99d",
          "name": "Amy Xin",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99e",
          "name": "Youfeng Liu",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99f",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b9a0",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b9a1",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:31:10.000Z",
      "submittedOnDailyAt": "2025-05-23T05:58:53.468Z",
      "title": "AGENTIF: 대규모 언어 모델의 지시 준수 테스트를 위한 에이전트 시나리오",
      "submittedOnDailyBy": {
        "_id": "6556cf2cee35f7d8bcf13bb3",
        "avatarUrl": "/avatars/0a945054cb4732c2ee7a5502c42628bf.svg",
        "isPro": false,
        "fullname": "Qi Yunjia",
        "user": "Kikkk",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)은 현실적인 효과적인 에이전트 시스템의 개발에 있어 높은 능력을 보여주고 있습니다. 연구의 발전은 사용 가능한 요구를 충족하는 LLM 기반의 에이전트의 개발을 목표로 하고 있지만, 새로운 도전이 발생했습니다. 에이전트 시나리오는 긴 명령과 복잡한 제약을 포함하는 시스템 프로ン프트와 세부적인 도구规格 등 포함됩니다. 에이전트 시스템의 적용에서 명령의 준수성은 중요하지만, LLMs가 이를 신뢰적으로 추월할 수 있는지는 아직 조사가 부족합니다. 본 논문에서는, 에이전트 시나리오에서 LLMs의 명령 추적 능력을 체계적으로 평가하기 위한 첫 번째 벤치마크인 \"AgentIF\"를 소개합니다. AgentIF는 다음과 같은 3가지 특징을 가지고 있습니다: 1. 리알리티를 반영하여, 50개의 실세계의 에이전트 시스템으로부터 구축되어 있습니다. 2. 긴, 평균 1,723 글자 이상 최대 15,630 글자를 초과합니다. 3. 복잡하며, 명령에 대한 제약의 평균수는 11.9이고, 도구规格와 조건 제약 등 다양한 제약 유형을 포함합니다. AgentIF의 구축에는 50개의 에이전트 태스크로부터 707건의 인간으로 어노테이션된 명령을 수집했습니다. 각 명령에 대해 관련 제약과 평가 지표를 어노테이트하고, 코드 기반 평가, LLM 기반 평가, 하이브리드 코드-LLM 평가를 포함하는 평가 지표를 포함합니다. AgentIF를 사용하여, 기존의 첨단 LLMs를 체계적으로 평가했습니다. 현재의 모델은, 특히 복잡한 제약 구조와 도구规格의 처리에 있어서 일반적으로 낮은 성능을 나타냅니다. 또한, 명령의 길이와 메타 제약에 대해 오류 분석과 분석적인 실험을 수행하고, 기존의 LLMs의 실패 모드에 대한 발견을 제공했습니다. 코드와 데이터를 공개하여 향후 연구를 지원하는 것을 목표로 합니다.",
      "upvotes": 5,
      "discussionId": "683023858d2fd6fc7ce9b9e4",
      "githubRepo": "https://github.com/THU-KEG/AgentIF",
      "ai_summary": "A new benchmark, AgentIF, evaluates Large Language Models' ability to follow complex instructions in realistic agentic scenarios, revealing performance limitations in handling constraints and tool specifications.",
      "ai_keywords": [
        "AgentIF",
        "Large Language Models (LLMs)",
        "agentic applications",
        "system prompts",
        "tool specifications",
        "instruction following",
        "constraint structures",
        "error analysis"
      ]
    },
    "publishedAt": "2025-05-22T13:31:10.000Z",
    "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in\n  Agentic Scenarios",
    "summary": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16944.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6556cf2cee35f7d8bcf13bb3",
      "avatarUrl": "/avatars/0a945054cb4732c2ee7a5502c42628bf.svg",
      "fullname": "Qi Yunjia",
      "name": "Kikkk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16854",
      "authors": [
        {
          "_id": "682fdd8691757629e1d58e16",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "682fdd8691757629e1d58e17",
          "name": "Kevin Qinghong Lin",
          "hidden": false
        },
        {
          "_id": "682fdd8691757629e1d58e18",
          "name": "James Cheng",
          "hidden": false
        },
        {
          "_id": "682fdd8691757629e1d58e19",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T16:13:29.000Z",
      "submittedOnDailyAt": "2025-05-23T01:09:25.197Z",
      "title": "선택적 이유론을 수행하는 리노즈 학습을 이용한 시각-언어 모델",
      "submittedOnDailyBy": {
        "_id": "64440be5af034cdfd69ca3a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
        "isPro": false,
        "fullname": "Qinghong (Kevin) Lin",
        "user": "KevinQHLin",
        "type": "user"
      },
      "summary": "강화학습(RL)은 시각 언어 모델(VLMs)의 논리론을 향상시키는 효과적인 훈련 후 전략임을 입증되어 있습니다. 그룹 상대적 최적화(GRPO)는 모델이 응답하기 전에 완전한 논리론 트래스 생성을 촉구하는 최근 중요한 방법 중 하나で, 토큰 사용량과 계산 비용 증가로 논리론의 효율화를 목표로하고 있습니다. 인간처럼 사고 과정 모델화하고, 간단한 문제를 대할 때 논리론을 생략하고 필요할 때만 신중히 생각하도록 학습시키는 방법을 조사하고, VLMs이 논리론이 필요할지 여부를 처음부터 판단할 수 있는 방법을 조사했습니다. 이를 실현하기 위해, TON(Think or Not)을 제안했습니다. TON은 2단계 훈련 전략으로, (i) 단순하고 효과적인 'thought dropout' 연산 포함의 정규화 微調(SFT) 단계로, 논리론 트래스이 랜덤으로 빈 생각으로 대체될 수 있는 think-or-not 형식을 도입하여 선택적 논리론의冷却를 제공하며, (ii) GRPO 단계에서는 모델이 자유롭게 생각할 수 있는지 결정하여, 태스크 관련 결과 보상을 최대화하는 것을 목표로 합니다. 실험 결과, TON은 베지어 GRPO보다 90%의 완료 시간을 줄일 수 있으며, 성능 손실 없이 개선할 수 있습니다. 3B와 7B 모델을 사용하여 다양한 시각 언어 태스크의 논리론 난이도 범위를 확장한 다양한 평가에서, 훈련이 진행되는 동안 모델이 불필요한 논리론 단계를 스킵하는 것을 점차적으로 학습하는 것을 경험적으로 확인했습니다. 이러한 발견은 강화학습 접근법으로 인간처럼 논리론 패턴을 향한 길을 조명하고 있습니다. 코드는 https://github.com/kokolerk/TON에 공개되어 있습니다.",
      "upvotes": 5,
      "discussionId": "682fdd8791757629e1d58e77",
      "projectPage": "https://huggingface.co/collections/kolerk/ton-682ad9038395c21e228a645b",
      "githubRepo": "https://github.com/kokolerk/TON",
      "ai_summary": "TON, a two-stage training strategy combining supervised fine-tuning with thought dropout and Group Relative Policy Optimization, reduces unnecessary reasoning steps in vision-language models without sacrificing performance.",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "vision-language models (VLMs)",
        "Group Relative Policy Optimization (GRPO)",
        "thought dropout",
        "selective reasoning"
      ]
    },
    "publishedAt": "2025-05-22T12:13:29.000Z",
    "title": "Think or Not? Selective Reasoning via Reinforcement Learning for\n  Vision-Language Models",
    "summary": "Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across diverse vision-language tasks-covering\na range of reasoning difficulties under both 3B and 7B models-consistently\nreveal that the model progressively learns to bypass unnecessary reasoning\nsteps as training advances. These findings shed light on the path toward\nhuman-like reasoning patterns in reinforcement learning approaches. Our code is\navailable at https://github.com/kokolerk/TON.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16854.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64440be5af034cdfd69ca3a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
      "fullname": "Qinghong (Kevin) Lin",
      "name": "KevinQHLin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 31
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16400",
      "authors": [
        {
          "_id": "682fe6644640a9db4d1f31d9",
          "name": "Yang Chen",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31da",
          "user": {
            "_id": "67d75b0117c2acac528f47b6",
            "avatarUrl": "/avatars/619aacd1a619aab64de3499ac3ee2229.svg",
            "isPro": false,
            "fullname": "Zhuolin Yang",
            "user": "zhuoliny",
            "type": "user"
          },
          "name": "Zhuolin Yang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:07:17.257Z",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31db",
          "name": "Zihan Liu",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31dc",
          "name": "Chankyu Lee",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31dd",
          "name": "Peng Xu",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31de",
          "name": "Mohammad Shoeybi",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31df",
          "name": "Bryan Catanzaro",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31e0",
          "user": {
            "_id": "663ee43bfeeb49803537da98",
            "avatarUrl": "/avatars/17c3e9c435cc36fb04b4589e6176a243.svg",
            "isPro": false,
            "fullname": "Wei Ping",
            "user": "wping",
            "type": "user"
          },
          "name": "Wei Ping",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:07:17.257Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T08:50:47.000Z",
      "submittedOnDailyAt": "2025-05-23T01:38:02.331Z",
      "title": "AceReason-Nemotron: 수학과 코드를 이해하는 리노ー스 학습",
      "submittedOnDailyBy": {
        "_id": "62bc9d90e81dfd65cced9316",
        "avatarUrl": "/avatars/05df14cd1fdbc7d6a80d2960a05a94f0.svg",
        "isPro": false,
        "fullname": "Yang Chen",
        "user": "ychenNLP",
        "type": "user"
      },
      "summary": "최근의 규모가 큰 강화학습(RL)에 대한 논리 모델의 개발에서, 고성능의 논리 모델을 구축하기 위한 훈련 디텍션이 아직 불분명하다. 선도 모델의 구현 세부 사항, 예를 들어 DeepSeek-R1의 데이터 커리큘레이션과 RL 훈련 디텍션이, 일반적으로 생략된다. 또한 최근의 연구는, 작은 모델에 대해는, 번역이 RL보다 효과적이라는 것을 보여주고 있다. 본 연구에서는, 규모가 큰 RL이 강력한 작은 또는 중간 크기의 모델의 논리 능력이 크게 향상되고, 가장 선진적인 번역 기반의 모델보다 우수한 결과를 구현할 수 있음을 보여준다. RL 훈련 프로세스를 구축하고, 확산 테스트를 수행하고, 간단하고 효과적인 접근 방식을 제안하고 있다. 처음에 수학만 있는 펫을 사용하여 훈련하고, 다음으로 코드만 있는 펫을 사용하여 훈련한다. 특히, 수학만 있는 RL은 강력한 번역 모델의 수학 벤치마크(예를 들어, 7B/14B 모델의 AIME 2025에서 각각 +14.6%/+17.2%의 성능을 크게 향상시키고, 코드 논리 태스크(예를 들어, 7B/14B 모델의 LiveCodeBench에서 각각 +6.8%/+5.8%의 향상을 보이는)을 향상시킨다는 것을 보여준다. 또한 확장된 코드만 있는 RL 훈련은, 수학 결과를 최소한으로 또는 디카リン그가 없는 상태에서 코드 벤치마크의 성능을 향상시키는 것을 보여준다. 고품질으로 확인 가능한 답과 테스트 케이스를 포함하는 어려운 펫을 모으기 위한 강력한 데이터 커리큘레이션 파이프라인을 개발하고, 두 분야에서 증명 기반의 RL을 가능하게 하고 있다. 마지막으로, 주요 실험의 피드백을 특정하고, 점진적으로 증가하는 응답 길이의 커리큘레이션 학습과 온라인 파라미터 업데이트의 안정화 효과에 포함하고 있다. RL은, 사전 학습 및 정규화 조정(예를 들어, 번역)의 때에 얻은 기초적인 논리 능력을 발휘하고, 모델의 논리 능력의 한계를 초월하고, 이전에 해결되지 못한 문제를 해결할 수 있게 하는 것이다.",
      "upvotes": 5,
      "discussionId": "682fe6654640a9db4d1f3229",
      "ai_summary": "Large-scale reinforcement learning enhances reasoning capabilities in small and mid-sized models more effectively than distillation, achieving superior results in both math and code benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "DeepSeek-R1",
        "data curation",
        "distillation",
        "math-only prompts",
        "code-only prompts",
        "AIME 2025",
        "LiveCodeBench",
        "curriculum learning",
        "on-policy parameter updates"
      ]
    },
    "publishedAt": "2025-05-22T04:50:47.000Z",
    "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through\n  Reinforcement Learning",
    "summary": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16400.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bc9d90e81dfd65cced9316",
      "avatarUrl": "/avatars/05df14cd1fdbc7d6a80d2960a05a94f0.svg",
      "fullname": "Yang Chen",
      "name": "ychenNLP",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16192",
      "authors": [
        {
          "_id": "68302db3b73ef22aebdce9c2",
          "name": "Chaoya Jiang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c3",
          "name": "Yongrui Heng",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c4",
          "name": "Wei Ye",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c5",
          "name": "Han Yang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c6",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c7",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c8",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c9",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9ca",
          "name": "Shikun Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:50:13.000Z",
      "submittedOnDailyAt": "2025-05-23T06:42:34.763Z",
      "title": "VLM-R^3: 영역 인식, 이유론, 개선을 위한 확장 다중 모델 컨티뉴어ㅎㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏ",
      "submittedOnDailyBy": {
        "_id": "645b10e80c73ea27d13f7aca",
        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
        "isPro": false,
        "fullname": "xuhaiyang",
        "user": "xhyandwyy",
        "type": "user"
      },
      "summary": "최근, 이유 기반의 MLLM은 긴 문장을 생성하기 위해 일정 수준의 성공을 달성했습니다. 그러나 복잡한 작업에서, 이미지의 시각 영역을 동적으로 처리하고 재독립하는 것이 필요하고, 이유를 문맥으로 정확히 맞추는 것은 시각적 증거에 대해 어렵습니다. 우리는 영역 인식과 이유에 대응하는 VLM-R^3(Visual Language Model with Region Recognition and Reasoning) 프레임워크를 시각 언어 모델로 소개합니다. 이 프레임워크는 MLLM이 다음 능력 제공합니다. (i) 추가적인 시각적 증거가 필요 여부를 판단하는, (ii) 이미지의 어느 부분에 이유를 맞추는지 결정하는, (iii) 관련된 서브 이미지의 내용을 연속된 이유 연결에 연결하는 것입니다. 우리의 방법의 핵심은 정보적인 영역을 선택하고, 적절한 변환(예: 추출, 확장)을 구성하고, 그 결과를 후속의 이유 단계에 통합하기 위한 영역 조건에 따른 강화 정책 최적화(R-GRPO)입니다. 이 정책을 시작하기 위해, 영역 선택과 문맥 이유의 단계 수준의 디렉터리 기반의 시각 언어 인터랙티브 레시오ン(VLIR) 코퍼스를 제작합니다. 이 코퍼스는 MathVista, ScienceQA, 기타 벤치마크의 확장 테스트를 통해, VLM-R^3은 0 shot과 피쳐 shot 설정에서 새로운 수준의 최고 수준을 설정하고, 복잡한 공간 이유 및 미세한 시각적 조각을 요구하는 문제에서 특히 큰 진전을 보여주고 있습니다.",
      "upvotes": 4,
      "discussionId": "68302db5b73ef22aebdcea32",
      "ai_summary": "VLM-R3 enhances multi-modal language models with region recognition and reasoning, achieving state-of-the-art performance on visual question answering tasks through region-conditioned reinforcement policy optimization.",
      "ai_keywords": [
        "VLM-R3",
        "Region-Conditioned Reinforcement Policy Optimization (R-GRPO)",
        "Visuo-Lingual Interleaved Rationale (VLIR) corpus",
        "MathVista",
        "ScienceQA"
      ]
    },
    "publishedAt": "2025-05-21T23:50:13.000Z",
    "title": "VLM-R^3: Region Recognition, Reasoning, and Refinement for Enhanced\n  Multimodal Chain-of-Thought",
    "summary": "Recently, reasoning-based MLLMs have achieved a degree of success in\ngenerating long-form textual reasoning chains. However, they still struggle\nwith complex tasks that necessitate dynamic and iterative focusing on and\nrevisiting of visual regions to achieve precise grounding of textual reasoning\nin visual evidence. We introduce VLM-R^3 (Visual\nLanguage Model with Region Recognition and\nReasoning), a framework that equips an MLLM with the ability to (i)\ndecide when additional visual evidence is needed, (ii) determine\nwhere to ground within the image, and (iii) seamlessly weave the\nrelevant sub-image content back into an interleaved chain-of-thought. The core\nof our method is Region-Conditioned Reinforcement Policy Optimization\n(R-GRPO), a training paradigm that rewards the model for selecting informative\nregions, formulating appropriate transformations (e.g.\\ crop, zoom), and\nintegrating the resulting visual context into subsequent reasoning steps. To\nbootstrap this policy, we compile a modest but carefully curated Visuo-Lingual\nInterleaved Rationale (VLIR) corpus that provides step-level supervision on\nregion selection and textual justification. Extensive experiments on MathVista,\nScienceQA, and other benchmarks show that VLM-R^3 sets a new state of the art\nin zero-shot and few-shot settings, with the largest gains appearing on\nquestions demanding subtle spatial reasoning or fine-grained visual cue\nextraction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16192.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15963",
      "authors": [
        {
          "_id": "682fdcc0087ea62f1663df96",
          "name": "Shujun Liu",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df97",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df98",
          "name": "Zejun Li",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df99",
          "name": "Jianxiang Wang",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df9a",
          "name": "Cheng Zeng",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df9b",
          "name": "Zhongyu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T19:26:09.000Z",
      "submittedOnDailyAt": "2025-05-23T00:57:50.603Z",
      "title": "OViP: 온라인 비전 언어 취향 학습",
      "submittedOnDailyBy": {
        "_id": "6534c1fc23e0af0e0d7e8ebd",
        "avatarUrl": "/avatars/d65237d82aea19328f28b5a0cc93b271.svg",
        "isPro": false,
        "fullname": "Siyuan Wang",
        "user": "Siyuanyuan",
        "type": "user"
      },
      "summary": "대시각 언어 모델(LVLMs)는 호라이징에 취약하고, 시각 입력과 내용이 일치하지 않는 내용을 자주 생성합니다. 최근의 접근은 호라이징을 완화하기 위해 다양한 유형의 직접적인 선호 최적화(DPO)를 추진하고 있지만, 일반적으로 사전 정의된 또는 랜덤으로 편집된 부정 샘플을 사용함으로써 실제 모델 오류를 반영하지 못하는 경우가 많기 때문에, 훈련 효과는 제한되어 있습니다. 본 연구에서는 모델이 호라이징한 출력에 기반하여 대비적인 훈련 데이터를 동적으로 구축하는 온라인 시각 언어 선호 학습(OViP) 프레임워크를 제안하고 있습니다. 샘플링된 응답 쌍의 의미적 차이를 식별하고, 확산 모델을 사용하여 부정 이미지를 합성함으로써, OViP는 실시간 관련 서브 객체 신호를 생성합니다. 이 오류가동 훈련은 양쪽 언어와 시각 선호의 적응적 조정을 가능하게 합니다. 또한 현재 평가 프로토콜을 개선하여 호라이징의 억제와 표현성의 트레이드오프를 더 잘 파악할 수 있습니다. 호라이징 및 일반적인 벤치마크 실험에서, OViP는 호라이징을 효과적으로 감소시키면서 핵심의 다형성 능력을 유지하는 것을 보여주고 있습니다.",
      "upvotes": 4,
      "discussionId": "682fdcc1087ea62f1663dfcb",
      "ai_summary": "OViP dynamically generates contrastive training data using a diffusion model to reduce hallucinations in large vision-language models while maintaining their multi-modal capabilities.",
      "ai_keywords": [
        "large vision-language models",
        "hallucination",
        "multi-modal Direct Preference Optimization",
        "OViP",
        "diffusion model",
        "contrastive training",
        "semantic differences",
        "failure-driven training"
      ]
    },
    "publishedAt": "2025-05-21T15:26:09.000Z",
    "title": "OViP: Online Vision-Language Preference Learning",
    "summary": "Large vision-language models (LVLMs) remain vulnerable to hallucination,\noften generating content misaligned with visual inputs. While recent approaches\nadvance multi-modal Direct Preference Optimization (DPO) to mitigate\nhallucination, they typically rely on predefined or randomly edited negative\nsamples that fail to reflect actual model errors, limiting training efficacy.\nIn this work, we propose an Online Vision-language Preference Learning (OViP)\nframework that dynamically constructs contrastive training data based on the\nmodel's own hallucinated outputs. By identifying semantic differences between\nsampled response pairs and synthesizing negative images using a diffusion\nmodel, OViP generates more relevant supervision signals in real time. This\nfailure-driven training enables adaptive alignment of both textual and visual\npreferences. Moreover, we refine existing evaluation protocols to better\ncapture the trade-off between hallucination suppression and expressiveness.\nExperiments on hallucination and general benchmarks demonstrate that OViP\neffectively reduces hallucinations while preserving core multi-modal\ncapabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15963.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6534c1fc23e0af0e0d7e8ebd",
      "avatarUrl": "/avatars/d65237d82aea19328f28b5a0cc93b271.svg",
      "fullname": "Siyuan Wang",
      "name": "Siyuanyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11711",
      "authors": [
        {
          "_id": "682e0d9540c6417d9962227a",
          "user": {
            "_id": "6255a34d7dacca56ac2b04e4",
            "avatarUrl": "/avatars/3e7751aa6ef7c880e3e36ac995c9a191.svg",
            "isPro": false,
            "fullname": "sagnik mukherjee",
            "user": "sagnikM",
            "type": "user"
          },
          "name": "Sagnik Mukherjee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:26.467Z",
          "hidden": false
        },
        {
          "_id": "682e0d9540c6417d9962227b",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "682e0d9540c6417d9962227c",
          "name": "Dilek Hakkani-Tur",
          "hidden": false
        },
        {
          "_id": "682e0d9540c6417d9962227d",
          "name": "Hao Peng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T21:42:28.000Z",
      "submittedOnDailyAt": "2025-05-23T00:08:00.344Z",
      "title": "강화학습에서, 대규모 언어 모델의 작은 서브네트워크를 미세 조정한다.",
      "submittedOnDailyBy": {
        "_id": "6255a34d7dacca56ac2b04e4",
        "avatarUrl": "/avatars/3e7751aa6ef7c880e3e36ac995c9a191.svg",
        "isPro": false,
        "fullname": "sagnik mukherjee",
        "user": "sagnikM",
        "type": "user"
      },
      "summary": "강화학습(RL)은 대규모 언어 모델(LLMs)의 하류 태스크 성능과 인간의 가치관의 일치에 큰 향상을 가져옵니다. 놀라운 점은 이 큰 향상은 파라미터의 5%에서 30%의 작은 부분 네트워크의 업데이트로 실현되어, 나머지는 효과적으로 변경되지 않았습니다. 이 현상을 RL에 의한 파라미터 업데이트의 희박성을 부르며, 이 희박성은 7가지 광범위하게 사용되고 있는 RL 알고리즘(예: PPO, GRPO, DPO)과 실험에서 사용된 10가지 다른 LLMs의 가족에 대해 관찰됩니다. 이 희박성은 명시적인 희박성 촉진의 정규화나 아키텍처의 제약을 포함하여 발생합니다. 부분 네트워크의 유일한 미세 조정으로 정확도가 회복되고, 놀라울 정도로 전체 네트워크의 미세 조정과 근사한 모델이 얻을 수 있습니다. 랜덤 시드, 학습 데이터, 그리고 RL 알고리즘의 네트워크는 예상보다 크게 겹칩니다. 분석에 따르면 이 희박성은 일부 레이어의 업데이트가 아닌, 거의 모든 파라미터 행렬에 적용되는 근사화된 희박な 업데이트를 보여줍니다. 또한 파라미터 행렬의 거의 모든 업데이트는 근사화된 전체 차원 업데이트가 되고, RL의 업데이트는 이러한 파라미터 행렬이 표현할 수 있는 근사화된 전체 차원을 덮는 작은 파라미터의 세트를 업데이트하는 것을 보여줍니다. 이 희박성은 정책 분포에 가까운 데이터의 학습, 정책이 미리 학습된 모델에 가까운 것처럼 만드는 기술(예: KL 정규화, 경사 클립핑)의 영향이 제한적이라는 가정을 합니다.",
      "upvotes": 4,
      "discussionId": "682e0d9540c6417d996222d7",
      "ai_summary": "Reinforcement learning improves large language models with minimal parameter updates, affecting only a small subnetwork without explicit sparsity techniques.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "parameter update sparsity",
        "PPO",
        "GRPO",
        "DPO",
        "policy distribution",
        "KL regularization",
        "gradient clipping"
      ]
    },
    "publishedAt": "2025-05-16T17:42:28.000Z",
    "title": "Reinforcement Learning Finetunes Small Subnetworks in Large Language\n  Models",
    "summary": "Reinforcement learning (RL) yields substantial improvements in large language\nmodels (LLMs) downstream task performance and alignment with human values.\nSurprisingly, such large gains result from updating only a small subnetwork\ncomprising just 5 percent to 30 percent of the parameters, with the rest\neffectively unchanged. We refer to this phenomenon as parameter update sparsity\ninduced by RL. It is observed across all 7 widely used RL algorithms (e.g.,\nPPO, GRPO, DPO) and all 10 LLMs from different families in our experiments.\nThis sparsity is intrinsic and occurs without any explicit sparsity promoting\nregularizations or architectural constraints. Finetuning the subnetwork alone\nrecovers the test accuracy, and, remarkably, produces a model nearly identical\nto the one obtained via full finetuning. The subnetworks from different random\nseeds, training data, and even RL algorithms show substantially greater overlap\nthan expected by chance. Our analysis suggests that this sparsity is not due to\nupdating only a subset of layers, instead, nearly all parameter matrices\nreceive similarly sparse updates. Moreover, the updates to almost all parameter\nmatrices are nearly full-rank, suggesting RL updates a small subset of\nparameters that nevertheless span almost the full subspaces that the parameter\nmatrices can represent. We conjecture that the this update sparsity can be\nprimarily attributed to training on data that is near the policy distribution,\ntechniques that encourage the policy to remain close to the pretrained model,\nsuch as the KL regularization and gradient clipping, have limited impact.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11711.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6255a34d7dacca56ac2b04e4",
      "avatarUrl": "/avatars/3e7751aa6ef7c880e3e36ac995c9a191.svg",
      "fullname": "sagnik mukherjee",
      "name": "sagnikM",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16186",
      "authors": [
        {
          "_id": "683005352b4a4d1ce546568b",
          "name": "Kaiwen Zhou",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568c",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568d",
          "name": "Gaowen Liu",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568e",
          "name": "Jayanth Srinivasa",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568f",
          "name": "Aosong Feng",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce5465690",
          "name": "Dawn Song",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce5465691",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/Qu6Z3hnjamfkpCZ9DuRkT.png"
      ],
      "publishedAt": "2025-05-22T03:46:03.000Z",
      "submittedOnDailyAt": "2025-05-23T03:51:00.196Z",
      "title": "SafeKey: 안전 이유로 아흉 모멘트 인싸이트를 강화합니다.",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "대규모 추론 모델(LRMs)는 명시적으로 이유를 제시하여 답을 제공하여 새로운 시대의 패러다임에 도입되고, 복잡한 태스크에서 놀라운 향상을 달성할 수 있습니다. 그러나, 유해한 쿼리 및 적대적인 공격에 대해서는 큰 안전성 위험을 동반합니다. 최근의主流의 LRMs의 안전성 향상을 위한 시도인 규범제어의 미세 조정(SFT)은 안전성 성능을 향상시킬 수 있지만, SFT에 따라 조정된 모델은 이전에 보지 못한 젓가락 브레이크 프롬프트에 대한 확장성이 낮다는 것을 확인합니다. LRMs의 생성을 자세히 조사하고, 안전성의 '아하' 모멘텀을 특정하고, 이 것이 안전성의 이유를 활성화하고, 안전한 응답을 추출할 수 있는 것을 인식합니다. 이 '아하' 모멘텀은 모델의 쿼리 이해 프로세스를 이어서 '키워드'로 일반적으로 나타나며, 모델이 안전하게 진행할 수 있는지를 나타낼 수 있습니다. 이러한 관점을 기반으로 SafeKey를 제안하고, 키워드에서 안전성의 '아하' 모멘텀을 더 잘 활성화하기 위한 두 가지 보완적인 목표를 포함하는 것이 됩니다. 1) 키워드 이전의 모델의 내부 표현의 안전성 신호를 강화하는 이중 패스 안전 헤더, 2) 쿼리 이해의 注意를 개선하는 쿼리 마스킹 모델링의 목표입니다. 여러 안전성 벤치마크에서의 실험은, 우리 방법의 범위가 광범위한 젓가락 브레이크 공격 및 분포 외의 유해한 프롬프트에 대한 안전성 확장성을 크게 향상시키고, 평균 유해성 비율을 9.6% 낮추고 일반적인 능력을 유지하는 것을 보여줍니다. 분석은 SafeKey가 안전성을 향상시킬 수 있는 방식이 어떻게 이루어지는지 밝혀, 내부의 注意를 변형시키고, 은닉 표현의 질을 향상시키는 것을 보여주었습니다.",
      "upvotes": 3,
      "discussionId": "683005362b4a4d1ce54656b1",
      "ai_summary": "SafeKey enhances the safety of large reasoning models by focusing on activating a safety aha moment in the key sentence through dual-path safety head and query-mask modeling, thereby improving generalization to harmful prompts.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRMs",
        "explicit reasoning",
        "safety risks",
        "adversarial attacks",
        "supervised fine-tuning",
        "SFT",
        "safety aha moment",
        "key sentence",
        "query understanding process",
        "Dual-Path Safety Head",
        "Query-Mask Modeling",
        "safety generalization",
        "jailbreak attacks",
        "out-of-distribution harmful prompts",
        "average harmfulness rate",
        "hidden representations"
      ]
    },
    "publishedAt": "2025-05-21T23:46:03.000Z",
    "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning",
    "summary": "Large Reasoning Models (LRMs) introduce a new generation paradigm of\nexplicitly reasoning before answering, leading to remarkable improvements in\ncomplex tasks. However, they pose great safety risks against harmful queries\nand adversarial attacks. While recent mainstream safety efforts on LRMs,\nsupervised fine-tuning (SFT), improve safety performance, we find that\nSFT-aligned models struggle to generalize to unseen jailbreak prompts. After\nthorough investigation of LRMs' generation, we identify a safety aha moment\nthat can activate safety reasoning and lead to a safe response. This aha moment\ntypically appears in the `key sentence', which follows models' query\nunderstanding process and can indicate whether the model will proceed safely.\nBased on these insights, we propose SafeKey, including two complementary\nobjectives to better activate the safety aha moment in the key sentence: (1) a\nDual-Path Safety Head to enhance the safety signal in the model's internal\nrepresentations before the key sentence, and (2) a Query-Mask Modeling\nobjective to improve the models' attention on its query understanding, which\nhas important safety hints. Experiments across multiple safety benchmarks\ndemonstrate that our methods significantly improve safety generalization to a\nwide range of jailbreak attacks and out-of-distribution harmful prompts,\nlowering the average harmfulness rate by 9.6\\%, while maintaining general\nabilities. Our analysis reveals how SafeKey enhances safety by reshaping\ninternal attention and improving the quality of hidden representations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/Qu6Z3hnjamfkpCZ9DuRkT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16186.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15517",
      "authors": [
        {
          "_id": "682e8bc5b38184d0edcd1671",
          "user": {
            "_id": "66d2af23f040611f7cea1b1b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d2af23f040611f7cea1b1b/vCToO_XgrLD0nfR8rzjZd.jpeg",
            "isPro": false,
            "fullname": "Kaiyuan Eric Chen",
            "user": "keplerccc",
            "type": "user"
          },
          "name": "Kaiyuan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:21.981Z",
          "hidden": false
        },
        {
          "_id": "682e8bc5b38184d0edcd1672",
          "name": "Shuangyu Xie",
          "hidden": false
        },
        {
          "_id": "682e8bc5b38184d0edcd1673",
          "name": "Zehan Ma",
          "hidden": false
        },
        {
          "_id": "682e8bc5b38184d0edcd1674",
          "name": "Ken Goldberg",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66d2af23f040611f7cea1b1b/f5gpZzVb5pLF6IXMt0_xZ.png"
      ],
      "publishedAt": "2025-05-21T13:42:52.000Z",
      "submittedOnDailyAt": "2025-05-23T03:39:07.197Z",
      "title": "Robo2VLM: 야외 로봇 조작 데이터 세트에서 시각적 질문 대답\n\n(注意：翻译中保持了原文的专业性和准确性，同时确保了韩语表达的自然流畅。)",
      "submittedOnDailyBy": {
        "_id": "66d2af23f040611f7cea1b1b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d2af23f040611f7cea1b1b/vCToO_XgrLD0nfR8rzjZd.jpeg",
        "isPro": false,
        "fullname": "Kaiyuan Eric Chen",
        "user": "keplerccc",
        "type": "user"
      },
      "summary": "비전 라ングワード 모delo (VLMs)는 인터넷 규모의 이미지 텍스트 코퍼스로부터 현실 세계의 지식과 일반적인 논리 능력을 습득합니다. 이들은 시나리오 이해와 태스크 계획을 강화하고, 로봇 트래지 데이터에 기반한 시각 모터 政策를 지원합니다. 우리는 반대의 패러다임을 검토하고 있습니다 - 풍부한, 진정한, 다형의 로봇 트래지 데이터를 사용하여 VLMs를 강화하고 평가하는 것입니다. 이 논문에서는 VLMs에 대한 시각 질문 대답 (VQA) 데이터 세트 생성 프레임워크인 Robo2VLM을 소개합니다. 인간이 텔레오 프로레이티드 로봇 트래지 데이터를 제공하면, Robo2VLM은 비시화된 및 비설명적인 센서 모더라이더로부터의 실제 데이터를 얻습니다. 이러한 모더라이더에 기반하여, 로봇 트래지 데이터를 동작 단계의 시퀀스로 분할합니다. 각 단계에서, Robo2VLM은 시나리오와 상호 작용을 이해하여, 로봇, 태스크 목표, 그리고 목표 물체의 3D 특성을 식별합니다. 이러한 특성을 사용하여, 공간적, 목표 조건付き, 상호 작용의 논리적인 VQA 쿼리 템플릿에 기반한 대표적인 VQA 쿼리르 생성합니다. Robo2VLM-1은 큰 규모의 로봇 트래지 데이터로부터의 데이터 세트를 제공합니다. 이 데이터 세트는 176k의 진정한 로봇 트래지 데이터에서 463 종류의 시나리오와 3,396 종류의 로봇 동작 태스크를 포함하며, 684,710 개의 문제를 기록합니다. 결과는 Robo2VLM-1이 공간적 및 상호 작용의 논리적인 VLM 능력을 벤치마크하고 향상시키는 것을 보여줍니다.",
      "upvotes": 2,
      "discussionId": "682e8bc6b38184d0edcd16bc",
      "githubRepo": "https://github.com/KeplerC/robo2VLM",
      "ai_summary": "Robo2VLM, a framework for generating Visual Question Answering datasets using robot trajectory data, enhances and evaluates Vision-Language Models by leveraging sensory modalities and 3D property understanding.",
      "ai_keywords": [
        "Visual-Language Models",
        "Visual Question Answering",
        "VQA",
        "robot trajectory data",
        "end-effector pose",
        "gripper aperture",
        "force sensing",
        "manipulation phases",
        "3D properties",
        "task goal",
        "target object",
        "spatial reasoning",
        "goal-conditioned reasoning",
        "interaction reasoning",
        "Robo2VLM-1"
      ]
    },
    "publishedAt": "2025-05-21T09:42:52.000Z",
    "title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot\n  Manipulation Datasets",
    "summary": "Vision-Language Models (VLMs) acquire real-world knowledge and general\nreasoning ability through Internet-scale image-text corpora. They can augment\nrobotic systems with scene understanding and task planning, and assist\nvisuomotor policies that are trained on robot trajectory data. We explore the\nreverse paradigm - using rich, real, multi-modal robot trajectory data to\nenhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual\nQuestion Answering (VQA) dataset generation framework for VLMs. Given a human\ntele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual\nand non-descriptive sensory modalities, such as end-effector pose, gripper\naperture, and force sensing. Based on these modalities, it segments the robot\ntrajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses\nscene and interaction understanding to identify 3D properties of the robot,\ntask goal, and the target object. The properties are used to generate\nrepresentative VQA queries - images with textural multiple-choice questions -\nbased on spatial, goal-conditioned, and interaction reasoning question\ntemplates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710\nquestions covering 463 distinct scenes and 3,396 robotic manipulation tasks\nfrom 176k real robot trajectories. Results suggest that Robo2VLM-1 can\nbenchmark and improve VLM capabilities in spatial and interaction reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66d2af23f040611f7cea1b1b/f5gpZzVb5pLF6IXMt0_xZ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15517.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d2af23f040611f7cea1b1b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d2af23f040611f7cea1b1b/vCToO_XgrLD0nfR8rzjZd.jpeg",
      "fullname": "Kaiyuan Eric Chen",
      "name": "keplerccc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17019",
      "authors": [
        {
          "_id": "683041f868160a3c0e525cae",
          "name": "Chenhao Zhang",
          "hidden": false
        },
        {
          "_id": "683041f868160a3c0e525caf",
          "name": "Yazhe Niu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:53.000Z",
      "submittedOnDailyAt": "2025-05-23T08:11:12.176Z",
      "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework\n\n이 글의 한국어 번역은 다음과 같습니다:\n\n\"Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework\"\n\n이 번역은 원본 문장의 내용이 그대로 유지되었으며, 전문성과 정확성을 유지합니다.",
      "submittedOnDailyBy": {
        "_id": "647daf00cfca67bc50f9a99f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647daf00cfca67bc50f9a99f/8Snmk1V6lZ8ecdTW3POfm.jpeg",
        "isPro": false,
        "fullname": "Chenhao(Leo) Zhang",
        "user": "MING-ZCH",
        "type": "user"
      },
      "summary": "이미지의 메타포를 이해하는 것은 AI 시스템에서 중요한 문제로, 현재의 모델들은 이미지 콘텐츠에 포함된 微妙한 문화적, 감정적, 그리고 맥락적인 의미에 대한 이해가 어려워진다. 다모달 대언어 모델(MLLMs)은 기본적인 Visual Question Answer(VQA) 작업에서 뛰어난 성능을 보지만, 이미지의 의미에 대한 이해는 맥락적인 결함이로 기본적인 한계를 가짐에 따라, 인간의 인지 처리 과정에 영감을 받아 \"Let Androids Dream(LAD)\"라는 새로운 프레임워크를 제안한다. LAD는 맥락적인 결함을 해결하기 위한 3단계 프레임워크를 사용하며, (1) 시각 정보를 풍부한 다레벨의 텍스트 표현으로 변환하는 \"Perception\", (2) 간접적으로 결함을 해결하고 맥락적인 지식을 통합하는 \"Search\", (3) 명시적인 이유로 맥락적인 이미지의 의미를 생성하는 \"Reasoning\"이다. Lightweight GPT-4o-mini 모델을 사용한 프레임워크는 영어 이미지 의미 벤치마크에서 15+MLLM보다 가장 先進한 성능을展现, 중국 벤치마크에서 큰 향상을 보였으며, Multiple-Choice Question(MCQ)에서 GPT-4o와 같은 성능을 보여주고, Open-Style Question(OSQ)에서 36.7% 이상의 개선을 보였다. 이 프로젝트는 시각 언어 추론과 인간-AI 상호작용 분야에서 AI가 이미지의 의미에 대해 어떻게 이해할 수 있는지에 대해 새로운 지침을 제공하며, 공개적으로 사용될 수 있다(https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep).",
      "upvotes": 1,
      "discussionId": "683041f968160a3c0e525cf2",
      "githubRepo": "https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep",
      "ai_summary": "LAD, a three-stage framework using GPT-4o-mini, achieves state-of-the-art performance in image implication understanding and reasoning tasks across different languages and question types.",
      "ai_keywords": [
        "Visual Question Answering (VQA)",
        "image implication understanding",
        "reasoning",
        "multimodal large language models (MLLMs)",
        "cross-domain knowledge",
        "context-alignment",
        "Multiple-Choice Question (MCQ)",
        "Open-Style Question (OSQ)"
      ]
    },
    "publishedAt": "2025-05-22T13:59:53.000Z",
    "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication\n  Understanding and Reasoning Framework",
    "summary": "Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17019.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647daf00cfca67bc50f9a99f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647daf00cfca67bc50f9a99f/8Snmk1V6lZ8ecdTW3POfm.jpeg",
      "fullname": "Chenhao(Leo) Zhang",
      "name": "MING-ZCH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16612",
      "authors": [
        {
          "_id": "6830410d37efd0e958fcaf9c",
          "name": "Daniel Scalena",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcaf9d",
          "user": {
            "_id": "5e7749883d77a72421292d07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
            "isPro": false,
            "fullname": "Gabriele Sarti",
            "user": "gsarti",
            "type": "user"
          },
          "name": "Gabriele Sarti",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T09:35:27.088Z",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcaf9e",
          "name": "Arianna Bisazza",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcaf9f",
          "name": "Elisabetta Fersini",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcafa0",
          "name": "Malvina Nissim",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/NW7x7zlN8SRZybdNR_ad1.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/Y5HbXIQzjGZjHUu99IGGT.png"
      ],
      "publishedAt": "2025-05-22T12:47:16.000Z",
      "submittedOnDailyAt": "2025-05-23T08:07:20.310Z",
      "title": "스テアリング・ラーグ・ラング・ラング・モデルフォラマッションマシントレンストレーションパーソナリゼーション\n\n(Note: The provided text is already in Korean, so no translation is necessary. If you intended to request a translation from English to Korean, please provide the English text for translation.)",
      "submittedOnDailyBy": {
        "_id": "5e7749883d77a72421292d07",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
        "isPro": false,
        "fullname": "Gabriele Sarti",
        "user": "gsarti",
        "type": "user"
      },
      "summary": "고품질의 대형 언어 모델(LLMs)에 기반한 기계 번역 시스템은 특정 스타일이론 제약을 반영하는 포탈 번역의 생산을 쉽게 만들었습니다. 그러나 스타일이론의 요구가 불분명하거나 전달이 쉬운 경우, 이러한 시스템은 어려움을 겪습니다. 우리는 LLM 생성의 포탈화를 개별화하기 위한 다양한 전략을 검토하고, 어려운 문학 번역 분야를 중점적으로 다루고 있습니다. 우리는 개별화된 스타일에 대한 모델 생성을 제어하기 위해 제시 전략과 추론 시의 개입을 검토하고, 희소 자동 인코더에서 추출되는 잠재적 개념을 활용한 비교적인 프레임워크를 제안합니다. 우리의 결과에 따르면 제어는 개인화의 강화와 동시에 번역 품질을 유지합니다. 또한 제어 효과에 의한 LLM 표현의 영향을 검토하고, 개인화에 관련된 모델 레이어는 동일한 영향을 받습니다. 이는 동일한 기관이 운영되고 있음을 나타냅니다.",
      "upvotes": 1,
      "discussionId": "6830410d37efd0e958fcafd5",
      "ai_summary": "Strategies including prompting and contrastive frameworks using latent concepts from sparse autoencoders effectively personalize LLM translations in low-resource settings while maintaining quality.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "literary translation",
        "prompting strategies",
        "inference-time interventions",
        "steering",
        "contrastive framework",
        "latent concepts",
        "sparse autoencoders",
        "personalization properties",
        "translation quality",
        "multi-shot prompting"
      ]
    },
    "publishedAt": "2025-05-22T08:47:16.000Z",
    "title": "Steering Large Language Models for Machine Translation Personalization",
    "summary": "High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/NW7x7zlN8SRZybdNR_ad1.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/Y5HbXIQzjGZjHUu99IGGT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16612.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e7749883d77a72421292d07",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
      "fullname": "Gabriele Sarti",
      "name": "gsarti",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 224
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16170",
      "authors": [
        {
          "_id": "68301340cc0d12cd873342e1",
          "user": {
            "_id": "65c0de12efbb14b39c97f78e",
            "avatarUrl": "/avatars/18485f79427a35bd9e19f71b67c88dce.svg",
            "isPro": false,
            "fullname": "Yuqing Yang",
            "user": "ayyyq",
            "type": "user"
          },
          "name": "Yuqing Yang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T06:35:23.129Z",
          "hidden": false
        },
        {
          "_id": "68301340cc0d12cd873342e2",
          "name": "Robin Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:16:00.000Z",
      "submittedOnDailyAt": "2025-05-23T05:10:31.730Z",
      "title": "모델의 신뢰성과 리트랙션의 역할 이해하기 - \"LLMs가 어떻게 오류를 인식하는지\"",
      "submittedOnDailyBy": {
        "_id": "65c0de12efbb14b39c97f78e",
        "avatarUrl": "/avatars/18485f79427a35bd9e19f71b67c88dce.svg",
        "isPro": false,
        "fullname": "Yuqing Yang",
        "user": "ayyyq",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 자신이 알고 있어야 할 것보다 더 잘 알고 있을 때 오류를 인정할 수 있는지를 질문한다. 본 연구에서는 이전에 생성한 대답의 오류를 인정하는 행동을 \"후퇴\"라고 정의하고, LLMs가 어떤 상황에서 왜 후퇴하는지를 이해하고자 한다. 먼저, 모델에 고유한 데이터 세트를 구축하여 모델이 자신의 파라미터 지식에 반하는 오류를 후퇴하는 것을 평가한다. LLMs는 후퇴의 능력을 가지고 있지만, 그는 자주 이루어지지 않는다. 후퇴는 모델의 내부적인 믿음과 관련이 있는 것을 보여주고, \"신뢰하고 있는\" 오류를 후퇴하지 않는 것을 보여주고 있다. 지도 실험은 내부적인 믿음이 후퇴를 영향을 미치는 원인적 관계를 보여주고, 모델이 자신의 대답을 믿지 않을 때 대답을 확인하는 시도를 촉구하고, 자각식 인식의 때의 행동을 변화시키는 것을 보여주고 있다. 마지막으로, 간단한 슈퍼바이저 미세 조정은 모델이 더욱 정확한 내부적인 믿음을 배우는 것을 통해 후퇴의 성능을 크게 향상시키는 것을 보여주고 있다. 코드와 데이터 세트는 https://github.com/ayyyq/llm-retraction에 공개되어 있다.",
      "upvotes": 1,
      "discussionId": "68301341cc0d12cd87334304",
      "githubRepo": "https://github.com/ayyyq/llm-retraction",
      "ai_summary": "LLMs rarely retract incorrect answers they believe to be factually correct, but supervised fine-tuning can improve their retraction performance by refining their internal beliefs.",
      "ai_keywords": [
        "retraction",
        "model-specific datasets",
        "parametric knowledge",
        "internal belief",
        "self-verification",
        "attention behavior",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-05-21T23:16:00.000Z",
    "title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model\n  Belief in Retraction",
    "summary": "Can large language models (LLMs) admit their mistakes when they should know\nbetter? In this work, we define the behavior of acknowledging errors in\npreviously generated answers as \"retraction\" and aim to understand when and why\nLLMs choose to retract. We first construct model-specific datasets to evaluate\nwhether a model will retract an incorrect answer that contradicts its own\nparametric knowledge. While LLMs are capable of retraction, they do so only\ninfrequently. We demonstrate that retraction is closely tied to previously\nidentified indicators of models' internal belief: models fail to retract wrong\nanswers that they \"believe\" to be factually correct. Steering experiments\nfurther demonstrate that internal belief causally influences model retraction.\nIn particular, when the model does not believe its answer, this not only\nencourages the model to attempt to verify the answer, but also alters attention\nbehavior during self-verification. Finally, we demonstrate that simple\nsupervised fine-tuning significantly improves retraction performance by helping\nthe model learn more accurate internal beliefs. Code and datasets are available\non https://github.com/ayyyq/llm-retraction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16170.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c0de12efbb14b39c97f78e",
      "avatarUrl": "/avatars/18485f79427a35bd9e19f71b67c88dce.svg",
      "fullname": "Yuqing Yang",
      "name": "ayyyq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16088",
      "authors": [
        {
          "_id": "68302befc518550cc0c2e505",
          "name": "Gagan Bhatia",
          "hidden": false
        },
        {
          "_id": "68302befc518550cc0c2e506",
          "name": "Maxime Peyrard",
          "hidden": false
        },
        {
          "_id": "68302befc518550cc0c2e507",
          "name": "Wei Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T00:06:29.000Z",
      "submittedOnDailyAt": "2025-05-23T06:35:22.205Z",
      "title": "데이터 프레임 워크: 시간적 추론의 은닉 토큰나이저 백로킹",
      "submittedOnDailyBy": {
        "_id": "60394599033b61166496163b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614366097007-noauth.jpeg",
        "isPro": false,
        "fullname": "Gagan Bhatia",
        "user": "gagan3012",
        "type": "user"
      },
      "summary": "현대의 BPE 토크나이저는 날짜를 의미없는 플래그먼트로 분할하는 경우가 많습니다. 예를 들어 20250312 → 202,503,12로 분할되어 토큰 카운트를 증가시키고, 강력한 시간적 논리 구조를 숨겨줍니다. 본 연구에서는 (1) 단순하고 해석 가능한 지표를 소개하고, 이를 날짜 플래그먼트 비율로 부르며, 이는 멀티 디지트의 날짜 구성 요소를 어떻게 정확히 보존하는지 평가합니다. (2) DateAugBench를 릴리스하고, 6500개의 사례를 포함하는 시스템을 제공하며, 세 가지 시간적 논리 태스크에 대응합니다: 맥락 기반의 날짜 해결, 포맷 불변성 퍼즐, 역사적, 현대적, 미래적인 레지렘의 날짜 연산. (3) 각 층의 프로브와因果적 注意 훔훔 분석을 통해, 대규모 언어 모델이 월, 일, 년의 구성 요소를 결합하여 시간적 논리를 수행하는 에미징 데이터 추상 구조를 밝혀냅니다. 실험 결과를 통해, 과도한 플래그먼트화는 역사적나 미래적인 날짜와 같은 비관측적인 날짜의 정확도를 10점 정도 낮춥니다. 또한, 모델의 크기가 커질수록 날짜 플래그먼트를 치료하는 에미징 데이터 추상은 빠르게 진행됩니다. 마지막으로, LLM이 플래그먼트를 조합하는 추론 패스를 관찰하며, 일반적으로 인간이 해석하는 방식과 다릅니다 (년 → 월 → 일).",
      "upvotes": 1,
      "discussionId": "68302bf0c518550cc0c2e52c",
      "ai_summary": "New DateAugBench benchmarks reveal how modern tokenizers fragment dates, impacting the accuracy of temporal reasoning in large language models, which compensate for fragmentation more effectively as they grow larger.",
      "ai_keywords": [
        "BPE tokenizers",
        "date fragmentation ratio",
        "DateAugBench",
        "temporal reasoning tasks",
        "context-based date resolution",
        "format-invariance puzzles",
        "date arithmetic",
        "layer-wise probing",
        "causal attention-hop analyses",
        "date-abstraction mechanism",
        "large language models",
        "historical dates",
        "futuristic dates"
      ]
    },
    "publishedAt": "2025-05-21T20:06:29.000Z",
    "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal\n  Reasoning",
    "summary": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 rightarrow 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future\nregimes; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year rightarrow month rightarrow\nday).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60394599033b61166496163b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614366097007-noauth.jpeg",
      "fullname": "Gagan Bhatia",
      "name": "gagan3012",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15865",
      "authors": [
        {
          "_id": "68301a21694f7a58a32919a8",
          "name": "Ingeol Baek",
          "hidden": false
        },
        {
          "_id": "68301a21694f7a58a32919a9",
          "name": "Hwan Chang",
          "hidden": false
        },
        {
          "_id": "68301a21694f7a58a32919aa",
          "name": "Sunghyun Ryu",
          "hidden": false
        },
        {
          "_id": "68301a21694f7a58a32919ab",
          "name": "Hwanhee Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T10:53:41.000Z",
      "submittedOnDailyAt": "2025-05-23T05:19:58.152Z",
      "title": "비전-언어 모델은 이미지 속의 텍스트를 어떻게 인식하는지를 설명하고, OCR 헤드의 특징적인 역할을 명확히 합니다.",
      "submittedOnDailyBy": {
        "_id": "63f6f245e94ed998c46316df",
        "avatarUrl": "/avatars/9c0ec8682d4a85b96d2180602b1bbe6c.svg",
        "isPro": false,
        "fullname": "ingeolbaek",
        "user": "ingeol",
        "type": "user"
      },
      "summary": "라르지비지조ン 라ングゲー지 모델(LVLMs)에서 뚜렷한 진보가 있는 데에도, 특히 해석성 및 이미지 내 문맥 정보의 검출과 해석에 대한 오류가 남아 있습니다. 본 논문에서는 이러한 LVLMs를 검토하고, 이미지에서 문자를 인식하기 위해 책임 있는 특정 헤드를 식별합니다. 이러한 헤드를 OCR 헤드로 부르겠습니다. 이러한 헤드에 대한 발견은 다음과 같습니다: 1) 적어도 희소하지 않음: 이전의 검색 헤드와 달리, 이미지에서 문맥 정보를 추출하기 위해 많은 헤드가 활성화 됩니다. 2) 질적으로 다른: OCR 헤드는 일반적인 검색 헤드와 달리, 특성에 대한 낮은 유사성을 나타냅니다. 3) 동적으로 활성화됨: 이러한 헤드의 활성화 빈도는 OCR 스코어와 밀접하게 일치합니다. 이러한 발견은 Chain-of-Thought(CoT)를 OCR 및 일반적인 검색 헤드에 적용하고, 이러한 헤드를 마스크하여 하류 태스크로 검증했습니다. 또한, OCR 헤드 내부에서 싱크 토큰의 값을 재배치함으로써 성능 향상을 확인했습니다. 이러한 통찰력은 LVLMs가 이미지 내의 내장 문자 정보를 처리할 때 사용하는 내부 구조에 대해 더 깊은 이해를 제공합니다.",
      "upvotes": 1,
      "discussionId": "68301a22694f7a58a32919ef",
      "ai_summary": "The study identifies and analyzes OCR Heads within Large Vision Language Models, revealing their unique activation patterns and roles in interpreting text within images.",
      "ai_keywords": [
        "Large Vision Language Models",
        "LVLMs",
        "Optical Character Recognition Head",
        "OCR Head",
        "retrieval heads",
        "Chain-of-Thought",
        "CoT",
        "sink-token values"
      ]
    },
    "publishedAt": "2025-05-21T06:53:41.000Z",
    "title": "How Do Large Vision-Language Models See Text in Image? Unveiling the\n  Distinctive Role of OCR Heads",
    "summary": "Despite significant advancements in Large Vision Language Models (LVLMs), a\ngap remains, particularly regarding their interpretability and how they locate\nand interpret textual information within images. In this paper, we explore\nvarious LVLMs to identify the specific heads responsible for recognizing text\nfrom images, which we term the Optical Character Recognition Head (OCR Head).\nOur findings regarding these heads are as follows: (1) Less Sparse: Unlike\nprevious retrieval heads, a large number of heads are activated to extract\ntextual information from images. (2) Qualitatively Distinct: OCR heads possess\nproperties that differ significantly from general retrieval heads, exhibiting\nlow similarity in their characteristics. (3) Statically Activated: The\nfrequency of activation for these heads closely aligns with their OCR scores.\nWe validate our findings in downstream tasks by applying Chain-of-Thought (CoT)\nto both OCR and conventional retrieval heads and by masking these heads. We\nalso demonstrate that redistributing sink-token values within the OCR heads\nimproves performance. These insights provide a deeper understanding of the\ninternal mechanisms LVLMs employ in processing embedded textual information in\nimages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15865.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f6f245e94ed998c46316df",
      "avatarUrl": "/avatars/9c0ec8682d4a85b96d2180602b1bbe6c.svg",
      "fullname": "ingeolbaek",
      "name": "ingeol",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14395",
      "authors": [
        {
          "_id": "682db49f167398cff979ec27",
          "user": {
            "_id": "654f3cca8cc59d5b490b805b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654f3cca8cc59d5b490b805b/WAZZjN4q8VGp3VS2lAWyy.png",
            "isPro": false,
            "fullname": "Seyoung Song",
            "user": "seyoungsong",
            "type": "user"
          },
          "name": "Seyoung Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T12:27:01.867Z",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec28",
          "name": "Seogyeong Jeong",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec29",
          "name": "Eunsu Kim",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2a",
          "name": "Jiho Jin",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2b",
          "name": "Dongkwan Kim",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2c",
          "name": "Jay Shin",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2d",
          "user": {
            "_id": "60e0251ea9b5d8282481f2b7",
            "avatarUrl": "/avatars/43441373af054a6184c22097bfeb97e4.svg",
            "isPro": false,
            "fullname": "Alice Oh",
            "user": "aliceoh",
            "type": "user"
          },
          "name": "Alice Oh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T19:06:44.285Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/654f3cca8cc59d5b490b805b/cxsyLLbll1-fv-itFCzp_.png"
      ],
      "publishedAt": "2025-05-20T14:14:00.000Z",
      "submittedOnDailyAt": "2025-05-23T07:02:40.832Z",
      "title": "MUG-Eval: 다언어 생성의 가상 평가 프레임워크\n  언어마다 가능한 능력",
      "submittedOnDailyBy": {
        "_id": "654f3cca8cc59d5b490b805b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654f3cca8cc59d5b490b805b/WAZZjN4q8VGp3VS2lAWyy.png",
        "isPro": false,
        "fullname": "Seyoung Song",
        "user": "seyoungsong",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)의 텍스트 생성 능력 평가는 어려움으로, 특히 직접 평가의 방법이 적은 저 자원 언어에서 특히 그렇습니다. 우리는 MUG-Eval라는 새로운 프레임워크를 제안합니다. 이 프레임워크는 기존 벤치마크를 대화 태스크로 변환하여, LLMs의 언어별 생성 능력을 평가합니다. 특히, 이러한 대화 태스크는 목표 언어에서 효과적인 커뮤니케이션이 필요로 되어 설계되었습니다. 그리고 이러한 태스크의 성공률은 대화 생성의 성공을代理로 사용합니다. 우리의 접근 방식은 두 가지 주요 장점을 제공합니다. 하나는 언어 고유의 NLP 도구나 注釈된 데이터 세트에 의존하지 않는 것입니다. 두 번째는 LLMs-as-judges에 의존하지 않는 것입니다. 따라서 평가의 품질은 다수의 고 자원 언어의 범위 외에서 떨어지지 않습니다. 우리는 8개의 LLMs를 30언어의 범위로 평가했습니다. 이는 고 자원, 중 자원, 저 자원 카테고리를 포함합니다. 우리는 MUG-Eval은 기존 벤치마크와 강한 상관관계를 가지고 있으며, 언어나 모델의 표준화로 비교가 가능합니다. 우리의 프레임워크는 언어별 생성 평가에 강력한 자원 효율적인 해결책을 제공하며, 수천의 언어에 확장할 수 있습니다.",
      "upvotes": 1,
      "discussionId": "682db4a0167398cff979ec67",
      "ai_summary": "MUG-Eval assesses LLMs' multilingual generation by transforming benchmarks into conversational tasks, offering a language-independent and NLP tool-free method that correlates well with established benchmarks.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "multilingual generation",
        "conversational tasks",
        "task success rate",
        "low-resource languages",
        "high-resource languages",
        "NLP tools",
        "annotated datasets",
        "MUG-Eval",
        "standardized comparisons"
      ]
    },
    "publishedAt": "2025-05-20T10:14:00.000Z",
    "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language",
    "summary": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks (r > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/654f3cca8cc59d5b490b805b/cxsyLLbll1-fv-itFCzp_.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14395.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654f3cca8cc59d5b490b805b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654f3cca8cc59d5b490b805b/WAZZjN4q8VGp3VS2lAWyy.png",
      "fullname": "Seyoung Song",
      "name": "seyoungsong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16048",
      "authors": [
        {
          "_id": "68302100d260f25aad14b1c7",
          "user": {
            "_id": "62a1e17591f85abff79c2cdf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
            "isPro": false,
            "fullname": "Philipp Siedler",
            "user": "philippds",
            "type": "user"
          },
          "name": "Philipp D. Siedler",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T07:26:08.014Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T22:00:20.000Z",
      "submittedOnDailyAt": "2025-05-23T05:49:26.287Z",
      "title": "SPhyR: 물질 분포에 대한 공간 물리 설명 벤치마크",
      "submittedOnDailyBy": {
        "_id": "62a1e17591f85abff79c2cdf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
        "isPro": false,
        "fullname": "Philipp Siedler",
        "user": "philippds",
        "type": "user"
      },
      "summary": "여기서 우리는 토폴로지 최적화에 기반한 새로운 데이터 세트를 소개합니다. 토폴로지 최적화는 지정된 하중과 지지점 조건하에 설계 공간 내의 최적의 재료 분포를 계산하는 방법입니다. 이 데이터 세트에서는 LLM(대규모 언어 모델)에 2차원 경계 조건, 적용된 힘과 지지점 등 조건을 제공하여 발생하는 최적의 재료 분포에 대한 이유를 설명하는 것이 필요합니다. 이 데이터 세트에는 부분 구조 내의 마스크된 영역을 채우는 작업부터 완전한 재료 분포를 예측하는 작업까지 다양한 작업이 포함되어 있습니다. 이러한 작업들을 해결하기 위해서는 주어진 제약 조건하에서의 힘의 흐름과 필요한 재료 분포를 이해하고, 시뮬레이션 도구나 명확한 물리 모델에 의존하지 않도록 이유를 설명하는 것이 필요합니다. 이로써 구조의 안정성과 공간의 조직을 이유를 설명하는 것이 과제입니다. 우리 데이터 세트는 2차원 설정에서 공간적 및 물리적 이유 능력 평가를 목표로하고, 전통적인 언어 및 논리 벤치마크에 대한 보완적인 시각을 제공합니다.",
      "upvotes": 0,
      "discussionId": "68302101d260f25aad14b215",
      "githubRepo": "https://github.com/philippds/SPhyR",
      "ai_summary": "A dataset benchmarks spatial and physical reasoning of LLMs using topology optimization tasks without simulation tools.",
      "ai_keywords": [
        "Large Language Models (LLM)",
        "topology optimization",
        "material distribution",
        "structural stability",
        "spatial reasoning",
        "physical reasoning",
        "boundary conditions",
        "applied forces",
        "supports"
      ]
    },
    "publishedAt": "2025-05-21T18:00:20.000Z",
    "title": "SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution",
    "summary": "We introduce a novel dataset designed to benchmark the physical and spatial\nreasoning capabilities of Large Language Models (LLM) based on topology\noptimization, a method for computing optimal material distributions within a\ndesign space under prescribed loads and supports. In this dataset, LLMs are\nprovided with conditions such as 2D boundary, applied forces and supports, and\nmust reason about the resulting optimal material distribution. The dataset\nincludes a variety of tasks, ranging from filling in masked regions within\npartial structures to predicting complete material distributions. Solving these\ntasks requires understanding the flow of forces and the required material\ndistribution under given constraints, without access to simulation tools or\nexplicit physical models, challenging models to reason about structural\nstability and spatial organization. Our dataset targets the evaluation of\nspatial and physical reasoning abilities in 2D settings, offering a\ncomplementary perspective to traditional language and logic benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16048.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a1e17591f85abff79c2cdf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
      "fullname": "Philipp Siedler",
      "name": "philippds",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]