[
  {
    "paper": {
      "id": "2505.10554",
      "authors": [
        {
          "_id": "6826a569ea77771e3880f793",
          "user": {
            "_id": "64351475901c5734bcb64248",
            "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
            "isPro": false,
            "fullname": "Zhiyuan Hu",
            "user": "zhiyuanhucs",
            "type": "user"
          },
          "name": "Zhiyuan Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:02:17.850Z",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f794",
          "name": "Yibo Wang",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f795",
          "user": {
            "_id": "63a3ff69f91ad3ea5703841d",
            "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
            "isPro": false,
            "fullname": "Hanze Dong",
            "user": "hendrydong",
            "type": "user"
          },
          "name": "Hanze Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:08:50.665Z",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f796",
          "user": {
            "_id": "6602869253a0518b2a98cafd",
            "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
            "isPro": false,
            "fullname": "Yuhui Xu",
            "user": "yuhuixu",
            "type": "user"
          },
          "name": "Yuhui Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:09:07.114Z",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f797",
          "user": {
            "_id": "6461c2905dba83471db3be53",
            "avatarUrl": "/avatars/6e36cf86201d590ac729a75d4a439cde.svg",
            "isPro": false,
            "fullname": "Amrita Saha",
            "user": "amritasaha87",
            "type": "user"
          },
          "name": "Amrita Saha",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:09:22.879Z",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f798",
          "user": {
            "_id": "649dbcc4e0fff1ed099dc80a",
            "avatarUrl": "/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg",
            "isPro": false,
            "fullname": "Caiming Xiong",
            "user": "cxiong",
            "type": "user"
          },
          "name": "Caiming Xiong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:09:29.160Z",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f799",
          "user": {
            "_id": "651d8032c50012d33e914f2f",
            "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
            "isPro": false,
            "fullname": "Bryan Hooi",
            "user": "bhooi",
            "type": "user"
          },
          "name": "Bryan Hooi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:09:35.640Z",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f79a",
          "user": {
            "_id": "61f9d3b54ac99e8a1bae85f4",
            "avatarUrl": "/avatars/ac47d13204dd22452e4bc46e280842d5.svg",
            "isPro": false,
            "fullname": "JunnanLi",
            "user": "JunnanLi",
            "type": "user"
          },
          "name": "Junnan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:09:57.841Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:58:33.000Z",
      "submittedOnDailyAt": "2025-05-16T01:09:52.437Z",
      "title": "효과적인 발견을 초월하여：대규모 논리 모델에서 체계적인 메타 능력의 대응에 관한 방향\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "64351475901c5734bcb64248",
        "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
        "isPro": false,
        "fullname": "Zhiyuan Hu",
        "user": "zhiyuanhucs",
        "type": "user"
      },
      "summary": "대논리 모델(LRMs)는 장기적인 체인 오사인 논리에 잠재적인 능력을 가지고 있습니다. 이전의 연구는 결과 기반의 강화 학습(RL)이 자율적으로 발전적인 논리 비즈니스와 같은 것을 추출할 수 있음을 보여주었습니다. 예를 들어, 자기 보정, 백트래킹, 검증 현상 등 모델의 '아헴모닝'이라고 불리는 것을 포함합니다. 그러나 이러한 발생 비즈니스의 시간과 일관성이 예측할 수 없고 제어할 수 없는 상태이며, LRMs의 논리 능력의 scalability와 신뢰성에 한계가 있습니다. 이러한 한계를 해결하기 위해, 프론트 프로ン프트에 의존하여 자율적인 '아헴모닝'을 초과하여 3가지 메타 능력인 추론, 추론, 역추론을 명시적으로 모델에 맞추는 데 사용됩니다. 자동적으로 생성되며, 자기 효과적인 태스크를 사용합니다. 우리 3단계 파이프라인 개별의 결합, 파라미터 공간의 통합, 영역 전문의 강화 학습을 통해, 지시 튜닝 기반 라인에 대해 10% 이상의 성능을 향상시킵니다. 또한, 메타 능력의 명시적인 결합이 영역 전문의 강화 학습에서의 완벽한 체크포인트에서의 성능의 한계에 평균 2%의 추가 효과를 나타내며, 수학, 코딩, 과학 벤치마크를 포함합니다. 이는 명시적인 메타 능력의 결합이 scalability와 신뢰성을 가진 기반을 제공함을 보여주는 것입니다. 코드는 아래 URL에서 사용 가능합니다: https://github.com/zhiyuanhubj/Meta-Ability-Alignment",
      "upvotes": 54,
      "discussionId": "6826a56aea77771e3880f7c8",
      "githubRepo": "https://github.com/zhiyuanhubj/Meta-Ability-Alignment",
      "ai_keywords": [
        "large reasoning models",
        "long chain-of-thought reasoning",
        "outcome-based reinforcement learning",
        "RL",
        "self-correction",
        "backtracking",
        "verification phenomena",
        "aha moment",
        "meta-abilities",
        "deduction",
        "induction",
        "abduction",
        "automatically generated tasks",
        "self-verifiable tasks",
        "parameter-space merging",
        "domain-specific reinforcement learning",
        "performance ceiling",
        "math benchmarks",
        "coding benchmarks",
        "science benchmarks",
        "Meta-Ability-Alignment"
      ]
    },
    "publishedAt": "2025-05-15T13:58:33.000Z",
    "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models",
    "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10554.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64351475901c5734bcb64248",
      "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
      "fullname": "Zhiyuan Hu",
      "name": "zhiyuanhucs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09666",
      "authors": [
        {
          "_id": "68269a1eaa8aded616d280a0",
          "user": {
            "_id": "64cfa0b9749587dbe01d0079",
            "avatarUrl": "/avatars/93ca0a1d9c5578d052c5af0d4d1a0252.svg",
            "isPro": false,
            "fullname": "Yumin Choi",
            "user": "YuminChoi",
            "type": "user"
          },
          "name": "Yumin Choi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:06.309Z",
          "hidden": false
        },
        {
          "_id": "68269a1eaa8aded616d280a1",
          "user": {
            "_id": "63036b6c5c70c21d0ea79d48",
            "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
            "isPro": false,
            "fullname": "Jinheon Baek",
            "user": "jinheon",
            "type": "user"
          },
          "name": "Jinheon Baek",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:10:10.549Z",
          "hidden": false
        },
        {
          "_id": "68269a1eaa8aded616d280a2",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T16:46:15.000Z",
      "submittedOnDailyAt": "2025-05-16T01:05:44.315Z",
      "title": "마타라닝에 의한 시스템 프림 프로노트 최적화",
      "submittedOnDailyBy": {
        "_id": "63036b6c5c70c21d0ea79d48",
        "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
        "isPro": false,
        "fullname": "Jinheon Baek",
        "user": "jinheon",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 입력 프롬프트의 최적화가 성능을 최대화하는 데 중요한 역할을 수행하고 있습니다. 그러나 LLM의 프롬프트는 시스템 프롬프트와 특정한 사용자 프롬프트로 구성되어 있으며, 현재의 프롬프트 최적화 연구는 개별적인 요청이나 특정한 태스크에 특화된 사용자 프롬프트를 중심으로 시스템 프롬프트의 중요성을 크게 무시하고 있습니다. 이에 대처하여 시스템 프롬프트의 강건성과 새로운 태스크로의 전이 가능성에 대한 새로운 문제를 제안합니다. 이 문제를 해결하기 위해 학습 프레임워크를 제안하고, 데이터셋의 범위 내에서 다양한 사용자 프롬프트를 최적화하는 동시에 시스템 프롬프트를 원 학습하고, 동시에 반복적으로 사용자 프롬프트를 업데이트하여 상호작용을 보장합니다. 5개의 다른 분야의 14개의未见 데이터셋을 사용하여 실험을 수행했으며, 우리의 접근법은 다양한 사용자 프롬프트에도 적용 가능한 시스템 프롬프트를 생성합니다. 또한 우리의 발견은 최적화된 시스템 프롬프트는 새로운 태스크로의 신속한 적응을 가능하게 하고, 테스트 시의 사용자 프롬프트의 훈련 단계 수를 줄이면서 성능을 향상시키는 것을 보여줍니다.",
      "upvotes": 35,
      "discussionId": "68269a1eaa8aded616d280d1",
      "githubRepo": "https://github.com/Dozi01/MetaSPO",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "bilevel system prompt optimization",
        "meta-learning framework",
        "system prompts",
        "user prompts",
        "unseen datasets",
        "domains",
        "rapid adaptation",
        "test-time user prompts"
      ]
    },
    "publishedAt": "2025-05-14T12:46:15.000Z",
    "title": "System Prompt Optimization with Meta-Learning",
    "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09666.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63036b6c5c70c21d0ea79d48",
      "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
      "fullname": "Jinheon Baek",
      "name": "jinheon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09723",
      "authors": [
        {
          "_id": "6826b00c251d26fc0cd035cc",
          "user": {
            "_id": "63c20105726f62e411fbe882",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c20105726f62e411fbe882/2UsU9O2psbDjJzz-sAmGH.jpeg",
            "isPro": false,
            "fullname": "Yuxin Jiang",
            "user": "YuxinJiang",
            "type": "user"
          },
          "name": "Yuxin Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:10:30.068Z",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035cd",
          "user": {
            "_id": "6575f9aeca03b6c514fe6e5c",
            "avatarUrl": "/avatars/a6e9d428beaa124ee989d702b9bf4f85.svg",
            "isPro": false,
            "fullname": "Shengcong Chen",
            "user": "Shengcong",
            "type": "user"
          },
          "name": "Shengcong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:10:37.358Z",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035ce",
          "user": {
            "_id": "63c7a33121bd95f80ed74652",
            "avatarUrl": "/avatars/7dd59afea785a2bff0ec2b757abd474e.svg",
            "isPro": false,
            "fullname": "Siyuan Huang",
            "user": "thuhsy",
            "type": "user"
          },
          "name": "Siyuan Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:10:50.217Z",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035cf",
          "user": {
            "_id": "640b00555a9c21b95c6449b3",
            "avatarUrl": "/avatars/5fa43b956f3acc671f033e31b7ca76c5.svg",
            "isPro": false,
            "fullname": "Liliang Chen",
            "user": "pathcn",
            "type": "user"
          },
          "name": "Liliang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:10:55.980Z",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d0",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d1",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d2",
          "name": "Xindong He",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d3",
          "name": "Chiming Liu",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d4",
          "user": {
            "_id": "65c04e9c27a5fdca81abcbd9",
            "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
            "isPro": false,
            "fullname": "Hongsheng LI",
            "user": "hsli-cuhk",
            "type": "user"
          },
          "name": "Hongsheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:11:29.857Z",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d5",
          "user": {
            "_id": "67739bfa64e8b7438ae68eb4",
            "avatarUrl": "/avatars/15193bfbce487b2de4ce8c86bd18885a.svg",
            "isPro": false,
            "fullname": "Maoqing Yao",
            "user": "AutobotZero",
            "type": "user"
          },
          "name": "Maoqing Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:11:36.940Z",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d6",
          "user": {
            "_id": "646ec9b135f55eb49e405faa",
            "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
            "isPro": false,
            "fullname": "Guanghui Ren",
            "user": "sundrops",
            "type": "user"
          },
          "name": "Guanghui Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:11:44.407Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/PBKoxKQrSb2bFzjsx2Dta.gif"
      ],
      "publishedAt": "2025-05-14T18:30:53.000Z",
      "submittedOnDailyAt": "2025-05-16T02:11:01.174Z",
      "title": "EnerVerse-AC: 액션 조건付의 구체적인 환경의 재구성",
      "submittedOnDailyBy": {
        "_id": "634e4120038b5879133552f5",
        "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
        "isPro": true,
        "fullname": "Siyuan",
        "user": "SiyuanH",
        "type": "user"
      },
      "summary": "로봇의 이마치어닝 학습은 정적 작업에서 동작의 상호작용 시나리오를 해결하는 데 발전했습니다が, 실시간 동적 환경과의 상호작용이 필요로 되어 테스트 및 평가가 고가의 어려움이 됩니다. 우리는 실제적인 로봇 추론을 제어할 수 있는 행동 조건付き의 세계 모델인 EnerVerse-AC(EVAC)를 제안합니다. 기존의 아키텍처를 기반으로 EVAC는 동적인 다각도 이미지 생성을 위해 다양한 수준의 행동 조건付き 구조와 라이의 맵 인코딩을 도입하여, 다양한 실패 경로를 포함하는 훈련 데이터를 확장하고 일반화에 개선합니다. EVAC는 데이터 엔진과 평가자 모두가 되는 시스템으로, 인간이 수집한 경로를 다양한 데이터 세트로 변환하여 정책 테스트를 위해 실제적인 행동 조건付き 비디오 관찰을 생성하고 물리적인 로봇이나 복잡한 시뮬레이션의 필요성을 제거합니다. 이 접근법은 로봇 조작 평가의 품질을 유지하면서 비용의 절감이 가능합니다. 확장된 실험은 우리 방법의 효과성을 입증합니다. 코드, 체크포인트, 데이터 세트는 <https://annaj2178.github.io/EnerverseAC.github.io>에서 찾을 수 있습니다.",
      "upvotes": 15,
      "discussionId": "6826b013251d26fc0cd037ba",
      "githubRepo": "https://github.com/AgibotTech/EnerVerse-AC",
      "ai_keywords": [
        "action-conditional world model",
        "future visual observations",
        "multi-level action-conditioning mechanism",
        "ray map encoding",
        "dynamic multi-view image generation",
        "diverse failure trajectories",
        "data engine",
        "evaluator",
        "human-collected trajectories",
        "diverse datasets",
        "action-conditioned video observations",
        "robotic manipulation evaluation"
      ]
    },
    "publishedAt": "2025-05-14T14:30:53.000Z",
    "title": "EnerVerse-AC: Envisioning Embodied Environments with Action Condition",
    "summary": "Robotic imitation learning has advanced from solving static tasks to\naddressing dynamic interaction scenarios, but testing and evaluation remain\ncostly and challenging due to the need for real-time interaction with dynamic\nenvironments. We propose EnerVerse-AC (EVAC), an action-conditional world model\nthat generates future visual observations based on an agent's predicted\nactions, enabling realistic and controllable robotic inference. Building on\nprior architectures, EVAC introduces a multi-level action-conditioning\nmechanism and ray map encoding for dynamic multi-view image generation while\nexpanding training data with diverse failure trajectories to improve\ngeneralization. As both a data engine and evaluator, EVAC augments\nhuman-collected trajectories into diverse datasets and generates realistic,\naction-conditioned video observations for policy testing, eliminating the need\nfor physical robots or complex simulations. This approach significantly reduces\ncosts while maintaining high fidelity in robotic manipulation evaluation.\nExtensive experiments validate the effectiveness of our method. Code,\ncheckpoints, and datasets can be found at\n<https://annaj2178.github.io/EnerverseAC.github.io>.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/PBKoxKQrSb2bFzjsx2Dta.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09723.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4120038b5879133552f5",
      "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
      "fullname": "Siyuan",
      "name": "SiyuanH",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10185",
      "authors": [
        {
          "_id": "68269f67a47cb2b87646b98c",
          "user": {
            "_id": "6550c4f27bbfce1878f5f280",
            "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
            "isPro": false,
            "fullname": "seongyun_lee",
            "user": "Seongyun",
            "type": "user"
          },
          "name": "Seongyun Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:14:03.612Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b98d",
          "user": {
            "_id": "6469949654873f0043b09c22",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6469949654873f0043b09c22/Lk7IJAR16Wa_sGJ2g81AQ.jpeg",
            "isPro": false,
            "fullname": "Seungone Kim",
            "user": "seungone",
            "type": "user"
          },
          "name": "Seungone Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:14:19.025Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b98e",
          "user": {
            "_id": "66f10ac605182775917d8c5a",
            "avatarUrl": "/avatars/21b21284d0a5a95413f91dde9dda346c.svg",
            "isPro": false,
            "fullname": "Minju Seo",
            "user": "Minju2136",
            "type": "user"
          },
          "name": "Minju Seo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:14:25.204Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b98f",
          "user": {
            "_id": "649e06313e5e7504763dfe03",
            "avatarUrl": "/avatars/1c4d19de5f2950d3342480c4b3e01047.svg",
            "isPro": false,
            "fullname": "Yongrae Jo",
            "user": "dreamgonfly",
            "type": "user"
          },
          "name": "Yongrae Jo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:14:30.806Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b990",
          "name": "Dongyoung Go",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b991",
          "user": {
            "_id": "647eaaf61a1fcad2fdc5d1ef",
            "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
            "isPro": false,
            "fullname": "Hyeonbin Hwang ",
            "user": "hbin0701",
            "type": "user"
          },
          "name": "Hyeonbin Hwang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:14:44.935Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b992",
          "user": {
            "_id": "638467ee8283412d401770dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638467ee8283412d401770dd/UAMwDHhSwf91XSsubfrS_.jpeg",
            "isPro": false,
            "fullname": "Jinho Park",
            "user": "Br3ad",
            "type": "user"
          },
          "name": "Jinho Park",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:14:51.443Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b993",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b994",
          "user": {
            "_id": "63e3f3c59db5da2dc1ef6889",
            "avatarUrl": "/avatars/f7546f57a5fd69bc99ff1640cc4a4853.svg",
            "isPro": false,
            "fullname": "Sean Welleck",
            "user": "wellecks",
            "type": "user"
          },
          "name": "Sean Welleck",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:15:17.990Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b995",
          "user": {
            "_id": "60de14638bedd2315529d43f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625166923504-noauth.png",
            "isPro": false,
            "fullname": "Graham Neubig",
            "user": "gneubig",
            "type": "user"
          },
          "name": "Graham Neubig",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:15:24.404Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b996",
          "name": "Moontae Lee",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b997",
          "user": {
            "_id": "621f05ba970615ad5861ceb1",
            "avatarUrl": "/avatars/7e1902aa71369a524afda9b0a9e88e22.svg",
            "isPro": false,
            "fullname": "Minjoon Seo",
            "user": "minjoon",
            "type": "user"
          },
          "name": "Minjoon Seo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:15:33.521Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T11:31:02.000Z",
      "submittedOnDailyAt": "2025-05-16T00:44:19.223Z",
      "title": "콘티뉴션・엔시언카바리: 론의 모델의 생각 방식을 분석, 예측, 제어하는 방법",
      "submittedOnDailyBy": {
        "_id": "6550c4f27bbfce1878f5f280",
        "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
        "isPro": false,
        "fullname": "seongyun_lee",
        "user": "Seongyun",
        "type": "user"
      },
      "summary": "긴 연속적 논리(CoT)는 현대의 대규모 언어 모델의 효과적인 사용에 중요한 요소 중 하나입니다. 그러나 이러한 모델의 능력의 근본적인 원리론 전략에 대한 이해는 제한되어 있습니다. 기존 연구에서는 모델이 생성한 CoT을 특정 전략 유형으로 분류하려고 시도하지만, 이러한 접근法是 인간의 직관에 제한되어 모델의 행동의 전체 다양성을 파악할 수 없습니다. 본 연구에서는 모델의 원리론을 분석하고 이를 제어하기 위한 기본적인 프레임워크를 도입합니다. 우리의 방법은 모델이 생성한 CoT에서 다양한 원리론의 기준을 자동으로 추출하고, 이를 семанти스 공간에 삽입하여 대표적인 카테고리로 클러스터링하여 원리론적인 행동을 해석하기 위한 비교적 규칙을 얻습니다. 인간 평가에 의해, 이 프레임워크는 현재의 방법보다 더 해석 가능하고 상세한 분석을 제공합니다. 또한 이러한 이해로 성능의 향상이 기대됩니다: 모델이 사용하는 전략을 예측하고, 더 효과적인 대체 방법을 안내할 수 있습니다. 마지막으로, 실용적인 통찰을 제공합니다: 데이터의 형식(예: 자유 형식과 복수 선택)이 데이터 영역보다 더 많은 영향을 미칠 수 있으며, 형식에 대한 모델 설계의 중요성을 강조합니다.",
      "upvotes": 14,
      "discussionId": "68269f68a47cb2b87646b9ed",
      "ai_keywords": [
        "long chain-of-thought (CoT)",
        "large language models",
        "reasoning strategies",
        "predefined strategy types",
        "CoT Encyclopedia",
        "bottom-up framework",
        "reasoning criteria",
        "semantic space",
        "contrastive rubrics",
        "reasoning behavior",
        "interpretability",
        "performance gains"
      ]
    },
    "publishedAt": "2025-05-15T07:31:02.000Z",
    "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think",
    "summary": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10185.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6550c4f27bbfce1878f5f280",
      "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
      "fullname": "seongyun_lee",
      "name": "Seongyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09694",
      "authors": [
        {
          "_id": "6826ae4611765454f5757d7c",
          "name": "Hu Yue",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d7d",
          "user": {
            "_id": "63c7a33121bd95f80ed74652",
            "avatarUrl": "/avatars/7dd59afea785a2bff0ec2b757abd474e.svg",
            "isPro": false,
            "fullname": "Siyuan Huang",
            "user": "thuhsy",
            "type": "user"
          },
          "name": "Siyuan Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:16:33.829Z",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d7e",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d7f",
          "user": {
            "_id": "6575f9aeca03b6c514fe6e5c",
            "avatarUrl": "/avatars/a6e9d428beaa124ee989d702b9bf4f85.svg",
            "isPro": false,
            "fullname": "Shengcong Chen",
            "user": "Shengcong",
            "type": "user"
          },
          "name": "Shengcong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:16:15.416Z",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d80",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d81",
          "user": {
            "_id": "640b00555a9c21b95c6449b3",
            "avatarUrl": "/avatars/5fa43b956f3acc671f033e31b7ca76c5.svg",
            "isPro": false,
            "fullname": "Liliang Chen",
            "user": "pathcn",
            "type": "user"
          },
          "name": "Liliang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:15:58.407Z",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d82",
          "user": {
            "_id": "67739bfa64e8b7438ae68eb4",
            "avatarUrl": "/avatars/15193bfbce487b2de4ce8c86bd18885a.svg",
            "isPro": false,
            "fullname": "Maoqing Yao",
            "user": "AutobotZero",
            "type": "user"
          },
          "name": "Maoqing Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:15:51.039Z",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d83",
          "user": {
            "_id": "646ec9b135f55eb49e405faa",
            "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
            "isPro": false,
            "fullname": "Guanghui Ren",
            "user": "sundrops",
            "type": "user"
          },
          "name": "Guanghui Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:15:44.660Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T18:00:19.000Z",
      "submittedOnDailyAt": "2025-05-16T01:55:39.761Z",
      "title": "EWMBench: 세계 모델의 시나리오, 동작, 그리고 의미질의 평가",
      "submittedOnDailyBy": {
        "_id": "634e4120038b5879133552f5",
        "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
        "isPro": true,
        "fullname": "Siyuan",
        "user": "SiyuanH",
        "type": "user"
      },
      "summary": "최근의 창의적인 AI의 발전으로, 언어 지시에 기반한 고품질의 이미지와 비디오의 합성이 가능해졌습니다. 이러한 발전에 따라, 문장을 기반으로 VIDEO의 확장 모델은 언어 명령에 의해 물리적으로 합리적인 시나리오를 생성할 수 있는 구체화된 월드 모델(EWMs)로 발전했습니다. 이 연구는 EWMs를 일반적인 시각적인 메트릭스보다 평가하기 위한 중요한 문제를 해결하고, 물리적으로 기반을 둔 및 행동적으로 일치하는 행동을 생성하는 것을 목표로 합니다. 우리는 시각적인 시나리오의 일치성, 움직임의 정확성, 세ман틱의 일치성 3가지의 핵심적인 측면에서 평가하기 위해, EWMBench(Embodied World Model Benchmark)을 제안했습니다. 우리의 접근법은 다양한 시나리오와 동작 패턴을 포함하는 정밀하게 제작된 데이터셋과, 상세한 다차원 평가 도구 패키지를 활용하여, 후보 모델을 평가하고 비교하기 위한 전문적인 프레임워크를 제공합니다. 제안된 벤치마크는 현재의 VIDEO 생성 모델이 구체화 태스크의 고유한 요구에 적합한지에 대한 제한을 밝혀주고, 미래의 발전에 대한 유익한 엔지니어링의 힌트를 제공합니다. 데이터셋과 평가 도구는 https://github.com/AgibotTech/EWMBench에서 공개되어 있습니다.",
      "upvotes": 13,
      "discussionId": "6826ae4911765454f5757e32",
      "ai_keywords": [
        "text-to-video diffusion models",
        "embodied world models",
        "physically plausible scenes",
        "language commands",
        "perceptual metrics",
        "visual scene consistency",
        "motion correctness",
        "semantic alignment",
        "Embodied World Model Benchmark (EWMBench)",
        "multi-dimensional evaluation toolkit",
        "video generation models"
      ]
    },
    "publishedAt": "2025-05-14T14:00:19.000Z",
    "title": "EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied\n  World Models",
    "summary": "Recent advances in creative AI have enabled the synthesis of high-fidelity\nimages and videos conditioned on language instructions. Building on these\ndevelopments, text-to-video diffusion models have evolved into embodied world\nmodels (EWMs) capable of generating physically plausible scenes from language\ncommands, effectively bridging vision and action in embodied AI applications.\nThis work addresses the critical challenge of evaluating EWMs beyond general\nperceptual metrics to ensure the generation of physically grounded and\naction-consistent behaviors. We propose the Embodied World Model Benchmark\n(EWMBench), a dedicated framework designed to evaluate EWMs based on three key\naspects: visual scene consistency, motion correctness, and semantic alignment.\nOur approach leverages a meticulously curated dataset encompassing diverse\nscenes and motion patterns, alongside a comprehensive multi-dimensional\nevaluation toolkit, to assess and compare candidate models. The proposed\nbenchmark not only identifies the limitations of existing video generation\nmodels in meeting the unique requirements of embodied tasks but also provides\nvaluable insights to guide future advancements in the field. The dataset and\nevaluation tools are publicly available at\nhttps://github.com/AgibotTech/EWMBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4120038b5879133552f5",
      "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
      "fullname": "Siyuan",
      "name": "SiyuanH",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10562",
      "authors": [
        {
          "_id": "6826a5d8154611642ada50da",
          "user": {
            "_id": "64c52b905e5bc55a9201a069",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qAgetu7v3KWc4jOjz85tP.jpeg",
            "isPro": false,
            "fullname": "Wenxuan Wang",
            "user": "gilnore",
            "type": "user"
          },
          "name": "Wenxuan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:16:46.660Z",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50db",
          "user": {
            "_id": "640ed40dc025ddf618950af7",
            "avatarUrl": "/avatars/a99abd4346fa649ad4144f284ebcc972.svg",
            "isPro": false,
            "fullname": "Fan Zhang",
            "user": "ryanzhangfan",
            "type": "user"
          },
          "name": "Fan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:04.101Z",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50dc",
          "user": {
            "_id": "648683de623b5f050213f2be",
            "avatarUrl": "/avatars/83ecbcf4a21f68d2893de79f0444d6e3.svg",
            "isPro": false,
            "fullname": "Yufeng Cui",
            "user": "YufengCui",
            "type": "user"
          },
          "name": "Yufeng Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:16:58.537Z",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50dd",
          "user": {
            "_id": "64b4a717aa03b6520839e9b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
            "isPro": false,
            "fullname": "Haiwen Diao",
            "user": "Paranioar",
            "type": "user"
          },
          "name": "Haiwen Diao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:17:04.879Z",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50de",
          "user": {
            "_id": "657187de7644d1128571495e",
            "avatarUrl": "/avatars/89412c94fd6136f6680055551de3ddc4.svg",
            "isPro": false,
            "fullname": "Zhuoyan Luo",
            "user": "RobertLuo1",
            "type": "user"
          },
          "name": "Zhuoyan Luo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:17:11.336Z",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50df",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50e0",
          "name": "Jing Liu",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50e1",
          "user": {
            "_id": "63ca558304c979828311c5a5",
            "avatarUrl": "/avatars/2a439d79fba2f987cabe780d10c94d25.svg",
            "isPro": false,
            "fullname": "Xinlong Wang",
            "user": "xinlongwang",
            "type": "user"
          },
          "name": "Xinlong Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:17:32.558Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/640ed40dc025ddf618950af7/c498wQOl3gwkr9MlW8sOO.png"
      ],
      "publishedAt": "2025-05-15T17:59:39.000Z",
      "submittedOnDailyAt": "2025-05-16T01:26:23.739Z",
      "title": "End-to-End Vision Tokenizer Tuning",
      "submittedOnDailyBy": {
        "_id": "640ed40dc025ddf618950af7",
        "avatarUrl": "/avatars/a99abd4346fa649ad4144f284ebcc972.svg",
        "isPro": false,
        "fullname": "Fan Zhang",
        "user": "ryanzhangfan",
        "type": "user"
      },
      "summary": "현재의 시각 토큰나이저화는 시각 토큰나이저의 최적화와 하류 훈련을 분리하고, 시각 토큰이 다양한 태스크(예: 이미지 생성과 시각 문제 답변)에서 잘 일반화할 수 있다는 단순한 가정을 하고 있습니다. 저레벨 재구성에 최적화된 시각 토큰나이저는 하류 태스크에 필요한 다양한 표현과 의미에 관계없이 되어 있습니다. 이 분리 패러다임에서 시각 토큰나이저의 손실이 목표 태스크의 표현 버틀넥인 됨을 중요한 오류라고 합니다. 예를 들어, 이미지 중 텍스트의 토큰나이징 오류는 인식하거나 생성할 때 불량한 결과를 내게 됩니다. 이에 대해 우리는 ETT(End-to-End Vision Tokenizer Tuning)을 제안합니다. ETT는 시각 토큰나이저와 목표의 자동 협조 태스크의 연계 최적화를 가능하게 합니다. 기존의 자동 협조 모델은 고정된 시각 토큰나이저로부터의 분산 인덱스만 사용했습니다지만, ETT는 토큰나이저 코더북의 시각 임베딩을 활용하여 재구성과 캡션 객체와 함께 시각 토큰나이저를 종단부터 최적화합니다. ETT는 현재의 훈련 파이프라인과 무난하게 통합할 수 있으며, 최소한의 아키텍처 변경이 필요합니다. 우리의 ETT는 구현 및 통합이 간단하며, 사용된 대규모 언어 모델의 원의 코더북이나 아키텍처를 변경할 필요가 없습니다. 확장된 실험은 ETT의 제안의 종단부터의 시각 토큰나이저 최적화가 고정된 토큰나이저 기반 선형과 비교하여, 다형성 이해와 시각 생성 태스크에서 2-6%의 성능 향상을 보여주었습니다. 우리는 이 매우 간단하고 강력한 방법이 이미지 생성과 이해 외에도 다형성 기반 모델을 강화할 수 있다는 것을 기대하고 있습니다.",
      "upvotes": 11,
      "discussionId": "6826a5d9154611642ada5122",
      "ai_keywords": [
        "vision tokenization",
        "end-to-end vision tokenizer tuning (ETT)",
        "autoregressive tasks",
        "visual embeddings",
        "tokenizer codebook",
        "multimodal understanding",
        "visual generation tasks"
      ]
    },
    "publishedAt": "2025-05-15T13:59:39.000Z",
    "title": "End-to-End Vision Tokenizer Tuning",
    "summary": "Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/640ed40dc025ddf618950af7/c498wQOl3gwkr9MlW8sOO.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10562.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640ed40dc025ddf618950af7",
      "avatarUrl": "/avatars/a99abd4346fa649ad4144f284ebcc972.svg",
      "fullname": "Fan Zhang",
      "name": "ryanzhangfan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.07782",
      "authors": [
        {
          "_id": "6822b3c8c10ac9c466c63e01",
          "user": {
            "_id": "6466e31a14e059dde8bbe4be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
            "isPro": true,
            "fullname": "Rushi Qiang",
            "user": "Jerrycool",
            "type": "user"
          },
          "name": "Rushi Qiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-14T07:35:59.704Z",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e02",
          "user": {
            "_id": "6471bddd609ae9f56368f132",
            "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
            "isPro": true,
            "fullname": "Yuchen Zhuang",
            "user": "yczhuang",
            "type": "user"
          },
          "name": "Yuchen Zhuang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:17:45.935Z",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e03",
          "user": {
            "_id": "68198e34612ca40b67abbf18",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Gj9eV6k3CcKJinTePC2CV.png",
            "isPro": false,
            "fullname": "Yinghao Li",
            "user": "yinghaoli-yh",
            "type": "user"
          },
          "name": "Yinghao Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:17:54.487Z",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e04",
          "name": "Dingu Sagar V K",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e05",
          "user": {
            "_id": "644a2c4e9a1c5faef7a5dbd8",
            "avatarUrl": "/avatars/fbbbc1347f8e423b2477e2506fdb43d9.svg",
            "isPro": false,
            "fullname": "Rongzhi Zhang",
            "user": "Solute",
            "type": "user"
          },
          "name": "Rongzhi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:47.628Z",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e06",
          "name": "Changhao Li",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e07",
          "name": "Ian Shu-Hei Wong",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e08",
          "name": "Sherry Yang",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e09",
          "user": {
            "_id": "6409651b9e9f790c905b2335",
            "avatarUrl": "/avatars/1fb8c80b60f21f65a0a027319101f236.svg",
            "isPro": false,
            "fullname": "Percy Liang",
            "user": "percyliang",
            "type": "user"
          },
          "name": "Percy Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:18:20.107Z",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e0a",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e0b",
          "user": {
            "_id": "635f93577c05eb9f59966209",
            "avatarUrl": "/avatars/add48d7e3790a6b500d6c451ef8b0f75.svg",
            "isPro": false,
            "fullname": "Intelligent Digital Creation",
            "user": "BoDai",
            "type": "user"
          },
          "name": "Bo Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:18:42.045Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T17:35:43.000Z",
      "submittedOnDailyAt": "2025-05-16T00:26:20.796Z",
      "title": "MLE-Dojo: 기계학습공학에서 LLM 에이전트의 능력 향상을 위한 상호작용 환경",
      "submittedOnDailyBy": {
        "_id": "6466e31a14e059dde8bbe4be",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
        "isPro": true,
        "fullname": "Rushi Qiang",
        "user": "Jerrycool",
        "type": "user"
      },
      "summary": "MLE-Dojo는 체계적인 강화학습, 평가, 개선을 위해 Gym처럼 작동하는 프레임워크입니다. 기존의 정적 데이터셋이나 한 번의 평가에 의존하는 벤치마크와 달리, MLE-Dojo는 인터랙티브 환경 제공하며, 출력을 통해 구조화된 플래그 루프를 통해 평가되고 해결책을 진보시킬 수 있게 됩니다. 200점 이상의 실제 세계의 Kaggle 챌린지에 기반하여, MLE-Dojo는 데이터 처리, 아키텍처 탐색, 하이퍼 파라미터 조정, 코드 디버깅 등 다양한, 개방적인 MLE 작업들을 검토하고, 실제적인 엔지니어링 시나리오를 반영한 것입니다. 완전히 실행 가능한 환경에서, 서브 풀링과 강화학습을 통해 출력을 평가하며, 복잡한 오류를 효율적으로 해결할 수 있게 됩니다. 8개의 최신 LLM의 검증에 따라, 현재의 모델은 의미 있는 여러 번의 개선을 실현하지만, 장기적 해결책의 자동 생성과 복잡한 오류의 효율적인 해결에는 상당한 한계가 있음을 명확히 나타났습니다. 또한, MLE-Dojo의 유연하고 확장 가능한 아키텍처는 다양한 데이터 소스, 도구, 평가 프로토콜을 쉽게 통합하며, 모델 기반의 출력 조정을 가능하게 하고, 상호 교환성, scalability, 재현성을 촉진합니다. 프레임워크와 벤치마크를 오픈소스로 하고, 다음 세대의 MLE 출력을 목표로 하는 커뮤니티 주도의 혁신을 촉진하는 것입니다.",
      "upvotes": 10,
      "discussionId": "6822b3c9c10ac9c466c63e8a",
      "projectPage": "https://mle-dojo.github.io/MLE-Dojo-page/",
      "githubRepo": "https://github.com/MLE-Dojo/MLE-Dojo",
      "ai_keywords": [
        "reinforcement learning",
        "autonomous large language model (LLM) agents",
        "iterative machine learning engineering (MLE) workflows",
        "Kaggle challenges",
        "MLE tasks",
        "data processing",
        "architecture search",
        "hyperparameter tuning",
        "code debugging",
        "supervised fine-tuning",
        "model-based agent tuning"
      ]
    },
    "publishedAt": "2025-05-12T13:35:43.000Z",
    "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine\n  Learning Engineering",
    "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement\nlearning, evaluating, and improving autonomous large language model (LLM)\nagents in iterative machine learning engineering (MLE) workflows. Unlike\nexisting benchmarks that primarily rely on static datasets or single-attempt\nevaluations, MLE-Dojo provides an interactive environment enabling agents to\niteratively experiment, debug, and refine solutions through structured feedback\nloops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,\nopen-ended MLE tasks carefully curated to reflect realistic engineering\nscenarios such as data processing, architecture search, hyperparameter tuning,\nand code debugging. Its fully executable environment supports comprehensive\nagent training via both supervised fine-tuning and reinforcement learning,\nfacilitating iterative experimentation, realistic data sampling, and real-time\noutcome verification. Extensive evaluations of eight frontier LLMs reveal that\nwhile current models achieve meaningful iterative improvements, they still\nexhibit significant limitations in autonomously generating long-horizon\nsolutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's\nflexible and extensible architecture seamlessly integrates diverse data\nsources, tools, and evaluation protocols, uniquely enabling model-based agent\ntuning and promoting interoperability, scalability, and reproducibility. We\nopen-source our framework and benchmarks to foster community-driven innovation\ntowards next-generation MLE agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6466e31a14e059dde8bbe4be",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
      "fullname": "Rushi Qiang",
      "name": "Jerrycool",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10558",
      "authors": [
        {
          "_id": "6826bc3cf032d8147549ac6b",
          "user": {
            "_id": "635eac5ea81c7f7424a23b8c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635eac5ea81c7f7424a23b8c/Cei8xiGF5jrwscDpMgftt.jpeg",
            "isPro": false,
            "fullname": "intchous",
            "user": "intchous",
            "type": "user"
          },
          "name": "Peiying Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-16T04:17:01.151Z",
          "hidden": false
        },
        {
          "_id": "6826bc3cf032d8147549ac6c",
          "user": {
            "_id": "6412b90e6e51a8e21887ff30",
            "avatarUrl": "/avatars/b0c3a8624a686481b9f609be96b1307c.svg",
            "isPro": false,
            "fullname": "Z",
            "user": "CHERRY-Z",
            "type": "user"
          },
          "name": "Nanxuan Zhao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-16T04:17:01.151Z",
          "hidden": false
        },
        {
          "_id": "6826bc3cf032d8147549ac6d",
          "name": "Jing Liao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:59:21.000Z",
      "submittedOnDailyAt": "2025-05-16T02:49:19.469Z",
      "title": "이미지 디퓨저에서 텍스트로부터 벡터의 스타일 커스터마이징\n\n초기화\n\n텍스트로부터 벡터의 스타일 커스터마이징은 이미지 디퓨저에서 중요한 요소로, 텍스트의 내용을 기반으로 이미지의 스타일을 조정하는 데 사용됩니다. 이 기술은 다양한 분야에서 활용되며, 예를 들어, 이미지 생성, 시각화, 그리고 콘텐츠 생성 분야에서 중요한 역할을 합니다. 벡터의 스타일 커스터마이징은 텍스트를 기반으로 이미지의 스타일을 조정하여, 사용자가 원하는 스타일을 쉽게 구현할 수 있게 해줍니다. 이 기술은 이미지 디퓨저에서 핵심 요소로, 다양한 응용 분야에서 중요한 역할을 합니다.",
      "submittedOnDailyBy": {
        "_id": "635eac5ea81c7f7424a23b8c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635eac5ea81c7f7424a23b8c/Cei8xiGF5jrwscDpMgftt.jpeg",
        "isPro": false,
        "fullname": "intchous",
        "user": "intchous",
        "type": "user"
      },
      "summary": "スケーラブルベクトルグラフィックス（SVGs）は、デザイナーにとって高い評価を受けており、解像度の独立性と整頓されたレイヤ構造によってさらに強力です。 既存のテキストからベクトル（T2V）生成手法は、SVGsをテキストプロンプトから生成することができますが、実用的なアプリケーションでの重要な必要性を飛ばしてしまいます：スタイルのカスタマイズ，これは、一致した可視的な外見とコラフィーな美術性を確保するために必要です。 現在のT2V手法をスタイルカスタマイズに拡張することは、特定の課題を抱えています。\n\n最適化基づきのT2Vモデルは、T2Iモデルの先驅を利用してスタイルカスタマイズを行うことができますが、構造的な正規性を維持することが難しいです。 逆に、前向きT2Vモデルは構造的な正規性を確保できますが、SVGの訓練データの限りによって内容とスタイルを分離することが難しくなります。\n\nこれらの課題に対処するために、私たちは、前向きT2VモデルとT2I画像の先驅の利点を活用する新型の2段階スタイルカスタマイズプインプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルン",
      "upvotes": 8,
      "discussionId": "6826bc3df032d8147549acac",
      "ai_keywords": [
        "T2V (text-to-vector)",
        "SVG (Scalable Vector Graphics)",
        "T2I (text-to-image)",
        "diffusion model",
        "path-level representation",
        "structural regularity",
        "expressive capabilities"
      ]
    },
    "publishedAt": "2025-05-15T13:59:21.000Z",
    "title": "Style Customization of Text-to-Vector Generation with Image Diffusion\n  Priors",
    "summary": "Scalable Vector Graphics (SVGs) are highly favored by designers due to their\nresolution independence and well-organized layer structure. Although existing\ntext-to-vector (T2V) generation methods can create SVGs from text prompts, they\noften overlook an important need in practical applications: style\ncustomization, which is vital for producing a collection of vector graphics\nwith consistent visual appearance and coherent aesthetics. Extending existing\nT2V methods for style customization poses certain challenges.\nOptimization-based T2V models can utilize the priors of text-to-image (T2I)\nmodels for customization, but struggle with maintaining structural regularity.\nOn the other hand, feed-forward T2V models can ensure structural regularity,\nyet they encounter difficulties in disentangling content and style due to\nlimited SVG training data.\n  To address these challenges, we propose a novel two-stage style customization\npipeline for SVG generation, making use of the advantages of both feed-forward\nT2V models and T2I image priors. In the first stage, we train a T2V diffusion\nmodel with a path-level representation to ensure the structural regularity of\nSVGs while preserving diverse expressive capabilities. In the second stage, we\ncustomize the T2V diffusion model to different styles by distilling customized\nT2I models. By integrating these techniques, our pipeline can generate\nhigh-quality and diverse SVGs in custom styles based on text prompts in an\nefficient feed-forward manner. The effectiveness of our method has been\nvalidated through extensive experiments. The project page is\nhttps://customsvg.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10558.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "635eac5ea81c7f7424a23b8c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635eac5ea81c7f7424a23b8c/Cei8xiGF5jrwscDpMgftt.jpeg",
      "fullname": "intchous",
      "name": "intchous",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10527",
      "authors": [
        {
          "_id": "682699da19c4a596dbcea4f5",
          "name": "Binghai Wang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f6",
          "name": "Runji Lin",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f7",
          "name": "Keming Lu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f8",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f9",
          "name": "Zhenru Zhang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fa",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fb",
          "name": "Chujie Zheng",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fc",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fd",
          "name": "Yang Fan",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fe",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4ff",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea500",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea501",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea502",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea503",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea504",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea505",
          "name": "Yu-Gang Jiang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea506",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea507",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea508",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:38:37.000Z",
      "submittedOnDailyAt": "2025-05-16T00:22:55.977Z",
      "title": "WorldPM: 인간의 취미 모델링의 스케일링",
      "submittedOnDailyBy": {
        "_id": "63d9d68c1cae35c27bf7a6a7",
        "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
        "isPro": false,
        "fullname": "Bowen Yu",
        "user": "Tigerph",
        "type": "user"
      },
      "summary": "모델 크기와 데이터 세트 크기에 따라 측정 손실의 파워럴의 스케일링 법칙에 따라 이 법칙이 선호 모델링에서도 존재함을 발견했습니다. 우리는 World Preference Modeling (WorldPM)을 제안하고 이 스케일링 잠재력을 강조합니다. World Preference는 인간의 취미의 통일된 표현입니다. 이 논문에서는 다양한 사용자 커뮤니티를 커버하는 공개 포럼에서 취미 데이터를 수집하여 15M 규모의 데이터를 사용하여 다양한 모델 (1.5B부터 72B 파라미터)으로 확장 훈련을 수행했습니다. 검증 메트릭에서 다음 특징이 발견되었습니다: (1) 광고 평가기 (장애 기능의 인식 능력)은 학습 데이터와 기본 모델 크기의 증가에 따라 일관된 스케일링합니다. (2) 목표 평가기 (정의된 답을 가진 객관적인 지식)은 큰 언어 모델에서 에피오퍼버지온 버전을 보여주고 WorldPM의 스케일링 잠재력을 강조합니다. (3) 주관 평가기 (유한한 인간이나 AI의 주관적인 취미)는 스케일링의 추세를 보여주지 않습니다. 진행된 실험은 WorldPM가 취미 미세 조정의 기초에 대한 효과성을 증명했습니다. 7개의 벤치마크의 20개의 서브 태스크에서 평가를 통해 WorldPM은 7K, 100K, 800K 샘플의 다른 크기의 인간 취미 데이터 세트의 확장 성능을 광범위하게 향상시키고 많은 키 서브 태스크에서 5% 이상의 성능 향상을 얻었습니다. 내부의 RLHF 파이프라인을 WorldPM와 통합하고 전문적인 평가 세트와 공개적인 평가 세트에서도显著な 상승을 보였습니다. 내부 평가에서 4% ~ 8%의显著한 상승이 보였습니다.",
      "upvotes": 8,
      "discussionId": "682699dd19c4a596dbcea604",
      "ai_keywords": [
        "preference modeling",
        "World Preference Modeling (WorldPM)",
        "World Preference",
        "unified representation",
        "human preferences",
        "preference data",
        "public forums",
        "user communities",
        "extensive training",
        "15M-scale data",
        "parameter-efficient fine-tuning",
        "Adversarial metrics",
        "deceptive features",
        "Objective metrics",
        "Subjective metrics",
        "preference fine-tuning",
        "generalization performance",
        "human preference datasets",
        "RLHF (Reinforcement Learning from Human Feedback)",
        "in-house evaluations",
        "public evaluation sets"
      ]
    },
    "publishedAt": "2025-05-15T13:38:37.000Z",
    "title": "WorldPM: Scaling Human Preference Modeling",
    "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d9d68c1cae35c27bf7a6a7",
      "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
      "fullname": "Bowen Yu",
      "name": "Tigerph",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10320",
      "authors": [
        {
          "_id": "6826a180cad9000ebc70f038",
          "name": "Chenxi Whitehouse",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f039",
          "name": "Tianlu Wang",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03a",
          "name": "Ping Yu",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03b",
          "name": "Xian Li",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03c",
          "name": "Jason Weston",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03d",
          "name": "Ilia Kulikov",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03e",
          "user": {
            "_id": "64b75f4b037d6452a30f71aa",
            "avatarUrl": "/avatars/5a0322e7ecda05164e45526d605e3619.svg",
            "isPro": false,
            "fullname": "Swarnadeep Saha",
            "user": "swarna92",
            "type": "user"
          },
          "name": "Swarnadeep Saha",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-16T02:22:57.318Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T14:05:15.000Z",
      "submittedOnDailyAt": "2025-05-16T00:58:47.493Z",
      "title": "J1: 강화학습을 통해 LLM-as-a-Judge의 개념을 촉진하는 방법",
      "submittedOnDailyBy": {
        "_id": "64b6feee17681d64b19b112b",
        "avatarUrl": "/avatars/afe708392d16af5b55d3bc4b42a585e8.svg",
        "isPro": false,
        "fullname": "Swarnadeep Saha",
        "user": "swarnaNLP",
        "type": "user"
      },
      "summary": "AI의 발전은 평가의 질에 따라 제한되어 있으며, 강력한 LLM-as-a-Judge 모델이 핵심적인 해결책으로 입증되어 있습니다. 강화된 판단력은 더 강력한 chain-of-thought reasoning으로 가능해졌으며, 이러한 모델의 최적의 훈련 매뉴얼을 찾기 위해 필요합니다. 본 논문에서는 J1이라는 리팩시온 시스템의 접근 방식을 통해 이러한 모델의 훈련에 대해 설명하겠습니다. 우리의 방법론은 두 가지 모두 증명 가능한Prompt를 판단 태스크로 변환하여 사고를 촉진하고 판단 편향을 줄이는 증명 가능한 보상을 제공합니다. 특히, 이 접근 방식은 8B 또는 70B 모델의 훈련 크기에서 현재 모든 모델보다 뛰어납니다. J1은 o1-mini보다 뛰어납니다. 또한 R1도 이러한 벤치마크에서 뛰어납니다. J1은 작은 모델에서도 뛰어납니다. 우리는 Pairwise-J1 vs Pointwise-J1 모델, Offline vs Online 훈련 매뉴얼, 보상 전략, Seed Prompt, 사고의 길이와 내용을 분석하고 비교를 제공합니다. 우리의 모델은 평가 기준을 학습하고, 자기 생성된 참조 답과 비교하여 모델의 답변의 정확성을 재평가함으로써 더 좋은 판단을 수행할 수 있음을 알게 되었습니다.",
      "upvotes": 8,
      "discussionId": "6826a181cad9000ebc70f0a3",
      "ai_keywords": [
        "reinforcement learning",
        "chain-of-thought reasoning",
        "judgment tasks",
        "judgment bias",
        "Pairwise-J1",
        "Pointwise-J1",
        "offline training",
        "online training",
        "reward strategies",
        "seed prompts",
        "evaluation criteria",
        "reference answers",
        "correctness of model responses"
      ]
    },
    "publishedAt": "2025-05-15T10:05:15.000Z",
    "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
    "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10320.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b6feee17681d64b19b112b",
      "avatarUrl": "/avatars/afe708392d16af5b55d3bc4b42a585e8.svg",
      "fullname": "Swarnadeep Saha",
      "name": "swarnaNLP",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09738",
      "authors": [
        {
          "_id": "6826c755f032d814754cadfe",
          "name": "Shaurya Sharthak",
          "hidden": false
        },
        {
          "_id": "6826c755f032d814754cadff",
          "name": "Vinayak Pahalwan",
          "hidden": false
        },
        {
          "_id": "6826c755f032d814754cae00",
          "user": {
            "_id": "6523d85b27d1f3d84ab3a0a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6523d85b27d1f3d84ab3a0a4/GF159gtSvxSWR9TAJIQby.jpeg",
            "isPro": false,
            "fullname": "Adithya Kamath",
            "user": "adi-kmt",
            "type": "user"
          },
          "name": "Adithya Kamath",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:11:49.739Z",
          "hidden": false
        },
        {
          "_id": "6826c755f032d814754cae01",
          "user": {
            "_id": "644bf6ef778ecbfb977e8e84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf6ef778ecbfb977e8e84/x2zFOkSIhDHf0LEZ1wzbU.jpeg",
            "isPro": true,
            "fullname": "Adarsh AS",
            "user": "adarshxs",
            "type": "user"
          },
          "name": "Adarsh Shirawalmath",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-16T05:19:45.785Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/644bf6ef778ecbfb977e8e84/Y-nis0xhyXSFc6TiLirrG.jpeg"
      ],
      "publishedAt": "2025-05-14T19:00:27.000Z",
      "submittedOnDailyAt": "2025-05-16T03:53:49.621Z",
      "title": "ヒューリスティックアダプターフィードバック와 초 토큰 학습을 통해 텍스처화 된 토크너의 유연성을 달성하는 방법",
      "submittedOnDailyBy": {
        "_id": "644bf6ef778ecbfb977e8e84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf6ef778ecbfb977e8e84/x2zFOkSIhDHf0LEZ1wzbU.jpeg",
        "isPro": true,
        "fullname": "Adarsh AS",
        "user": "adarshxs",
        "type": "user"
      },
      "summary": "予測 라벨: \"장문 번역\"\n\n予측 결과:\n\n予측 결과의 세부 사항: \"장문 번역\"의 결과가 텍스트의 전체 내용을 일본어로 번역된 결과입니다. 번역은 텍스트의 내용을 충실하게 변환하고, 일본어의 표현에 맞게 만들어졌습니다. 번역 과정에서는 텍스트의 구조와 의미에 대한 이해를 위해 자연어 처리 기술이 사용되었습니다. 번역 결과는 텍스트의 문제 해결이나 이해를 위해 사용할 수 있습니다.\n\n予측 결과의 텍스트: 예측 결과의 텍스트는 텍스트의 전체 내용을 일본어로 번역된 결과입니다. 번역 결과는 텍스트의 문제 해결이나 이해를 위해 사용할 수 있습니다.",
      "upvotes": 8,
      "discussionId": "6826c756f032d814754cae4d",
      "githubRepo": "https://github.com/Tinycompany-AI/tokenadapt",
      "ai_keywords": [
        "Tokenadapt",
        "Supertokens",
        "subword decomposition",
        "semantically similar tokens",
        "zero-shot perplexity",
        "perplexity ratios"
      ]
    },
    "publishedAt": "2025-05-14T15:00:27.000Z",
    "title": "Achieving Tokenizer Flexibility in Language Models through Heuristic\n  Adaptation and Supertoken Learning",
    "summary": "Pretrained language models (LLMs) are often constrained by their fixed\ntokenization schemes, leading to inefficiencies and performance limitations,\nparticularly for multilingual or specialized applications. This tokenizer\nlock-in presents significant challenges. standard methods to overcome this\noften require prohibitive computational resources. Although tokenizer\nreplacement with heuristic initialization aims to reduce this burden, existing\nmethods often require exhaustive residual fine-tuning and still may not fully\npreserve semantic nuances or adequately address the underlying compression\ninefficiencies. Our framework introduces two innovations: first, Tokenadapt, a\nmodel-agnostic tokenizer transplantation method, and second, novel\npre-tokenization learning for multi-word Supertokens to enhance compression and\nreduce fragmentation. Tokenadapt initializes new unique token embeddings via a\nhybrid heuristic that combines two methods: a local estimate based on subword\ndecomposition using the old tokenizer, and a global estimate utilizing the\ntop-k semantically similar tokens from the original vocabulary. This\nmethodology aims to preserve semantics while significantly minimizing\nretraining requirements. Empirical investigations validate both contributions:\nthe transplantation heuristic successfully initializes unique tokens, markedly\noutperforming conventional baselines and sophisticated methods including\nTranstokenizer and ReTok, while our Supertokens achieve notable compression\ngains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid\ninitialization consistently yields lower perplexity ratios compared to both\nReTok and TransTokenizer baselines across different base models and newly\ntrained target tokenizers. TokenAdapt typically reduced the overall perplexity\nratio significantly compared to ReTok, yielding at least a 2-fold improvement\nin these aggregate scores.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/644bf6ef778ecbfb977e8e84/Y-nis0xhyXSFc6TiLirrG.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09738.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644bf6ef778ecbfb977e8e84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf6ef778ecbfb977e8e84/x2zFOkSIhDHf0LEZ1wzbU.jpeg",
      "fullname": "Adarsh AS",
      "name": "adarshxs",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10565",
      "authors": [
        {
          "_id": "6826bda8caf89edf94b736dc",
          "user": {
            "_id": "6425761a175bd295228311a0",
            "avatarUrl": "/avatars/dcd0d267445563d0616d5a31b5d754b7.svg",
            "isPro": false,
            "fullname": "zehan wang",
            "user": "sleetwang6",
            "type": "user"
          },
          "name": "Zehan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:01.498Z",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736dd",
          "name": "Siyu Chen",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736de",
          "name": "Lihe Yang",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736df",
          "name": "Jialei Wang",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736e0",
          "name": "Ziang Zhang",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736e1",
          "name": "Hengshuang Zhao",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736e2",
          "name": "Zhou Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:59:50.000Z",
      "submittedOnDailyAt": "2025-05-16T02:55:45.280Z",
      "title": "Depth Anything with Any Prior",
      "submittedOnDailyBy": {
        "_id": "663b4d6aa55b0634634cd302",
        "avatarUrl": "/avatars/1191982568ad67895225f22844b6da99.svg",
        "isPro": false,
        "fullname": "ZehanWang",
        "user": "ZehanWang",
        "type": "user"
      },
      "summary": "이 연구에서는 \"Prior Depth Anything\" 프레임워크를 제안합니다. 이 프레임워크는 측정에서 불완전한 정확한 메트릭 정보와 측정에서 상대적인 완전한 기하 구조를 통합하여, 어떤 시나리오에서도 정확한 밀도있는 상세한 메트릭 측정 맵을 생성하는 것을 목표로 합니다. 이를 위해, 우리는 두 가지 보간된 측정 소스를 체계적으로 통합하기 위한 거시부터 디테일한 프로세스를 설계했습니다. 먼저, 우리는 픽셀 수준의 메트릭 어레이먼트와 거리에 대한 가중치를 사용하여 명시적으로 측정을 사용하여 다양한 메트릭 선두를 미리 내장하는 방법을 제안합니다. 이는 선두 패턴의 영역 차이를 효과적으로 줄이고 다양한 시나리오에서의 일반화에 도움을 줄 수 있습니다. 다음으로, 우리는 조건부 모노투브 측정 모델(MDE)을 개발하여 측정 선두의 내장 노이즈를 미세화합니다. 모델은 미리 내장된 정규화된 선두와 예측에 기반하여 두 가지 보간된 측정 소스를 은닉적으로 융합할 수 있습니다. 우리의 모델은 7개의 실세계 데이터 세트에서 측정의 완료, 초해상, 인풋에 대한 우수한 0-shot 일반화에서 뛰어난 성능을 보여주며, 기존의 작업专用적인 방법보다 뛰어나게 수행합니다. 더욱 중요한 점은, 이것은 어려운, 보이지 않는 혼합된 선두에 대해도 우수한 동작을 보여줍니다. 예측 모델을 변경하여 테스트 시의 개선을 수행할 수 있으며, MDE 모델의 발전과 함께 유연한 정확도와 효율의 조정을 제공합니다.",
      "upvotes": 7,
      "discussionId": "6826bdaacaf89edf94b73753",
      "projectPage": "https://prior-depth-anything.github.io/",
      "githubRepo": "https://github.com/SpatialVision/Prior-Depth-Anything",
      "ai_keywords": [
        "metric depth maps",
        "coarse-to-fine pipeline",
        "pixel-level metric alignment",
        "distance-aware weighting",
        "metric priors",
        "domain gap",
        "generalization",
        "conditioned monocular depth estimation (MDE)",
        "zero-shot generalization",
        "depth completion",
        "super-resolution",
        "inpainting",
        "test-time improvements",
        "accuracy-efficiency trade-off"
      ]
    },
    "publishedAt": "2025-05-15T13:59:50.000Z",
    "title": "Depth Anything with Any Prior",
    "summary": "This work presents Prior Depth Anything, a framework that combines incomplete\nbut precise metric information in depth measurement with relative but complete\ngeometric structures in depth prediction, generating accurate, dense, and\ndetailed metric depth maps for any scene. To this end, we design a\ncoarse-to-fine pipeline to progressively integrate the two complementary depth\nsources. First, we introduce pixel-level metric alignment and distance-aware\nweighting to pre-fill diverse metric priors by explicitly using depth\nprediction. It effectively narrows the domain gap between prior patterns,\nenhancing generalization across varying scenarios. Second, we develop a\nconditioned monocular depth estimation (MDE) model to refine the inherent noise\nof depth priors. By conditioning on the normalized pre-filled prior and\nprediction, the model further implicitly merges the two complementary depth\nsources. Our model showcases impressive zero-shot generalization across depth\ncompletion, super-resolution, and inpainting over 7 real-world datasets,\nmatching or even surpassing previous task-specific methods. More importantly,\nit performs well on challenging, unseen mixed priors and enables test-time\nimprovements by switching prediction models, providing a flexible\naccuracy-efficiency trade-off while evolving with advancements in MDE models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10565.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "663b4d6aa55b0634634cd302",
      "avatarUrl": "/avatars/1191982568ad67895225f22844b6da99.svg",
      "fullname": "ZehanWang",
      "name": "ZehanWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09990",
      "authors": [
        {
          "_id": "6826dce4682e62074d0ed686",
          "name": "Long Cheng",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed687",
          "name": "Jiafei Duan",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed688",
          "name": "Yi Ru Wang",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed689",
          "name": "Haoquan Fang",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68a",
          "name": "Boyang Li",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68b",
          "name": "Yushan Huang",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68c",
          "name": "Elvis Wang",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68d",
          "name": "Ainaz Eftekhar",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68e",
          "name": "Jason Lee",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68f",
          "name": "Wentao Yuan",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed690",
          "name": "Rose Hendrix",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed691",
          "name": "Noah A. Smith",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed692",
          "name": "Fei Xia",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed693",
          "name": "Dieter Fox",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed694",
          "name": "Ranjay Krishna",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/IGaHXqPEos-rGXUNMY4mE.png"
      ],
      "publishedAt": "2025-05-15T06:04:42.000Z",
      "submittedOnDailyAt": "2025-05-16T05:07:15.117Z",
      "title": "PointArena: 언어를 가이드한 여러 모형의 기본적인 특성 조사",
      "submittedOnDailyBy": {
        "_id": "632b42626110e37dba3d5bcb",
        "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg",
        "isPro": false,
        "fullname": "Duan",
        "user": "Jiafei1224",
        "type": "user"
      },
      "summary": "指し手는 시각적 컨텍스트 내의 언어를 기반으로 기계인, 보조언어 기술, 인터랙티브 AI 시스템 등 다양한 분야에 적용되는 기본적이고 직관적인 기능입니다. 최근의 다모달 모델들은 처음으로 指し手의 기능을 지원하고 있지만, 현재의 벤치마크는 주로 상대적인 물체 위치 결정 태스크를 중심으로 진행되고 있습니다. PointArena라는 다모달 指し手의 평가를 수행하는 한 가지 상세한 플랫폼을 소개합니다. PointArena는 3가지 구성 요소로 구성되어 있습니다: (1) Point-Bench, 약 1,000개의 指し手 태스크를 포함하는 데이터셋, 5가지 논리 카테고리를 가지고 있습니다. (2) Point-Battle, 상호작용적인 웹 기반 아리언나, 뿌린드 모델 비교를 촉진하고 현재 4,500이상의 익명 투표를 수집하고 있습니다. (3) Point-Act, 실제 세계의 기계인 조작 시스템, 사용자가 직접 다모달 모델의 指し手 기능을 유용한 설정으로 평가할 수 있게 합니다. 가장 先端的开放 소스 모델과 所有権 모델을 확장적으로 평가했습니다. 결과는 Molmo-72B가 다른 모델을 일관되게 초월하고 있지만, 所有権 모델의 성능이 상대적으로 향상되고 있습니다. 또한, 指し手 태스크에 대한 서브 객체 훈련은 모델의 성능을 크게 향상시켰습니다. 우리의 다단계 평가 파이프라인 중도 강한 상관관계를 보였으며, 다모달 모델이 추상적인 논리와 구체적인 실제 세계적인 행동을 효과적으로 연결하는 데 필요한 指し手 기능의 정확도가 중요한 역할을 하는 것을 강조합니다. 프로젝트 페이지: https://pointarena.github.io/",
      "upvotes": 7,
      "discussionId": "6826dce8682e62074d0ed7eb",
      "projectPage": "https://pointarena.github.io/",
      "githubRepo": "https://github.com/pointarena/pointarena",
      "ai_keywords": [
        "multimodal models",
        "referential object localization tasks",
        "PointArena",
        "Point-Bench",
        "reasoning categories",
        "Point-Battle",
        "Point-Act",
        "blind, pairwise model comparisons",
        "Molmo-72B",
        "supervised training",
        "precise pointing capabilities"
      ]
    },
    "publishedAt": "2025-05-15T02:04:42.000Z",
    "title": "PointArena: Probing Multimodal Grounding Through Language-Guided\n  Pointing",
    "summary": "Pointing serves as a fundamental and intuitive mechanism for grounding\nlanguage within visual contexts, with applications spanning robotics, assistive\ntechnologies, and interactive AI systems. While recent multimodal models have\nstarted to support pointing capabilities, existing benchmarks typically focus\nonly on referential object localization tasks. We introduce PointArena, a\ncomprehensive platform for evaluating multimodal pointing across diverse\nreasoning scenarios. PointArena comprises three components: (1) Point-Bench, a\ncurated dataset containing approximately 1,000 pointing tasks across five\nreasoning categories; (2) Point-Battle, an interactive, web-based arena\nfacilitating blind, pairwise model comparisons, which has already gathered over\n4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation\nsystem allowing users to directly evaluate multimodal model pointing\ncapabilities in practical settings. We conducted extensive evaluations of both\nstate-of-the-art open-source and proprietary multimodal models. Results\nindicate that Molmo-72B consistently outperforms other models, though\nproprietary models increasingly demonstrate comparable performance.\nAdditionally, we find that supervised training specifically targeting pointing\ntasks significantly enhances model performance. Across our multi-stage\nevaluation pipeline, we also observe strong correlations, underscoring the\ncritical role of precise pointing capabilities in enabling multimodal models to\neffectively bridge abstract reasoning with concrete, real-world actions.\nProject page: https://pointarena.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/IGaHXqPEos-rGXUNMY4mE.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632b42626110e37dba3d5bcb",
      "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg",
      "fullname": "Duan",
      "name": "Jiafei1224",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09926",
      "authors": [
        {
          "_id": "68268c3408f7cb26defd82fc",
          "name": "Bin-Bin Gao",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd82fd",
          "name": "Yue Zhu",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd82fe",
          "name": "Jiangtao Yan",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd82ff",
          "name": "Yuezhi Cai",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8300",
          "name": "Weixi Zhang",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8301",
          "name": "Meng Wang",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8302",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8303",
          "name": "Yong Liu",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8304",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8305",
          "name": "Chengjie Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T03:24:28.000Z",
      "submittedOnDailyAt": "2025-05-16T00:07:29.541Z",
      "title": "AdaptCLIP: CLIP의 일반화된 시각적 이상 검출 적용",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "Universal visual anomaly detection의 목표는 추가의 미세 조정 훈련을 필요로 하지 않고, 새로운나未见의 시각 영역에서 발생한 이상을 식별하는 것입니다. 개방된 시나리오에서 매우 중요합니다. 최근의 연구는 CLIP 등 사전 학습된 시각 언어 모델이 0 또는 거의 일반적인 이미지에서 강력한 일반화 성능을 보여주는 것을 밝혀 냈습니다. 그러나 현재의 방법들은 Prompt 템플릿의 설계, 복잡한 토큰의 상호작용, 또는 추가의 미세 조정 훈련이 필요하여 유연성이 제한되어 있습니다. 본 논문에서는 두 가지 주요 전망에 기반하여 간단하고 효과적인 방법을 제안하고 있습니다. 첫 번째는 시각적 및 문학적 표현은 동시에 학습하는 것이 아니라 교차 학습하는 것이 더 바람직합니다. 두 번째는 질문과 일반적인 이미지의 Prompt의 비교 학습은上下文中와 대응된 잔차 특성을 포함하고, 잔차 특성에만 의존하지 않는 것이 중요합니다. AdaptCLIP은 CLIP 모델을 기반 서비스로 삼고, 입력 또는 출력단에 3개의 간단한 어댑터를 추가하여 있습니다. AdaptCLIP은 기반 데이터 세트에서 훈련 후, 영역 간의 0/적어도 적은샷의 일반화를 지원하여 목표 영역에서 훈련이 필요하지 않습니다. AdaptCLIP은 12개의 산업 및 의료 분야의 이상 검출 벤치마크에서 가장 先端의 성능을 달성하고, 현재의 최신 방법보다 크게 뛰어넘습니다. AdaptCLIP의 코드와 모델은 https://github.com/gaobb/AdaptCLIP에 공개됩니다.",
      "upvotes": 5,
      "discussionId": "68268c3808f7cb26defd83bf",
      "ai_keywords": [
        "CLIP",
        "pre-trained vision-language models",
        "prompt templates",
        "token interactions",
        "fine-tuning",
        "adaptive visual and textual representations",
        "comparative learning",
        "query and normal image prompt",
        "contextual and aligned residual features",
        "residual features",
        "visual adapter",
        "textual adapter",
        "prompt-query adapter",
        "zero-/few-shot generalization",
        "training-free",
        "anomaly detection benchmarks",
        "industrial domains",
        "medical domains"
      ]
    },
    "publishedAt": "2025-05-14T23:24:28.000Z",
    "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection",
    "summary": "Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09926.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.08617",
      "authors": [
        {
          "_id": "6826aa068caf98415c50897f",
          "name": "Zhaochen Su",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508980",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508981",
          "name": "Mingyang Song",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508982",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508983",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508984",
          "name": "Jun Zhang",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508985",
          "name": "Guanjie Chen",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508986",
          "name": "Jiawei Gu",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508987",
          "name": "Juntao Li",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508988",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508989",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-13T14:35:51.000Z",
      "submittedOnDailyAt": "2025-05-16T01:29:32.761Z",
      "title": "OpenThinkIMG: 이미지에 기반한 사고를 배우는 데 사용되는 시각화 도구를 활용한 강화 학습",
      "submittedOnDailyBy": {
        "_id": "64264095ba51f8a2136946a0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
        "isPro": false,
        "fullname": "Zhaochen Su",
        "user": "Warrieryes",
        "type": "user"
      },
      "summary": "인간은 복잡한 문제 해결에서 상호작용적인 시각적 지식의 유연한 활용이 가능하지만, LVLM(대시각 언어 모델)은 시각 도구를 사용하여 유사한 적응적인 행동을 학습하는 데 어려움이 있습니다. 현재 표준화된 표준 인프라의 부족은 다양한 도구의 통합, 풍부한 상호작용 데이터의 생성, 견고한 에이전트의 효과적인 학습에 부담을 줍니다. 이러한 단점을 해결하기 위해, 첫 번째 오픈 소스의 기능付き LVLM의 일관된 전체 프레임워크인 OpenThinkIMG을 도입하고 있습니다. 표준화된 시각 도구 인터페이스, scalable 파라미터 생성, 유연한 학습 환경이 특징으로 있으며, 동적 도구 호출에 대한 정책의 일반화에 제한하는静的示唆에 기반한 정규화의 微調節(SFT)은 새로운 강화학습(RL) 프레임워크 V-ToolRL을 제안하고 있습니다. V-ToolRL은 LVLM을 동적인 정책의 학습을 위해 외부 시각 도구를 호출하는 것을 학습시키는 것을 목표로 합니다. V-ToolRL은 도구의 상호작용으로부터의 피드백을 직접 최적화하여, LVLM이 자동으로 최적의 도구 사용 전략을 발견할 수 있게 합니다. V-ToolRL은 어려운 차트 논리론 태스크에서 실험적으로 검증되었으며, SFT로 초기화된 것을 +28.83점 초과, タコ, CogCom과 같은 기존의 정규화된 도구 학습 기반 라인을 평균 +12.7점 초과, GPT-4.1과 같은 폐쇄 소스 모델을 +8.68 정확도 포인트 초과했습니다. 우리는 OpenThinkIMG가 동적인, 기능付き 시각적 논리론의 발전을 촉진하는 기초적인 프레임워크로役立つことを望んでいます.",
      "upvotes": 5,
      "discussionId": "6826aa078caf98415c5089d6",
      "githubRepo": "https://github.com/zhaochen0110/OpenThinkIMG",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "OpenThinkIMG",
        "vision tool interfaces",
        "scalable trajectory generation",
        "policy initialization",
        "flexible training environment",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "V-ToolRL",
        "adaptive policies",
        "external vision tools",
        "optimal tool-usage strategies",
        "task success",
        "feedback from tool interactions",
        "chart reasoning tasks",
        "Qwen2-VL-2B",
        "Taco",
        "CogCom",
        "GPT-4.1"
      ]
    },
    "publishedAt": "2025-05-13T10:35:51.000Z",
    "title": "OpenThinkIMG: Learning to Think with Images via Visual Tool\n  Reinforcement Learning",
    "summary": "While humans can flexibly leverage interactive visual cognition for complex\nproblem-solving, enabling Large Vision-Language Models (LVLMs) to learn\nsimilarly adaptive behaviors with visual tools remains challenging. A\nsignificant hurdle is the current lack of standardized infrastructure, which\nhinders integrating diverse tools, generating rich interaction data, and\ntraining robust agents effectively. To address these gaps, we introduce\nOpenThinkIMG, the first open-source, comprehensive end-to-end framework for\ntool-augmented LVLMs. It features standardized vision tool interfaces, scalable\ntrajectory generation for policy initialization, and a flexible training\nenvironment. Furthermore, considering supervised fine-tuning (SFT) on static\ndemonstrations offers limited policy generalization for dynamic tool\ninvocation, we propose a novel reinforcement learning (RL) framework V-ToolRL\nto train LVLMs to learn adaptive policies for invoking external vision tools.\nV-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies\nby directly optimizing for task success using feedback from tool interactions.\nWe empirically validate V-ToolRL on challenging chart reasoning tasks. Our\nRL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its\nSFT-initialized counterpart (+28.83 points) and surpasses established\nsupervised tool-learning baselines like Taco and CogCom by an average of +12.7\npoints. Notably, it also surpasses prominent closed-source models like GPT-4.1\nby +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational\nframework for advancing dynamic, tool-augmented visual reasoning, helping the\ncommunity develop AI agents that can genuinely \"think with images\".",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08617.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64264095ba51f8a2136946a0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
      "fullname": "Zhaochen Su",
      "name": "Warrieryes",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.08581",
      "authors": [
        {
          "_id": "6825c47fbe0e3c3bfbf4d285",
          "user": {
            "_id": "66bf75432777c05070bf49dc",
            "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
            "isPro": false,
            "fullname": "Haofeng Liu",
            "user": "HeverLaw",
            "type": "user"
          },
          "name": "Haofeng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:36.358Z",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d286",
          "name": "Mingqi Gao",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d287",
          "name": "Xuxiao Luo",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d288",
          "name": "Ziyue Wang",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d289",
          "name": "Guanyi Qin",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d28a",
          "name": "Junde Wu",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d28b",
          "name": "Yueming Jin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66bf75432777c05070bf49dc/-a-XXHMaiah3MqSJzX3FE.mp4"
      ],
      "publishedAt": "2025-05-13T13:56:10.000Z",
      "submittedOnDailyAt": "2025-05-16T02:23:20.718Z",
      "title": "ReSurgSAM2: 수술 비디오에서 신뢰성 있는 장기 탬킹을 이용한 객체 참조 분할",
      "submittedOnDailyBy": {
        "_id": "66bf75432777c05070bf49dc",
        "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
        "isPro": false,
        "fullname": "Haofeng Liu",
        "user": "HeverLaw",
        "type": "user"
      },
      "summary": "手術シーン分割는 컴퓨터 협업 수술에서 중요하며, 수술의 질과 환자의 결과를 향상시키기 위해 필수적이다. 최근, 수술의 참조 분할이 등장하며, 수술 의사에게 목표물 분할을 인터랙티브 경험으로 제공하기 위해 이점을 기반으로 있다. 그러나 현재의 방법은 낮은 효율성과 단기간 추적에 제한되어 있으며, 복잡한 현실적인 수술 시나리오에서의 적용이 어려워진다. 본 논문에서는 Segment Anything Model 2를 활용하여, 맥락에 따라 대상물 검출을 수행하고, 신뢰성을 가지는 초기 프레임의 인식과 다양성을 강화한 장기 기억을 결합한 2단계의 수술 참조 분할 프레임워크 ReSurgSAM2를 제시한다. 검출 단계에서, 교차 모달 스펙트럼 시간 맵을 사용하여 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지하고, 일관된 장기간 추적을 보장함으로써, 검출 단계에서 검출 및 분할 결과를 생성하고, 이를 기반으로 신뢰성을 가지는 초기 프레임 선택 전략을 제안하고, 시작 프레임을 선택한 후, 추적 단계에서 다양성을 강화한 메모리 구조를 결합하여 신뢰성을 가지는 다양한 메모리 바ン크를 유지",
      "upvotes": 4,
      "discussionId": "6825c480be0e3c3bfbf4d2c0",
      "githubRepo": "https://github.com/jinlab-imvr/ReSurgSAM2",
      "ai_keywords": [
        "Segment Anything Model 2",
        "text-referred target detection",
        "cross-modal spatial-temporal Mamba",
        "initial frame identification",
        "diversity-driven long-term memory",
        "diversity-driven memory mechanism",
        "memory bank",
        "real-time tracking",
        "real-time operation"
      ]
    },
    "publishedAt": "2025-05-13T09:56:10.000Z",
    "title": "ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible\n  Long-term Tracking",
    "summary": "Surgical scene segmentation is critical in computer-assisted surgery and is\nvital for enhancing surgical quality and patient outcomes. Recently, referring\nsurgical segmentation is emerging, given its advantage of providing surgeons\nwith an interactive experience to segment the target object. However, existing\nmethods are limited by low efficiency and short-term tracking, hindering their\napplicability in complex real-world surgical scenarios. In this paper, we\nintroduce ReSurgSAM2, a two-stage surgical referring segmentation framework\nthat leverages Segment Anything Model 2 to perform text-referred target\ndetection, followed by tracking with reliable initial frame identification and\ndiversity-driven long-term memory. For the detection stage, we propose a\ncross-modal spatial-temporal Mamba to generate precise detection and\nsegmentation results. Based on these results, our credible initial frame\nselection strategy identifies the reliable frame for the subsequent tracking.\nUpon selecting the initial frame, our method transitions to the tracking stage,\nwhere it incorporates a diversity-driven memory mechanism that maintains a\ncredible and diverse memory bank, ensuring consistent long-term tracking.\nExtensive experiments demonstrate that ReSurgSAM2 achieves substantial\nimprovements in accuracy and efficiency compared to existing methods, operating\nin real-time at 61.2 FPS. Our code and datasets will be available at\nhttps://github.com/jinlab-imvr/ReSurgSAM2.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66bf75432777c05070bf49dc/-a-XXHMaiah3MqSJzX3FE.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08581.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66bf75432777c05070bf49dc",
      "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
      "fullname": "Haofeng Liu",
      "name": "HeverLaw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10046",
      "authors": [
        {
          "_id": "6826b00ed4c8b864e5ed1c0f",
          "name": "Bingda Tang",
          "hidden": false
        },
        {
          "_id": "6826b00ed4c8b864e5ed1c10",
          "name": "Boyang Zheng",
          "hidden": false
        },
        {
          "_id": "6826b00ed4c8b864e5ed1c11",
          "name": "Xichen Pan",
          "hidden": false
        },
        {
          "_id": "6826b00ed4c8b864e5ed1c12",
          "name": "Sayak Paul",
          "hidden": false
        },
        {
          "_id": "6826b00ed4c8b864e5ed1c13",
          "name": "Saining Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T07:43:23.000Z",
      "submittedOnDailyAt": "2025-05-16T01:55:37.654Z",
      "title": "심층 융합의 대규모 언어 모델과 분기 분기의 Transformers를 활용한 문장으로부터 이미지 합성에 대한 연구",
      "submittedOnDailyBy": {
        "_id": "5f7fbd813e94f16a85448745",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
        "isPro": false,
        "fullname": "Sayak Paul",
        "user": "sayakpaul",
        "type": "user"
      },
      "summary": "이 논문은 새로운 방법을 설명하지 않습니다. 대신, 최근의 맥락에서 이미지 합성에 대한 중요한 연구가 부족한 설계 공간에 대해 상세한 탐색을 제공합니다. 특히, 대규모 언어 모델(LLMs)과 확산 변환기(DiTs)의 깊은 융합을 사용하여 다형성 생성에 관한 것입니다. 선행 연구는 주로 전체적인 시스템의 성능을 중심으로 있었지만, 다양한 방법과의 상세한 비교와 중요한 설계 세부 사항 및 훈련 문서가 일반적으로 공개되지 않았습니다. 이러한 결함이 이 접근 방식의 실제 가능성에 대한 불안을 불러일으키고 있습니다. 이러한 결함을 보완하기 위해, 맥락에서 이미지 생성에 대한 실험적인 연구를 수행하고, 기존 기준과의 제어된 비교를 수행하고, 중요한 설계 선택지 분석을 수행하고, 규모 업에 대한 훈련 가능한 레시피를 제공합니다. 우리는 이 연구가 미래의 다형성 생성에 대한 연구에 의미 있는 데이터 포인트와 실용적인 가이드라인을 제공함을 감사드립니다.",
      "upvotes": 3,
      "discussionId": "6826b00ed4c8b864e5ed1c4f",
      "githubRepo": "https://github.com/tang-bd/fuse-dit",
      "ai_keywords": [
        "large language models (LLMs)",
        "diffusion transformers (DiTs)",
        "multi-modal generation",
        "text-to-image synthesis",
        "controlled comparisons",
        "established baselines",
        "important design choices",
        "training recipes",
        "reproducible recipe",
        "training at scale"
      ]
    },
    "publishedAt": "2025-05-15T03:43:23.000Z",
    "title": "Exploring the Deep Fusion of Large Language Models and Diffusion\n  Transformers for Text-to-Image Synthesis",
    "summary": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10046.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f7fbd813e94f16a85448745",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
      "fullname": "Sayak Paul",
      "name": "sayakpaul",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 619
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09265",
      "authors": [
        {
          "_id": "682547e6501a31b392e78f6a",
          "user": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "isPro": false,
            "fullname": "Bin-Bin Gao",
            "user": "csgaobb",
            "type": "user"
          },
          "name": "Bin-Bin Gao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-15T01:54:50.125Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T10:25:26.000Z",
      "submittedOnDailyAt": "2025-05-16T00:08:33.750Z",
      "title": "MetaUAS: 유니버설 비정상 분할을 한 프로노트에 의한 메타 학습을 통해 실현する",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "Zero- and few-shot 시각적 이상 분리는, 사용자의 직접적인 설계된 맥락 프로노프토를 사용하여, 이전에 본 적이 없는 이상들을 검출하기 위해 강력한 시각 언어 모델을 의존하고 있습니다. 그러나 시각적 표현은 언어에 고유하며, 각각 독립되어 있습니다. 본 논문에서는, 광범위하게 사용되고 있는 시각 언어 모델을 대신하여, 절대적으로 시각적인 기초 모델의 가능성을 검토하고, 일반적인 시각적 이상 분리를 수행하는 것을 목표로 합니다. 새로운 패러다임을 제안하고, 이상 분리를 변화 분리와 하나로 합칩니다. 이 패러다임에서, 현재 존재하는 이미지 데이터셋에서 얻을 수 있는 물체 레벨과 지역 영역의 변화를 특징으로 하는 대규모 합성 이미지 페어를 사용합니다. 이러한 데이터셋은 목표의 이상 데이터셋에 의존하지 않습니다. 메타 학습 프레임워크를 제안하고, Universal Anomaly Segmentation (MetaUAS)를 실현합니다. 이 프레임워크는 합성 데이터셋을 통해 훈련되어, 실세계의 새로운하거나 본 적이 없는 시각적 이상들을 효과적으로 분리할 수 있습니다. 프로노프토와 질문 이미지 사이의 기하학적인 변화를 처리하기 위해, 쌍 이미지의 변화 인식과 단일 이미지의 세ман틱 분할을 결합하는 소프트 특징 배열 모듈을 제안합니다. 이는 특수한 이상 검출 데이터셋과 평단 훈련된 시각 언어 모델을 의존시키지 않고, 절대적으로 시각적인 모델을 사용하여 일반적인 이상 분리를 실현할 수 있는 최초의 연구입니다. 우리 방법은, 그 한 가지의 일반적인 이미지 프로노프토를 사용하여, 언어에 의한 가이드를 받지 않고 효과적으로 효율적으로 이상들을 분리할 수 있습니다. MetaUAS는 기존의 zero-shot, few-shot, 그리고 모든射 이상 분리 방법과 비교하여 현저히 뛰어납니다. 코드와 평단 훈련 모델은, https://github.com/gaobb/MetaUAS 에 제공됩니다.",
      "upvotes": 3,
      "discussionId": "682547eb501a31b392e79038",
      "githubRepo": "https://github.com/gaobb/MetaUAS",
      "ai_keywords": [
        "Meta-learning",
        "Universal Anomaly Segmentation (MetaUAS)",
        "synthetic image pairs",
        "object-level changes",
        "local region changes",
        "prompt",
        "query images",
        "soft feature alignment module",
        "paired-image change perception",
        "single-image semantic segmentation",
        "universal anomaly segmentation",
        "pure vision model",
        "zero-shot",
        "few-shot",
        "full-shot anomaly segmentation"
      ]
    },
    "publishedAt": "2025-05-14T06:25:26.000Z",
    "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning",
    "summary": "Zero- and few-shot visual anomaly segmentation relies on powerful\nvision-language models that detect unseen anomalies using manually designed\ntextual prompts. However, visual representations are inherently independent of\nlanguage. In this paper, we explore the potential of a pure visual foundation\nmodel as an alternative to widely used vision-language models for universal\nvisual anomaly segmentation. We present a novel paradigm that unifies anomaly\nsegmentation into change segmentation. This paradigm enables us to leverage\nlarge-scale synthetic image pairs, featuring object-level and local region\nchanges, derived from existing image datasets, which are independent of target\nanomaly datasets. We propose a one-prompt Meta-learning framework for Universal\nAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and\nthen generalizes well to segment any novel or unseen visual anomalies in the\nreal world. To handle geometrical variations between prompt and query images,\nwe propose a soft feature alignment module that bridges paired-image change\nperception and single-image semantic segmentation. This is the first work to\nachieve universal anomaly segmentation using a pure vision model without\nrelying on special anomaly detection datasets and pre-trained visual-language\nmodels. Our method effectively and efficiently segments any anomalies with only\none normal image prompt and enjoys training-free without guidance from\nlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,\nand even full-shot anomaly segmentation methods. The code and pre-trained\nmodels are available at https://github.com/gaobb/MetaUAS.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09265.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09264",
      "authors": [
        {
          "_id": "682548bff4997d78fe92cd57",
          "user": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "isPro": false,
            "fullname": "Bin-Bin Gao",
            "user": "csgaobb",
            "type": "user"
          },
          "name": "Bin-Bin Gao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-15T01:54:51.290Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T10:25:14.000Z",
      "submittedOnDailyAt": "2025-05-16T00:10:20.034Z",
      "title": "학습하여, 한 장의 정상 이미지만으로 다양한 클래스의 이상을 감지합니다.",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "無ラベル化重建ネットワーク를 自動注意変換器（self-attention transformers）을 사용하여, 다 클래스(통합) 이상검출의 최상급 성능을 달성했습니다. 그러나 이러한 자동 注意력 reconstruction 모델은 주로 목표 특성에 대해 작동하며, 이로 인해 정상과 이상의 특성 모두 맥락과 높은 일치성을 유지하기 때문에, 완전히 reconstruction이 가능한 점에서 이상 검출에 실패합니다. 또한 이러한 모델은 낮은 스펙트럴 분석도의 잠재 공간에서 reconstruction을 수행하기 때문에, 잘못된 이상 부분 분할을 생성합니다. 모델의 효율성을 높일 수 있는 동시에 통합 이상 검출의 확장성을 향상시키기 위해, 하나의 정상 이미지 프로ンプト(OneNIP)을 사용하여 정상 특성을 reconstruction하고 이상 특성을 복원하는 간단하고 효과적인 방법을 제안합니다. 이전 연구와 달리, OneNIP은 처음에 하나의 정상 이미지 프로ンプト를 사용하여 이상의 reconstruction이나 복원을 가능하게 하며, 통합 이상 검출의 성능을 효과적으로 향상시킵니다. 또한 실제 정상 이미지와 합성된 이상 이미지를 사용하여 reconstruction 오차를 회귀로 하여 픽셀 수준의 이상 부분 분할을 크게 향상시킵니다. OneNIP는 MVTec, BTAD, VisA의 3가지 산업 이상 검출 벤치마크에서 이전 방법보다 뛰어납니다. 코드와 사전 학습 모델은 https://github.com/gaobb/OneNIP에 제공됩니다.",
      "upvotes": 3,
      "discussionId": "682548c1f4997d78fe92cdbc",
      "githubRepo": "https://github.com/gaobb/OneNIP",
      "ai_keywords": [
        "self-attention transformers",
        "multi-class anomaly detection",
        "self-attention reconstruction models",
        "target features",
        "context",
        "latent space",
        "One Normal Image Prompt (OneNIP)",
        "supervised refiner",
        "reconstruction errors",
        "MVTec",
        "BTAD",
        "VisA"
      ]
    },
    "publishedAt": "2025-05-14T06:25:14.000Z",
    "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image\n  Prompt",
    "summary": "Unsupervised reconstruction networks using self-attention transformers have\nachieved state-of-the-art performance for multi-class (unified) anomaly\ndetection with a single model. However, these self-attention reconstruction\nmodels primarily operate on target features, which may result in perfect\nreconstruction for both normal and anomaly features due to high consistency\nwith context, leading to failure in detecting anomalies. Additionally, these\nmodels often produce inaccurate anomaly segmentation due to performing\nreconstruction in a low spatial resolution latent space. To enable\nreconstruction models enjoying high efficiency while enhancing their\ngeneralization for unified anomaly detection, we propose a simple yet effective\nmethod that reconstructs normal features and restores anomaly features with\njust One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP\nallows for the first time to reconstruct or restore anomalies with just one\nnormal image prompt, effectively boosting unified anomaly detection\nperformance. Furthermore, we propose a supervised refiner that regresses\nreconstruction errors by using both real normal and synthesized anomalous\nimages, which significantly improves pixel-level anomaly segmentation. OneNIP\noutperforms previous methods on three industry anomaly detection benchmarks:\nMVTec, BTAD, and VisA. The code and pre-trained models are available at\nhttps://github.com/gaobb/OneNIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09264.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09263",
      "authors": [
        {
          "_id": "682549a75b5784e023ed7d8a",
          "name": "Guan Gui",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8b",
          "user": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "isPro": false,
            "fullname": "Bin-Bin Gao",
            "user": "csgaobb",
            "type": "user"
          },
          "name": "Bin-Bin Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-15T10:31:36.508Z",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8c",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8d",
          "name": "Chengjie Wang",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8e",
          "name": "Yunsheng Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T10:25:06.000Z",
      "submittedOnDailyAt": "2025-05-16T00:12:32.405Z",
      "title": "Feedback Anomaly Driven Anomaly Classification and Segmentation",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "異常 검출은 산업 관측에서 비정상 샘플의 부족으로 인해 실질적이고 어려운 문제로 알려져 있습니다. 현재의 비정상 검출 방법들은 노이즈나 외부 데이터로 비정상을 합성하여 이 문제를 해결하지만, 합성된 비정상과 실제 세계의 비정상 사이에 큰 의미적 차이가 존재하여 비정상 검출의 성능이 약해집니다. 이러한 문제를 해결하기 위해 우리는 몇 피ッ크 모드의 비정상 구동 생성(AnoGen) 메소드를 제안합니다. 이는 실제의 몇 가지 비정상 샘플을 보고하여 실제적이고 다양한 비정상을 생성하는 데 사용되며, 비정상 검출 모델의 훈련에 도움이 됩니다. 특히, 우리의 연구는 3 단계로 구성되어 있습니다. 첫 번째 단계에서는 실제의 몇 가지 비정상 샘플을 기반으로 비정상의 분포를 학습하고 학습된 지식이 내장되어 있습니다. 두 번째 단계에서는 내장과 제공된 Bounding Box를 사용하여 특정 물체(또는 텍스트)에 대한 실제적이고 다양한 비정상을 생성하는 데 사용됩니다. 최종 단계에서는 생성된 비정상을 사용하여 약한 비정상 검출 메소드를 제안하고, 이를 통해 더 강력한 모델을 훈련합니다. 우리의 방법은 DRAEM과 DesTSeg를 기반 모델로, 일반적인 산업용 비정상 검출 데이터 세트를 사용하여 실험을 수행합니다. 실험은 우리가 생성한 비정상이 비정상 분류와 분할 작업의 모델 성능을 효과적으로 향상시키는 것을 보여주었습니다. 예를 들어, DRAEM과 DesTSeg는 분할 작업의 AU-PR 메트릭에서 각각 5.8%와 1.5%의 향상을 얻었습니다. 코드와 생성된 비정상 데이터는 https://github.com/gaobb/AnoGen에 액세스할 수 있습니다.",
      "upvotes": 3,
      "discussionId": "682549ac5b5784e023ed7e72",
      "ai_keywords": [
        "Anomaly-driven Generation (AnoGen)",
        "diffusion model",
        "anomaly distribution",
        "embedding",
        "bounding boxes",
        "weakly-supervised anomaly detection",
        "DRAEM",
        "DesTSeg",
        "MVTec",
        "AU-PR metric"
      ]
    },
    "publishedAt": "2025-05-14T06:25:06.000Z",
    "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and\n  Segmentation",
    "summary": "Anomaly detection is a practical and challenging task due to the scarcity of\nanomaly samples in industrial inspection. Some existing anomaly detection\nmethods address this issue by synthesizing anomalies with noise or external\ndata. However, there is always a large semantic gap between synthetic and\nreal-world anomalies, resulting in weak performance in anomaly detection. To\nsolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)\nmethod, which guides the diffusion model to generate realistic and diverse\nanomalies with only a few real anomalies, thereby benefiting training anomaly\ndetection models. Specifically, our work is divided into three stages. In the\nfirst stage, we learn the anomaly distribution based on a few given real\nanomalies and inject the learned knowledge into an embedding. In the second\nstage, we use the embedding and given bounding boxes to guide the diffusion\nmodel to generate realistic and diverse anomalies on specific objects (or\ntextures). In the final stage, we propose a weakly-supervised anomaly detection\nmethod to train a more powerful model with generated anomalies. Our method\nbuilds upon DRAEM and DesTSeg as the foundation model and conducts experiments\non the commonly used industrial anomaly detection dataset, MVTec. The\nexperiments demonstrate that our generated anomalies effectively improve the\nmodel performance of both anomaly classification and segmentation tasks\nsimultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement\nin AU-PR metric on segmentation task, respectively. The code and generated\nanomalous data are available at https://github.com/gaobb/AnoGen.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09263.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]