[
  {
    "paper": {
      "id": "2504.21635",
      "authors": [
        {
          "_id": "68130be55342cbe1ddefb262",
          "user": {
            "_id": "65704741e1cfce1764ce652e",
            "avatarUrl": "/avatars/9189aaf417426af4ebe381ed364a6c0e.svg",
            "isPro": false,
            "fullname": "Zeina Aldallal",
            "user": "ZeinaD",
            "type": "user"
          },
          "name": "Zeina Aldallal",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-01T05:51:34.441Z",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb263",
          "name": "Sara Chrouf",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb264",
          "user": {
            "_id": "65276c7911a8a521c91bc10f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
            "isPro": false,
            "fullname": "Khalil Hennara",
            "user": "Hennara",
            "type": "user"
          },
          "name": "Khalil Hennara",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-01T06:55:15.787Z",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb265",
          "user": {
            "_id": "63aa7667769a10efc404fbbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63aa7667769a10efc404fbbc/tn8ZxUmTEMS0Gze7_F7JL.jpeg",
            "isPro": false,
            "fullname": "Mohamed Motasim Hamed",
            "user": "Moatasem444",
            "type": "user"
          },
          "name": "Mohamed Motaism Hamed",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-01T07:00:18.613Z",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb266",
          "user": {
            "_id": "6496df4b3c64d75523a11973",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6496df4b3c64d75523a11973/I_Qn5-3Czngle-NsGmabO.jpeg",
            "isPro": false,
            "fullname": "Muhammad Hreden",
            "user": "hr99",
            "type": "user"
          },
          "name": "Muhammad Hreden",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-01T07:16:03.684Z",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb267",
          "name": "Safwan AlModhayan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T13:37:24.000Z",
      "submittedOnDailyAt": "2025-05-01T05:10:38.792Z",
      "title": "사데드：아랍어의 번역주석을 추가하기 위한 소규모 언어 모델",
      "submittedOnDailyBy": {
        "_id": "65276c7911a8a521c91bc10f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
        "isPro": false,
        "fullname": "Khalil Hennara",
        "user": "Hennara",
        "type": "user"
      },
      "summary": "아랍어의 디아클리픽션은 언어의 어휘의 풍부함으로 인해 자연어 처리 분야에서 장기적인 문제로 남아 있습니다. 본 논문에서는 Kuwain 1.5B Hennara et al. [2025]에 기반한 새로운 접근 방식인 Sadeed를 소개합니다. 이 모델은 다양한 아랍어 코퍼스에서 초기에 훈련된 압축된 모델입니다. Sadeed는 엄격한 데이터 클리닝 및 정규화 프로세스를 통해 생성된 고품질의 디아클리픽션 데이터 세트에서 정밀하게 조정됩니다. 계산 자원이 제한되어 있으며, 이대로도 판매 허가된 대규모 언어 모델과 비교하여 상대적으로 우수한 결과를 얻으며, 유사한 분야에서 훈련된 전통 모델을 초과합니다. 또한 현재의 아랍어의 디아클리픽션의 벤치마크 프로세스의 중요한 한계점을 지적하고, 이러한 문제를 해결하기 위해 SadeedDiac-25라는 새로운 벤치마크를 소개합니다. 이 벤치마크는 다양한 텍스트 장르와 복잡도 수준에서 공정하고 전체적인 평가가 가능한 것입니다. Sadeed와 SadeedDiac-25는 기계 번역, 음성 합성, 언어 학습 도구 등 아랍어의 NLP 애플리케이션의 발전에 강력한 기초를 제공합니다.",
      "upvotes": 40,
      "discussionId": "68130be65342cbe1ddefb2a6",
      "ai_keywords": [
        "decoder-only language model",
        "Kuwain 1.5B Hennara",
        "fine-tuned",
        "diacritized datasets",
        "data-cleaning",
        "normalization",
        "benchmarking",
        "SadeedDiac-25"
      ]
    },
    "publishedAt": "2025-04-30T09:37:24.000Z",
    "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model",
    "summary": "Arabic text diacritization remains a persistent challenge in natural language\nprocessing due to the language's morphological richness. In this paper, we\nintroduce Sadeed, a novel approach based on a fine-tuned decoder-only language\nmodel adapted from Kuwain 1.5B Hennara et al. [2025], a compact model\noriginally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully\ncurated, high-quality diacritized datasets, constructed through a rigorous\ndata-cleaning and normalization pipeline. Despite utilizing modest\ncomputational resources, Sadeed achieves competitive results compared to\nproprietary large language models and outperforms traditional models trained on\nsimilar domains. Additionally, we highlight key limitations in current\nbenchmarking practices for Arabic diacritization. To address these issues, we\nintroduce SadeedDiac-25, a new benchmark designed to enable fairer and more\ncomprehensive evaluation across diverse text genres and complexity levels.\nTogether, Sadeed and SadeedDiac-25 provide a robust foundation for advancing\nArabic NLP applications, including machine translation, text-to-speech, and\nlanguage learning tools.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21635.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65276c7911a8a521c91bc10f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
      "fullname": "Khalil Hennara",
      "name": "Hennara",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.21776",
      "authors": [
        {
          "_id": "6812d593060494e99e4835e0",
          "name": "Xiaoxi Li",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e1",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e2",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e3",
          "name": "Hongjin Qian",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e4",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e5",
          "name": "Yongkang Wu",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e6",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e7",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T16:25:25.000Z",
      "submittedOnDailyAt": "2025-05-01T00:33:55.498Z",
      "title": "WebThinker: 깊은 연구에 기반한 대규모 추론 모형의 권능 부여",
      "submittedOnDailyBy": {
        "_id": "61cd4b833dd34ba1985e0753",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
        "isPro": false,
        "fullname": "KABI",
        "user": "dongguanting",
        "type": "user"
      },
      "summary": "대규모 논리 모델(LRMs)、특히 OpenAI-o1과 DeepSeek-R1을 대표하는 모델은 장기적인 논리 능력이 있음을 보여줍니다. 그러나 이러한 모델들은 정적 내부 지식에 의존하며, 복잡한 지식밀도 작업에 대한 성능이 제한되어 있으며, 여러 네트워크 정보의 통합이 필요한 연구보고서의 상세한 연구에 도움이 되는 능력이 제한되어 있습니다. 이러한 문제를 해결하기 위해, WebThinker라는 깊은 연구 에이전트를 제안합니다. WebThinker는 LRMs를 자동으로 웹에서 검색하고 페이지를 이동하여 연구보고서의 초안을 작성할 수 있도록 합니다. WebThinker는 Deep Web Explorer 모듈을 포함하고 있으며, 지식의 결함이 발생할 때 웹에서 정보를 동적으로 검색하고 얻을 수 있도록 합니다. 또한 자동적인 Think-Search-and-Draft 전략을 사용하여 모델은 논리, 정보의 집합, 보고서의 작성을 시간적으로 순차적으로 결합할 수 있습니다. 또한, RL 기반의 훈련 전략을 통해 여러 온라인 Direct Preference Optimization(DPO)를 통해 연구 도구의 사용을 촉진합니다. 복잡한 논리 벤치마크(GPQA, GAIA, WebWalkerQA, HLE)와 과학 보고서의 생성 작업(Glaive)에서, WebThinker는 현재의 방법보다 크게 우월함을 입증합니다. 우리의 접근법은 LRMs의 신뢰성과 적용 가능성의 복잡한 스크립마에 대한 향상과 더 나은 능력과 다양한 깊은 연구 시스템의 준비를 위해 준비되어 있습니다. 코드는 https://github.com/RUC-NLPIR/WebThinker에 공개되어 있습니다.",
      "upvotes": 18,
      "discussionId": "6812d594060494e99e48361c",
      "ai_keywords": [
        "Large reasoning models (LRMs)",
        "WebThinker",
        "Deep Web Explorer",
        "Autonomous Think-Search-and-Draft strategy",
        "RL-based training strategy",
        "iterative online Direct Preference Optimization (DPO)",
        "GPQA",
        "GAIA",
        "WebWalkerQA",
        "HLE",
        "Glaive"
      ]
    },
    "publishedAt": "2025-04-30T12:25:25.000Z",
    "title": "WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability",
    "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose WebThinker, a deep research agent that empowers LRMs\nto autonomously search the web, navigate web pages, and draft research reports\nduring the reasoning process. WebThinker integrates a Deep Web\nExplorer module, enabling LRMs to dynamically search, navigate, and extract\ninformation from the web when encountering knowledge gaps. It also employs an\nAutonomous Think-Search-and-Draft strategy, allowing the model to\nseamlessly interleave reasoning, information gathering, and report writing in\nreal time. To further enhance research tool utilization, we introduce an\nRL-based training strategy via iterative online Direct Preference\nOptimization (DPO). Extensive experiments on complex reasoning benchmarks\n(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)\ndemonstrate that WebThinker significantly outperforms existing methods and\nstrong proprietary systems. Our approach enhances LRM reliability and\napplicability in complex scenarios, paving the way for more capable and\nversatile deep research systems. The code is available at\nhttps://github.com/RUC-NLPIR/WebThinker.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61cd4b833dd34ba1985e0753",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
      "fullname": "KABI",
      "name": "dongguanting",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21233",
      "authors": [
        {
          "_id": "6812d62ae74b39182bd17c9c",
          "name": "Haoran Xu",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17c9d",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17c9e",
          "name": "Hany Awadalla",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17c9f",
          "name": "Dongdong Chen",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca0",
          "name": "Yen-Chun Chen",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca1",
          "name": "Mei Gao",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca2",
          "name": "Young Jin Kim",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca3",
          "name": "Yunsheng Li",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca4",
          "name": "Liliang Ren",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca5",
          "name": "Yelong Shen",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca6",
          "name": "Shuohang Wang",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca7",
          "name": "Weijian Xu",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca8",
          "name": "Jianfeng Gao",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca9",
          "name": "Weizhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T00:04:35.000Z",
      "submittedOnDailyAt": "2025-05-01T00:32:47.316Z",
      "title": "Phi-4-Mini-Reasoning: 수학에서 작은 논리언어 모델의 한계를 찾는다.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Chain-of-Thought (CoT)는 대규모 언어 모델 (LLMs)의 형식적 추론 능력을 크게 향상시키기 위해 모델을 실제적인 중간 추론 단계를 생성하도록 훈련시키는 것이 효과적이다. 그러나 소규모 언어 모델 (SLMs)의 추론 능력을 향상시키기는 모델 용량의 한계로 어려워진다. Deepseek-R1의 최근 연구에서 LLM에서 생성된 합성 데이터로부터의 경험 축소가 SLM의 추론 능력을 크게 향상시키는 것을 보여주고 있다. 그러나 구체적인 모델링 레시피는 공개되지 않았다. 본 논문에서는 SLMs의 체계적인 훈련 레시피를 제안한다. 이 레시피는 다음과 같은 4단계로 구성됨: 1) 다양한 경험 축소 데이터에 의한 대규모 중간 훈련, 2) 고품질의 긴 경험 축소 데이터에 의한 규범적 조정, 3) Rollout DPO를 통해 더 신중하게 선택된 선호 데이터 세트의 사용, 4) 증명 가능한 보상을 이용한 강화학습 (RL). 이 방법을 Phi-4-Mini (380M 파라미터의 소규모 모델)에 적용하여, Phi-4-Mini-Reasoning 모델은 수리 추론 태스크에서 더 큰 추론 모델을 초월한다. 예를 들어, DeepSeek-R1-Distill-Qwen-7B을 3.2점, DeepSeek-R1-Distill-Llama-8B을 7.7점 초과한다. 이 결과를 통해, 고품질의 CoT 데이터를 사용하여 신중하게 설계된 훈련 레시피가 자원 제한된 소규모 모델에서도 강력한 추론 능력을 발휘할 수 있음을 증명한다.",
      "upvotes": 14,
      "discussionId": "6812d62be74b39182bd17cdb",
      "ai_keywords": [
        "Chain-of-Thought (CoT)",
        "Large Language Models (LLMs)",
        "Small Language Models (SLMs)",
        "distillation",
        "synthetic data",
        "mid-training",
        "diverse distilled long-CoT data",
        "supervised fine-tuning",
        "high-quality long-CoT data",
        "Rollout DPO",
        "preference dataset",
        "Reinforcement Learning (RL)",
        "Verifiable Reward",
        "Phi-4-Mini",
        "resource-constrained small models"
      ]
    },
    "publishedAt": "2025-04-29T20:04:35.000Z",
    "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math",
    "summary": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities\nin Large Language Models (LLMs) by training them to explicitly generate\nintermediate reasoning steps. While LLMs readily benefit from such techniques,\nimproving reasoning in Small Language Models (SLMs) remains challenging due to\ntheir limited model capacity. Recent work by Deepseek-R1 demonstrates that\ndistillation from LLM-generated synthetic data can substantially improve the\nreasoning ability of SLM. However, the detailed modeling recipe is not\ndisclosed. In this work, we present a systematic training recipe for SLMs that\nconsists of four steps: (1) large-scale mid-training on diverse distilled\nlong-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)\nRollout DPO leveraging a carefully curated preference dataset, and (4)\nReinforcement Learning (RL) with Verifiable Reward. We apply our method on\nPhi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning\nmodel exceeds, on math reasoning tasks, much larger reasoning models, e.g.,\noutperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and\nDeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate\nthat a carefully designed training recipe, with large-scale high-quality CoT\ndata, is effective to unlock strong reasoning capabilities even in\nresource-constrained small models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21233.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6753
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20966",
      "authors": [
        {
          "_id": "6812e473060494e99e4c5116",
          "name": "Zayd M. K. Zuhri",
          "hidden": false
        },
        {
          "_id": "6812e473060494e99e4c5117",
          "name": "Erland Hilman Fuadi",
          "hidden": false
        },
        {
          "_id": "6812e473060494e99e4c5118",
          "name": "Alham Fikri Aji",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60cf8a354061635e43b28f60/ZQTvDnn0F_MaUg5zlVk62.png"
      ],
      "publishedAt": "2025-04-29T17:36:18.000Z",
      "submittedOnDailyAt": "2025-05-01T01:38:03.643Z",
      "title": "Softpick: 무注意力 사ン크, Rectified Softmax에 의한 큰 활성화 노드 없음",
      "submittedOnDailyBy": {
        "_id": "60cf8a354061635e43b28f60",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cf8a354061635e43b28f60/o8gv9mG5cvnopJZJnNibi.jpeg",
        "isPro": true,
        "fullname": "Zayd Muhammad Kawakibi Zuhri",
        "user": "zaydzuhri",
        "type": "user"
      },
      "summary": "ソフトピック, Transformer Attention Mechanism에서의 ソフトマックス의 수정版을 소개합니다. 이 수정된 ソフトマックス는 총합이 1이 되지 않지만, 직접 대입이 가능한 것을 소개합니다. ソフトピック는 Attention Sink와 많은 활성화를 줄이는 방식으로, 성능을 유지하면서 총합이 1이 되지 않는 효과를 보입니다. 340M 파라미터 모델의 실험에서, ソフトピック는 표준 벤치마크에서의 성능과 ソフトマックス가 동일하며, Attention Sink율은 0%에 도달했습니다. ソフトピック Transformer는 カルチオス(340 vs 33,510)의 낮은 은닉 상태를 생성하고, 희소한 Attention Map(46.97%의 희소성)을 생성합니다. ソフトピック을 사용하는 모델은 ソフトマックス보다 자동화 시 계속적으로 우수하며, 특히 저 비트 정확도를 사용할 때 특히 효과적이다. 분석과 논의에서, ソフトピック는 자동화, 저 비트 정확도 훈련, 희소 최적화, 평면, 해석성에 새로운 가능성을 열어주는 것을 보여줍니다. 코드는 https://github.com/zaydzuhri/softpick-attention에서 사용 가능합니다.",
      "upvotes": 11,
      "discussionId": "6812e475060494e99e4c519f",
      "ai_keywords": [
        "softpick",
        "rectified",
        "drop-in replacement",
        "softmax",
        "transformer attention mechanisms",
        "attention sink",
        "massive activations",
        "performance parity",
        "kurtosis",
        "sparse attention maps",
        "quantized",
        "bit precisions",
        "quantization",
        "low-precision training",
        "sparsity optimization",
        "pruning",
        "interpretability"
      ]
    },
    "publishedAt": "2025-04-29T13:36:18.000Z",
    "title": "Softpick: No Attention Sink, No Massive Activations with Rectified\n  Softmax",
    "summary": "We introduce softpick, a rectified, not sum-to-one, drop-in replacement for\nsoftmax in transformer attention mechanisms that eliminates attention sink and\nmassive activations. Our experiments with 340M parameter models demonstrate\nthat softpick maintains performance parity with softmax on standard benchmarks\nwhile achieving 0% sink rate. The softpick transformer produces hidden states\nwith significantly lower kurtosis (340 vs 33,510) and creates sparse attention\nmaps (46.97% sparsity). Models using softpick consistently outperform softmax\nwhen quantized, with particularly pronounced advantages at lower bit\nprecisions. Our analysis and discussion shows how softpick has the potential to\nopen new possibilities for quantization, low-precision training, sparsity\noptimization, pruning, and interpretability. Our code is available at\nhttps://github.com/zaydzuhri/softpick-attention.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60cf8a354061635e43b28f60/ZQTvDnn0F_MaUg5zlVk62.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20966.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "60cf8a354061635e43b28f60",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cf8a354061635e43b28f60/o8gv9mG5cvnopJZJnNibi.jpeg",
      "fullname": "Zayd Muhammad Kawakibi Zuhri",
      "name": "zaydzuhri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21318",
      "authors": [
        {
          "_id": "6812d3c3e74b39182bd0dc82",
          "name": "Marah Abdin",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc83",
          "name": "Sahaj Agarwal",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc84",
          "name": "Ahmed Awadallah",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc85",
          "name": "Vidhisha Balachandran",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc86",
          "name": "Harkirat Behl",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc87",
          "name": "Lingjiao Chen",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc88",
          "name": "Gustavo de Rosa",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc89",
          "name": "Suriya Gunasekar",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8a",
          "name": "Mojan Javaheripi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8b",
          "name": "Neel Joshi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8c",
          "name": "Piero Kauffmann",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8d",
          "name": "Yash Lara",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8e",
          "name": "Caio César Teodoro Mendes",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8f",
          "name": "Arindam Mitra",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc90",
          "name": "Besmira Nushi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc91",
          "name": "Dimitris Papailiopoulos",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc92",
          "name": "Olli Saarikivi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc93",
          "name": "Shital Shah",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc94",
          "name": "Vaishnavi Shrivastava",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc95",
          "name": "Vibhav Vineet",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc96",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc97",
          "name": "Safoora Yousefi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc98",
          "name": "Guoqing Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T05:05:09.000Z",
      "submittedOnDailyAt": "2025-05-01T00:22:34.583Z",
      "title": "Phi-4-reasoning 기술보고서",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "フィーシー・4・ライニング, 140억 파라미터의 복잡한 논리任務에서 우수한 성능을 달성하는 논리 모델을 소개합니다. フィーシー・4에 의한规格化의 최종튜닝을 통해, 적절한 복잡성과 다양성을 가진「学えられる」プロンプト의選択とo3-mini를 사용하여 논리例を生成し、推論時の計算を効果的に活用した詳細な理由論のチェーンを生成します。また, オプチマイズされた学習を通じて開発されたフィーシー・4・ライニング・プラスは, 長い理由論のトレースを生成し、より高い性能を収めます。理由論任務の広い範囲で, 両モデルはDeepSeek-R1-Distill-Llama-70Bモデルや全DeepSeek-R1モデルの性能を超え、より大きなオープンウェイトモデルを凌駈します。数学と科学の理由論、コーディング、アルゴリズムの問題解決、計画、空間的理解の評価ベンチマークを拡張した詳細な評価を行います。興味深いながら, 一般用のベンチマークにも非凡的な改善が見られます。このレポートでは, 学習データ、学習手法、評価についてのフィードバックを提供します。SFTの规格化のデータの规格化の利益は理由論モデルにも延び、RLによって進化させることができます。最後に, 理由論モデルの性能と強固性の評価方法の向上の機会を指摘します。",
      "upvotes": 8,
      "discussionId": "6812d3c4e74b39182bd0dcd1",
      "ai_keywords": [
        "parameter reasoning model",
        "supervised fine-tuning",
        "reasoning demonstrations",
        "inference-time compute",
        "outcome-based reinforcement learning",
        "reasoning chains",
        "reasoning traces",
        "reasoning language models",
        "general-purpose benchmarks",
        "performance assessment",
        "robustness assessment"
      ]
    },
    "publishedAt": "2025-04-30T01:05:09.000Z",
    "title": "Phi-4-reasoning Technical Report",
    "summary": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that\nachieves strong performance on complex reasoning tasks. Trained via supervised\nfine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected\nfor the right level of complexity and diversity-and reasoning demonstrations\ngenerated using o3-mini, Phi-4-reasoning generates detailed reasoning chains\nthat effectively leverage inference-time compute. We further develop\nPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based\nreinforcement learning that offers higher performance by generating longer\nreasoning traces. Across a wide range of reasoning tasks, both models\noutperform significantly larger open-weight models such as\nDeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full\nDeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and\nscientific reasoning, coding, algorithmic problem solving, planning, and\nspatial understanding. Interestingly, we observe a non-trivial transfer of\nimprovements to general-purpose benchmarks as well. In this report, we provide\ninsights into our training data, our training methodologies, and our\nevaluations. We show that the benefit of careful data curation for supervised\nfine-tuning (SFT) extends to reasoning language models, and can be further\namplified by reinforcement learning (RL). Finally, our evaluation points to\nopportunities for improving how we assess the performance and robustness of\nreasoning models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21318.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6753
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.19720",
      "authors": [
        {
          "_id": "68118b63439785d7ff81a879",
          "user": {
            "_id": "647e96507f9ad5e44bac50bd",
            "avatarUrl": "/avatars/2551f99401540676cfe5cd0ed99e70c1.svg",
            "isPro": false,
            "fullname": "Ranran Zhen",
            "user": "zenRRan",
            "type": "user"
          },
          "name": "Ranran Zhen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:56:15.143Z",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87a",
          "name": "Juntao Li",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87b",
          "name": "Yixin Ji",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87c",
          "name": "Zhenlin Yang",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87d",
          "name": "Tong Liu",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87e",
          "name": "Qingrong Xia",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87f",
          "name": "Xinyu Duan",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a880",
          "name": "Zhefeng Wang",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a881",
          "name": "Baoxing Huai",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a882",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T12:14:02.000Z",
      "submittedOnDailyAt": "2025-05-01T02:08:35.108Z",
      "title": "타이다ン즈의 제어: 효율적인 LLM 추론 서비스의 셈플\n\n(注意: \"シーケット\"는 \"스탠드\"로 번역되지만, \"스탠드\"는 주로 소프트웨어 개발이나 데이터 처리 분야에서 사용됨. 따라서 \"스탠드\"를 사용하여도 무관하지만, \"스탠드\"가 더 적합한 경우를 고려할 수 있습니다.)",
      "submittedOnDailyBy": {
        "_id": "647e96507f9ad5e44bac50bd",
        "avatarUrl": "/avatars/2551f99401540676cfe5cd0ed99e70c1.svg",
        "isPro": false,
        "fullname": "Ranran Zhen",
        "user": "zenRRan",
        "type": "user"
      },
      "summary": "대규모 언어 모델(LLMs)가 생성 AI 분야에서 놀라운 발전을 거두고 있으며, 복잡한 방식과 광범위한 도구로서 다양한 분야와 애플리케이션에서 광범위하게 도입되고 있습니다. 그러나 그 큰 파라미터 수에 따른 큰 메모리 오버헤드와 注意 구조의 높은 계산 요구는 LLM 추론 서비스의 저 성능과 고 트랜스포머 수준에 대한 큰 문제점을 드러냅니다. 최근, 先進의 연구에 의해 이 분야에서 크게 가속화되고 있습니다. 이 논문은 이러한 방법들에 대해 전면적인 조사를 제공하며, 기본적인 인스턴스 수준의 접근, 깊은 클러스터 수준의 전략,新兴의 스케나리오, 그리고 다른 여러 중요 분야를 포함하여 기록되어 있습니다. 인스턴스 수준에서는 모델의 구성, 요청 스케줄링, 디코딩 길이 예측, 저장 관리, 그리고 분산 패러다임에 대해 조사하고 있습니다. 클러스터 수준에서는 GPU 클러스터의 사용, 멀티 인스턴스의 부하 균형, 클라우드 서비스의 해결책에 대해 조사하고 있습니다.新兴의 스케나리오에서는 특정의 태스크, 모듈, 그리고 보조 메소드에 대해 논의를 진행하고 있습니다. 전체적인 개요를 보장하기 위해 다양한 중요한 분야를 특히 언급하고 있습니다. 마지막으로, 이 분야를 발전시키기 위한 잠재적인 연구 방향을 제시하고 있습니다.",
      "upvotes": 7,
      "discussionId": "68118b64439785d7ff81a8cd",
      "githubRepo": "https://github.com/zenrran4nlp/Awesome-LLM-Inference-Serving",
      "ai_keywords": [
        "large language models (LLMs)",
        "generative AI",
        "attention mechanism",
        "model placement",
        "request scheduling",
        "decoding length prediction",
        "storage management",
        "disaggregation paradigm",
        "GPU cluster deployment",
        "multi-instance load balancing",
        "cloud service solutions"
      ]
    },
    "publishedAt": "2025-04-28T08:14:02.000Z",
    "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving",
    "summary": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647e96507f9ad5e44bac50bd",
      "avatarUrl": "/avatars/2551f99401540676cfe5cd0ed99e70c1.svg",
      "fullname": "Ranran Zhen",
      "name": "zenRRan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18904",
      "authors": [
        {
          "_id": "6812d36790b45f422b0bfdc2",
          "name": "Haoran Geng",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc3",
          "name": "Feishi Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc4",
          "name": "Songlin Wei",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc5",
          "name": "Yuyang Li",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc6",
          "name": "Bangjun Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc7",
          "name": "Boshi An",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc8",
          "name": "Charlie Tianyue Cheng",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc9",
          "name": "Haozhe Lou",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdca",
          "name": "Peihao Li",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcb",
          "name": "Yen-Jen Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcc",
          "name": "Yutong Liang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcd",
          "name": "Dylan Goetting",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdce",
          "name": "Chaoyi Xu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcf",
          "name": "Haozhe Chen",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd0",
          "name": "Yuxi Qian",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd1",
          "name": "Yiran Geng",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd2",
          "name": "Jiageng Mao",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd3",
          "name": "Weikang Wan",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd4",
          "name": "Mingtong Zhang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd5",
          "name": "Jiangran Lyu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd6",
          "name": "Siheng Zhao",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd7",
          "name": "Jiazhao Zhang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd8",
          "name": "Jialiang Zhang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd9",
          "name": "Chengyang Zhao",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdda",
          "name": "Haoran Lu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddb",
          "name": "Yufei Ding",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddc",
          "name": "Ran Gong",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddd",
          "name": "Yuran Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdde",
          "name": "Yuxuan Kuang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddf",
          "name": "Ruihai Wu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde0",
          "name": "Baoxiong Jia",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde1",
          "name": "Carlo Sferrazza",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde2",
          "name": "Hao Dong",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde3",
          "name": "Siyuan Huang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde4",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde5",
          "name": "Jitendra Malik",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde6",
          "name": "Pieter Abbeel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-26T12:31:04.000Z",
      "submittedOnDailyAt": "2025-05-01T00:21:25.704Z",
      "title": "로보바ー스：확장성 및 일반화 가능한 로봇 학습을 위한 통합 플랫폼, 데이터 셋 및 벤치마크에 대한 정보",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "데이터 스케일링과 표준화된 평가 벤치마크는 자연어 처리와 컴퓨터 비전 분야에서 큰 발전을 촉발하고 있습니다. 그러나 로봇학은 데이터 스케일링과 평가 프로토콜 구축에 특유의 문제점을 가지고 있습니다. 실세계 데이터의 수집은 자원적으로 효율적이지 않으며 복잡합니다. 또한 실세계 시나리오에서의 벤치마크는 매우 복잡합니다. 합성 데이터와 시뮬레이션은 유망한 대안이 될 수 있지만, 현재의 노력을 데이터의 품질, 다양성과 벤치마크의 표준화에 약점을 가지고 있습니다. 이러한 문제를 해결하기 위해, 우리는 RoboVerse를 소개합니다. 이것은 시뮬레이션 플랫폼, 합성 데이터 세트, 단위 벤치마크를 구성하는 일체화된 프레임워크입니다. 시뮬레이션 플랫폼은 여러 시뮬레이터와 로봇의 구성을 지원하고, 다양한 환경의 차이를 무시하여 순환 가능한 방식으로 설계되어 있습니다. 합성 데이터 세트는 고정밀의 물리와 현실적인 렌더링을 특징으로, 다양한 접근법을 통해 구축되어 있습니다. 또한, 우리는 학습 기능의 표준화를 위해 모사 학습과 강화 학습의 단위 벤치마크를 제안하고 있습니다. 이들은 다양한 일반화 수준에서의 평가가 가능합니다. 시뮬레이션 플랫폼의 중심은 MetaSim입니다. 이것은 다양한 시뮬레이션 환경을 하나의 일반적인 인터페이스로 추상화하고, 기존의 시뮬레이션 환경을 시뮬레이터 무관한 설정 시스템으로 재구성하며, 시뮬레이터의 기능을 표준화하는 API를 제공합니다. 이 추상화는 상호 교환성과 확장성을 보장합니다. 세부적인 실험은 RoboVerse가 모사 학습, 강화 학습, 월드 모델 학습, 시뮬레이션에서 실체로의 전환의 성능을 향상시키는 것을 보여주었습니다. 이러한 결과를 통해, 데이터 세트와 벤치마크의 신뢰성을 입증하고, RoboVerse가 로봇 학습의 발전에 강력한 해결책으로 채택될 수 있음을 확립했습니다.",
      "upvotes": 6,
      "discussionId": "6812d36c90b45f422b0bff56",
      "ai_keywords": [
        "simulation platform",
        "synthetic dataset",
        "unified benchmarks",
        "MetaSim",
        "simulator-agnostic configuration system",
        "API",
        "physics engine",
        "imitation learning",
        "reinforcement learning",
        "world model learning",
        "sim-to-real transfer"
      ]
    },
    "publishedAt": "2025-04-26T08:31:04.000Z",
    "title": "RoboVerse: Towards a Unified Platform, Dataset and Benchmark for\n  Scalable and Generalizable Robot Learning",
    "summary": "Data scaling and standardized evaluation benchmarks have driven significant\nadvances in natural language processing and computer vision. However, robotics\nfaces unique challenges in scaling data and establishing evaluation protocols.\nCollecting real-world data is resource-intensive and inefficient, while\nbenchmarking in real-world scenarios remains highly complex. Synthetic data and\nsimulation offer promising alternatives, yet existing efforts often fall short\nin data quality, diversity, and benchmark standardization. To address these\nchallenges, we introduce RoboVerse, a comprehensive framework comprising a\nsimulation platform, a synthetic dataset, and unified benchmarks. Our\nsimulation platform supports multiple simulators and robotic embodiments,\nenabling seamless transitions between different environments. The synthetic\ndataset, featuring high-fidelity physics and photorealistic rendering, is\nconstructed through multiple approaches. Additionally, we propose unified\nbenchmarks for imitation learning and reinforcement learning, enabling\nevaluation across different levels of generalization. At the core of the\nsimulation platform is MetaSim, an infrastructure that abstracts diverse\nsimulation environments into a universal interface. It restructures existing\nsimulation environments into a simulator-agnostic configuration system, as well\nas an API aligning different simulator functionalities, such as launching\nsimulation environments, loading assets with initial states, stepping the\nphysics engine, etc. This abstraction ensures interoperability and\nextensibility. Comprehensive experiments demonstrate that RoboVerse enhances\nthe performance of imitation learning, reinforcement learning, world model\nlearning, and sim-to-real transfer. These results validate the reliability of\nour dataset and benchmarks, establishing RoboVerse as a robust solution for\nadvancing robot learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18904.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6753
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21850",
      "authors": [
        {
          "_id": "6812d4d6ce88881cb51b3b70",
          "name": "Xindi Wu",
          "hidden": false
        },
        {
          "_id": "6812d4d6ce88881cb51b3b71",
          "name": "Hee Seung Hwang",
          "hidden": false
        },
        {
          "_id": "6812d4d6ce88881cb51b3b72",
          "name": "Polina Kirichenko",
          "hidden": false
        },
        {
          "_id": "6812d4d6ce88881cb51b3b73",
          "name": "Olga Russakovsky",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T17:57:22.000Z",
      "submittedOnDailyAt": "2025-05-01T00:32:13.807Z",
      "title": "COMPACT: 복잡한 시각화 능력을 조정하기 위해 구성된 원자부터 시작합니다.",
      "submittedOnDailyBy": {
        "_id": "613940c0905b1938233881e3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/613940c0905b1938233881e3/Vb4kFWKEq6AILUP9KmCZA.png",
        "isPro": false,
        "fullname": "Xindi Wu",
        "user": "xindiw",
        "type": "user"
      },
      "summary": "マルチモーダル大語言モデル（MLLMs）는 단순한 시각言語タスク에서 뛰어난 성능을 보입니다が, 복잡한 작업에 대해서는 동시에 물체를 인식하고 세기, 그 공간적 관계를 이해하는 능력이 부족해졌습니다. 이 현상은 Visual Instruction Tuning（VIT）가 MLLMs의 중요한 훈련 단계 중 하나이며, 데이터량을 확장하는 데만 몰두하지만, 훈련 샘플의 구성 복잡성을 고려하지 않는 부분적인 원인입니다. 우리는 COMPACT（구성 복잡성의 원子的から 복잡한 시각 능력의 조정）를 제안하여, 훈련 샘플의 구성 복잡성을 명시적으로 제어한 훈련 데이터 세트를 생성합니다. COMPACT의 데이터는 MLLMs가 원子的 능력의 조합을 학습하고, 복잡한 능력을 효율적으로 학습할 수 있도록 훈련할 수 있습니다. 모든 벤치마크에서 COMPACT는 LLaVA-665k VIT와 비교하여相当한 성능을 달성하며, 그 데이터 버킷의 10% 미만을 사용합니다. 특히, 복잡한 다능성 작업에 대한 경우, COMPACT는 그를 초과하는 수준으로 뛰어납니다. 예를 들어, MMStar과 MM-Vet에 대해, 특히 복잡한 문제에서 4개 이상의 원子的 능력이 필요할 때, COMPACT는 LLaVA-665k VIT의 데이터 버킷의 10% 미만을 사용하여 83.3%의 큰 향상 또는 94.0%의 향상을 달성합니다. COMPACT는 복잡한 시각언어 작업에 대한 개선을 위해, Scalable 및 데이터 효율적인 시각 구성 복잡성 조정의 레시피를 제공합니다.",
      "upvotes": 5,
      "discussionId": "6812d4d7ce88881cb51b3bad",
      "projectPage": "https://princetonvisualai.github.io/compact/",
      "githubRepo": "https://github.com/princetonvisualai/compact",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Visual Instruction Tuning (VIT)",
        "compositional complexity",
        "COMPACT (COMPositional Atomic-to-complex visual Capability Tuning)",
        "atomic capabilities",
        "complex capabilities",
        "MMStar",
        "MM-Vet",
        "visual compositional tuning"
      ]
    },
    "publishedAt": "2025-04-30T13:57:22.000Z",
    "title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning",
    "summary": "Multimodal Large Language Models (MLLMs) excel at simple vision-language\ntasks but struggle when faced with complex tasks that require multiple\ncapabilities, such as simultaneously recognizing objects, counting them, and\nunderstanding their spatial relationships. This might be partially the result\nof the fact that Visual Instruction Tuning (VIT), a critical training step for\nMLLMs, has traditionally focused on scaling data volume, but not the\ncompositional complexity of training examples. We propose COMPACT\n(COMPositional Atomic-to-complex visual Capability Tuning), which generates a\ntraining dataset explicitly controlling for the compositional complexity of the\ntraining examples. The data from COMPACT allows MLLMs to train on combinations\nof atomic capabilities to learn complex capabilities more efficiently. Across\nall benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT\nwhile using less than 10% of its data budget, and even outperforms it on\nseveral, especially those involving complex multi-capability tasks. For\nexample, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%\nimprovement on MM-Vet compared to the full-scale VIT on particularly complex\nquestions that require four or more atomic capabilities. COMPACT offers a\nscalable, data-efficient, visual compositional tuning recipe to improve on\ncomplex visual-language tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21850.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "613940c0905b1938233881e3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/613940c0905b1938233881e3/Vb4kFWKEq6AILUP9KmCZA.png",
      "fullname": "Xindi Wu",
      "name": "xindiw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21855",
      "authors": [
        {
          "_id": "6812db1567abbb1d11fb4e9d",
          "name": "Qihao Liu",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4e9e",
          "name": "Ju He",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4e9f",
          "name": "Qihang Yu",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4ea0",
          "name": "Liang-Chieh Chen",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4ea1",
          "name": "Alan Yuille",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/639f1e519f1f2baab2f00d22/h3IWiXbIAOy1NxTWKDAkO.mp4"
      ],
      "publishedAt": "2025-04-30T17:59:56.000Z",
      "submittedOnDailyAt": "2025-05-01T00:56:11.135Z",
      "title": "Revisión: 고품질, 저비용의 동영상 생성에 있어서 명확한 3D 물리 모델링을 활용한 복잡한 동작 및 상호작용",
      "submittedOnDailyBy": {
        "_id": "639f1e519f1f2baab2f00d22",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg",
        "isPro": true,
        "fullname": "Qihao Liu",
        "user": "QHL067",
        "type": "user"
      },
      "summary": "최근, 이미지 생성 분야에서 뚜렷한 진전을 보이고 있지만, 복잡한 동작과 상호작용의 생성에 여전히 많은 문제를 남아 있습니다. 이러한 문제를 해결하기 위해, ReVision라는 플러그인 및 라이브 프레임워크를 소개합니다. ReVision은 수렴된 조건付き 이미지 생성 모델에 명시적으로 파라미터화된 3D 물리적 지식을 통합하여, 복잡한 동작과 상호작용을 포함하는 고품질의 이미지 생성 능력을 크게 향상시킵니다.\n\n특히, ReVision은 세 가지 단계로 구성되어 있습니다. 먼저, 이미지 디퓨전 모델을 사용하여 대략적인 이미지를 생성합니다. 다음으로, 대략적인 이미지에서 2D와 3D의 특징을 추출하여 3D 대상 중심 표현을 구축합니다. 이 표현은 우리 제안한 파라미터화된 물리적 선행 모델로 정밀화되어, 정확한 3D 동작 시퀀스를 생성합니다. 마지막으로, 이 정밀화된 동작 시퀀스는 동일한 이미지 디퓨전 모델에 되돌려, 추가 조건에 따라 동작의 일관성을 보장하며, 복잡한 액션과 상호작용을 포함하는 경우에도 동작의 일관성을 보장합니다. 우리의 접근 방식의 효과는, Stable Video Diffusion에서 실험을 통해 동작의 충실성과 일관성을 크게 향상시켰습니다. 특히, 15억 파라미터의 최고의 이미지 생성 모델을 초월하는 대폭적인 효과를 보여주며, 1.5억 파라미터로 복잡한 이미지 생성을 수행합니다. 우리의 결과를 통해, 3D 물리적 지식을 통합함으로써 상대적으로 작은 이미지 디퓨전 모델도 복잡한 동작과 상호작용을 생성할 수 있음을 보여줍니다. 이는 물리적으로 가능한 이미지 생성 문제의 해결책으로 유망한 것으로 평가됩니다.",
      "upvotes": 3,
      "discussionId": "6812db1767abbb1d11fb4f42",
      "projectPage": "https://revision-video.github.io/",
      "ai_keywords": [
        "video diffusion model",
        "3D object-centric representation",
        "parameterized physical prior model",
        "motion-consistent videos",
        "3D physical knowledge",
        "motion fidelity",
        "coherence",
        "physically plausible video generation"
      ]
    },
    "publishedAt": "2025-04-30T13:59:56.000Z",
    "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction",
    "summary": "In recent years, video generation has seen significant advancements. However,\nchallenges still persist in generating complex motions and interactions. To\naddress these challenges, we introduce ReVision, a plug-and-play framework that\nexplicitly integrates parameterized 3D physical knowledge into a pretrained\nconditional video generation model, significantly enhancing its ability to\ngenerate high-quality videos with complex motion and interactions.\nSpecifically, ReVision consists of three stages. First, a video diffusion model\nis used to generate a coarse video. Next, we extract a set of 2D and 3D\nfeatures from the coarse video to construct a 3D object-centric representation,\nwhich is then refined by our proposed parameterized physical prior model to\nproduce an accurate 3D motion sequence. Finally, this refined motion sequence\nis fed back into the same video diffusion model as additional conditioning,\nenabling the generation of motion-consistent videos, even in scenarios\ninvolving complex actions and interactions. We validate the effectiveness of\nour approach on Stable Video Diffusion, where ReVision significantly improves\nmotion fidelity and coherence. Remarkably, with only 1.5B parameters, it even\noutperforms a state-of-the-art video generation model with over 13B parameters\non complex video generation by a substantial margin. Our results suggest that,\nby incorporating 3D physical knowledge, even a relatively small video diffusion\nmodel can generate complex motions and interactions with greater realism and\ncontrollability, offering a promising solution for physically plausible video\ngeneration.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/639f1e519f1f2baab2f00d22/h3IWiXbIAOy1NxTWKDAkO.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21855.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639f1e519f1f2baab2f00d22",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg",
      "fullname": "Qihao Liu",
      "name": "QHL067",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21039",
      "authors": [
        {
          "_id": "6812f89338bee548818e68a1",
          "user": {
            "_id": "6573a9fe769f3ee9bdf4d9c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xC41F7Vp9SVzVHc3cUiRU.jpeg",
            "isPro": false,
            "fullname": "Paul Kassianik",
            "user": "paulkass",
            "type": "user"
          },
          "name": "Paul Kassianik",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-01T04:29:53.088Z",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a2",
          "name": "Baturay Saglam",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a3",
          "name": "Alexander Chen",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a4",
          "name": "Blaine Nelson",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a5",
          "name": "Anu Vellore",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a6",
          "name": "Massimo Aufiero",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a7",
          "name": "Fraser Burch",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a8",
          "name": "Dhruv Kedia",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a9",
          "name": "Avi Zohary",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68aa",
          "name": "Sajana Weerawardhena",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68ab",
          "name": "Aman Priyanshu",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68ac",
          "name": "Adam Swanda",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68ad",
          "name": "Amy Chang",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68ae",
          "name": "Hyrum Anderson",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68af",
          "name": "Kojin Oshiba",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68b0",
          "name": "Omar Santos",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68b1",
          "name": "Yaron Singer",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68b2",
          "name": "Amin Karbasi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T08:41:12.000Z",
      "submittedOnDailyAt": "2025-05-01T03:07:21.083Z",
      "title": "Llama-3.1-FoundationAI-SecurityLLM-Base-8B 기술보고서",
      "submittedOnDailyBy": {
        "_id": "620042b28c2eb991da50d34e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620042b28c2eb991da50d34e/Q5cj1GIq3XnKj-K64Mtyd.jpeg",
        "isPro": true,
        "fullname": "Aman Priyanshu",
        "user": "AmanPriyanshu",
        "type": "user"
      },
      "summary": "Transformer 기반의 대규모 언어 모델(LLMs)가 사회에 점차 확장되는 가운데, 이들은 소프트웨어 개발, 창의적인 작가 활동, 디지털 아트 등 분야에 혁신적인 영향을 미치고 있습니다. 그러나 사이버 보안 분야에서의 도입은 특화된 훈련 데이터의 부족과 사이버 보안 고유의 지식의 표현의 복잡성 등 문제로 제한되어 있습니다. 이러한 결함이 해결하기 위해, 우리는 Llama 3.1 아키텍처에 기반한 사이버 보안 초점 LLM, Foundation-Sec-8B를 소개합니다. 이 모델은 엄격하게 선택된 사이버 보안 코퍼스에 의한 지속적인 훈련을 통해 강화되었습니다. Foundation-Sec-8B는 신규 및 기존의 사이버 보안 벤치마크에서 평가되었으며, 특정의 사이버 보안 태스크에서 Llama 3.1-70B와 GPT-4o-mini와 같은 성능을 보여주고 있습니다. 모델을 공개하는 것은 인공지능 주도 도구의 발전과 도입을 촉진하고, 공공 및 개인적인 사이버 보안 컨텍스트에서 도구의 도입을 가속화하려고 합니다.",
      "upvotes": 3,
      "discussionId": "6812f89338bee548818e68e3",
      "ai_keywords": [
        "transformer-based large language models (LLMs)",
        "cybersecurity-focused LLM",
        "Llama 3.1 architecture",
        "continued pretraining",
        "cybersecurity corpus",
        "cybersecurity benchmarks",
        "Llama 3.1-70B",
        "GPT-4o-mini",
        "cybersecurity-specific tasks"
      ]
    },
    "publishedAt": "2025-04-28T04:41:12.000Z",
    "title": "Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report",
    "summary": "As transformer-based large language models (LLMs) increasingly permeate\nsociety, they have revolutionized domains such as software engineering,\ncreative writing, and digital arts. However, their adoption in cybersecurity\nremains limited due to challenges like scarcity of specialized training data\nand complexity of representing cybersecurity-specific knowledge. To address\nthese gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on\nthe Llama 3.1 architecture and enhanced through continued pretraining on a\ncarefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across\nboth established and new cybersecurity benchmarks, showing that it matches\nLlama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By\nreleasing our model to the public, we aim to accelerate progress and adoption\nof AI-driven tools in both public and private cybersecurity contexts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21039.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620042b28c2eb991da50d34e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620042b28c2eb991da50d34e/Q5cj1GIq3XnKj-K64Mtyd.jpeg",
      "fullname": "Aman Priyanshu",
      "name": "AmanPriyanshu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21336",
      "authors": [
        {
          "_id": "68133878475d26db59993ae2",
          "name": "Linshan Wu",
          "hidden": false
        },
        {
          "_id": "68133878475d26db59993ae3",
          "name": "Yuxiang Nie",
          "hidden": false
        },
        {
          "_id": "68133878475d26db59993ae4",
          "name": "Sunan He",
          "hidden": false
        },
        {
          "_id": "68133878475d26db59993ae5",
          "name": "Jiaxin Zhuang",
          "hidden": false
        },
        {
          "_id": "68133878475d26db59993ae6",
          "name": "Hao Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T05:51:48.000Z",
      "submittedOnDailyAt": "2025-05-01T07:32:18.778Z",
      "title": "UniBiomed: WIRO-COMBINATORIAL 기반의 기초 모델로, 기초가 있는 바이오MEDICAL 이미지 해석\n\n(注意：WIRO-COMBINATORIAL는 原文中的 \"WIRO-COMBINATORIAL\" 直接翻译而来，但在韩语中可能需要根据具体上下文进行调整。这里保持原文的翻译以确保准确性。)",
      "submittedOnDailyBy": {
        "_id": "65d86ea2685624d5f206d7ec",
        "avatarUrl": "/avatars/b9bc4c398d5def393bc782e9a7c5e302.svg",
        "isPro": false,
        "fullname": "Linshan Wu",
        "user": "Luffy503",
        "type": "user"
      },
      "summary": "다모달 생물의학 영상 해석은 생물의학 영상 분석의 새로운 기회를 개척하고 있습니다. 전통적인 AI 접근법은 일반적으로 분리된 훈련을 기반으로 합니다. 즉, LLMs(Large Language Models)를 사용하여 임상 문서 생성을 수행하고, 목표 추출 모델을 사용하여 목표 추출을 수행합니다. 이 접근법은 실제 세계에서의 실행의 적절성과 전체적인 생물의학 정보의 활용에 실패합니다. 이에 따라, 우리는 UniBiomed를 소개합니다. 이는 처음의 일반적인 기반 모델이며, 기초적인 생물의학 영상 해석에 설계되어 있습니다. UniBiomed는 새로운 Multi-modal Large Language Model(MLLM)과 Segment Anything Model(SAM)의 통합을 기반으로 임상 문서의 생성과 상대적인 생물의학 객체의 분할을 효과적으로 통합합니다. 이렇게 UniBiomed는 10가지의 다양한 생물의학 영상 모델로 광범위한 생물의학 태스크를 해결할 수 있습니다. UniBiomed의 개발 과정에서, 10가지의 영상 모델에 대한 2700만 쌍 이상의 이미지, 注釈, 문서 설명과 같은 규모적인 데이터 세트를 선택했습니다. 84 페이지의 내부 및 외부 데이터 세트에 대한 확장 검증을 통해, UniBiomed는 분할, 질병 인식, 영역에 대한 진단, 시각화 문제 답변, 보고서 생성 등에서 가장 선진적인 성능을 달성했습니다. 또한, 이전의 모델은 임상 전문의의 예측과 손동으로 정밀한 문서 및 시각화 프로ン퓰트의 생성에 의존했습니다. 그러나 UniBiomed는 자동적이고 끝부터 끝까지의 기초적인 해석을 제공하여 생물의학 영상 분석에서 자동화된 해석을 실현합니다. 이는 임상 작업 흐름에서 새로운 패러다임 전환을 의미하며 진단 효율을 크게 향상시킵니다. 총적으로, UniBiomed는 생물의학 AI의 새로운 혁신을 상징하며, 더 정확한 효율적인 생물의학 영상 분석의 기초적인 해석 능력을 개발합니다.",
      "upvotes": 2,
      "discussionId": "6813387e475d26db59993c9e",
      "ai_keywords": [
        "Multi-modal Large Language Model (MLLM)",
        "Segment Anything Model (SAM)",
        "grounded biomedical image interpretation",
        "biomedical tasks",
        "biomedical imaging modalities",
        "segmentation",
        "disease recognition",
        "region-aware diagnosis",
        "visual question answering",
        "report generation",
        "grounded interpretation"
      ]
    },
    "publishedAt": "2025-04-30T01:51:48.000Z",
    "title": "UniBiomed: A Universal Foundation Model for Grounded Biomedical Image\n  Interpretation",
    "summary": "Multi-modal interpretation of biomedical images opens up novel opportunities\nin biomedical image analysis. Conventional AI approaches typically rely on\ndisjointed training, i.e., Large Language Models (LLMs) for clinical text\ngeneration and segmentation models for target extraction, which results in\ninflexible real-world deployment and a failure to leverage holistic biomedical\ninformation. To this end, we introduce UniBiomed, the first universal\nfoundation model for grounded biomedical image interpretation. UniBiomed is\nbased on a novel integration of Multi-modal Large Language Model (MLLM) and\nSegment Anything Model (SAM), which effectively unifies the generation of\nclinical texts and the segmentation of corresponding biomedical objects for\ngrounded interpretation. In this way, UniBiomed is capable of tackling a wide\nrange of biomedical tasks across ten diverse biomedical imaging modalities. To\ndevelop UniBiomed, we curate a large-scale dataset comprising over 27 million\ntriplets of images, annotations, and text descriptions across ten imaging\nmodalities. Extensive validation on 84 internal and external datasets\ndemonstrated that UniBiomed achieves state-of-the-art performance in\nsegmentation, disease recognition, region-aware diagnosis, visual question\nanswering, and report generation. Moreover, unlike previous models that rely on\nclinical experts to pre-diagnose images and manually craft precise textual or\nvisual prompts, UniBiomed can provide automated and end-to-end grounded\ninterpretation for biomedical image analysis. This represents a novel paradigm\nshift in clinical workflows, which will significantly improve diagnostic\nefficiency. In summary, UniBiomed represents a novel breakthrough in biomedical\nAI, unlocking powerful grounded interpretation capabilities for more accurate\nand efficient biomedical image analysis.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21336.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d86ea2685624d5f206d7ec",
      "avatarUrl": "/avatars/b9bc4c398d5def393bc782e9a7c5e302.svg",
      "fullname": "Linshan Wu",
      "name": "Luffy503",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20708",
      "authors": [
        {
          "_id": "6813376e2379095ee572f2f8",
          "name": "Hasan Abed Al Kader Hammoud",
          "hidden": false
        },
        {
          "_id": "6813376e2379095ee572f2f9",
          "name": "Hani Itani",
          "hidden": false
        },
        {
          "_id": "6813376e2379095ee572f2fa",
          "name": "Bernard Ghanem",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/lojSGsqwNqzSQxNp4FHXA.png"
      ],
      "publishedAt": "2025-04-29T12:39:07.000Z",
      "submittedOnDailyAt": "2025-05-01T07:28:39.014Z",
      "title": "마지막으로 답을 넘어서: 당신의 이유론의 흔적이 생각보다 많은 것을 밝혀준다.",
      "submittedOnDailyBy": {
        "_id": "642b51385bf2355d02a23d15",
        "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
        "isPro": true,
        "fullname": "Hasan Abed Al Kader Hammoud",
        "user": "hammh0a",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)은 복잡한 문제를 해결하기 위해 단계별로 이유론을 사용합니다. 표준 평가의 실천은 완전한 이유론의トレース를 생성하고, 최종적인 답변의 정확성을 평가하기 위해 제공된 결론을 평가하는 것입니다. 본 논문에서는 최종적인 답변에 의존하는 것을 의심하고, 다음과 같은 두 가지 질문을 제시합니다: 최종적인 답변이 모델의 최적의 결론을 신뢰적으로 표현하고 있는지? 다른 이유론의 경로가 다른 결과를 가져올 수 있는지? 이러한 질문을 해결하기 위해 중간의 이유론 단계를 \"subthoughts\"라고 부르고, 이를 기반으로 한 방법을 제안합니다. 우리 접근법은 언어적인 괄호에 기반하여 이유론トレー스를 순차적으로 분할하고, 각 subthoughts에서의 다음을 모델에 폼을 줍니다. 각 subthoughts에서 완료된 다음으로부터 잠재적인 답변을 추출합니다. 이러한 답변을 선택하여 가장 빈번한 것(모드)을 선택함으로써, 원의 완전한 트레이스에서 얻을 수 있는 답변보다 크게 높은 정확도를 얻을 수 있습니다. 서로 다른 subthoughts에서 얻은 답변의 일관성을 분석함으로써, 모델의 신뢰성과 정확성에 관련된 특징을 밝혀, 신뢰도가 낮은 답변을 특정할 수 있는 가능성을 제시합니다. LLMs와 도전적인 수학적인 이유론 데이터 세트(AIME2024와 AIME2025)의 광범위한 범위에서의 실험에서, 일치하는 정확도 향상을 보였으며, 각각 13%와 10%의 효과를 보입니다. 구현은 다음과 같은 URL에 있습니다: https://github.com/hammoudhasan/SubthoughtReasoner.",
      "upvotes": 2,
      "discussionId": "6813376f2379095ee572f34c",
      "projectPage": "https://hammoudhasan.github.io/SubthoughtReasoner/",
      "githubRepo": "https://github.com/hammoudhasan/SubthoughtReasoner",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "step-by-step reasoning",
        "reasoning trace",
        "subthoughts",
        "linguistic cues",
        "continuations",
        "aggregating answers",
        "mode",
        "model's confidence",
        "correctness",
        "AIME2024",
        "AIME2025"
      ]
    },
    "publishedAt": "2025-04-29T08:39:07.000Z",
    "title": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You\n  Think",
    "summary": "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex\nproblems. Standard evaluation practice involves generating a complete reasoning\ntrace and assessing the correctness of the final answer presented at its\nconclusion. In this paper, we challenge the reliance on the final answer by\nposing the following two questions: Does the final answer reliably represent\nthe model's optimal conclusion? Can alternative reasoning paths yield different\nresults? To answer these questions, we analyze intermediate reasoning steps,\ntermed subthoughts, and propose a method based on our findings. Our approach\ninvolves segmenting a reasoning trace into sequential subthoughts based on\nlinguistic cues. We start by prompting the model to generate continuations from\nthe end-point of each intermediate subthought. We extract a potential answer\nfrom every completed continuation originating from different subthoughts. We\nfind that aggregating these answers by selecting the most frequent one (the\nmode) often yields significantly higher accuracy compared to relying solely on\nthe answer derived from the original complete trace. Analyzing the consistency\namong the answers derived from different subthoughts reveals characteristics\nthat correlate with the model's confidence and correctness, suggesting\npotential for identifying less reliable answers. Our experiments across various\nLLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)\nshow consistent accuracy improvements, with gains reaching up to 13\\% and 10\\%\nrespectively. Implementation is available at:\nhttps://github.com/hammoudhasan/SubthoughtReasoner.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/lojSGsqwNqzSQxNp4FHXA.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20708.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642b51385bf2355d02a23d15",
      "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
      "fullname": "Hasan Abed Al Kader Hammoud",
      "name": "hammh0a",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  }
]