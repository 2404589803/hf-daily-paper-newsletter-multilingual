[
  {
    "paper": {
      "id": "2505.02567",
      "authors": [
        {
          "_id": "681c7895c7211b7efbc49f17",
          "name": "Xinjie Zhang",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f18",
          "name": "Jintao Guo",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f19",
          "user": {
            "_id": "66ab4c8a1703f12f49583c6d",
            "avatarUrl": "/avatars/59c77d4556edc049bb410e180813d5e3.svg",
            "isPro": false,
            "fullname": "zss",
            "user": "Suikong",
            "type": "user"
          },
          "name": "Shanshan Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T10:07:03.107Z",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1a",
          "name": "Minghao Fu",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1b",
          "name": "Lunhao Duan",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1c",
          "user": {
            "_id": "636f4c6b5d2050767e4a1491",
            "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
            "isPro": false,
            "fullname": "Guo-Hua Wang",
            "user": "Flourish",
            "type": "user"
          },
          "name": "Guo-Hua Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:04.757Z",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1d",
          "name": "Qing-Guo Chen",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1e",
          "name": "Zhao Xu",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1f",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f20",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658a8a837959448ef5500ce5/DwtDeTTjSrZoe2r4sL-a4.png"
      ],
      "publishedAt": "2025-05-05T11:18:03.000Z",
      "submittedOnDailyAt": "2025-05-08T07:57:47.854Z",
      "title": "통합 다모둠 이해와 생성 모둠의 발전, 문제와 기회",
      "submittedOnDailyBy": {
        "_id": "658a8a837959448ef5500ce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
        "isPro": false,
        "fullname": "Shiyin Lu",
        "user": "runninglsy",
        "type": "user"
      },
      "summary": "최근, 다 모델 이해 모델과 이미지 생성 모델의 두 가지 분야 모두 놀라운 진전을 보입니다. 각 분야의 성공에 더해, 이 두 분야는 독립적으로 발전하고, 서로 다른 아키텍처 패러다임으로 형성되어 있습니다: 자동 회귀 기반의 아키텍처가 다 모델 이해에 주로 사용되고, 분산 기반의 모델이 이미지 생성의 기본으로 되어 있습니다. 최근, 이 두 분야의 작업을 통합하는 프레임워크의 개발에 대한 관심이 증가하고 있습니다. GPT-4o의 새로운 기능은 이 추세의 예시로, 통합의 가능성을 밝혀줍니다. 그러나, 이 두 분야의 아키텍처적 차이는 큰 문제를 제기하고 있습니다. 현재의 노력을 명확히 보여주고, 향후 연구를 가이드하는 데 도움이 될 수 있도록, 이러한 노력을 제공합니다. 먼저, 다 모델 이해와 텍스트로부터 이미지 생성 모델의 기초적인 개념과 최근의 진전을 소개합니다. 다음으로, 기존의 통합 모델을 3가지의 주요 아키텍처 패러다임으로 분류하고, 각 카테고리에 대해 구조 설계와 관련 연구에 의한 혁신을 분석합니다. 또한, 통합 모델에 적합한 데이터셋과 벤치마크를 수집하여, 향후 논의를 위한 리소스를 제공합니다. 마지막으로, 이 새로운 분야의 주요 문제에 대해 논의합니다. 토큰화 전략, 크로스 모달 注意, 데이터 등이 포함됩니다. 이 분야는 아직 초기 단계에 있으며, 급속한 진전을 예상하고, 이 조사를 정기적으로 업데이트합니다. 우리의 목표는 연구를 촉진하고, 커뮤니티에 효과적인 리소스를 제공하는 것입니다. 이 조사에 관련된参考文献는 GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models)에서 접근할 수 있습니다.",
      "upvotes": 33,
      "discussionId": "681c7896c7211b7efbc49f76",
      "githubRepo": "https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models",
      "ai_keywords": [
        "autoregressive-based architectures",
        "diffusion-based models",
        "unified frameworks",
        "GPT-4o",
        "multimodal understanding",
        "text-to-image generation models",
        "diffusion-based",
        "autoregressive-based",
        "hybrid approaches",
        "cross-modal attention"
      ]
    },
    "publishedAt": "2025-05-05T07:18:03.000Z",
    "title": "Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities",
    "summary": "Recent years have seen remarkable progress in both multimodal understanding\nmodels and image generation models. Despite their respective successes, these\ntwo domains have evolved independently, leading to distinct architectural\nparadigms: While autoregressive-based architectures have dominated multimodal\nunderstanding, diffusion-based models have become the cornerstone of image\ngeneration. Recently, there has been growing interest in developing unified\nframeworks that integrate these tasks. The emergence of GPT-4o's new\ncapabilities exemplifies this trend, highlighting the potential for\nunification. However, the architectural differences between the two domains\npose significant challenges. To provide a clear overview of current efforts\ntoward unification, we present a comprehensive survey aimed at guiding future\nresearch. First, we introduce the foundational concepts and recent advancements\nin multimodal understanding and text-to-image generation models. Next, we\nreview existing unified models, categorizing them into three main architectural\nparadigms: diffusion-based, autoregressive-based, and hybrid approaches that\nfuse autoregressive and diffusion mechanisms. For each category, we analyze the\nstructural designs and innovations introduced by related works. Additionally,\nwe compile datasets and benchmarks tailored for unified models, offering\nresources for future exploration. Finally, we discuss the key challenges facing\nthis nascent field, including tokenization strategy, cross-modal attention, and\ndata. As this area is still in its early stages, we anticipate rapid\nadvancements and will regularly update this survey. Our goal is to inspire\nfurther research and provide a valuable reference for the community. The\nreferences associated with this survey are available on GitHub\n(https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658a8a837959448ef5500ce5/DwtDeTTjSrZoe2r4sL-a4.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02567.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "658a8a837959448ef5500ce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
      "fullname": "Shiyin Lu",
      "name": "runninglsy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04588",
      "authors": [
        {
          "_id": "681c15ab84d0a008fcdb1ee8",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1ee9",
          "name": "Zile Qiao",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eea",
          "user": {
            "_id": "66224557c61c7fbd98099079",
            "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
            "isPro": false,
            "fullname": "GJ",
            "user": "SpaceProduct",
            "type": "user"
          },
          "name": "Jiayan Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:01.834Z",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eeb",
          "name": "Xuanbo Fan",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eec",
          "name": "Yingyan Hou",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eed",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eee",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eef",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1ef0",
          "name": "Yan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T17:30:22.000Z",
      "submittedOnDailyAt": "2025-05-08T00:54:07.103Z",
      "title": "ZeroSearch: 검색 능력 증진에 도움이 되는 검색을 필요로 하지 않음",
      "submittedOnDailyBy": {
        "_id": "66224557c61c7fbd98099079",
        "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
        "isPro": false,
        "fullname": "GJ",
        "user": "SpaceProduct",
        "type": "user"
      },
      "summary": "有效な 정보 검색은 대규모 언어 모델(LLMs)의 논리론과 생성 능력을 향상시키기 위해 중요합니다. 최근의 연구에서는 실세계 환경에서 실시간 검색 엔진과 상호작용을 활용하여 LLMs의 검색 능력을 개선하기 위한 강화 인지 학습(RL)을 검토하고 있습니다. 이러한 접근 방식은 기대되는 결과를 나타내지만, 두 가지 큰 문제점이 있습니다: (1) 문서의 질의 제어가 불가능: 검색 엔진이 반환하는 문서의 질은 예측할 수 없는 경우가 많고, 훈련 프로세스에 노이즈와 불안정성을 불러옵니다. (2) API 비용의 급상승: RL 훈련은 빈번한 로우아웃을 필요로 하며, 수백만의 검색 리ク Estests를 포함할 가능성이 있으므로, API 비용이 크게 증가하고, scalability를 엄격하게 제한합니다. 이러한 문제를 해결하기 위해, ZeroSearch라는 강화 인지 학습 프레임워크를 소개합니다. 이 접근 방식은 실세계의 검색 엔진과 상호작용을 피하여 LLMs의 검색 능력을 촉진하는 것을 목표로 합니다. 우리의 접근 방식은 가벼운 감독付き의 미세 조정을 시작하여, LLM을 검색 모듈로 기능하도록 변형시키는 것입니다. 이후, RL 훈련 시에는, 콜렉터 기반의 로우아웃 전략을 사용하며, 생성되는 문서의 질을 단계적으로 떨어뜨리며, 모델의 논리론 능력을 단계적으로 발휘시키기 위해, 모델에 의해 어려운 검색 시나리오에 노출시키도록 합니다. 광범위한 실험은, ZeroSearch는 3B의 LLM을 검색 모듈로 하여, LLMs의 검색 능력을 효과적으로 촉진하는 것을 보여줍니다. 놀라울 정도로, 7B의 검색 모듈은 실제 검색 엔진과 같은 성능을 달성하고, 14B의 검색 모듈은 그를 초월하고 있습니다. 또한, 다양한 파라미터 크기의 기본 모델과 인스탠션 훈련 모델 모두 확장될 수 있으며, 광범위한 RL 알고리즘과의 호환성이 있습니다.",
      "upvotes": 23,
      "discussionId": "681c15ac84d0a008fcdb1f21",
      "projectPage": "https://alibaba-nlp.github.io/ZeroSearch/",
      "githubRepo": "https://github.com/Alibaba-nlp/ZeroSearch",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "large language models (LLMs)",
        "search capabilities",
        "live search engines",
        "real-world environments",
        "document quality",
        "noise",
        "instability",
        "training process",
        "API costs",
        "rollouts",
        "search requests",
        "ZeroSearch",
        "lightweight supervised fine-tuning",
        "retrieval module",
        "relevant documents",
        "noisy documents",
        "query",
        "curriculum-based rollout strategy",
        "reasoning ability",
        "retrieval scenarios",
        "base models",
        "instruction-tuned models",
        "parameter sizes",
        "RL algorithms"
      ]
    },
    "publishedAt": "2025-05-07T13:30:22.000Z",
    "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
    "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04588.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66224557c61c7fbd98099079",
      "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
      "fullname": "GJ",
      "name": "SpaceProduct",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04512",
      "authors": [
        {
          "_id": "681c546817fc8222efed5318",
          "name": "Teng Hu",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed5319",
          "name": "Zhentao Yu",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531a",
          "name": "Zhengguang Zhou",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531b",
          "name": "Sen Liang",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531c",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531d",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531e",
          "name": "Qinglin Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T15:33:18.000Z",
      "submittedOnDailyAt": "2025-05-08T05:21:39.978Z",
      "title": "폼웰카스텀：커스텀화된 비디오의 다양성 주도 아키텍처",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "定制视频生成는 사용자가 유연하게 정의한 조건 하에 특정 주제를 포함하는 비디오를 생성하는 것을 목표로 합니다. 그러나 현재의 방법들은 정체성의 일관성과 입력 모델의 제한에 대한 고민을 가지고 있습니다. 본 논문에서는 HunyuanCustom라는 다 모델 기반의定制视频生成 프레임워크를 제안합니다. 이 프레임워크는 주제의 일관성을 중시하면서 이미지, 음성, 비디오, 텍스트 조건을 지원하는 것을 목표로 합니다. HunyuanVideo에서 구축된 모델은 LLaVA 기반의 텍스트 이미지 융합 모듈과 이미지 ID 강화 모듈을 도입하여 이미지 텍스트 조건付き 생성 태스크를 해결합니다. 또한 음성 모델과 비디오 조건付き 생성을 가능하게 하기 위해 모델 기반 조건 注入 구조를 제안합니다. AudioNet 모듈은 공간ク로ス 어텐션을 사용하여 계층적 어레이멘션을 구현하고, 비디오 구동 인젝트 모듈은 패치 기반의 특징 대응 네트워크를 사용하여 잠재압축된 조건 비디오를 통합합니다. 단일 주제와 다수 주제의 경우의 실험은 ID의 일관성, 현실성, 텍스트 비디오의 어레이멘션에 있어서 현재의 가장 선진한 오픈 사이스스와 클로즈드 사이스스의 방법을 크게 초과하는 것을 보여주며, 다운 스트림 태스크의 연계성을 검증하여 음성 비디오 구동의定制 视频生成에서도 강력한 성능을 보여주었습니다. 결과는 제어 가능한 비디오 생성의 발전에 있어 다 모델 조건付き 생성과 정체성 유지 전략의 효과성을 높일 것이 중요한 것을 보여주었습니다. 모든 코드와 모델은 https://hunyuancustom.github.io 에서 사용 가능합니다.",
      "upvotes": 8,
      "discussionId": "681c546e17fc8222efed54ce",
      "ai_keywords": [
        "LLaVA",
        "text-image fusion module",
        "image ID enhancement module",
        "temporal concatenation",
        "modality-specific condition injection mechanisms",
        "AudioNet module",
        "spatial cross-attention",
        "video-driven injection module",
        "latent-compressed conditional video",
        "patchify-based feature-alignment network",
        "ID consistency",
        "text-video alignment",
        "controllable video generation"
      ]
    },
    "publishedAt": "2025-05-07T11:33:18.000Z",
    "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation",
    "summary": "Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04512.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 50
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04622",
      "authors": [
        {
          "_id": "681c03418ff29a163ef5f370",
          "name": "Jingwen Ye",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f371",
          "user": {
            "_id": "64c903957b4d0d947ce86bc6",
            "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
            "isPro": false,
            "fullname": "Yuze He",
            "user": "hyz317",
            "type": "user"
          },
          "name": "Yuze He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:10.350Z",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f372",
          "name": "Yanning Zhou",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f373",
          "name": "Yiqin Zhu",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f374",
          "user": {
            "_id": "6441491c5d600fb0951cd872",
            "avatarUrl": "/avatars/d98892f3b52d87c2328201efa9897110.svg",
            "isPro": false,
            "fullname": "Kaiwen Xiao",
            "user": "loktarxiao",
            "type": "user"
          },
          "name": "Kaiwen Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:12.445Z",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f375",
          "name": "Yong-Jin Liu",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f376",
          "name": "Wei Yang",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f377",
          "name": "Xiao Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T17:59:46.000Z",
      "submittedOnDailyAt": "2025-05-08T05:41:14.360Z",
      "title": "Primitive Aniki: Human-made 3D Primitive Assembly Generation with Auto-regressive Transformer",
      "submittedOnDailyBy": {
        "_id": "64c903957b4d0d947ce86bc6",
        "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
        "isPro": false,
        "fullname": "Yuze He",
        "user": "hyz317",
        "type": "user"
      },
      "summary": "형상 요소 추상화는 복잡한 3D 형상을 단순한 기하학적 요소로 분해하는 데 중요한 역할을 하고 있으며, 인간의 시각 인지에 큰 영향을 미치고, 컴퓨터 비전과 그래픽 분야에서 광범위한 응용을 가지고 있습니다. 최근 3D 콘텐츠 생성의 발전은 놀라운 진전을 보여주고 있지만, 현재의 요소 추상화 방법들은 기하학적인 최적화에 의존하며, 제한적인 의미 이해를 가지고 있으며, 작규모의 클래스별 데이터 세트에서 학습되어, 다양한 형상 클래스 간의 일반화에 어려움을 겪습니다. 우리는 새로운 프레임워크인 PrimitiveAnything를 소개합니다. 이 프레임워크는 요소 추상화를 요소의 조합 생성의 태스크로 재 정의하고, 형상 조건을 포함한 요소 트랜지셔를 포함합니다. PrimitiveAnything는 요소의 조합 생성을 자동으로 수행하기 위한 요소 트랜지셔와, 다양한 요소의 종류를 일관적으로 표현하는 오류 없는 파라미터화 방식이 포함됩니다. 제안된 프레임워크는 인간이 만든 대규모 추상화를 통해 요소의 조합을 직접 학습하고, 인간이 복잡한 형상을 요소 요소로 분해하는 방법을 이해할 수 있습니다. 확장된 실험을 통해, PrimitiveAnything는 다양한 형상 클래스에서도 기하метри의 정확성을 유지하면서, 인간의 관찰에 따라 좋은 요소의 조합을 생성할 수 있음을 보여주고, 3D 엔드 프로젝트의 다양한 응용을 지휘하고, 게임에서의 요소 기반의 사용자 콘텐츠(UGC)의 가능성을 보여줍니다. 프로젝트 페이지: https://primitiveanything.github.io",
      "upvotes": 7,
      "discussionId": "681c03468ff29a163ef5f4d7",
      "projectPage": "https://primitiveanything.github.io/",
      "githubRepo": "https://github.com/PrimitiveAnything/PrimitiveAnything",
      "ai_keywords": [
        "shape primitive abstraction",
        "geometric elements",
        "human visual cognition",
        "computer vision",
        "graphics",
        "3D content generation",
        "geometric optimization",
        "semantic understanding",
        "category-specific datasets",
        "primitive assembly generation task",
        "shape-conditioned primitive transformer",
        "auto-regressive generation",
        "ambiguity-free parameterization scheme",
        "human-crafted abstractions",
        "high-quality primitive assemblies",
        "human perception",
        "geometric fidelity",
        "3D applications",
        "user-generated content (UGC)"
      ]
    },
    "publishedAt": "2025-05-07T13:59:46.000Z",
    "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer",
    "summary": "Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04622.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c903957b4d0d947ce86bc6",
      "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
      "fullname": "Yuze He",
      "name": "hyz317",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.04364",
      "authors": [
        {
          "_id": "681c189c791c72783efe5a94",
          "user": {
            "_id": "6205fefd3f1dc8a642d70b10",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
            "isPro": false,
            "fullname": "Kai Ruan",
            "user": "6cf",
            "type": "user"
          },
          "name": "Kai Ruan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:58:58.134Z",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a95",
          "name": "Mowen Huang",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a96",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a97",
          "name": "Hao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T12:32:01.000Z",
      "submittedOnDailyAt": "2025-05-08T01:06:26.256Z",
      "title": "LLM의 군집 지능의 벤치마크",
      "submittedOnDailyBy": {
        "_id": "6205fefd3f1dc8a642d70b10",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
        "isPro": false,
        "fullname": "Kai Ruan",
        "user": "6cf",
        "type": "user"
      },
      "summary": "대 언어 모델（LLMs）는 복잡한 논리론의 가능성에 대한 가능성을 보여주지만, 자연의 군집의 특징으로 제한된 특성（예를 들어, 제한된 지역적 인지와 통신）하에서 멀티 에이전트 시스템（MAS）에서 에피오디적 협조의 기능에 대한 조사는 주로 이루어지지 않았습니다. 현재의 벤치마크는, 에이전트가 불완전한 공간 시간 정보에 기반해서 행동할 때 발생하는 분산적 협조의 특징적인 문제들을 완전히 파악하지 못합니다. 이러한 공백을 메우기 위해, SwarmBench라는 새로운 벤치마크를 도입합니다. SwarmBench는 분산된 에이전트로서의 LLMs의 군집 지식 능력을 체계적으로 평가하기 위해 설계되었습니다. SwarmBench는, 설정 가능한 2D 그리드 영역 내의 5개의 기본적인 MAS 협조 태스크를 제시하고, 에이전트가 주로 지역적 감각 입력（k×k의 시각 영역）과 지역적 통신을 의존시킬 수 있도록 설계되었습니다. 협조의 효과성에 대한 지표들을 제안하고, 나타나는 에피오디적 군집 다이나믹스를 분석합니다. 0 shot 설정에서 여러 선진적인 LLMs를 평가하고, 각 태스크에서 유의미한 성능의 차이를 보입니다. 지역적 정보 제한으로 인한 문제들을 명확히 합니다. 에피오디적 협조는 나타나지만, 이러한 분산된 시나리오에서 불확실성에 의한 강력한 계획과 전략 형성에는 제한이 있습니다. LLMs의 성능을 군집적 조건에 대해 평가하는 것은 미래의 분산된 시스템에서 제안을 실현하는 데 중요합니다. SwarmBench는 정해진 기계적 특성을 가진 조정 가능한 스케일러블な 물리 시스템으로 구축되어, 환경, 프로ン프트, 평가 스크립트, 상세한 실험 데이터 세트를 제공하며, LLM 기반의 MAS 협조의 재현 가능한 연구와 몸소 MAS의 이론적 기초에 기여합니다. 코드 리포지토리는, https://github.com/x66ccff/swarmbench에 공개되어 있습니다.",
      "upvotes": 7,
      "discussionId": "681c189e791c72783efe5b2d",
      "githubRepo": "https://github.com/x66ccff/swarmbench",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Multi-Agent Systems (MAS)",
        "swarm intelligence",
        "decentralized coordination",
        "spatio-temporal information",
        "SwarmBench",
        "foundational MAS coordination tasks",
        "2D grid environment",
        "local sensory input",
        "local communication",
        "coordination effectiveness",
        "emergent group dynamics",
        "zero-shot setting",
        "robust planning",
        "strategy formation",
        "uncertainty",
        "decentralized scenarios",
        "Embodied MAS"
      ]
    },
    "publishedAt": "2025-05-07T08:32:01.000Z",
    "title": "Benchmarking LLMs' Swarm intelligence",
    "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04364.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6205fefd3f1dc8a642d70b10",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
      "fullname": "Kai Ruan",
      "name": "6cf",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04528",
      "authors": [
        {
          "_id": "681c5152c7211b7efbba4b73",
          "user": {
            "_id": "641aef7b1911d3be67425338",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641aef7b1911d3be67425338/CmCbWWB6NxkAaus59q31w.jpeg",
            "isPro": false,
            "fullname": "Qi Liu",
            "user": "purewhite42",
            "type": "user"
          },
          "name": "Qi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:58:55.624Z",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b74",
          "name": "Xinhao Zheng",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b75",
          "name": "Renqiu Xia",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b76",
          "name": "Xingzhi Qi",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b77",
          "name": "Qinxiang Cao",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b78",
          "name": "Junchi Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T16:02:14.000Z",
      "submittedOnDailyAt": "2025-05-08T05:14:50.449Z",
      "title": "식별된 문제 해결을 위한 공식화, 프레임워크 및 벤치마크",
      "submittedOnDailyBy": {
        "_id": "65b7ae76768464877cdb2e39",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
        "isPro": false,
        "fullname": "Renqiu Xia",
        "user": "renqiux0302",
        "type": "user"
      },
      "summary": "문제 해결은 그 자체로 명백한 작업으로, 과학과 공학의 중요한 구성 요소 중 하나입니다. 그러나 일반적인, 구체적인 문제 해결의 규정이 부족합니다. 최근 AI 기반의 문제 해결 에이전트의 개발에 따라, 프로세스 수준의 검증 가능성의 요구가 급격히 증가하고, 아직 조사가 부족합니다. 이러한 부족점을 보완하기 위해, 우리는 문제 해결의 원칙적인 규정을确定的な马尔可夫决策过程(MDP)로, FPS(Formal Problem-Solving)로 제안합니다. FPS는 기존의 FTP(Formal Theorem Proving) 환경에 기반하여, 프로세스 검증된 문제 해결을 수행하는 새로운 프레임워크입니다. 또한, D-FPS(Deductive FPS)에서는 해결과 답의 검증을 분리하여, 더 인간적인 해결을 목표로 합니다. 이러한 프레임워크의 표현력, 정확성과 완전성은 증명되어 있습니다. 문제 해결에 대한 3가지 벤치마크를 구축했습니다: FormalMath500은 MATH500 벤치마크의 일부를 공식화한 것입니다. MiniF2F-Solving과 PutnamBench-Solving은 FTP 벤치마크 MiniF2F과 PutnamBench의 개선판입니다. RPE(Restricted Propositional Equivalence)라는, 형식 증명을 사용한 기호적 접근을 제안하여, 정교한, 해석 가능한, 인간적인 평가의 목적입니다. RPE는 답의 정확성을 결정하기 위한 형식 증명에 의한 방법입니다. 4가지 일반적인 FTP 모델과 2가지 프론트 프로밍 방법为基础로, FormalMath500의 최대 23.77%, MiniF2F-Solving의 최대 27.47%, PutnamBench-Solving의 최대 0.31%를 해결할 수 있습니다.",
      "upvotes": 5,
      "discussionId": "681c5153c7211b7efbba4bb4",
      "githubRepo": "https://github.com/Purewhite2019/formal_problem_solving_main",
      "ai_keywords": [
        "Markov decision process",
        "FPS (Formal Problem-Solving)",
        "FTP (formal theorem proving)",
        "D-FPS (Deductive FPS)",
        "FormalMath500",
        "MiniF2F-Solving",
        "PutnamBench-Solving",
        "RPE (Restricted Propositional Equivalence)"
      ]
    },
    "publishedAt": "2025-05-07T12:02:14.000Z",
    "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving",
    "summary": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04528.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b7ae76768464877cdb2e39",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
      "fullname": "Renqiu Xia",
      "name": "renqiux0302",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03912",
      "authors": [
        {
          "_id": "681c549cb322a2fe864c8b0d",
          "name": "Can Cui",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b0e",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b0f",
          "name": "Wenxuan Song",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b10",
          "name": "Shuanghao Bai",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b11",
          "name": "Xinyang Tong",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b12",
          "name": "Zirui Ge",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b13",
          "name": "Runze Suo",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b14",
          "name": "Wanqi Zhou",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b15",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b16",
          "name": "Bofang Jia",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b17",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b18",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b19",
          "name": "Donglin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T18:35:07.000Z",
      "submittedOnDailyAt": "2025-05-08T05:23:33.004Z",
      "title": "OpenHelix: 짧은 서비스, 실험적 분석, 오픈 소스\n로봇 조작의 이중 시스템 VLA 모델",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "ダブルシステムVLA（Vision-Language-Action）アーキテクチャ는 구체적인 지능 연구의 핫トピックとなりましたが、進歩的な性能分析および最適化に関する十分な開放ソースのワークが不足しています。この問題に対処するため、本論文では、既存のダブルシステムアーキテクチャ의 構造設計を要約し、その核心設計要素について系統的な実験的評価を行います。最終的には、進歩的な探索に向けた低コスト開放ソースモデルを提供します。当然、このプロジェクトは、様々な実験結果と性能向上の開放ソースモデルを継続的に追加し、みんなが選択肢として利用できるようにします。プロジェクトページ：https://openhelix-robot.github.io/。",
      "upvotes": 3,
      "discussionId": "681c549eb322a2fe864c8b6e"
    },
    "publishedAt": "2025-05-06T14:35:07.000Z",
    "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation",
    "summary": "Dual-system VLA (Vision-Language-Action) architectures have become a hot\ntopic in embodied intelligence research, but there is a lack of sufficient\nopen-source work for further performance analysis and optimization. To address\nthis problem, this paper will summarize and compare the structural designs of\nexisting dual-system architectures, and conduct systematic empirical\nevaluations on the core design elements of existing dual-system architectures.\nUltimately, it will provide a low-cost open-source model for further\nexploration. Of course, this project will continue to update with more\nexperimental conclusions and open-source models with improved performance for\neveryone to choose from. Project page: https://openhelix-robot.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03912.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03418",
      "authors": [
        {
          "_id": "681c4d5b5971460af345032a",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032b",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032c",
          "name": "Junwei Su",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032d",
          "name": "Yuchen Tian",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032e",
          "name": "Yuqi Zhu",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032f",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450330",
          "name": "Lanning Wei",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450331",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450332",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T10:53:58.000Z",
      "submittedOnDailyAt": "2025-05-08T04:51:36.213Z",
      "title": "知識加算의 복잡한 문제 해결법을 대규모 언어 모델에 의해 실현하는 조사",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "문제 해결은 여러 분야에서 인간 진화의 기본적인 동기로 역사적으로 흔히 인식되어 왔습니다. 인공지능의 발전에 따라, 대규모 언어 모델(LLMs)은 복잡한 문제를 다양한 분야에서 해결하는 강력한 도구로 등장했습니다. 전통적인 계산 시스템과 달리, LLMs는 과剩의 계산력과 인간의 논리적 설명의 근사성을 결합하여 해결책을 생성하고 추론을 수행하며 외부 계산 도구를 사용할 수 있습니다. 그러나 실제 세계적인 문제 해결에 LLMs를 적용할 때, 단계별 논리적 설명, 분야 지식의 통합, 결과를 검증 등 큰 문제들이 있습니다. 이 조사는 복잡한 문제 해결에 대한 LLMs의 능력과 한계를 조사하고, Chain-of-Thought(CoT) 논리적 설명, 지식의 확장, LLMs 및 도구 기반의 검증 방법 등을 검토하고 있습니다. 또한 소프트웨어 개발, 수학적 논리적 설명과 증명, 데이터 분석과 모델링, 과학 연구 등 특정 분야에 특화된 문제를 밝혀 있습니다. 이 논문은 현재의 LLMs 솔루션의 기본적인 한계를 논의하고, 단계별 논리적 설명, 분야 지식의 통합, 결과를 검증의 관점에서 LLMs 기반의 복잡한 문제 해결의 미래 방향을 더 나은 논의를 진행합니다.",
      "upvotes": 2,
      "discussionId": "681c4d5f5971460af3450465",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Chain-of-Thought (CoT) reasoning",
        "knowledge augmentation",
        "verification techniques",
        "software engineering",
        "mathematical reasoning and proving",
        "data analysis and modeling",
        "scientific research",
        "multi-step reasoning",
        "domain knowledge integration",
        "result verification"
      ]
    },
    "publishedAt": "2025-05-06T06:53:58.000Z",
    "title": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey",
    "summary": "Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03418.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03821",
      "authors": [
        {
          "_id": "681c7a3829ba66a745217db5",
          "user": {
            "_id": "63caf7ce9f78909f9f81eb72",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
            "isPro": true,
            "fullname": "Gracjan Goral",
            "user": "Gracjan",
            "type": "user"
          },
          "name": "Gracjan Góral",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:02.558Z",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db6",
          "name": "Alicja Ziarko",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db7",
          "name": "Piotr Miłoś",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db8",
          "name": "Michał Nauman",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db9",
          "name": "Maciej Wołczyk",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217dba",
          "name": "Michał Kosiński",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63caf7ce9f78909f9f81eb72/b8OMQONPXPyBtcitD1Yg7.jpeg"
      ],
      "publishedAt": "2025-05-03T00:10:41.000Z",
      "submittedOnDailyAt": "2025-05-08T08:19:59.040Z",
      "title": "초상식: 시각 모델의 시각적 입장 이해 평가",
      "submittedOnDailyBy": {
        "_id": "63caf7ce9f78909f9f81eb72",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
        "isPro": true,
        "fullname": "Gracjan Goral",
        "user": "Gracjan",
        "type": "user"
      },
      "summary": "ビジョン・ラングラウジングモデル(VLMs)의 시각적 포터레이팅 능력을 조사하고, 기존의 인간 테스트에 가장 유사한 새로운 시각 태스크를 사용합니다. 우리의 접근 방식은 단일의 인형 미니 피지와 단일의 오브젝트를 조합하여 조정된 시나리오를 사용합니다. 오브젝트의 위치와 인형 미니 피지의 방향을 체계적으로 변경하고, 조새의 눈과 표면 수준의 관점을 모두 사용하며, 144가지의 고유한 시각 태스크를 만들었습니다. 각 시각 태스크는 7가지 진단 질의 시리즈로 구성되어, 시나리오 이해, 공간 이유, 포터레이팅을 평가하기 위해 설계되었습니다. 우리는 GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct, Claude Sonnet의 버전의 몇 가지 최신 모델을 평가하고, 이들이 시나리오 이해에서 뛰어난 반면, 공간 이유에서 성능이 크게 떨어지고, 포터레이팅에서 더욱 악화되어 있음을 밝혀냅니다. 우리의 분석은 표면 수준의 오브젝트 인식과 복잡한 시각 태스크에 필요한 깊은 공간적 및 포터레이팅의 이유 사이에 있는 격차를 보여, 미래의 VLM 개발에서 명확한 기하학적 표현과 훈련 프로토콜의 통합이 필요함을 주장합니다.",
      "upvotes": 2,
      "discussionId": "681c7a3e29ba66a745217f0c"
    },
    "publishedAt": "2025-05-02T20:10:41.000Z",
    "title": "Beyond Recognition: Evaluating Visual Perspective Taking in Vision\n  Language Models",
    "summary": "We investigate the ability of Vision Language Models (VLMs) to perform visual\nperspective taking using a novel set of visual tasks inspired by established\nhuman tests. Our approach leverages carefully controlled scenes, in which a\nsingle humanoid minifigure is paired with a single object. By systematically\nvarying spatial configurations - such as object position relative to the\nhumanoid minifigure and the humanoid minifigure's orientation - and using both\nbird's-eye and surface-level views, we created 144 unique visual tasks. Each\nvisual task is paired with a series of 7 diagnostic questions designed to\nassess three levels of visual cognition: scene understanding, spatial\nreasoning, and visual perspective taking. Our evaluation of several\nstate-of-the-art models, including GPT-4-Turbo, GPT-4o,\nLlama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that\nwhile they excel in scene understanding, the performance declines significantly\non spatial reasoning and further deteriorates on perspective-taking. Our\nanalysis suggests a gap between surface-level object recognition and the deeper\nspatial and perspective reasoning required for complex visual tasks, pointing\nto the need for integrating explicit geometric representations and tailored\ntraining protocols in future VLM development.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63caf7ce9f78909f9f81eb72/b8OMQONPXPyBtcitD1Yg7.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63caf7ce9f78909f9f81eb72",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
      "fullname": "Gracjan Goral",
      "name": "Gracjan",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.00358",
      "authors": [
        {
          "_id": "68154d77c8ab88a66b8d81a7",
          "name": "Albert Ge",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81a8",
          "name": "Tzu-Heng Huang",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81a9",
          "name": "John Cooper",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81aa",
          "name": "Avi Trost",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ab",
          "name": "Ziyi Chu",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ac",
          "name": "Satya Sai Srinath Namburi GNVV",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ad",
          "name": "Ziyang Cai",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ae",
          "name": "Kendall Park",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81af",
          "name": "Nicholas Roberts",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81b0",
          "name": "Frederic Sala",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T07:08:19.000Z",
      "submittedOnDailyAt": "2025-05-08T05:38:42.650Z",
      "title": "R&B: 영역의 재구성과 데이터의 혼잡 균형에 의한 효율적인 기초 모델의 훈련",
      "submittedOnDailyBy": {
        "_id": "650263c89a612aa33a018383",
        "avatarUrl": "/avatars/a1415a4696da98ffebc1ac798cbab052.svg",
        "isPro": false,
        "fullname": "Albert Ge",
        "user": "albertge",
        "type": "user"
      },
      "summary": "데이터 혼합 시스템은 언어 모델의 훈련에 있어 비용 절감을 가능하게 합니다. 이러한 방법들은 두 가지 단점을 가지고 있습니다. 첫째, 데이터 영역(예: 데이터 소스, 태스크의 종류)이 사전에 결정되어 있어, 중요한 언어적 nuance를 이해할 수 없게 되고, 성능 향상에 연결되지 않습니다. 둘째, 이러한 방법들은 계산적으로 어려워질 것으로 보입니다. 영역의 수가 증가함에 따라 계산량이 늘어나게 됩니다. 이러한 도전에 대처하기 위해, R&B 프레임워크를 사용하여, 언어적 유사성에 기반한 훈련 데이터를 재분할하고, 더 작은 영역을 만들고, 훈련 전 과정에서 얻은 영역 경사를 이용하여 Gram 행렬을 활용하여, 데이터의 구성을 효율적으로 최적화합니다. 선행 연구와 달리, 평가 정보와 같은 손실이나 경사를 얻기 위한 추가 계산이 필요하지 않습니다. 표준 정규 조건 하에서 이 기술에 대한 분석을 수행하고, R&B의 효과성을 비적합 혼합 접근에 비교하여 이론적인合规성을 제공합니다. 실험적으로는, 자연어 처리, 추론, 다형 데이터 태스크의 5가지 다른 데이터 세트에서 R&B의 효과성을 보여줍니다. R&B는 상태의 최상급의 데이터 혼합 시스템의 성능을 초월하거나 대등하게 할 수 있는 0.01%의 추가 계산 오버헤드만으로도 실현할 수 있습니다.",
      "upvotes": 2,
      "discussionId": "68154d78c8ab88a66b8d820c",
      "ai_keywords": [
        "semantic similarity",
        "Gram matrix",
        "domain gradients"
      ]
    },
    "publishedAt": "2025-05-01T03:08:19.000Z",
    "title": "R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training",
    "summary": "Data mixing strategies have successfully reduced the costs involved in\ntraining language models. While promising, such methods suffer from two flaws.\nFirst, they rely on predetermined data domains (e.g., data sources, task\ntypes), which may fail to capture critical semantic nuances, leaving\nperformance on the table. Second, these methods scale with the number of\ndomains in a computationally prohibitive way. We address these challenges via\nR&B, a framework that re-partitions training data based on semantic similarity\n(Regroup) to create finer-grained domains, and efficiently optimizes the data\ncomposition (Balance) by leveraging a Gram matrix induced by domain gradients\nobtained throughout training. Unlike prior works, it removes the need for\nadditional compute to obtain evaluation information such as losses or\ngradients. We analyze this technique under standard regularity conditions and\nprovide theoretical insights that justify R&B's effectiveness compared to\nnon-adaptive mixing approaches. Empirically, we demonstrate the effectiveness\nof R&B on five diverse datasets ranging from natural language to reasoning and\nmultimodal tasks. With as little as 0.01% additional compute overhead, R&B\nmatches or exceeds the performance of state-of-the-art data mixing strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00358.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650263c89a612aa33a018383",
      "avatarUrl": "/avatars/a1415a4696da98ffebc1ac798cbab052.svg",
      "fullname": "Albert Ge",
      "name": "albertge",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03570",
      "authors": [
        {
          "_id": "681b518bf497fd5e45b55eeb",
          "user": {
            "_id": "667ed2bf12e48bee0e972ccc",
            "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
            "isPro": false,
            "fullname": "Mariya Davydova",
            "user": "mariya-davydova",
            "type": "user"
          },
          "name": "Mariya Davydova",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:24.254Z",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eec",
          "name": "Daniel Jeffries",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eed",
          "name": "Patrick Barker",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eee",
          "name": "Arturo Márquez Flores",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eef",
          "name": "Sinéad Ryan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/fe49DABHt5h8Uw-_GCzaB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/yfxA_jM6W5khwa_hHL_mF.png"
      ],
      "publishedAt": "2025-05-06T14:29:47.000Z",
      "submittedOnDailyAt": "2025-05-08T07:36:52.978Z",
      "title": "OSUniverse: 다모달 GUI 네비게이션 AI 에이전트의 벤치마크",
      "submittedOnDailyBy": {
        "_id": "667ed2bf12e48bee0e972ccc",
        "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
        "isPro": false,
        "fullname": "Mariya Davydova",
        "user": "mariya-davydova",
        "type": "user"
      },
      "summary": "이 논문에서는 OSUniverse:desktop에 대한 복잡하고 다양한 작업의 벤치마크를 소개합니다. 이 벤치마크는 사용자 친화성, 확장성, 테스트 케이스의 완전한 커버리지, 자동화 검증을 중점으로 다루며, 기본적인 정확도 픽클링부터 여러 단계, 여러 애플리케이션 테스트까지 수행해야 하는 AI 에이전트의 역량, 정확도, 명확한 사고를 요구합니다. 벤치마크 버전 1에서 소개된 내용은 여기에서 소개됩니다. 테스트 케이스의 복잡도를 조정하여 발표 당시의 SOTA 에이전트가 50% 이상의 결과를 얻지 않도록 하며, 평균의 白領이 이러한 작업들을 완전한 정확도로 수행할 수 있는지 확인합니다. 벤치마크는 수동으로 점수를 매길 수 있으며, 평균 오차율이 2% 이하인 자동화 검증 구조도 소개됩니다. 이 벤치마크는 단기 및 중장기의 시각에서 완전한 자동화의 발전, 능력, GUI-navigation AI 에이전트의 효율성을 측정할 수 있는 견고한 기초를 제공합니다. 벤치마크의 소스 코드는 https://github.com/agentsea/osuniverse에서 사용할 수 있습니다.",
      "upvotes": 1,
      "discussionId": "681b518cf497fd5e45b55f0f",
      "projectPage": "https://agentsea.github.io/osuniverse/",
      "githubRepo": "https://github.com/agentsea/osuniverse"
    },
    "publishedAt": "2025-05-06T10:29:47.000Z",
    "title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents",
    "summary": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\nease of use, extensibility, comprehensive coverage of test cases, and automated\nvalidation. We divide the tasks in increasing levels of complexity, from basic\nprecision clicking to multistep, multiapplication tests requiring dexterity,\nprecision, and clear thinking from the agent. In version one of the benchmark,\npresented here, we have calibrated the complexity of the benchmark test cases\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\ndo not achieve results higher than 50%, while the average white collar worker\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\nmanually, but we also introduce an automated validation mechanism that has an\naverage error rate less than 2%. Therefore, this benchmark presents solid\nground for fully automated measuring of progress, capabilities and the\neffectiveness of GUI-navigation AI agents over the short and medium-term\nhorizon. The source code of the benchmark is available at\nhttps://github.com/agentsea/osuniverse.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/fe49DABHt5h8Uw-_GCzaB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/yfxA_jM6W5khwa_hHL_mF.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03570.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667ed2bf12e48bee0e972ccc",
      "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
      "fullname": "Mariya Davydova",
      "name": "mariya-davydova",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02393",
      "authors": [
        {
          "_id": "681c423f198e1dea5c26f2f4",
          "user": {
            "_id": "6445e9bd1cfc9ae6bb40985c",
            "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
            "isPro": false,
            "fullname": "Evan Jeong",
            "user": "Eavn",
            "type": "user"
          },
          "name": "Sungheon Jeong",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-08T09:38:37.142Z",
          "hidden": false
        },
        {
          "_id": "681c423f198e1dea5c26f2f5",
          "user": {
            "_id": "646b57c6e5abcbf6709fabf6",
            "avatarUrl": "/avatars/e9749acf7866eeaf017f0a43351794fc.svg",
            "isPro": false,
            "fullname": "Jihong Park",
            "user": "Paper9795",
            "type": "user"
          },
          "name": "Jihong Park",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-08T05:36:09.797Z",
          "hidden": false
        },
        {
          "_id": "681c423f198e1dea5c26f2f6",
          "name": "Mohsen Imani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:33:20.000Z",
      "submittedOnDailyAt": "2025-05-08T04:12:55.976Z",
      "title": "불확실성을 중시하는 이미지 및 이벤트의 다양한 융합을 활용한 비디오 이상성 검출",
      "submittedOnDailyBy": {
        "_id": "6445e9bd1cfc9ae6bb40985c",
        "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
        "isPro": false,
        "fullname": "Evan Jeong",
        "user": "Eavn",
        "type": "user"
      },
      "summary": "다수의 현재의 비디오 이상 검출 시스템은 RGB 프레임만 의존하지만, 이는 급격한 혹은 순간적인 이동 코드를 감지하기 위해 필요한 시간적 분석 능력을 부족하게 하여, 이상사상의 지표가 되어 있습니다. 이러한 한계를 해결하기 위해, 우리는 RGB 비디오에서 이벤트 표현의 합성과 이미지 특징의 융합을 수행하는 \"이미지 이벤트 융합 비디오 이상 검출(IEF-VAD)\" 프레임워크를 제안합니다. 이 프레임워크는 기본적으로, 불확실성에 대한 과정에서 이미지 프레임을 직접 합성하고 이미지 특징과 융합합니다. 이 시스템은 (i) Student's-t 적합성을 사용하여 중위의 센서 노이즈를 모델화하고 Laplace 近似을 사용하여 값 수준의 역분산 가중치를 얻습니다; (ii) Kalman 风格의 프레임 별 업데이트를 적용하여 시간적 모델의 균형을 조정합니다; (iii) 융합된 잠재 상태를 반복적으로 정밀화하고 나머지 크로스 모드 노이즈를 제거합니다. IEF-VAD는 전문적인 이벤트 센서나 프레임 레벨의 라벨을 필요로 하지 않습니다, 여러 실제 세계의 이상 검출 벤치마크에서 가장 선진한 상태를 설정합니다. 이러한 발견은 RGB 프레임에서 일반적으로 표현되지 않는 이동 코드를 강조하기 위한 합성 이벤트 표현의 유용성을 보여주고, 다양한 애플리케이션에서 정확한 및 강건한 비디오 이해를 실현할 수 있습니다. 코드와 모델은 https://github.com/EavnJeong/IEF-VAD에 액세스할 수 있습니다.",
      "upvotes": 1,
      "discussionId": "681c4243198e1dea5c26f3cd",
      "githubRepo": "https://github.com/EavnJeong/IEF-VAD",
      "ai_keywords": [
        "Image-Event Fusion",
        "Video Anomaly Detection",
        "event representations",
        "Student`s-t likelihood",
        "Laplace approximation",
        "Kalman-style frame-wise updates",
        "fused latent state",
        "cross-modal noise"
      ]
    },
    "publishedAt": "2025-05-05T02:33:20.000Z",
    "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection",
    "summary": "Most existing video anomaly detectors rely solely on RGB frames, which lack\nthe temporal resolution needed to capture abrupt or transient motion cues, key\nindicators of anomalous events. To address this limitation, we propose\nImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that\nsynthesizes event representations directly from RGB videos and fuses them with\nimage features through a principled, uncertainty-aware process. The system (i)\nmodels heavy-tailed sensor noise with a Student`s-t likelihood, deriving\nvalue-level inverse-variance weights via a Laplace approximation; (ii) applies\nKalman-style frame-wise updates to balance modalities over time; and (iii)\niteratively refines the fused latent state to erase residual cross-modal noise.\nWithout any dedicated event sensor or frame-level labels, IEF-VAD sets a new\nstate of the art across multiple real-world anomaly detection benchmarks. These\nfindings highlight the utility of synthetic event representations in\nemphasizing motion cues that are often underrepresented in RGB frames, enabling\naccurate and robust video understanding across diverse applications without\nrequiring dedicated event sensors. Code and models are available at\nhttps://github.com/EavnJeong/IEF-VAD.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02393.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6445e9bd1cfc9ae6bb40985c",
      "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
      "fullname": "Evan Jeong",
      "name": "Eavn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]