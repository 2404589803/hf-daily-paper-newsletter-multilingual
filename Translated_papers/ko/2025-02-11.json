[
  {
    "paper": {
      "id": "2502.06394",
      "authors": [
        {
          "_id": "67aafead3711ca5b760f324c",
          "user": {
            "_id": "61ade264f602880813dbe10b",
            "avatarUrl": "/avatars/a92dea7d853bbabbf60b351c207b6875.svg",
            "isPro": false,
            "fullname": "Daniil Moskovskiy",
            "user": "etomoscow",
            "type": "user"
          },
          "name": "Daniil Moskovskiy",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:17.448Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f324d",
          "user": {
            "_id": "634c72e6fe1bfa967d6c2b5c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634c72e6fe1bfa967d6c2b5c/WFWIAlWl-FsiJRyGxQTTx.jpeg",
            "isPro": false,
            "fullname": "Nikita Sushko",
            "user": "chameleon-lizard",
            "type": "user"
          },
          "name": "Nikita Sushko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:21.453Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f324e",
          "user": {
            "_id": "5dfa8e07da6d0311fd3d5430",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651090418656-5dfa8e07da6d0311fd3d5430.png",
            "isPro": false,
            "fullname": "Sergey Pletenev",
            "user": "memyprokotow",
            "type": "user"
          },
          "name": "Sergey Pletenev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:59:47.063Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f324f",
          "user": {
            "_id": "662f8d645c4db70c77a203b0",
            "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
            "isPro": false,
            "fullname": "Elena Tutubalina",
            "user": "tlenusik",
            "type": "user"
          },
          "name": "Elena Tutubalina",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:59:50.003Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f3250",
          "name": "Alexander Panchenko",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T12:30:25.000Z",
      "title": "SynthDetoxM: 현대 LLMs는 피드백 디토ク시피팩션 데이터의 싱글샷 병렬 설계의 어노테이터입니다.",
      "summary": "현재의 다언어 텍스트의 디토кси화 처리에 있어서 기존의 접근 방식은 병렬 다언어 데이터 세트의 부족으로 제한되어 있습니다. 본 연구에서는 다언어 병렬 디토кси화 데이터의 생성을 위한 파이프라인을 통해 해결책을 제안합니다. 또한, 손동으로 수집된, 합성적으로 생성된 다언어 병렬 텍스트 디토кси화 데이터 세트 SynthDetoxM을 통해 드론, 프랑스어, 스페인어 및 러시아어의 고품질 16,000건의 디토кси화 문의 쌍을 구성하고 있습니다. 이 데이터는 다양한 독성 평가 데이터 세트로 구축되었으며, 9개의 현대의 오픈 소스 LLM에 의해 몇 피어 시팅으로 재작성하여 구현되었습니다. 실험 결과를 통해, 이 합성 데이터 세트로 훈련된 모델은 데이터의 제한이 있는 상황에서도, 인간 Annotation으로 훈련된 MultiParaDetox 데이터 세트로 훈련된 모델보다 우수한 성능을 나타냅니다. SynthDetoxM으로 훈련된 모델은 모든 평가된 LLM을 초과합니다. 우리의 데이터 세트와 코드를 공개하여 더 많은 다언어 텍스트 디토кси화 연구에 기여하겠습니다.",
      "upvotes": 55,
      "discussionId": "67aafeae3711ca5b760f3280"
    },
    "publishedAt": "2025-02-11T03:03:12.135Z",
    "title": "SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06394.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61ade264f602880813dbe10b",
      "avatarUrl": "/avatars/a92dea7d853bbabbf60b351c207b6875.svg",
      "fullname": "Daniil Moskovskiy",
      "name": "etomoscow",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06781",
      "authors": [
        {
          "_id": "67aacd7e078cdf445284f9f6",
          "name": "Chengqi Lyu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f7",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f8",
          "name": "Yuzhe Gu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f9",
          "user": {
            "_id": "64e8505321540e1da3226b54",
            "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
            "isPro": false,
            "fullname": "Wenwei Zhang",
            "user": "ZwwWayne",
            "type": "user"
          },
          "name": "Wenwei Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:40.279Z",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fa",
          "name": "Jianfei Gao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fb",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fc",
          "name": "Ziyi Wang",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fd",
          "name": "Shuaibin Li",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fe",
          "name": "Qian Zhao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9ff",
          "name": "Haian Huang",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa00",
          "name": "Weihan Cao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa01",
          "name": "Jiangning Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa02",
          "name": "Hongwei Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa03",
          "name": "Junnan Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa04",
          "user": {
            "_id": "630716d11801ecc7d2595021",
            "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
            "isPro": false,
            "fullname": "Songyang Zhang",
            "user": "zsytony",
            "type": "user"
          },
          "name": "Songyang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:37.733Z",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa05",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa06",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:57:29.000Z",
      "title": "탐험 성과 보상의 학습 수학 논리의 한계",
      "summary": "인공 지능의 핵심 요소 중 하나인 추론 능력, 특히 복잡한 수학 문제를 해결하는 능력, 최근에像是 OpenAI의 o-series 모델과 같은 전문 회사가 주목을 받는 발전을 보였습니다. 그러나 전체 기술 디테일이 공개되지 않고, 확실히 사용된 기술은 강화 학습(RL)과 긴 체인 사고로 추정됩니다. 본 논문에서는, 결과 기반의 보상을 통해 수학 추론 작업에서 가능한 성능의 극한을 추구하기 위한 새로운 RL 프레임워크인 OREAL을 제안합니다. 이 프레임워크는 이진 결과 보상이 쉽게 얻을 수 있는 환경에서 KL 정규화된 최적 전략을 학습할 수 있는 Behavior Cloning이 충분하다는 이론적 증명을 제공합니다. 이 공식은 또한 부정 샘플의 보상을 재형상을 통해 양과 부정 샘플 사이의 경사 일관성을 보장해야 한다는 것을 의미합니다. RL에서 오랜 기간 존재하는 희소 보상 문제를 완화시키기 위해, 이 문제를 심화시키는 수학 추론 작업의 긴 체인 사고의 부분적인 정확성을 고려하여, 학습을 위한 중요한 토큰을 추론 경로에서 샘플링하는 토큰 기반 보상 모델을 추가로 적용했습니다. OREAL을 통해 7B 모델은 처음으로 RL에서 MATH-500의 94.0 pass@1 정확도를 달성했습니다, 이는 32B 모델과 비교할 수 있습니다. OREAL-32B는 MATH-500에서 95.0 pass@1의 정확도를 달성하여, 이전에蒸馏 훈련된 32B 모델을 초과했습니다. 우리의 연구는 초기 전략 모델과 RL 훈련 쿼리의 중요성을 나타냅니다. 코드, 모델, 데이터는 향후 연구를 촉진하기 위해 공개될 예정입니다. https://github.com/InternLM/OREAL.",
      "upvotes": 30,
      "discussionId": "67aacd7f078cdf445284fa4b"
    },
    "publishedAt": "2025-02-10T23:18:11.727Z",
    "title": "Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06781.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06703",
      "authors": [
        {
          "_id": "67aabf93c0f8648f68c68ce4",
          "user": {
            "_id": "667187ba9ab144eb3ac43a1b",
            "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
            "isPro": false,
            "fullname": "Runze Liu",
            "user": "RyanLiu112",
            "type": "user"
          },
          "name": "Runze Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:22.940Z",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce5",
          "name": "Junqi Gao",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce6",
          "name": "Jian Zhao",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce7",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:18.725Z",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce8",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce9",
          "name": "Biqing Qi",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68cea",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ceb",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T17:30:23.000Z",
      "title": "1B LLM가 405B LLM를 초과할 수 있는가? 테스트 시간 계산 최적화 크기 업을 재 고려する",
      "summary": "TTS(Test Time Scaling)는 추론 단계에서 추가 계산을 사용하여 대규모 언어 모델(LLMs)의 성능을 향상시키는 중요한 방법입니다. 그러나 현재의 연구는 정책 모델, Process Reward Models(PRMs), 문제를 어떻게 TTS에 영향을 미칠지에 대한 체계적인 분석을 하지 않고 있습니다. 이 분석의 부족은 TTS 방법의 이해와 실용적인 활용을 제한하고 있습니다. 본 논문에서는 두 가지 핵심적인 질문에 초점을 맞추었습니다: (1) 정책 모델, PRMs, 문제의 난이도 수준을 통일하여 TTS 계산을 스케일하는 최적의 접근法是 무엇인가? (2) 확장 계산이 LLMs의 복잡한 태스크의 성능 향상에 어떤 영향을 미치며, 이 접근법으로 작은 언어 모델이 큰 모델을 초월할 수 있는지? MATH-500과 도전적인 AIME24 태스크에 대해 상세한 실험을 수행하였으며, 다음과 같은 결론을 얻었습니다: (1) 계산 최적화된 TTS 전략은 정책 모델, PRM, 문제의 난이도의 선택에 매우 의존합니다. (2) 계산 최적화된 TTS 전략을 사용함으로써, 매우 작은 정책 모델이 큰 모델을 초월할 수 있습니다. 예를 들어, 1B LLM은 MATH-500에서 405B LLM을 초월합니다. 또한 MATH-500과 AIME24에서 0.5B LLM은 GPT-4o를 초월하고, 3B LLM은 405B LLM을 초월하고, 7B LLM은 o1과 DeepSeek-R1을 초월하며, 동시에 추론 효율이 높은 것을 보여주었습니다. 이러한 결과를 통해 TTS 전략을 각 태스크와 모델의 특정한 특성에 맞는 것이 중요함을 보여주고, TTS는 LLMs의 성능 향상을 위한 잠재적으로 유망한 접근법이라는 것을 보여줍니다.",
      "upvotes": 25,
      "discussionId": "67aabf94c0f8648f68c68d19"
    },
    "publishedAt": "2025-02-11T00:36:11.270Z",
    "title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05609",
      "authors": [
        {
          "_id": "67aacaaaa03eecbc2d72835f",
          "user": {
            "_id": "64ec4c04c782d648d28d70fc",
            "avatarUrl": "/avatars/6975526fcf4b513cc934b5bc45370a48.svg",
            "isPro": false,
            "fullname": "Sukmin Cho",
            "user": "zomss",
            "type": "user"
          },
          "name": "Sukmin Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:43.377Z",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728360",
          "name": "Sangjin Choi",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728361",
          "user": {
            "_id": "64d1e70a84f205869017703b",
            "avatarUrl": "/avatars/215d0d4db5f79cb74df4d888b18c6a0d.svg",
            "isPro": false,
            "fullname": "Taeho Hwang",
            "user": "doubleyyh",
            "type": "user"
          },
          "name": "Taeho Hwang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:45.737Z",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728362",
          "name": "Jeongyeon Seo",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728363",
          "name": "Soyeong Jeong",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728364",
          "name": "Huije Lee",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728365",
          "name": "Hoyun Song",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728366",
          "name": "Jong C. Park",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728367",
          "name": "Youngjin Kwon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T15:32:53.000Z",
      "title": "無失真な 대규모 언어 모델의 가속화에서 시간적 근접성을 기반으로 하는 계층적 작성법",
      "summary": "대 언어 모뎀(LLMs)의 추론 속도를 향상시키는 것은 실시간 인터랙션에서 중요합니다. 추론 속도를 개선하기 위해 주목받은 알고리즘적 해결책 중 하나는, 추론을 가속화하기 위해 토큰을 미리보기하고 그 정확성을 확인하여 한 번의 전향 캐쉬에서 여러 토큰을 생성하는 방법이 있습니다. 그러나 현재의 미리보기 전략은 일반적으로 중요한 미세 조정이 필요하거나, 각 태스크마다 불균형한 성능을 보여주는 경우가 많습니다. 이러한 문제를 해결하기 위해, 우리는 시간적 지역성을 기반으로 하는 다층 구조의 프레임워크를 통해 서로 다른 토큰 소스를 구성하는 손실 없는 미리보기 접근법인 Hierarchy Drafting(HD)를 제안합니다. 미리보기 단계에서, HD는 가장 높은 지역성을부터 가장 낮은 지역성에 따라 순차적으로 여러 데이터베이스를 액세스하여, 다양한 태스크에서도 일관된 가속을 보장하고, 미리보기의 지연을 최소화합니다. Spec-Bench에서의 실험에 따르면, 7B와 13B 파라미터의 LLMs를 사용하였을 때, HD는 기존의 데이터베이스 미리보기 방법보다 뛰어나고, 모델 크기, 태스크, 온도에 따른 강력한 추론 속도 향상을 실현했습니다.",
      "upvotes": 12,
      "discussionId": "67aacaaca03eecbc2d728394"
    },
    "publishedAt": "2025-02-10T22:58:41.471Z",
    "title": "Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05609.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec4c04c782d648d28d70fc",
      "avatarUrl": "/avatars/6975526fcf4b513cc934b5bc45370a48.svg",
      "fullname": "Sukmin Cho",
      "name": "zomss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05415",
      "authors": [
        {
          "_id": "67aaea0a0acaa007694aed73",
          "user": {
            "_id": "65708920806dee337da0eef5",
            "avatarUrl": "/avatars/945e328dedc8e1e3111f48c344ad5b03.svg",
            "isPro": false,
            "fullname": "xuchenkai",
            "user": "UnhurriedDawn",
            "type": "user"
          },
          "name": "Chenkai Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:28.861Z",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed74",
          "user": {
            "_id": "6644548a3a16452261cdb173",
            "avatarUrl": "/avatars/4643db904204e3a60202a29e8c884139.svg",
            "isPro": false,
            "fullname": "wangxu",
            "user": "asunalove",
            "type": "user"
          },
          "name": "Xu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:26.432Z",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed75",
          "name": "Zhenyi Liao",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed76",
          "name": "Yishun Li",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed77",
          "name": "Tianqi Hou",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed78",
          "user": {
            "_id": "64bba541da140e461924dfed",
            "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
            "isPro": false,
            "fullname": "zhijie deng",
            "user": "zhijie3",
            "type": "user"
          },
          "name": "Zhijie Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:24.089Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T02:52:25.000Z",
      "title": "쇼 오투어볼： 통합 모노모더널 이해와 생성을 가속화하기 위한 것",
      "summary": "특정 분야의 연구 흥미가 증가하고 있으며, 특히 Show-o가 주목을 받고 있으며, 텍스트에서 이미지로, 이미지에서 텍스트로 생성하는 데 뛰어난 성능을 자랑하고 있습니다. Show-o의 추론은 이미지 토큰의 진화적인 디노이즈와 텍스트 토큰의 자동 복원 설명을 포함하여, 두 가지 모두에서 부적절한 효과 문제에 직면하여 있습니다. 본 논문에서는 Show-o Turbo를 소개하여 이러한 그룹 간의 연결을 이루는 데 도움을 줍니다. 먼저, Show-o에서 이미지와 텍스트의 생성에서 텍스트 토큰의 병렬 설명에 기반한 통일적인 디노이즈 관점을 식별합니다. 다음으로, Show-o의 다모달 디노이즈 프로젝트에 일관성 디스태일링(CD)을 확장하여 디퓨션 모델의 디노이즈 프로세스를 줄이기 위한 적절한 접근 방식을 제공합니다. 프로젝트 분할 전략과 클레크러닝 프로세스를 도입하여 훈련의 수렴을 개선합니다. 실험적으로는, 텍스트에서 이미지로의 생성에서 CFG를 사용하지 않고 4 스텝으로 Custom-Free Guide(CFG)를 사용하면 GenEval 스코어가 0.625로, 원래 Show-o를 초과합니다. 이미지에서 텍스트로의 생성에서 성능을 크게 손실시키지 않고 1.5배의 속도 향상을 나타냅니다. 코드는 https://github.com/zhijie-group/Show-o-Turbo에 접근할 수 있습니다.",
      "upvotes": 9,
      "discussionId": "67aaea100acaa007694aeea5"
    },
    "publishedAt": "2025-02-11T02:09:27.778Z",
    "title": "Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bba541da140e461924dfed",
      "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
      "fullname": "zhijie deng",
      "name": "zhijie3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06772",
      "authors": [
        {
          "_id": "67aac8adfe33f6d8d695bc40",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc41",
          "name": "Zhaochen Yu",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc42",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc43",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:51:47.000Z",
      "title": "ReasonFlux: 스케일링된 템플릿을 통해 계층적인 LLM 논리\n\n(注意：虽然要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，我进行了适当的调整，以确保翻译符合韩国语的习惯表达方式。)",
      "summary": "우리는 스케일링된 간단한 생각 방식 템플릿을 사용하여 휴리스틱 LLM의 논리론리를 제안하고, 강력한 LLM（예: OpenAI o1-preview, DeepSeek V3）의 수학적 논리론력 능력을 우월시킬 수 있음을 보여줍니다. 우리는 ReasonFlux-32B 모델을 8 GAF 프로카스스로 훈련시키고 3 가지 혁신을 도입합니다: (i) 구조화된 장르 공통의 생각 방식 템플릿 라이브러리, 약 500 개의 고 수준의 생각 방식 템플릿을 포함하며, 유사하거나 관련된 논리론리 문제를 일반화할 수 있습니다; (ii) 긴 CoTs보다 순서적인 생각 방식 템플릿의 열로 휴리스틱한 강화학습을 수행하고, 기본 LLM을 최적의 템플릿 트래지즌에 계획하여 복잡한 문제를 단계적으로 해결하는 것을 최적화합니다; (iii) 새로운 추론 스케일링 시스템을 도입하고, 추론 시 생각 방식 템플릿을 적응적으로 스케일링하여 휴리스틱 LLM의 논리론리를 가능하게 합니다. 순서적인 생각 방식 템플릿을 포함하는 템플릿 트래지즌을 가진 것을 통해, ReasonFlux-32B의 수학적 논리론력은 최선 수준에 도달합니다. 특히, MATH 벤치마크에서 정확도 91.2%를 달성하며, o1-preview을 6.7% 초과합니다. USA Math Olympiad (AIME) 벤치마크에서, ReasonFlux-32B는 평균 56.7%의 문제를 해결할 수 있으며, o1-preview과 DeepSeek-V3를 각각 27%와 45% 초과합니다. 코드: https://github.com/Gen-Verse/ReasonFlux",
      "upvotes": 9,
      "discussionId": "67aac8affe33f6d8d695bcbd"
    },
    "publishedAt": "2025-02-10T22:49:56.390Z",
    "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06772.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06049",
      "authors": [
        {
          "_id": "67aac01bd7b18841e7c266df",
          "name": "Jikun Kang",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e0",
          "name": "Wenqi Wu",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e1",
          "name": "Filippos Christianos",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e2",
          "name": "Alex J. Chan",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e3",
          "name": "Fraser Greenlee",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e4",
          "name": "George Thomas",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e5",
          "name": "Marvin Purtorab",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e6",
          "name": "Andy Toulis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T22:11:42.000Z",
      "title": "大記憶 모듈 (Dae Jiyeo Mudeol)",
      "summary": "이 논문에서는 Large Memory Model (LM2)를 소개합니다. LM2는 일반적인 Transformer의 단계별 추론, 관계 논리, 긴 문맥에서 분산된 정보를 합성하는 데에 제한을 해결하기 위해 보조 메모리 모듈을 추가한 해석기 기반의 Transformer 아키텍처입니다. 제안된 LM2는 메모리 모듈을 포함하여 컨텍스트 표현의 리포지토리 역할을 수행하며, 입력 토큰과 크로스 어텐션을 통해 상호작용하고 게이트 구조를 통해 업데이트됩니다. 일반적인 Transformer의 기능을 유지하기 위해, LM2는 보조 메모리 패스웨이를 통합하면서 원 정보 흐름을 유지합니다. BABILong 벤치마크에서 실험 결과를 통해, LM2 모델은 RMT 모델보다 37.1% 이상, 기본 Llama-3.2 모델보다 86.3% 이상 평균적으로 더 좋은 결과를 나타냅니다. LM2는 단계별 추론, 숫자 논리, 긴 문맥의 질문에 대한 답변에 특화된 능력을 나타냅니다. MMLU 데이터 세트에서, 학습된 버전 모델보다 5.0%의 개선을 달성했으며, 그 메모리 모듈이 일반적인 태스크에 대한 성능을 저하시키지 않는 것을 보여주었습니다. 또한, 분석에서는 메모리의 설명성, 메모리 모듈의 효과, 테스트 시의 행동을 검토했습니다. 우리의 발견은 Transformer 아키텍처를 강화하기 위한 명시적 메모리의 중요성을 강조합니다.",
      "upvotes": 8,
      "discussionId": "67aac01dd7b18841e7c26739"
    },
    "publishedAt": "2025-02-10T22:13:17.117Z",
    "title": "LM2: Large Memory Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06049.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6489e10ca13f65198dc6e122",
      "avatarUrl": "/avatars/4aa9eab488157711b2f0298ddadee2f4.svg",
      "fullname": "Kang",
      "name": "JaxonK",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03628",
      "authors": [
        {
          "_id": "67aab82e6024056209d727a8",
          "name": "Zhuowei Li",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727a9",
          "name": "Haizhou Shi",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727aa",
          "name": "Yunhe Gao",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ab",
          "name": "Di Liu",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ac",
          "name": "Zhenting Wang",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ad",
          "name": "Yuxiao Chen",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ae",
          "name": "Ting Liu",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727af",
          "name": "Long Zhao",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727b0",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727b1",
          "name": "Dimitris N. Metaxas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T21:34:02.000Z",
      "title": "토큰의 은닉된 생활: 시각정보제어에 의한 대형 비전 언어 모델의 해시미션의 감소",
      "summary": "대시야 비전 언어 모델(LVLMs)은 문맥과 시각 입력을 효과적으로 추론할 수 있지만, 이들은 문법적으로 일치하지만 시각적으로 기반이 없는 내용을 환상화하는 경향이 있습니다. 본 논문에서는 생성 과정에서 토큰 로지트의 순서를 조사하여 LVLMs가 정보 처리 시 나타나는 3가지 주요 패턴을 밝혀봅니다: 1) 진행 중 시각 정보의 손실 - 시각적으로 기반이 있는 토큰은 생성 진행 중 우선 순위가 점차 떨어집니다. 2) 초기의 예기 - 의미적으로 의미 있는 토큰은 최종층보다 빠르게 층 내 활성 최고점을 달성합니다. 3) 은닉된 진짜 정보 - 시각적으로 기반이 있는 토큰은 최종적으로 결정되지만 추론 시 상대적으로 높은 순위를 유지합니다. 이러한 패턴에 기반하여, VISTA(토큰 로지트 증강을 통한 시각 정보 조종)을 제안합니다. VISTA는 외부 슈퍼바이저를 필요로 하지 않는 추론 시 인터버젼 프레임워크이며, 환상화를 줄이면서 진짜 정보를 촉진합니다. VISTA는 활성 공간에서 시각 정보의 강화와 초기 층의 활성을 활용하여, 의미적으로 의미 있는 디코딩을 촉진합니다. 기존 방법과 비교하여, VISTA는 외부 슈퍼바이저를 필요로 하지 않는 점으로 다양한 디코딩 전략에 적용할 수 있습니다. 확장된 실험에 따라, VISTA는 평균 40% 정도의 환상화를 줄이고, 4가지 벤치마크에서 3가지 디코딩 전략을 조합하여, 기존 방법보다 뛰어난 결과를 얻었습니다.",
      "upvotes": 8,
      "discussionId": "67aab82f6024056209d727f6"
    },
    "publishedAt": "2025-02-10T21:38:53.032Z",
    "title": "The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03628.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64dfcc62e8b6f3f3baa950e0",
      "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
      "fullname": "Zhenting Wang",
      "name": "ztwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06786",
      "authors": [
        {
          "_id": "67aae91b83b1182df7c0cf54",
          "name": "Pranav Nair",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf55",
          "name": "Puranjay Datta",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf56",
          "name": "Jeff Dean",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf57",
          "name": "Prateek Jain",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf58",
          "name": "Aditya Kusupati",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:59:10.000Z",
      "title": "MATRIZOHA KUANCHITUSHONG\n\n（注意：此翻译严格遵循原文，未进行任何额外解释或文本添加。）",
      "summary": "모델의 가중치를 양자화하는 것은 큰 모델의 통신 비용과 추론 비용을 줄이는 데 중대한 의미를 가지고 있습니다. 그러나 모델의 양자화（특히 int4나 int2와 같은 저 정밀도와 관련하여）는 모델의 품질을 손실로 나타내는 부정적인 효과를 보입니다. 특히 int2는 모델의 품질을 엄격하게 저하시키는 경우가 알려져 있습니다. 따라서 실무자들은 일반적으로 다양한 양자화 수준의 여러 모델을 보유하거나 가장 높은 품질과 지연 손실을 만족하는 모델을 제공하는 것이 필요합니다. 반면 정수 데이터형（예시로 int8）은 작은 비트폭의 정수（예시로 int4나 int2）이 가장 의미 있는 비트로 감싸지는 '마트리콰' 구조를 갖습니다. 본 논문에서는 '마트리콰 양자화'（MatQuant）라는 새로운 다스케일 양자화 기술이 제안되고, 다양한 양자화 모델의 필요성을 해결하려는 목표를 가지고 있습니다. 이는 하나의 모델을 훈련시키고 이를 다른 정밀도 수준으로 제공할 수 있게 합니다. 또한 MatQuant에 의한 공동 학습과 공동 정규화로 인해, MatQuant에서 얻은 int2 정밀도의 모델은 표준적인 int2 양자화（QAT나 OmniQuant와 같은 기술 사용）보다 10% 이상의 정확도를 향상시킬 수 있습니다. 이는 같은 레시피를 사용하여, int2의 FFN 양자화된 Gemma-2 9B 모델이 int8의 FFN 양자화된 Gemma-2 2B 모델보다 더 높은 정확도를 나타내며, 모델 양자화에 대한 발전을 보여줍니다.",
      "upvotes": 7,
      "discussionId": "67aae91d83b1182df7c0cff6"
    },
    "publishedAt": "2025-02-11T01:07:50.116Z",
    "title": "Matryoshka Quantization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06786.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06788",
      "authors": [
        {
          "_id": "67aac64de37429ebdbdafc40",
          "name": "Haiwen Diao",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc41",
          "name": "Xiaotong Li",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc42",
          "name": "Yufeng Cui",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc43",
          "name": "Yueze Wang",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc44",
          "name": "Haoge Deng",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc45",
          "user": {
            "_id": "6565bc5ee5aac326bfc98e39",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vIfHy9Y1yAK6A96UCHNBH.jpeg",
            "isPro": false,
            "fullname": "Ting Pan",
            "user": "PhyscalX",
            "type": "user"
          },
          "name": "Ting Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:09.401Z",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc46",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc47",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc48",
          "name": "Xinlong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:59:58.000Z",
      "title": "EVEv2: 비전-언어 모델의 인코더 없는 기본라인 개선",
      "summary": "현재의 Encoder-less의 시각 언어 모델(VLMs)은 Encoder 기반의 모델과 성능 간격을 급격히 좁혀가고 있습니다. 이는 구조적 단순성과 효율적인 통합 모형의 가능성에 대한 명확한 증거를 제시하고 있습니다. 우리는 Encoder-less의 VLMs의 성능 간격을 체계적으로 명확히 하고, pretrained 시각 Encoder, Discrete Tokenizer, 그리고 시각 층의 minimalist layer를 사용하여 구축된 VLMs의 시각과 언어의 특성을 깊이 있게 조사했습니다. 우리는 Encoder-less의 VLMs과主流의 Encoder 기반의 모델과 동일한 성능을 경쟁하는 효율적인 전략을 개발했습니다. 상세한 조사를 통해, 우리는 새로운, 개선된 Encoder-less의 VLMs의 가족인 EVEv2.0을 발표했습니다. 우리는 다음과 같은 것을 보여줍니다: (i) 시각과 언어를 단일 모형 내에서 적절하게 분해하고 계층적으로 연결함으로써, 모델 간의 교란을 줄일 수 있습니다. (ii) 좋은 훈련 전략은 Encoder-less의 VLMs의 효과적인 최적화를 가능하게 합니다. 확장된 평가를 통해, 우리의 EVEv2.0은 모델 간의 Decoder만 개발된 아키텍처를 보여주고, 높은 데이터 효율성과 강력한 시각 인식 능력을 보여주었습니다. 코드는 공개적으로 사용 가능합니다: https://github.com/baaivision/EVE.",
      "upvotes": 6,
      "discussionId": "67aac64ee37429ebdbdafc96"
    },
    "publishedAt": "2025-02-10T22:40:39.442Z",
    "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06788.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4a717aa03b6520839e9b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
      "fullname": "Haiwen Diao",
      "name": "Paranioar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06782",
      "authors": [
        {
          "_id": "67aae76c71a9983f50e134ef",
          "name": "Dongyang Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f0",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f1",
          "name": "Yutong Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f2",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f3",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f4",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f5",
          "name": "Qi Qin",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f6",
          "name": "Yufei Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f7",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f8",
          "name": "Zhongyu Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f9",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fa",
          "name": "Chenyang Si",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fb",
          "name": "Yuewen Cao",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fc",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fd",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fe",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134ff",
          "name": "Qibin Hou",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e13500",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e13501",
          "name": "Peng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:58:11.000Z",
      "title": "Lumina-Video: 다스케일로 효율적이고 유연한 비디오 생성에 대한 Next-DiT\n\n(注意：原文中的“多スケールでの効率的かつ柔軟なビデオ生成におけるNext-DiT”在翻译时，\"多スケール\"被翻译为\"다스케일\"，这是因为\"다스케일\"在韩语中更常用来表示多尺度。如果需要更精确的翻译，可以考虑使用\"다단계\"或\"다단계 스케일\"，但\"다스케일\"在技术文献中也是常见的翻译。)",
      "summary": "최근의 발전은 Diffusion Transformers (DiTs)를 생성 모델링의 주요 프레임워크로 자리잡은 데에 있습니다. 이 성공에 기반하여 Lumina-Next는 Next-DiT을 사용하여 현실적인 이미지 생성에 매우 높은 성능을 달성했습니다. 그러나 비디오 생성의 가능성은 크게 개발되지 않았으며, 비디오 데이터에 고유한 공간 시간 복잡성을 모델링하는 데에 중대한 문제를 가지고 있습니다. 이에 대한 대응으로, 우리는 Lumina-Video를 소개합니다. 이 프레임워크는 Next-DiT의 강점을 활용하면서, 비디오 합성에 적합한 해결책을 도입하고 있습니다. Lumina-Video는 다 스케일의 Next-DiT 아키텍처를 사용하며, 이를 학습하면서 효율성과 유연성을 동시에 보장합니다. 또한, 움직임 스코어를 명시적 조건으로 사용함으로써, Lumina-Video는 생성된 비디오의 동적 정도를 직접 제어할 수 있습니다. 이 방법은 발전적인 학습 스키마로, 높은 해상도와 FPS를 늘리며, 자연스럽게 합성 데이터와 함께 학습하는 다소스 학습 스키마를 조합하여, 높은 학습과 추론의 효율성을 동반한, 매력적인 미술적 질과 흐름의 우수성을 갖습니다. 또한, 우리는 Next-DiT 기반의 비디오에서 오디오로 모델을 제안하는 Lumina-V2A를 제안합니다. 이 모델은 생성된 비디오와 동기화된 음성을 생성하는 것을 목표로 합니다. 코드는 https://www.github.com/Alpha-VLLM/Lumina-Video에서 릴리즈되었습니다.",
      "upvotes": 5,
      "discussionId": "67aae76e71a9983f50e1357d"
    },
    "publishedAt": "2025-02-11T01:00:25.383Z",
    "title": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05431",
      "authors": [
        {
          "_id": "67aac392385da1f07cc7fcbd",
          "user": {
            "_id": "64f58b970b24e548a85522bc",
            "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
            "isPro": false,
            "fullname": "Xinyu Yang",
            "user": "Hanyuezhuohua",
            "type": "user"
          },
          "name": "Xinyu Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:13.131Z",
          "hidden": false
        },
        {
          "_id": "67aac392385da1f07cc7fcbe",
          "name": "Tianqi Chen",
          "hidden": false
        },
        {
          "_id": "67aac392385da1f07cc7fcbf",
          "name": "Beidi Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T03:41:16.000Z",
      "title": "APE: 적응적 병렬 인코딩에 의한 고속화와 장기 컨텍스트 추가의 생성",
      "summary": "Context-augmented generation (CAG)手法, RAG 및 ICL을 포함하여, 여러 컨텍스트를 효율적으로 조합하여 사용자의 요청에 대한 답변을 생성하는 데 필요합니다. 이러한 컨텍스트를 순차적으로 입력하는 것은 컨텍스트의 조합이 큰 계산 부담을 불러일으키므로, 각 요청에 대해 다시 인코딩됩니다. 이 문제를 해결하기 위해, 병렬 인코딩의 가능성을 검토하고, 각 컨텍스트의 KV 상태를 독립적으로 미리 계산하고 캐시합니다. 이 접근 방식은 추론 시 캐시된 상태를 직접 읽어 들일 수 있게 하여, 컨텍스트의 수를 늘릴 수 있습니다. 그러나 주목 분포의 비대칭성으로, 병렬 인코딩을 직접 적용하면 성능이 크게 떨어질 수 있습니다. 이를 해결하기 위해, Adaptive Parallel Encoding (APE)를 제안하고, 공통의 prefix, 주목 온도, 스케일링 팩터를 사용하여 병렬 인코딩의 분포를 순차 인코딩의 분포와 일치시킵니다. RAG 및 ICL의 태스크에서의 결과를 보면, 동일한 입력을 사용하여 순차 인코딩의 성능을 98%와 93%로 유지하며, 병렬 인코딩은 3.6%와 7.9%를 초과합니다. 또한, 여러 샷의 CAG에도 적용할 수 있으며, 수백 개의 컨텍스트를 동시에 인코딩할 수 있습니다. 효율성 평가에 따르면, APE는 128K 길이의 컨텍스트의 예측 시간을 28배 줄이고, 4.5배의 속도 업그레이드까지 실현할 수 있습니다.",
      "upvotes": 5,
      "discussionId": "67aac393385da1f07cc7fd17"
    },
    "publishedAt": "2025-02-10T22:29:36.102Z",
    "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05431.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64f58b970b24e548a85522bc",
      "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
      "fullname": "Xinyu Yang",
      "name": "Hanyuezhuohua",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06155",
      "authors": [
        {
          "_id": "67aab9b4a2bf5e5ea03d4c19",
          "user": {
            "_id": "643a451ee2b979ae6141329d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a451ee2b979ae6141329d/HN3M5vyroanQoUEiXJFyB.jpeg",
            "isPro": false,
            "fullname": "Hangliang Ding",
            "user": "foreverpiano",
            "type": "user"
          },
          "name": "Hangliang Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:29.115Z",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1a",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1b",
          "name": "Runlong Su",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1c",
          "name": "Peiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1d",
          "user": {
            "_id": "64bba541da140e461924dfed",
            "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
            "isPro": false,
            "fullname": "zhijie deng",
            "user": "zhijie3",
            "type": "user"
          },
          "name": "Zhijie Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:25.471Z",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1e",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1f",
          "name": "Hao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T05:00:56.000Z",
      "title": "Efficient-vDiT: 효율성 비디오 디피어션 트랜스포머 어텐션\n\n注意：原始文本中的\"Efficient-vDiT\"是直接翻译的，因为它看起来像是一个特定的技术术语或模型名称。如果需要更详细的解释或背景信息，请提供更多上下文。",
      "summary": "DiTs는 3D 완전 注意 구조를 갖기 때문에 고품질의 비디오 합성이 가능하지만, 注意 계산의 복잡성과 많은 샘플링 단계로 인하여 인피리언스 비용이 높습니다. 예를 들어, 인기 있는 Open-Sora-Plan 모델은 1 장의 29 프레임의 비디오를 생성하기 위해 9 분 이상 시간이 필요합니다. 본 논문에서는 이 불리한 문제를 해결하기 위해 2 가지 측면에서 접근합니다: 1) 비디오 데이터 내의 무용성을 기반으로 3D 완전 注意를 줄입니다. 비디오 데이터의 3D 注意 맵에서 보인 널리 보이는 타일 타입의 재현 패턴을 식별하고, 비디오 프레임 수에 대한 선형 복잡도를 가진 새로운 familiy의 스パ르스 3D 注意를 제안합니다. 2) 기존의 다 단계 일관성 디스 태일 시션을 도입하여 샘플링 프로세스를 단축합니다. 전체 샘플링 트래지즌을 수리할 수 있는 단계로 분할하고, 각 단계 내에서 일관성 디스 태일 시션을 수행하여 다수 스텝 생성 능력을 활성화합니다. 또한, 저 복잡도의 注意와 다수 스텝 생성 능력을 통합하기 위해 3 단계 트레이닝 프로세스를 설계합니다. 특히, 0.1%의 사전 학습 데이터를 사용함으로써 Open-Sora-Plan-1.2 모델을 7.4x-7.8x 더 빠르게 만들 수 있으며, VBench에서 성능의 미세 조정이 가능합니다. 또한, 우리의 접근 방식은 분산 인피리언스를 적용할 수 있으며, 4 그래픽스 상에서 시퀀스 병렬 프로그래밍을 사용함으로써 3.91x의 추가 속도 업을 실현할 수 있습니다.",
      "upvotes": 5,
      "discussionId": "67aab9bca2bf5e5ea03d4e3c"
    },
    "publishedAt": "2025-02-10T22:09:58.181Z",
    "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06155.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63565cc56d7fcf1bedb7d347",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
      "fullname": "Zhang Peiyuan",
      "name": "PY007",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 82
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06527",
      "authors": [
        {
          "_id": "67aae4128d478dcb4b39a097",
          "name": "D. She",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a098",
          "name": "Mushui Liu",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a099",
          "name": "Jingxuan Pang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09a",
          "name": "Jin Wang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09b",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09c",
          "name": "Wanggui He",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09d",
          "name": "Guanghao Zhang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09e",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09f",
          "name": "Qihan Huang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a0",
          "name": "Haobin Tang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a1",
          "name": "Yunlong Yu",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a2",
          "name": "Siming Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T14:50:32.000Z",
      "title": "CustomVideoX: 3D 참고에 기반한 동력적 적응을 이용한 0샷에 기반한 사용자定制 VIDEODIFFUSION TRANSFORMER",
      "summary": "定制生成在图像合成领域取得了显著的进展，但个性化视频生成因时间上的不确定性和质量下降而变得困难。本文介绍了一种新的框架“CustomVideoX”，用于从参考图像生成个性化视频。CustomVideoX利用预先训练的视频网络，通过独立训练LoRA参数来提取参考特征，从而确保效率和适应性。为了促进参考图像与视频内容之间的无缝交互，提出了3D Reference Attention。这使得参考图像的特征与视频所有帧的空间和时间维度之间能够进行直接且同时的对应。为了在推理时减少参考图像特征和语境指导对生成的视频内容的过度影响，实施了Time-Aware Reference Attention Bias (TAB) 策略。该策略根据时间步骤动态调整参考偏置。此外，还介绍了Entity Region-Aware Enhancement (ERAE) 模块。该模块根据参考特征的注入，调整注意偏置，以增强关键实体标记的高度激活区域，从而提高效果。为了详细评估个性化视频生成，构建了包含50个以上物体和100个以上提示的新基准“VideoBench”。实验结果表明，CustomVideoX在视频的一致性和质量方面大幅超越了现有方法。",
      "upvotes": 4,
      "discussionId": "67aae4178d478dcb4b39a1e7"
    },
    "publishedAt": "2025-02-11T00:46:11.168Z",
    "title": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06635",
      "authors": [
        {
          "_id": "67aac0ba91e6f5eb5476ea76",
          "name": "Qingshui Gu",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea77",
          "name": "Shu Li",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea78",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tianyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:15.968Z",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea79",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T16:31:37.000Z",
      "title": "Steel-LLM: 스크래치에서 오픈소스로 -- 중국어 중심의 LLM 구축의 개인적인 여정",
      "summary": "スチール-LLM는 계산 자원의 한계에도 불구하고 고품질의 오픈 소스 모델을 제작하기 위해 개발된 중국 중심의 언어 모델입니다. 2024년 3월 발표된 이 프로젝트는 10억 파라미터 모델을 훈련하는 것을 목표로, 투명성과 실용적인 정보의 공유를 우선시하여, 커뮤니티의 다른 사람들에게 도움이 될 수 있도록 설계되었습니다. 훈련 프로세스는 주로 중국의 데이터를 대상으로 하며, 일부 영어 데이터를 포함하여, 현재 오픈 소스 LLM의 부족점을 보완하고, 모델 제작의 여정을 더 자세히 설명할 수 있도록 제공되었습니다. 스チール-LLM는 CEVAL, CMMLU 등 벤치마크에서 경쟁적인 성능을示し, 대규모 기관으로부터의 초기 모델을 초월했습니다. 이 논문은 프로젝트의 주요 기여를 총괄적으로 요약한 것입니다. 모델 개발에 대한 연구자 및 실천자에게 유효한 리소스로 제공됩니다. 모델 체크포인트와 훈련 스크립트는 https://github.com/zhanshijinwat/Steel-LLM에서 사용 가능합니다.",
      "upvotes": 4,
      "discussionId": "67aac0bb91e6f5eb5476eab8"
    },
    "publishedAt": "2025-02-10T22:20:38.168Z",
    "title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06635.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ab99dcb76bfd863eba64c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
      "fullname": "TY.Zheng",
      "name": "aaabiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.04370",
      "authors": [
        {
          "_id": "67aafd90141fac22732a79b3",
          "name": "Zhenglin Zhou",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b4",
          "name": "Xiaobo Xia",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b5",
          "name": "Fan Ma",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b6",
          "name": "Hehe Fan",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b7",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b8",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T11:03:08.000Z",
      "title": "DreamDPO: 인간의 취향과 일치하는 Text-to-3D 생성에 대한 직접적인 취향 최적화",
      "summary": "텍스트로부터 3D 생성은 텍스트 설명으로부터 3D 콘텐츠의 자동 생성을 수행하며 다양한 분야에서 혁신적인 가능성을 가지고 있습니다. 그러나 현재의 방법들은 생성된 콘텐츠와 사람의 취향의 일치를 어렵게 만들고 적용 범위와 유연성을 제한하고 있습니다. 이러한 제한을 해결하기 위해 본 논문에서는 직접적인 취향 최적화를 통해 3D 생성 프로세스에 사람을 통합하는 최적화 기반 프레임워크 \"DreamDPO\"를 제안합니다. 실用上, DreamDPO는 처음에 페어 와이스의 예를 구축하고 이들이 가지고 있는 취향과 일치를 상여 기능이나 큰 규모의 다모달로 비교하며, 최종적으로 취향을 주도한 손실 함수를 사용하여 3D 표현을 최적화합니다. 페어 와이스의 비교를 통해 취향을 반영하는 데 사용되어, DreamDPO는 정확도가 높은 점별 평가에 의존하지 않고 취향을 가이드하는 최적화에 의해 微妙한 제어 가능도를 증가시킵니다. 실험은 DreamDPO가 현재의 방법에 비해 우수한 결과를 구현하고, 고품질으로 제어 가능한 3D 콘텐츠 제공함을 보여줍니다. 코드와 모델은 오픈 소스로 제공됩니다.",
      "upvotes": 3,
      "discussionId": "67aafd94141fac22732a7adc"
    },
    "publishedAt": "2025-02-11T02:46:33.870Z",
    "title": "DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6425318d175bd2952281065e/R7cMLIsmYovAMtL1vhsDn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6425318d175bd2952281065e",
      "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
      "fullname": "ZhenglinZhou",
      "name": "zhenglin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05957",
      "authors": [
        {
          "_id": "67aaecec114e64d6e15e7f41",
          "name": "Jiabin Tang",
          "hidden": false
        },
        {
          "_id": "67aaecec114e64d6e15e7f42",
          "name": "Tianyu Fan",
          "hidden": false
        },
        {
          "_id": "67aaecec114e64d6e15e7f43",
          "name": "Chao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T16:53:56.000Z",
      "title": "MetaChain: 완전 자동화 및 코드 없는 프레임워크를 사용하여 LLM 에이전트를 구축합니다.",
      "summary": "대 언어 모델(LLM) 에이전트는 태스크 자동화와 지능적인 결정에 있어서 놀라운 능력을 보여주고 있으며, LangChain, AutoGen과 같은 에이전트 개발 프레임워크의 광범위한 도입을 촉발하고 있습니다. 그러나 이러한 프레임워크는 주로 기술적인 전문 지식을 풍부한 개발자만 제공되어, 기술 배경이 있는 사람들만 사용할 수 있는 상황은 심각합니다. 이러한 엄격한 접근성은 세계 인구의 0.03%의 사람들이 필요한 프로그래밍 기술을 가진 것으로 나타내며, 이는 큰 한계로 작용합니다. 이러한 엄격한 접근성은 기술 배경이 없는 사람이 LLM 에이전트를 직접 구축할 수 있는지의 기본적인 문제를 제기하고 있습니다. 이러한 도전에 대처하기 위해, 우리는 MetaChain라는 완전한 자동화된, 높은 수준의 자동 개발 프레임워크를 소개합니다. 이 프레임워크는 LLM 에이전트를 생성하고 배포하기 위해 자연어 텍스트만 사용합니다. MetaChain은 자동 에이전트 운영 시스템으로 동작하며, Agentic System Utilities, LLM Port Driving Engine, Self-Managing File System, Self-Play Agent Customization 모듈의 4개의 주요 구성 요소로 구성되어 있습니다. 이 시스템은 코드 요구도와 손동작이 필요하지 않아도, 도구, 에이전트, 작업 흐름의 효율적이고 동적인 생성과 변경이 가능합니다. MetaChain은 코딩을 필요로 하지 않는 에이전트 개발 기능을 가지고 있기 때문에, 더 이상 일반적인 다 에이전트 시스템에서도 다양성을 유지합니다. GAIA 벤치마크에서 MetaChain의 일반 다 에이전트 태스크에 대한 효율성을 보여주며, 현재의 최상위 방법보다 뛰어납니다. 또한 MetaChain의 리뷰 에이전트 제네레이터(RAG) 관련 기능은 여러 대차원 LLM 기반 솔루션과 비교하여 일관된 우수한 성능을 보여주고 있습니다.",
      "upvotes": 3,
      "discussionId": "67aaecef114e64d6e15e802c"
    },
    "publishedAt": "2025-02-11T01:33:35.134Z",
    "title": "MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05957.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643b751cc5f633a7fa84b325",
      "avatarUrl": "/avatars/a094b856cf3d51eb78d16a14361def62.svg",
      "fullname": "Tang",
      "name": "Jiabin99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06764",
      "authors": [
        {
          "_id": "67aac6052c02e43558b6b4b0",
          "name": "Kiwhan Song",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b1",
          "name": "Boyuan Chen",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b2",
          "name": "Max Simchowitz",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b3",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b4",
          "name": "Russ Tedrake",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b5",
          "name": "Vincent Sitzmann",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:44:25.000Z",
      "title": "역사 가이드 드라이브 디퓨쯔",
      "summary": "필터프리어드 ġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġġ",
      "upvotes": 3,
      "discussionId": "67aac6072c02e43558b6b543"
    },
    "publishedAt": "2025-02-11T00:55:33.866Z",
    "title": "History-Guided Video Diffusion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06764.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06023",
      "authors": [
        {
          "_id": "67aac3a9ef5570c0c9047095",
          "user": {
            "_id": "640f6299ef5c6dcac8b1df52",
            "avatarUrl": "/avatars/022f21183abc8a8b5ce1b198d3ba96dc.svg",
            "isPro": false,
            "fullname": "Amir",
            "user": "sahsaeedi",
            "type": "user"
          },
          "name": "Amir Saeidi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-11T03:31:48.492Z",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047096",
          "name": "Yiran Luo",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047097",
          "name": "Agneet Chatterjee",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047098",
          "name": "Shamanthak Hegde",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047099",
          "name": "Bimsara Pathiraja",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c904709a",
          "name": "Yezhou Yang",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c904709b",
          "name": "Chitta Baral",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T20:34:43.000Z",
      "title": "듀얼캡시온 선호도 최적화 모델",
      "summary": "최근, 인간의 취미 최적화의 발전은 원래 Large Language Models (LLMs)에 개발된 것이며, 문장부터 이미지로의 확산 모델의 개선에도 상당한 가능성을 보여주고 있다. 이러한 방법들은 취미의 분포를 학습하는 동시에 그와 다른 분포를 구분하는 것을 목표로 한다. 그러나 현재의 취미 데이터 세트는 이러한 분포가 겹쳐져, 충돌 분포가 많은 경우가 많다. 또한, 입력 프롬프트에 무관한 이미지에 대한 정보를 포함하고 있는 것을 발견하여, 취미 최적화 방법의 노이즈의 정확한 예측을 제한함으로써, 무관 프롬프트 문제로 알려진 것이다. 이러한 문제를 해결하기 위해, 우리는 새로운 접근 방식인 Dual Caption Preference Optimization (DCPO)를 제안하고 있다. 이는 무관 프롬프트를 억제하기 위해 두 가지 다른 캡션을 사용하도록 한다. 충돌 분포를 해결하기 위해, Pick-Double Caption 데이터 세트를 제안하고 있다. 이것은 Pick-a-Pic v2의 개선판으로, 취미의 이미지와 무관 이미지에 대해 각각의 캡션을 사용하도록 한다. 또한, 세 가지 다른 캡션 생성 전략을 제안하고 있다: 캡치닝, 패르미브티브, 하이브리드 메소드. 우리의 실험은 DCPO가 이미지의 품질과 프롬프트의 관련성을 크게 개선하고, Stable Diffusion (SD) 2.1, SFT_Chosen, Diffusion-DPO, MaPO를 초과하는 것을 보여주며, Pickscore, HPSv2.1, GenEval, CLIPscore, ImageReward의 여러 측정 지표로 SD 2.1을 백드롭으로 微調節した 것이다.",
      "upvotes": 3,
      "discussionId": "67aac3b1ef5570c0c9047264"
    },
    "publishedAt": "2025-02-10T22:33:17.468Z",
    "title": "Dual Caption Preference Optimization for Diffusion Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06023.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640f6299ef5c6dcac8b1df52",
      "avatarUrl": "/avatars/022f21183abc8a8b5ce1b198d3ba96dc.svg",
      "fullname": "Amir",
      "name": "sahsaeedi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06060",
      "authors": [
        {
          "_id": "67ab1314385da1f07cda1271",
          "user": {
            "_id": "63abbf74ad514ca8d14a0548",
            "avatarUrl": "/avatars/b1357b73b8f9a8ff9908710ad64154ef.svg",
            "isPro": false,
            "fullname": "Bidipta Sarkar",
            "user": "bidiptas",
            "type": "user"
          },
          "name": "Bidipta Sarkar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:51:17.933Z",
          "hidden": false
        },
        {
          "_id": "67ab1314385da1f07cda1272",
          "name": "Warren Xia",
          "hidden": false
        },
        {
          "_id": "67ab1314385da1f07cda1273",
          "name": "C. Karen Liu",
          "hidden": false
        },
        {
          "_id": "67ab1314385da1f07cda1274",
          "name": "Dorsa Sadigh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T22:44:45.000Z",
      "title": "다 에이전트 강화학습을 이용한 사회 대화에 대한 언어 모델의 훈련",
      "summary": "자연어에서의 커뮤니케이션은 효과적인 도구로, 다 에이전트 설정에서 독립된 에이전트가 부분 관측된 정보를 공유하고, 인간과의 0샷 협조를 가능하게 합니다. 그러나 선행 연구들은 대부분 대규모인간의 демонс레이션을 기반으로 훈련하거나, 자연적이고 효과적인 커뮤니케이션 전략을 생성하는 능력이 제한되어 있습니다. 본 논문에서는, 인간의 демонс레이션을 포함하여, 자연어로 환경에 대한 논의를 수행함으로써 커뮤니케이션을 훈련합니다. 커뮤니케이션 문제를 청취와 대화로 분해하고, 에이전트의 목표를 활용하여, 유용한 정보를 예측하여 대화를 가이드하는 데 필요한 데ン스 리드 신호를 생성하는 기본적인 아이디어로 생각됩니다. 특히, 청취 능력을 향상시키기 위해, 토론에 기반한 환경에 대한 정보를 예측하여 모델을 훈련하고, 동시에 다 에이전트의 강화 학습을 사용하여, 에이전트 간 영향에 기반한 메시지를 보상하여 대화의 능력을 향상시킵니다. 복잡한 사회적인 환경에서 커뮤니케이션의 역할과 필요성을 조사하기 위해, 샘플러에 기반한 사회 설명 게임을 연구하고, 적대적인 비규칙적인 인물을 조사하는 중요한 문제로 나옵니다. 우리 방법론으로 발생한 현상을 분석하고, 의문점, 증거 제공 등 현상을 발견하여, 강력한 토론을 가능하게 하고, 표준 RL 대비 2배의 승률을 달성했습니다. 우리 코드와 모델은 다음 URL에서 릴리즈됩니다. https://socialdeductionllm.github.io/",
      "upvotes": 2,
      "discussionId": "67ab1315385da1f07cda12a5"
    },
    "publishedAt": "2025-02-11T04:08:55.672Z",
    "title": "Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63abbf74ad514ca8d14a0548",
      "avatarUrl": "/avatars/b1357b73b8f9a8ff9908710ad64154ef.svg",
      "fullname": "Bidipta Sarkar",
      "name": "bidiptas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05795",
      "authors": [
        {
          "_id": "67ab189a8087b66340398b01",
          "name": "Wenfang Sun",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b02",
          "name": "Xinyuan Song",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b03",
          "user": {
            "_id": "64245f2c089d5fae56b4549a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
            "isPro": false,
            "fullname": "Pengxiang Li",
            "user": "pengxiang",
            "type": "user"
          },
          "name": "Pengxiang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:51:15.671Z",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b04",
          "name": "Lu Yin",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b05",
          "name": "Yefeng Zheng",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b06",
          "name": "Shiwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T07:03:36.000Z",
      "title": "「대언어 모댈의 심도의 저咒」",
      "summary": "이 논문에서는 \"Depth of Depth\"라는 개념을 통해, 현대의 대규모 언어 모델(LLMs)에서 절반 이상의 레이어가 예상치 못한 낮은 효과를 발휘하는 현상을 밝혀내며, 해석하고 해결책을 제안합니다. 먼저, Llama, Mistral, DeepSeek, Qwen 등 가장 인기 있는 LLMs의 가족에서 이러한 현상의 광범위한 존재를 확인합니다. 이론적 및 실험적 분석에 따라, LLMs의 깊은 레이어의 무효성의 근본적인 원인으로 Pre-Layer Normalization(Pre-LN)의 광범위한 사용이 식별됩니다. Pre-LN은 Transformer LLMs의 훈련을 안정화하지만, 출력 분산은 모델의 깊이에 지수적으로 증가하며, 깊은 Transformer 블록의 미분은 등분행렬로 변하여 훈련에 거의 기여하지 않습니다. 이러한 훈련의 문제를 해결하기 위해, LayerNorm Scaling을 제안합니다. LayerNorm Scaling은 레이어 정규화의 출력 분산을 깊이의 제곱근의 역으로 스케일링하여, 깊은 Transformer 레이어의 출력 분산의 폭발을 억제하고 기여를 향상시킵니다. 실험 결과는 130M부터 1B의 모델 크기 범위에서, Pre-LN과 비교하여 LayerNorm Scaling이 LLM의 사전 학습 성능을 크게 향상시키는 것을 보여주며, 이러한 향상은 LayerNorm Scaling이 훈련 중 깊은 레이어가 더 효과적으로 기여하는 것을 허용하는 것입니다.",
      "upvotes": 1,
      "discussionId": "67ab189b8087b66340398b3b"
    },
    "publishedAt": "2025-02-11T04:30:30.043Z",
    "title": "The Curse of Depth in Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05795.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64245f2c089d5fae56b4549a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
      "fullname": "Pengxiang Li",
      "name": "pengxiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]