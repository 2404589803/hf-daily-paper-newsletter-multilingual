[
  {
    "paper": {
      "id": "2502.01362",
      "authors": [
        {
          "_id": "67a2ad6ac7caec9bf5a45e61",
          "name": "Nikita Gushchin",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e62",
          "name": "David Li",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e63",
          "name": "Daniil Selikhanovych",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e64",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e65",
          "name": "Dmitry Baranchuk",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e66",
          "name": "Alexander Korotin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T13:56:03.000Z",
      "title": "역 브릿지 매칭 디스틸루션",
      "summary": "학습 디피유전 브릿지 모달은 간단하지만, 이를 고속화하여 실용적으로 만드는 것은 예술적입니다. 디피유전 브릿지 모달(DBMs)는 이미지 변환의 적용 분야에서 디피유전 모달의 유망한 확장으로 평가됩니다. 그러나 다수의 현대적인 디피유전 모달과 플로우 모달과 마찬가지로, DBMs는 추론 속도의 느린 문제를 겪습니다. 이에대해, 우리는 역 브릿지 매칭의 공식에 기반한 새로운 연기 기술에 제안하고, 실용적인 해결책을 찾기 위해 계산 가능한 목적 함수를 계산합니다. 이전에 개발된 DBM 연기 기술과 다른 점은 제안된 방법은 조건부 및 비조건부 두 가지의 DBMs를 모두 연기할 수 있으며, 한 단계 제네레이터로 연기하고, 그리고 그뿐만 아니라 파괴된 이미지만을 사용하여 훈련할 수 있습니다. 조건부 및 비조건부 두 가지의 브릿지 매칭에 대해 광범위한 설정에서 평가하였으며, 슈퍼 리소루션, JPEG 리프팅, 스크래치에서 이미지로의 변환 및 기타 작업 포함, 우리의 연기 기술은 DBMs의 추론 속도를 4배에서 100배까지 가속화하고, 특정 설정에 따라는 교사 모달의 생성 품질을 초과하는 것을 보여주었습니다.",
      "upvotes": 16,
      "discussionId": "67a2ad70c7caec9bf5a45fb0"
    },
    "publishedAt": "2025-02-05T03:01:40.464Z",
    "title": "Inverse Bridge Matching Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01362.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "672503c59f68afdd63cc81a2",
      "avatarUrl": "/avatars/91207207b56a1fc2b4a8197b1ab3a7f9.svg",
      "fullname": "Nikita Gushchin",
      "name": "ngushchin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01718",
      "authors": [
        {
          "_id": "67a2d995c97974764a8c294c",
          "name": "Huaye Zeng",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294d",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294e",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294f",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c2950",
          "name": "Xiaotong Chen",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c2951",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:46:04.000Z",
      "title": "ACECODER: 자동 테스트 케이스 합성에 의한 우수한 코더 RL\n\n(Note: The original text \"ACECODER: 自動テストケース合成による優秀なコーダーRL\" was translated to Korean as \"ACECODER: 자동 테스트 케이스 합성에 의한 우수한 코더 RL\". This translation maintains the original structure and meaning while ensuring accuracy and professionalism.)",
      "summary": "최근의 코드 모델의 발전은 여러 가지 지도 학습(SFT)에 의해 구동되고 있지만, 强化学習(RL)의 가능성은 코드 영역에서 신뢰할 수 있는 보상 데이터와 모델의 부족으로 주로 탐색되어 있지 않습니다. 본 논문에서는 이러한 도전을 해결하기 위해 자동화된 대규모 테스트 케이스의 합성을 활용하여 코드 모델의 훈련을 강화합니다. 특히, 현재의 코드 데이터에서 광범위한 (문제, 테스트 케이스) 쌍을 생성하는 파이프라인을 설계합니다. 이러한 테스트 케이스를 사용하여 프로그램의 통과율에 기반한 선호 쌍을 구축하고, ブライディー〜テリー 손실을 사용하여 보상 모델을 훈련합니다. 이로 인해 Llama-3.1-8B-Ins에서 평균 10점의 상승, Qwen2.5-Coder-7B-Ins에서 5점의 상승이 관찰되었습니다(32개의 샘플 중 가장 좋은 3개를 선택). 또한 보상 모델과 테스트 케이스의 통과 보상을 사용하여 强化学習을 수행하고, HumanEval, MBPP, BigCodeBench, LiveCodeBench(V4)의 각 테스트에서 일관된 향상이 관찰되었습니다. 특히, R1 스타일의 훈련을 시작하여 Qwen2.5-Coder-base에서 직접 훈련을 시작하여 HumanEval-plus에서 25% 이상, MBPP-plus에서 6%의 상승이 관찰되었습니다(80 단계의 최적화). 우리는 코드 모델에서 强化学習의 큰 가능성에 대해 믿습니다.",
      "upvotes": 12,
      "discussionId": "67a2d996c97974764a8c29a1"
    },
    "publishedAt": "2025-02-04T22:23:07.858Z",
    "title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01718.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5946
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.02492",
      "authors": [
        {
          "_id": "67a2ec904ea0e3138ac966f2",
          "user": {
            "_id": "6181c72cdcc1df2c9de8a4d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
            "isPro": false,
            "fullname": "Hila Chefer",
            "user": "Hila",
            "type": "user"
          },
          "name": "Hila Chefer",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-05T04:44:03.218Z",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f3",
          "name": "Uriel Singer",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f4",
          "name": "Amit Zohar",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f5",
          "name": "Yuval Kirstain",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f6",
          "name": "Adam Polyak",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f7",
          "name": "Yaniv Taigman",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f8",
          "name": "Lior Wolf",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f9",
          "name": "Shelly Sheynin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T17:07:10.000Z",
      "title": "VideoJAM: 비디오 모델에서 기능적인 향상을 추구하는 출현과 동작의 공통 표현 방법",
      "summary": "최근의 놀라운 발전에도 불구하고, 생성 비디오 모델은 실제 세계의 움직임, 역학, 그리고 물리를 쉽게捉える能力が 부족하다. 우리는 이러한 제한이 전통적인 픽셀 재구성의 목표 함수로 인한 모델의 외관 충실성과 움직임의 일관성에 우선시되어 있는 것을 보여줍니다. 이를 대처하기 위해, 우리는 VideoJAM라는 새로운 프레임워크를 도입하여, 효과적인 움직임의 선구를 비디오 생성기로 제공하여, 외관과 움직임의 공통 표현을 학습시킬 수 있도록 제안합니다. VideoJAM은 두 개의 보간 단위로 구성되어 있으며, 훈련 기간에는 목표 함수를 확장하여 생성된 픽셀과 상대적인 움직임을 예측하여, 학습된 표현에서만 수행하도록 목표를 세팅합니다. 추론 기간에는, 자신의 움직임 예측의 변화를 동적인 가이드라인 신호로 활용하여, 움직임의 일관성에 대한 생성을 지도하는 Inner-Guidance 구조를 도입합니다. 특히, 우리의 프레임워크는 훈련 데이터나 모델의 크기에 대한 변경이 필요하지 않도록 설계되어 있습니다. VideoJAM은 움직임의 일관성에서 가장 先端적인 성능을 보인 반면, 높은 경쟁력을 가진 고급 모델을 초월하며, 생성물의 시각적 질량을 향상시킵니다. 이러한 발견은, 외관과 움직임이 보간적이며, 이들이 유효하게 통합되면, 비디오 생성의 시각적 품질과 일관성을 양방향으로 향상시킬 수 있다는 점을 강조합니다. 프로젝트 웹 사이트: https://hila-chefer.github.io/videojam-paper.github.io/",
      "upvotes": 11,
      "discussionId": "67a2ec934ea0e3138ac9678e"
    },
    "publishedAt": "2025-02-04T23:46:17.626Z",
    "title": "VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6181c72cdcc1df2c9de8a4d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
      "fullname": "Hila Chefer",
      "name": "Hila",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.02584",
      "authors": [
        {
          "_id": "67a2d59fd5ad3369a66ff394",
          "name": "Zongyu Lin",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff395",
          "name": "Yao Tang",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff396",
          "name": "Xingcheng Yao",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff397",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff398",
          "name": "Ziniu Hu",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff399",
          "name": "Yizhou Sun",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff39a",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T18:58:31.000Z",
      "title": "QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLAS",
      "summary": "언어 에이전트는 복잡한 상호작용 태스크에 대한 유망한 해결책으로 발전했습니다. 에이전트의 성공에 있어서 중요한 요소 중 하나는 에이전트 흐름의 경로 상의 보상 모델입니다. 이 모델은 학습 및 추론 과정에서 유익한 가이드를 제공합니다. 그러나 중간적인 상호작용의 설명이 부족하기 때문에 많은 연구에서는 최종 결과의 보상 모델을 사용하여 전체 경로를 구성하는 정책을 최적화하고 있습니다. 이는 최적의 정책을 얻을 수 있는 것을 방해하고 전체 성능을 저하시키는 경우가 있습니다. 이러한 문제를 해결하기 위해 QLASS(Q-GUIDED LANGUAGE AGENT STEP-WISE STEERING)를 제안합니다. QLASS는 오픈 언어 에이전트에 대해 단계별로 Q 값을 계산하여 자동으로 설명을 생성합니다. 이유의 나무를 제시하고 과정 보상 모델링을 수행하여 QLASS는 각 단계에 효과적인 중간 설명을 제공합니다. 단계별 설명을 받고, QLASS는 장기적인 가치에 의한 더 좋은 적응성을 갖게되어 복잡한 상호작용 에이전트 태스크의 모델 추론 시 성능을 크게 향상시킵니다. 특히, 절반 가까운 설명 데이터 사용 시도 QLASS는 강력한 성능을 유지합니다. 또한 QLASS는 질적인 분석을 통해 더 효과적인 결정을 취할 수 있음을 실험적으로 입증했습니다. 코드와 데이터를 공개합니다.",
      "upvotes": 7,
      "discussionId": "67a2d5a0d5ad3369a66ff3d4"
    },
    "publishedAt": "2025-02-04T22:08:25.652Z",
    "title": "QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4670a51d5df8c2d92fce",
      "avatarUrl": "/avatars/c52d7150b4de6a2eb2d83b345d35cbc2.svg",
      "fullname": "Da Yin",
      "name": "DaYin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01941",
      "authors": [
        {
          "_id": "67a2e2a02dd2adbc88755a47",
          "name": "Xiang Liu",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a48",
          "name": "Zhenheng Tang",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a49",
          "name": "Hong Chen",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4a",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4b",
          "name": "Zeyu Li",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4c",
          "name": "Xiuze Zhou",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4d",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4e",
          "name": "Xuming Hu",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4f",
          "name": "Xiaowen Chu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T02:23:06.000Z",
      "title": "LLM은 KV 캐시의 압축 아래 기본적인 능력을 유지할 수 있는지?",
      "summary": "이 논문은 대규모 언어 모델(LLMs)에서 조사되지 않은 문제에 대한 조사를 수행하고 있으며, KV 캐시의 압축 방법이 LLMs의 기본적인 능력을 어떻게 영향을 미치는지에 대한 영향에 대한 조사를 수행하고 있습니다. 기존의 방법들은 긴 문맥 벤치마크에서 놀라운 압축비를 달성하지만, 이들이 핵심 모델 능력에 미치는 영향은 아직 조사가 부족합니다. 우리는 세계적인 지식, 통찰력, 산술 추론, 코드 생성, 안전성, 긴 문맥 이해 및 생성에 대한 다양한 태스크에서 선도적인 KV 캐시 압축 방법을 평가하는 세부적인 실험 결과를 제공합니다. 분석에 따르면, KV 캐시 압축 방법은 특정 태스크에 따라 성능 저하를 나타냅니다. 산술 추론 태스크는 특히 심한 압축에 민감하며, 17.4% ~ 43.3%의 성능 저하를 보입니다. 특히, DeepSeek R1 Distill 모델은 지시 훈련 모델에 비해 압축의 견고성이 높으며, 9.67% ~ 25.53%의 성능 저하를 보입니다. 우리는 유의 패턴과 크로스 태스크 압축 성능 분석에 기반하여, 의미적인 일관성을 유지하는 동시에, 프리프일과 디코딩 페이즈를 특별히 처리하는 새로운 압축 접근 방식을 제안합니다. 실험 결과에 따르면, 심한 압축 비율에서도 긴 문맥 생성 태스크에서 9% ~ 18%의 성능 향상이 Realized 됩니다.",
      "upvotes": 6,
      "discussionId": "67a2e2a22dd2adbc88755ab4"
    },
    "publishedAt": "2025-02-04T23:04:25.888Z",
    "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63024676056ec3a2a8714b24/XcgjmhpXd3dH6LnFZGupJ.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63024676056ec3a2a8714b24/hxWz1iVOUcE76E_K5z-B0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.02508",
      "authors": [
        {
          "_id": "67a2d1f9bc9d072d9459e857",
          "user": {
            "_id": "6553c985a7aded0380b5f928",
            "avatarUrl": "/avatars/36109d6f536d2b34d98822b88eac9608.svg",
            "isPro": false,
            "fullname": "Maohao Shen",
            "user": "maohaos2",
            "type": "user"
          },
          "name": "Maohao Shen",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-05T03:00:33.470Z",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e858",
          "name": "Guangtao Zeng",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e859",
          "name": "Zhenting Qi",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85a",
          "name": "Zhang-Wei Hong",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85b",
          "name": "Zhenfang Chen",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85c",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85d",
          "name": "Gregory Wornell",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85e",
          "name": "Subhro Das",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85f",
          "name": "David Cox",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e860",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T17:26:58.000Z",
      "title": "Satori: 행동연쇄적 사고를 활용한 강화학습로 LLM을 강화\n자동추론적인 검색에 의한 논리론리\n\n(Note: The translation is provided as requested, maintaining professionalism and accuracy. The format of the original text is preserved for clarity.)",
      "summary": "대규모 언어 모델(LLMs)는 다양한 분야에서 놀라울만한 논리론 능력을 보여주고 있습니다. 최근의 연구에 따르면, 테스트 시 계산량의 증가가 LLMs의 논리론 능력을 향상시키는 것을 알게 되었습니다. 이것은 일반적으로 추론 시 외부 LLM 검증 데이터에 의한 광범위한 샘플링을 포함하며, 이를 통해 2 플레이어 시스템이 형성됩니다. 외부 가이드가 있는 지 상관없이, 이 시스템의 효율성은 단일 LLM이 복잡한 작업을 수행할 수 있음을 보여주고 있습니다. 따라서, 우리는 새로운 연구 문제를 제안합니다: 단일 LLM의 논리론 능력을 근본적으로 향상시키기 위해, 탐색 능력을 내부화할 수 있는지 여부에 대해 연구를 진행합니다. 이 연구는 후 학습의 LLMs를 중심으로, 자동 협업 탐색(즉, 자기 반성 및 새로운 전략의 자기 탐색을 포함한 확장된 논리론 프로세스)에 초점을 맞추는 직교 방향의 연구를 진행합니다. 이를 실현하기 위해, Chain-of-Action-Thought(COAT) 논리론과 2 단계의 학습 패러다임(1) COAT 논리론의 형식을 내부화하는 소규모 형식 훈련 단계와 (2) 큰 규모의 자기 개선 단계에서의 강화 학습을 활용합니다. 우리 접근법으로, Satori라는 7B LLM는 오픈 소스 모델과 데이터에 의해 훈련되었습니다. 확장된 실험 평가에 따라, Satori는 수학적인 논리론 벤치마크에서 가장 선진적인 성능을 달성하며, 영역 외의 태스크에 강한 일반화 능력을 보여주고 있습니다. 코드, 데이터, 모델은 모두 완전한 오픈 소스로 합니다.",
      "upvotes": 5,
      "discussionId": "67a2d1fcbc9d072d9459e91b"
    },
    "publishedAt": "2025-02-04T21:55:09.693Z",
    "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60ad0de755f970745d4ec28d",
      "avatarUrl": "/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg",
      "fullname": "GtZeng",
      "name": "chaoscodes",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01720",
      "authors": [
        {
          "_id": "67a2fddb4044bf1c86f765a3",
          "name": "Nupur Kumari",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a4",
          "name": "Xi Yin",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a5",
          "name": "Jun-Yan Zhu",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a6",
          "name": "Ishan Misra",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a7",
          "name": "Samaneh Azadi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:59:41.000Z",
      "title": "텍스트로부터 이미지의 맞춤형 생성 - 멀티이미지 합성 데이터의 생성에 대한 기술",
      "summary": "テキストモデル의 カスタマイズ化使用者が カスタム概念を挿入し、見たことのない設定でその概念を生成できるようにすることができます。現在の方法は、コスト高いテスト時最適化を依存しているか、または、単一画像のトレーニングデータセットでエンコーダーをトレーニングしているため、画像の質が悪くなることがあります。私たちは両方の制限を解決する簡単なアプローチを提案します。まず、既存のテキストから画像モデルと3Dデータセットを利用して、同じ物体の様々な照明、背景、姿勢の画像を含む高品質の合成カスタマイズデータセット（SynCD）を作成します。次に、共有アテンション機構に基づいた新しいエンコーダーアーキテクチャを提案し、入力画像からより細かい可視的詳細をより良く組み込むことを目指します。最後に、推論時のオーバーエクスポーション問題を軽減するために、テキストと画像ガイドベクトルを正規化する新しい推論手法を提案します。拡張された実験で、提案されたエンコーダーと推論アルゴリズムを用いた合成データセットでトレーニングされたモデルが、標準的なカスタマイズバーチャーで現在のトーンフリー方法を上回ることを示します。",
      "upvotes": 2,
      "discussionId": "67a2fde34044bf1c86f767ba"
    },
    "publishedAt": "2025-02-05T00:59:11.275Z",
    "title": "Generating Multi-Image Synthetic Data for Text-to-Image Customization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f6a894c3372328414c7021",
      "avatarUrl": "/avatars/e8b10912355712f38f10805c31bea962.svg",
      "fullname": "Nupur Kumari",
      "name": "nupurkmr9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]