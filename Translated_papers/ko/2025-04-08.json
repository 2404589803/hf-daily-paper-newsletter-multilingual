[
  {
    "paper": {
      "id": "2504.05298",
      "authors": [
        {
          "_id": "67f4a0ccfefcc542f83f84e7",
          "user": {
            "_id": "646ebf2298e8f749fc60ce38",
            "avatarUrl": "/avatars/5a76c9343451c24e9697d3165cdc0af6.svg",
            "isPro": false,
            "fullname": "Karan Dalal",
            "user": "karansdalal",
            "type": "user"
          },
          "name": "Karan Dalal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:09:24.283Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84e8",
          "user": {
            "_id": "66b01b126038fe024ae979d3",
            "avatarUrl": "/avatars/75b561be803fa28cb78c6cff9eb5ac54.svg",
            "isPro": false,
            "fullname": "Daniel Koceja",
            "user": "koceja",
            "type": "user"
          },
          "name": "Daniel Koceja",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:09:31.602Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84e9",
          "user": {
            "_id": "638835fede1fa6adc5485a05",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669871090087-noauth.jpeg",
            "isPro": false,
            "fullname": "Gashon Hussein",
            "user": "GashonHussein",
            "type": "user"
          },
          "name": "Gashon Hussein",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:09:39.483Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ea",
          "name": "Jiarui Xu",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84eb",
          "user": {
            "_id": "638fe91639f7e2a7f9d2a8c6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
            "isPro": false,
            "fullname": "Yue Zhao",
            "user": "zhaoyue-zephyrus",
            "type": "user"
          },
          "name": "Yue Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:29.995Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ec",
          "name": "Youjin Song",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ed",
          "name": "Shihao Han",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ee",
          "name": "Ka Chun Cheung",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ef",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f0",
          "user": {
            "_id": "677c45511fbb93b90f1c6f3d",
            "avatarUrl": "/avatars/3a78fdd8d1debc8d267e80e4b7a6bf77.svg",
            "isPro": false,
            "fullname": "Carlos Guestrin",
            "user": "guestrin",
            "type": "user"
          },
          "name": "Carlos Guestrin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:10:26.609Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f1",
          "user": {
            "_id": "67ecbb7eb57a7b083182ea3f",
            "avatarUrl": "/avatars/8fb800e0729771e61ff0d2f9a05eb5b9.svg",
            "isPro": false,
            "fullname": "Tatsunori Hashimoto",
            "user": "hashtag56",
            "type": "user"
          },
          "name": "Tatsunori Hashimoto",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:10:20.291Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f2",
          "user": {
            "_id": "64931e7e2da595588288f161",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64931e7e2da595588288f161/4jOhJOFsU7RVFMgGk5kO7.jpeg",
            "isPro": false,
            "fullname": "Sanmi Koyejo",
            "user": "sanmikoyejo",
            "type": "user"
          },
          "name": "Sanmi Koyejo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:10:14.122Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f3",
          "user": {
            "_id": "64d42729f63b01b7f676b176",
            "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
            "isPro": false,
            "fullname": "Yejin Choi",
            "user": "yejinchoinka",
            "type": "user"
          },
          "name": "Yejin Choi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:10:07.480Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f4",
          "name": "Yu Sun",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f5",
          "name": "Xiaolong Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638fe91639f7e2a7f9d2a8c6/cTDthYKFDTs8NDfvfVKJI.mp4"
      ],
      "publishedAt": "2025-04-07T17:56:31.000Z",
      "submittedOnDailyAt": "2025-04-08T02:38:37.647Z",
      "title": "1분간의 비디오 생성에 대해 테스트 시 학습을 사용합니다.",
      "submittedOnDailyBy": {
        "_id": "638fe91639f7e2a7f9d2a8c6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
        "isPro": false,
        "fullname": "Yue Zhao",
        "user": "zhaoyue-zephyrus",
        "type": "user"
      },
      "summary": "Transformers는 현재 1분 길이의 비디오를 생성하는 데 어려움을 겪고 있습니다. 이는 자동주의 계층이 긴 컨텍스트에서 효율적이지 않기 때문입니다. 반면, Mandala 계층, Gated Delta Network, Sliding Window Attention 계층 등 대체 방법들은 복잡한 다스페이스 스토리리에 대해 은닉 상태의 표현력이 약해 어려워집니다. 우리는 TTT 계층을 실험했습니다. 이 계층은 은닉 상태 자체가 신경망으로 구성되어 있기 때문에 더 강한 표현력을 가집니다. TTT 계층을 사전 학습된 Transformer에 추가하면 텍스트 스토리리보드에서 1분 길이의 비디오를 생성할 수 있습니다. 증명을 위해, 우리는 토姆과 조의 카트 밴드为基础로 데이터셋을 구축했습니다. Mandala~2, Gated Delta Network, Sliding Window Attention 계층 등 기본 모델과 비교하여, TTT 계층은 복잡한 스토리리를 더 연속적인 비디오로 전달하고, 100 비디오 당 인간 평가에서 34 Elo 포인트를 초과합니다. 이는 홍보적인 사항이지만, 결과는 사전 학습된 5B 모델의 제한된 능력에 의한 오차가 존재할 가능성이 있습니다. 우리 구현의 효율화도 개선할 수 있습니다. 리소스의 제약으로 1분 길이의 비디오만 실험하였지만, 이 접근법은 긴 비디오나 복잡한 스토리리에 확장할 수 있습니다. 샘플 비디오, 코드 및 주석은 아래 URL에서 사용 가능합니다: https://test-time-training.github.io/video-dit",
      "upvotes": 29,
      "discussionId": "67f4a0cefefcc542f83f8592",
      "projectPage": "https://test-time-training.github.io/video-dit/",
      "githubRepo": "https://github.com/test-time-training/ttt-video-dit"
    },
    "publishedAt": "2025-04-07T13:56:31.000Z",
    "title": "One-Minute Video Generation with Test-Time Training",
    "summary": "Transformers today still struggle to generate one-minute videos because\nself-attention layers are inefficient for long context. Alternatives such as\nMamba layers struggle with complex multi-scene stories because their hidden\nstates are less expressive. We experiment with Test-Time Training (TTT) layers,\nwhose hidden states themselves can be neural networks, therefore more\nexpressive. Adding TTT layers into a pre-trained Transformer enables it to\ngenerate one-minute videos from text storyboards. For proof of concept, we\ncurate a dataset based on Tom and Jerry cartoons. Compared to baselines such as\nMamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers\ngenerate much more coherent videos that tell complex stories, leading by 34 Elo\npoints in a human evaluation of 100 videos per method. Although promising,\nresults still contain artifacts, likely due to the limited capability of the\npre-trained 5B model. The efficiency of our implementation can also be\nimproved. We have only experimented with one-minute videos due to resource\nconstraints, but the approach can be extended to longer videos and more complex\nstories. Sample videos, code and annotations are available at:\nhttps://test-time-training.github.io/video-dit",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638fe91639f7e2a7f9d2a8c6/cTDthYKFDTs8NDfvfVKJI.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05298.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "638fe91639f7e2a7f9d2a8c6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
      "fullname": "Yue Zhao",
      "name": "zhaoyue-zephyrus",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.04718",
      "authors": [
        {
          "_id": "67f48eff463c4d1c4ae5284b",
          "user": {
            "_id": "64b74920fe6a108d03fed767",
            "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
            "isPro": false,
            "fullname": "Minki Kang",
            "user": "Nardien",
            "type": "user"
          },
          "name": "Minki Kang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:12:42.979Z",
          "hidden": false
        },
        {
          "_id": "67f48eff463c4d1c4ae5284c",
          "name": "Jongwon Jeong",
          "hidden": false
        },
        {
          "_id": "67f48eff463c4d1c4ae5284d",
          "name": "Jaewoong Cho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T04:01:17.000Z",
      "submittedOnDailyAt": "2025-04-08T06:04:55.039Z",
      "title": "小型 언어 모델로 테스트 시 도구 통합에 의한 계산 스케일링의 자동 인증을 수행하는 도구 통합된 테스트 시의 계산 스케일링의 자동 인증을 수행하는小型 언어 모델",
      "submittedOnDailyBy": {
        "_id": "64b74920fe6a108d03fed767",
        "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
        "isPro": false,
        "fullname": "Minki Kang",
        "user": "Nardien",
        "type": "user"
      },
      "summary": "최근의 연구에 따르면, 테스트 시의 계산 스케일링이 소규모의 언어 모델(sLMs)의 성능을 효과적으로 향상시킬 수 있음을 보여주고 있습니다. 그러나 기존 연구에서는 테스트 시의 계산 스케일링을 수행하기 위해 더 큰 모델을 추가하여 확인을 위해 사용하였으나, sLMs가 스스로 확인을 할 수 있는 것(자기확인)에 대한 조사는 미흡한 상태였습니다. 본 연구에서는 sLMs가 테스트 시의 스케일링 하에서 확신적으로 자신의 출력을 확인할 수 있는지 조사합니다. 결과적으로, 더 큰 확인자으로부터의 지식의 흡수에 의한 경우와 같이, sLMs는 기억이 필요한 검증 태스크(예: 숫자 계산, 사실 확인)에 어려움을 보입니다. 이러한 제한을 해결하기 위해, 기억이 많은 검증 단계를 외부 도구(예: 코드 인터프리터)에 맡기는 \"툴 통합된 자기확인(T1)\"을 제안합니다. 이론적인 분석에 따르면, 툴 통합은 기억의 필요성을 줄이고, 테스트 시의 스케일링 성능을 향상시킬 수 있음을 보여주었습니다. MATH 벤치마크의 실험에서 T1을 사용하였던 Llama-3.2 1B 모델은 테스트 시의 스케일링 하에서, 훨씬 더 큰 Llama-3.1 8B 모델을 크게 초월할 수 있음을 확인했습니다. 또한, T1은 수학적(MATH500) 및 다양한 분야의 지식밀집 태스크(MMLU-Pro)에 효과적으로 일반화됩니다. 본 연구의 결과를 통해, 툴 통합이 sLMs의 자기확인 능력에 큰 향상을 초래할 수 있다는 가능성을 보여줍니다.",
      "upvotes": 20,
      "discussionId": "67f48f00463c4d1c4ae52882"
    },
    "publishedAt": "2025-04-07T00:01:17.000Z",
    "title": "T1: Tool-integrated Self-verification for Test-time Compute Scaling in\n  Small Language Models",
    "summary": "Recent studies have demonstrated that test-time compute scaling effectively\nimproves the performance of small language models (sLMs). However, prior\nresearch has mainly examined test-time compute scaling with an additional\nlarger model as a verifier, leaving self-verification by sLMs underexplored. In\nthis work, we investigate whether sLMs can reliably self-verify their outputs\nunder test-time scaling. We find that even with knowledge distillation from\nlarger verifiers, sLMs struggle with verification tasks requiring memorization,\nsuch as numerical calculations and fact-checking. To address this limitation,\nwe propose Tool-integrated self-verification (T1), which delegates\nmemorization-heavy verification steps to external tools, such as a code\ninterpreter. Our theoretical analysis shows that tool integration reduces\nmemorization demands and improves test-time scaling performance. Experiments on\nthe MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under\ntest-time scaling outperforms the significantly larger Llama-3.1 8B model.\nMoreover, T1 generalizes effectively to both mathematical (MATH500) and\nmulti-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the\npotential of tool integration to substantially improve the self-verification\nabilities of sLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04718.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b74920fe6a108d03fed767",
      "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
      "fullname": "Minki Kang",
      "name": "Nardien",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05299",
      "authors": [
        {
          "_id": "67f4cf5b504263bce1236d87",
          "user": {
            "_id": "65d66b494bbd0d92b641cdbb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
            "isPro": false,
            "fullname": "Andres Marafioti",
            "user": "andito",
            "type": "user"
          },
          "name": "Andrés Marafioti",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T08:46:33.366Z",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d88",
          "user": {
            "_id": "648c9605565e3a44f3c9bb7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648c9605565e3a44f3c9bb7b/W5chvk17Zol6-2QSWkFVR.jpeg",
            "isPro": true,
            "fullname": "Orr Zohar",
            "user": "orrzohar",
            "type": "user"
          },
          "name": "Orr Zohar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T09:11:26.498Z",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d89",
          "user": {
            "_id": "61ed0ff29539bc0a3bbc89f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61ed0ff29539bc0a3bbc89f4/iYWK7GParA7Ke5F6q132W.jpeg",
            "isPro": false,
            "fullname": "Miquel Farré",
            "user": "mfarre",
            "type": "user"
          },
          "name": "Miquel Farré",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T09:11:24.824Z",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8a",
          "name": "Merve Noyan",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8b",
          "name": "Elie Bakouch",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8c",
          "name": "Pedro Cuenca",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8d",
          "name": "Cyril Zakka",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8e",
          "user": {
            "_id": "61c141342aac764ce1654e43",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg",
            "isPro": false,
            "fullname": "Loubna Ben Allal",
            "user": "loubnabnl",
            "type": "user"
          },
          "name": "Loubna Ben Allal",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T09:11:22.957Z",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8f",
          "name": "Anton Lozhkov",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d90",
          "name": "Nouamane Tazi",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d91",
          "name": "Vaibhav Srivastav",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d92",
          "name": "Joshua Lochner",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d93",
          "name": "Hugo Larcher",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d94",
          "name": "Mathieu Morlon",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d95",
          "name": "Lewis Tunstall",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d96",
          "name": "Leandro von Werra",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d97",
          "name": "Thomas Wolf",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:58:57.000Z",
      "submittedOnDailyAt": "2025-04-08T06:10:52.675Z",
      "title": "SmolVLM: 작다면 효율적인 다모달 모델을 재 정의할 수 있는 방법\n\n(Note: The original text \"小さければもしかして効率的な多モーダルモデルを再定義する\" is a Japanese phrase. The translation provided above is an attempt to convey the meaning in Korean. However, the phrase \"小さければもしかして\" is a bit idiomatic and may not have a direct equivalent in Korean. If you have a specific context or a more precise translation in mind, please let me know.)",
      "submittedOnDailyBy": {
        "_id": "65d66b494bbd0d92b641cdbb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
        "isPro": false,
        "fullname": "Andres Marafioti",
        "user": "andito",
        "type": "user"
      },
      "summary": "대시각 언어 모델(VLMs)는 뛰어난 성능을 제공하지만, 계산 자원의 많은 필요로 인해 휴대 장치나 에지 디바이스에서 도입이 제한되어 있습니다. 작은 VLMs는 일반적으로 큰 모델의 설계 선택을 카드로 만들어, 광범위한 이미지 토큰화 작업을 포함하여, GPU 메모리의 효율적인 사용과 기기 상 애플리케이션의 실용성을 제한합니다.\n\n우리는 자원 효율적인 추론을 목표로 하는 압축을 위한 다형 모델 시리즈인 SmolVLM을 소개합니다. 우리는 저 계산 오버헤드에 최적화된 아키텍처의 설계, 토큰화 전략, 데이터 커리큘레이션에 의한 체계적인 탐색을 수행합니다. 이로 인해, 이미지 및 비디오 작업에서 성능의 큰 향상을 위해 중요한 설계 선택을 결정합니다.\n\n우리의 가장 작은 모델인 SmolVLM-256M은 추론 시 1GB 이하의 GPU 메모리를 사용하며, 18개월의 개발 간격을 초과하여 300배 큰 Idefics-80B 모델을 초과합니다. 우리의 가장 큰 모델은 22억 파라미터로, GPU 메모리의 2배 이상을 사용하는 최신 VLMs과 대결에서도 우수한 성능을 나타냅니다. SmolVLM 모델은 정적 이미지를 초과하여 강력한 비디오 이해 능력을 나타냅니다.\n\n우리의 결과를 통해, 전략적인 아키텍처의 최적화, 강력한 효율적인 토큰화, 더 나은 데이터의 더 신중한 선택에 의한 다양한 성능의 큰 향상을 보여주며, 실용적이고 에너지 효율적인 소규모 도입을 촉진합니다.",
      "upvotes": 17,
      "discussionId": "67f4cf5d504263bce1236dda",
      "projectPage": "https://huggingface.co/collections/HuggingFaceTB/smolvlm2-smallest-video-lm-ever-67ab6b5e84bf8aaa60cb17c7",
      "githubRepo": "https://github.com/huggingface/smollm"
    },
    "publishedAt": "2025-04-07T13:58:57.000Z",
    "title": "SmolVLM: Redefining small and efficient multimodal models",
    "summary": "Large Vision-Language Models (VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. Smaller VLMs typically mirror design choices of larger\nmodels, such as extensive image tokenization, leading to inefficient GPU memory\nusage and constrained practicality for on-device applications.\n  We introduce SmolVLM, a series of compact multimodal models specifically\nengineered for resource-efficient inference. We systematically explore\narchitectural configurations, tokenization strategies, and data curation\noptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during\ninference and outperforms the 300-times larger Idefics-80B model, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend\nbeyond static images, demonstrating robust video comprehension capabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal performance, facilitating practical, energy-efficient\ndeployments at significantly smaller scales.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05299.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d66b494bbd0d92b641cdbb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
      "fullname": "Andres Marafioti",
      "name": "andito",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 131
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05305",
      "authors": [
        {
          "_id": "67f495437e1624ebbaf2d90e",
          "user": {
            "_id": "64842d1edc475c315e41123a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64842d1edc475c315e41123a/Yqam_dy0MnQm4Rn1_1_4N.jpeg",
            "isPro": false,
            "fullname": "Sangbeom Lim",
            "user": "SammyLim",
            "type": "user"
          },
          "name": "Sangbeom Lim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:37.341Z",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d90f",
          "user": {
            "_id": "64c2c45ae818eec6128fdda3",
            "avatarUrl": "/avatars/d4399e25e6399345e263c7902789047e.svg",
            "isPro": false,
            "fullname": "Jun-Wan KIM",
            "user": "junwann",
            "type": "user"
          },
          "name": "Junwan Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:34.302Z",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d910",
          "name": "Heeji Yoon",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d911",
          "user": {
            "_id": "65d02dc017e2b305e0d7bf4f",
            "avatarUrl": "/avatars/2a50fd0541e7b0e200c577a661956696.svg",
            "isPro": false,
            "fullname": "Jaewoo Jung",
            "user": "crepejung00",
            "type": "user"
          },
          "name": "Jaewoo Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:32.321Z",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d912",
          "user": {
            "_id": "65cf717450818a335a1d3021",
            "avatarUrl": "/avatars/382a0e0f40f661cda1b2531e3e6ea2ee.svg",
            "isPro": false,
            "fullname": "Seungryong Kim",
            "user": "seungryong",
            "type": "user"
          },
          "name": "Seungryong Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:13:34.541Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:59:44.000Z",
      "submittedOnDailyAt": "2025-04-08T01:57:21.829Z",
      "title": "URECA: 다른 분야에서도 그 어느 곳에서든 댓글을 반환하세요.",
      "submittedOnDailyBy": {
        "_id": "64842d1edc475c315e41123a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64842d1edc475c315e41123a/Yqam_dy0MnQm4Rn1_1_4N.jpeg",
        "isPro": false,
        "fullname": "Sangbeom Lim",
        "user": "SammyLim",
        "type": "user"
      },
      "summary": "영역 수준의 캡티ング는 특정 이미지 영역에 자연어로 설명을 생성하고 그 특징을 강조하는 것을 목표로 합니다. 그러나 현재의 방법은 다양성이 있는 영역의 고유한 캡티ング을 생성하는 것이 어려워 실제적인 적용 가능성은 제한되어 있습니다. 이러한 문제를 해결하기 위해 우리는 영역 수준의 이해를 필요로 하는 점을 고려하여 URECA 데이터셋을 소개합니다. URECA 데이터셋은 기존의 데이터셋과 달리 다양한 물체, 부분, 배경 요소를 포함하여 영역과 캡티ング의 대응 관계를 일관된 방식으로 보장하기 위해 구축되었습니다. 이를 중심으로 단계별 데이터 커리큘티브 파이프라인을 구축하고 각 단계에서 영역 선택과 캡티ング 생성을 단계적으로 개선합니다. 이 파이프라인에서 각 단계에서 Multimodal Large Language Models (MLLMs)를 활용하여 고유한 컨텍스트를 가진 캡티ング을 생성하고 정확성과 문학적 다양성을 향상시킵니다. URECA 데이터셋을 기반으로 우리는 다양한 영역을 효과적으로 인코딩하기 위한 새로운 캡티ング 모델인 URECA를 소개합니다. URECA는 현재의 MLLMs에 간단하지만 영향력 있는 변경을 가며 공간적 특성을 유지하면서 세부 및 문학적으로 풍부한 영역 설명을 가능하게 합니다. 우리 접근 방식에서는 동적인 마스크 모델링과 고해상도 마스크 인코더를 도입하여 캡티ング의 고유성을 향상시킵니다. 실험은 URECA는 URECA 데이터셋에서 가장 先端의 성능을 달성하고 현재의 영역 수준의 캡티ング 벤치마크에 더 나은 확장성을 보여줍니다.",
      "upvotes": 16,
      "discussionId": "67f495477e1624ebbaf2da2f",
      "projectPage": "https://cvlab-kaist.github.io/URECA/",
      "githubRepo": "https://github.com/cvlab-kaist/URECA"
    },
    "publishedAt": "2025-04-07T13:59:44.000Z",
    "title": "URECA: Unique Region Caption Anything",
    "summary": "Region-level captioning aims to generate natural language descriptions for\nspecific image regions while highlighting their distinguishing features.\nHowever, existing methods struggle to produce unique captions across\nmulti-granularity, limiting their real-world applicability. To address the need\nfor detailed region-level understanding, we introduce URECA dataset, a\nlarge-scale dataset tailored for multi-granularity region captioning. Unlike\nprior datasets that focus primarily on salient objects, URECA dataset ensures a\nunique and consistent mapping between regions and captions by incorporating a\ndiverse set of objects, parts, and background elements. Central to this is a\nstage-wise data curation pipeline, where each stage incrementally refines\nregion selection and caption generation. By leveraging Multimodal Large\nLanguage Models (MLLMs) at each stage, our pipeline produces distinctive and\ncontextually grounded captions with improved accuracy and semantic diversity.\nBuilding upon this dataset, we present URECA, a novel captioning model designed\nto effectively encode multi-granularity regions. URECA maintains essential\nspatial properties such as position and shape through simple yet impactful\nmodifications to existing MLLMs, enabling fine-grained and semantically rich\nregion descriptions. Our approach introduces dynamic mask modeling and a\nhigh-resolution mask encoder to enhance caption uniqueness. Experiments show\nthat URECA achieves state-of-the-art performance on URECA dataset and\ngeneralizes well to existing region-level captioning benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05305.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64842d1edc475c315e41123a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64842d1edc475c315e41123a/Yqam_dy0MnQm4Rn1_1_4N.jpeg",
      "fullname": "Sangbeom Lim",
      "name": "SammyLim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.04823",
      "authors": [
        {
          "_id": "67f4a8ead83d88e30c450332",
          "user": {
            "_id": "6411c22dd52c57f628f7c331",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678885386428-noauth.jpeg",
            "isPro": false,
            "fullname": "ruikang liu",
            "user": "ruikangliu",
            "type": "user"
          },
          "name": "Ruikang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:24.004Z",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450333",
          "name": "Yuxuan Sun",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450334",
          "name": "Manyi Zhang",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450335",
          "name": "Haoli Bai",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450336",
          "name": "Xianzhi Yu",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450337",
          "name": "Tiezheng Yu",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450338",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450339",
          "name": "Lu Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T08:22:45.000Z",
      "submittedOnDailyAt": "2025-04-08T03:14:58.007Z",
      "title": "카ン마이시즈드헬츠링? 카ン마이시즈드리터스신티닝모듈의 실험적 연구",
      "submittedOnDailyBy": {
        "_id": "6411c22dd52c57f628f7c331",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678885386428-noauth.jpeg",
        "isPro": false,
        "fullname": "ruikang liu",
        "user": "ruikangliu",
        "type": "user"
      },
      "summary": "최근의 언어 모델의 발전은 복잡한 작업에서 뛰어난 성능을 보여주고 있지만, 장기적인 chain-of-thought reasoning 프로세스는 추론 오버헤드를 증가시킵니다. 양호화는 대규모 언어 모델의 추론 비용을 줄이기 위해 광범위하게 사용되고 있지만, 양호화 모델에 대한 영향은 연구가 부족했습니다. 본 연구에서는 처음의 체계적인 연구를 수행하고, DeepSeek-R1-Distilled Qwen과 LLaMA의 가족(1.5B부터 70B까지의 파라미터 수) 및 QwQ-32B를 평가했습니다. 연구 내용은 가중치, KV 캐시, 활성화의 양호화를 포함하며, 최신 알고리즘을 사용하여 다양한 비트 폭으로 수행되었습니다. 수학(AIME, MATH-500), 과학(GPQA), 프로그래밍(LiveCodeBench)의 이유 벤치마크에서 극한 평가가 수행되었습니다. 결과적으로 W8A8 또는 W4A16의 무손실 양호화가 가능한 것이 밝혀졌지만, 낮은 비트 폭은 정확도 위험을 일으키ます. 또한, 모델 크기, 모델의 원천, 작업의 난이도가 성능의 중요한 결정 요인으로 밝혀졌습니다. 예상하지 못한 것은 양호화된 모델은 출력 길이가 증가하지 않는 것을 알게 되었습니다. 또한, 모델 크기나 이유 스텝의 스케일링을 전략적으로 수행하여 성능을 향상시킬 수 있습니다. 모든 양호화된 모델과 코드는 https://github.com/ruikangliu/Quantized-Reasoning-Models에 공개되었습니다.",
      "upvotes": 12,
      "discussionId": "67f4a8efd83d88e30c45043c"
    },
    "publishedAt": "2025-04-07T04:22:45.000Z",
    "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
    "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04823.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6411c22dd52c57f628f7c331",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678885386428-noauth.jpeg",
      "fullname": "ruikang liu",
      "name": "ruikangliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02828",
      "authors": [
        {
          "_id": "67f01efe81f4f7a1b43f4930",
          "user": {
            "_id": "6279a4f6812ee439d9c72d3f",
            "avatarUrl": "/avatars/a35a5674d1168d345d9fc5018485283e.svg",
            "isPro": false,
            "fullname": "Jinqi Luo",
            "user": "peterljq",
            "type": "user"
          },
          "name": "Jinqi Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-06T08:11:28.131Z",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4931",
          "name": "Tianjiao Ding",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4932",
          "user": {
            "_id": "6627043551cedbbb0b352047",
            "avatarUrl": "/avatars/8c4f80a24f8ba8761d33692c4ed28c29.svg",
            "isPro": false,
            "fullname": "Kwan Ho Ryan Chan",
            "user": "ryanckh",
            "type": "user"
          },
          "name": "Kwan Ho Ryan Chan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:13:58.769Z",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4933",
          "name": "Hancheng Min",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4934",
          "name": "Chris Callison-Burch",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4935",
          "name": "René Vidal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:59:58.000Z",
      "submittedOnDailyAt": "2025-04-08T02:59:02.267Z",
      "title": "Lancet: 조합 표현을 이용한 이미지 편집\n트랜스퓸\n\n(注意：虽然要求不添加额外的文本，但为了确保翻译的准确性和专业性，我在这里提供了一个更自然的韩语表达方式。如果需要完全按照原文的格式，可以直接使用上述翻译。)",
      "submittedOnDailyBy": {
        "_id": "6279a4f6812ee439d9c72d3f",
        "avatarUrl": "/avatars/a35a5674d1168d345d9fc5018485283e.svg",
        "isPro": false,
        "fullname": "Jinqi Luo",
        "user": "peterljq",
        "type": "user"
      },
      "summary": "디피유젼 모둔은 이미지 편집 템스크 템에 광범위하게 사용되고 있습니다. 현재의 편집 방법들은 텍스트 삽입 공간이나 스코어 공간에서 편집 방향을 선택하고 표현 조작 순서를 설계하고 있습니다. 그러나 이 순서에는 중요한 문제점이 있습니다: 편집의 강도를 과도하게 평가하면 시각적 일관성을 해칠 수 있지만, 평가가 부족하면 편집 템스크 템을 달성할 수 없습니다. 특히, 소스 이미지는 서로 다른 편집 강도를 필요로 하며, 적절한 강도를 찾기 위해 시도와 실패를 반복하는 것은 비용적으로 비효율적입니다. 이러한 문제를 해결하기 위해, 우리는 Concept Lancet (CoLan)을 제안합니다. CoLan은 디피유젼 기반의 이미지 편집에서 원칙적인 표현 조작을 수행하기 위한 0 세트 플러그인 플레임 프레임워크입니다. 추론 시에는 소스 입력을 잠재 공간 (텍스트 삽입 공간이나 디피유젼 스코어 공간)에서 희소한 선형 결합으로 분해합니다. 이로 인해 각 이미지에 개념이 존재하는지를 정확하게 평가할 수 있으며, 이는 편집에 도움이 됩니다. 편집 템스크 템 (교체/추가/제거)에 기반하여, 사용자 정의된 개념 전파 프로세스를 수행하고 대응하는 편집 방향을 부여합니다. 개념 공간을 충분히 모델화하기 위해, 우리는 CoLan-150K의 개념적 표현 데이터 세트를 제작합니다. 이 데이터 세트는 잠재 사전의 라벨 디렉토리에서 다양한 시각적 용어와 문구의 설명과 시나리오를 포함합니다. 여러 디피유젼 기반의 이미지 편집 기반 라인의 실험에서, CoLan을 적용한 방법은 편집의 효과성과 일관성 보존에 있어서 가장 先端의 성능을 달성했습니다.",
      "upvotes": 11,
      "discussionId": "67f01eff81f4f7a1b43f4971",
      "projectPage": "https://peterljq.github.io/project/colan",
      "githubRepo": "https://github.com/peterljq/Concept-Lancet",
      "ai_keywords": [
        "diffusion models",
        "image editing",
        "text embedding",
        "score space",
        "latent space",
        "sparse linear combination",
        "visual concepts",
        "concept transplantation",
        "conceptual representation dataset",
        "CoLan-150K",
        "CoLan"
      ]
    },
    "publishedAt": "2025-04-03T13:59:58.000Z",
    "title": "Concept Lancet: Image Editing with Compositional Representation\n  Transplant",
    "summary": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02828.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6279a4f6812ee439d9c72d3f",
      "avatarUrl": "/avatars/a35a5674d1168d345d9fc5018485283e.svg",
      "fullname": "Jinqi Luo",
      "name": "peterljq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05288",
      "authors": [
        {
          "_id": "67f4adf70864398533373364",
          "name": "Mingyang Fu",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373365",
          "name": "Yuyang Peng",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373366",
          "name": "Benlin Liu",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373367",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373368",
          "name": "Dongping Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:39:31.000Z",
      "submittedOnDailyAt": "2025-04-08T03:33:51.436Z",
      "title": "LiveVQA: 라이브비지컬 지식탐구",
      "submittedOnDailyBy": {
        "_id": "643be8879f5d314db2d9ed23",
        "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
        "isPro": false,
        "fullname": "Chen Dongping",
        "user": "shuaishuaicdp",
        "type": "user"
      },
      "summary": "LiveVQA는 인터넷에서 최신의 시각적 지식을 자동으로 수집하여 합성된 VQA 문제들을 포함하는 데이터 세트입니다. LiveVQA는 6개의 뉴스 웹 사이트에서 14개의 뉴스 카테고리로부터 3,602건의 단일 및 복수 단계의 시각적 질문으로 구성되어 있습니다. 고품질의 이미지-텍스트의 일치성과 진실성 있는 정보를 특징으로 하고 있습니다. 15개의 MLLM(예: GPT-4o, Gemma-3, Qwen-2.5-VL familiy)의 평가에 따라 강화된 모델이 전반적으로 더 좋은 성능을 보여주고, 복잡한 복수 단계의 질문에서 중요한 시각적 추론 능력이 나타났습니다. 맥락적인 문제를 능숙한 모델은 최신 시각적 지식을 필요로 하는 시각적 질문에 대해 타겟 엔진이나 다른 도구를 사용했을 때 큰 오류를 남기는 것을 명확히 보여주고, 미래의 연구의 중요한 분야를 밝혀줍니다.",
      "upvotes": 8,
      "discussionId": "67f4adfb086439853337346e"
    },
    "publishedAt": "2025-04-07T13:39:31.000Z",
    "title": "LiveVQA: Live Visual Knowledge Seeking",
    "summary": "We introduce LiveVQA, an automatically collected dataset of latest visual\nknowledge from the Internet with synthesized VQA problems. LiveVQA consists of\n3,602 single- and multi-hop visual questions from 6 news websites across 14\nnews categories, featuring high-quality image-text coherence and authentic\ninformation. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and\nQwen-2.5-VL family) demonstrates that stronger models perform better overall,\nwith advanced visual reasoning capabilities proving crucial for complex\nmulti-hop questions. Despite excellent performance on textual problems, models\nwith tools like search engines still show significant gaps when addressing\nvisual questions requiring latest visual knowledge, highlighting important\nareas for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05288.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.04715",
      "authors": [
        {
          "_id": "67f4a59bf51362f606eb84e8",
          "name": "Will Cai",
          "hidden": false
        },
        {
          "_id": "67f4a59bf51362f606eb84e9",
          "name": "Tianneng Shi",
          "hidden": false
        },
        {
          "_id": "67f4a59bf51362f606eb84ea",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "67f4a59bf51362f606eb84eb",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T03:57:41.000Z",
      "submittedOnDailyAt": "2025-04-08T02:57:43.185Z",
      "title": "돈을 지불하여 받을 수 있는 것이 있을까? LLM API에서 모델 교체의 모니터링",
      "submittedOnDailyBy": {
        "_id": "6275a465597c70eb8949fce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
        "isPro": false,
        "fullname": "Xuandong Zhao",
        "user": "Xuandong",
        "type": "user"
      },
      "summary": "LLM API를 사용하여 개발된 대규모 언어 모델(LLM)의 확산은 신뢰성 문제를 크게 초래합니다: 사용자는 광고된 모델의 기능(예: 크기, 성능)에 따라 서비스를 지불하지만, 제공원은 운영 비용 감소를 위해 지정된 모델을 가벼운, 품질이 낮은 대체 모델로 변경할 수 있습니다. 이러한 불투명성은 공정성을 파괴하고 신뢰성을 줄이고 신뢰할 수 있는 벤치마크를 복잡하게 만듭니다. 이러한 대체를 감지하는 것은 블랙박스의 특성에 따라 어렵지만, 일반적으로 입력 출력 쿼리와만 상호작용이 제한됩니다. 본 논문에서는 LLM API에서 모델 대체 감지 문제를 형식화합니다. 실제적인 공격 시나리오(예: 모델의 훈련, 무작위 대체, 벤치마크의 회避)의 背景에서, 현재의 검증 방법(출력기 기반 통계 테스트, 벤치마크 평가, 로그 확률 분석)을 체계적으로 평가합니다. 우리의 발견은 문의 출력에만 의존하는 방법의 한계를 밝혀내며, 특히 미묘한 움직임의 공격에 대한 한계를 보여줍니다. 로그 확률 분석은 이러한 경우 강력한 보장을 제공하지만, 일반적으로 접근성은 제한되어 있습니다. 최종적으로 Trusted Execution Environments(TEEs)와 같은 하드웨어 기반의 해결책의 가능성에 대해 논의하고, 보안성, 성능, 제공원의 수용의 트레이드오프를 명확히 합니다. 코드는 https://github.com/sunblaze-ucb/llm-api-audit에 있습니다.",
      "upvotes": 3,
      "discussionId": "67f4a59cf51362f606eb8529",
      "githubRepo": "https://github.com/sunblaze-ucb/llm-api-audit"
    },
    "publishedAt": "2025-04-06T23:57:41.000Z",
    "title": "Are You Getting What You Pay For? Auditing Model Substitution in LLM\n  APIs",
    "summary": "The proliferation of Large Language Models (LLMs) accessed via black-box APIs\nintroduces a significant trust challenge: users pay for services based on\nadvertised model capabilities (e.g., size, performance), but providers may\ncovertly substitute the specified model with a cheaper, lower-quality\nalternative to reduce operational costs. This lack of transparency undermines\nfairness, erodes trust, and complicates reliable benchmarking. Detecting such\nsubstitutions is difficult due to the black-box nature, typically limiting\ninteraction to input-output queries. This paper formalizes the problem of model\nsubstitution detection in LLM APIs. We systematically evaluate existing\nverification techniques, including output-based statistical tests, benchmark\nevaluations, and log probability analysis, under various realistic attack\nscenarios like model quantization, randomized substitution, and benchmark\nevasion. Our findings reveal the limitations of methods relying solely on text\noutputs, especially against subtle or adaptive attacks. While log probability\nanalysis offers stronger guarantees when available, its accessibility is often\nlimited. We conclude by discussing the potential of hardware-based solutions\nlike Trusted Execution Environments (TEEs) as a pathway towards provable model\nintegrity, highlighting the trade-offs between security, performance, and\nprovider adoption. Code is available at\nhttps://github.com/sunblaze-ucb/llm-api-audit",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04715.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6275a465597c70eb8949fce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
      "fullname": "Xuandong Zhao",
      "name": "Xuandong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.05304",
      "authors": [
        {
          "_id": "67f48c11412c65a9d4e3e552",
          "user": {
            "_id": "638067fcb334960c987fbeda",
            "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
            "isPro": false,
            "fullname": "Hansheng Chen",
            "user": "Lakonik",
            "type": "user"
          },
          "name": "Hansheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:39.675Z",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e553",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e554",
          "name": "Hao Tan",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e555",
          "name": "Zexiang Xu",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e556",
          "name": "Fujun Luan",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e557",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e558",
          "name": "Gordon Wetzstein",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e559",
          "name": "Sai Bi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:59:42.000Z",
      "submittedOnDailyAt": "2025-04-08T01:13:17.992Z",
      "title": "Gaussian Mixture Flow Model",
      "submittedOnDailyBy": {
        "_id": "638067fcb334960c987fbeda",
        "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
        "isPro": false,
        "fullname": "Hansheng Chen",
        "user": "Lakonik",
        "type": "user"
      },
      "summary": "Diffusionモデル는 노이즈의 분포를 가우시안 분포와 근사하여 평균을 예측합니다. 반면, flow matchingモデル는 가우시안의 평균을 유동 속도로 재설정합니다. 그러나 이러한 모델들은 이산화 오류로 인해 적은 단계에서 샘플링할 때 성능이 떨어지고, 클래스 분류기 자유의 가이드(CFG)를 위해 오버샘플링된 색상을 생성하는 경향이 있습니다. 이러한 제한에 대처하기 위해 우리는 새로운 가우시안 혼합 유동 매칭(GMFlow)モデル를 제안합니다: GMFlow는 평균을 예측하는 것이 아니라 동적인 가우시안 혼합(GM) 파라미터를 예측하여 다모달한 유동 속도 분포를捉えます. 이는 KL 분산 손실을 사용하여 학습할 수 있습니다. GMFlow는 L_2 노이즈 제거 손실을 사용하여 학습된 단일 가우시안을 학습한 이전의 Diffusionモデル과 flow matchingモデル를 일반화합니다. 추론 시에는 분석적인 노이즈 분포와 속도장을 사용하여 GM-SDE/ODE 솔버를 제안하고, 적은 단계에서 정확한 샘플링을 수행할 수 있습니다. 또한 새로운 확률적인 가이드 기법을 도입하여 CFG의 오버샘플링 문제를 완화시키고, 이미지 생성의 질을 향상시킵니다. 확장된 실험에 따라, GMFlow는 ImageNet 256×256에서 6단계만으로 Precision 0.942를 달성하고, 생성 품질에서 flow matching 기반 라인에 대해 일관되게 우수합니다.",
      "upvotes": 2,
      "discussionId": "67f48c13412c65a9d4e3e5d7",
      "githubRepo": "https://github.com/Lakonik/GMFlow"
    },
    "publishedAt": "2025-04-07T13:59:42.000Z",
    "title": "Gaussian Mixture Flow Matching Models",
    "summary": "Diffusion models approximate the denoising distribution as a Gaussian and\npredict its mean, whereas flow matching models reparameterize the Gaussian mean\nas flow velocity. However, they underperform in few-step sampling due to\ndiscretization error and tend to produce over-saturated colors under\nclassifier-free guidance (CFG). To address these limitations, we propose a\nnovel Gaussian mixture flow matching (GMFlow) model: instead of predicting the\nmean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a\nmulti-modal flow velocity distribution, which can be learned with a KL\ndivergence loss. We demonstrate that GMFlow generalizes previous diffusion and\nflow matching models where a single Gaussian is learned with an L_2 denoising\nloss. For inference, we derive GM-SDE/ODE solvers that leverage analytic\ndenoising distributions and velocity fields for precise few-step sampling.\nFurthermore, we introduce a novel probabilistic guidance scheme that mitigates\nthe over-saturation issues of CFG and improves image generation quality.\nExtensive experiments demonstrate that GMFlow consistently outperforms flow\nmatching baselines in generation quality, achieving a Precision of 0.942 with\nonly 6 sampling steps on ImageNet 256times256.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05304.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638067fcb334960c987fbeda",
      "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
      "fullname": "Hansheng Chen",
      "name": "Lakonik",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.03193",
      "authors": [
        {
          "_id": "67f3f25019592b36b6d8b30c",
          "user": {
            "_id": "6436290e76dbfd731bcf1f55",
            "avatarUrl": "/avatars/63af84ca382d799c4a000594c994d4bb.svg",
            "isPro": false,
            "fullname": "Xin Zhang",
            "user": "XinNUS",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:52:23.735Z",
          "hidden": false
        },
        {
          "_id": "67f3f25019592b36b6d8b30d",
          "name": "Robby T. Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T05:44:45.000Z",
      "submittedOnDailyAt": "2025-04-08T06:12:20.759Z",
      "title": "맨다바르 버전： 비지션 fundamenttion 모델과 비지션 언어 모델의 교차점에서, 도메인 일반화 세마antic 세그먼테이션 모델을 검토합니다.",
      "submittedOnDailyBy": {
        "_id": "6436290e76dbfd731bcf1f55",
        "avatarUrl": "/avatars/63af84ca382d799c4a000594c994d4bb.svg",
        "isPro": false,
        "fullname": "Xin Zhang",
        "user": "XinNUS",
        "type": "user"
      },
      "summary": "Vision Foundation Models (VFMs)와 Vision-Language Models (VLMs)는 강력한 일반화 능력을 가지고 있으며, 광범위한 도메인에서 표식 분리(DGSS)에 주목을 받고 있습니다. 그러나 현재의 DGSS手法는 VFMs나 VLMs의 보완적인 강점을 무시하고 있습니다. VFMs(예: DINOv2)은 미세한 특징을 인식하는 우수한 성능을 보여주고 있으며, VLMs(예: CLIP)은 강력한 문서 alignment을 제공하지만, 거대한 규모의 처리에 어려움을 겪습니다. VFMs와 VLMs의 보완적인 강점은 있지만, 注意机构을 사용하여 효과적으로 통합하는 것은 어렵습니다. 토큰의 증가는 긴 시퀀스 모델링을 복잡하게 만듭니다. 이에 대해 우리는 선형 확장성을 유지하면서 VFMs와 VLMs의 강점을 효율적으로 통합하는 새로운 Mamba 기반의 결합 프레임워크인 MFuser를 제안합니다. MFuser는 MVFuser와 MTEnhancer의 두 가지 핵심 구성 요소로 구성되어 있으며, 두 모델을 동시에 미세 조정하고, 순서적 및 공간적인 동작을 모두捉捉하는 공동 적용기입니다. 또한 MTEnhancer는 이미지 프로파일을 감싸는 통합적 attention-Mamba 모듈로, 문서埋め込み를 정밀하게 합니다. 우리의 접근법은 계산량의 추가가 크게 되지만, 정밀한 특징의 지역성과 강력한 문서 alignment을 실현합니다. 확장된 실험은 MFuser가 가장 先端의 DGSS手法를 크게 초월하고, 합성-실제 및 실제-실제 벤치마크에서 각각 68.20 mIoU와 71.87 mIoU를 달성했습니다. 코드는 https://github.com/devinxzhang/MFuser에 공개되어 있습니다.",
      "upvotes": 2,
      "discussionId": "67f3f25219592b36b6d8b3d0",
      "projectPage": "https://devinxzhang.github.io/MFuser_ProjPage/",
      "githubRepo": "https://github.com/devinxzhang/MFuser"
    },
    "publishedAt": "2025-04-04T01:44:45.000Z",
    "title": "Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language\n  Models for Domain-Generalized Semantic Segmentation",
    "summary": "Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained\ntraction in Domain Generalized Semantic Segmentation (DGSS) due to their strong\ngeneralization capabilities. However, existing DGSS methods often rely\nexclusively on either VFMs or VLMs, overlooking their complementary strengths.\nVFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g.,\nCLIP) provide robust text alignment but struggle with coarse granularity.\nDespite their complementary strengths, effectively integrating VFMs and VLMs\nwith attention mechanisms is challenging, as the increased patch tokens\ncomplicate long-sequence modeling. To address this, we propose MFuser, a novel\nMamba-based fusion framework that efficiently combines the strengths of VFMs\nand VLMs while maintaining linear scalability in sequence length. MFuser\nconsists of two key components: MVFuser, which acts as a co-adapter to jointly\nfine-tune the two models by capturing both sequential and spatial dynamics; and\nMTEnhancer, a hybrid attention-Mamba module that refines text embeddings by\nincorporating image priors. Our approach achieves precise feature locality and\nstrong text alignment without incurring significant computational overhead.\nExtensive experiments demonstrate that MFuser significantly outperforms\nstate-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and\n71.87 mIoU on real-to-real benchmarks. The code is available at\nhttps://github.com/devinxzhang/MFuser.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03193.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6436290e76dbfd731bcf1f55",
      "avatarUrl": "/avatars/63af84ca382d799c4a000594c994d4bb.svg",
      "fullname": "Xin Zhang",
      "name": "XinNUS",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02812",
      "authors": [
        {
          "_id": "67f4b54ddf6757586b52a4eb",
          "name": "Van Nguyen Nguyen",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ec",
          "name": "Stephen Tyree",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ed",
          "name": "Andrew Guo",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ee",
          "name": "Mederic Fourmy",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ef",
          "name": "Anas Gouda",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f0",
          "name": "Taeyeop Lee",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f1",
          "name": "Sungphill Moon",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f2",
          "name": "Hyeontae Son",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f3",
          "name": "Lukas Ranftl",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f4",
          "name": "Jonathan Tremblay",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f5",
          "name": "Eric Brachmann",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f6",
          "name": "Bertram Drost",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f7",
          "name": "Vincent Lepetit",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f8",
          "name": "Carsten Rother",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f9",
          "name": "Stan Birchfield",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fa",
          "name": "Jiri Matas",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fb",
          "name": "Yann Labbe",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fc",
          "name": "Martin Sundermeyer",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fd",
          "name": "Tomas Hodan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:55:19.000Z",
      "submittedOnDailyAt": "2025-04-08T04:04:46.852Z",
      "title": "BOP 2024 모델 기반 및 모델 자유 6D 물체 자세 추정\n\n(Note: The translation is provided as requested, adhering to the specified format without additional explanations.)",
      "submittedOnDailyBy": {
        "_id": "67400e2bf2d0992e1f373b9c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lysHv-F1aIjZXPaApGS8k.png",
        "isPro": false,
        "fullname": "hugw",
        "user": "hugw",
        "type": "user"
      },
      "summary": "BOP チャレンジ 2024의 평가방법, 데이터셋 및 결과를 소개합니다. 이 대회는 6D 객체 포즈 추정 및 관련된 최신 기술에 대한 공개 컴퍼티션으로 6번째 대회입니다. 2024년에는 BOP을 실제 세계적인 시나리오로 전환하는 것을 목표로 합니다.\n\n먼저, 모델 없이 태스크를 도입했습니다. 이 태스크에서 3D 객체 모델이 사용할 수 없는 상황에서, 방법들은 제공된 참조 비디오에서 객체를 온보드에 배치하는 데 필요합니다. 다음으로, 6D 객체 검출 태스크를 새로 정의하고, 테스트 이미지에 보이는 객체의 인식 정보를 입력으로 제공하지 않도록 했습니다. 또한, 고해상도 센서와 AR/VR 헤드셋을 사용하여 새로운 BOP-H3 데이터셋을 도입하고, 실제 세계적인 시나리오에 가까운 것을 제공했습니다. BOP-H3는 모델 기반 및 모델 없이 태스크를 지원하기 위해 3D 모델과 온보드 비디오를 포함합니다.\n\n참가자들은 7개의 도전 트랙에서 경쟁했습니다. 이 트랙들은 태스크, 온보드 세트, 데이터셋 그룹에 따라 정의되어 있습니다. 특히, 2024년에 최적의 모델 기반 6D 위치 결정 방법 (FreeZeV2.1)은 2023년에 최적의 방법 (GenFlow)에 비해 BOP-Classic-Core에서 22%의 정확도 상승을 보였고, 2023년에 최적의 방법 (GPose2023)에 비해 4% 떨어졌지만, 대폭 느리게 (1 이미지 당 24.9초) 수행됩니다. 이 태스크에 대한 실용적인 2024년 방법은 Co-op입니다. Co-op는 1 이미지 당 0.8초로 실행되며, GenFlow보다 25배 빠르고, 13%의 정확도 상승을 보입니다.\n\n방법은 6D 검출과 6D 위치 결정과 같은 순위를 보여주지만, 실행 시간이 길다는 점에 유의해야 합니다. 모델 기반의 2D 검출에서, 2024년에 최적의 방법 (MUSE)은 2023년에 최적의 방법 (CNOS)에 비해 21%의 상대적인 개선을 보였습니다. 그러나未见의 객체의 2D 검출 정확도는 GDet2023에 비해 明顯に 낮은 (-53%) 것을 인지합니다. 온라인 평가 시스템은 개방되어 있으며, http://bop.felk.cvut.cz/에서 사용 가능합니다.",
      "upvotes": 2,
      "discussionId": "67f4b552df6757586b52a613"
    },
    "publishedAt": "2025-04-03T13:55:19.000Z",
    "title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose\n  Estimation",
    "summary": "We present the evaluation methodology, datasets and results of the BOP\nChallenge 2024, the sixth in a series of public competitions organized to\ncapture the state of the art in 6D object pose estimation and related tasks. In\n2024, our goal was to transition BOP from lab-like setups to real-world\nscenarios. First, we introduced new model-free tasks, where no 3D object models\nare available and methods need to onboard objects just from provided reference\nvideos. Second, we defined a new, more practical 6D object detection task where\nidentities of objects visible in a test image are not provided as input. Third,\nwe introduced new BOP-H3 datasets recorded with high-resolution sensors and\nAR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D\nmodels and onboarding videos to support both model-based and model-free tasks.\nParticipants competed on seven challenge tracks, each defined by a task, object\nonboarding setup, and dataset group. Notably, the best 2024 method for\nmodel-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher\naccuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only\n4% behind the best 2023 method for seen objects (GPose2023) although being\nsignificantly slower (24.9 vs 2.7s per image). A more practical 2024 method for\nthis task is Co-op which takes only 0.8s per image and is 25X faster and 13%\nmore accurate than GenFlow. Methods have a similar ranking on 6D detection as\non 6D localization but higher run time. On model-based 2D detection of unseen\nobjects, the best 2024 method (MUSE) achieves 21% relative improvement compared\nto the best 2023 method (CNOS). However, the 2D detection accuracy for unseen\nobjects is still noticealy (-53%) behind the accuracy for seen objects\n(GDet2023). The online evaluation system stays open and is available at\nhttp://bop.felk.cvut.cz/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02812.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67400e2bf2d0992e1f373b9c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lysHv-F1aIjZXPaApGS8k.png",
      "fullname": "hugw",
      "name": "hugw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.02882",
      "authors": [
        {
          "_id": "67f4d15a8f00d281c155880f",
          "name": "Sunghee Jung",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558810",
          "name": "Donghun Lee",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558811",
          "name": "Shinbok Lee",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558812",
          "name": "Gaeun Seo",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558813",
          "name": "Daniel Lee",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558814",
          "name": "Byeongil Ko",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558815",
          "name": "Junrae Cho",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558816",
          "name": "Kihyun Kim",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558817",
          "name": "Eunggyun Kim",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558818",
          "name": "Myeongcheol Shin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T05:47:28.000Z",
      "submittedOnDailyAt": "2025-04-08T06:11:32.041Z",
      "title": "DiaTool-DPO: 툴付속의 대규모 언어 모델의 다턴 직접 취미 최적화",
      "submittedOnDailyBy": {
        "_id": "65e30342e8b017ee1384824c",
        "avatarUrl": "/avatars/e5d07b037f611ccfaf719959d971d102.svg",
        "isPro": false,
        "fullname": "Sunghee Jung",
        "user": "hash2430",
        "type": "user"
      },
      "summary": "Tool-Augmented Large Language Models (TA-LLMs)는 현실적인 응용에 좋은 결과를 보여주지만, 불완전한 요청과 범위 외의 요청 처리에 문제가 있습니다. 기존의 접근 방식은 주로 전문가 프로젝트를 기반으로 한 여러 개의 지도 학습으로 이루어져 있으며, 여기에서는 Direct Preference Optimization을 사용한 새로운 방법인 DiaTool-DPO를 제안합니다. TA-LLM의 인터페이스를 5가지의 다른 대화 상태로马尔코프 인코딩 프로세스를 통해 구성하고, 대화 상태의 이동 경로에 따라 사용자 요청을 3가지의 종류로 분류합니다. 올바른 대화 흐름의 쌍의 프로젝트 데이터 세트를 자동적으로 구축하고, 대화 제어의 특화된 목적 함수 손실을 도입합니다. 세부적인 평가에 따르면 DiaTool-DPO는 GPT-4o의 성능을 근사하며 (정보 수집: 94.8%, 도구 호출 거부: 91%) 기준과 비교하여 큰 개선이 보입니다 (44% vs 9.6%의 값) 그리고 핵심 기능은 유지됩니다. 이 접근 방식은 추가의 전문가의 시뮬레이션이나 인간 레이블링이 필요하지 않는 다양한 현실적인 시나리오를 다루는 TA-LLM의 개발에 새로운 가능성을 열어줍니다.",
      "upvotes": 2,
      "discussionId": "67f4d15c8f00d281c1558870"
    },
    "publishedAt": "2025-04-02T01:47:28.000Z",
    "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models",
    "summary": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02882.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e30342e8b017ee1384824c",
      "avatarUrl": "/avatars/e5d07b037f611ccfaf719959d971d102.svg",
      "fullname": "Sunghee Jung",
      "name": "hash2430",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03964",
      "authors": [
        {
          "_id": "67f4889c7e1624ebbaef710d",
          "user": {
            "_id": "652ee41e7b0079ff035b2269",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ee41e7b0079ff035b2269/jv-2-UPHN_e9Lhwl3Evwh.jpeg",
            "isPro": false,
            "fullname": "Simon Lee",
            "user": "Simonlee711",
            "type": "user"
          },
          "name": "Simon A. Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:44.271Z",
          "hidden": false
        },
        {
          "_id": "67f4889c7e1624ebbaef710e",
          "name": "Anthony Wu",
          "hidden": false
        },
        {
          "_id": "67f4889c7e1624ebbaef710f",
          "name": "Jeffrey N. Chiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T22:14:12.000Z",
      "submittedOnDailyAt": "2025-04-08T05:40:27.532Z",
      "title": "クリニカル・モダン・BERT: 생물의학문서의 효율적인 긴 문맥 인코더",
      "submittedOnDailyBy": {
        "_id": "652ee41e7b0079ff035b2269",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ee41e7b0079ff035b2269/jv-2-UPHN_e9Lhwl3Evwh.jpeg",
        "isPro": false,
        "fullname": "Simon Lee",
        "user": "Simonlee711",
        "type": "user"
      },
      "summary": "クリニカル・モダン・BERT를 소개합니다. 이 모델은 대규모의 생물의학문헌, 클린이컬 기록, 의료 온토로지에 기반하여, PubMed의 요약, MIMIC IV의 클린이컬 데이터, 그리고 의료 코드와 그 맥락적 설명을 통합한 Transformer 기반의 인코더입니다. 현재의 가장 先端한 자연어 문맥 인코더인 ModernBERT를 기반으로, 회전 위치 삽입(RoPE), Flash Attention, 8,192 토큰의 확장 컨텍스트 길이 등 다양한 아키텍처 업그레이드들을 도입하여, 특히 생물의학과 클린이컬 분야에 적합한 혁신을 적용하고 있습니다.クリニカル・モダン・BERT는 긴 문맥을 적응할 수 있는 의미적으로 풍부한 표현을 생성하는 장점이 있습니다. 이를 증명하기 위해, 학습된 가중치 분석과 클린이컬 NLP 벤치마크의 실험적 평가들을 수행합니다.",
      "upvotes": 1,
      "discussionId": "67f4889d7e1624ebbaef715a",
      "githubRepo": "https://github.com/Simonlee711/Clinical_ModernBERT"
    },
    "publishedAt": "2025-04-04T18:14:12.000Z",
    "title": "Clinical ModernBERT: An efficient and long context encoder for\n  biomedical text",
    "summary": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on\nlarge scale biomedical literature, clinical notes, and medical ontologies,\nincorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with\ntheir textual descriptions. Building on ModernBERT the current state of the art\nnatural language text encoder featuring architectural upgrades such as rotary\npositional embeddings (RoPE), Flash Attention, and extended context length up\nto 8,192 tokens our model adapts these innovations specifically for biomedical\nand clinical domains. Clinical ModernBERT excels at producing semantically rich\nrepresentations tailored for long context tasks. We validate this both by\nanalyzing its pretrained weights and through empirical evaluation on a\ncomprehensive suite of clinical NLP benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03964.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652ee41e7b0079ff035b2269",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ee41e7b0079ff035b2269/jv-2-UPHN_e9Lhwl3Evwh.jpeg",
      "fullname": "Simon Lee",
      "name": "Simonlee711",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.03770",
      "authors": [
        {
          "_id": "67f4a80a261cb1c328d9b0a2",
          "name": "Yi Nian",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a3",
          "user": {
            "_id": "6614243f67d7bfc73afc6b77",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614243f67d7bfc73afc6b77/ifdjAb_BPraEJ2eIySk_K.jpeg",
            "isPro": false,
            "fullname": "Shenzhe Zhu",
            "user": "Chouoftears",
            "type": "user"
          },
          "name": "Shenzhe Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:27.601Z",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a4",
          "name": "Yuehan Qin",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a5",
          "name": "Li Li",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a6",
          "name": "Ziyi Wang",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a7",
          "name": "Chaowei Xiao",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a8",
          "name": "Yue Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6614243f67d7bfc73afc6b77/z0oklTQ627SEQN42jYoat.png"
      ],
      "publishedAt": "2025-04-03T05:00:28.000Z",
      "submittedOnDailyAt": "2025-04-08T03:10:00.355Z",
      "title": "JailDAM: Vision-Language Model의 Adaptive Memory에 의한 Jail Break Detection",
      "submittedOnDailyBy": {
        "_id": "6614243f67d7bfc73afc6b77",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614243f67d7bfc73afc6b77/ifdjAb_BPraEJ2eIySk_K.jpeg",
        "isPro": false,
        "fullname": "Shenzhe Zhu",
        "user": "Chouoftears",
        "type": "user"
      },
      "summary": "다모달 대언어 모댄스(MLLM)는 시각 언어 태스크에서 뛰어난 성능을 보입니다が, 특히 젓가락 브레이크 공격으로 인한 유해 콘텐츠 생성에 있어 중대한 위험이 존재합니다. 젓가락 브레이크 공격은 모델의 안전 기능에 대한 의도적인 조작으로, 적절하지 않거나 불안정한 콘텐츠의 생성으로 위험이 높아집니다. 이러한 공격을 감지하는 것은 MLLM의 책임적 도입을 보장하는 데 중요합니다. 현재의 젓가락 브레이크 감지 방법은 3가지 주요 문제를 직면하고 있습니다: (1) 대부분 모델의 은닉 상태나 경사를 기반으로, 白箱 모델에만 적용할 수 있으며, 모델의 내부 기능을 접근할 수 있는;(2) 확률 기반 분석에서 높은 계산 오버헤드로, 실시간 감지를 제한합니다;(3) 완전하게 라벨링 된 유해 데이터 셋은 현실적인 환경에서 희귀합니다. 이러한 문제를 대처하기 위해, 우리는 테스트 시 적용 가능한 프레임워크「JAILDAM」를 사용합니다. 우리의 방법은 정책 주도의 불안정한 지식 표현을 지도하는 메모리 기반의 접근 방식을 사용하며, 유해 데이터에 직접접촉을 제거합니다. 테스트 시 불안정한 지식이 동적으로 업데이트되는 방식에 의해, 우리의 프레임워크는未见의 젓가락 브레이크 스테이지에 대한 일반화를 개선하고, 동시에 효율을 유지합니다. 많은 VLM 젓가락 브레이크 벤치마크에서의 실험은, JAILDAM이 유해 콘텐츠 감지에 가장 선진적인 성능을 제공하며, 정확성과 속도를 모두 향상시키는 것을 보여줍니다.",
      "upvotes": 1,
      "discussionId": "67f4a80b261cb1c328d9b0e9",
      "githubRepo": "https://github.com/ShenzheZhu/JailDAM"
    },
    "publishedAt": "2025-04-03T01:00:28.000Z",
    "title": "JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language\n  Model",
    "summary": "Multimodal large language models (MLLMs) excel in vision-language tasks but\nalso pose significant risks of generating harmful content, particularly through\njailbreak attacks. Jailbreak attacks refer to intentional manipulations that\nbypass safety mechanisms in models, leading to the generation of inappropriate\nor unsafe content. Detecting such attacks is critical to ensuring the\nresponsible deployment of MLLMs. Existing jailbreak detection methods face\nthree primary challenges: (1) Many rely on model hidden states or gradients,\nlimiting their applicability to white-box models, where the internal workings\nof the model are accessible; (2) They involve high computational overhead from\nuncertainty-based analysis, which limits real-time detection, and (3) They\nrequire fully labeled harmful datasets, which are often scarce in real-world\nsettings. To address these issues, we introduce a test-time adaptive framework\ncalled JAILDAM. Our method leverages a memory-based approach guided by\npolicy-driven unsafe knowledge representations, eliminating the need for\nexplicit exposure to harmful data. By dynamically updating unsafe knowledge\nduring test-time, our framework improves generalization to unseen jailbreak\nstrategies while maintaining efficiency. Experiments on multiple VLM jailbreak\nbenchmarks demonstrate that JAILDAM delivers state-of-the-art performance in\nharmful content detection, improving both accuracy and speed.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6614243f67d7bfc73afc6b77/z0oklTQ627SEQN42jYoat.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03770.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6614243f67d7bfc73afc6b77",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614243f67d7bfc73afc6b77/ifdjAb_BPraEJ2eIySk_K.jpeg",
      "fullname": "Shenzhe Zhu",
      "name": "Chouoftears",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]