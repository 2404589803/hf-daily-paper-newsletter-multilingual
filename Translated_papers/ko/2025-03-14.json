[
  {
    "paper": {
      "id": "2503.10613",
      "authors": [
        {
          "_id": "67d393ca336d57afb21bbf63",
          "user": {
            "_id": "67a99ec47b754f038d110926",
            "avatarUrl": "/avatars/e1ff318a42ccb75b094bbe7dae0cabec.svg",
            "isPro": false,
            "fullname": "Advait Gupta",
            "user": "advaitgupta",
            "type": "user"
          },
          "name": "Advait Gupta",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:36.855Z",
          "hidden": false
        },
        {
          "_id": "67d393ca336d57afb21bbf64",
          "user": {
            "_id": "672f89e6d7f4171f374dacea",
            "avatarUrl": "/avatars/4a8b378e13e862586bb428fdf000b3cc.svg",
            "isPro": false,
            "fullname": "NandaKiran Velaga",
            "user": "nandakiran09",
            "type": "user"
          },
          "name": "NandaKiran Velaga",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:34.327Z",
          "hidden": false
        },
        {
          "_id": "67d393ca336d57afb21bbf65",
          "name": "Dang Nguyen",
          "hidden": false
        },
        {
          "_id": "67d393ca336d57afb21bbf66",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:39.157Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:55:45.000Z",
      "submittedOnDailyAt": "2025-03-14T01:33:20.201Z",
      "title": "CoSTAast: 비용신시애티티터그먼트 에이전트이며, 다턴 이미지 편집용 툴 에이전트입니다.",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "텍스트로부터 이미지로의 모델（예：Stable Diffusion 및 DALLE-3）는 다단계 이미지 편집에 대한 어려움을 드러내고 있습니다. 이 작업은 효과적인 에이전트 워크 플로우 (패스)으로 분해하여 여러 서브 태스크를 순차적으로 해결할 수 있는 방법을 고려합니다. 전통적인 탐색 알고리즘은 도구 패스를 찾기 위해 고가의 탐색이 필요합니다. 반면에, 대규모 언어 모델 (LLMs)은 서브 태스크의 계획에 대한 사전 지식을 가지고 있지만, 각 서브 태스크에서 사용하는 도구의 능력과 비용을 정확한 평가할 수 있는지는 미지수입니다. LLMs와 그래프 탐색의 장점을 통합하여 비용 효율적인 도구 패스를 찾기가 가능한지 확인합니다. \"CoSTA*\"라는 세 단계 접근 방식을 제안하고, LLMs를 사용하여 서브 태스크의 트리를 구축하고, 작업에 대한 AI 도구의 그래프를 줄이고, 그 후 작은 서브 그래프에 대한 A* 탐색을 수행합니다. 작업 전체의 비용과 질의 균형을 더 잘 맞추기 위해, CoSTA*는 각 서브 태스크의 도구의 두 평가 기준을 통합하고, A* 탐색을 가이드합니다. 각 서브 태스크의 출력은 시각 언어 모델 (VLM)으로 평가되며, 실패가 발생할 경우, 서브 태스크의 도구의 비용과 질을 업데이트합니다. 따라서, A* 탐색은 실패로부터 빠르게 회복하여 다른 패스를 탐색할 수 있습니다. 또한, CoSTA*는 서브 태스크 간에 모델링을 자동적으로 변경하여, 더 좋은 비용과 질의 균형을 실현할 수 있습니다. 새로운 어려운 다단계 이미지 편집 벤치마크를 구축하고, CoSTA*는 이 벤치마크에서 비용과 질 모두에서 가장 先端한 이미지 편집 모델이나 에이전트를 초과하며, 사용자의 취향에 맞는 다양한 균형을 실현합니다.",
      "upvotes": 32,
      "discussionId": "67d393cf336d57afb21bc0db",
      "githubRepo": "https://github.com/tianyi-lab/CoSTAR",
      "ai_keywords": [
        "text-to-image models",
        "stable diffusion",
        "DALLE-3",
        "multi-turn image editing",
        "agentic workflow",
        "tool use",
        "subtasks",
        "AI tools",
        "cost-efficient",
        "large language models (LLMs)",
        "subtask planning",
        "graph search",
        "three-stage approach",
        "CoSTA*",
        "subtask tree",
        "pruning",
        "A* search",
        "subgraph",
        "cost-quality trade-off",
        "vision-language model (VLM)",
        "failure",
        "total cost",
        "quality",
        "modality switching",
        "benchmark",
        "state-of-the-art image-editing models",
        "user preference"
      ]
    },
    "publishedAt": "2025-03-13T13:55:45.000Z",
    "title": "CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing",
    "summary": "Text-to-image models like stable diffusion and DALLE-3 still struggle with\nmulti-turn image editing. We decompose such a task as an agentic workflow\n(path) of tool use that addresses a sequence of subtasks by AI tools of varying\ncosts. Conventional search algorithms require expensive exploration to find\ntool paths. While large language models (LLMs) possess prior knowledge of\nsubtask planning, they may lack accurate estimations of capabilities and costs\nof tools to determine which to apply in each subtask. Can we combine the\nstrengths of both LLMs and graph search to find cost-efficient tool paths? We\npropose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask\ntree, which helps prune a graph of AI tools for the given task, and then\nconducts A* search on the small subgraph to find a tool path. To better balance\nthe total cost and quality, CoSTA* combines both metrics of each tool on every\nsubtask to guide the A* search. Each subtask's output is then evaluated by a\nvision-language model (VLM), where a failure will trigger an update of the\ntool's cost and quality on the subtask. Hence, the A* search can recover from\nfailures quickly to explore other paths. Moreover, CoSTA* can automatically\nswitch between modalities across subtasks for a better cost-quality trade-off.\nWe build a novel benchmark of challenging multi-turn image editing, on which\nCoSTA* outperforms state-of-the-art image-editing models or agents in terms of\nboth cost and quality, and performs versatile trade-offs upon user preference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10613.png",
    "numComments": 7,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10480",
      "authors": [
        {
          "_id": "67d38a42d3d16e1166d81bed",
          "user": {
            "_id": "64c3c631e77ea9f28111172a",
            "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
            "isPro": false,
            "fullname": "Siyin Wang",
            "user": "sinwang",
            "type": "user"
          },
          "name": "Siyin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:44.686Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bee",
          "user": {
            "_id": "629ef8544313a7c1dd671130",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png",
            "isPro": false,
            "fullname": "Zhaoye Fei",
            "user": "ngc7293",
            "type": "user"
          },
          "name": "Zhaoye Fei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:04.607Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bef",
          "name": "Qinyuan Cheng",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf0",
          "user": {
            "_id": "64196e45060a651c415d5cf7",
            "avatarUrl": "/avatars/71a43232a7bae851eca252782387a63d.svg",
            "isPro": false,
            "fullname": "Shiduo Zhang",
            "user": "CyberDJ",
            "type": "user"
          },
          "name": "Shiduo Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:20.440Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf1",
          "name": "Panpan Cai",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf2",
          "user": {
            "_id": "618497ea8aaadc9253c2dfa9",
            "avatarUrl": "/avatars/2eb3954a99f5aede6f31b8ae49b8c910.svg",
            "isPro": false,
            "fullname": "Fu Jinlan",
            "user": "Jinlan",
            "type": "user"
          },
          "name": "Jinlan Fu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:38.981Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf3",
          "user": {
            "_id": "61457b8deff2c9fdb4de4988",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg",
            "isPro": false,
            "fullname": "Xipeng Qiu",
            "user": "xpqiu",
            "type": "user"
          },
          "name": "Xipeng Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:46.041Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T15:49:56.000Z",
      "submittedOnDailyAt": "2025-03-14T01:42:40.120Z",
      "title": "세계 모델링은 더 좋은 계획자를 만들 수 있습니다: 구조화된 태스크 계획의 이중 선호도 최적화",
      "submittedOnDailyBy": {
        "_id": "64c3c631e77ea9f28111172a",
        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
        "isPro": false,
        "fullname": "Siyin Wang",
        "user": "sinwang",
        "type": "user"
      },
      "summary": "최근의 대형 시각 언어 모델(LVLMs)의 발전은 구체적인 계획 태스크에 대한 기대할 수 있는 가능성을 보여주고 있지만, 의존 관계 제약과 효율성 문제에 직면하고 있습니다. 기존의 접근 방식은 행동 선택만 최적화하거나 추론 시 세계 모델을 사용하지만, 세계를 모델링하여 계획 능력을 향상시키는 이점을 보지 않습니다. 우리는 상태 예측과 행동 선택을 함께 선호 학습에 의한 최적화를 수행하는 새로운 학습 프레임워크 \"Dual Preference Optimization (D^2PO)\"를 제안합니다. 이로써 LVLMs가 환경의 동력학을 이해하고 더 좋은 계획을 가능하게 합니다. 인간의 피드백을 제외한 경로와 단계별 선호 데이터는 확장 탐색을 위해 트리 탐색 구조를 도입하여 자동적으로 수집합니다. VoTa-Bench에서 수행된 확장된 실험은 D^2PO 기반의 방법が Qwen2-VL (7B), LLaVA-1.6 (7B), LLaMA-3.2 (11B)에 대해 현재의 방법 및 GPT-4o를 크게 초월하며 효율적인 실행 경로와 함께 높은 태스크 성공률을 달성하는 것을 보여줍니다.",
      "upvotes": 25,
      "discussionId": "67d38a44d3d16e1166d81c54",
      "ai_keywords": [
        "Dual Preference Optimization (D$^2$PO)",
        "preference learning",
        "state prediction",
        "action selection",
        "environment dynamics",
        "tree search mechanism",
        "VoTa-Bench",
        "Qwen2-VL",
        "LLaVA-1.6",
        "LLaMA-3.2",
        "task success rates",
        "efficient execution paths"
      ]
    },
    "publishedAt": "2025-03-13T11:49:56.000Z",
    "title": "World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning",
    "summary": "Recent advances in large vision-language models (LVLMs) have shown promise\nfor embodied task planning, yet they struggle with fundamental challenges like\ndependency constraints and efficiency. Existing approaches either solely\noptimize action selection or leverage world models during inference,\noverlooking the benefits of learning to model the world as a way to enhance\nplanning capabilities. We propose Dual Preference Optimization (D^2PO), a new\nlearning framework that jointly optimizes state prediction and action selection\nthrough preference learning, enabling LVLMs to understand environment dynamics\nfor better planning. To automatically collect trajectories and stepwise\npreference data without human annotation, we introduce a tree search mechanism\nfor extensive exploration via trial-and-error. Extensive experiments on\nVoTa-Bench demonstrate that our D^2PO-based method significantly outperforms\nexisting methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and\nLLaMA-3.2 (11B), achieving superior task success rates with more efficient\nexecution paths.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10480.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "64c3c631e77ea9f28111172a",
      "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
      "fullname": "Siyin Wang",
      "name": "sinwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09669",
      "authors": [
        {
          "_id": "67d37754e07f664c7325f236",
          "user": {
            "_id": "63bbf972d8d676a2299cdb44",
            "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
            "isPro": false,
            "fullname": "Sangwon",
            "user": "agwmon",
            "type": "user"
          },
          "name": "Sangwon Jang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:49.038Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f237",
          "user": {
            "_id": "66c6edcc91dced946471bc13",
            "avatarUrl": "/avatars/55cc8593da6540e1566e1de9d7133f9f.svg",
            "isPro": false,
            "fullname": "June Suk Choi",
            "user": "wchoi403",
            "type": "user"
          },
          "name": "June Suk Choi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:59.636Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f238",
          "user": {
            "_id": "65e5bd4568234ef5d6decadc",
            "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
            "isPro": false,
            "fullname": "Jaehyeong Jo",
            "user": "harryjo97",
            "type": "user"
          },
          "name": "Jaehyeong Jo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:06:19.773Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f239",
          "user": {
            "_id": "635097ec59bfa9a85d4207b2",
            "avatarUrl": "/avatars/787085894e9e6538b6b3e3051efe9eea.svg",
            "isPro": false,
            "fullname": "Kimin Lee",
            "user": "kiminle2",
            "type": "user"
          },
          "name": "Kimin Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:06:26.649Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f23a",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T17:21:57.000Z",
      "submittedOnDailyAt": "2025-03-14T02:05:42.787Z",
      "title": "サイレントブランドリングアタック：トリガー無しデータポイズニングアタックとしてのテキストから画像への拡散モデルに対する攻撃\n\n翻译结果：\n\nサイレントブランドリングアタック：トリガー없이 데이터포이ズ닝アタック로서 텍스트로부터 이미지로 확산모형에 대한 공격",
      "submittedOnDailyBy": {
        "_id": "63bbf972d8d676a2299cdb44",
        "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
        "isPro": false,
        "fullname": "Sangwon",
        "user": "agwmon",
        "type": "user"
      },
      "summary": "텍스트에서 이미지로 확산된 모델은 텍스트 프로리미어에서 고품질의 콘텐츠 생성을 위해 놀라운 성공을 거두고 있습니다. 그러나 공개 가능한 데이터의 의존성과 데이터 공유의 증가는 이러한 모델이 특히 데이터 포이징 공격에 취약해지게 됩니다. 본 논문에서는 새로운 데이터 포이징 방법인 '사일렌트 브랜딩 공격'을 소개합니다. 이 방법은 텍스트에서 이미지로 확산된 모델을 조작하여 특정 브랜드 로고나 상징을 포함하는 이미지 생성을 위해 텍스트 트라이거가 없는 경우도 포함합니다. 우리는 특정 시각적 패턴이 훈련 데이터에 반복되어 모델이 자연스럽게 재현하도록 학습하는 것을 발견했습니다. 이를 활용하여 자동화된 데이터 포이징 알고리즘을 개발하고, 로고를 숨기지 않고 자연스럽게 혼합하여 감지되지 않도록 원본 이미지에 로고를 추가하지 않고 처리했습니다. 이 포이징 데이터 세트를 사용하여 훈련된 모델은 로고가 포함된 이미지를 생성할 수 있으며, 이미지의 품질 저하와 텍스트의 배치 파괴를 피할 수 있습니다. 우리는 두 개의 실용적인 설정인 큰 질량의 고품질 이미지 데이터 세트와 스타일 파서나리첨션 데이터 세트에서 특정 텍스트 트라이거가 없는 경우에도 높은 성공률을 달성할 수 있음을 실험적으로 증명했습니다. 인간 평가와 로고 감지의 정량적 메트릭을 포함하여, 우리의 방법론이 로고를 숨겨서 삽입할 수 있음을 증명했습니다.",
      "upvotes": 25,
      "discussionId": "67d37759e07f664c7325f3c5",
      "projectPage": "https://silent-branding.github.io/",
      "ai_keywords": [
        "text-to-image diffusion models",
        "high-quality contents",
        "text prompts",
        "data poisoning attacks",
        "Silent Branding Attack",
        "visual patterns",
        "data poisoning algorithm",
        "logos",
        "style personalization datasets",
        "logo detection"
      ]
    },
    "publishedAt": "2025-03-12T13:21:57.000Z",
    "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on\n  Text-to-Image Diffusion Models",
    "summary": "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality contents from text prompts. However, their reliance on publicly\navailable data and the growing trend of data sharing for fine-tuning make these\nmodels particularly vulnerable to data poisoning attacks. In this work, we\nintroduce the Silent Branding Attack, a novel data poisoning method that\nmanipulates text-to-image diffusion models to generate images containing\nspecific brand logos or symbols without any text triggers. We find that when\ncertain visual patterns are repeatedly in the training data, the model learns\nto reproduce them naturally in its outputs, even without prompt mentions.\nLeveraging this, we develop an automated data poisoning algorithm that\nunobtrusively injects logos into original images, ensuring they blend naturally\nand remain undetected. Models trained on this poisoned dataset generate images\ncontaining logos without degrading image quality or text alignment. We\nexperimentally validate our silent branding attack across two realistic\nsettings on large-scale high-quality image datasets and style personalization\ndatasets, achieving high success rates even without a specific text trigger.\nHuman evaluation and quantitative metrics including logo detection show that\nour method can stealthily embed logos.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbf972d8d676a2299cdb44",
      "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
      "fullname": "Sangwon",
      "name": "agwmon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10639",
      "authors": [
        {
          "_id": "67d3a632db36a4d5d95dbcff",
          "user": {
            "_id": "65b8724123d948d884b379b1",
            "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
            "isPro": false,
            "fullname": "Rongyao Fang",
            "user": "LucasFang",
            "type": "user"
          },
          "name": "Rongyao Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:14.110Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd00",
          "user": {
            "_id": "64a2b496e2e19de17db7de65",
            "avatarUrl": "/avatars/241448ca487833d6cc5d57bb1fdb6ee5.svg",
            "isPro": false,
            "fullname": "Duan Chengqi",
            "user": "gogoduan",
            "type": "user"
          },
          "name": "Chengqi Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:01.612Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd01",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd02",
          "user": {
            "_id": "65fc7c824d36be78e66ba92d",
            "avatarUrl": "/avatars/d4a55c820cae533f91724e062427516a.svg",
            "isPro": false,
            "fullname": "Linjiang Huang",
            "user": "LjHuang",
            "type": "user"
          },
          "name": "Linjiang Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:08.707Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd03",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd04",
          "user": {
            "_id": "65273fea0ef49cfb783fa5c1",
            "avatarUrl": "/avatars/0c9e204bc2151c8cc533311900d05a36.svg",
            "isPro": false,
            "fullname": "shilinyan",
            "user": "shilinyan",
            "type": "user"
          },
          "name": "Shilin Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:18.212Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd05",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd06",
          "user": {
            "_id": "666d4a0fe70e5838d95aebee",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6dkjoFA_sOjCkjvcvozZ5.jpeg",
            "isPro": false,
            "fullname": "zengxingyu",
            "user": "zengxingyu",
            "type": "user"
          },
          "name": "Xingyu Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:42.264Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd07",
          "name": "Rui Zhao",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd08",
          "user": {
            "_id": "64686f7172d9180d4ac8b4e4",
            "avatarUrl": "/avatars/db67dd6c4b2b41054ddcce5a18ade6f8.svg",
            "isPro": false,
            "fullname": "Jifeng Dai",
            "user": "daijifeng",
            "type": "user"
          },
          "name": "Jifeng Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:14:59.460Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd09",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:14:50.924Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd0a",
          "user": {
            "_id": "65c04e9c27a5fdca81abcbd9",
            "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
            "isPro": false,
            "fullname": "Hongsheng LI",
            "user": "hsli-cuhk",
            "type": "user"
          },
          "name": "Hongsheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:14:43.402Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:59.000Z",
      "submittedOnDailyAt": "2025-03-14T02:16:04.349Z",
      "title": "GoT: 시각적 생성과 편집을 위한 다중모달 대형 언어 모델의 논리 능력의 자유\n\n(注意：虽然您要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，我保留了原文中的缩写“GoT”，并保持了其原有的格式。如果需要完全按照您的要求进行翻译，不保留缩写和格式，请告知。)",
      "submittedOnDailyBy": {
        "_id": "65b8724123d948d884b379b1",
        "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
        "isPro": false,
        "fullname": "Rongyao Fang",
        "user": "LucasFang",
        "type": "user"
      },
      "summary": "현재의 이미지 생성 및 편집 방법들은 텍스트 프롬프트를 직접 입력으로 처리하며 시각적인 구성과 명시적인 조작에 대한 이유를 고려하지 않는다. 우리는 출력 이미지에 앞서 명시적인 언어적 이유론리 프로세스를 통해 생성 및 편집을 가능하게 하는 새로운 패러다임인 \"Generation Chain-of-Thought (GoT)\"를 제안합니다. 이 접근법은 의미적 관계와 공간 배치를 분석하기 위한 이유론리 가이드 프레임워크로 변환하여 가치를 높입니다. GoT의 정규화를 정의하고, 의미적-공간 관계를 파악하는 세부적인 이유 체인을 포함하는 9M 이상의 샘플을 포함하는 큰 규모의 GoT 데이터 세트를 구축합니다. GoT의 우수한 측면을 활용하기 위해, Qwen2.5-VL를 사용한 이유 체인 생성과, 우리 새로운 의미적-공간 가이드 모뎀을 통해 강화된 단말에서부터 확장 모델을 통합한 일련의 프레임워크를 구현합니다. 실험은 생성 및 편집 태스크에서 베이스라인보다 크게 향상을示し, 명시적인 이유론리 프로세스의 변경을 가능하게 하고, 이미지의 조정을 정확하게 수행할 수 있는 상호작용적인 시각화 생성을 가능하게 합니다. GoT는 이유론리를 드라이버로 하는 시각화 생성 및 편집의 새로운 방향을 개척하고, 인간 의도에 따라 더 좋은 일치하는 이미지를 생성할 수 있는 방법을 제공합니다. 향후 연구를 위해, 우리는 데이터 세트, 코드, 프리트레이드 모델을 공개합니다.",
      "upvotes": 21,
      "discussionId": "67d3a636db36a4d5d95dbdeb",
      "githubRepo": "https://github.com/rongyaofang/GoT",
      "ai_keywords": [
        "Generation Chain-of-Thought (GoT)",
        "text-to-image generation",
        "editing tasks",
        "reasoning chain generation",
        "end-to-end diffusion model",
        "Semantic-Spatial Guidance Module",
        "semantic relationships",
        "spatial arrangements",
        "large-scale GoT datasets",
        "detailed reasoning chains",
        "semantic-spatial relationships",
        "interactive visual generation",
        "reasoning steps",
        "human intent"
      ]
    },
    "publishedAt": "2025-03-13T13:59:59.000Z",
    "title": "GoT: Unleashing Reasoning Capability of Multimodal Large Language Model\n  for Visual Generation and Editing",
    "summary": "Current image generation and editing methods primarily process textual\nprompts as direct inputs without reasoning about visual composition and\nexplicit operations. We present Generation Chain-of-Thought (GoT), a novel\nparadigm that enables generation and editing through an explicit language\nreasoning process before outputting images. This approach transforms\nconventional text-to-image generation and editing into a reasoning-guided\nframework that analyzes semantic relationships and spatial arrangements. We\ndefine the formulation of GoT and construct large-scale GoT datasets containing\nover 9M samples with detailed reasoning chains capturing semantic-spatial\nrelationships. To leverage the advantages of GoT, we implement a unified\nframework that integrates Qwen2.5-VL for reasoning chain generation with an\nend-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance\nModule. Experiments show our GoT framework achieves excellent performance on\nboth generation and editing tasks, with significant improvements over\nbaselines. Additionally, our approach enables interactive visual generation,\nallowing users to explicitly modify reasoning steps for precise image\nadjustments. GoT pioneers a new direction for reasoning-driven visual\ngeneration and editing, producing images that better align with human intent.\nTo facilitate future research, we make our datasets, code, and pretrained\nmodels publicly available at https://github.com/rongyaofang/GoT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b8724123d948d884b379b1",
      "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
      "fullname": "Rongyao Fang",
      "name": "LucasFang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10633",
      "authors": [
        {
          "_id": "67d3ba5e4d3a41ed9f8651eb",
          "user": {
            "_id": "630dd4218df86f1e5beb2ed7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
            "isPro": false,
            "fullname": "Eliahu Horwitz",
            "user": "Eliahu",
            "type": "user"
          },
          "name": "Eliahu Horwitz",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:04.270Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ec",
          "user": {
            "_id": "674ec6d1ce68874ee4f2d53b",
            "avatarUrl": "/avatars/4c15c9bdcf51d4bf5e6fceb86195e480.svg",
            "isPro": false,
            "fullname": "Nitzan Kurer",
            "user": "nitzankur",
            "type": "user"
          },
          "name": "Nitzan Kurer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T10:09:18.093Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ed",
          "user": {
            "_id": "6465fd33dac127ac80f0b334",
            "avatarUrl": "/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg",
            "isPro": false,
            "fullname": "Jonathan Kahana",
            "user": "jonkahana",
            "type": "user"
          },
          "name": "Jonathan Kahana",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T10:09:25.338Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ee",
          "user": {
            "_id": "669ffff5944b597ce2a1aa5b",
            "avatarUrl": "/avatars/559ca0ad82b1a52208510f09492fafa6.svg",
            "isPro": false,
            "fullname": "Liel Amar",
            "user": "LielAmar",
            "type": "user"
          },
          "name": "Liel Amar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:01.634Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ef",
          "user": {
            "_id": "646cfc3b4220471ca0c56b20",
            "avatarUrl": "/avatars/19d6ab141ec2cd25c1c3b45fd8f69910.svg",
            "isPro": false,
            "fullname": "Yedid Hoshen",
            "user": "yedid",
            "type": "user"
          },
          "name": "Yedid Hoshen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T10:09:39.859Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/630dd4218df86f1e5beb2ed7/HClm12KfVuYMozbJaIp9_.png"
      ],
      "publishedAt": "2025-03-13T17:59:53.000Z",
      "submittedOnDailyAt": "2025-03-14T03:51:41.703Z",
      "title": "하우징페이스 모델 아토라스를 차트와 네비게이션으로 표시합니다.",
      "submittedOnDailyBy": {
        "_id": "630dd4218df86f1e5beb2ed7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
        "isPro": false,
        "fullname": "Eliahu Horwitz",
        "user": "Eliahu",
        "type": "user"
      },
      "summary": "현재 공개적으로 사용 가능한 뉴럴네트워크가 수백만 개 존재하여, 모델의 디렉토리를 검색하고 분석하는 것이 중요해졌습니다. 이러한 모델을 다양한 것들로 선택하기 위해서는 아토라스가 필요하지만, 대부분의 모델은 설명이 부족하여 아토라스의 구성이 어렵습니다. 모델 디렉토리의 잠재적인 가능성 탐색을 위해, 우리는 Hugging Face의 설명이 포함된 초기 아토라스를 만들었습니다. 이것은 모델의 구조와 진화를 놀라워하는 시각화할 수 있습니다. 우리는 이 아토라스의 설명을 사용하여 모델의 속성(예: 정확도)를 예측하거나 컴퓨터 비전 모델의 트렌드를 분석하는 방법을 보여줍니다. 그러나 현재의 아토라스는 완성되지 않은 상태이며, 우리는 설명되지 않은 영역을 차트하는 방법을 제안합니다. 특히, 우리는 현실적인 모델의 훈련 프로세스에 기반한 고신뢰성 구조적 선도를 특정하고, 이러한 선도를 활용하여 아토라스의 이전에 설명되지 않은 부분을 정확히 매핑할 수 있도록 합니다. 우리는 데이터셋, 코드, 그리고 인터랙티브 아토라스를 공개합니다.",
      "upvotes": 20,
      "discussionId": "67d3ba634d3a41ed9f86533a",
      "projectPage": "https://horwitz.ai/model-atlas",
      "githubRepo": "https://github.com/eliahuhorwitz/Model-Atlas",
      "ai_keywords": [
        "neural networks",
        "model repositories",
        "atlas",
        "model landscape",
        "model evolution",
        "predicting model attributes",
        "trends in computer vision models",
        "high-confidence structural priors",
        "dominant real-world model training practices",
        "interactive atlas"
      ]
    },
    "publishedAt": "2025-03-13T13:59:53.000Z",
    "title": "Charting and Navigating Hugging Face's Model Atlas",
    "summary": "As there are now millions of publicly available neural networks, searching\nand analyzing large model repositories becomes increasingly important.\nNavigating so many models requires an atlas, but as most models are poorly\ndocumented charting such an atlas is challenging. To explore the hidden\npotential of model repositories, we chart a preliminary atlas representing the\ndocumented fraction of Hugging Face. It provides stunning visualizations of the\nmodel landscape and evolution. We demonstrate several applications of this\natlas including predicting model attributes (e.g., accuracy), and analyzing\ntrends in computer vision models. However, as the current atlas remains\nincomplete, we propose a method for charting undocumented regions.\nSpecifically, we identify high-confidence structural priors based on dominant\nreal-world model training practices. Leveraging these priors, our approach\nenables accurate mapping of previously undocumented areas of the atlas. We\npublicly release our datasets, code, and interactive atlas.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/630dd4218df86f1e5beb2ed7/HClm12KfVuYMozbJaIp9_.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10633.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630dd4218df86f1e5beb2ed7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
      "fullname": "Eliahu Horwitz",
      "name": "Eliahu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09662",
      "authors": [
        {
          "_id": "67d3daf40034469b0d6cc872",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc873",
          "name": "Zikai Zhou",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc874",
          "name": "Dian Xie",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc875",
          "name": "Yuetong Fang",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc876",
          "name": "Tian Ye",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc877",
          "name": "Lichen Bai",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc878",
          "name": "Zeke Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T15:15:25.000Z",
      "submittedOnDailyAt": "2025-03-14T06:07:49.038Z",
      "title": "코레^2: 수집, 반성, 재개시하여, 더 좋은 오미くじ를 빠르게 생성する",
      "submittedOnDailyBy": {
        "_id": "66015e8aa4d296af07de538e",
        "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
        "isPro": false,
        "fullname": "Ye",
        "user": "Owen777",
        "type": "user"
      },
      "summary": "テキストから 이미지(T2I)의 생성 모델의 샘플링을 모두高速하고 좋은 방향으로 개선하는 것은 유망한 연구 방향 중 대표적인 것으로 알려져 있습니다. 과거의 연구는 합성 이미지의 시각적 품질을 향상시키기 위해 샘플링 효율을 희생하거나, 샘플링을 크게 가속화하면서 기본 모델의 생성 능력에 개선을 미치지 않았습니다. 또한, 대부분의 추론 방법들은 안정된 성능을 DMs(Diffusion Models)과 ARMs(Vision Auto-regressive Models) 모두에서 보장할 수 없었습니다. 본 논문에서는 새로운 플러그인과 파라미터 기반 추론 패러다임인 CoRe^2를 소개합니다. CoRe^2는 Collect, Reflect, Refine의 3가지 프로세스로 구성됩니다. CoRe^2는 처음에 클래스 필터링 제한 없는 가이드(CFG)의 트래지렉트를 수집하고, 수집된 데이터를 사용하여 학습이 쉬운 내용을 반영하면서 추론 시 함수 평가 수를 절반으로 줄이는 약한 모델을 훈련시킵니다. 다음으로, CoRe^2는 약한 가이드로부터 강한 가이드에 의한 조건부 출력을 개선하고, 기본 모델이 쉽게 이해할 수 있는 고주파 및 현실적인 콘텐츠의 생성 능력을 향상시킵니다. 우리의 지식의 한계에서, CoRe^2는 SDXL, SD3.5, FLUX 등 DMs와 LlamaGen 등 ARMs의 광범위한 범위에서 효율성과 효과성을 모두 보여주는 것이 처음으로 알려져 있습니다. HPD v2, Pick-of-Pic, Drawbench, GenEval, T2I-Compbench에서 유의미한 성능 향상이 관찰되었습니다. 또한, CoRe^2는 가장 先端의 Z-Sampling과 무간잡한 통합이 가능하며, PickScore과 AES에서 각각 0.3과 0.16의 개선을 나타내며, SD3.5에서 5.64초의 시간 감소를 달성했습니다. 코드는 https://github.com/xie-lab-ml/CoRe/tree/main에 공개되어 있습니다.",
      "upvotes": 20,
      "discussionId": "67d3dafb0034469b0d6ccac0",
      "ai_keywords": [
        "diffusion models (DMs)",
        "visual autoregressive models (ARMs)",
        "classifier-free guidance (CFG)",
        "HPD v2",
        "Pick-of-Pic",
        "Drawbench",
        "GenEval",
        "T2I-Compbench",
        "PickScore",
        "AES",
        "Z-Sampling",
        "SDXL",
        "SD3.5",
        "FLUX",
        "LlamaGen"
      ]
    },
    "publishedAt": "2025-03-12T11:15:25.000Z",
    "title": "CoRe^2: Collect, Reflect and Refine to Generate Better and Faster",
    "summary": "Making text-to-image (T2I) generative model sample both fast and well\nrepresents a promising research direction. Previous studies have typically\nfocused on either enhancing the visual quality of synthesized images at the\nexpense of sampling efficiency or dramatically accelerating sampling without\nimproving the base model's generative capacity. Moreover, nearly all inference\nmethods have not been able to ensure stable performance simultaneously on both\ndiffusion models (DMs) and visual autoregressive models (ARMs). In this paper,\nwe introduce a novel plug-and-play inference paradigm, CoRe^2, which comprises\nthree subprocesses: Collect, Reflect, and Refine. CoRe^2 first collects\nclassifier-free guidance (CFG) trajectories, and then use collected data to\ntrain a weak model that reflects the easy-to-learn contents while reducing\nnumber of function evaluations during inference by half. Subsequently, CoRe^2\nemploys weak-to-strong guidance to refine the conditional output, thereby\nimproving the model's capacity to generate high-frequency and realistic\ncontent, which is difficult for the base model to capture. To the best of our\nknowledge, CoRe^2 is the first to demonstrate both efficiency and effectiveness\nacross a wide range of DMs, including SDXL, SD3.5, and FLUX, as well as ARMs\nlike LlamaGen. It has exhibited significant performance improvements on HPD v2,\nPick-of-Pic, Drawbench, GenEval, and T2I-Compbench. Furthermore, CoRe^2 can be\nseamlessly integrated with the state-of-the-art Z-Sampling, outperforming it by\n0.3 and 0.16 on PickScore and AES, while achieving 5.64s time saving using\nSD3.5.Code is released at https://github.com/xie-lab-ml/CoRe/tree/main.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09662.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "66015e8aa4d296af07de538e",
      "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
      "fullname": "Ye",
      "name": "Owen777",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10622",
      "authors": [
        {
          "_id": "67d3b0a87443e648e8aa1ea6",
          "user": {
            "_id": "6552126dd8a8835b66653767",
            "avatarUrl": "/avatars/0b1dad9ebaeada8f5e7ebe453123960b.svg",
            "isPro": false,
            "fullname": "Jiachen Zhu",
            "user": "JiachenZhu",
            "type": "user"
          },
          "name": "Jiachen Zhu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T05:13:31.648Z",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1ea7",
          "name": "Xinlei Chen",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1ea8",
          "name": "Kaiming He",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1ea9",
          "name": "Yann LeCun",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1eaa",
          "name": "Zhuang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:06.000Z",
      "submittedOnDailyAt": "2025-03-14T02:59:49.783Z",
      "title": "Transformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransformers without Normalization\n\nTransform",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "정규화 레이어는 현대의 뉴럴네트워크에서 거의 없지만 오랜 기간 동안 중요시 되었습니다. 본 논문에서는 정규화를 포함하지 않는 Transformer가 매우 간단한 방법을 사용하여 동일한이나 더 좋은 성능을 구현할 수 있음을 보여줍니다. Dynamic Tanh (DyT)라는 요소별로 동작하는 DyT(x) = tanh(alpha x)를 Transformer의 정규화 레이어의 드롭아웃 대체로 소개합니다. DyT는 Transformer의 정규화 레이어가 S형의 입력-출력 매핑을 생성하는 것을 관찰한 후, tanh 함수의 특성을 갖도록 설계되었습니다. DyT를 적용하면 정규화를 포함하지 않는 Transformer는 거의 파라미터 조정 없이 정규화된 토치 컴피의 성능을 초월할 수 있습니다. DyT를 포함하는 Transformer의 효과를 생성, 감독 학습, 자동 학습, 컴퓨터 비전, 언어 모델 등 다양한 설정에서 검증했습니다. 이러한 발견은 현대의 뉴럴네트워크에서 정규화 레이어가 필수적이라는 전통적인 이해를 질疑하고, 깊은 신경망에서 정규화 레이어의 역할을 새로운 시각을 제공합니다.",
      "upvotes": 16,
      "discussionId": "67d3b0a97443e648e8aa1f22",
      "ai_keywords": [
        "Dynamic Tanh (DyT)",
        "Transformers",
        "normalization layers",
        "layer normalization",
        "hyperparameter tuning",
        "supervised learning",
        "self-supervised learning",
        "computer vision",
        "language models"
      ]
    },
    "publishedAt": "2025-03-13T13:59:06.000Z",
    "title": "Transformers without Normalization",
    "summary": "Normalization layers are ubiquitous in modern neural networks and have long\nbeen considered essential. This work demonstrates that Transformers without\nnormalization can achieve the same or better performance using a remarkably\nsimple technique. We introduce Dynamic Tanh (DyT), an element-wise operation\nDyT(x) = tanh(alpha x), as a drop-in replacement for normalization\nlayers in Transformers. DyT is inspired by the observation that layer\nnormalization in Transformers often produces tanh-like, S-shaped input-output\nmappings. By incorporating DyT, Transformers without normalization can match or\nexceed the performance of their normalized counterparts, mostly without\nhyperparameter tuning. We validate the effectiveness of Transformers with DyT\nacross diverse settings, ranging from recognition to generation, supervised to\nself-supervised learning, and computer vision to language models. These\nfindings challenge the conventional understanding that normalization layers are\nindispensable in modern neural networks, and offer new insights into their role\nin deep networks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10622.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10596",
      "authors": [
        {
          "_id": "67d3a8950ada3dfbf617fc23",
          "name": "Rui Hu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc24",
          "name": "Lianghui Zhu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc25",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc26",
          "name": "Tianheng Cheng",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc27",
          "name": "Lei Liu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc28",
          "name": "Heng Liu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc29",
          "name": "Longjin Ran",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc2a",
          "name": "Xiaoxin Chen",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc2b",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc2c",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:43:10.000Z",
      "submittedOnDailyAt": "2025-03-14T02:31:30.611Z",
      "title": "GroundingSuite: 복잡한 다粒도 픽셀 감지 평가\n\n(请注意，虽然我遵循了您的要求，但翻译可能在专业术语的准确性上存在一定的局限性，特别是在“多粒度”和“像素检测”等技术术语的翻译上。如果需要更精确的翻译，建议咨询相关领域的专家或使用专业翻译服务。)",
      "submittedOnDailyBy": {
        "_id": "646b3db131968a60a01e4cf5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
        "isPro": false,
        "fullname": "Tianheng Cheng",
        "user": "wondervictor",
        "type": "user"
      },
      "summary": "Pixel grounding은, Referring Expression Segmentation (RES) 등 다양한 태스크를 포함하여 시각과 언어의 모델 간의 공백을 메우는 큰 가능성에 기반하여 많은 관심을 받고 있습니다. 그러나 현재의 발전은 대상물 카테고리의 제한, 맥락의 다양성 부족, 고품질의 설명의 부족 등 다양한 제한으로 제한되어 있습니다. 이러한 제한을 완화하기 위해 GroundingSuite를 소개합니다. GroundingSuite는 다음과 같은 3가지 구성 요소를 포함합니다: 1) Vision-Language Model (VLM) 에이전트를 활용한 데이터 설명 프레임워크, 2) 956만건의 다양한 Referring Expression 및 그 대응하는 분할을 포함하는 대규모 훈련 데이터 세트, 3) 3,800장의 이미지로 구축된 미세한 현재 데이터베이스의 평가 벤치마크. GroundingSuite의 훈련 데이터 세트는 훈련된 모델이 가장 최신의 결과를 구현할 수 있도록 합니다. 특히, gRefCOCO에서 cIoU 68.9, RefCOCOm에서 gIoU 55.3을 달성했습니다. 또한, GroundingSuite의 설명 프레임워크는 현재의 발전된 데이터 설명 방법과 비교하여 GLaMM보다 4.5배 효율적이며, 이러한 효율성은 설명 작업의 속도와 품질에 직접적인 영향을 미칩니다.",
      "upvotes": 15,
      "discussionId": "67d3a8960ada3dfbf617fc8d",
      "ai_keywords": [
        "Referring Expression Segmentation (RES)",
        "Vision-Language Model (VLM)",
        "GroundingSuite",
        "cIoU",
        "gIoU",
        "gRefCOCO",
        "RefCOCOm",
        "GLaMM"
      ]
    },
    "publishedAt": "2025-03-13T13:43:10.000Z",
    "title": "GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding",
    "summary": "Pixel grounding, encompassing tasks such as Referring Expression Segmentation\n(RES), has garnered considerable attention due to its immense potential for\nbridging the gap between vision and language modalities. However, advancements\nin this domain are currently constrained by limitations inherent in existing\ndatasets, including limited object categories, insufficient textual diversity,\nand a scarcity of high-quality annotations. To mitigate these limitations, we\nintroduce GroundingSuite, which comprises: (1) an automated data annotation\nframework leveraging multiple Vision-Language Model (VLM) agents; (2) a\nlarge-scale training dataset encompassing 9.56 million diverse referring\nexpressions and their corresponding segmentations; and (3) a meticulously\ncurated evaluation benchmark consisting of 3,800 images. The GroundingSuite\ntraining dataset facilitates substantial performance improvements, enabling\nmodels trained on it to achieve state-of-the-art results. Specifically, a cIoU\nof 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the\nGroundingSuite annotation framework demonstrates superior efficiency compared\nto the current leading data annotation method, i.e., 4.5 times faster than\nthe GLaMM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10596.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646b3db131968a60a01e4cf5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
      "fullname": "Tianheng Cheng",
      "name": "wondervictor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 27
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10351",
      "authors": [
        {
          "_id": "67d39b35acb72b994659d4fd",
          "name": "Sinuo Liu",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d4fe",
          "name": "Chenyang Lyu",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d4ff",
          "name": "Minghao Wu",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d500",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d501",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d502",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6527d8b077bceabaab382a75/3_8muRazw1wwHmG9IxRGk.png"
      ],
      "publishedAt": "2025-03-13T13:27:53.000Z",
      "submittedOnDailyAt": "2025-03-14T01:29:07.562Z",
      "title": "새로운 현대적인 기계 번역의 트렌드에 대해, 대규모의 논리 모델을 사용하는 방법들을 소개합니다.",
      "submittedOnDailyBy": {
        "_id": "6527d8b077bceabaab382a75",
        "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
        "isPro": false,
        "fullname": "Chenyang Lyu",
        "user": "ChenyangLyu",
        "type": "user"
      },
      "summary": "최근의 대규모 추론 모델(LRMs)의 발전, 특히 Chain-of-Thought 추론(CoT)을 활용한 모델은 기계 번역(MT)에 새로운 가능성들을 개척하고 있습니다. 이 논문은 LRMs가 번역을 맥락적, 문화적, 언어의 이해와 추론을 위한 동적인 추론 임무로 재구성하고, 전통적인 뉴럴 MT나 LLMs 기반의 MT 패러다임을 크게 변화시킨 점에 대해 주장합니다. 우리는 세 가지 기초적인 변화를 명확히 합니다: 1) 맥락적 일관성, LRMs는 문장이나 복잡한 맥락, 또는 맥락의 부족을 명시적으로 추론하여 불확실성을 해결하고 논리 구조를 유지합니다; 2) 문화적 의도성, 모델이 대화자의 의도, 청취자의 기대, 사회 언어 규칙을 추론하여 출력을 적응할 수 있게 되었습니다; 3) 자기 인식, LRMs는 추론 시 잠재적인 번역 오류를 수정할 수 있으며, 특히 매우 노이즈가 많은 경우에도 더 강한 강도를 보여주는 데 성공합니다. 우리는 스타일화된 번역, 문서 수준 번역, 모델 다양화 번역 등 다양한 번역 시나리오를 조사하고, 이러한 실험 사례를 통해 LRMs의 번역의 우위를 보여주고 있습니다. 또한, LRMs의 번역에 대한 흥미로운 현상, 자동 피버트 번역, 번역의 과도 지역화, 추론의 효율성 등 중요한 문제를 명확히 합니다. 결론적으로, 우리는 LRMs가 번역 시스템을 텍스트 변환기 대신 다언어적인 인지 아군으로 번역을 재 정의하고 있다고 생각합니다. 이 패러다임의 변화는 LRMs를 사용하여 번역 문제를 더 넓은 맥락에서 고려하는 것을 촉발시키고, 이로 인해 또 다른 가능성들을 개척할 수 있다는 것을 생각합니다.",
      "upvotes": 14,
      "discussionId": "67d39b40acb72b994659d916",
      "ai_keywords": [
        "Chain-of-Thought reasoning (CoT)",
        "Large Reasoning Models (LRMs)",
        "Neural MT",
        "Contextual coherence",
        "Cultural intentionality",
        "Self-reflection",
        "Stylized translation",
        "Document-level translation",
        "Multimodal translation",
        "Auto-pivot translation",
        "Over-localisation",
        "Inference efficiency",
        "Multilingual cognitive agents"
      ]
    },
    "publishedAt": "2025-03-13T09:27:53.000Z",
    "title": "New Trends for Modern Machine Translation with Large Reasoning Models",
    "summary": "Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6527d8b077bceabaab382a75/3_8muRazw1wwHmG9IxRGk.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10351.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6527d8b077bceabaab382a75",
      "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
      "fullname": "Chenyang Lyu",
      "name": "ChenyangLyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04723",
      "authors": [
        {
          "_id": "67d39576de5ce3cc428b1909",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190a",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190b",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqing Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:31.682Z",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190c",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190d",
          "name": "Ming Shan Hee",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190e",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190f",
          "name": "Roy Ka-Wei Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T18:59:37.000Z",
      "submittedOnDailyAt": "2025-03-14T01:04:48.148Z",
      "title": "Shifting Long-Context LLMs Research from Input to Output\n\n1. 输入에서부터의 긴 컨텍스트 LLMs 연구의 전환\n2. 긴 컨텍스트 LLMs 연구를 출력으로 전환하기\n3. 긴 컨텍스트 LLMs 연구의 전환: 입력에서 출력으로\n4. 긴 컨텍스트 LLMs 연구의 전환: 입력을 출력으로\n5. 긴 컨텍스트 LLMs 연구의 전환: 출력으로 전환하기\n6. 긴 컨텍스트 LLMs 연구를 출력으로 전환하기\n7. 긴 컨텍스트 LLMs 연구의 전환: 출력으로\n8. 긴 컨텍스트 LLMs 연구의 전환: 출력으로 전환하기\n9. 긴 컨텍스트 LLMs 연구를 출력으로 전환하기\n10. 긴 컨텍스트 LLMs 연구의 전환: 출력으로 전환하기",
      "submittedOnDailyBy": {
        "_id": "63369da91ba5d5ece24118a4",
        "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
        "isPro": false,
        "fullname": "wuyuhao",
        "user": "mozhu",
        "type": "user"
      },
      "summary": "최근의 긴 문맥 대 언어 모델(LLM)의 발전은 긴 문맥의 이해에 집중하고 긴 문맥을 이해하는 데 큰 진전을 거둔 데에만 한 가지를 제외하고, 긴 문맥의 출력 생성은 상대적으로 덜 주목을 받았습니다. 본 논문은 NLP 연구의 패러다임의 변화에 주력하고 긴 문맥의 출력 생성의 도전을 해결하기 위한 데로 있습니다. 새로운 글쓰기, 장기적인 계획, 복잡한 이유를 요구하는 태스크는 모델이 매우 넓은 문맥을 이해하고 그 속에서 연결적이고 문맥이 풍부하며 서로 조화되는 긴 문맥의 문장을 생성하는 것이 필요합니다. 이러한 요구 사항은 현재의 LLM의 능력에 있어서 중요한 한계점을 드러냅니다. 이러한 조사 부족의 영역의 중요성을 강조하고, 고품질의 긴 문맥의 출력을 생성하기 위한 기초적인 LLM의 개발에 집중적인 노력을 요구하고 있습니다. 이러한 모델은 현실적인 응용 분야에서 큰 잠재력을 가지고 있습니다.",
      "upvotes": 12,
      "discussionId": "67d39577de5ce3cc428b194f",
      "ai_keywords": [
        "long-context Large Language Models (LLMs)",
        "long-context comprehension",
        "long-output generation",
        "novel writing",
        "long-term planning",
        "complex reasoning",
        "coherent",
        "contextually rich",
        "logically consistent",
        "extended text",
        "high-quality",
        "long-form outputs"
      ]
    },
    "publishedAt": "2025-03-06T13:59:37.000Z",
    "title": "Shifting Long-Context LLMs Research from Input to Output",
    "summary": "Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04723.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63369da91ba5d5ece24118a4",
      "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
      "fullname": "wuyuhao",
      "name": "mozhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10582",
      "authors": [
        {
          "_id": "67d387ff45b17e31c16d05d1",
          "name": "Yiming Jia",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d2",
          "name": "Jiachen Li",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d3",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d4",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d5",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d6",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d7",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-14T01:36:13.720Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/VBzj4fQkEBEzfx26BsANS.png"
      ],
      "publishedAt": "2025-03-13T17:32:48.000Z",
      "submittedOnDailyAt": "2025-03-14T00:47:38.699Z",
      "title": "VisualWebInstruct: 웹 검색에 의한 다모달 명령 데이터의 확장",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해, 비슷한 주제의 데이터셋의 부족으로 인해,",
      "upvotes": 9,
      "discussionId": "67d3880d45b17e31c16d09d1",
      "projectPage": "https://tiger-ai-lab.github.io/VisualWebInstruct/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VisualWebInstruct",
      "ai_keywords": [
        "Vision-Language Models",
        "VisualWebInstruct",
        "search engine",
        "question-answer pairs",
        "visual QA pairs",
        "text QA pairs",
        "fine-tuned",
        "Llava-OV-mid",
        "MAmmoTH-VL",
        "MAmmoTH-VL2",
        "MMMU-Pro-std",
        "MathVerse",
        "DynaMath"
      ]
    },
    "publishedAt": "2025-03-13T13:32:48.000Z",
    "title": "VisualWebInstruct: Scaling up Multimodal Instruction Data through Web\n  Search",
    "summary": "Vision-Language Models have made significant progress on many\nperception-focused tasks, however, their progress on reasoning-focused tasks\nseem to be limited due to the lack of high-quality and diverse training data.\nIn this work, we aim to address the scarcity issue of reasoning-focused\nmultimodal datasets. We propose VisualWebInstruct - a novel approach that\nleverages search engine to create a diverse, and high-quality dataset spanning\nmultiple disciplines like math, physics, finance, chemistry, etc. Starting with\nmeticulously selected 30,000 seed images, we employ Google Image search to\nidentify websites containing similar images. We collect and process the HTMLs\nfrom over 700K unique URL sources. Through a pipeline of content extraction,\nfiltering and synthesis, we build a dataset of approximately 900K\nquestion-answer pairs, with 40% being visual QA pairs and the rest as text QA\npairs. Models fine-tuned on VisualWebInstruct demonstrate significant\nperformance gains: (1) training from Llava-OV-mid shows 10-20% absolute point\ngains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain.\nOur best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B\nparameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath\n(55.7%). These remarkable results highlight the effectiveness of our dataset in\nenhancing VLMs' reasoning capabilities for complex multimodal tasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/VBzj4fQkEBEzfx26BsANS.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10582.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10618",
      "authors": [
        {
          "_id": "67d3d2dec4a225b653154b3a",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3b",
          "name": "Rui Qian",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3c",
          "name": "Wenze Hu",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3d",
          "name": "Tsu-Jui Fu",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3e",
          "name": "Lezhi Li",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3f",
          "name": "Bowen Zhang",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b40",
          "name": "Alex Schwing",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b41",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b42",
          "name": "Yinfei Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:57:25.000Z",
      "submittedOnDailyAt": "2025-03-14T05:26:46.588Z",
      "title": "DiT-Air: 효율성을 재평가하여, 텍스트로부터 이미지 생성에 대한 설계에 관한 연구\n\n(注意: 이 번역은 원본 문장의 구조와 의미 그대로 번역되었고, 'DiT-Air'는 특정 연구나 모델의 이름으로 사용되어, 번역에는 변경하지 않았습니다.)",
      "submittedOnDailyBy": {
        "_id": "656c2fa772c19de72367bd69",
        "avatarUrl": "/avatars/540bb3d8a2afe2ef927b80d895cae28b.svg",
        "isPro": false,
        "fullname": "Alex Yang",
        "user": "yyf86",
        "type": "user"
      },
      "summary": "이 연구에서는 Diffusion Transformers (DiTs)를 사용하여 텍스트로부터 이미지 생성에 대한 실험적 연구를 수행하며, 구조적 선택, 텍스트 조건화 전략, 훈련 프로토콜에 초점을 맞추었습니다. 다양한 DiTs 기반 구조를 평가하고, 표준 DiTs 기반 구조는 직접 결합된 텍스트와 노이즈 입력을 처리하는 구조로 비교하였습니다. 놀라울 정도로 표준 DiTs의 성능은 특화된 모델과 비교하여 상대적으로 높은 것을 알 수 있었으며, 특히 파라미터의 효율성이 높은 것을 알 수 있었습니다. 계층별 파라미터 공유 전략을 활용하여, MMDiT 구조 대비 66%의 모델 크기 감소를 달성하였으며, 최소한의 성능 영향을 입히도록 하였습니다. 텍스트 엔코더와 Variational Auto-Encoders (VAEs) 등 중요한 구성 요소에 대한 상세한 분석을 기반으로 DiT-Air와 DiT-Air-Lite를 소개하였습니다. DiT-Air는 서브젝트와 reward의 미세 조정을 사용하여 GenEval과 T2I CompBench에서 가장 先端의 성능을 달성하였으며, DiT-Air-Lite는 작은 크기에서도 높은 경쟁력을 나타내며, 현재의 모델을 초과합니다.",
      "upvotes": 8,
      "discussionId": "67d3d302c4a225b6531556d6",
      "ai_keywords": [
        "Diffusion Transformers (DiTs)",
        "text-to-image generation",
        "architectural choices",
        "text-conditioning strategies",
        "training protocols",
        "PixArt-style",
        "MMDiT variants",
        "concatenated text and noise inputs",
        "parameter-efficiency",
        "layer-wise parameter sharing strategy",
        "Variational Auto-Encoders (VAEs)",
        "DiT-Air",
        "DiT-Air-Lite",
        "supervised and reward fine-tuning",
        "GenEval",
        "T2I CompBench",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-03-13T13:57:25.000Z",
    "title": "DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture\n  Design in Text to Image Generation",
    "summary": "In this work, we empirically study Diffusion Transformers (DiTs) for\ntext-to-image generation, focusing on architectural choices, text-conditioning\nstrategies, and training protocols. We evaluate a range of DiT-based\narchitectures--including PixArt-style and MMDiT variants--and compare them with\na standard DiT variant which directly processes concatenated text and noise\ninputs. Surprisingly, our findings reveal that the performance of standard DiT\nis comparable with those specialized models, while demonstrating superior\nparameter-efficiency, especially when scaled up. Leveraging the layer-wise\nparameter sharing strategy, we achieve a further reduction of 66% in model size\ncompared to an MMDiT architecture, with minimal performance impact. Building on\nan in-depth analysis of critical components such as text encoders and\nVariational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With\nsupervised and reward fine-tuning, DiT-Air achieves state-of-the-art\nperformance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly\ncompetitive, surpassing most existing models despite its compact size.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656c2fa772c19de72367bd69",
      "avatarUrl": "/avatars/540bb3d8a2afe2ef927b80d895cae28b.svg",
      "fullname": "Alex Yang",
      "name": "yyf86",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10589",
      "authors": [
        {
          "_id": "67d3adc5c14480a46038cecf",
          "name": "Yuwei Guo",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced0",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced1",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced2",
          "name": "Zhibei Ma",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced3",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced4",
          "name": "Zhenheng Yang",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced5",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced6",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:40:07.000Z",
      "submittedOnDailyAt": "2025-03-14T02:47:38.511Z",
      "title": "장문맥 조정에 의한 영화 생성",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "최근의 이미지 생성 기술은 스케일러블な 분산 트랜스포머를 사용하여 1분간 현실적인 단일 스라쉬 이미지 생성이 가능해졌다. 그러나 실제 세계적인 노리티브 이미지는 시각적 및 동적인 일관성을 요구하는 여러 스라쉬 시나리오가 필요합니다. 본 논문에서는 장기 컨텍스트 튜닝(LCT)을 도입하여 미리 학습된 단일 스라쉬 이미지의 분산 모델의 컨텍스트 윈도우를 확장하고, 데이터로부터 직접적으로 시나리오 수준의 일관성을 학습하는 튜닝 패러다임을 제안합니다. 우리 방법은 각 단일 스라쉬의 전체 注意 기능에 대해, 각 시나리오 내의 모든 스라쉬를 확장하여 3D 위치 매핑과 비동기 노이즈 스태지를 조합하여 추가 파라미터를 포함하지 않도록, 공통적이고 자동 리턴 가능한 스라쉬 생성을 가능하게 합니다. LCT 후의 바이디렉션 어텐션을 가지는 모델은 컨텍스트 카우스 어텐션에서 최종 튜닝을 수행하여 효율적인 KV 캐시를 사용한 자동 리턴 가능한 생성을 가능하게 합니다. 실험은 LCT 후의 단일 스라쉬 모델이 일관된 여러 스라쉬 시나리오를 생성하고, 구조적 생성 및 상호작용 쇼 확장 등 새로운 능력을 보여주는 것을 보여주며, 실용적인 시각 콘텐츠의 제작에 대한 길을 열어줍니다. 자세한 내용은 https://guoyww.github.io/projects/long-context-video/ 를 참고하세요.",
      "upvotes": 6,
      "discussionId": "67d3adc7c14480a46038cf5e",
      "ai_keywords": [
        "scalable diffusion transformers",
        "context window",
        "scene-level consistency",
        "Long Context Tuning (LCT)",
        "full attention mechanisms",
        "interleaved 3D position embedding",
        "asynchronous noise strategy",
        "joint and auto-regressive shot generation",
        "bidirectional attention",
        "context-causal attention",
        "efficient KV-cache",
        "compositional generation",
        "interactive shot extension"
      ]
    },
    "publishedAt": "2025-03-13T13:40:07.000Z",
    "title": "Long Context Tuning for Video Generation",
    "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10437",
      "authors": [
        {
          "_id": "67d3adf57360ea908cf5f0bc",
          "name": "Wanhua Li",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0bd",
          "user": {
            "_id": "66f0d2036a483077eed42bfb",
            "avatarUrl": "/avatars/f7f3f726842c26b8e52c9bdd48774b8e.svg",
            "isPro": false,
            "fullname": "Renping Zhou",
            "user": "rpzhou",
            "type": "user"
          },
          "name": "Renping Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:08.159Z",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0be",
          "name": "Jiawei Zhou",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0bf",
          "name": "Yingwei Song",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c0",
          "name": "Johannes Herter",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c1",
          "name": "Minghan Qin",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c2",
          "name": "Gao Huang",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c3",
          "name": "Hanspeter Pfister",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/H5DUFDgeiid82tHVuW9cx.mp4"
      ],
      "publishedAt": "2025-03-13T14:58:22.000Z",
      "submittedOnDailyAt": "2025-03-14T02:49:34.475Z",
      "title": "4D LangSplat: 4D 언어 가우스 스플랫을 다양화 레이징 대 언어 모델에 의해 구현",
      "submittedOnDailyBy": {
        "_id": "658bb7e47459b6e471b9d2e6",
        "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
        "isPro": false,
        "fullname": "Wanhua Li",
        "user": "EthanTaylor",
        "type": "user"
      },
      "summary": "4D 언어 영역의 학습은 동적인 스케인에서 시간적适应적인, 개방적인 언어 쿼리를 실현하기 위해 필수적이다. LangSplat은 CLIP 특성을 3D 가우시안 표현에 기반하여 3D静的 스케인에서 정확성과 효율성을 달성했지만, CLIP는静的 이미지 텍스트 태스크를 위한 것이므로 영화의 시간적 동작을 파악할 수 없게 된다. 현실적인 환경은 물체의 의미가 시간에 따라 변화하여 동적으로 작동한다. 4D 언어 영역의 정확한 구축에는 현재의 시각 모델이 어려운 픽셀 대응의 물체의 비디오 특성을 얻는 것이 필요합니다. 이러한 문제를 해결하기 위해 4D LangSplat을 제안합니다. 4D LangSplat은 4D 언어 영역을 학습하여 동적인 스케인에서 시간적适应적인 또는 시간적 관계없는 개방적인 봇어 쿼리를 효율적으로 처리하는 것을 목표로하고 있습니다. 4D LangSplat은 시각 특성으로부터 언어 영역의 학습을 피하고 물체의 비디오 캡쳐로부터 생성된 텍스트를 모델로 하여 Multimodal Large Language Models (MLLMs)에서 직접 학습합니다. 특히, 우리는 비디오 프롬프트 메소드를 제안하며, 이는 시각 및 텍스트 프롬프트를 사용하여 MLLMs를 깊은, 시간적인, 고품질의 캡쳐를 생성하는 것입니다. 이러한 캡쳐는 Large Language Model을 사용하여 프레임에 맞는, 물체의 특성을 지원하고 공유된 임베딩 공간을 통해 개방적인 봇어 쿼리를 실현합니다. 4D 스케인의 물체는 상태 간에 평활한 이동을 나타냅니다. 따라서, 우리는 상태 변형 네트워크를 제안하여 시간적인 변화를 효과적으로 모델링하는 것입니다. 많은 벤치마크에서의 결과를 통해 4D LangSplat은 시간적适应적인 또는 시간적 관계없는 개방적인 봇어 쿼리에 대해 정확하고 효율적인 결과를 구현했습니다.",
      "upvotes": 6,
      "discussionId": "67d3adf87360ea908cf5f182",
      "ai_keywords": [
        "4D language fields",
        "time-sensitive queries",
        "open-ended language queries",
        "LangSplat",
        "CLIP features",
        "3D Gaussian representations",
        "dynamic 4D fields",
        "temporal dynamics",
        "pixel-aligned",
        "object-wise video features",
        "4D LangSplat",
        "time-agnostic queries",
        "Multimodal Large Language Models (MLLMs)",
        "multimodal object-wise video prompting",
        "visual prompts",
        "text prompts",
        "detailed captions",
        "temporally consistent captions",
        "sentence embeddings",
        "shared embedding spaces",
        "status deformable network",
        "continuous changes"
      ]
    },
    "publishedAt": "2025-03-13T10:58:22.000Z",
    "title": "4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large\n  Language Models",
    "summary": "Learning 4D language fields to enable time-sensitive, open-ended language\nqueries in dynamic scenes is essential for many real-world applications. While\nLangSplat successfully grounds CLIP features into 3D Gaussian representations,\nachieving precision and efficiency in 3D static scenes, it lacks the ability to\nhandle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot\ncapture temporal dynamics in videos. Real-world environments are inherently\ndynamic, with object semantics evolving over time. Building a precise 4D\nlanguage field necessitates obtaining pixel-aligned, object-wise video\nfeatures, which current vision models struggle to achieve. To address these\nchallenges, we propose 4D LangSplat, which learns 4D language fields to handle\ntime-agnostic or time-sensitive open-vocabulary queries in dynamic scenes\nefficiently. 4D LangSplat bypasses learning the language field from vision\nfeatures and instead learns directly from text generated from object-wise video\ncaptions via Multimodal Large Language Models (MLLMs). Specifically, we propose\na multimodal object-wise video prompting method, consisting of visual and text\nprompts that guide MLLMs to generate detailed, temporally consistent,\nhigh-quality captions for objects throughout a video. These captions are\nencoded using a Large Language Model into high-quality sentence embeddings,\nwhich then serve as pixel-aligned, object-specific feature supervision,\nfacilitating open-vocabulary text queries through shared embedding spaces.\nRecognizing that objects in 4D scenes exhibit smooth transitions across states,\nwe further propose a status deformable network to model these continuous\nchanges over time effectively. Our results across multiple benchmarks\ndemonstrate that 4D LangSplat attains precise and efficient results for both\ntime-sensitive and time-agnostic open-vocabulary queries.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/H5DUFDgeiid82tHVuW9cx.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10437.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658bb7e47459b6e471b9d2e6",
      "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
      "fullname": "Wanhua Li",
      "name": "EthanTaylor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10357",
      "authors": [
        {
          "_id": "67d3da3ce0b767b3d0fae2a4",
          "user": {
            "_id": "63bbfd74141c7d395c471768",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
            "isPro": false,
            "fullname": "Viktor Moskvoretskii",
            "user": "VityaVitalich",
            "type": "user"
          },
          "name": "Viktor Moskvoretskii",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T07:38:54.624Z",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a5",
          "name": "Alina Lobanova",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a6",
          "name": "Ekaterina Neminova",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a7",
          "name": "Chris Biemann",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a8",
          "name": "Alexander Panchenko",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a9",
          "name": "Irina Nikishina",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63bbfd74141c7d395c471768/7wZapqnRg87iwXLqxiKQL.jpeg"
      ],
      "publishedAt": "2025-03-13T13:37:54.000Z",
      "submittedOnDailyAt": "2025-03-14T05:58:28.512Z",
      "title": "私는 `cat.n.01` 과 같은 것을 보시나요? 분류학 이미지 생성 벤치마크",
      "submittedOnDailyBy": {
        "_id": "63bbfd74141c7d395c471768",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
        "isPro": false,
        "fullname": "Viktor Moskvoretskii",
        "user": "VityaVitalich",
        "type": "user"
      },
      "summary": "이 논문은 0shot set에서 텍스트를 사용하여 이미지 모델을 생성하여 텍스토미즈 개념의 이미지를 생성하는 가능성을 조사합니다. 텍스토미즈의 확장에 있어서 텍스트 기반의 방법은 이미 확립되어 있습니다만, 시각적인 차원의 가능성은 아직 조사되지 않았습니다. 이에 대해 우리는 텍스토미즈 이미지 생성의 평가기준을 제안합니다. 이 평가기준은 모델이 텍스토미즈 개념을 이해하고 관련성이 있는 고품질의 이미지를 생성하는 능력을 평가합니다. 평가기준에는 일반 지식과 랜덤으로 샘플링된 WordNet 개념, LLM이 생성한 예측도 포함됩니다. 12개의 모델은 9가지 새로운 텍스토미즈 관련 텍스트에서 이미지로 평가기준과 인간의 피드백을 사용하여 평가되었습니다. 또한, GPT-4의 피드백을 사용하여 비교적인 평가를 처음 도입하고 이미지 생성을 수행합니다. 실험 결과를 통해 모델의 순위는 표준적인 T2I 작업과 유의미하게 다르습니다. Playground-v2와 FLUX는 다양한 메트릭과 서브셋에서 일관되게 상위에 위치하며, 검색 기반의 접근法是 불량입니다. 이러한 발견은 구조화된 데이터 리소스의 자동화 칼레이팅의 가능성에 대한 것을 명확히 합니다.",
      "upvotes": 6,
      "discussionId": "67d3da42e0b767b3d0fae455",
      "projectPage": "https://huggingface.co/collections/VityaVitalich/generated-image-wordnet-67d2c868ff1414ec2f8e0d3d",
      "ai_keywords": [
        "text-to-image models",
        "zero-shot setup",
        "taxonomy concepts",
        "text-based methods",
        "taxonomy enrichment",
        "comprehensive benchmark",
        "WordNet",
        "LLM generated predictions",
        "taxonomy-related text-to-image metrics",
        "pairwise evaluation",
        "GPT-4 feedback",
        "image generation",
        "ranking of models",
        "retrieval-based approach",
        "automating the curation of structured data resources"
      ]
    },
    "publishedAt": "2025-03-13T09:37:54.000Z",
    "title": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark",
    "summary": "This paper explores the feasibility of using text-to-image models in a\nzero-shot setup to generate images for taxonomy concepts. While text-based\nmethods for taxonomy enrichment are well-established, the potential of the\nvisual dimension remains unexplored. To address this, we propose a\ncomprehensive benchmark for Taxonomy Image Generation that assesses models'\nabilities to understand taxonomy concepts and generate relevant, high-quality\nimages. The benchmark includes common-sense and randomly sampled WordNet\nconcepts, alongside the LLM generated predictions. The 12 models are evaluated\nusing 9 novel taxonomy-related text-to-image metrics and human feedback.\nMoreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for\nimage generation. Experimental results show that the ranking of models differs\nsignificantly from standard T2I tasks. Playground-v2 and FLUX consistently\noutperform across metrics and subsets and the retrieval-based approach performs\npoorly. These findings highlight the potential for automating the curation of\nstructured data resources.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63bbfd74141c7d395c471768/7wZapqnRg87iwXLqxiKQL.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10357.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbfd74141c7d395c471768",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
      "fullname": "Viktor Moskvoretskii",
      "name": "VityaVitalich",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09642",
      "authors": [
        {
          "_id": "67d3ab8d032b54ab876cb7ec",
          "name": "Xiangyu Peng",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ed",
          "name": "Zangwei Zheng",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ee",
          "name": "Chenhui Shen",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ef",
          "name": "Tom Young",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f0",
          "name": "Xinying Guo",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f1",
          "name": "Binluo Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f2",
          "name": "Hang Xu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f3",
          "name": "Hongxin Liu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f4",
          "name": "Mingyan Jiang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f5",
          "name": "Wenjun Li",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f6",
          "name": "Yuhui Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f7",
          "name": "Anbang Ye",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f8",
          "name": "Gang Ren",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f9",
          "name": "Qianran Ma",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fa",
          "name": "Wanying Liang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fb",
          "name": "Xiang Lian",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fc",
          "name": "Xiwen Wu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fd",
          "name": "Yuting Zhong",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fe",
          "name": "Zhuangyan Li",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ff",
          "name": "Chaoyu Gong",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb800",
          "name": "Guojun Lei",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb801",
          "name": "Leijun Cheng",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb802",
          "name": "Limin Zhang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb803",
          "name": "Minghao Li",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb804",
          "name": "Ruijie Zhang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb805",
          "name": "Silan Hu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb806",
          "name": "Shijie Huang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb807",
          "name": "Xiaokang Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb808",
          "name": "Yuanheng Zhao",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb809",
          "name": "Yuqi Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb80a",
          "name": "Ziang Wei",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb80b",
          "name": "Yang You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T05:00:07.000Z",
      "submittedOnDailyAt": "2025-03-14T02:39:06.938Z",
      "title": "Open-Sora 2.0: 20만 달러로 상업 수준의 비디오 생성 모델의 훈련\n\n(注意：虽然用户要求不添加解释或额外文本，但为了确保翻译的准确性和专业性，这里使用了最直接的翻译方式。如果需要进一步的解释或调整，请告知。)",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "비디오 생성 모델은 지난 1년 동안 놀라운 진전을 거쳤습니다. AI 비디오의 품질이 향상되었지만, 대신 모델 크기가 커지고 데이터량이 증가하고 훈련 계산량의 요구가 증가하고 있습니다. 이 보고서에서는, Commercial Level의 비디오 생성 모델인 Open-Sora 2.0를 소개합니다. 이 모델은 $200k로 훈련되었습니다. 이 모델을 사용하여, TOP Performance의 비디오 생성 모델의 훈련 비용은 크게 제어할 수 있음을 보여줍니다. 이 효율 향상의 모든 기술에 대해 자세히 설명합니다. 이는 데이터 캐리톰, 모델 아키텍처, 훈련 전략, 시스템 최적화에 포함됩니다. 인간 평가 결과와 VBench 점수를 기준으로, Open-Sora 2.0는 오픈 소스의 HunyuanVideo와 클로즈드 소스의 Runway Gen-3 Alpha를 포함한 세계적인 비디오 생성 모델과 비교됩니다. Open-Sora 2.0를 완전히 오픈 소스로 만들고, 발전된 비디오 생성 기술의 접근성을 민주화하고, 콘텐츠 제작의 광범위한 혁신과 창의성을 촉진하는 것을 목표로 합니다. 모든 리소스는 https://github.com/hpcaitech/Open-Sora에서 공개되어 있습니다.",
      "upvotes": 6,
      "discussionId": "67d3ab93032b54ab876cb9b0",
      "ai_keywords": [
        "video generation models",
        "data curation",
        "model architecture",
        "training strategy",
        "system optimization",
        "human evaluation",
        "VBench scores",
        "HunyuanVideo",
        "Runway Gen-3 Alpha",
        "open-source"
      ]
    },
    "publishedAt": "2025-03-12T01:00:07.000Z",
    "title": "Open-Sora 2.0: Training a Commercial-Level Video Generation Model in\n  $200k",
    "summary": "Video generation models have achieved remarkable progress in the past year.\nThe quality of AI video continues to improve, but at the cost of larger model\nsize, increased data quantity, and greater demand for training compute. In this\nreport, we present Open-Sora 2.0, a commercial-level video generation model\ntrained for only $200k. With this model, we demonstrate that the cost of\ntraining a top-performing video generation model is highly controllable. We\ndetail all techniques that contribute to this efficiency breakthrough,\nincluding data curation, model architecture, training strategy, and system\noptimization. According to human evaluation results and VBench scores,\nOpen-Sora 2.0 is comparable to global leading video generation models including\nthe open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By\nmaking Open-Sora 2.0 fully open-source, we aim to democratize access to\nadvanced video generation technology, fostering broader innovation and\ncreativity in content creation. All resources are publicly available at:\nhttps://github.com/hpcaitech/Open-Sora.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09642.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10630",
      "authors": [
        {
          "_id": "67d39f4828221b583a33be2c",
          "name": "Hang Yin",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be2d",
          "name": "Xiuwei Xu",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be2e",
          "name": "Lingqing Zhao",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be2f",
          "name": "Ziwei Wang",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be30",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be31",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:48.000Z",
      "submittedOnDailyAt": "2025-03-14T01:45:48.071Z",
      "title": "UniGoal: 목표를 향해 일반적인 Zero-shot Goal-oriented Navigation으로\n\n(Note: The translation provided is a direct literal translation of the given text. In a professional context, it would be important to ensure that the translation accurately reflects the intended meaning and context of the original text. If there are specific terms or concepts that need to be accurately translated, please provide additional context or clarification.)",
      "submittedOnDailyBy": {
        "_id": "648ac65fd044b25978015634",
        "avatarUrl": "/avatars/2278a66fdc953220e9f8fc0ccce3ff00.svg",
        "isPro": false,
        "fullname": "Xiuwei Xu",
        "user": "xuxw98",
        "type": "user"
      },
      "summary": "이 논문에서는 일반적인 0차원 목표지향적인 지도 학습의 일반화된 프레임워크를 제안합니다. 현재의 0차원 방법들은 특정한 태스크에 대해 큰 언어 모델(LLM)을 기반으로 추론 프레임워크를 구축하고, 전체적인 파이프라인이 크게 다릅니다. 또한, 서로 다른 목표 간의 일반화가 불가능합니다. 일반적인 0차원 지도 학습의 목적에 맞춰, 서로 다른 목표를 통일하기 위해 일관된 그래프 표현을 제안하고, 객체 카테고리, 인스턴스 이미지, 텍스트 설명을 포함합니다. 또한, 출력의 관찰을 온라인으로 유지하는 시나리오 그래프를 변환하고, 이 일관된 시나리오와 목표의 표현을 사용하여 텍스트보다 많은 구조적 정보를 보존하며, LLM을 그래프 기반으로 활용할 수 있도록 합니다. 특히, 각 시간 단계에서 시나리오 그래프와 목표 그래프의 그래프 매칭을 수행하고, 서로 다른 매칭 상태에 따라 장기 목표의 탐색을 위한 다양한 전략을 제안합니다. 출력은 0차원 매칭의 경우 목표의 부분 그래프를 반복적으로 탐색하고, 부분 매칭의 경우 좌표 프로젝션과 아너 페어 어레이먼트를 사용하여 목표의 위치를 추정합니다. 마지막으로, 시나리오 그래프 수정과 목표 확인을 적용하여 완전한 매칭을 구현합니다. 또한, 순서의 적절한 변경을 가능하게 하기 위해 블랙리스트 기능을 도입합니다. 여러 수의 벤치마크에서 확장된 실험을 통해, 우리 UniGoal은 3가지 연구된 지도 학습 태스크에 대해 하나의 모델로 가장 先端的 0차원 성능을 달성하고, 태스크专用의 0차원 방법과 관찰적인 일반적인 방법을 초월할 수 있습니다.",
      "upvotes": 5,
      "discussionId": "67d39f4928221b583a33be7f",
      "ai_keywords": [
        "universal zero-shot goal-oriented navigation",
        "zero-shot methods",
        "large language models (LLM)",
        "uniform graph representation",
        "object category",
        "instance image",
        "text description",
        "scene graph",
        "explicit graph-based reasoning",
        "graph matching",
        "long-term goal of exploration",
        "subgraph search",
        "coordinate projection",
        "anchor pair alignment",
        "scene graph correction",
        "goal verification",
        "blacklist mechanism",
        "UniGoal"
      ]
    },
    "publishedAt": "2025-03-13T13:59:48.000Z",
    "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
    "summary": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10630.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648ac65fd044b25978015634",
      "avatarUrl": "/avatars/2278a66fdc953220e9f8fc0ccce3ff00.svg",
      "fullname": "Xiuwei Xu",
      "name": "xuxw98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10460",
      "authors": [
        {
          "_id": "67d3a87df6ea55297c3cd071",
          "name": "Liang Wen",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd072",
          "name": "Yunke Cai",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd073",
          "name": "Fenrui Xiao",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd074",
          "name": "Xin He",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd075",
          "name": "Qi An",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd076",
          "name": "Zhenyu Duan",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd077",
          "name": "Yimin Du",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd078",
          "name": "Junchen Liu",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd079",
          "name": "Lifu Tang",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07a",
          "name": "Xiaowei Lv",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07b",
          "name": "Haosheng Zou",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07c",
          "name": "Yongchao Deng",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07d",
          "name": "Shousheng Jia",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07e",
          "name": "Xiangzheng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T15:29:22.000Z",
      "submittedOnDailyAt": "2025-03-14T02:25:27.538Z",
      "title": "Light-R1: カリキュラムSFT, DPO와 RL은 스크라ッチから 장기적인 COT보다 나은 성능을 보입니다.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "이 논문에서는 Light-R1 시리즈의 연구 결과를 소개합니다. 모델, 데이터, 코드 모두 공개되었습니다.\n\n먼저, 장기 COT 모델의 시작부터 학습에 초점을 맞추겠습니다. 특히, 초기에 장기 COT 능력이 없는 모델에서 시작하는 것이 중요하다고 주장합니다. 2 단계의 SFT와 반 파라미터 정책 DPO를 구성한 커리큘럼 훈련 레시피를 사용함으로써, Qwen2.5-32B-Instruct에서 Light-R1-32B 모델을 학습하였으며, DeepSeek-R1-Distill-Qwen-32B와 비교하여 수학 성능이 상위에 위치했습니다. 수학 데이터만 학습된 Light-R1-32B는 다른 분야에서도 강력한 일반화 성능을 보여주었습니다.\n\n다음 단계에서는, 2 번째 SFT 단계에 구축된 3k 데이터 세트의 중요성을 주장하고, 이 데이터 세트를 사용하여 DeepSeek-R1-Distilled 모델을 미세 조정하여 새로운 SOTA 모델을 얻었습니다. 7B과 14B 모델에 대해, 32B 모델인 Light-R1-32B-DS는 QwQ-32B와 DeepSeek-R1과 동일한 성능을 나타냅니다.\n\n또한, 장기 COT 모델에 강화 학습, 특히 GRPO를 적용하여 추론 성능을 개선하는 시도를 했습니다. 최종적인 Light-R1-14B-DS 모델은 RL을 사용하여 학습되었으며, 수학 분야에서 14B 파라미터 모델의 SOTA 성능을 달성했습니다. AIME24와 25의 점수는 각각 74.0과 60.2로, Light-R1-14B-DS는 32B 모델이나 DeepSeek-R1-Distill-Llama-70B을 초월했습니다. 또한, RL 학습에 의한 기대하는 행동을 보여주며, 응답의 길이와 보상 점수가 동시에 증가했습니다.\n\nLight-R1 시리즈의 연구는 장기 COT 모델의 시작부터 학습을 증명하고, SFT 데이터의 예술성을 보여주고, RL을 사용한 SOTA 모델의 공개를 실현했습니다.",
      "upvotes": 5,
      "discussionId": "67d3a87ef6ea55297c3cd0d0",
      "ai_keywords": [
        "COT models",
        "curriculum training",
        "two-stage SFT",
        "semi-on-policy DPO",
        "long COT capabilities",
        "SOTA",
        "generalization",
        "3k dataset",
        "fine-tuning",
        "GRPO",
        "reasoning performance",
        "AIME24 & 25 scores",
        "response length",
        "reward score"
      ]
    },
    "publishedAt": "2025-03-13T11:29:22.000Z",
    "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond",
    "summary": "This paper presents our work on the Light-R1 series, with models, data, and\ncode all released.\n  We first focus on training long COT models from scratch, specifically\nstarting from models initially lacking long COT capabilities. Using a\ncurriculum training recipe consisting of two-stage SFT and semi-on-policy DPO,\nwe train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in\nsuperior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite\nbeing trained exclusively on math data, Light-R1-32B shows strong\ngeneralization across other domains. In the subsequent phase of this work, we\nhighlight the significant benefit of the 3k dataset constructed for the second\nSFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled\nmodels using this dataset, we obtain new SOTA models in 7B and 14B, while the\n32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.\n  Furthermore, we extend our work by applying reinforcement learning,\nspecifically GRPO, on long-COT models to further improve reasoning performance.\nWe successfully train our final Light-R1-14B-DS with RL, achieving SOTA\nperformance among 14B parameter models in math. With AIME24 & 25 scores of 74.0\nand 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and\nDeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected\nbehavior, showing simultaneous increase in response length and reward score.\n  The Light-R1 series of work validates training long-COT models from scratch,\nshowcases the art in SFT data and releases SOTA models from RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10460.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09905",
      "authors": [
        {
          "_id": "67d393660d51cf27930a5e5d",
          "user": {
            "_id": "67d3930e4d3a41ed9f7ac71e",
            "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
            "isPro": false,
            "fullname": "Allison Andreyev",
            "user": "allisonandreyev",
            "type": "user"
          },
          "name": "Allison Andreyev",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T02:25:50.445Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T23:50:35.000Z",
      "submittedOnDailyAt": "2025-03-14T00:57:22.699Z",
      "title": "OpenAI의 Whisper 모델의 디지털화: 비교적 분석",
      "submittedOnDailyBy": {
        "_id": "67d3930e4d3a41ed9f7ac71e",
        "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
        "isPro": false,
        "fullname": "Allison Andreyev",
        "user": "allisonandreyev",
        "type": "user"
      },
      "summary": "自動음성 인식 모델(ASR)는 캡처, 언어 번역, 라이브 텍스트화 등 다양한 응용 분야에서 중요한 역할을 수행하고 있습니다. 본 논문에서는 Whisper 모델과 2가지 모델의 변체를 연구하고 있습니다: 라이브 스피크 스트리밍을 위한 모델과 오프라인 텍스트화를 위한 모델입니다. 특히, 이러한 모델들은 오류 읽기나 가상 콘텐츠의 생성으로 텍스트화의 신뢰도가 떨어지는 것을 관찰할 수 있습니다. 또한, 큰 모델은 라디언스 컨스트레인된 디바이스에서 처리될 때 라디언스의 증가와 처리의 문제점이 있습니다. 본 논문에서는 3가지 Whisper 모델의 유사점과 차이점을 분석하고 질적 평가를 수행합니다. 다음으로, 모델의 데이터 처리 전략이 라디언스에 어떻게 영향을 미치는지 정량적으로 평가하고 에지 디바이스의 처리 가능성을 평가합니다. LibriSpeech 데이터 세트를 사용하여 INT4, INT5, INT8 등 3가지 데이터 처리 방법을 사용하여 Whispercpp의 단어 오류율(WER)와 라디언스의 분석을 수행합니다. 결과적으로, 데이터 처리는 라디언스를 19% 줄이고 모델 크기를 45% 줄이면서 동시에 텍스트화 정확도를 유지하는 것을 알 수 있습니다. 이러한 결과를 통해 다양한 Whisper 모델의 최적 사용 사례와 에지 디바이스의 처리 가능성을 제공합니다. 모든 코드, 데이터 세트, 구현 세부 사항은 공개된 GitHub 리포지토리에 있습니다: https://github.com/allisonandreyev/WhisperQuantization.git",
      "upvotes": 5,
      "discussionId": "67d393670d51cf27930a5e98",
      "githubRepo": "https://github.com/allisonandreyev/WhisperQuantization",
      "ai_keywords": [
        "Whisper",
        "live speech streaming",
        "offline transcription",
        "hallucinated content",
        "latency",
        "deployment",
        "quantization",
        "LibriSpeech dataset",
        "word error rate (WER)",
        "whispercpp",
        "INT4",
        "INT5",
        "INT8",
        "speech recognition (ASR)",
        "transcription accuracy",
        "edge deployment"
      ]
    },
    "publishedAt": "2025-03-12T19:50:35.000Z",
    "title": "Quantization for OpenAI's Whisper Models: A Comparative Analysis",
    "summary": "Automated speech recognition (ASR) models have gained prominence for\napplications such as captioning, speech translation, and live transcription.\nThis paper studies Whisper and two model variants: one optimized for live\nspeech streaming and another for offline transcription. Notably, these models\nhave been found to generate hallucinated content, reducing transcription\nreliability. Furthermore, larger model variants exhibit increased latency and\npose challenges for deployment on resource-constrained devices. This study\nanalyzes the similarities and differences between three Whisper models,\nqualitatively examining their distinct capabilities. Next, this study\nquantifies the impact of model quantization on latency and evaluates its\nviability for edge deployment. Using the open source LibriSpeech dataset, this\npaper evaluates the word error rate (WER) along with latency analysis of\nwhispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that\nquantization reduces latency by 19\\% and model size by 45\\%, while preserving\ntranscription accuracy. These findings provide insights into the optimal use\ncases of different Whisper models and edge device deployment possibilities. All\ncode, datasets, and implementation details are available in a public GitHub\nrepository: https://github.com/allisonandreyev/WhisperQuantization.git",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67d3930e4d3a41ed9f7ac71e",
      "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
      "fullname": "Allison Andreyev",
      "name": "allisonandreyev",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09641",
      "authors": [
        {
          "_id": "67d3b008e18f86384bd33fa1",
          "name": "Junsong Chen",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa2",
          "name": "Shuchen Xue",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa3",
          "name": "Yuyang Zhao",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa4",
          "name": "Jincheng Yu",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa5",
          "user": {
            "_id": "5f7fbd813e94f16a85448745",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
            "isPro": false,
            "fullname": "Sayak Paul",
            "user": "sayakpaul",
            "type": "user"
          },
          "name": "Sayak Paul",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:06.327Z",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa6",
          "name": "Junyu Chen",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa7",
          "name": "Han Cai",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa8",
          "name": "Enze Xie",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa9",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T04:53:07.000Z",
      "submittedOnDailyAt": "2025-03-14T02:57:11.544Z",
      "title": "SANA-Sprint: 1스텝 디퓨저와 연속 시간의 일관성 경험적 전송",
      "submittedOnDailyBy": {
        "_id": "5f7fbd813e94f16a85448745",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
        "isPro": false,
        "fullname": "Sayak Paul",
        "user": "sayakpaul",
        "type": "user"
      },
      "summary": "이 논문에서는 효율적인 SANA-Sprint 모델을 소개하고, 초고속 텍스트로부터 이미지(T2I)를 생성하는 방법을 구현합니다. SANA-Sprint는 사전 학습된 기반 모델에 구축되어, 하이브리드 디스틸레이션을 추가하여 계산 시간(20 단계에서 1-4 단계까지)을 크게 줄입니다. 우리는 세 가지 핵심의 혁신을 소개합니다: (1) 무 학습 접근 방식을 제안하고, 연속 시간의 일치성을 보장하는 디스틸레이션(sCM)에 플로어 매칭 모델을 적용하여 시작부터의 고비용 학습을 제거하고, 높은 학습 효율을 실현합니다. 하이브리드 디스틸레이션 전략은 sCM와 잠재적인 상대적 대결 디스틸레이션(LADD)을 결합합니다: sCM는 교사 모델과의 조화를 보장하고, LADD는 1 단계의 생성의 신뢰성을 향상시킵니다. (2) SANA-Sprint는 1-4 단계에서 고품질의 생성을 구현하며, 단계별 학습을 제거하여 효율을 향상시킵니다. 이 연속적인 혁신은 속도와 품질의 트레이드오프에 새로운 패러도 프언트를 설정하고, 1 단계에서 7.59 FID와 0.74 GenEval의 최신 성능을 달성합니다(FLUX-schnell(7.94 FID / 0.71 GenEval)을 초과하며, H100에서 10배 빠르다(0.1s vs 1.1s)). 또한, H100에서 1024 x 1024 이미지의 T2I에서 0.1s, ControlNet에서 0.25s의 라틴 시를 구현하며, RTX 4090에서 T2I에서 0.31s를 달성하여, AI 기반의 소비자 애플리케이션(AIPC)의 기능과 잠재력을 보여주고 있습니다. 코드와 사전 학습된 모델은 오픈 소스로 공개됩니다.",
      "upvotes": 5,
      "discussionId": "67d3b00be18f86384bd3408f",
      "ai_keywords": [
        "diffusion model",
        "text-to-image (T2I)",
        "pre-trained foundation model",
        "hybrid distillation",
        "flow-matching model",
        "continuous-time consistency distillation (sCM)",
        "latent adversarial distillation (LADD)",
        "unified step-adaptive model",
        "ControlNet",
        "real-time interactive image generation",
        "FID",
        "GenEval",
        "FLUX-schnell",
        "H100",
        "RTX 4090",
        "AI-powered consumer applications (AIPC)"
      ]
    },
    "publishedAt": "2025-03-12T00:53:07.000Z",
    "title": "SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency\n  Distillation",
    "summary": "This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast\ntext-to-image (T2I) generation. SANA-Sprint is built on a pre-trained\nfoundation model and augmented with hybrid distillation, dramatically reducing\ninference steps from 20 to 1-4. We introduce three key innovations: (1) We\npropose a training-free approach that transforms a pre-trained flow-matching\nmodel for continuous-time consistency distillation (sCM), eliminating costly\ntraining from scratch and achieving high training efficiency. Our hybrid\ndistillation strategy combines sCM with latent adversarial distillation (LADD):\nsCM ensures alignment with the teacher model, while LADD enhances single-step\ngeneration fidelity. (2) SANA-Sprint is a unified step-adaptive model that\nachieves high-quality generation in 1-4 steps, eliminating step-specific\ntraining and improving efficiency. (3) We integrate ControlNet with SANA-Sprint\nfor real-time interactive image generation, enabling instant visual feedback\nfor user interaction. SANA-Sprint establishes a new Pareto frontier in\nspeed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID\nand 0.74 GenEval in only 1 step - outperforming FLUX-schnell (7.94 FID / 0.71\nGenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s\n(T2I) and 0.25s (ControlNet) latency for 1024 x 1024 images on H100, and 0.31s\n(T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for\nAI-powered consumer applications (AIPC). Code and pre-trained models will be\nopen-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09641.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f7fbd813e94f16a85448745",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
      "fullname": "Sayak Paul",
      "name": "sayakpaul",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 586
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10637",
      "authors": [
        {
          "_id": "67d3a2a6977358f62157977d",
          "user": {
            "_id": "636daf1b56c0762cfda074b5",
            "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
            "isPro": false,
            "fullname": "Rohit Gandikota",
            "user": "RohitGandikota",
            "type": "user"
          },
          "name": "Rohit Gandikota",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:16.089Z",
          "hidden": false
        },
        {
          "_id": "67d3a2a6977358f62157977e",
          "name": "David Bau",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:56.000Z",
      "submittedOnDailyAt": "2025-03-14T02:03:33.686Z",
      "title": "다양성과 제어를 위한 분산 모델의 활용",
      "submittedOnDailyBy": {
        "_id": "636daf1b56c0762cfda074b5",
        "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
        "isPro": false,
        "fullname": "Rohit Gandikota",
        "user": "RohitGandikota",
        "type": "user"
      },
      "summary": "蒸馏 Difﬁcultion 모델은 기초 모델과 비교하여 샘플의 다양성이 크게 감소하는 중대한 제한이 있습니다. 본 연구에서는 이러한 다양성의 감소에 대해서도,蒸馏 모델은 기초 모델의 기본적인 개념 표현을 유지하고 있음을 밝혀 냈습니다. 이를 통해, Control Distillation을 제안하고 기초 모델에서 훈련된 Control 구조(예: Concept Sliders와 LoRAs)이 무시되지 않고蒸馏 모델로 이동할 수 있으며, 반대로 동일한 효과를 발휘할 수 있음을 시사했습니다. 이러한 표현 구조의 보존으로,蒸馏 과정에서 다양성 붕괴 구조에 대한 조사를 시작했습니다. Difﬁcultion Target(DT) Visualization이라는 분석 및 디버깅 도구를 도입하여, 모델이 최종 출력을 예측하기 위한 중간 단계에서의 동작을 명확히 만들었습니다. DT-Visualization을 통해, 생성 피드백, 불연속성을 식별하고, 초기 Difﬁcultion 단계가 출력의 다양성을 불균형하게 결정하는 것을 시사하고, 후 단계는 주로 세부 사항을 정밀화하는 것을 시사했습니다. 이러한 관점에서, 다양성 Difﬁcultion Distillation이라는 하이브리드 추론 접근 방식을 도입하고, 기초 모델을 처음의 중요한 단계만 사용하여 효율적인蒸馏 모델로 이동하는 것을 전략적으로 시사했습니다. 이 간단한 변경은 기초 모델으로부터 다양성 기능을 회복하고 놀라운 수준으로 뛰어넘는 것을 시사하며, 동시에蒸馏 추론의 계산 효율을 유지하는 것을 시사하고, 추가적인 훈련이나 모델 변경이 필요하지 않도록 시사했습니다. 본 연구의 코드 및 데이터는 https://distillation.baulab.info에서 이용할 수 있습니다.",
      "upvotes": 4,
      "discussionId": "67d3a2ab977358f6215798fc",
      "projectPage": "https://distillation.baulab.info",
      "githubRepo": "https://github.com/rohitgandikota/distillation",
      "ai_keywords": [
        "distilled diffusion models",
        "sample diversity",
        "base models",
        "Concept Sliders",
        "LoRAs",
        "control distillation",
        "representational structure",
        "diversity collapse",
        "Diffusion Target (DT) Visualization",
        "intermediate steps",
        "generation artifacts",
        "inconsistencies",
        "diffusion timesteps",
        "diversity distillation",
        "hybrid inference approach"
      ]
    },
    "publishedAt": "2025-03-13T13:59:56.000Z",
    "title": "Distilling Diversity and Control in Diffusion Models",
    "summary": "Distilled diffusion models suffer from a critical limitation: reduced sample\ndiversity compared to their base counterparts. In this work, we uncover that\ndespite this diversity loss, distilled models retain the fundamental concept\nrepresentations of base models. We demonstrate control distillation - where\ncontrol mechanisms like Concept Sliders and LoRAs trained on base models can be\nseamlessly transferred to distilled models and vice-versa, effectively\ndistilling control without any retraining. This preservation of\nrepresentational structure prompted our investigation into the mechanisms of\ndiversity collapse during distillation. To understand how distillation affects\ndiversity, we introduce Diffusion Target (DT) Visualization, an analysis and\ndebugging tool that reveals how models predict final outputs at intermediate\nsteps. Through DT-Visualization, we identify generation artifacts,\ninconsistencies, and demonstrate that initial diffusion timesteps\ndisproportionately determine output diversity, while later steps primarily\nrefine details. Based on these insights, we introduce diversity distillation -\na hybrid inference approach that strategically employs the base model for only\nthe first critical timestep before transitioning to the efficient distilled\nmodel. Our experiments demonstrate that this simple modification not only\nrestores the diversity capabilities from base to distilled models but\nsurprisingly exceeds it, while maintaining nearly the computational efficiency\nof distilled inference, all without requiring additional training or model\nmodifications. Our code and data are available at\nhttps://distillation.baulab.info",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10637.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636daf1b56c0762cfda074b5",
      "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
      "fullname": "Rohit Gandikota",
      "name": "RohitGandikota",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10615",
      "authors": [
        {
          "_id": "67d3aa3f2f42ed5552e8ea0c",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea0d",
          "name": "Xiaoxuan He",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea0e",
          "name": "Hongkun Pan",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea0f",
          "name": "Xiyan Jiang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea10",
          "name": "Yan Deng",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea11",
          "name": "Xingtao Yang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea12",
          "name": "Haoyu Lu",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea13",
          "name": "Dacheng Yin",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea14",
          "name": "Fengyun Rao",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea15",
          "name": "Minfeng Zhu",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea16",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea17",
          "name": "Wei Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:56:05.000Z",
      "submittedOnDailyAt": "2025-03-14T02:32:20.499Z",
      "title": "R1-Onevision: 확장된 다모달 추론을 위한 크로스모달 형식화",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "대 언어 모델은 복잡한 문맥 태스크에서 뛰어난 논리력으로 뛰어난 성능을 보입니다. 그러나 시각정보와 텍스트 정보의 통합이 필요한 다모달 논리론은 여전히 큰 문제로 남아 있습니다. 현재의 시각 라벨 모델은 시각 콘텐츠의 유효한 분석과 논리론에 어려움을 겪으며, 복잡한 논리론 태스크에서 최적의 성능을 보여주는 경우가 많습니다. 또한 상세한 벤치마크의 부족은 다모달 논리론 능력의 정확한 평가에 방해되어 있습니다. 본 논문에서는 R1-Onevision라는 다모달 논리론 모델을 소개합니다. 이 모델은 시각 인식과 깊은 논리론 사이의 간극을 연결하기 위해 설계되었습니다. 이를 위해, 이미지를 공식적인 문맥 표현으로 변환하기 위한 교차모달 논리론 파이프라인을 제안합니다. 이 파이프라인을 활용하여, R1-Onevision 데이터셋을 구축하고, 다양한 분야에서 상세한 단계별 다모달 논리론 어노테이션을 제공합니다. 또한 R1-Onevision 모델은 정규화의 미세 조정과 강화 학습을 기반으로, 높은 논리력과 강한 일반화 능력을 키워줍니다. 다모달 논리론 성능을 종합적으로 평가하기 위해, 중학교부터 대학까지의 인간교육 단계에 맞는 R1-Onevision-Bench 벤치마크를 소개합니다. 실험 결과를 통해, R1-Onevision은 가장 先端의 성능을 달성하며, GPT-4o, Qwen2.5-VL 등 복잡한 다모달 논리론 벤치마크에서 성능을 초월하는 것을 확인했습니다.",
      "upvotes": 4,
      "discussionId": "67d3aa422f42ed5552e8eaee",
      "ai_keywords": [
        "multimodal reasoning",
        "cross-modal reasoning",
        "formal textural representations",
        "R1-Onevision dataset",
        "supervised fine-tuning",
        "reinforcement learning",
        "R1-Onevision-Bench",
        "GPT-4o",
        "Qwen2.5-VL"
      ]
    },
    "publishedAt": "2025-03-13T13:56:05.000Z",
    "title": "R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization",
    "summary": "Large Language Models have demonstrated remarkable reasoning capability in\ncomplex textual tasks. However, multimodal reasoning, which requires\nintegrating visual and textual information, remains a significant challenge.\nExisting visual-language models often struggle to effectively analyze and\nreason visual content, resulting in suboptimal performance on complex reasoning\ntasks. Moreover, the absence of comprehensive benchmarks hinders the accurate\nassessment of multimodal reasoning capabilities. In this paper, we introduce\nR1-Onevision, a multimodal reasoning model designed to bridge the gap between\nvisual perception and deep reasoning. To achieve this, we propose a cross-modal\nreasoning pipeline that transforms images into formal textural representations,\nenabling precise language-based reasoning. Leveraging this pipeline, we\nconstruct the R1-Onevision dataset which provides detailed, step-by-step\nmultimodal reasoning annotations across diverse domains. We further develop the\nR1-Onevision model through supervised fine-tuning and reinforcement learning to\ncultivate advanced reasoning and robust generalization abilities. To\ncomprehensively evaluate multimodal reasoning performance across different\ngrades, we introduce R1-Onevision-Bench, a benchmark aligned with human\neducational stages, covering exams from junior high school to university and\nbeyond. Experimental results show that R1-Onevision achieves state-of-the-art\nperformance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple\nchallenging multimodal reasoning benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10615.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10568",
      "authors": [
        {
          "_id": "67d3a952687a7a8a4963a030",
          "user": {
            "_id": "64e86fbd0c2413c3571ef7a6",
            "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
            "isPro": false,
            "fullname": "Haopeng Li",
            "user": "hp-l33",
            "type": "user"
          },
          "name": "Haopeng Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:10.236Z",
          "hidden": false
        },
        {
          "_id": "67d3a952687a7a8a4963a031",
          "name": "Jinyue Yang",
          "hidden": false
        },
        {
          "_id": "67d3a952687a7a8a4963a032",
          "name": "Guoqi Li",
          "hidden": false
        },
        {
          "_id": "67d3a952687a7a8a4963a033",
          "name": "Huan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:19:51.000Z",
      "submittedOnDailyAt": "2025-03-14T02:32:07.479Z",
      "title": "랜덤화 병렬 디코딩을 이용한 자기 회귀 이미지 생성",
      "submittedOnDailyBy": {
        "_id": "64e86fbd0c2413c3571ef7a6",
        "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
        "isPro": false,
        "fullname": "Haopeng Li",
        "user": "hp-l33",
        "type": "user"
      },
      "summary": "ARPG를 소개합니다. ARPG는 새로운 시각적인 자동 함수 회귀 모델로, 현재의 LasTAR 순서 접근의 고유한 한계를 해결하고, 추론 효율성과 Zero-Shot 일반화에 부담을 주지 않도록 랜덤화된 병렬 생성을 가능하게 합니다. 우리 주요한 견해는, 효과적인 랜덤 순서 모델링에 있어서, 다음 예측 태그의 위치를 결정하기 위해 명시적인 가이드라인이 필요합니다. 따라서, 우리는 위치의 가이드라인과 콘텐츠 표현을 분리하고, 각각의 쿼리와 키-밸류 페어로 인코딩하는 새로운 가이드라인 프레임워크를 제안합니다. 이 가이드라인을 직접적인 원인적 어텐션 구조에 통합함으로써, 우리의 접근法是 완전한 랜덤 순서의 훈련과 생성을 가능하게 하며, 양방향 어텐션의 필요성을 제거합니다. 이렇게, ARPG는 이미지의 인풋, OOP, 레소리쿤 확장 등 Zero-Shot 태스크에 자연스럽게 일반화합니다. 또한, 랜덤 샘플링 단계를 64 단계로, ImageNet-1K 256 벤치마크에서 대표적인 최근의 자동 함수 회귀 모델과 같은 크기로 75% 이상의 메모리 소비를 줄일 때 20배 이상의 트랜잭션 증가를 실현합니다.",
      "upvotes": 4,
      "discussionId": "67d3a957687a7a8a4963a179",
      "projectPage": "https://hp-l33.github.io/projects/arpg",
      "githubRepo": "https://github.com/hp-l33/ARPG",
      "ai_keywords": [
        "autoregressive model",
        "randomized parallel generation",
        "raster-order approaches",
        "inference efficiency",
        "zero-shot generalization",
        "sequential, predefined token generation order",
        "guided decoding framework",
        "positional guidance",
        "content representation",
        "queries",
        "key-value pairs",
        "causal attention mechanism",
        "fully random-order training",
        "bidirectional attention",
        "image inpainting",
        "outpainting",
        "resolution expansion",
        "parallel inference",
        "ImageNet-1K 256 benchmark",
        "FID",
        "sampling steps",
        "throughput",
        "memory consumption"
      ]
    },
    "publishedAt": "2025-03-13T13:19:51.000Z",
    "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
    "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10568.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e86fbd0c2413c3571ef7a6",
      "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
      "fullname": "Haopeng Li",
      "name": "hp-l33",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10391",
      "authors": [
        {
          "_id": "67d39679ea264394acf948ad",
          "name": "Yufan Deng",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948ae",
          "name": "Xun Guo",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948af",
          "name": "Yizhi Wang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b0",
          "name": "Jacob Zhiyuan Fang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b1",
          "name": "Angtian Wang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b2",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:29.782Z",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b3",
          "name": "Yiding Yang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b4",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b5",
          "name": "Haibin Huang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b6",
          "name": "Chongyang Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T14:07:58.000Z",
      "submittedOnDailyAt": "2025-03-14T01:07:59.040Z",
      "title": "CINEMA: 코히렌트 다중 서브젝트 비디오 생성에 의한 MLLM 기반 가이드",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "ビデオ 생성은, 딥러닝 모델, 특히 분산 모델의 등장으로 놀라운 진전을 보이고 있습니다. 기존의 방법들은 텍스트 프로젝트나 단일 이미지에서 고품질의 비디오를 생성하는 데 뛰어납니다. 그러나 개인화된 여러 주제의 비디오 생성은 큰 문제로 남아 있으며, 아직 크게 탐색되지 않았습니다. 이 문제는 다양한 주제를 포함하는 비디오를 합성하고 시간적 및 공간적 일관성을 보장하는 데 복잡합니다. 현재의 접근 방식은 주로 주제 이미지가 텍스트 프로젝트의 키워드에 대응하도록 의존하고 있습니다. 그러나 이는 불확실성을 불러오고, 주제의 관계를 효과적으로 모델화할 수 없게 되었습니다. 본 논문에서는, 멀티모달 대형 언어 모델(MLLM)을 확장하여 일관된 여러 주제의 비디오 생성의 새로운 프레임워크를 제안합니다. 우리의 접근 방식은 주제 이미지와 텍스트 엔티티 사이의 명확한 대응이 필요하도록 하지 않고, 불확실성을 줄이고, 注釈의 노력을 줄입니다. MLLM을 주제 관계를 해석하는 데 의해, 우리의 방법은 scalability를 촉진하고, 큰 다양한 데이터 세트를 사용하여 훈련할 수 있습니다. 또한, 우리의 프레임워크는 변할 수 있는 주제의 수에 따라 조건부되어서, 개인화된 콘텐츠의 창작에 더 큰 유연성을 제공합니다. 확장된 평가를 통해, 우리의 접근 방식이 주제의 일관성과 전체 비디오의 일관성을 크게 향상시키고, 이야기 전개, 상호작용 미디어, 개인화된 비디오 생성의 첨단 애플리케이션에 대한 길을 열어줍니다.",
      "upvotes": 4,
      "discussionId": "67d3967aea264394acf94915",
      "ai_keywords": [
        "diffusion models",
        "multimodal large language model (MLLM)"
      ]
    },
    "publishedAt": "2025-03-13T10:07:58.000Z",
    "title": "CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance",
    "summary": "Video generation has witnessed remarkable progress with the advent of deep\ngenerative models, particularly diffusion models. While existing methods excel\nin generating high-quality videos from text prompts or single images,\npersonalized multi-subject video generation remains a largely unexplored\nchallenge. This task involves synthesizing videos that incorporate multiple\ndistinct subjects, each defined by separate reference images, while ensuring\ntemporal and spatial consistency. Current approaches primarily rely on mapping\nsubject images to keywords in text prompts, which introduces ambiguity and\nlimits their ability to model subject relationships effectively. In this paper,\nwe propose CINEMA, a novel framework for coherent multi-subject video\ngeneration by leveraging Multimodal Large Language Model (MLLM). Our approach\neliminates the need for explicit correspondences between subject images and\ntext entities, mitigating ambiguity and reducing annotation effort. By\nleveraging MLLM to interpret subject relationships, our method facilitates\nscalability, enabling the use of large and diverse datasets for training.\nFurthermore, our framework can be conditioned on varying numbers of subjects,\noffering greater flexibility in personalized content creation. Through\nextensive evaluations, we demonstrate that our approach significantly improves\nsubject consistency, and overall video coherence, paving the way for advanced\napplications in storytelling, interactive media, and personalized video\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 34
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10636",
      "authors": [
        {
          "_id": "67d39ae1faaad4ed2df1cc61",
          "user": {
            "_id": "63041b541dd5d3c62486c294",
            "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
            "isPro": false,
            "fullname": "Ho Kei Cheng",
            "user": "hkchengrex",
            "type": "user"
          },
          "name": "Ho Kei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:19.361Z",
          "hidden": false
        },
        {
          "_id": "67d39ae1faaad4ed2df1cc62",
          "name": "Alexander Schwing",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63041b541dd5d3c62486c294/ZHc69zwDiWkyl0cNnp5zn.png"
      ],
      "publishedAt": "2025-03-13T17:59:56.000Z",
      "submittedOnDailyAt": "2025-03-14T01:27:30.007Z",
      "title": "조건의咒文：조건부 플로우베이스 생성의 최적송달을 분석하여 개선する",
      "submittedOnDailyBy": {
        "_id": "63041b541dd5d3c62486c294",
        "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
        "isPro": false,
        "fullname": "Ho Kei Cheng",
        "user": "hkchengrex",
        "type": "user"
      },
      "summary": "Minibatch 최적 전송은 무조건 흐름의 경로를 정렬합니다. 이는 테스트 시 수치 미분 방정식을 해결할 때, 적은 수의 적분 단계와 낮은 복잡도의 수치 해법을 사용함으로써 계산량을 줄일 수 있습니다. 그러나 조건부인 경우, Minibatch 최적 전송은 부족합니다. 이는 기본적인 최적 전송 매핑이 조건을 무시하고, 훈련 시 조건부 편향이 발생하기 때문입니다. 반면, 테스트 시는 편향된 초기 분포에 접근할 수 없으며, 모든 무편의 초기 분포에서 샘플링됩니다. 이 훈련과 테스트 사이의 간극은 기능의 성능이 저하됩니다. 이 간격을 메우기 위해, 우리는 조건부 최적 전송 C^2OT를 제안합니다. C^2OT는 최적 전송 매핑을 계산할 때, 비용 행렬에 조건부 가중치를 추가합니다. 실험은 8gaussians-to-moons, CIFAR-10, ImageNet-32x32, ImageNet-256x256에서, 이산 및 연속 조건에도 대응합니다. 우리의 방법은 현재의 베이스라인과 비교하여, 다른 함수 평가 버킷 내에서 전체적으로 더 좋은 성능을 보여주며, 코드는 https://hkchengrex.github.io/C2OT에서 사용할 수 있습니다.",
      "upvotes": 3,
      "discussionId": "67d39ae4faaad4ed2df1cd42",
      "projectPage": "https://hkchengrex.com/C2OT/",
      "githubRepo": "https://github.com/hkchengrex/C2OT",
      "ai_keywords": [
        "minibatch optimal transport",
        "flow matching",
        "integration steps",
        "numerical solvers",
        "ordinary differential equation",
        "optimal transport mapping",
        "conditional optimal transport",
        "C^2OT",
        "cost matrix",
        "optimal transport assignment",
        "discrete conditions",
        "continuous conditions",
        "8gaussians-to-moons",
        "CIFAR-10",
        "ImageNet-32x32",
        "ImageNet-256x256",
        "function evaluation budgets"
      ]
    },
    "publishedAt": "2025-03-13T13:59:56.000Z",
    "title": "The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation",
    "summary": "Minibatch optimal transport coupling straightens paths in unconditional flow\nmatching. This leads to computationally less demanding inference as fewer\nintegration steps and less complex numerical solvers can be employed when\nnumerically solving an ordinary differential equation at test time. However, in\nthe conditional setting, minibatch optimal transport falls short. This is\nbecause the default optimal transport mapping disregards conditions, resulting\nin a conditionally skewed prior distribution during training. In contrast, at\ntest time, we have no access to the skewed prior, and instead sample from the\nfull, unbiased prior distribution. This gap between training and testing leads\nto a subpar performance. To bridge this gap, we propose conditional optimal\ntransport C^2OT that adds a conditional weighting term in the cost matrix when\ncomputing the optimal transport assignment. Experiments demonstrate that this\nsimple fix works with both discrete and continuous conditions in\n8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method\nperforms better overall compared to the existing baselines across different\nfunction evaluation budgets. Code is available at\nhttps://hkchengrex.github.io/C2OT",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63041b541dd5d3c62486c294/ZHc69zwDiWkyl0cNnp5zn.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10636.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63041b541dd5d3c62486c294",
      "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
      "fullname": "Ho Kei Cheng",
      "name": "hkchengrex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 25
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10072",
      "authors": [
        {
          "_id": "67d390de29a092bdbb0a2aeb",
          "user": {
            "_id": "6331c3f618711776b468e9ec",
            "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
            "isPro": false,
            "fullname": "Mia Mohammad Imran",
            "user": "imranraad",
            "type": "user"
          },
          "name": "Mia Mohammad Imran",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T02:14:33.331Z",
          "hidden": false
        },
        {
          "_id": "67d390de29a092bdbb0a2aec",
          "name": "Jaydeb Sarker",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T05:39:29.000Z",
      "submittedOnDailyAt": "2025-03-14T00:44:11.345Z",
      "title": "\"무언가 하지 않음\": 버그 리포트 논의에서 독성 조사",
      "submittedOnDailyBy": {
        "_id": "6331c3f618711776b468e9ec",
        "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
        "isPro": false,
        "fullname": "Mia Mohammad Imran",
        "user": "imranraad",
        "type": "user"
      },
      "summary": "バグ報告의 독성은 오픈 소스 소프트웨어 개발의 협력 구조에 중대한 문제를 야기합니다. バグ報告는 결함의 식별과 해결에 중요하지만, 그 본질적인 문제의 초점을 맞추는 성질과 감정적으로 풍부한 맥락에 의해 독성적인 상호작용에 쉽게 빠질 수 있습니다. 본 연구는 GitHub의 バグ報告의 독성을 분석하기 위해 203개의 バグ트리를 연구했습니다. (81개 독성이 있는 것 포함). 우리의 발견은, バグ의 심각성과 우선순위의 오해, 해결된 것이 없는 도구에 대한 불만, 그리고 전문적인 통신의 부족으로 인해 독성이 자주 발생함을 보여줍니다. 이러한 독성적인 상호작용은 생산적인 토론을 포기하고 실질적인 결과를 줄 수 있는 가능성을 줄입니다. 우리의 초기 발견은 독성의 감소로 バグ 해결의 개선에 대한 구체적인 실행 가능한 제안을 제공합니다.",
      "upvotes": 2,
      "discussionId": "67d390df29a092bdbb0a2b2d",
      "projectPage": "https://zenodo.org/records/15015619"
    },
    "publishedAt": "2025-03-13T01:39:29.000Z",
    "title": "\"Silent Is Not Actually Silent\": An Investigation of Toxicity on Bug\n  Report Discussion",
    "summary": "Toxicity in bug report discussions poses significant challenges to the\ncollaborative dynamics of open-source software development. Bug reports are\ncrucial for identifying and resolving defects, yet their inherently\nproblem-focused nature and emotionally charged context make them susceptible to\ntoxic interactions. This study explores toxicity in GitHub bug reports through\na qualitative analysis of 203 bug threads, including 81 toxic ones. Our\nfindings reveal that toxicity frequently arises from misaligned perceptions of\nbug severity and priority, unresolved frustrations with tools, and lapses in\nprofessional communication. These toxic interactions not only derail productive\ndiscussions but also reduce the likelihood of actionable outcomes, such as\nlinking issues with pull requests. Our preliminary findings offer actionable\nrecommendations to improve bug resolution by mitigating toxicity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10072.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6331c3f618711776b468e9ec",
      "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
      "fullname": "Mia Mohammad Imran",
      "name": "imranraad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10291",
      "authors": [
        {
          "_id": "67d3cbea16d1ecea57ed096c",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed096d",
          "name": "Zhangwei Gao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed096e",
          "name": "Lianjie Chen",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed096f",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0970",
          "name": "Jinguo Zhu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0971",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0972",
          "name": "Yangzhou Liu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0973",
          "name": "Yue Cao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0974",
          "name": "Shenglong Ye",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0975",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0976",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0977",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0978",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0979",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed097a",
          "name": "Wenhai Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VPQRFEXv78LkPf2h5dgXm.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/w91GHKp_sv2u4U-8VFLOn.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/rG_h65J1RHcO0Lkbo5rg4.png"
      ],
      "publishedAt": "2025-03-13T12:03:37.000Z",
      "submittedOnDailyAt": "2025-03-14T06:40:16.854Z",
      "title": "VisualPRM: 효과적인 모델화 프로세스 보상 모델의 다양한 이유",
      "submittedOnDailyBy": {
        "_id": "619507e7b74b6c591f794340",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
        "isPro": false,
        "fullname": "Weiyun Wang",
        "user": "Weiyun1025",
        "type": "user"
      },
      "summary": "VisualPRM는 8B 파라미터를 가진 발전된 다 모델 프로세스 보상 모델(PRM)입니다. 이 모델은 현재의 다 모델 대 언어 모델(MLLMs)의 능력을 다양한 모델 규모와 가족 사이에서 Best-of-N(BoN) 평가 전략을 사용하여 향상시킵니다. 특히, 우리의 모델은 3가지의 MLLM과 4가지의 모델 규모의 이유 성능을 향상시킵니다. High-capacity 모델인 InternVL2.5-78B에 대해도 7개의 다 모델 이유 벤치마크에서 5.9점의 향상을 달성했습니다. 실험 결과를 통해, 우리의 모델은 Outcome Reward Models와 Self-Consistency와 비교하여 BoN 평가에서 상위 성능을 보여주고 있습니다. 또한, 다 모델 PRM의 훈련을 촉진하기 위해, 자동화된 데이터 파이프라인을 사용하여 VisualPRM400K라는 다 모델 프로세스 서브젝트 데이터 세트를 구축했습니다. 또한, 다 모델 PRM의 평가에서, 우리는 인간이 설명된 단계별 정확성 라벨을 가지는 VisualProcessBench라는 벤치마크를 제안하고, PRM이 다 모델 이유 작업에서 오류 단계를 감지하는 능력을 평가하기 위해 목표를 설정했습니다. 우리의 연구는 향후 연구에 의해 다 모델 대 언어 모델의 개발에 기여하고, 더进一步的 발전을 촉진하고자 합니다. 우리의 모델, 데이터, 벤치마크는 https://internvl.github.io/blog/2025-03-13-VisualPRM/에서 공개됩니다.",
      "upvotes": 1,
      "discussionId": "67d3cbed16d1ecea57ed0a75",
      "projectPage": "https://internvl.github.io/blog/2025-03-13-VisualPRM/",
      "githubRepo": "https://github.com/OpenGVLab/InternVL",
      "ai_keywords": [
        "Process Reward Model (PRM)",
        "Multimodal Large Language Models (MLLMs)",
        "Best-of-N (BoN)",
        "multimodal reasoning benchmarks",
        "Automated data pipeline",
        "VisualPRM400K",
        "VisualProcessBench",
        "human-annotated step-wise correctness labels",
        "multimodal PRMs",
        "erroneous steps in multimodal reasoning tasks"
      ]
    },
    "publishedAt": "2025-03-13T08:03:37.000Z",
    "title": "VisualPRM: An Effective Process Reward Model for Multimodal Reasoning",
    "summary": "We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM)\nwith 8B parameters, which improves the reasoning abilities of existing\nMultimodal Large Language Models (MLLMs) across different model scales and\nfamilies with Best-of-N (BoN) evaluation strategies. Specifically, our model\nimproves the reasoning performance of three types of MLLMs and four different\nmodel scales. Even when applied to the highly capable InternVL2.5-78B, it\nachieves a 5.9-point improvement across seven multimodal reasoning benchmarks.\nExperimental results show that our model exhibits superior performance compared\nto Outcome Reward Models and Self-Consistency during BoN evaluation. To\nfacilitate the training of multimodal PRMs, we construct a multimodal process\nsupervision dataset VisualPRM400K using an automated data pipeline. For the\nevaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with\nhuman-annotated step-wise correctness labels, to measure the abilities of PRMs\nto detect erroneous steps in multimodal reasoning tasks. We hope that our work\ncan inspire more future research and contribute to the development of MLLMs.\nOur model, data, and benchmark are released in\nhttps://internvl.github.io/blog/2025-03-13-VisualPRM/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VPQRFEXv78LkPf2h5dgXm.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/w91GHKp_sv2u4U-8VFLOn.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/rG_h65J1RHcO0Lkbo5rg4.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10291.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619507e7b74b6c591f794340",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
      "fullname": "Weiyun Wang",
      "name": "Weiyun1025",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09368",
      "authors": [
        {
          "_id": "67d2ca4be4696fda20bac029",
          "user": {
            "_id": "656c8721e8bf55919a9732c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
            "isPro": false,
            "fullname": "Nikolai",
            "user": "Nikolai10",
            "type": "user"
          },
          "name": "Nikolai Körber",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:57:39.634Z",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02a",
          "name": "Eduard Kromer",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02b",
          "name": "Andreas Siebert",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02c",
          "name": "Sascha Hauke",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02d",
          "name": "Daniel Mueller-Gritschneder",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02e",
          "name": "Björn Schuller",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T13:14:51.000Z",
      "submittedOnDailyAt": "2025-03-14T07:51:16.414Z",
      "title": "PerCoV2: 은닉 히어로러 키드 마스크 이미지 모델링에 의한 개선된 초저비트 레이트 시각적 이미지 압축",
      "submittedOnDailyBy": {
        "_id": "656c8721e8bf55919a9732c5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
        "isPro": false,
        "fullname": "Nikolai",
        "user": "Nikolai10",
        "type": "user"
      },
      "summary": "PerCoV2는, 밴디트와 스토ア르잔스가 제한된 애플리케이션에 대해 새로운 개방적인 초저비트レート의 시각적인 이미지 압축 시스템입니다. Careil 등의 선행 연구에 기초하여, PerCoV2는 Stable Diffusion 3 생태계에 확장되어, 이산적인 초잠상 분포를 명시적으로 모델링하여 엔트로피 코딩의 효율을 향상시킵니다. 이로 인해, 최근의 자동 리턴 게임 방법(VAR와 MaskGIT)을 상세히 비교하고, 큰 규모의 MSCOCO-30k 벤치마크에서 검증합니다. PerCoV2는 선행 연구와 비교하여, (i) 비트レー트를 더욱 낮추는 동시에 이미지의 정확도가 높아져 시각적 품질이 경쟁적입니다, (ii) 진행할 수 있는 비트レー트 감소를 위한 하이브리드 생성 모드를 제시하고, (iii) 공개된 컴포넌트만으로 구축되어 있습니다. 코드와 훈련 모델은, https://github.com/Nikolai10/PerCoV2에서 공개됩니다.",
      "upvotes": 1,
      "discussionId": "67d2ca50e4696fda20bac1a8",
      "githubRepo": "https://github.com/Nikolai10/PerCoV2",
      "ai_keywords": [
        "PerCoV2",
        "ultralow bit-rate",
        "perceptual image compression",
        "bandwidth-constrained applications",
        "Stable Diffusion 3",
        "entropy coding",
        "discrete hyper-latent image distribution",
        "autoregressive methods",
        "VAR",
        "MaskGIT",
        "MSCOCO-30k",
        "image fidelity",
        "perceptual quality",
        "hybrid generation mode",
        "public components"
      ]
    },
    "publishedAt": "2025-03-12T09:14:51.000Z",
    "title": "PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with\n  Implicit Hierarchical Masked Image Modeling",
    "summary": "We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image\ncompression system designed for bandwidth- and storage-constrained\napplications. Building upon prior work by Careil et al., PerCoV2 extends the\noriginal formulation to the Stable Diffusion 3 ecosystem and enhances entropy\ncoding efficiency by explicitly modeling the discrete hyper-latent image\ndistribution. To this end, we conduct a comprehensive comparison of recent\nautoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our\napproach on the large-scale MSCOCO-30k benchmark. Compared to previous work,\nPerCoV2 (i) achieves higher image fidelity at even lower bit-rates while\nmaintaining competitive perceptual quality, (ii) features a hybrid generation\nmode for further bit-rate savings, and (iii) is built solely on public\ncomponents. Code and trained models will be released at\nhttps://github.com/Nikolai10/PerCoV2.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09368.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656c8721e8bf55919a9732c5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
      "fullname": "Nikolai",
      "name": "Nikolai10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]