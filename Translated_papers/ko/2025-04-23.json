[
  {
    "paper": {
      "id": "2504.15120",
      "authors": [
        {
          "_id": "680733cf7722bb6407ca0787",
          "user": {
            "_id": "65276c7911a8a521c91bc10f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
            "isPro": false,
            "fullname": "Khalil Hennara",
            "user": "Hennara",
            "type": "user"
          },
          "name": "Khalil Hennara",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-22T09:37:47.479Z",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca0788",
          "name": "Sara Chrouf",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca0789",
          "user": {
            "_id": "63aa7667769a10efc404fbbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63aa7667769a10efc404fbbc/tn8ZxUmTEMS0Gze7_F7JL.jpeg",
            "isPro": false,
            "fullname": "Mohamed Motasim Hamed",
            "user": "Moatasem444",
            "type": "user"
          },
          "name": "Mohamed Motaism Hamed",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:37:25.702Z",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078a",
          "user": {
            "_id": "65704741e1cfce1764ce652e",
            "avatarUrl": "/avatars/9189aaf417426af4ebe381ed364a6c0e.svg",
            "isPro": false,
            "fullname": "Zeina Aldallal",
            "user": "ZeinaD",
            "type": "user"
          },
          "name": "Zeina Aldallal",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-23T05:30:42.569Z",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078b",
          "name": "Omar Hadid",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078c",
          "name": "Safwan AlModhayan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T14:17:25.000Z",
      "submittedOnDailyAt": "2025-04-23T03:28:02.778Z",
      "title": "クワイン 1.5B: 언어注入로 인한 阿拉伯語 SLM",
      "submittedOnDailyBy": {
        "_id": "65276c7911a8a521c91bc10f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
        "isPro": false,
        "fullname": "Khalil Hennara",
        "user": "Hennara",
        "type": "user"
      },
      "summary": "AI 개발의 중요한 한 단계로, 기존 모델에 새로운 지식을 추가하는 경우가 있습니다. 본 논문에서는, 대규모 언어 모델(LLM)에 새로운 언어를 통합하는 새로운 방법을 소개합니다. 우리의 접근法是, 기존의 LLM에 이전에 보지 못한 목표 언어를 기록하고, 그 지식을 파괴하지 않도록 성공했습니다. 주로 영어로 훈련된 작은 오픈 소스 모델에 阿拉伯語를 注入し, 15억 파라미터를 가진 작은 모델「Kuwain」을 훈련했습니다. 우리의 방법은, 다양한 벤치마크에서 평균 8%의 阿拉伯語의 성능 향상을示し、 기존의 지식을 유지하고, 원 모델의 데이터의 최소량으로 훈련된 것입니다. 이는, 영어와 阿拉伯語의 두 언어 모두에서 세부적인 모델을 훈련하는 대신, 비용 효율적인 선택입니다. 결과는, 확장된 언어 모델의 효율적인, 특정 언어에 대한 확장의 가능성을 보여주며, 확장 및 자원 강제의 프로세스를 피할 수 있음을 밝혀줍니다.",
      "upvotes": 58,
      "discussionId": "680733d07722bb6407ca07da",
      "githubRepo": "https://github.com/misraj-ai/Kuwain-Arabic-cleaner",
      "ai_keywords": [
        "large language model (LLM)",
        "tiny model",
        "Kuwain",
        "language integration",
        "Arabic language",
        "benchmarks",
        "language model expansion"
      ]
    },
    "publishedAt": "2025-04-21T10:17:25.000Z",
    "title": "Kuwain 1.5B: An Arabic SLM via Language Injection",
    "summary": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15120.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65276c7911a8a521c91bc10f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
      "fullname": "Khalil Hennara",
      "name": "Hennara",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16084",
      "authors": [
        {
          "_id": "6808558a07e80b69b2e351b5",
          "name": "Yuxin Zuo",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b6",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:51.438Z",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b7",
          "name": "Shang Qu",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b8",
          "name": "Li Sheng",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b9",
          "name": "Xuekai Zhu",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351ba",
          "name": "Biqing Qi",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bb",
          "name": "Youbang Sun",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bc",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bd",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351be",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-23T01:22:31.055Z",
      "title": "TTRL: 학습 시의 강화 학습",
      "submittedOnDailyBy": {
        "_id": "60bc94cd85a3ab33829b6211",
        "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
        "isPro": false,
        "fullname": "Kaiyan Zhang",
        "user": "iseesaw",
        "type": "user"
      },
      "summary": "이 논문은 대 언어 모델(LLMs)에서 추론 시 명시적인 라벨이 없는 데이터 위에서의 추론 임무에 대한 강화 학습(RL)을 조사합니다. 문제의 핵심적인 문제점은 추론 시 보상 평가가 가능한 상태에 대한 값 정보에 액세스할 수 없는 상황입니다. 이 설정은 단순히 보이는 것처럼, 테스트 시간 스케일링(TTS)의 일반적인 실천으로 다수결이 있으며, 이로 인해 놀라운 효과를 보입니다. 이 연구에서는, 라벨 없는 데이터 위에서의 RL을 이용한 LLMs의 훈련의 새로운 방법인 테스트 시간 강화 학습(TTRL)을 소개합니다. TTRL은 사전 학습 모델의 선행 결과를 활용하여 LLMs의 자기 진화를 가능하게 합니다. 실험 결과를 통해, TTRL은 다양한 태스크와 모델에서 경험적으로 성능 향상을 보여줍니다. 특히, AIME 2024에서, 라벨 없는 테스트 데이터를 사용하여 Qwen-2.5-Math-7B의 pass@1 성능을 약 159% 향상시켰습니다. 또한, TTRL은 만의@N 메트릭에만 지원을 받고, 초기 모델의 성능의 상한을 초과하고, 테스트 데이터와 실제 라벨을 사용하여 직접 훈련된 모델의 성능에 근접합니다. 실험 결과를 통해, 다양한 태스크에서 TTRL의 일반적인 효과성을 입증하고, TTRL의 광범위한 태스크와 분야의 가능성도 밝혀줍니다. GitHub: https://github.com/PRIME-RL/TTRL",
      "upvotes": 44,
      "discussionId": "6808558b07e80b69b2e351f3",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "Large Language Models (LLMs)",
        "reward estimation",
        "Test-Time Scaling (TTS)",
        "majority voting",
        "Test-Time Reinforcement Learning (TTRL)",
        "self-evolution",
        "pre-trained models",
        "pass@1",
        "Qwen-2.5-Math-7B",
        "AIME 2024",
        "Maj@N metric"
      ]
    },
    "publishedAt": "2025-04-22T13:59:56.000Z",
    "title": "TTRL: Test-Time Reinforcement Learning",
    "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60bc94cd85a3ab33829b6211",
      "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
      "fullname": "Kaiyan Zhang",
      "name": "iseesaw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15521",
      "authors": [
        {
          "_id": "6808458f07e80b69b2df2440",
          "name": "Minghao Wu",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2441",
          "name": "Weixuan Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2442",
          "name": "Sinuo Liu",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2443",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2444",
          "name": "Xintong Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2445",
          "name": "Yu Zhao",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2446",
          "user": {
            "_id": "6527d8b077bceabaab382a75",
            "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
            "isPro": false,
            "fullname": "Chenyang Lyu",
            "user": "ChenyangLyu",
            "type": "user"
          },
          "name": "Chenyang Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:16.770Z",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2447",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2448",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2449",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T01:47:37.000Z",
      "submittedOnDailyAt": "2025-04-23T00:13:52.385Z",
      "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\n\n이 글은 2,000개 이상의 다국어 벤치마크를 통해 얻은 '매우 짠 교훈'을 요약한 글입니다.",
      "submittedOnDailyBy": {
        "_id": "62d4bf8c97ab9eb08762a975",
        "avatarUrl": "/avatars/73c6228e317cf37b4e3c3e7a4b3d8ae8.svg",
        "isPro": false,
        "fullname": "Minghao Wu",
        "user": "minghaowu",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)의 언어 능력이 지속적으로 발전하면서 강력한 다언어 평가가 기술적 진보의 공정성을 촉진하는 데 중요하게 자리잡고 있습니다. 이 입장 논문에서는 2021년부터 2024년까지 148개국으로부터 공개된 2,000개 이상의 다언어(영어 외) 벤치마크를 평가하고 과거, 현재, 미래의 다언어 벤치마크의 실용성을 평가합니다. 우리의 조사 결과에서, 수십 백 달러 이상의 대규모 투자 속에서도, 영어가 이러한 벤치마크에 가장 과도하게 표현되어 있음을 명확히 확인했습니다. 또한 대부분의 벤치마크는 번역된 것이 아니라, 원본 언어의 콘텐츠로 사용되어 있으며, 중국, 인도, 독일, 영국, 미국 등 자원 풍부한 국가에서부터의 데이터가 주요 자원으로 되어 있습니다. 또한 벤치마크의 성능과 인간 판단과의 비교에서 명확히 드러난 차이점이 있습니다. STEM 관련 태스크는 인간 평가와 강한 상관관계를 나타내며(0.70~0.85), 반면, 질문-답변 시스템(예: XQuAD) 등 전통적인 NLP 태스크는 이러한 경향보다 약한 상관관계(0.11~0.30)를 나타냅니다. 또한 영어 벤치마크를 다른 언어로 번역하는 것은 충분하지 않습니다. 지역적인 벤치마크는 지역인간의 판단과 높은 일치율(0.68)을 보여주며, 번역된 벤치마크(0.47)에 비해도 높습니다. 이는 문화적 및 언어적 일치를 위한 벤치마크의 중요성을 강조하고, 번역만 의존하지 않는 것이 중요함을 보여주고 있습니다. 이러한 상세한 분석을 통해, 현재의 다언어 평가의 실용에서 6가지 중요한 한계점을 지적하고, 효과적인 다언어 벤치마크를 위한 지침을 제시하고 5가지 중요한 연구 방향을 명확히 합니다. 마지막으로, 현실적인 적용을 중시하는 인간에 맞는 벤치마크 개발을 위해 국제적인 협력을 촉구합니다.",
      "upvotes": 41,
      "discussionId": "6808459007e80b69b2df249e",
      "ai_keywords": [
        "multilingual large language models (LLMs)",
        "multilingual benchmarks",
        "benchmark performance",
        "human judgments",
        "STEM-related tasks",
        "question answering (e.g., XQuAD)",
        "culturally and linguistically tailored benchmarks",
        "human-aligned benchmarks",
        "real-world applications"
      ]
    },
    "publishedAt": "2025-04-21T21:47:37.000Z",
    "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
    "summary": "As large language models (LLMs) continue to advance in linguistic\ncapabilities, robust multilingual evaluation has become essential for promoting\nequitable technological progress. This position paper examines over 2,000\nmultilingual (non-English) benchmarks from 148 countries, published between\n2021 and 2024, to evaluate past, present, and future practices in multilingual\nbenchmarking. Our findings reveal that, despite significant investments\namounting to tens of millions of dollars, English remains significantly\noverrepresented in these benchmarks. Additionally, most benchmarks rely on\noriginal language content rather than translations, with the majority sourced\nfrom high-resource countries such as China, India, Germany, the UK, and the\nUSA. Furthermore, a comparison of benchmark performance with human judgments\nhighlights notable disparities. STEM-related tasks exhibit strong correlations\nwith human evaluations (0.70 to 0.85), while traditional NLP tasks like\nquestion answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).\nMoreover, translating English benchmarks into other languages proves\ninsufficient, as localized benchmarks demonstrate significantly higher\nalignment with local human judgments (0.68) than their translated counterparts\n(0.47). This underscores the importance of creating culturally and\nlinguistically tailored benchmarks rather than relying solely on translations.\nThrough this comprehensive analysis, we highlight six key limitations in\ncurrent multilingual evaluation practices, propose the guiding principles\naccordingly for effective multilingual benchmarking, and outline five critical\nresearch directions to drive progress in the field. Finally, we call for a\nglobal collaborative effort to develop human-aligned benchmarks that prioritize\nreal-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15521.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d4bf8c97ab9eb08762a975",
      "avatarUrl": "/avatars/73c6228e317cf37b4e3c3e7a4b3d8ae8.svg",
      "fullname": "Minghao Wu",
      "name": "minghaowu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16072",
      "authors": [
        {
          "_id": "6808467a867c3ef14f8326ce",
          "user": {
            "_id": "63797c273f575acc2f6893c0",
            "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
            "isPro": true,
            "fullname": "Long(Tony) Lian",
            "user": "longlian",
            "type": "user"
          },
          "name": "Long Lian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:14.686Z",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326cf",
          "name": "Yifan Ding",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d0",
          "name": "Yunhao Ge",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d1",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d2",
          "name": "Hanzi Mao",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d3",
          "user": {
            "_id": "620dd3888528f797e88cb9b5",
            "avatarUrl": "/avatars/af04728788d78fe7d6375e19e32a535e.svg",
            "isPro": false,
            "fullname": "Boyi Li",
            "user": "Boyiliee",
            "type": "user"
          },
          "name": "Boyi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:09.738Z",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d4",
          "name": "Marco Pavone",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d5",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d6",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d7",
          "user": {
            "_id": "6333a9195a032dcd095dda13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664329996201-noauth.jpeg",
            "isPro": false,
            "fullname": "Adam Yala",
            "user": "yala",
            "type": "user"
          },
          "name": "Adam Yala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:12.415Z",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d8",
          "user": {
            "_id": "649f05367b57fab3a5b27c8b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649f05367b57fab3a5b27c8b/UDJB4yqF2NmaRwCyTOfcl.jpeg",
            "isPro": true,
            "fullname": "Yin Cui",
            "user": "richardaecn",
            "type": "user"
          },
          "name": "Yin Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:06.739Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63797c273f575acc2f6893c0/37vXp0iDKwhBbVyNnCGHy.qt"
      ],
      "publishedAt": "2025-04-22T17:51:41.000Z",
      "submittedOnDailyAt": "2025-04-23T00:22:38.011Z",
      "title": "상세한 지역별 이미지와 동영상 캡처\n\n(注意：虽然要求不添加任何解释或额外的文本，但为了确保翻译的准确性和专业性，这里提供了翻译结果。如果需要进一步的解释或文本，请告知。)",
      "submittedOnDailyBy": {
        "_id": "63797c273f575acc2f6893c0",
        "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
        "isPro": true,
        "fullname": "Long(Tony) Lian",
        "user": "longlian",
        "type": "user"
      },
      "summary": "특정 이미지와 영상의 특정 영역에 대한 상세하고 정확한 설명을 생성하는 것은 시각 언어 모델의 기본적인 문제입니다. 여기서는 구체적인 세부 설명을 목표로 하는 모델을 소개합니다. Describe Anything Model (DAM)은 지역적 세부 사항과 글로벌 컨텍스트를 동시에 유지하기 위해 두 가지 혁신적인 요소를 도입했습니다. 이는 목표 영역의 고해상도 인코딩을 보장하는 포커스 프로ン프트와 지역적 설명과 글로벌 컨텍스트를 통합하는 지역적 시각 백트랙킹으로 이루어집니다. 고품질의 DLC 데이터의 부족을 해결하기 위해, Semi-supervised learning (SSL) 기반의 데이터 파이프라인 (DLC-SDP)을 제안합니다. DLC-SDP는 기존의 분할 데이터 세트를 시작으로 SSL을 사용하여 무라벨 웹 이미지로 확장합니다. DLC-Bench라는 벤치마크를 소개합니다. 이는 참조 캡처를 의존하지 않고 DLC를 평가하기 위해 설계되었습니다. DAM은 7개의 벤치마크에서 새로운 최선으로 도달했습니다. 이들은 키워드 수준, 문장 수준 및 지역적 이미지와 영상 캡처를 위한 세부적인 문장 수준으로 이루어져 있습니다.",
      "upvotes": 28,
      "discussionId": "6808467e867c3ef14f832831",
      "projectPage": "https://describe-anything.github.io",
      "githubRepo": "https://github.com/NVlabs/describe-anything",
      "ai_keywords": [
        "focal prompt",
        "localized vision backbone",
        "Semi-supervised learning (SSL)-based Data Pipeline (DLC-SDP)",
        "segmentation datasets",
        "DLC-Bench",
        "keyword-level",
        "phrase-level",
        "detailed multi-sentence localized image and video captioning"
      ]
    },
    "publishedAt": "2025-04-22T13:51:41.000Z",
    "title": "Describe Anything: Detailed Localized Image and Video Captioning",
    "summary": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63797c273f575acc2f6893c0/37vXp0iDKwhBbVyNnCGHy.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16072.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63797c273f575acc2f6893c0",
      "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
      "fullname": "Long(Tony) Lian",
      "name": "longlian",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15466",
      "authors": [
        {
          "_id": "6808480c49c8f78b6a4e492f",
          "name": "Jiayi Pan",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4930",
          "user": {
            "_id": "644570ba2d91b15b4c7f6311",
            "avatarUrl": "/avatars/d5e66012066d0c330b8f23718b1499d8.svg",
            "isPro": false,
            "fullname": "Xiuyu Li",
            "user": "xiuyul",
            "type": "user"
          },
          "name": "Xiuyu Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:59.248Z",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4931",
          "name": "Long Lian",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4932",
          "name": "Charlie Snell",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4933",
          "name": "Yifei Zhou",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4934",
          "user": {
            "_id": "6333a9195a032dcd095dda13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664329996201-noauth.jpeg",
            "isPro": false,
            "fullname": "Adam Yala",
            "user": "yala",
            "type": "user"
          },
          "name": "Adam Yala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:02.029Z",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4935",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4936",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4937",
          "name": "Alane Suhr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T22:29:02.000Z",
      "submittedOnDailyAt": "2025-04-23T00:30:52.876Z",
      "title": "학습 어댑티브 패럴レル 논리",
      "submittedOnDailyBy": {
        "_id": "63797c273f575acc2f6893c0",
        "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
        "isPro": true,
        "fullname": "Long(Tony) Lian",
        "user": "longlian",
        "type": "user"
      },
      "summary": "スカライング의 추론 시간 계산 확장은 언어 모델의 논리 능력에 큰 상승을 가져왔습니다. 그러나 현재의 방법들은 중요한 한계가 있습니다: 체인 Consistency Approach는 과도하게 긴 출력을 생성하고, Latency와 Context Window의 끝에 의해 증가하는 것을 초래합니다. 반면, 병렬 방식처럼 자기 일관성에 의존하는 방법들은 충분한 협동성 부족으로 인해冗長한 계산과 성능의 한계 등 문제를 놓치고 있습니다. 이러한 결점을 해결하기 위해, 우리는 언어 모델이 병렬 계산과 체인 계산을 통합한 엔드 투 엔드 계산을 협조하는 새로운 논리 프레임워크를 제안합니다. APR은 현재의 논리 모델을 일반화하고, 스パノン()과 JOIN() 연산을 사용하여 적응적인 다형성 계산을 가능하게 합니다. 핵심의 혁신적인 점은, 부모와 자식의 추론 시간을 최적화하기 위해 엔드에서부터의 강화 학습 전략으로, 추론 구조의 사전 정의가 필요하지 않도록 작업의 성공율을 높입니다. Countdown Reasoning Task의 실험에서, APR의 큰 이점이 나타났습니다: (1) 같은 Context Window 내에서 높은 성능 (83.4% vs. 60.0% at 4k context); (2) 증가된 계산으로 인한 상위 스케일러의 유연성 (80.1% vs. 66.6% at 20k total tokens); (3) 같은 Latency에서 정확도 향상 (75.2% vs. 57.3% at approximately 5,000ms). APR은 언어 모델이 자동으로 계산의 적응적인 배분을 통해 논리 프로세스를 최적화하는 것을 가능하게 합니다.",
      "upvotes": 27,
      "discussionId": "6808480c49c8f78b6a4e4968",
      "githubRepo": "https://github.com/Parallel-Reasoning/APR",
      "ai_keywords": [
        "Adaptive Parallel Reasoning (APR)",
        "serialized chain-of-thought approaches",
        "parallel methods",
        "self-consistency",
        "adaptive multi-threaded inference",
        "spawn()",
        "join()",
        "reinforcement learning strategy",
        "parent inference threads",
        "child inference threads",
        "Countdown reasoning task",
        "context window",
        "scalability",
        "total tokens",
        "reasoning processes",
        "adaptive allocation of computation"
      ]
    },
    "publishedAt": "2025-04-21T18:29:02.000Z",
    "title": "Learning Adaptive Parallel Reasoning with Language Models",
    "summary": "Scaling inference-time computation has substantially improved the reasoning\ncapabilities of language models. However, existing methods have significant\nlimitations: serialized chain-of-thought approaches generate overly long\noutputs, leading to increased latency and exhausted context windows, while\nparallel methods such as self-consistency suffer from insufficient\ncoordination, resulting in redundant computations and limited performance\ngains. To address these shortcomings, we propose Adaptive Parallel Reasoning\n(APR), a novel reasoning framework that enables language models to orchestrate\nboth serialized and parallel computations end-to-end. APR generalizes existing\nreasoning methods by enabling adaptive multi-threaded inference using spawn()\nand join() operations. A key innovation is our end-to-end reinforcement\nlearning strategy, optimizing both parent and child inference threads to\nenhance task success rate without requiring predefined reasoning structures.\nExperiments on the Countdown reasoning task demonstrate significant benefits of\nAPR: (1) higher performance within the same context window (83.4% vs. 60.0% at\n4k context); (2) superior scalability with increased computation (80.1% vs.\n66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%\nvs. 57.3% at approximately 5,000ms). APR represents a step towards enabling\nlanguage models to autonomously optimize their reasoning processes through\nadaptive allocation of computation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15466.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63797c273f575acc2f6893c0",
      "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
      "fullname": "Long(Tony) Lian",
      "name": "longlian",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15415",
      "authors": [
        {
          "_id": "68084b04ba1dd0e6a077e09f",
          "user": {
            "_id": "64c910233d5a0dfed5ce5abb",
            "avatarUrl": "/avatars/8c73f380219c05ae7e7c2fad75a570d8.svg",
            "isPro": false,
            "fullname": "dma",
            "user": "mdh98",
            "type": "user"
          },
          "name": "David Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:56.437Z",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a0",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a1",
          "user": {
            "_id": "6704ee27386892c420db1938",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6704ee27386892c420db1938/lb5mtEwYhn47RawkynYPs.jpeg",
            "isPro": false,
            "fullname": "JinCheng Ren",
            "user": "JinChengRen",
            "type": "user"
          },
          "name": "Jincheng Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:36:46.274Z",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a2",
          "name": "Jarvis Guo",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a3",
          "name": "Yifan Yao",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a4",
          "name": "Zhenlin Wei",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a5",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a6",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a7",
          "name": "Boyu Feng",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a8",
          "name": "Jun Ma",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a9",
          "name": "Xiao Gu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0aa",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ab",
          "name": "King Zhu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ac",
          "name": "Yancheng He",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ad",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ae",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0af",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b0",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b1",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b2",
          "name": "Xiaojie Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T19:53:44.000Z",
      "submittedOnDailyAt": "2025-04-23T00:59:32.168Z",
      "title": "IV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Bench: 이미지 기반 비디오 인식 및 논리 기반의 벤치마크\n\nIV-Ben",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "현재의 Multimodal Large Language Models (MLLMs)의 평가 프레임워크는 주로 이미지의 해석과 일반적인 동영상 이해 태스크에 집중하여, 동영상 이해에서 이미지 컨텍스트의 중요한 역할을 크게 빕니다. 이를 메우기 위해, IV-Bench라는 첫 번째 종합적인 벤치마크를 제안합니다. IV-Bench는 967개의 동영상과 2,585개의 정밀하게 注解된 이미지-텍스트 쿼리를 가지고 있으며, 13개의 태스크(7개의 포즈 태스크와 6개의 해석 태스크)과 5가지 대표적인 카테고리로 구성되어 있습니다. 최신의 오픈 소스(예: InternVL2.5, Qwen2.5-VL)과 클로즈드 소스(예: GPT-4o, Gemini2-Flash, Gemini2-Pro) MLLM의 세부적인 평가에 따라, 현재의 모델은 이미지 컨텍스트 기반의 동영상 인식과 해석에 대해 대폭 낮은 성능을 보였으며, 최고로도 28.9%의 정확도를 달성했습니다. 나아가는 분석은 IV-Bench에서 모델의 성능에 영향을 미치는 요인을 밝혀, 추론 패턴, 프레임 수, 해상도 등을 포함하며, 이러한 것을 보여줍니다. 또한, 간단한 데이터 합성 접근 방식을 통해, IV-Bench의 문제점은 학습 프로세스에서 데이터 형식의 대응뿐만 아니라 더 광범위한 범위를 포함하는 것을 보여줍니다. 이러한 발견은 향후 연구에 유익한 조언을 제공합니다. 코드와 데이터는 https://github.com/multimodal-art-projection/IV-Bench에서 공개되어 있습니다.",
      "upvotes": 15,
      "discussionId": "68084b0bba1dd0e6a077e279",
      "githubRepo": "https://github.com/multimodal-art-projection/IV-Bench",
      "ai_keywords": [
        "Image-Grounded Video Perception and Reasoning",
        "IV-Bench",
        "image-text queries",
        "frame number",
        "resolution"
      ]
    },
    "publishedAt": "2025-04-21T15:53:44.000Z",
    "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning\n  in Multimodal LLMs",
    "summary": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)\nprimarily focus on image reasoning or general video understanding tasks,\nlargely overlooking the significant role of image context in video\ncomprehension. To bridge this gap, we propose IV-Bench, the first comprehensive\nbenchmark for evaluating Image-Grounded Video Perception and Reasoning.\nIV-Bench consists of 967 videos paired with 2,585 meticulously annotated\nimage-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5\nrepresentative categories. Extensive evaluations of state-of-the-art\nopen-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,\nGemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models\nsubstantially underperform in image-grounded video Perception and Reasoning,\nmerely achieving at most 28.9% accuracy. Further analysis reveals key factors\ninfluencing model performance on IV-Bench, including inference pattern, frame\nnumber, and resolution. Additionally, through a simple data synthesis approach,\nwe demonstratethe challenges of IV- Bench extend beyond merely aligning the\ndata format in the training proecss. These findings collectively provide\nvaluable insights for future research. Our codes and data are released in\nhttps://github.com/multimodal-art-projection/IV-Bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14538",
      "authors": [
        {
          "_id": "680863ed3767f6ed7c969fbf",
          "name": "Yiting Ran",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc0",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc1",
          "name": "Tian Qiu",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc2",
          "name": "Jiaqing Liang",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc3",
          "name": "Yanghua Xiao",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc4",
          "name": "Deqing Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-20T08:56:27.000Z",
      "submittedOnDailyAt": "2025-04-23T02:23:09.187Z",
      "title": "북월드: 소설에서 창조적인 이야기의 생성을 목표로 하는 인터랙티브 효과 그룹에 초대합니다.",
      "submittedOnDailyBy": {
        "_id": "64c7bf2c4524c2aea7eac0b3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7bf2c4524c2aea7eac0b3/5ocZ69MvN4RFv86Aa7ks3.png",
        "isPro": false,
        "fullname": "Xintao Wang",
        "user": "Neph0s",
        "type": "user"
      },
      "summary": "최근의 대규모 언어 모델（LLMs）의 발전으로, 다 에이전트 시스템을 활용한 시뮬레이션이 가능해졌습니다. 기존의 시도는 새로 정의된 ピースナ를 할당한 에이전트 소셜성을 만들었지만, 이미 확립된 가상의 세계와 캐릭터를 시뮬레이션하는 것은 실용적인 가치가 있어도, 크게 조사되어 있지 않았습니다. 본 논문에서는, 기본의 다 에이전트 소셜성을 구축하고 시뮬레이션을 위한 기능을 통합한 시스템「BookWorld」를 소개합니다. 「BookWorld」의 설계는 다양성과 동적인 캐릭터, 가상의 세계관, 지리학적한 제약 및 변화 등 실세계의 복잡성을 광범위하게 포함하고 있습니다. 「BookWorld」는 이야기 생성, 인터랙티브 게임, 사회 시뮬레이션 등 다양한 애플리케이션을 제공하며, 사랑받는 가상의 작품을 확장하고 새로운 탐색 방법을 제공합니다. 확장된 실험을 통해, 「BookWorld」는 소스 책에 충실한 고품질의 시뮬레이션을 생성하고 이전의 방법을 초월하는 것을 보여줍니다（승률: 75.36%）. 본 논문의 코드는 프로젝트 페이지에서 공개되어 있습니다: https://bookworld2025.github.io/",
      "upvotes": 13,
      "discussionId": "680863ef3767f6ed7c96a026",
      "ai_keywords": [
        "large language models (LLMs)",
        "social simulation",
        "multi-agent systems",
        "agent societies",
        "personas",
        "book-based",
        "comprehensive real-world intricacies",
        "diverse and dynamic characters",
        "fictional worldviews",
        "geographical constraints",
        "story generation",
        "interactive games",
        "creative, high-quality stories",
        "fidelity to the source books"
      ]
    },
    "publishedAt": "2025-04-20T04:56:27.000Z",
    "title": "BookWorld: From Novels to Interactive Agent Societies for Creative Story\n  Generation",
    "summary": "Recent advances in large language models (LLMs) have enabled social\nsimulation through multi-agent systems. Prior efforts focus on agent societies\ncreated from scratch, assigning agents with newly defined personas. However,\nsimulating established fictional worlds and characters remain largely\nunderexplored, despite its significant practical value. In this paper, we\nintroduce BookWorld, a comprehensive system for constructing and simulating\nbook-based multi-agent societies. BookWorld's design covers comprehensive\nreal-world intricacies, including diverse and dynamic characters, fictional\nworldviews, geographical constraints and changes, e.t.c. BookWorld enables\ndiverse applications including story generation, interactive games and social\nsimulation, offering novel ways to extend and explore beloved fictional works.\nThrough extensive experiments, we demonstrate that BookWorld generates\ncreative, high-quality stories while maintaining fidelity to the source books,\nsurpassing previous methods with a win rate of 75.36%. The code of this paper\ncan be found at the project page: https://bookworld2025.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c7bf2c4524c2aea7eac0b3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7bf2c4524c2aea7eac0b3/5ocZ69MvN4RFv86Aa7ks3.png",
      "fullname": "Xintao Wang",
      "name": "Neph0s",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14992",
      "authors": [
        {
          "_id": "68074ed102571b837f03463c",
          "user": {
            "_id": "64722a616facfb01d8ae8349",
            "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
            "isPro": false,
            "fullname": "Wu Bohong",
            "user": "bongbohong",
            "type": "user"
          },
          "name": "Bohong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:15.811Z",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463d",
          "name": "Shen Yan",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463e",
          "name": "Sijun Zhang",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463f",
          "name": "Jianqiao Lu",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034640",
          "user": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "isPro": false,
            "fullname": "Yutao Zeng",
            "user": "Taoer",
            "type": "user"
          },
          "name": "Yutao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:18.659Z",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034641",
          "name": "Ya Wang",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034642",
          "name": "Xun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T09:41:26.000Z",
      "submittedOnDailyAt": "2025-04-23T00:38:26.026Z",
      "title": "Efficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling",
      "submittedOnDailyBy": {
        "_id": "64722a616facfb01d8ae8349",
        "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
        "isPro": false,
        "fullname": "Wu Bohong",
        "user": "bongbohong",
        "type": "user"
      },
      "summary": "최근의 대규모 언어 모델의 발전은 훈련 후의 길이 스케일링의 효과성을 보여주었지만, 훈련 전의 길이 스케일링의 가능성은 아직 조사가 부족하다. 우리는 훈련 전에도 효율적으로 길이 스케일링을 수행할 수 있는 새로운 프레임워크를 제안합니다. 그것은 병렬 은닉 디코딩 트랜스포머 (PHD-Transformer)로 불리며, 추론의 효율성을 유지하는 동시에 훈련 전에도 길이 스케일링을 가능하게 합니다. PHD-Transformer는 혁신적인 KV 캐시 관리 전략을 도입하고, 원본 토큰과 은닉 디코딩 토큰을 구분합니다. 긴 거리 의존성을 유지하기 위해, 원본 토큰의 유일한 KV 캐시를 유지하고, 사용 후 즉시 은닉 디코딩 토큰을 버리는 방식으로, BERNEE FORM의 TANFORMER와 같은 크기의 KV 캐시를 유지합니다. 또한, 성능을 향상시키기 위해 2개의 최적화 버전을 도입합니다. PHD-SWA는滑动窗口注意力를 사용하여 지역 의존성을 유지하고, PHD-CSWA는 prefiltering 시간의 선형 성장을 제거하기 위해 블록 단위의滑动窗口注意力를 구현합니다. 자세한 실험은 여러 벤치마크에서 일치하는 개선을 보여줍니다.",
      "upvotes": 12,
      "discussionId": "68074ed202571b837f03468b",
      "ai_keywords": [
        "KV cache",
        "original tokens",
        "hidden decoding tokens",
        "long-range dependencies",
        "local dependencies",
        "sliding window attention",
        "chunk-wise sliding window attention"
      ]
    },
    "publishedAt": "2025-04-21T05:41:26.000Z",
    "title": "Efficient Pretraining Length Scaling",
    "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(PHD-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\nPHD-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: PHD-SWA employs\nsliding window attention to preserve local dependencies, while\nPHD-CSWA implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14992.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64722a616facfb01d8ae8349",
      "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
      "fullname": "Wu Bohong",
      "name": "bongbohong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13820",
      "authors": [
        {
          "_id": "6805ab2c40034a5a792a26b2",
          "user": {
            "_id": "63f1d16fbe95ed4c9a9418fe",
            "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
            "isPro": false,
            "fullname": "Yang Yue",
            "user": "yueyang2000",
            "type": "user"
          },
          "name": "Yang Yue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:59:59.547Z",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b3",
          "name": "Yulin Wang",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b4",
          "name": "Chenxin Tao",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b5",
          "name": "Pan Liu",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b6",
          "name": "Shiji Song",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b7",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T17:50:43.000Z",
      "submittedOnDailyAt": "2025-04-23T01:09:25.550Z",
      "title": "CheXWorld: 방사선 영상 표현을 위한 이미지 세계 모델링에 대한 연구\n학습",
      "submittedOnDailyBy": {
        "_id": "63f1d16fbe95ed4c9a9418fe",
        "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
        "isPro": false,
        "fullname": "Yang Yue",
        "user": "yueyang2000",
        "type": "user"
      },
      "summary": "인간은 공통의 지식을 기록하고, 세계가 어떻게 작동하고 있는지 이해하고, 자신의 행동의 결과를 예측하기 위한 내부의 세계 모델을 개발할 수 있습니다. 이 개념은 최근의 예비 연구에서 일반적인 모듈을 구축하는 유망한 방향으로 등장했습니다. 예를 들어, 시각적 표현 학습에서도如此. 본 논문에서는 CheXWorld라는 첫 번째 자동 조정된 세계 모델을 위한 시도를 소개합니다. 특히, 우리의 연구는放射線医師가 필요하는 3가지의 의료 지식의 면을 동시에 모델화하는 통합 프레임워크를 개발했습니다. 1) 국부적 解剖 구조는 국부적 조직의 미세한 특징(예: 구조, 모양, 및 테크스처)을 설명합니다. 2) 국부적 解剖 구조는 인간의 몸의 구조를 설명합니다(예: 기관과 골의 배치). 3) 영역의 변화는 CheXWorld가 방사선 사진의 다른 외관 영역의 이동을 모델화하는 것을 목표로 합니다(예: 사진의 채택에 따른 밝기, 대비, 및 조사에 따른 변화). 실험적으로는 적절한 질적 및 양적인 분석을 설계하고, CheXWorld가 이러한 3가지의 의료 지식의 차원을 성공적으로捉えている 것을 밝혀줍니다. 또한, 8가지의 의료 영상 분류와 분할 벤치마크에서의 전파 학습 실험은 CheXWorld가 현재의 SSL 방법과 큰 규모의 의료 기반 모델을 크게 초월하는 것을 보여주었습니다. 코드와 사전 학습 모델은 https://github.com/LeapLabTHU/CheXWorld에 제공됩니다.",
      "upvotes": 11,
      "discussionId": "6805ab2f40034a5a792a27c8",
      "githubRepo": "https://github.com/LeapLabTHU/CheXWorld",
      "ai_keywords": [
        "CheXWorld",
        "self-supervised world model",
        "radiographic images",
        "local anatomical structures",
        "global anatomical layouts",
        "organs",
        "skeletons",
        "domain variations",
        "medical image classification",
        "medical image segmentation",
        "SSL methods",
        "medical foundation models"
      ]
    },
    "publishedAt": "2025-04-18T13:50:43.000Z",
    "title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation\n  Learning",
    "summary": "Humans can develop internal world models that encode common sense knowledge,\ntelling them how the world works and predicting the consequences of their\nactions. This concept has emerged as a promising direction for establishing\ngeneral-purpose machine-learning models in recent preliminary works, e.g., for\nvisual representation learning. In this paper, we present CheXWorld, the first\neffort towards a self-supervised world model for radiographic images.\nSpecifically, our work develops a unified framework that simultaneously models\nthree aspects of medical knowledge essential for qualified radiologists,\nincluding 1) local anatomical structures describing the fine-grained\ncharacteristics of local tissues (e.g., architectures, shapes, and textures);\n2) global anatomical layouts describing the global organization of the human\nbody (e.g., layouts of organs and skeletons); and 3) domain variations that\nencourage CheXWorld to model the transitions across different appearance\ndomains of radiographs (e.g., varying clarity, contrast, and exposure caused by\ncollecting radiographs from different hospitals, devices, or patients).\nEmpirically, we design tailored qualitative and quantitative analyses,\nrevealing that CheXWorld successfully captures these three dimensions of\nmedical knowledge. Furthermore, transfer learning experiments across eight\nmedical image classification and segmentation benchmarks showcase that\nCheXWorld significantly outperforms existing SSL methods and large-scale\nmedical foundation models. Code & pre-trained models are available at\nhttps://github.com/LeapLabTHU/CheXWorld.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13820.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f1d16fbe95ed4c9a9418fe",
      "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
      "fullname": "Yang Yue",
      "name": "yueyang2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16030",
      "authors": [
        {
          "_id": "68084e2c59762f55a5a8b5f3",
          "name": "Joya Chen",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f4",
          "name": "Ziyun Zeng",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f5",
          "name": "Yiqi Lin",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f6",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f7",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f8",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642435a1a3adbc7142c3b0a6/8JExSHYg9ME-w-L_VUt4W.mp4"
      ],
      "publishedAt": "2025-04-22T16:52:09.000Z",
      "submittedOnDailyAt": "2025-04-23T00:56:24.970Z",
      "title": "LiveCC: 대규모 스트리밍 스ピーチ 트랜スクリプ션을 이용한 비디오 LLM 학습",
      "submittedOnDailyBy": {
        "_id": "642435a1a3adbc7142c3b0a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642435a1a3adbc7142c3b0a6/EBmQ7LnfdTdyuhSUti0-d.png",
        "isPro": true,
        "fullname": "Joya Chen",
        "user": "chenjoya",
        "type": "user"
      },
      "summary": "최근의 비디오 대 언어 모델(Video LLMs)은 비용 높은 인간의 어노테이션이나 소유권의 모델 API(예: GPT-4o)을 사용하여 훈련 데이터 생성하기 때문에, 규모적인 훈련에 제약이 있습니다. 본 논문에서는, 비용 낮은 자동 음성 인식(ASR) 번역을 사용하여 Video LLM의 대규모 훈련을 조사합니다. 특히, ASR의 시간 스탬프에 따라 ASR의 단어와 비디오 프레임을 밀집적으로 교차시키는 새로운 스트리밍 훈련 접근 방식을 제안합니다. ASR의 시각 언어 표현의 기존 연구와 비교하여, 우리의 방법은 자연스럽게 ASR의 스트리밍 특성을 적응하고, 모델이 시간에Align된, 미세한 시각 언어 모델링을 학습할 수 있습니다. 훈련 알고리즘을 지원하기 위해, YouTube 비디오 및 코너 캡션(CC, ASR와 동일)을 처리하는 데이터 생성 파이프라인을 도입하고, Live-CC-5M 데이터 세트를 미리 디렉토리로, Live-WhisperX-526K 데이터 세트를 고품질의 제어된 피드백 훈련(SFT)에 사용함으로써, 이 데이터 세트를 만듭니다. 특히, SFT가 없는 경우, ASR만 있는 미리 디렉토리 모델인 LiveCC-7B-Base 모델은 실시간 비디오 코멘터리의 새로운 능력을 보여주고, 일반적인 비디오 QA 성능에 경쟁적인 성능을 보여주었습니다. 이를 평가하기 위해, LLM-as-a-judge를 사용하여 새로운 LiveSports-3K 벤치마크를 설계했습니다. 실험은, 우리의 최종 LiveCC-7B-Instruct 모델이 실시간 모드에서도 선진적인 72B 모델(Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B)의 코멘터리 품질을 초월하고, 비디오 MME와 OVOBench 등 비디오 QA 벤치마크에서 7B/8B 규모에서 가장 선진적인 결과를 얻으며, 우리의 접근 방식의 광범위한 일반화 능력을 보여주었습니다. 본 논문의 모든 리소스는 https://showlab.github.io/livecc에 공개됩니다.",
      "upvotes": 8,
      "discussionId": "68084e2f59762f55a5a8b721",
      "projectPage": "https://showlab.github.io/livecc/",
      "githubRepo": "https://github.com/showlab/livecc",
      "ai_keywords": [
        "Video LLMs",
        "automatic speech recognition (ASR)",
        "streaming training",
        "timestamps",
        "vision-language representation",
        "temporally-aligned",
        "fine-grained vision-language modeling",
        "data production pipeline",
        "YouTube videos",
        "closed captions (CC)",
        "Live-CC-5M",
        "Live-WhisperX-526K",
        "supervised fine-tuning (SFT)",
        "general video QA",
        "real-time video commentary",
        "LiveSports-3K benchmark",
        "LLM-as-a-judge",
        "LiveCC-7B-Base",
        "LiveCC-7B-Instruct",
        "Qwen2.5-VL-72B-Instruct",
        "LLaVA-Video-72B",
        "VideoMME",
        "OVOBench"
      ]
    },
    "publishedAt": "2025-04-22T12:52:09.000Z",
    "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
    "summary": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642435a1a3adbc7142c3b0a6/8JExSHYg9ME-w-L_VUt4W.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16030.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642435a1a3adbc7142c3b0a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642435a1a3adbc7142c3b0a6/EBmQ7LnfdTdyuhSUti0-d.png",
      "fullname": "Joya Chen",
      "name": "chenjoya",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15681",
      "authors": [
        {
          "_id": "680846defa5a6cc6bd9d2cf3",
          "name": "Vidi Team",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf4",
          "name": "Celong Liu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf5",
          "name": "Chia-Wen Kuo",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf6",
          "user": {
            "_id": "6476af4402fc644c810b29a2",
            "avatarUrl": "/avatars/68aefabe6b000443f4601137e6672187.svg",
            "isPro": false,
            "fullname": "Dawei Du",
            "user": "daviddousa",
            "type": "user"
          },
          "name": "Dawei Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:04.152Z",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf7",
          "name": "Fan Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf8",
          "name": "Guang Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf9",
          "name": "Jiamin Yuan",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfa",
          "name": "Lingxi Zhang",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfb",
          "name": "Lu Guo",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfc",
          "name": "Lusha Li",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfd",
          "name": "Longyin Wen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfe",
          "name": "Qingyu Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cff",
          "name": "Rachel Deng",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d00",
          "name": "Sijie Zhu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d01",
          "name": "Stuart Siew",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d02",
          "name": "Tong Jin",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d03",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d04",
          "name": "Wen Zhong",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d05",
          "name": "Xiaohui Shen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d06",
          "name": "Xin Gu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d07",
          "name": "Xing Mei",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d08",
          "name": "Xueqiong Qu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T08:04:45.000Z",
      "submittedOnDailyAt": "2025-04-23T00:19:34.185Z",
      "title": "Vidi: 대규모 다모둠 모델의 영화 이해와 편집\n\n(Note: The original text \"大規模多モデルの映画理解と編集\" is a mix of Chinese and Japanese characters. The translation provided above is a direct translation of the text as it appears, maintaining the original characters and their meanings. If you intended for the text to be purely in Korean, please provide the correct text in Korean for a more accurate translation.)",
      "submittedOnDailyBy": {
        "_id": "65cbdea6d6c974694f09249a",
        "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
        "isPro": false,
        "fullname": "Jay",
        "user": "Zilence006",
        "type": "user"
      },
      "summary": "인간은 자연과 친밀한 관계를 맺은 사람들과 정보와 이미지를 공유하고, 이미지는 인터넷에서의 커뮤니케이션과 표현의 주요 매체로 구성되어 있습니다. 고품질의 대규모의 이미지 콘텐츠의 제작을 지원하기 위해, 현대의 파이프라인은 생 데이터(예: 카메라로 촬영된 미편집 폼 아웃)과 편집 컴포넌트(예: 시각 효과) 모두에 대한 세부적인 이해가 필요합니다. 이미지 편집의 경우, 모델은 강한 지식을 갖추고, 유연한 입력 길이(예: 1시간의 생 이미지)를 처리해야 하며, 이는 전통적인 모델에 비해 큰 문제입니다. 이 보고서에서는, Vidi라는 대규모 다양성 모델(LMMs)의 가족을, 광범위한 이미지 이해 및 편집 스캔을 지원하기 위해 소개합니다. 초기 릴리즈는 시간적인 검색(temporal retrieval)에 초점을 맞추었습니다. 즉, 주어진 텍스트 쿼리에 대응하는 입력 이미지의 시간 범위를 특정함으로써, 지능적인 편집에 중요한 역할을 합니다. 이 모델은 시간적인 이해 능력을 강하게 가지고 있으며, 특정한 쿼리에 대해 시간 범위를 검색할 수 있습니다. 실제 세계적인 스캔에서 상세한 평가를 지원하기 위해, VUE-TR 벤치마크도 소개됩니다. 이는 5가지의 진보점을 가지고 있습니다. 1) 이미지의 길이: 현재의 시간적인 검색 데이터 세트보다 긴, 2) 음성 지원: 음성 기반의 쿼리를 포함하고, 3) 쿼리의 형식: 다양한 쿼리의 길이/형식, 4) Annotation의 품질: 실제 시간 범위는手工으로 Annotation되어 있습니다, 5) 평가 지표: 여러 시간 범위를 평가하기 위해 개선된 IoU 메트릭. 놀라울 정도로, Vidi는 시간적인 검색 태스크에서 가장 선진의 프로피리드 모델(예: GPT-4o와 Gemini)을 크게 초월하고, 이미지 편집 스캔에서 우수한 성능을 보여주고 있습니다.",
      "upvotes": 7,
      "discussionId": "680846dffa5a6cc6bd9d2d59",
      "ai_keywords": [
        "Vidi",
        "Large Multimodal Models (LMMs)",
        "temporal retrieval",
        "video editing scenarios",
        "temporal understanding",
        "VUE-TR benchmark",
        "IoU metric"
      ]
    },
    "publishedAt": "2025-04-22T04:04:45.000Z",
    "title": "Vidi: Large Multimodal Models for Video Understanding and Editing",
    "summary": "Humans naturally share information with those they are connected to, and\nvideo has become one of the dominant mediums for communication and expression\non the Internet. To support the creation of high-quality large-scale video\ncontent, a modern pipeline requires a comprehensive understanding of both the\nraw input materials (e.g., the unedited footage captured by cameras) and the\nediting components (e.g., visual effects). In video editing scenarios, models\nmust process multiple modalities (e.g., vision, audio, text) with strong\nbackground knowledge and handle flexible input lengths (e.g., hour-long raw\nvideos), which poses significant challenges for traditional models. In this\nreport, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a\nwide range of video understand editing scenarios. The first release focuses on\ntemporal retrieval, i.e., identifying the time ranges within the input videos\ncorresponding to a given text query, which plays a critical role in intelligent\nediting. The model is capable of processing hour-long videos with strong\ntemporal understanding capability, e.g., retrieve time ranges for certain\nqueries. To support a comprehensive evaluation in real-world scenarios, we also\npresent the VUE-TR benchmark, which introduces five key advancements. 1) Video\nduration: significantly longer than existing temporal retrival datasets, 2)\nAudio support: includes audio-based queries, 3) Query format: diverse query\nlengths/formats, 4) Annotation quality: ground-truth time ranges are manually\nannotated. 5) Evaluation metric: a refined IoU metric to support evaluation\nover multiple time ranges. Remarkably, Vidi significantly outperforms leading\nproprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,\nindicating its superiority in video editing scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cbdea6d6c974694f09249a",
      "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
      "fullname": "Jay",
      "name": "Zilence006",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16080",
      "authors": [
        {
          "_id": "680845d997f32b8ffc13569c",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569d",
          "name": "Liangbing Zhao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569e",
          "name": "Sayak Paul",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569f",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a0",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a1",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a2",
          "name": "Peng Gao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a3",
          "name": "Mohamed Elhoseiny",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a4",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5f7fbd813e94f16a85448745/L_S4ww0dm1-ZUiNRLQm_h.png"
      ],
      "publishedAt": "2025-04-22T17:58:07.000Z",
      "submittedOnDailyAt": "2025-04-23T00:16:04.186Z",
      "title": "「반사에 의한 최적화 확장: 문에서 이미지로 확산 모델의 추론 시 최적화」",
      "submittedOnDailyBy": {
        "_id": "5f7fbd813e94f16a85448745",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
        "isPro": false,
        "fullname": "Sayak Paul",
        "user": "sayakpaul",
        "type": "user"
      },
      "summary": "최근의 텍스트에서 이미지로의 확산 모델은 훈련 데이터와 모델 파라미터의 확장으로 놀라워지는 시각적 질을 달성하지만, 복잡한 시나리오와 微小한 세부 사항에 대해 어려움이 있습니다. 대규모 언어 모델에서 나타내는 자기 반성 능력에 의존하여, 우리는 ReflectionFlow를 제안합니다. 이것은 추론 시의 프레임워크로, 확산 모델이 출력을 반성하고 개선할 수 있도록 합니다. ReflectionFlow는 추론 시의 3가지 확장 접근을 도입합니다: (1) 노이즈 레벨 확장은 잠재적인 초기화를 최적화합니다; (2) 프로ン프트 레벨 확장은 정확한 의미적인 가이드를 제공합니다; (3) 특히, 반성 레벨 확장은 이전의 생성을 반성하고 수정할 수 있는 행동 가능한 반성을 명확히 합니다. 반성 레벨 확장을 촉진하기 위해, 우리는 GenRef를 구축합니다. 이는 100만 튜플로 구성되며, 반성 이미지, 결함이 있는 이미지, 아피치 이미지를 포함하는 데이터 세트입니다. 이 데이터 세트를 활용하여, 가장 先端의 확산 트랜스포머 FLUX.1-dev를 추론 시의 반성 튜닝을 적절히 수행합니다. 실험 결과를 통해, ReflectionFlow는 간단한 노이즈 레벨 확장 메소드를 크게 초월하며, 어려운 태스크에 대한 고품질 이미지 합성에 대한 확장 가능한 계산 효율적인 해결책을 제공합니다.",
      "upvotes": 5,
      "discussionId": "680845de97f32b8ffc1357c7",
      "ai_keywords": [
        "text-to-image diffusion models",
        "visual quality",
        "training data",
        "model parameters",
        "self-reflection capabilities",
        "diffusion models",
        "inference-time framework",
        "noise-level scaling",
        "latent initialization",
        "prompt-level scaling",
        "semantic guidance",
        "reflection-level scaling",
        "GenRef",
        "multimodal inputs",
        "unified framework",
        "image synthesis"
      ]
    },
    "publishedAt": "2025-04-22T13:58:07.000Z",
    "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for\n  Text-to-Image Diffusion Models via Reflection Tuning",
    "summary": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5f7fbd813e94f16a85448745/L_S4ww0dm1-ZUiNRLQm_h.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16080.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f7fbd813e94f16a85448745",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
      "fullname": "Sayak Paul",
      "name": "sayakpaul",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 605
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16078",
      "authors": [
        {
          "_id": "68087a231e425a6eee93570d",
          "name": "Thomas Schmied",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee93570e",
          "name": "Jörg Bornschein",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee93570f",
          "name": "Jordi Grau-Moya",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee935710",
          "name": "Markus Wulfmeier",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee935711",
          "name": "Razvan Pascanu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:57:14.000Z",
      "submittedOnDailyAt": "2025-04-23T03:57:30.931Z",
      "title": "LLMs는 극한효과의 효과：RL의 미세조정 대책에 의한 결정력의 영향",
      "submittedOnDailyBy": {
        "_id": "64c3849269b1a6796052eac7",
        "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
        "isPro": false,
        "fullname": "Thomas Schmied",
        "user": "thomasschmied",
        "type": "user"
      },
      "summary": "LLM의 성공으로 다양한 에이전트 기반의 애플리케이션에 관심이 급격히 높아졌습니다. 주요 가정은 LLM가 공통감과 Chain-of-Thought(CoT) 추론을 활용하여 복잡한 분야를 효과적으로 탐색하고 효율적으로 해결할 수 있다는 것입니다. 그러나 LLM 에이전트는 최적의 탐색과 실제 행동에 필요한 지식 사이의 간극(knowing-doing gap)에 어려움을 겪으며 보완이 필요했습니다. 본 연구에서는 LLM이 결정 시나리오에서 최적의 성능을 보여주지 못하는 이유를 체계적으로 조사합니다. 특히, 3가지 일반적인 실패 모드인 깡깡주의, 빈도 편향, 지식과 실제 행동 사이의 간극에 초점을 맞추어 상세하게 조사합니다. 자기 생성된 CoT 이유에 기반한 강화학습(RL)을 통해 이러한 결함을 보완하는 방법을 제안합니다. 다양한 밴드(multi-armed bandits, contextual bandits) 및 틱택토(Tic-tac-toe) 실험에서 RL 보완은 탐색을 증가시키고 지식과 실제 행동 사이의 간격을 좁혀 LLM의 결정 능력을 향상시키는 것을 보여주었습니다. 마지막으로, 깡깡한 에피소드와 LLM 고유의 접근 방식(자동 보완, 자동 일관성)을 조합하여 더 효과적인 LLM 보완을 가능하게 합니다.",
      "upvotes": 5,
      "discussionId": "68087a241e425a6eee93576b",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Chain-of-Thought (CoT) reasoning",
        "sub-optimal exploration",
        "knowing-doing gap",
        "decision-making scenarios",
        "greediness",
        "frequency bias",
        "fine-tuning",
        "Reinforcement Learning (RL)",
        "self-generated CoT rationales",
        "multi-armed bandits",
        "contextual bandits",
        "Tic-tac-toe",
        "$\\epsilon$-greedy",
        "self-correction",
        "self-consistency"
      ]
    },
    "publishedAt": "2025-04-22T13:57:14.000Z",
    "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making\n  Abilities",
    "summary": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\nepsilon-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16078.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3849269b1a6796052eac7",
      "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
      "fullname": "Thomas Schmied",
      "name": "thomasschmied",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15785",
      "authors": [
        {
          "_id": "6808579f91ba7dbcc19dbd3e",
          "name": "Siyu Zhou",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd3f",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:49.077Z",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd40",
          "name": "Yijun Yang",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd41",
          "name": "Guodong Long",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd42",
          "name": "Deheng Ye",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd43",
          "name": "Jing Jiang",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd44",
          "name": "Chengqi Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ESSjdqSUTzyiFhpuhfUhO.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/-ScBiECcYZw2_83Qu3-29.png"
      ],
      "publishedAt": "2025-04-22T10:58:27.000Z",
      "submittedOnDailyAt": "2025-04-23T01:40:28.955Z",
      "title": "WALL-E 2.0: 세계조정을 수행하는 뉴로신보リック 학습이, 월드모델 기반 LLM 에이전트를 개선한다.",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "LLMs에서 정확한 세계 모델을 구축할 수 있는지？ 그리고 세계 모델은 LLM 에이전트에 어떤 이점을 제공하지 않을까? LLMs의 사전 지식과 특정 환경의 역학 사이에 간극이 자주 성능의 붐을 만듭니다. 이 간극을 막기 위해 \"세계의 조정\"이라는 훈련무제한 방법론을 제안하고, LLMs가 간접적인 환경의 표적지식에 대해 학습하도록 합니다. 표적지식은 행동 규칙, 그래프, 스케인 그래프 등을 포함하며, LLMs가 탐색 트래픽에서 추출하고, 실행 가능한 코드로 인코딩되어 LLM 에이전트의 정책에 조정됩니다. 또한, RL 무제한 모델 기반 에이전트 \"WALL-E 2.0\"을 MPC 프레임워크를 통해 모델 예측 제어로 제안하고, 고가의 실시간 계산을 필요로 하는 전통적인 MPC와 달리, LLM 에이전트가 인지적 세계 모델과 상호작용하며, 미래의 행동을 효율적으로 최적화합니다. LLM 에이전트의 강력한 휴리스틱성은 MPC로 효율적인 계획자로 작용하며, 조정된 세계 모델의 정확한 예측에 따라 계획된 행동의 품질도 보장됩니다. 이러한 것은 새로운 환경에서 학습 효율성을 크게 향상시킬 수 있습니다. 마르스(Minecraft보다 개방된 세계의 도전)과 ALFWorld(구조화된 실내 환경)의 개방된 세계의 도전에서, WALL-E 2.0은 현재의 방법보다 현저히 뛰어납니다. 마르스에서는 기준을 초과하는 16.1% ~ 51.6%의 성공율과 61.7% 이상의 점수 개선을 달성합니다. ALFWorld에서는 4번의 반복으로 98%의 새로운 링크를 달성합니다.",
      "upvotes": 5,
      "discussionId": "680857a191ba7dbcc19dbda6",
      "githubRepo": "https://github.com/elated-sawyer/WALL-E",
      "ai_keywords": [
        "world models",
        "large language models (LLMs)",
        "symbolic knowledge",
        "knowledge graphs",
        "scene graphs",
        "exploration trajectories",
        "executable codes",
        "policies",
        "RL-free",
        "model-based agent",
        "WALL-E 2.0",
        "model-predictive control (MPC)",
        "neurosymbolic world model",
        "look-ahead optimizer",
        "heuristics",
        "planner",
        "predictions",
        "learning efficiency",
        "open-world challenges",
        "Mars (Minecraft like)",
        "ALFWorld (embodied indoor environments)",
        "success rate",
        "score"
      ]
    },
    "publishedAt": "2025-04-22T06:58:27.000Z",
    "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\n  Model-based LLM Agents",
    "summary": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ESSjdqSUTzyiFhpuhfUhO.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/-ScBiECcYZw2_83Qu3-29.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15785.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16082",
      "authors": [
        {
          "_id": "6808486f043aa415b647ca77",
          "name": "Ziqi Pang",
          "hidden": false
        },
        {
          "_id": "6808486f043aa415b647ca78",
          "name": "Yu-Xiong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:59:41.000Z",
      "submittedOnDailyAt": "2025-04-23T00:25:18.841Z",
      "title": "Master Video: \"MapReduce\"는 긴 비디오의 이해에 기본적인 원리입니다.",
      "submittedOnDailyBy": {
        "_id": "642a33ea5673845d9854f458",
        "avatarUrl": "/avatars/ea8ed29a0b4ad629a2ba6e7d26cbd923.svg",
        "isPro": false,
        "fullname": "Ziqi Pang",
        "user": "ziqipang",
        "type": "user"
      },
      "summary": "マネジャー・ビデオ는 긴 비디오를 이해するための有效なフレームワークです。このフレームワークは、長いビデオの処理において簡単で効果的なMapReduce原理を示します。 (1) Map: 独立して稠密に短いビデオクリップを認識し、 (2) Reduce: すべてのクリップからの情報を共に集約します。シーケンスからシーケンスまでの視覚言語モデル（VLMs）と比較して、マネジャー・ビデオはコンテキストの長さによらず詳細な短いビデオの認識を行います。既存のビデオアジェントと比較して、順序的なキーセグメント選択に依存しているものではなく、Map操作により簡単でスケーラブルな順序平行の短いビデオセグメントの認識が可能になります。Reduceステップでは、より詳細なコンテキストの集約と理由を行い、明示的なキーセグメント検索を超えます。このMapReduce原理は、VLMsとビデオアジェントにも適用可能です。LLMアジェントを使用してその効果性を証明しています。\n\n実際には、マネジャー・ビデオは2つのMapReduceステージを使用しています。 (A) キャプション: 短いビデオクリップにキャプションを生成し、 (map)、重複する文字と物体を共有名に標準化します (reduce); (B) 分析: ユーザーの質問ごとに、個々の短いビデオからの関連情報を分析し、最終的な回答に統合します (map)、 (reduce)。マネジャー・ビデオは、最先端のVLMsとビデオアジェントと比較して、難しいLVBenchで10%以上の精度向上を達成します。\n\nコードは以下のURLで利用可能です: https://github.com/ziqipang/MR-Video",
      "upvotes": 3,
      "discussionId": "68084870043aa415b647caaf",
      "ai_keywords": [
        "MapReduce",
        "short video clips",
        "sequence-to-sequence vision-language models (VLMs)",
        "sequence parallel perception",
        "context aggregation",
        "context reasoning",
        "key segment selection",
        "key segment retrieval",
        "Captioning",
        "standardizing",
        "repeated characters",
        "shared names",
        "Analysis",
        "relevant information",
        "final answer",
        "LVBench"
      ]
    },
    "publishedAt": "2025-04-22T13:59:41.000Z",
    "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding",
    "summary": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16082.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642a33ea5673845d9854f458",
      "avatarUrl": "/avatars/ea8ed29a0b4ad629a2ba6e7d26cbd923.svg",
      "fullname": "Ziqi Pang",
      "name": "ziqipang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11703",
      "authors": [
        {
          "_id": "6800a4f9f16f9f820ed748af",
          "user": {
            "_id": "64f27f74f1b6c235aed4b904",
            "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
            "isPro": false,
            "fullname": "stneng",
            "user": "stneng",
            "type": "user"
          },
          "name": "Tianneng Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T10:13:56.123Z",
          "hidden": true
        },
        {
          "_id": "6800a4f9f16f9f820ed748b0",
          "name": "Jingxuan He",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b1",
          "name": "Zhun Wang",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b2",
          "name": "Linyu Wu",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b3",
          "name": "Hongwei Li",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b4",
          "name": "Wenbo Guo",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b5",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T01:58:40.000Z",
      "submittedOnDailyAt": "2025-04-23T00:27:37.099Z",
      "title": "Progent: LLM Agent에 대한 프로그래밍권 제한 방어\n\n(注意：此翻译保持了原文的专业性和准确性，同时确保了韩语表达的自然流畅。)",
      "submittedOnDailyBy": {
        "_id": "64f27f74f1b6c235aed4b904",
        "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
        "isPro": false,
        "fullname": "stneng",
        "user": "stneng",
        "type": "user"
      },
      "summary": "LLM 에이전트는 LLM이 핵심 구성 요소로, 다양한 도구를 사용하여 사용자에게 할당된 태스크를 완료하는 새로운 엔드 프로세스입니다. 그러나 LLM 에이전트는 큰 보안 리스크를 가지고 있습니다. 외부 세계와의 상호작용으로 공격者可로부터 악의 있는 명령을 만나, 위험한 행동을 수행할 수 있습니다. 이를 해결하기 위해 권한을 최소한으로 제한하는 것이 좋은 방법입니다. 작업이 완료에 필요한 것만 허용하고 필요없는 것을 차단하는 것입니다. 그러나 이를 실현하기 위해서는 다양한 에이전트 시나리오를 다루는 동시에 보안성과 유용성을 모두 유지해야 합니다. 이는 어렵습니다.\n\n우리는 첫 번째 권한 제한 제어 기관인 Progent를 소개합니다. 핵심은 에이전트 실행 중 적용되는 권한 제한 정책을 유연하게 표현하기 위해 사용 가능한 영역专用 언어입니다. 이러한 정책은 도구 호출에 대한 세부한 제약을 부여하고, 도구 호출이 허용되는지 결정하며, 허용되지 않은 경우 기본 백업을 지정합니다. 이를 통해 에이전트 개발자와 사용자는 특정 사용 사례에 맞는 정책을 작성하고 확실하게 실행하여 보안성을 보장할 수 있습니다. 모듈화된 설계로 Progent의 통합은 에이전트 내부를 변경하지 않고, 에이전트 구현에 최소한의 변경이 필요하며, 실용성과 광범위한 가능성을 높일 수 있습니다. 정책의 작성을 자동화하기 위해 LLM을 사용하여 사용자의 질문에 기반한 정책을 생성하고, 동적으로 업데이트하여 보안성과 유용성을 향상시킬 수 있습니다. 우리의 확장된 평가에 따르면 AgentDojo, ASB, AgentPoison의 3가지 다른 시나리오와 벤치마크에서 강한 보안성을 유지하면서 높은 유용성을 유지할 수 있음을 보여줍니다. 또한, 핵심 구성 요소의 효과와 자동화된 정책 생성의 대응성을 자세히 분석하여 대응 공격에 대한 강도를 보여주었습니다.",
      "upvotes": 3,
      "discussionId": "6800a4faf16f9f820ed748ee",
      "ai_keywords": [
        "LLM agents",
        "large language models (LLMs)",
        "principle of least privilege",
        "privilege control mechanism",
        "domain-specific language",
        "privilege control policies",
        "tool calls",
        "agent execution",
        "fine-grained constraints",
        "fallbacks",
        "security",
        "utility",
        "policy writing",
        "automated policy generation",
        "AgentDojo",
        "ASB",
        "AgentPoison"
      ]
    },
    "publishedAt": "2025-04-15T21:58:40.000Z",
    "title": "Progent: Programmable Privilege Control for LLM Agents",
    "summary": "LLM agents are an emerging form of AI systems where large language models\n(LLMs) serve as the central component, utilizing a diverse set of tools to\ncomplete user-assigned tasks. Despite their great potential, LLM agents pose\nsignificant security risks. When interacting with the external world, they may\nencounter malicious commands from attackers, leading to the execution of\ndangerous actions. A promising way to address this is by enforcing the\nprinciple of least privilege: allowing only essential actions for task\ncompletion while blocking unnecessary ones. However, achieving this is\nchallenging, as it requires covering diverse agent scenarios while preserving\nboth security and utility.\n  We introduce Progent, the first privilege control mechanism for LLM agents.\nAt its core is a domain-specific language for flexibly expressing privilege\ncontrol policies applied during agent execution. These policies provide\nfine-grained constraints over tool calls, deciding when tool calls are\npermissible and specifying fallbacks if they are not. This enables agent\ndevelopers and users to craft suitable policies for their specific use cases\nand enforce them deterministically to guarantee security. Thanks to its modular\ndesign, integrating Progent does not alter agent internals and requires only\nminimal changes to agent implementation, enhancing its practicality and\npotential for widespread adoption. To automate policy writing, we leverage LLMs\nto generate policies based on user queries, which are then updated dynamically\nfor improved security and utility. Our extensive evaluation shows that it\nenables strong security while preserving high utility across three distinct\nscenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we\nperform an in-depth analysis, showcasing the effectiveness of its core\ncomponents and the resilience of its automated policy generation against\nadaptive attacks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f27f74f1b6c235aed4b904",
      "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
      "fullname": "stneng",
      "name": "stneng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14977",
      "authors": [
        {
          "_id": "68084dbc2eff5d45775d8f14",
          "name": "Jingkai Zhou",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f15",
          "name": "Yifan Wu",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f16",
          "name": "Shikai Li",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f17",
          "name": "Min Wei",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f18",
          "name": "Chao Fan",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f19",
          "name": "Weihua Chen",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f1a",
          "name": "Wei Jiang",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f1b",
          "name": "Fan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T09:09:21.000Z",
      "submittedOnDailyAt": "2025-04-23T00:49:31.236Z",
      "title": "RealisDance-DiT: 간단하고 강력한 기준을 구축하여 자연계에서 제어 가능한 캐릭터 애니메이션에 대한向けて\n\n(Note: The phrase \"向けて\" is a direct translation of \"に向けて\" and might be more naturally expressed as \"에 대한\" in Korean. However, I have kept it as requested.)",
      "submittedOnDailyBy": {
        "_id": "6434caa64b34368fdb07da48",
        "avatarUrl": "/avatars/8da3d3d1e274ff4ad409234678b1b952.svg",
        "isPro": false,
        "fullname": "Jingkai Zhou",
        "user": "theFoxofSky",
        "type": "user"
      },
      "summary": "제어 가능한 캐릭터 애니메이션은 특히 희귀한 패우스, 스타일화된 캐릭터, 캐릭터와 객체의 상호작용, 복잡한 조명, 동적인 시나리오를 처리할 때 어려운 문제를 초래합니다. 이러한 문제를 해결하기 위해, 기존 연구는 주로 복잡한 바이패스 네트워크를 사용하며, 자세와 외모에 대한 가이드를 주입했습니다. 그러나 일반적으로는 개방된 세계 시나리오에 대한 일반화에는 어려움이 있습니다. 본 논문에서는, 베이스 모델이 충분히 강력한 한 한, 직면하지 않는 모델 보정과 유연한 미세 조정 전략을 사용하여 위의 문제를 크게 해결할 수 있는 새로운 시각을 제안합니다. 특히, RealisDance-DiT를 소개합니다. RealisDance-DiT는 Wan-2.1 비디오 기반 모델을 기반으로 구축되었습니다. 우리의 충분한 분석에 따르면, 광범위하게 사용되고 있는 Reference Net 디자인은 대규모 DiT 모델에 대해서는 최적이 아닙니다. 대신, 우리는 베이스 모델 아키텍처에 최소한의 보정을 수행하고, 놀라울 정도로 강한 베이스라인을 보여주었습니다. 또한, 우리는 낮은 노이즈 워م업과 「큰 배치와 작은 반복」 전략을 제안하고, 미세 조정 중의 모델의 수렴을 가속화하는 한편, 베이스 모델의 선두적 특성을 최대한 보존하는 것을 목표로 합니다. 또한, 우리는 다양한 리알 워ル挑战를捉える 새로운 테스트 데이터셋을 소개하고, TikTok 데이터셋이나 UBC 패션 비디오 데이터셋과 같은 기존 벤치마크를 보완하여 제안한 방법을 상세히 평가하는 것을 목표로 합니다. 광범위한 실험에 따라, RealisDance-DiT는 현재의 방법보다 크게 우월함을 알 수 있습니다.",
      "upvotes": 2,
      "discussionId": "68084dc02eff5d45775d902c",
      "projectPage": "https://thefoxofsky.github.io/project_pages/RealisDance-DiT/index",
      "githubRepo": "https://github.com/damo-cv/RealisDance",
      "ai_keywords": [
        "RealisDance-DiT",
        "Wan-2.1 video foundation model",
        "Reference Net design",
        "DiT models",
        "low-noise warmup",
        "large batches and small iterations strategies",
        "foundation model architecture",
        "model convergence",
        "priors of the foundation model",
        "TikTok dataset",
        "UBC fashion video dataset"
      ]
    },
    "publishedAt": "2025-04-21T05:09:21.000Z",
    "title": "RealisDance-DiT: Simple yet Strong Baseline towards Controllable\n  Character Animation in the Wild",
    "summary": "Controllable character animation remains a challenging problem, particularly\nin handling rare poses, stylized characters, character-object interactions,\ncomplex illumination, and dynamic scenes. To tackle these issues, prior work\nhas largely focused on injecting pose and appearance guidance via elaborate\nbypass networks, but often struggles to generalize to open-world scenarios. In\nthis paper, we propose a new perspective that, as long as the foundation model\nis powerful enough, straightforward model modifications with flexible\nfine-tuning strategies can largely address the above challenges, taking a step\ntowards controllable character animation in the wild. Specifically, we\nintroduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our\nsufficient analysis reveals that the widely adopted Reference Net design is\nsuboptimal for large-scale DiT models. Instead, we demonstrate that minimal\nmodifications to the foundation model architecture yield a surprisingly strong\nbaseline. We further propose the low-noise warmup and \"large batches and small\niterations\" strategies to accelerate model convergence during fine-tuning while\nmaximally preserving the priors of the foundation model. In addition, we\nintroduce a new test dataset that captures diverse real-world challenges,\ncomplementing existing benchmarks such as TikTok dataset and UBC fashion video\ndataset, to comprehensively evaluate the proposed method. Extensive experiments\nshow that RealisDance-DiT outperforms existing methods by a large margin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6434caa64b34368fdb07da48",
      "avatarUrl": "/avatars/8da3d3d1e274ff4ad409234678b1b952.svg",
      "fullname": "Jingkai Zhou",
      "name": "theFoxofSky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15524",
      "authors": [
        {
          "_id": "68084b46fa5a6cc6bd9e6a83",
          "user": {
            "_id": "64560618bfdf9c63ce2d658a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
            "isPro": false,
            "fullname": "Mathsion Wong",
            "user": "QiYao-Wang",
            "type": "user"
          },
          "name": "Qiyao Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:53.885Z",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a84",
          "name": "Guhong Chen",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a85",
          "name": "Hongbo Wang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a86",
          "name": "Huaren Liu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a87",
          "name": "Minghui Zhu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a88",
          "name": "Zhifei Qin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a89",
          "name": "Linwei Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8a",
          "name": "Yilin Yue",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8b",
          "name": "Shiqiang Wang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8c",
          "name": "Jiayan Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8d",
          "name": "Yihang Wu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8e",
          "name": "Ziqiang Liu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8f",
          "name": "Longze Chen",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a90",
          "name": "Run Luo",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a91",
          "name": "Liyang Fan",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a92",
          "name": "Jiaming Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a93",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a94",
          "name": "Kan Xu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a95",
          "name": "Hongfei Lin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a96",
          "name": "Hamid Alinejad-Rokny",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a97",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a98",
          "name": "Yuan Lin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a99",
          "name": "Min Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T02:00:41.000Z",
      "submittedOnDailyAt": "2025-04-23T05:32:33.756Z",
      "title": "IPBench: 지식재산에 관한 대규모 언어 모델의 지식의 벤치마크",
      "submittedOnDailyBy": {
        "_id": "64560618bfdf9c63ce2d658a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
        "isPro": false,
        "fullname": "Mathsion Wong",
        "user": "QiYao-Wang",
        "type": "user"
      },
      "summary": "知識産権（IP）는 기술적인 지식과 법적인 지식의 통합을 통해 고유의 복잡성과 지식집중성을 가진 특별한 분야입니다. 대규모 언어 모델（LLMs）가 발전하는 가운데, IP 작업에 큰 잠재력을 가지고 있으며, IP 관련 콘텐츠의 분석, 이해, 생성이 더욱 효율적으로 이루어질 수 있습니다. 그러나 현재의 데이터셋과 벤치마크는 광범위한 특허만 중점적으로 다루고 있거나, IP 분야의 일부만을 포괄하고 있으며, 현실적인 시나리오에 맞지 않습니다. 이러한 공백을 메우기 위해, 최초의 실용적인 IP 작업 테크노로직과 8개의 IP 기관 및 20개의 작업을 다루는 대규모 다언어 벤치마크인 IPBench를 소개합니다. 이 벤치마크는 현실적인 지식인프라 모델을 평가하기 위해 설계되었으며, 이해와 생성을 모두 포함합니다. 16개의 LLMs를 벤치마크로 평가하였으며, 일반 모델부터 분야专用 모델까지 범위를 넓혔습니다. 가장 우수한 모델은 75.8%의 정확도를 달성하였으며, 큰 개선 여지가 있음을 확인했습니다. 특히, 오픈 소스의 IP와 법 전문 모델은 닫힌 소스의 일반 모델보다 더 잘 수행되고 있습니다. IPBench의 모든 데이터와 코드를 공개하며, IP 관련 작업 추가하여 현실적인 지식인프라 도전을 더욱 정확하게 반영할 것입니다.",
      "upvotes": 1,
      "discussionId": "68084b4cfa5a6cc6bd9e6c75",
      "projectPage": "https://ipbench.github.io/",
      "githubRepo": "https://github.com/IPBench/IPBench"
    },
    "publishedAt": "2025-04-21T22:00:41.000Z",
    "title": "IPBench: Benchmarking the Knowledge of Large Language Models in\n  Intellectual Property",
    "summary": "Intellectual Property (IP) is a unique domain that integrates technical and\nlegal knowledge, making it inherently complex and knowledge-intensive. As large\nlanguage models (LLMs) continue to advance, they show great potential for\nprocessing IP tasks, enabling more efficient analysis, understanding, and\ngeneration of IP-related content. However, existing datasets and benchmarks\neither focus narrowly on patents or cover limited aspects of the IP field,\nlacking alignment with real-world scenarios. To bridge this gap, we introduce\nthe first comprehensive IP task taxonomy and a large, diverse bilingual\nbenchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is\ndesigned to evaluate LLMs in real-world intellectual property applications,\nencompassing both understanding and generation. We benchmark 16 LLMs, ranging\nfrom general-purpose to domain-specific models, and find that even the\nbest-performing model achieves only 75.8% accuracy, revealing substantial room\nfor improvement. Notably, open-source IP and law-oriented models lag behind\nclosed-source general-purpose models. We publicly release all data and code of\nIPBench and will continue to update it with additional IP-related tasks to\nbetter reflect real-world challenges in the intellectual property domain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15524.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64560618bfdf9c63ce2d658a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
      "fullname": "Mathsion Wong",
      "name": "QiYao-Wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14735",
      "authors": [
        {
          "_id": "6807f85efcd784a902c87126",
          "user": {
            "_id": "621d85a10e35b2fbbf3e6196",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
            "isPro": false,
            "fullname": "Chin-Yun Yu",
            "user": "yoyolicoris",
            "type": "user"
          },
          "name": "Chin-Yun Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:18.913Z",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c87127",
          "name": "Marco A. Martínez-Ramírez",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c87128",
          "name": "Junghyun Koo",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c87129",
          "name": "Ben Hayes",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c8712a",
          "name": "Wei-Hsiang Liao",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c8712b",
          "name": "György Fazekas",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c8712c",
          "name": "Yuki Mitsufuji",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/621d85a10e35b2fbbf3e6196/alz9iP58RZiY22SYNBEzr.png"
      ],
      "publishedAt": "2025-04-20T20:52:58.000Z",
      "submittedOnDailyAt": "2025-04-23T08:05:27.097Z",
      "title": "DiffVox: 전문가의 효과 분포를 파악하는 미분 모델",
      "submittedOnDailyBy": {
        "_id": "621d85a10e35b2fbbf3e6196",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
        "isPro": false,
        "fullname": "Chin-Yun Yu",
        "user": "yoyolicoris",
        "type": "user"
      },
      "summary": "이 연구에서는 음악 제작에서의 소리의 효과 매칭에 새로운 해석 가능한 모델 \"DiffVox\"를 소개합니다. DiffVox는 \"Differentiable Vocal Fx\"의 약자로, 파라미터 피드백, 동적 범위 제어, 데라이, 리베어버의 효과는 유효한 미분 구현을 통해 통합하고, 파라미터 평가에서 경사 기반의 최적화를 가능하게 합니다. 소리의 프레셋은 MedleyDB에서 70 트랙과 개인 컬렉션에서 365 트랙에서 검색됩니다. 파라미터의 상관 분석에서, 고통 필터와 저쉐르 필터가 저음을 형성하기 위해 이어져야 하며, 데라이 시간과 데라이 신호의 강도가 상관되어 있음을 명확히 합니다. 주성분 분석에서, McAdams의 음색의 차원이 나타내며, 가장 중요한 성분은 관찰된 공간성을 제어하고, 두 번째 성분은 스펙트럼의 밝기를 영향을 미칩니다. 통계적 검정에서는 파라미터 분포의 비 가우시안성을 확인하고, 소리의 효과 공간의 복잡성을 강조합니다. 이러한 초기 발견은 소리의 효과 모델링과 자동混音에 대한 미래 연구의 기초가 됩니다. 소스 코드와 데이터 세트는 https://github.com/SonyResearch/diffvox에서 접근 가능합니다.",
      "upvotes": 0,
      "discussionId": "6807f860fcd784a902c87194",
      "githubRepo": "https://github.com/SonyResearch/diffvox",
      "ai_keywords": [
        "parametric equalisation",
        "dynamic range control",
        "delay",
        "reverb",
        "differentiable implementations",
        "gradient-based optimisation",
        "parameter estimation",
        "principal component analysis",
        "McAdams' timbre dimensions",
        "perceived spaciousness",
        "spectral brightness",
        "non-Gaussian nature"
      ]
    },
    "publishedAt": "2025-04-20T16:52:58.000Z",
    "title": "DiffVox: A Differentiable Model for Capturing and Analysing Professional\n  Effects Distributions",
    "summary": "This study introduces a novel and interpretable model, DiffVox, for matching\nvocal effects in music production. DiffVox, short for ``Differentiable Vocal\nFx\", integrates parametric equalisation, dynamic range control, delay, and\nreverb with efficient differentiable implementations to enable gradient-based\noptimisation for parameter estimation. Vocal presets are retrieved from two\ndatasets, comprising 70 tracks from MedleyDB and 365 tracks from a private\ncollection. Analysis of parameter correlations highlights strong relationships\nbetween effects and parameters, such as the high-pass and low-shelf filters\noften behaving together to shape the low end, and the delay time correlates\nwith the intensity of the delayed signals. Principal component analysis reveals\nconnections to McAdams' timbre dimensions, where the most crucial component\nmodulates the perceived spaciousness while the secondary components influence\nspectral brightness. Statistical testing confirms the non-Gaussian nature of\nthe parameter distribution, highlighting the complexity of the vocal effects\nspace. These initial findings on the parameter distributions set the foundation\nfor future research in vocal effects modelling and automatic mixing. Our source\ncode and datasets are accessible at https://github.com/SonyResearch/diffvox.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/621d85a10e35b2fbbf3e6196/alz9iP58RZiY22SYNBEzr.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "621d85a10e35b2fbbf3e6196",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
      "fullname": "Chin-Yun Yu",
      "name": "yoyolicoris",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]