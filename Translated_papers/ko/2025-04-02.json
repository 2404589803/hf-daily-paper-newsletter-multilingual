[
  {
    "paper": {
      "id": "2503.24379",
      "authors": [
        {
          "_id": "67ec0a7262144ec35d0e571d",
          "user": {
            "_id": "64c139d867eff857ea51caa8",
            "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
            "isPro": false,
            "fullname": "Shengqiong Wu",
            "user": "ChocoWu",
            "type": "user"
          },
          "name": "Shengqiong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:23:35.392Z",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e571e",
          "user": {
            "_id": "6360d9f0472131c3bc4f61df",
            "avatarUrl": "/avatars/c5d884e5ef19b781e3405aba6dd68ca8.svg",
            "isPro": false,
            "fullname": "WeicaiYe",
            "user": "WeicaiYe",
            "type": "user"
          },
          "name": "Weicai Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:36:51.426Z",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e571f",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5720",
          "name": "Quande Liu",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5721",
          "user": {
            "_id": "60e272ca6c78a8c122b12127",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
            "isPro": false,
            "fullname": "Xintao Wang",
            "user": "Xintao",
            "type": "user"
          },
          "name": "Xintao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:38:20.890Z",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5722",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5723",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:38:50.116Z",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5724",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5725",
          "user": {
            "_id": "67eaa070b9fa8908e151fd7d",
            "avatarUrl": "/avatars/1fe2fd678d2e71099a83a9bcb9ab517e.svg",
            "isPro": false,
            "fullname": "shuicheng yan",
            "user": "shuicheng",
            "type": "user"
          },
          "name": "Shuicheng Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:39:18.884Z",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5726",
          "user": {
            "_id": "647773a1168cb428e00e9a8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
            "isPro": false,
            "fullname": "Hao Fei",
            "user": "scofield7419",
            "type": "user"
          },
          "name": "Hao Fei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:23:38.087Z",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5727",
          "user": {
            "_id": "6570ae84c4993b8fb96f41a8",
            "avatarUrl": "/avatars/21f7d79d46ac4df0ecff8eca7678b33f.svg",
            "isPro": false,
            "fullname": "Tat-Seng Chua",
            "user": "chuats",
            "type": "user"
          },
          "name": "Tat-Seng Chua",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:39:25.537Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:59:01.000Z",
      "submittedOnDailyAt": "2025-04-02T01:29:41.083Z",
      "title": "Any2Caption: 조건을 읽어서, 제어 가능한 비디오의 캡션을 생성합니다.",
      "submittedOnDailyBy": {
        "_id": "64c139d867eff857ea51caa8",
        "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
        "isPro": false,
        "fullname": "Shengqiong Wu",
        "user": "ChocoWu",
        "type": "user"
      },
      "summary": "현재 비디오 생성 커뮤니티에서 정확한 사용자 의도 해석에 대한 붕괴점 문제를 해결하기 위해, 우리는 Any2Caption을 소개합니다. 이것은 제어 가능한 비디오 생성의 새로운 프레임워크입니다. 핵심 개념은 비디오 합성 단계에서 다른 조건 해석 단계를 분리하는 것입니다. 현대의 다모달 대언어 모델(MLLMs)을 활용하여, Any2Caption은 텍스트, 이미지, 비디오, 그리고 특수 카테고리(지역, 움직임, 카메라의 자세 등)를 복잡한 입력으로 받아들이고, 높은 밀도의 구조화된 샘플을 생성하여 비디오 생성기를 통해 좋은 가이드를 제공하는 것입니다. 또한, Any2CapIns라는 큰 데이터 세트를 소개합니다. 이 데이터 세트는 337K개의 인스턴스와 407K개의 조건을 가지고 있으며, 어떤 조건에서 샘플링에 대한 지시 조정을 사용할 수 있습니다. 자세한 평가는 현재 비디오 생성 모델의 다양한 측면에서 제어 가능성과 비디오 품질의 향상에 대해 보여줍니다. 프로젝트 페이지는 https://sqwu.top/Any2Cap/입니다.",
      "upvotes": 39,
      "discussionId": "67ec0a7562144ec35d0e57fc",
      "projectPage": "https://sqwu.top/Any2Cap/",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "dense, structured captions",
        "region",
        "motion",
        "camera poses",
        "any-condition-to-caption instruction tuning"
      ]
    },
    "publishedAt": "2025-03-31T13:59:01.000Z",
    "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation",
    "summary": "To address the bottleneck of accurate user intent interpretation within the\ncurrent video generation community, we present Any2Caption, a novel framework\nfor controllable video generation under any condition. The key idea is to\ndecouple various condition interpretation steps from the video synthesis step.\nBy leveraging modern multimodal large language models (MLLMs), Any2Caption\ninterprets diverse inputs--text, images, videos, and specialized cues such as\nregion, motion, and camera poses--into dense, structured captions that offer\nbackbone video generators with better guidance. We also introduce Any2CapIns, a\nlarge-scale dataset with 337K instances and 407K conditions for\nany-condition-to-caption instruction tuning. Comprehensive evaluations\ndemonstrate significant improvements of our system in controllability and video\nquality across various aspects of existing video generation models. Project\nPage: https://sqwu.top/Any2Cap/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24379.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64c139d867eff857ea51caa8",
      "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
      "fullname": "Shengqiong Wu",
      "name": "ChocoWu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24376",
      "authors": [
        {
          "_id": "67eca2b8351721d62aa537df",
          "user": {
            "_id": "60d045c4778bafd0fbcfa3f5",
            "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
            "isPro": false,
            "fullname": "Yi Chen",
            "user": "ChenYi99",
            "type": "user"
          },
          "name": "Yi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:30.143Z",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e0",
          "user": {
            "_id": "6455cc8f654d8bccae50e4d4",
            "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
            "isPro": false,
            "fullname": "Yuying Ge",
            "user": "tttoaster",
            "type": "user"
          },
          "name": "Yuying Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:40:05.288Z",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e1",
          "user": {
            "_id": "62e0f1314db2175cd270ad08",
            "avatarUrl": "/avatars/1d3d6af6c63557f4abf0484e028fa942.svg",
            "isPro": false,
            "fullname": "Rui Wang",
            "user": "ruiwang",
            "type": "user"
          },
          "name": "Rui Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:40:12.320Z",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e2",
          "user": {
            "_id": "640e9762b03f4cd29f58d982",
            "avatarUrl": "/avatars/81da37d628163fe3e094b247c7c3a3b5.svg",
            "isPro": false,
            "fullname": "Yixiao Ge",
            "user": "yxgeee",
            "type": "user"
          },
          "name": "Yixiao Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:40:19.876Z",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e3",
          "name": "Lu Qiu",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e4",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:40:26.551Z",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e5",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:40:32.831Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:55:23.000Z",
      "submittedOnDailyAt": "2025-04-02T01:06:48.435Z",
      "title": "강화학습 효과 조사 및 영화 이해에 대한 영향 분석:\nSEED-Bench-R1에서의 아인사이언스\n\n(注意: \"SEED-Bench-R1\"는 특정 연구나 평가 기준의 이름으로, 이 번역에서 그대로 사용하였습니다.)",
      "submittedOnDailyBy": {
        "_id": "60d045c4778bafd0fbcfa3f5",
        "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
        "isPro": false,
        "fullname": "Yi Chen",
        "user": "ChenYi99",
        "type": "user"
      },
      "summary": "최근의 Chain of Thought (COT) 생성의 발전은 Large Language Models (LLMs)의 논리 능력이 크게 향상되고, 강화학습 (RL)이 후 하이퍼 트레이닝 방법 중 효과적이라는 사실을 명확히 밝혀졌습니다. Multimodal Large Language Models (MLLMs)는 이 논리 잠재력을 이어가고 있지만, 시각적 인식과 논리적인 논리가 필요로 하는 작업에 대해서는 조사가 부족했습니다. 이에 대응하여, SEED-Bench-R1을 소개합니다. SEED-Bench-R1은 MLLM의 후 하이퍼 트레이닝 방법을 체계적으로 평가하기 위해 설계되었습니다. 이는 복잡한 현실 세계의 이미지와 일상적인 계획 태스크를 포함하며, 높은 시각적 인식과 논리적인 논리가 필요로 하는 문제를 요구하는 다중 선택 문제 형식으로 구성되어 있습니다. SEED-Bench-R1은 일반화 능력을 평가하기 위해, 분포 내, 환경 간, 환경 태스크 간의 세 단계 휴리스틱을 사용하여, 큰 훈련 데이터 세트와 쉽게 확인 가능한 정답을 충족하는 것입니다. Qwen2-VL-Instruct-7B을 기반 모델로, RL과 초관찰 학습 (SFT)을 비교하여, RL의 데이터 효율성과 분포 내 및 분포 외 태스크의 우수한 성능을 보여주며, LongVideoBench와 같은 일반적인 이미지 이해 벤치마크에서 SFT를 초과하는 것을 보여주었습니다. 세부적인 분석에서, RL은 시각적 인식을 강화하지만, 논리적인 논리 키가 줄었다는 것을 확인했습니다. 불확실한 논리와 시각적 키를 놓치는 것처럼, 키 한계가 식별되었으며, RL의 기초 모델의 논리, 보상 모델링, 그리고 노이즈 신호에 대한 강건성을 개선하는 미래의 개선을 제안했습니다.",
      "upvotes": 21,
      "discussionId": "67eca2b9351721d62aa53822",
      "githubRepo": "https://github.com/TencentARC/SEED-Bench-R1",
      "ai_keywords": [
        "Chain of Thought (COT)",
        "Large Language Models (LLMs)",
        "reinforcement learning (RL)",
        "Multimodal Large Language Models (MLLMs)",
        "SEED-Bench-R1",
        "video understanding",
        "multiple-choice questions",
        "in-distribution",
        "cross-environment",
        "cross-environment-task scenarios",
        "Qwen2-VL-Instruct-7B",
        "supervised fine-tuning (SFT)",
        "LongVideoBench",
        "visual perception",
        "reasoning chains",
        "inconsistent reasoning",
        "reward modeling"
      ]
    },
    "publishedAt": "2025-03-31T13:55:23.000Z",
    "title": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1",
    "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d045c4778bafd0fbcfa3f5",
      "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
      "fullname": "Yi Chen",
      "name": "ChenYi99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23145",
      "authors": [
        {
          "_id": "67eb710f1f8a09c48b2e3ba1",
          "user": {
            "_id": "674286496efe2b931f7ce354",
            "avatarUrl": "/avatars/8f920618b777dbff5f2a117ebd9e9caa.svg",
            "isPro": false,
            "fullname": "Anjiang Wei",
            "user": "anjiangwei",
            "type": "user"
          },
          "name": "Anjiang Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:42.109Z",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba2",
          "user": {
            "_id": "65e7bb35e5e78134ab049942",
            "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
            "isPro": false,
            "fullname": "Tarun Suresh",
            "user": "tarsur909",
            "type": "user"
          },
          "name": "Tarun Suresh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:23:41.048Z",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba3",
          "name": "Jiannan Cao",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba4",
          "name": "Naveen Kannan",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba5",
          "name": "Yuheng Wu",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba6",
          "user": {
            "_id": "65de7628deee79773f0f46f6",
            "avatarUrl": "/avatars/6c509dbe96e47b47271eb74178c1c9ba.svg",
            "isPro": false,
            "fullname": "Kai Yan",
            "user": "kaiyan289",
            "type": "user"
          },
          "name": "Kai Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:43:08.150Z",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba7",
          "name": "Thiago S. F. X. Teixeira",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba8",
          "name": "Ke Wang",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba9",
          "name": "Alex Aiken",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-29T16:50:39.000Z",
      "submittedOnDailyAt": "2025-04-02T04:28:51.051Z",
      "title": "CodeARC: 논리적 능력의 벤치마크로 인디언프로그래밍 합성을 위한 LLM 에이전트",
      "submittedOnDailyBy": {
        "_id": "65e7bb35e5e78134ab049942",
        "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
        "isPro": false,
        "fullname": "Tarun Suresh",
        "user": "tarsur909",
        "type": "user"
      },
      "summary": "Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning.",
      "upvotes": 20,
      "discussionId": "67eb71101f8a09c48b2e3bee",
      "ai_keywords": [
        "inductive program synthesis",
        "programming by example",
        "function synthesis",
        "input-output examples",
        "large language model agents",
        "natural language",
        "evaluation protocols",
        "feedback mechanism",
        "real-world scenarios",
        "reverse engineering",
        "CodeARC",
        "Code Abstraction and Reasoning Challenge",
        "hidden target function",
        "querying",
        "candidate functions",
        "differential testing oracle",
        "interactive setting",
        "function calls",
        "self-correction",
        "large-scale benchmark",
        "general-purpose inductive program synthesis",
        "fine-tuning",
        "LLaMA-3.1-8B-Instruct",
        "synthesis traces",
        "relative performance gain"
      ]
    },
    "publishedAt": "2025-03-29T12:50:39.000Z",
    "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive\n  Program Synthesis",
    "summary": "Inductive program synthesis, or programming by example, requires synthesizing\nfunctions from input-output examples that generalize to unseen inputs. While\nlarge language model agents have shown promise in programming tasks guided by\nnatural language, their ability to perform inductive program synthesis is\nunderexplored. Existing evaluation protocols rely on static sets of examples\nand held-out tests, offering no feedback when synthesized functions are\nincorrect and failing to reflect real-world scenarios such as reverse\nengineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge,\na new evaluation framework where agents interact with a hidden target function\nby querying it with new inputs, synthesizing candidate functions, and\niteratively refining their solutions using a differential testing oracle. This\ninteractive setting encourages agents to perform function calls and\nself-correction based on feedback. We construct the first large-scale benchmark\nfor general-purpose inductive program synthesis, featuring 1114 functions.\nAmong 18 models evaluated, o3-mini performs best with a success rate of 52.7%,\nhighlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on\ncurated synthesis traces yields up to a 31% relative performance gain. CodeARC\nprovides a more realistic and challenging testbed for evaluating LLM-based\nprogram synthesis and inductive reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23145.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e7bb35e5e78134ab049942",
      "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
      "fullname": "Tarun Suresh",
      "name": "tarsur909",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00050",
      "authors": [
        {
          "_id": "67ec9bf3e58745dc7d652587",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d652588",
          "user": {
            "_id": "64351475901c5734bcb64248",
            "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
            "isPro": false,
            "fullname": "Zhiyuan Hu",
            "user": "zhiyuanhucs",
            "type": "user"
          },
          "name": "Zhiyuan Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:44:43.089Z",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d652589",
          "user": {
            "_id": "6730a1fed66bf1b6378cd451",
            "avatarUrl": "/avatars/5ec9b7313213a951b7c325d35ca26692.svg",
            "isPro": false,
            "fullname": "qy",
            "user": "qingyunzou",
            "type": "user"
          },
          "name": "Qingyun Zou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:44:49.663Z",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d65258a",
          "name": "Jiaying Wu",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d65258b",
          "name": "Qian Wang",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d65258c",
          "user": {
            "_id": "651d8032c50012d33e914f2f",
            "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
            "isPro": false,
            "fullname": "Bryan Hooi",
            "user": "bhooi",
            "type": "user"
          },
          "name": "Bryan Hooi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:45:24.844Z",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d65258d",
          "name": "Bingsheng He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T02:18:51.000Z",
      "submittedOnDailyAt": "2025-04-02T04:52:20.239Z",
      "title": "ジャッジLRM: 대논리 모델을 평가하고\n\n(Note: The translation is provided as requested, maintaining professionalism and accuracy. The model is named \"Judge LRM\" in Korean, which is a direct translation of \"Judge LRM\" from English to Korean. The term \"대논리 모델\" is a direct translation of \"Large Logical Model\" to Korean, and \"평가하고\" is the Korean verb for \"to evaluate\" in the imperative form, which fits the context of the original English text.)",
      "submittedOnDailyBy": {
        "_id": "64351475901c5734bcb64248",
        "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
        "isPro": false,
        "fullname": "Zhiyuan Hu",
        "user": "zhiyuanhucs",
        "type": "user"
      },
      "summary": "LLM의 발전은 인간 기록에 비해 스케일러블한 평가 방법의 기능 제공을 통해 이루어졌지만, 현재의 Supervised Fine-Tuning (SFT)의 판단자 요구에 있어서 복잡한 논리론을 요구하는 분야에서는 그 효과는 제한되어 있습니다. 본 논문에서는 LLM의 판단자가 실제로 복잡한 논리론 능력을 갖게 될 수 있는지 조사하고 있습니다. 평가 태스크의 논리론 요구를 상세히 분석하고, SFT의 성능 향상과 논리론 요구가 높은 샘플의 비율과의 음의 상관관계를 밝혀 SFT의 한계를 보여주고 있습니다. 이를 대처하기 위해 판단자专用의 LLM familiy인 JudgeLRM을 도입하고, 판단자의 지식에 기반한 보상을 제공하는 강화학습(RL)을 사용하여 훈련합니다. JudgeLRM 모델은 SFT 조정된 모델이나 가장 先端的 논리론 모델을 가장 뛰어난 성능으로 보여주고 있습니다. 특히, JudgeLRM-3B는 GPT-4를 초월하고, JudgeLRM-7B는 DeepSeek-R1에 대한 F1 스코어에서 2.79% 높게 나섰으며, 복잡한 논리론이 필요한 판단자 태스크에서 특히 뛰어납니다.",
      "upvotes": 18,
      "discussionId": "67ec9bf4e58745dc7d6525c1",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Supervised Fine-Tuning (SFT)",
        "reasoning capabilities",
        "reinforcement learning (RL)",
        "judge-wise, outcome-driven rewards",
        "JudgeLRM",
        "GPT-4",
        "DeepSeek-R1"
      ]
    },
    "publishedAt": "2025-03-30T22:18:51.000Z",
    "title": "JudgeLRM: Large Reasoning Models as a Judge",
    "summary": "The rise of Large Language Models (LLMs) as evaluators offers a scalable\nalternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for\njudges approaches often fall short in domains requiring complex reasoning. In\nthis work, we investigate whether LLM judges truly benefit from enhanced\nreasoning capabilities. Through a detailed analysis of reasoning requirements\nacross evaluation tasks, we reveal a negative correlation between SFT\nperformance gains and the proportion of reasoning-demanding samples -\nhighlighting the limitations of SFT in such scenarios. To address this, we\nintroduce JudgeLRM, a family of judgment-oriented LLMs trained using\nreinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM\nmodels consistently outperform both SFT-tuned and state-of-the-art reasoning\nmodels. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms\nDeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks\nrequiring deep reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00050.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64351475901c5734bcb64248",
      "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
      "fullname": "Zhiyuan Hu",
      "name": "zhiyuanhucs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.01016",
      "authors": [
        {
          "_id": "67ec958ebb1d6dd924f94a31",
          "user": {
            "_id": "65f8e4778dc7bb5b4db97f92",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/gBmQdovmANmjV72k3gaW8.png",
            "isPro": false,
            "fullname": "Tian-Xing Xu",
            "user": "slothfulxtx",
            "type": "user"
          },
          "name": "Tian-Xing Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:51:06.226Z",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a32",
          "user": {
            "_id": "64c0953a8137192a1e2474dc",
            "avatarUrl": "/avatars/546405a7eaf2f60ad108ceaa0dda7d08.svg",
            "isPro": false,
            "fullname": "xiangjun gao",
            "user": "xiangjun0211",
            "type": "user"
          },
          "name": "Xiangjun Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:51:14.978Z",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a33",
          "user": {
            "_id": "657a7458afbb0117ba15c59f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
            "isPro": false,
            "fullname": "Wenbo Hu",
            "user": "wbhu-tc",
            "type": "user"
          },
          "name": "Wenbo Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:52.313Z",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a34",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a35",
          "name": "Song-Hai Zhang",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a36",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:51:45.869Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/6He3mQcB_AO1G_Sq8xYc0.mp4"
      ],
      "publishedAt": "2025-04-01T17:58:03.000Z",
      "submittedOnDailyAt": "2025-04-02T00:15:17.585Z",
      "title": "기하학 추정에서 일관성 없는 개방 세계 비디오의 Diffusion Priors",
      "submittedOnDailyBy": {
        "_id": "657a7458afbb0117ba15c59f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
        "isPro": false,
        "fullname": "Wenbo Hu",
        "user": "wbhu-tc",
        "type": "user"
      },
      "summary": "映像의 깊이 추론에 대한 놀라운 진보가 있습니다만, 현재의 방법은 Affine 불변 예측을 통해 기하학적인 정확도를 달성하기 위해 고유한 한계가 있으며, 재구성이나 다른 메트릭에 기반한 다음 태스크의 적용 범위가 제한되어 있습니다. 우리는 GeometryCrafter라는 새로운 프레임워크를 제안합니다. 이 프레임워크는 개방된 월드의 이미지에서 시간적 일관성을 유지하는 고품질 포인트 맵 시퀀스를 복원하고 정확한 3D/4D 재구성, 카메라 파라미터 추정 및 다른 깊이 기반 애플리케이션을 가능하게 합니다. 우리의 접근 방식의 핵심은 포인트 맵 바이너리 오토인코더(VAE)입니다. 이 VAE는 이미지의 잠재적 분포와 상관없이 잠재 공간에 대한 학습을 수행하고, 효과적인 인코딩 및 디코딩을 수행할 수 있습니다. VAE를 활용하여, 입력 이미지에 기반한 포인트 맵 시퀀스의 분포를 모델화하기 위한 이미지 디퓨저 모델을 훈련합니다. 다양한 데이터 세트에 대한 확장 평가로, GeometryCrafter는 가장 선진적인 3D 정확도, 시간적 일관성 및 일반화 능력을 달성합니다.",
      "upvotes": 12,
      "discussionId": "67ec9593bb1d6dd924f94b3e",
      "projectPage": "https://geometrycrafter.github.io/",
      "githubRepo": "https://github.com/TencentARC/GeometryCrafter",
      "ai_keywords": [
        "GeometryCrafter",
        "point map Variational Autoencoder (VAE)",
        "latent space",
        "video latent distributions",
        "point map encoding",
        "point map decoding",
        "video diffusion model",
        "point map sequences",
        "3D accuracy",
        "temporal consistency",
        "generalization capability"
      ]
    },
    "publishedAt": "2025-04-01T13:58:03.000Z",
    "title": "GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors",
    "summary": "Despite remarkable advancements in video depth estimation, existing methods\nexhibit inherent limitations in achieving geometric fidelity through the\naffine-invariant predictions, limiting their applicability in reconstruction\nand other metrically grounded downstream tasks. We propose GeometryCrafter, a\nnovel framework that recovers high-fidelity point map sequences with temporal\ncoherence from open-world videos, enabling accurate 3D/4D reconstruction,\ncamera parameter estimation, and other depth-based applications. At the core of\nour approach lies a point map Variational Autoencoder (VAE) that learns a\nlatent space agnostic to video latent distributions for effective point map\nencoding and decoding. Leveraging the VAE, we train a video diffusion model to\nmodel the distribution of point map sequences conditioned on the input videos.\nExtensive evaluations on diverse datasets demonstrate that GeometryCrafter\nachieves state-of-the-art 3D accuracy, temporal consistency, and generalization\ncapability.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/6He3mQcB_AO1G_Sq8xYc0.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01016.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a7458afbb0117ba15c59f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
      "fullname": "Wenbo Hu",
      "name": "wbhu-tc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00906",
      "authors": [
        {
          "_id": "67ec9340913c072638c16bbf",
          "user": {
            "_id": "627837664f2afdc41bbd622a",
            "avatarUrl": "/avatars/b7bfc4fb77830bba71839c04a4aeea64.svg",
            "isPro": false,
            "fullname": "Saaket Agashe",
            "user": "saa1605",
            "type": "user"
          },
          "name": "Saaket Agashe",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:49:55.477Z",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc0",
          "name": "Kyle Wong",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc1",
          "name": "Vincent Tu",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc2",
          "user": {
            "_id": "65edff6233c279253952e0bd",
            "avatarUrl": "/avatars/3130f6f9873ae0a943631e56f6d8d341.svg",
            "isPro": false,
            "fullname": "Jiachen Yang",
            "user": "jc-y42",
            "type": "user"
          },
          "name": "Jiachen Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:46:55.173Z",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc3",
          "name": "Ang Li",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc4",
          "user": {
            "_id": "64679a226192d39142245e5e",
            "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
            "isPro": false,
            "fullname": "Xin Eric Wang",
            "user": "xw-eric",
            "type": "user"
          },
          "name": "Xin Eric Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:46:30.920Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T15:40:27.000Z",
      "submittedOnDailyAt": "2025-04-02T00:02:12.706Z",
      "title": "컴퓨터용 에이전트의 구성론적 일반자와 전문자의 프레임워크",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "컴퓨터 사용 에이전트는 컴퓨터와 모바일 장치의 그래픽 사용자 인터페이스(GUI)를 직접 인터랙티브하게 하여 사용자의 요청의 개방된 공간을 완성하여 인간 생산성을 크게 향상시킬 수 있는 것을 목표로 합니다. 그러나 현재의 에이전트는 GUI 요소의 불확실한 기반화, 장기적인 태스크 계획의 어려움, 다양한 인지 태스크를 수행하는 단일의 일반주의 모델에 의존하는 성능의 한계 등 중요한 문제를 감시하고 있습니다. 이에 우리는 다양한 일반주의 모델과 전문가 모델 사이에서 인지 책임을 배분하는 새로운 조합 프레임워크 \"Agent S2\"를 소개합니다. 우리는 정확한 GUI 로케시션을 달성하기 위해 새로운 \"Grounding Mixture\" 방법을 제안하고 변화하는 관측에 대응하여 다양한 시간 스케일에서 동적으로 행동 계획을 보정하는 \"Proactive Hierarchical Planning\"을 도입합니다. 평가 결과를 통해 Agent S2는 3가지의显著한 컴퓨터 사용 벤치마크에서 새로운 최선(SOTA) 성능을 기록했습니다. 특히, OSWorld 15 단계와 50 단계 평가에서 Claude Computer Use와 UI-TARS의 선도적인 에이전트를 초과하여 각각 18.9%와 32.7%의 상대적인 향상을 기록했습니다. 또한 Agent S2는 다른 운영 시스템과 애플리케이션에도 효과적으로 확장할 수 있으며, WindowsAgentArena에서 52.8%와 AndroidWorld에서 16.52%의 이전의 최선 방법보다 뛰어납니다. 코드는 https://github.com/simular-ai/Agent-S에서 사용 가능합니다.",
      "upvotes": 12,
      "discussionId": "67ec9343913c072638c16c5f",
      "projectPage": "https://www.simular.ai/articles/agent-s2-technical-review",
      "githubRepo": "https://github.com/simular-ai/Agent-S",
      "ai_keywords": [
        "Mixture-of-Grounding",
        "Proactive Hierarchical Planning",
        "compositional framework",
        "GUI localization",
        "action plans",
        "state-of-the-art (SOTA) performance",
        "OSWorld",
        "WindowsAgentArena",
        "AndroidWorld"
      ]
    },
    "publishedAt": "2025-04-01T11:40:27.000Z",
    "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer\n  Use Agents",
    "summary": "Computer use agents automate digital tasks by directly interacting with\ngraphical user interfaces (GUIs) on computers and mobile devices, offering\nsignificant potential to enhance human productivity by completing an open-ended\nspace of user queries. However, current agents face significant challenges:\nimprecise grounding of GUI elements, difficulties with long-horizon task\nplanning, and performance bottlenecks from relying on single generalist models\nfor diverse cognitive tasks. To this end, we introduce Agent S2, a novel\ncompositional framework that delegates cognitive responsibilities across\nvarious generalist and specialist models. We propose a novel\nMixture-of-Grounding technique to achieve precise GUI localization and\nintroduce Proactive Hierarchical Planning, dynamically refining action plans at\nmultiple temporal scales in response to evolving observations. Evaluations\ndemonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance\non three prominent computer use benchmarks. Specifically, Agent S2 achieves\n18.9% and 32.7% relative improvements over leading baseline agents such as\nClaude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation.\nMoreover, Agent S2 generalizes effectively to other operating systems and\napplications, surpassing previous best methods by 52.8% on WindowsAgentArena\nand by 16.52% on AndroidWorld relatively. Code available at\nhttps://github.com/simular-ai/Agent-S.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00906.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00810",
      "authors": [
        {
          "_id": "67eca0c4cfce948cbbbcff9a",
          "user": {
            "_id": "646f3443c261dc413383b8a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3443c261dc413383b8a4/hEJd8wLyR5HTdMzApaloN.png",
            "isPro": false,
            "fullname": "Zhaojian Yu",
            "user": "zjy2001",
            "type": "user"
          },
          "name": "Zhaojian Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:38.129Z",
          "hidden": false
        },
        {
          "_id": "67eca0c4cfce948cbbbcff9b",
          "user": {
            "_id": "650ed552dc509ae7d7bb1ccc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650ed552dc509ae7d7bb1ccc/CPR6gaIjPfnBGfr-D_rpO.jpeg",
            "isPro": false,
            "fullname": "Yinghao Wu",
            "user": "yh1567",
            "type": "user"
          },
          "name": "Yinghao Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:50:27.577Z",
          "hidden": false
        },
        {
          "_id": "67eca0c4cfce948cbbbcff9c",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:34.689Z",
          "hidden": false
        },
        {
          "_id": "67eca0c4cfce948cbbbcff9d",
          "user": {
            "_id": "5f5ba21188f57f65f951f255",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1599840760465-noauth.png",
            "isPro": false,
            "fullname": "Arman Cohan",
            "user": "armanc",
            "type": "user"
          },
          "name": "Arman Cohan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:50:33.530Z",
          "hidden": false
        },
        {
          "_id": "67eca0c4cfce948cbbbcff9e",
          "name": "Xiao-Ping Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T14:01:50.000Z",
      "submittedOnDailyAt": "2025-04-02T01:59:40.340Z",
      "title": "Z1: 효율적인 테스트 시 스케일링을 위한 코드",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "대 언어 모형(LLMs)은 테스트 시의 계산 스케일링을 통해 복잡한 문제 해결에 뛰어난 성능을 달성할 수 있지만, 이는 긴 컨텍스트와 많은 이유 토큰 비용에 따라 많은 문제가 발생합니다. 본 논문에서는 LLMs에서 코드 관련 이유 트래지젝트를 활용한 효율적인 테스트 시 스케일링 방법을 제안합니다. 이를 통해 이유 토큰을 줄일 수 있는 동시에 성능을 유지할 수 있습니다. 먼저, 간단하고 복잡한 코드 문제와 그 짧은 및 긴 해결 트래지젝트를 조합한 컬렉션 데이터 세트 Z1-Code-Reasoning-107K을 생성합니다. 다음으로, 이유 토큰의 과도한 고려 비용 감소를 위해 새로운 Shifted Thinking Window를 제안합니다. 이를 통해 컨텍스트 제한 태그(예: <think> ... </think>)을 제거하고 이유 토큰을 제한합니다. 긴 및 짧은 트래지젝트 데이터로 훈련되어 Shifted Thinking Window를 작동시킬 수 있는 모델 Z1-7B는 문제의 복잡성에 따라 이유 수준을 조정할 수 있는 능력과 이유 태스크의 적절한 스케일링을 보여주며, R1-Distill-Qwen-7B의 성능을 초월할 수 있으며, 평균 생각 토큰의 30%를 사용합니다. 특히, 코드 트래지젝트만 미세 조정된 Z1-7B는 광범위한 이유 태스크에 대한 확장성(GPQA Diamond에서 47.5%)을 나타냅니다. 효율적인 이유 발견 분석도 향후 연구에 유익한 콘텐츠를 제공합니다.",
      "upvotes": 12,
      "discussionId": "67eca0c6cfce948cbbbd0042",
      "githubRepo": "https://github.com/efficientscaling/Z1",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "test-time computing scaling",
        "thinking tokens",
        "Z1-Code-Reasoning-107K",
        "solution trajectories",
        "Shifted Thinking Window",
        "context-delimiting tags",
        "efficient test-time scaling",
        "R1-Distill-Qwen-7B",
        "GPQA Diamond",
        "efficient reasoning elicitation"
      ]
    },
    "publishedAt": "2025-04-01T10:01:50.000Z",
    "title": "Z1: Efficient Test-time Scaling with Code",
    "summary": "Large Language Models (LLMs) can achieve enhanced complex problem-solving\nthrough test-time computing scaling, yet this often entails longer contexts and\nnumerous reasoning token costs. In this paper, we propose an efficient\ntest-time scaling method that trains LLMs on code-related reasoning\ntrajectories, facilitating their reduction of excess thinking tokens while\nmaintaining performance. First, we create Z1-Code-Reasoning-107K, a curated\ndataset of simple and complex coding problems paired with their short and long\nsolution trajectories. Second, we present a novel Shifted Thinking Window to\nmitigate overthinking overhead by removing context-delimiting tags (e.g.,\n<think>. . . </think>) and capping reasoning tokens. Trained with long and\nshort trajectory data and equipped with Shifted Thinking Window, our model,\nZ1-7B, demonstrates the ability to adjust its reasoning level as the complexity\nof problems and exhibits efficient test-time scaling across different reasoning\ntasks that matches R1-Distill-Qwen-7B performance with about 30% of its average\nthinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B\ndemonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond).\nOur analysis of efficient reasoning elicitation also provides valuable insights\nfor future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00810.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6566
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24377",
      "authors": [
        {
          "_id": "67eca439a62c82ed64354e36",
          "user": {
            "_id": "67298b338c66e235932ca088",
            "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
            "isPro": false,
            "fullname": "WANG Rui",
            "user": "Ray121381",
            "type": "user"
          },
          "name": "Rui Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:52:11.104Z",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e37",
          "user": {
            "_id": "65f906e5c3dbdcae83ff7aac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f906e5c3dbdcae83ff7aac/mdjiVkLDJgJcGLwv0rMe4.jpeg",
            "isPro": false,
            "fullname": "Hongru Wang",
            "user": "Merlin-Hongru",
            "type": "user"
          },
          "name": "Hongru Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:52:28.243Z",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e38",
          "user": {
            "_id": "66ab39295558689cb8676559",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/La5iNOHELvZ2peByiakqe.png",
            "isPro": false,
            "fullname": "XUE Boyang",
            "user": "BeyondHsueh",
            "type": "user"
          },
          "name": "Boyang Xue",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:52:39.163Z",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e39",
          "user": {
            "_id": "64a3d40815655921915b8ce2",
            "avatarUrl": "/avatars/6b6b550d96be4a6473e2ccf74df438f7.svg",
            "isPro": false,
            "fullname": "Jianhuipang",
            "user": "pangjh3",
            "type": "user"
          },
          "name": "Jianhui Pang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:52:45.274Z",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3a",
          "user": {
            "_id": "654ce87af0b05673196a9f45",
            "avatarUrl": "/avatars/7b9c854eb98e487e3057479b1c7860ac.svg",
            "isPro": false,
            "fullname": "Shudong Liu",
            "user": "Sudanl",
            "type": "user"
          },
          "name": "Shudong Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:52:51.029Z",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3b",
          "user": {
            "_id": "609fee945ad152dff7bb3b77",
            "avatarUrl": "/avatars/3694f5affe50ace501df7191e1b952d4.svg",
            "isPro": false,
            "fullname": "Yichen",
            "user": "Yichen",
            "type": "user"
          },
          "name": "Yi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:52:58.486Z",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3c",
          "name": "Jiahao Qiu",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3d",
          "user": {
            "_id": "648bd523805e4bcc541ec320",
            "avatarUrl": "/avatars/443ca0c7cbeda3a08eb4af6a0e2da8bc.svg",
            "isPro": false,
            "fullname": "Derek Wong",
            "user": "derekfw",
            "type": "user"
          },
          "name": "Derek Fai Wong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:53:28.254Z",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3e",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3f",
          "name": "Kam-Fai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:58:07.000Z",
      "submittedOnDailyAt": "2025-04-02T01:17:01.812Z",
      "title": "경제를 활용한 추론: 대규모 언어 모델의 효율적인 추론 조사",
      "submittedOnDailyBy": {
        "_id": "67298b338c66e235932ca088",
        "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
        "isPro": false,
        "fullname": "WANG Rui",
        "user": "Ray121381",
        "type": "user"
      },
      "summary": "최근의 대형 언어 모델(LLMs)의 발전은 복잡한 논리적인 임무를 수행할 수를 크게 향상시키고 직감적인 사고(System 1)에서 깊은 논리적 사고(System 2)로 전환을 이루었습니다. System 2의 논리적 사고는 작업의 정확도를 향상시킬 수 있지만, 느린 사고의 특성과 유효하지 않은 논리적 행동으로 인해 계산 비용이 크게 발생합니다. 반면, System 1의 논리적 사고는 계산적으로 효율적이지만, 최적의 성능을 구현하지 않습니다. 이러한 전환에 따라, 성능(이익)과 계산 비용(비지우) 사이의 균형을 찾는 중요성을 제기하고, 논리적 사고의 경제적이라는 개념이 탄생했습니다. 이 조사에서는 LLMs의 훈련 후와 테스트 시의 추론 단계에서 논리적 사고의 경제적이, 논리적 사고의 무효의 원인, 다양한 논리적 사고 패턴의 행동 분석, 논리적 사고의 경제적이 달성하는 잠재적 해결책에 대한 상세한 분석을 제공합니다. 가능한 통찰을 제공하며, 개방적인 문제를 명확히 하고, LLMs의 논리적 사고의 경제적이 향상시키기 위한 전략에 대해 가장 많이 언급된 내용이 있습니다.",
      "upvotes": 10,
      "discussionId": "67eca43aa62c82ed64354e82",
      "githubRepo": "https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers"
    },
    "publishedAt": "2025-03-31T13:58:07.000Z",
    "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models",
    "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24377.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67298b338c66e235932ca088",
      "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
      "fullname": "WANG Rui",
      "name": "Ray121381",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.01019",
      "authors": [
        {
          "_id": "67ecd893553eb1c01a311882",
          "user": {
            "_id": "62ace9bc717ee4c12b72e275",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ace9bc717ee4c12b72e275/VfR-7R3H-5FV0OILa0XzL.jpeg",
            "isPro": false,
            "fullname": "Pablo Ruiz-Ponce",
            "user": "pabloruizponce",
            "type": "user"
          },
          "name": "Pablo Ruiz-Ponce",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:21:16.654Z",
          "hidden": false
        },
        {
          "_id": "67ecd893553eb1c01a311883",
          "user": {
            "_id": "64fac0f3b961d0d12c756b59",
            "avatarUrl": "/avatars/27cc7dbad927df818ba2f91a5b6942f9.svg",
            "isPro": false,
            "fullname": "German Barquero",
            "user": "Germs96",
            "type": "user"
          },
          "name": "German Barquero",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:53:49.237Z",
          "hidden": false
        },
        {
          "_id": "67ecd893553eb1c01a311884",
          "name": "Cristina Palmero",
          "hidden": false
        },
        {
          "_id": "67ecd893553eb1c01a311885",
          "name": "Sergio Escalera",
          "hidden": false
        },
        {
          "_id": "67ecd893553eb1c01a311886",
          "name": "José García-Rodríguez",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62ace9bc717ee4c12b72e275/zNX1xkmtw7ZpRcK_qxAHv.mp4"
      ],
      "publishedAt": "2025-04-01T17:59:44.000Z",
      "submittedOnDailyAt": "2025-04-02T05:06:30.372Z",
      "title": "MixerMDM: 인간의 움직임을 학습 가능한 Diffusion 모델",
      "submittedOnDailyBy": {
        "_id": "62ace9bc717ee4c12b72e275",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ace9bc717ee4c12b72e275/VfR-7R3H-5FV0OILa0XzL.jpeg",
        "isPro": false,
        "fullname": "Pablo Ruiz-Ponce",
        "user": "pabloruizponce",
        "type": "user"
      },
      "summary": "텍스트 설명에 따라 인간의 행동을 생성하는 것은 고품질의 행동과 그에 대응하는 조건의 쌍을 포함하는 데이터 세트의 필요성에 의해 어려움을 겪습니다. 더욱이 세밀한 제어를 목표로 하든지, 이 어려움은 증가합니다. 이에 따라 선행 연구들은 서로 다른 조건의 데이터 세트에서 사전 학습된 다양한 행동의 디피렌셜 모델을 조합하는 방법을 제안하고, 여러 조건에서 제어를 가능하게 하였습니다. 그러나 제안된 통합 전략은 사전 학습된 생성 모델의 특성이나 특이한 텍스트 설명에 의존하는 가장 적절한 조합 방법이 고려되지 않은 점을 발견합니다. 이러한 맥락에서 우리는 첫 번째 학습 가능한 모델 조합 방법인 MixerMDM을 통해 텍스트 설명에 의한 인간 행동의 디피렌셜 모델을 조합하는 방법을 도입합니다. 선행의 접근과 달리, MixerMDM은 적대적인 방법으로 학습된 행동의 디피렌셜 프로세스를 조합하는 동적 전략을 제공하며, 생성을 구동하는 조건의 세트에 따라 각 모델의 디피렌셜 프로세스를 조합하는 것을 학습합니다. MixerMDM을 사용하여 단일 및 복수인간의 행동의 디피렌셜 모델을 조합함으로써, 각각의 행동의 동적 제어를 실현하고, 전체적인 상호작용에 대한 동일한 제어를 가능하게 합니다. 또한, 이 작업에서 처음으로, 혼합 생성된 행동과 그 조건의 어레이먼트 및 MixerMDM이 행동을 혼합하는 과정에서의 적응성을 평가하는 새로운 평가 방법을 제안합니다.",
      "upvotes": 9,
      "discussionId": "67ecd894553eb1c01a3118df",
      "projectPage": "https://www.pabloruizponce.com/papers/MixerMDM",
      "githubRepo": "https://github.com/pabloruizponce/MixerMDM",
      "ai_keywords": [
        "motion diffusion models",
        "text-conditioned",
        "learnable model composition",
        "dynamic mixing strategy",
        "adversarial fashion",
        "denoising process",
        "fine-grained control",
        "single-person",
        "multi-person",
        "interaction",
        "evaluation technique",
        "alignment between generated motions and conditions"
      ]
    },
    "publishedAt": "2025-04-01T13:59:44.000Z",
    "title": "MixerMDM: Learnable Composition of Human Motion Diffusion Models",
    "summary": "Generating human motion guided by conditions such as textual descriptions is\nchallenging due to the need for datasets with pairs of high-quality motion and\ntheir corresponding conditions. The difficulty increases when aiming for finer\ncontrol in the generation. To that end, prior works have proposed to combine\nseveral motion diffusion models pre-trained on datasets with different types of\nconditions, thus allowing control with multiple conditions. However, the\nproposed merging strategies overlook that the optimal way to combine the\ngeneration processes might depend on the particularities of each pre-trained\ngenerative model and also the specific textual descriptions. In this context,\nwe introduce MixerMDM, the first learnable model composition technique for\ncombining pre-trained text-conditioned human motion diffusion models. Unlike\nprevious approaches, MixerMDM provides a dynamic mixing strategy that is\ntrained in an adversarial fashion to learn to combine the denoising process of\neach model depending on the set of conditions driving the generation. By using\nMixerMDM to combine single- and multi-person motion diffusion models, we\nachieve fine-grained control on the dynamics of every person individually, and\nalso on the overall interaction. Furthermore, we propose a new evaluation\ntechnique that, for the first time in this task, measures the interaction and\nindividual quality by computing the alignment between the mixed generated\nmotions and their conditions as well as the capabilities of MixerMDM to adapt\nthe mixing throughout the denoising process depending on the motions to mix.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62ace9bc717ee4c12b72e275/zNX1xkmtw7ZpRcK_qxAHv.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01019.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ace9bc717ee4c12b72e275",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ace9bc717ee4c12b72e275/VfR-7R3H-5FV0OILa0XzL.jpeg",
      "fullname": "Pablo Ruiz-Ponce",
      "name": "pabloruizponce",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00595",
      "authors": [
        {
          "_id": "67ecaf516560da48c5c34106",
          "user": {
            "_id": "63d34004b734eaa4d4faeccf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
            "isPro": false,
            "fullname": "Weizhi Wang",
            "user": "weizhiwang",
            "type": "user"
          },
          "name": "Weizhi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:20.594Z",
          "hidden": false
        },
        {
          "_id": "67ecaf516560da48c5c34107",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "67ecaf516560da48c5c34108",
          "user": {
            "_id": "67cd0b291580ba5d5ee65ffd",
            "avatarUrl": "/avatars/9584a55473868e5ca1fa09b1536ca546.svg",
            "isPro": false,
            "fullname": "yanglinjie",
            "user": "yanglj55",
            "type": "user"
          },
          "name": "Linjie Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:55:04.382Z",
          "hidden": false
        },
        {
          "_id": "67ecaf516560da48c5c34109",
          "name": "Heng Wang",
          "hidden": false
        },
        {
          "_id": "67ecaf516560da48c5c3410a",
          "user": {
            "_id": "65cd4785c40ab294321d610e",
            "avatarUrl": "/avatars/3f0053aa2b3d90a10b60ab24cf575fd5.svg",
            "isPro": false,
            "fullname": "Xifeng Yan",
            "user": "windwest",
            "type": "user"
          },
          "name": "Xifeng Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:54:38.935Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T09:54:00.000Z",
      "submittedOnDailyAt": "2025-04-02T02:00:49.375Z",
      "title": "Open-Qwen2VL: 학술 자원에 기반한 완전한 멀티모달 LLM의 효율적인 사전 학습",
      "submittedOnDailyBy": {
        "_id": "63d34004b734eaa4d4faeccf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
        "isPro": false,
        "fullname": "Weizhi Wang",
        "user": "weizhiwang",
        "type": "user"
      },
      "summary": "가장 선진한 다모달 LLM의 사전 학습 재현은 각 단계에서 고품질 데이터 필터링, 다모달 데이터의 혼합 전략, 시퀀스 패킹 기술, 훈련 프레임워크에 의해 생기는 벽을 가지고 있습니다. 여기서는 Open-Qwen2VL를 소개합니다. 이는 29M 이미지-텍스트 페어를 사용하여 2B 파라미터의 완전한 오픈소스 다모달 대형 언어 모델을 효율적으로 사전 학습한 것입니다. 우리의 접근 방식은 낮은 해상도에서 높은 해상도로 동작하는 동적인 이미지 해상도와 다모달 시퀀스 패킹을 사용하여 사전 학습의 효율을 크게 향상시킵니다. 훈련 데이터 세트는 MLLM 기반의 필터링 방법(예: MLM-Filter)과 전통적인 CLIP 기반의 필터링 방법을 모두 사용하며, 데이터의 품질과 훈련의 효율을 크게 향상시켰습니다. Open-Qwen2VL의 사전 학습은 UCSB에서 8xA100-40G GPU를 사용하여 5B 패킹된 다모달 토큰으로 수행되었습니다. 이는 Qwen2-VL의 1.4T 다모달 사전 학습 토큰의 0.36%입니다. 최종적으로, 지시에 기반한 훈련을 수행한 Open-Qwen2VL은 MMBench, SEEDBench, MMstar, MathVista의 다모달 벤치마크에서 부분적으로 오픈된 최신 MLLM Qwen2-VL-2B를 초과했습니다. 우리는 계산 효율적이고 데이터 효율적인 훈련 세부 사항, 데이터 필터링 방법, 시퀀스 패킹 스크립트, WebDataset 형식의 사전 학습 데이터, FSDP 기반의 훈련 코드베이스, 모델 개발에 사용된 모든 사전 학습 데이터와 지도된 보완 데이터, 베이스 모델과 지시에 기반한 훈련 모델 체크포인트를 모두 오픈소스화합니다. 우리는 다모달 LLM의 \"완전 오픈\"을 재 정의하고, 다음 3가지의 완전 릴리즈를 의미합니다: 1) 훈련 코드베이스, 2) 세부적인 데이터 필터링 기술, 3) 모델 개발에 사용된 모든 사전 학습 데이터와 지도된 보완 데이터.",
      "upvotes": 9,
      "discussionId": "67ecaf546560da48c5c341dc",
      "projectPage": "https://victorwz.github.io/Open-Qwen2VL/",
      "githubRepo": "https://github.com/Victorwz/Open-Qwen2VL",
      "ai_keywords": [
        "multimodal LLM",
        "pre-training",
        "high-quality data filtering",
        "multimodal data mixture strategies",
        "sequence packing techniques",
        "training frameworks",
        "Open-Qwen2VL",
        "fully open-source",
        "2B-parameter",
        "image-text pairs",
        "A100-40G GPU hours",
        "low-to-high dynamic image resolution",
        "multimodal sequence packing",
        "data quality",
        "academic level 8xA100-40G GPUs",
        "UCSB",
        "packed multimodal tokens",
        "Qwen2-VL",
        "instruction-tuned",
        "MMBench",
        "SEEDBench",
        "MMstar",
        "MathVista",
        "compute-efficient",
        "FSDP-based training",
        "WebDataset format",
        "pre-training data",
        "training codebase",
        "data filtering techniques",
        "sequence packing scripts",
        "pre-training data in WebDataset format",
        "FSDP-based training codebase",
        "base and instruction"
      ]
    },
    "publishedAt": "2025-04-01T05:54:00.000Z",
    "title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources",
    "summary": "The reproduction of state-of-the-art multimodal LLM pre-training faces\nbarriers at every stage of the pipeline, including high-quality data filtering,\nmultimodal data mixture strategies, sequence packing techniques, and training\nframeworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter\nMultimodal Large Language Model pre-trained efficiently on 29M image-text pairs\nusing only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic\nimage resolution and multimodal sequence packing to significantly enhance\npre-training efficiency. The training dataset was carefully curated using both\nMLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based\nfiltering methods, substantially improving data quality and training\nefficiency. The Open-Qwen2VL pre-training is conducted on academic level\n8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T\nmultimodal pre-training tokens of Qwen2-VL. The final instruction-tuned\nOpen-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on\nvarious multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista,\nindicating the remarkable training efficiency of Open-Qwen2VL. We open-source\nall aspects of our work, including compute-efficient and data-efficient\ntraining details, data filtering methods, sequence packing scripts,\npre-training data in WebDataset format, FSDP-based training codebase, and both\nbase and instruction-tuned model checkpoints. We redefine \"fully open\" for\nmultimodal LLMs as the complete release of: 1) the training codebase, 2)\ndetailed data filtering techniques, and 3) all pre-training and supervised\nfine-tuning data used to develop the model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00595.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "63d34004b734eaa4d4faeccf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
      "fullname": "Weizhi Wang",
      "name": "weizhiwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.22952",
      "authors": [
        {
          "_id": "67eb58e5969b278277adb831",
          "user": {
            "_id": "60b9e6837946aff342f734ae",
            "avatarUrl": "/avatars/a711a6aa35757dfd7b78b26098a964fc.svg",
            "isPro": false,
            "fullname": "Yuxuan Wang",
            "user": "ColorfulAI",
            "type": "user"
          },
          "name": "Yuxuan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T09:42:25.151Z",
          "hidden": false
        },
        {
          "_id": "67eb58e5969b278277adb832",
          "name": "Yueqian Wang",
          "hidden": false
        },
        {
          "_id": "67eb58e5969b278277adb833",
          "name": "Bo Chen",
          "hidden": false
        },
        {
          "_id": "67eb58e5969b278277adb834",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "67eb58e5969b278277adb835",
          "name": "Dongyan Zhao",
          "hidden": false
        },
        {
          "_id": "67eb58e5969b278277adb836",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T03:09:26.871Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-29T02:46:58.000Z",
      "submittedOnDailyAt": "2025-04-02T07:32:05.470Z",
      "title": "OmniMMI: 흐름있는 영상 컨텍스트에서 상세한 다모드 인터랙션 벤치마크",
      "submittedOnDailyBy": {
        "_id": "60b9e6837946aff342f734ae",
        "avatarUrl": "/avatars/a711a6aa35757dfd7b78b26098a964fc.svg",
        "isPro": false,
        "fullname": "Yuxuan Wang",
        "user": "ColorfulAI",
        "type": "user"
      },
      "summary": "다모뢰언어 모델(MLLMs)의 급격한 발전이 촉진한 Omni언어 모델의 개발은 다양한 모드의 데이터를 처리하고 능동적으로 응답하는 것을 목표로 하고 있습니다. 그 가능성은 있지만, 실세계의 상호작용 능력을 평가하기 위한 흐름 이미지 컨텍스트에서 큰 문제로 여겨지는 것입니다. 본 논문에서는 OmniLLMs의 흐름 이미지 컨텍스트에 적합한 세부적인 다양한 상호작용 벤치마크인 OmniMMI를 소개합니다. OmniMMI는 1,121개 이상의 이미지와 2,290개 이상의 질문를 포함하며, 흐름 이미지 이해와 능동적인 이유론을 포함한 두 가지 중요한 조사 부족 문제를 해결하는 6가지 다른 서브 태스크를 제시합니다. 또한, 새로운 프레임워크인 다모달 다중플렉싱 모델링(M4)을 제안하고, 추론 효율적인 흐름 모델을 가능하게 하며, 시청하면서 생성할 수 있는 것을 설계합니다.",
      "upvotes": 9,
      "discussionId": "67eb58e6969b278277adb887",
      "ai_keywords": [
        "multi-modal language models",
        "GPT-4o",
        "Omni language models",
        "streaming video contexts",
        "OmniMMI",
        "multi-modal interaction benchmark",
        "streaming video understanding",
        "proactive reasoning",
        "Multi-modal Multiplexing Modeling",
        "inference-efficient streaming model"
      ]
    },
    "publishedAt": "2025-03-28T22:46:58.000Z",
    "title": "OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming\n  Video Contexts",
    "summary": "The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has\npropelled the development of Omni language models, designed to process and\nproactively respond to continuous streams of multi-modal data. Despite their\npotential, evaluating their real-world interactive capabilities in streaming\nvideo contexts remains a formidable challenge. In this work, we introduce\nOmniMMI, a comprehensive multi-modal interaction benchmark tailored for\nOmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and\n2,290 questions, addressing two critical yet underexplored challenges in\nexisting video benchmarks: streaming video understanding and proactive\nreasoning, across six distinct subtasks. Moreover, we propose a novel\nframework, Multi-modal Multiplexing Modeling (M4), designed to enable an\ninference-efficient streaming model that can see, listen while generating.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60b9e6837946aff342f734ae",
      "avatarUrl": "/avatars/a711a6aa35757dfd7b78b26098a964fc.svg",
      "fullname": "Yuxuan Wang",
      "name": "ColorfulAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00509",
      "authors": [
        {
          "_id": "67ecae49a34baf018ca3c4cd",
          "user": {
            "_id": "65de7628deee79773f0f46f6",
            "avatarUrl": "/avatars/6c509dbe96e47b47271eb74178c1c9ba.svg",
            "isPro": false,
            "fullname": "Kai Yan",
            "user": "kaiyan289",
            "type": "user"
          },
          "name": "Kai Yan",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-02T03:26:02.443Z",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4ce",
          "user": {
            "_id": "642957104e073875f6a5ddd0",
            "avatarUrl": "/avatars/32b706d35c5ff52932c5029b94caa7b9.svg",
            "isPro": false,
            "fullname": "Yufei Xu",
            "user": "yfxu",
            "type": "user"
          },
          "name": "Yufei Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:23.307Z",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4cf",
          "name": "Zhengyin Du",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4d0",
          "name": "Xuesong Yao",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4d1",
          "name": "Zheyu Wang",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4d2",
          "name": "Xiaowen Guo",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4d3",
          "name": "Jiecao Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T07:57:58.000Z",
      "submittedOnDailyAt": "2025-04-02T01:56:35.333Z",
      "title": "레시피오더로지온: 어떤 방식으로 선진적인 언어 모델이 초등학교 수준의 논리 문제를 해결하지 못하나요?",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "최근 몇 년간 LLM 벤치마크의 난이도는 유치원 수준부터 첨단 문제를 급격히 업그레이드하고 있으며, 연구자들은 우리들이 인간 지능을 약간 초과하여 만들었다는 놀라운 경이로움을 만들어냈다. 그러나 LLM의 놀라운 논리 능력은 인간 표준의 진정한 지능인지, 혹은 텍스트 데이터 학습 중 본래 해결책을 재현하는지 여부에 달려있다. 이러한 문제를 연구하기 위해, 우리는 RoR-Bench라는 새로운, 다형성 벤치마크를 제안하고, 이 벤치마크에서 실험적인 분석을 수행합니다. 그 결과, 모든 첨단 LLM은 매우 엄격한 재현적인 행동을 보여주고, 조건이 한 단어만 변경되면, OpenAI-o1이나 DeepSeek-R1과 같은 최상위 모델은 유치원 수준의 산술 및 논리 문제를 60%의 성능 손실로 보입니다. 이러한 발견은 LLM 커뮤니티에 경고를 내며, 첨단 LLM의 진정한 지능 수준을 재평가하는 필요성을 촉구합니다.",
      "upvotes": 7,
      "discussionId": "67ecae4aa34baf018ca3c506",
      "projectPage": "https://team.doubao.com/zh/publication/recitation-over-reasoning-how-cutting-edge-language-models-can-fail-on-elementary-school-level-reasoning-problems?view_from=homepage_recommend",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "RoR-Bench",
        "multi-modal benchmark",
        "recitation behavior",
        "elementary school-level arithmetic",
        "reasoning problems",
        "OpenAI-o1",
        "DeepSeek-R1"
      ]
    },
    "publishedAt": "2025-04-01T03:57:58.000Z",
    "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?",
    "summary": "The rapid escalation from elementary school-level to frontier problems of the\ndifficulty for LLM benchmarks in recent years have weaved a miracle for\nresearchers that we are only inches away from surpassing human intelligence.\nHowever, is the LLMs' remarkable reasoning ability indeed comes from true\nintelligence by human standards, or are they simply reciting solutions\nwitnessed during training at an Internet level? To study this problem, we\npropose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's\nrecitation behavior when asked simple reasoning problems but with conditions\nsubtly shifted, and conduct empirical analysis on our benchmark. Surprisingly,\nwe found existing cutting-edge LLMs unanimously exhibits extremely severe\nrecitation behavior; by changing one phrase in the condition, top models such\nas OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary\nschool-level arithmetic and reasoning problems. Such findings are a wake-up\ncall to the LLM community that compels us to re-evaluate the true intelligence\nlevel of cutting-edge LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00509.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6566
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01005",
      "authors": [
        {
          "_id": "67ec95ea3d267d26663ea34b",
          "name": "Nishad Singhi",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea34c",
          "name": "Hritik Bansal",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea34d",
          "name": "Arian Hosseini",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea34e",
          "name": "Aditya Grover",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea34f",
          "name": "Kai-Wei Chang",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea350",
          "name": "Marcus Rohrbach",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea351",
          "name": "Anna Rohrbach",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T17:41:57.000Z",
      "submittedOnDailyAt": "2025-04-02T00:12:49.083Z",
      "title": "어떤 상황에서 해결하고 어떤 상황에서 확인하는가: 최적의 계산 문제 해결과 생성적인 확인에 대해",
      "submittedOnDailyBy": {
        "_id": "61c5c25705aa54027c52f7b3",
        "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
        "isPro": false,
        "fullname": "Hritik Bansal",
        "user": "hbXNov",
        "type": "user"
      },
      "summary": "スケーリングテストタイムコンピュート는 대규모 언어 모델(LLMs)의 설명 능력을 향상시키기 위한 중요한 전략으로 등장하고 있습니다, 특히 수학 문제 해결과 같은 복잡한 태스크에서 특히如此. 기존의 접근 방식에는 Self-Consistency(SC)가 문제를 위해 여러 개의 해를 생성하고 다수결로 가장 좋은 답변을 선택하여 최적의 해를 선택할 수 있었습니다. 일반적인 방법 중 하나는 각 해를 보상 모델(배리데이타)로 점수를 설정하여 가장 좋은 해를 선택하는 것입니다. 최근 Generative Reward Models(GenRM)의 발전은 배리데이션을 다음 토큰 예측 태스크로 재구성하고 추론 시 컴퓨팅의 스케일링을 새로운 축에 따라 진행할 수 있게 되었습니다. 특히 GenRM은 각 해를 평가하기 위해 여러 가지 배리데이션의 쉼 스토크를 생성합니다. 추론 버킷이 제한된 상황에서는 기본적인 트레이드오프를 발생시키게 됩니다: 해의 스케일링에 버킷을 사용하거나 해의 생성을 줄이고 배리데이션에 컴퓨팅을 할당하는 선택을 요구합니다. 이에 반해 SC와 GenRM을 고정한 추론 버킷 아래로 비교했습니다. 흥미로운 사실은 SC는 다양한 모델과 데이터셋에서 거의 모든 경우의 실용적인 추론 버킷에서 GenRM보다 컴퓨팅 효율적이었습니다. 예를 들어 GenRM은 8배의 추론 컴퓨팅을 사용하여 SC와 같은 성능을 달성하지만 더 많은 컴퓨팅을 필요로 합니다. 또한 GenRM 패러다임의 추론 스케일링의 법칙을 발견하고, 컴퓨팅 최적화의 추론은 해의 스케일링보다 배리데이션의 수의 스케일링을 더 급격히 수행하는 것이 유리하다는 것을 명확히 만들었습니다. 우리의 연구는 해의 생성과 배리데이션의 균형을 조정하여 테스트 시간 스케일링을 최적화하는 실용적인 가이드라인을 제공합니다. 코드는 https://github.com/nishadsinghi/sc-genrm-scaling에 공개되어 있습니다.",
      "upvotes": 6,
      "discussionId": "67ec95eb3d267d26663ea38b",
      "ai_keywords": [
        "Large language models (LLMs)",
        "Mathematical problem-solving",
        "Self-Consistency (SC)",
        "Reward model (verifier)",
        "Generative Reward Models (GenRM)",
        "Next-token prediction task",
        "Chains-of-thought",
        "Inference budget",
        "Compute-efficient",
        "Inference scaling laws",
        "Compute-optimal inference"
      ]
    },
    "publishedAt": "2025-04-01T13:41:57.000Z",
    "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and\n  Generative Verification for LLM Reasoning",
    "summary": "Scaling test-time compute has emerged as a key strategy for enhancing the\nreasoning capabilities of large language models (LLMs), particularly in tasks\nlike mathematical problem-solving. A traditional approach, Self-Consistency\n(SC), generates multiple solutions to a problem and selects the most common\nanswer via majority voting. Another common method involves scoring each\nsolution with a reward model (verifier) and choosing the best one. Recent\nadvancements in Generative Reward Models (GenRM) reframe verification as a\nnext-token prediction task, enabling inference-time scaling along a new axis.\nSpecifically, GenRM generates multiple verification chains-of-thought to score\neach solution. Under a limited inference budget, this introduces a fundamental\ntrade-off: should you spend the budget on scaling solutions via SC or generate\nfewer solutions and allocate compute to verification via GenRM? To address\nthis, we evaluate GenRM against SC under a fixed inference budget.\nInterestingly, we find that SC is more compute-efficient than GenRM for most\npractical inference budgets across diverse models and datasets. For instance,\nGenRM first matches SC after consuming up to 8x the inference compute and\nrequires significantly more compute to outperform it. Furthermore, we derive\ninference scaling laws for the GenRM paradigm, revealing that compute-optimal\ninference favors scaling solution generation more aggressively than scaling the\nnumber of verifications. Our work provides practical guidance on optimizing\ntest-time scaling by balancing solution generation and verification. The code\nis available at https://github.com/nishadsinghi/sc-genrm-scaling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01005.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "61c5c25705aa54027c52f7b3",
      "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
      "fullname": "Hritik Bansal",
      "name": "hbXNov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00557",
      "authors": [
        {
          "_id": "67eca9a501a4d1c29e4e70f1",
          "name": "Jewon Lee",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f2",
          "name": "Ki-Ung Song",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f3",
          "name": "Seungmin Yang",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f4",
          "name": "Donguk Lim",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f5",
          "name": "Jaeyeon Kim",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f6",
          "name": "Wooksu Shin",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f7",
          "name": "Bo-Kyeong Kim",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f8",
          "name": "Yong Jae Lee",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f9",
          "name": "Tae-Ho Kim",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/61f44bab7eba274ea80b74ce/Tk5AAk1Qg7HtPWEYdavBk.png"
      ],
      "publishedAt": "2025-04-01T09:10:32.000Z",
      "submittedOnDailyAt": "2025-04-02T01:41:42.016Z",
      "title": "Efficient LLaMA-3.2-Vision를 실현하기 위해, Cross-Attended Visual Feature를 줄이는 방법",
      "submittedOnDailyBy": {
        "_id": "61f44bab7eba274ea80b74ce",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61f44bab7eba274ea80b74ce/BRbKX1jephdZ7D44FATl4.jpeg",
        "isPro": false,
        "fullname": "Hyoung-Kyu Song",
        "user": "deepkyu",
        "type": "user"
      },
      "summary": "그림특징량의 확장으로 추론비용을 줄이는 데에 활용되는 시각토큰 축소는 대규모 시각 언어 모델(LVLMs)의 추론 비용을 낮추는데 도움을 줌. 이 연구는 자동주의에만 초점을 맞추는 LVLMs의 관련 연구와 달리, 우리 연구는 특히 크로스주의 기반 모델을 대상으로 우수한 성능을 구현하는 데에 집중하고 있습니다. 우리 연구팀은 크로스주의 계층의 그림 토큰의 키 밸류(KV) 캐시 크기가 텍스트 토큰보다 크게, 계산의 한계로 작용하는 중요한 문제로 밝혀졌습니다. 이 문제를 완화하기 위해, 우리는 크로스주의 매핑의 희소성을 활용하여 과도한 시각특징량을 선택적으로 축소하는 방법을 사용했습니다. 우리 연구팀의 \"トリミングラマ\"는 추가적인 훈련이 필요하지 않도록, 캐시의 요구를 효과적으로 줄이는 데에 사용됩니다. 50%의 시각특징량을 축소함으로써, 우리의 모델은 추론 라틴 시와 메모리 사용량을 줄이면서 동일한 성능을 달성할 수 있습니다.",
      "upvotes": 5,
      "discussionId": "67eca9a601a4d1c29e4e7131",
      "ai_keywords": [
        "visual token reduction",
        "inference costs",
        "image features",
        "large vision-language models (LVLMs)",
        "self-attention-only LVLMs",
        "cross-attention-based models",
        "key-value (KV) cache size",
        "self-attention layers",
        "cross-attention layers",
        "sparse nature",
        "cross-attention maps",
        "redundant visual features",
        "Trimmed Llama",
        "KV cache demands",
        "inference latency",
        "memory usage",
        "benchmark parity"
      ]
    },
    "publishedAt": "2025-04-01T05:10:32.000Z",
    "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
    "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/61f44bab7eba274ea80b74ce/Tk5AAk1Qg7HtPWEYdavBk.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00557.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61f44bab7eba274ea80b74ce",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61f44bab7eba274ea80b74ce/BRbKX1jephdZ7D44FATl4.jpeg",
      "fullname": "Hyoung-Kyu Song",
      "name": "deepkyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00294",
      "authors": [
        {
          "_id": "67ecadb199892db0bda56561",
          "name": "Vidhisha Balachandran",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56562",
          "name": "Jingya Chen",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56563",
          "name": "Lingjiao Chen",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56564",
          "name": "Shivam Garg",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56565",
          "name": "Neel Joshi",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56566",
          "name": "Yash Lara",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56567",
          "name": "John Langford",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56568",
          "name": "Besmira Nushi",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56569",
          "name": "Vibhav Vineet",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda5656a",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda5656b",
          "name": "Safoora Yousefi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T23:40:28.000Z",
      "submittedOnDailyAt": "2025-04-02T01:53:41.171Z",
      "title": "推論 시의 스케일링에 있어 복잡한 작업: 현재 상태와 다음 단계",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "推理 시의 스케일링은, 단계별로 문제를 해결하는 데 기반한 복잡한 문제를 해결할 때, 대규모 언어 모델(LLMs)의 추론 능력이 향상될 수 있습니다. 긴 기간 내의 생성 탬플은 수학 태스크에 효과적이라는 것이 증명되어 있지만, 이 접근 방식의 광범위한 영향을 다른 태스크에 대해 명확하게 밝혀져 있지 않습니다. 본 연구에서는, 9개의 가장 先端 모델과 8개의 어려운 태스크(수학, STEM의 논리, 일요일 계획, NP-hard 문제, 노선, 공간의 논리)에 대해, 스케일링 방법의 이점과 한계를 조사합니다. 전통적인 모델(예: GPT-4o)과 추론 시의 스케일링에 대해 조정된 모델(예: o1)을 비교하기 위해, 단순한 모델 호출의 반복이나 순차적인 평가 프로토콜을 포함하는 평가 프로토콜을 사용합니다. 이러한 평가는 각 모델의 성능의 하한과 상한을 근사하며, 향후 성능 향상의 가능성을 보여주고 있습니다. 이러한 평가는 강화 학습과 다 모델 추론 시스템을 통해 실현 가능한 성능 향상의 가능성을 보여줍니다. 엄밀한 실험적 분석에 의해, 추론 시의 스케일링의 이점은 태스크에 따라 달라지고, 문제의 복잡도가 증가함에 따라 감소합니다. 또한, 모델이 생성하는 토큰을 단순히 늘려도, 이러한 어려운 영역에서 정확도의 확실한 향상은 얻지 못하는 것을 알 수 있습니다. 전통적인 모델을 사용한 여러 독립적인 실험의 결과를 통해, 포트포어 밸리데이션을 사용했을 때, 특정 태스크에서 오늘 가장 先端의 논리 모델의 평균 성능에 가까운 성능을 달성할 수 있습니다. 그러나 다른 태스크에서, 매우 높은 스케일링 레지멍에도 불구하고, 뚜렷한 성능 간격을 유지합니다. 흥미로운 점은, 모든 모델은 포트포어 밸리데이션을 사용하거나 강력한 피드백을 받을 때, 뚜렷한 효과를 보여주고, 향후 개선의 충분한 가능성이 있음을 시사합니다.",
      "upvotes": 4,
      "discussionId": "67ecadb299892db0bda565aa",
      "ai_keywords": [
        "inference-time scaling",
        "reasoning capabilities",
        "large language models (LLMs)",
        "step-by-step problem solving",
        "generated scratchpads",
        "state-of-the-art models",
        "math and STEM reasoning",
        "calendar planning",
        "NP-hard problems",
        "navigation",
        "spatial reasoning",
        "conventional models",
        "fine-tuned models",
        "model calls",
        "performance bounds",
        "multi-model inference systems",
        "empirical analysis",
        "token usage",
        "accuracy",
        "perfect verifiers",
        "performance gap"
      ]
    },
    "publishedAt": "2025-03-31T19:40:28.000Z",
    "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies\n  Ahead",
    "summary": "Inference-time scaling can enhance the reasoning capabilities of large\nlanguage models (LLMs) on complex problems that benefit from step-by-step\nproblem solving. Although lengthening generated scratchpads has proven\neffective for mathematical tasks, the broader impact of this approach on other\ntasks remains less clear. In this work, we investigate the benefits and\nlimitations of scaling methods across nine state-of-the-art models and eight\nchallenging tasks, including math and STEM reasoning, calendar planning,\nNP-hard problems, navigation, and spatial reasoning. We compare conventional\nmodels (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g.,\no1) through evaluation protocols that involve repeated model calls, either\nindependently or sequentially with feedback. These evaluations approximate\nlower and upper performance bounds and potential for future performance\nimprovements for each model, whether through enhanced training or multi-model\ninference systems. Our extensive empirical analysis reveals that the advantages\nof inference-time scaling vary across tasks and diminish as problem complexity\nincreases. In addition, simply using more tokens does not necessarily translate\nto higher accuracy in these challenging regimes. Results from multiple\nindependent runs with conventional models using perfect verifiers show that,\nfor some tasks, these models can achieve performance close to the average\nperformance of today's most advanced reasoning models. However, for other\ntasks, a significant performance gap remains, even in very high scaling\nregimes. Encouragingly, all models demonstrate significant gains when inference\nis further scaled with perfect verifiers or strong feedback, suggesting ample\npotential for future improvements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00294.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6566
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23361",
      "authors": [
        {
          "_id": "67eb57a56522661171fb4725",
          "user": {
            "_id": "64d660308ebc40443813f014",
            "avatarUrl": "/avatars/516bb2d2383be99794e366dfb41636b6.svg",
            "isPro": false,
            "fullname": "Linxin Song",
            "user": "linxinso",
            "type": "user"
          },
          "name": "Linxin Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:23:45.180Z",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb4726",
          "name": "Xuwei Ding",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb4727",
          "name": "Jieyu Zhang",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb4728",
          "user": {
            "_id": "62e1b3cb3eb0730f621a83f6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg",
            "isPro": false,
            "fullname": "Taiwei Shi",
            "user": "MaksimSTW",
            "type": "user"
          },
          "name": "Taiwei Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:48.349Z",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb4729",
          "name": "Ryotaro Shimizu",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb472a",
          "name": "Rahul Gupta",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb472b",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb472c",
          "name": "Jian Kang",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb472d",
          "name": "Jieyu Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T08:33:56.000Z",
      "submittedOnDailyAt": "2025-04-02T00:04:15.369Z",
      "title": "마스터캡의 지식 부족을 언어 모델로 발견합니다.",
      "submittedOnDailyBy": {
        "_id": "62e1b3cb3eb0730f621a83f6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg",
        "isPro": false,
        "fullname": "Taiwei Shi",
        "user": "MaksimSTW",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 자랑스러운 언어 능력을 가지고 있지만, 사실적인 지식의 정확한 유지가 어려워, 헛소문과 불신의 출력을 발생시킵니다. LLMs의 지식의 결함이 이해하기 위해, 모든 크기의 지식베이스에 대해 엄격한 평가를 수행하는 것은, 특히 클로즈드웨이트 모델에 대해 계산적으로 어려워서, 특히 어려워요. 우리는 클로즈드웨이트 LLMs의 지식의 결함이(오류)를 발견하기 위한 스케일러블하고 효율적인 프레임워크인 'stochastic error ascent (SEA)'를 제안합니다. SEA는 모든 지식 후보를 무용하게 조사하는 것보다, 로직적으로 유사성을 사용하여 이전에 본 실패로 인해 새로운 고오류 후보를 체계적으로 구축합니다. SEA는 문서와 문장 수준의 휴리스틱 검색을 수행하고, 오류의 전파를 모델화하고, 시스템적인 실패 모드를 특정하기 위해 관련적인 방향 그래프를 구축합니다. 실험적으로는, SEA는 Automated Capability Discovery보다 40.7배, AutoBencher보다 26.7% 더 많은 지식 오류를 발견하고, 오류 별 비용은 599배와 9배 감소합니다. 인간 평가는 생성된 질 높은 질문의 질을 확인하고, ablation과 convergence 분석은 SEA의 각 구성 요소의 기여를 증명합니다. 발견된 오류의 진화 분석은 LLM 가족 간 관련 파일과 재현 가능한 결함이 보여서, 미래의 LLM 개발에서 더 좋은 데이터 커버리지와 목표지향적인 微調校的 필요성을 강조합니다.",
      "upvotes": 4,
      "discussionId": "67eb57a66522661171fb476a",
      "githubRepo": "https://github.com/uscnlp-lime/SEA",
      "ai_keywords": [
        "stochastic error ascent (SEA)",
        "knowledge deficiencies (errors)",
        "closed-weight LLMs",
        "stochastic optimization process",
        "semantic similarity",
        "hierarchical retrieval",
        "document level",
        "paragraph level",
        "relation directed acyclic graph (DAG)",
        "error propagation",
        "systematic failure modes",
        "Automated Capability Discovery",
        "AutoBencher",
        "cost-per-error",
        "human evaluation",
        "ablation analysis",
        "convergence analysis",
        "correlated failure patterns",
        "LLM families",
        "data coverage",
        "targeted fine-tuning"
      ]
    },
    "publishedAt": "2025-03-30T04:33:56.000Z",
    "title": "Discovering Knowledge Deficiencies of Language Models on Massive\n  Knowledge Base",
    "summary": "Large language models (LLMs) possess impressive linguistic capabilities but\noften fail to faithfully retain factual knowledge, leading to hallucinations\nand unreliable outputs. Understanding LLMs' knowledge deficiencies by\nexhaustively evaluating against full-scale knowledge bases is computationally\nprohibitive, especially for closed-weight models. We propose stochastic error\nascent (SEA), a scalable and efficient framework for discovering knowledge\ndeficiencies (errors) in closed-weight LLMs under a strict query budget. Rather\nthan naively probing all knowledge candidates, SEA formulates error discovery\nas a stochastic optimization process: it iteratively retrieves new high-error\ncandidates by leveraging the semantic similarity to previously observed\nfailures. To further enhance search efficiency and coverage, SEA employs\nhierarchical retrieval across document and paragraph levels, and constructs a\nrelation directed acyclic graph to model error propagation and identify\nsystematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors\nthan Automated Capability Discovery and 26.7% more than AutoBencher, while\nreducing the cost-per-error by 599x and 9x, respectively. Human evaluation\nconfirms the high quality of generated questions, while ablation and\nconvergence analyses validate the contribution of each component in SEA.\nFurther analysis on the discovered errors reveals correlated failure patterns\nacross LLM families and recurring deficits, highlighting the need for better\ndata coverage and targeted fine-tuning in future LLM development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23361.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e1b3cb3eb0730f621a83f6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg",
      "fullname": "Taiwei Shi",
      "name": "MaksimSTW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.01017",
      "authors": [
        {
          "_id": "67ecebc51f669fb5591616cd",
          "name": "David Fan",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616ce",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616cf",
          "name": "Jiachen Zhu",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d0",
          "name": "Koustuv Sinha",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d1",
          "name": "Zhuang Liu",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d2",
          "name": "Xinlei Chen",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d3",
          "name": "Michael Rabbat",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d4",
          "name": "Nicolas Ballas",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d5",
          "name": "Yann LeCun",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d6",
          "name": "Amir Bar",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d7",
          "name": "Saining Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T17:59:15.000Z",
      "submittedOnDailyAt": "2025-04-02T06:19:35.566Z",
      "title": "스케일링 프레임워크 랜그젼 언어 없는 시각적 표현 학습",
      "submittedOnDailyBy": {
        "_id": "6374cbb7255276f3a22b4b35",
        "avatarUrl": "/avatars/7cf1bbb83447441e5fa2e1e4fcf7617b.svg",
        "isPro": true,
        "fullname": "Peter Tong",
        "user": "tsbpp",
        "type": "user"
      },
      "summary": "Visual Self-Supervised Learning (SSL)은 Visual Question Answering (VQA) 등 다양한 모델 설정에서 CLIP보다 낮은 성능을 보인 상태에 있다. 이러한 모델 간 차이는 주로 언어의 서브어에 의해 추출된 의미를 나타내는 경우가 많지만, Visual SSL 모델과 CLIP 모델은 서로 다른 데이터로 훈련되어 있다는 점을 고려하고 있다. 본 논문에서는 \"Visual SSL이 CLIP보다 낮은 성능을 보인 것은 언어의 서브어의 부족성 또는 훈련 데이터의 차이 때문인지\"라는 문제를 조사한다. Visual SSL 모델과 CLIP 모델을 동일한 MetaCLIP 데이터로 훈련시키고 VQA를 다양한 테스트 데이터로 활용하여 이 문제를 조사한다. 이러한 제어된 설정에서, Visual SSL 모델은 데이터와 모델의 용량에 따라 CLIP 모델보다 더 잘 확장되며 7B 파라미터까지 확장해도 성능이 감소하지 않는다. 따라서, Visual SSL 모델은 VQA와 전통적인 시각 벤치마크에서 CLIP 수준의 성능을 달성하는 것을 관찰한다. 이러한 발견은 언어 서브어에 의한 시각 사전 학습과 비교하여, 완전한 Visual SSL도 확장에도 비슷한 성능을 달성할 수 있음을 보여준다, 시각 코너 케이스의 표현 학습의 새로운 기회를 개척한다.",
      "upvotes": 3,
      "discussionId": "67ecebc61f669fb559161742",
      "ai_keywords": [
        "Visual Self-Supervised Learning (SSL)",
        "Contrastive Language-Image Pretraining (CLIP)",
        "Visual Question Answering (VQA)",
        "MetaCLIP data",
        "vision encoders",
        "data capacity",
        "CLIP-level performance",
        "vision-centric representation learning"
      ]
    },
    "publishedAt": "2025-04-01T13:59:15.000Z",
    "title": "Scaling Language-Free Visual Representation Learning",
    "summary": "Visual Self-Supervised Learning (SSL) currently underperforms Contrastive\nLanguage-Image Pretraining (CLIP) in multimodal settings such as Visual\nQuestion Answering (VQA). This multimodal gap is often attributed to the\nsemantics introduced by language supervision, even though visual SSL and CLIP\nmodels are often trained on different data. In this work, we ask the question:\n\"Do visual self-supervised approaches lag behind CLIP due to the lack of\nlanguage supervision, or differences in the training data?\" We study this\nquestion by training both visual SSL and CLIP models on the same MetaCLIP data,\nand leveraging VQA as a diverse testbed for vision encoders. In this controlled\nsetup, visual SSL models scale better than CLIP models in terms of data and\nmodel capacity, and visual SSL performance does not saturate even after scaling\nup to 7B parameters. Consequently, we observe visual SSL methods achieve\nCLIP-level performance on a wide range of VQA and classic vision benchmarks.\nThese findings demonstrate that pure visual SSL can match language-supervised\nvisual pretraining at scale, opening new opportunities for vision-centric\nrepresentation learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01017.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6374cbb7255276f3a22b4b35",
      "avatarUrl": "/avatars/7cf1bbb83447441e5fa2e1e4fcf7617b.svg",
      "fullname": "Peter Tong",
      "name": "tsbpp",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00927",
      "authors": [
        {
          "_id": "67ecc23b6a2bc6abdc2e9183",
          "name": "Olga Golovneva",
          "hidden": false
        },
        {
          "_id": "67ecc23b6a2bc6abdc2e9184",
          "name": "Tianlu Wang",
          "hidden": false
        },
        {
          "_id": "67ecc23b6a2bc6abdc2e9185",
          "name": "Jason Weston",
          "hidden": false
        },
        {
          "_id": "67ecc23b6a2bc6abdc2e9186",
          "name": "Sainbayar Sukhbaatar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T15:59:32.000Z",
      "submittedOnDailyAt": "2025-04-02T03:21:25.809Z",
      "title": "**Multi-Token Attention**\n\n이것은 기계 학습 및 자연어 처리 분야에서 중요한 개념으로, Multi-Token Attention(다 토큰 주목)입니다. 이 접근 방식은 일반적인 단일 토큰 주목(Single-Token Attention)보다 문맥을 더 복잡하게 이해할 수 있습니다. 특히, 긴 문장나 복잡한 문맥을 처리할 때 이 접근 방식이 유용합니다.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Soft attention는 LLMs가 제공된 кон텍스트 내의 관련 부분을 특정하기 위한 중요한 기능입니다. 그러나 개별적인 attention weights는 그 하나의 query와 key token vector의 유사성에 의해 결정됩니다. 이 \"단일 토큰 attention\"은 전체 컨텍스트에서 관련 부분을 구분하기 위해 사용할 수 있는 정보량을 제한하고 있습니다. 이러한 문제를 대처하기 위해, 우리는 LLMs가 동시에 여러 query와 key vector에 의해 attention weights를 조건화하는 새로운 attention 방법인 Multi-Token Attention (MTA)를 제안합니다. 이 방법은 queries, keys와 heads에 컨버지션 연산을 적용하여 가까운 query와 key가 각각의 attention weights를 영향을 미칠 수 있도록 구현됩니다. 이렇게, 우리의 방법은 한 vector의 용량을 초과하는 풍부하고 더 복잡한 정보를 사용하여 관련 컨텍스트를 특정할 수 있습니다. 상세한 평가를 통해, MTA가 다양한 인기 있는 벤치마크에서 성능을 향상시키는 것을 보여주었습니다. 특히, 표준의 언어 모델링 태스크에서 Transformer의 기준 모델보다 우수하며, 긴 컨텍스트 내의 정보 검색을 필요로 하는 태스크에서도, 우리의 방법의 풍부한 정보를 활용하는 것이 특히 유리합니다.",
      "upvotes": 3,
      "discussionId": "67ecc23c6a2bc6abdc2e91c2",
      "ai_keywords": [
        "Soft attention",
        "LLMs (Large Language Models)",
        "single token attention",
        "Multi-Token Attention (MTA)",
        "convolution operations",
        "queries",
        "keys",
        "heads",
        "attention weights"
      ]
    },
    "publishedAt": "2025-04-01T11:59:32.000Z",
    "title": "Multi-Token Attention",
    "summary": "Soft attention is a critical mechanism powering LLMs to locate relevant parts\nwithin a given context. However, individual attention weights are determined by\nthe similarity of only a single query and key token vector. This \"single token\nattention\" bottlenecks the amount of information used in distinguishing a\nrelevant part from the rest of the context. To address this issue, we propose a\nnew attention method, Multi-Token Attention (MTA), which allows LLMs to\ncondition their attention weights on multiple query and key vectors\nsimultaneously. This is achieved by applying convolution operations over\nqueries, keys and heads, allowing nearby queries and keys to affect each\nother's attention weights for more precise attention. As a result, our method\ncan locate relevant context using richer, more nuanced information that can\nexceed a single vector's capacity. Through extensive evaluations, we\ndemonstrate that MTA achieves enhanced performance on a range of popular\nbenchmarks. Notably, it outperforms Transformer baseline models on standard\nlanguage modeling tasks, and on tasks that require searching for information\nwithin long contexts, where our method's ability to leverage richer information\nproves particularly beneficial.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00927.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6566
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00869",
      "authors": [
        {
          "_id": "67ecb60276900f68cd1df503",
          "name": "Xiaoke Huang",
          "hidden": false
        },
        {
          "_id": "67ecb60276900f68cd1df504",
          "name": "Juncheng Wu",
          "hidden": false
        },
        {
          "_id": "67ecb60276900f68cd1df505",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "67ecb60276900f68cd1df506",
          "name": "Xianfeng Tang",
          "hidden": false
        },
        {
          "_id": "67ecb60276900f68cd1df507",
          "name": "Yuyin Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T14:57:43.000Z",
      "submittedOnDailyAt": "2025-04-02T02:30:28.553Z",
      "title": "의료 진단의 테스트 시 스케일링의 잠재력을 대규모 언어 모델에 의해서 해방하는 것.",
      "submittedOnDailyBy": {
        "_id": "63318b2349a9563915469f3b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg",
        "isPro": true,
        "fullname": "Xiaoke Huang",
        "user": "xk-huang",
        "type": "user"
      },
      "summary": "検証時スケーリング는 대규모 언어 모델의 논리 능력 향상을 위해 강력한 기술로 등장한 반면,其在医学伦理中的效果는 불분명하고, 의료 분야는 수학 태스크와 비교하여 구조적으로 다른 지식 표현 및 판단 과정에 기반한다. 본 논문에서는, 검증 시간 스케일링의 첫 번째 상세 조사를 제공하고, m1라는 간단하고 효과적인 접근 방식을 제안하여, 추론 시 모델의 의료 논리 능력 향상을 목표로 한다. 다양한 의료 태스크의 평가에 따라, 검증 시간 스케일링은 의료 논리를 일관적으로 향상시키고, 10B 파라미터 미만의 가벼운 조정된 모델이 새로운 최상급 성능을 달성하며, 우리 32B 모델은 지난 주 70B 스케일의 의료 LLM과 비교하여 우수한 성능을 보였다. 그러나 최적의 논리 토큰 벡터는 약 4K에 가까운 것이며, 더 많은 경우 잘못된 판단이 성능 저하의 원인으로 나타나는 것을 확인했다. 반복적인 프롬프트를 통한 검증 시간 계산을 연장하는 토큰 벡터 최적화는 모델이 답을 체크하도록 촉발하지만, 전체적인 의료 QA 성능 향상은 비확실하며, 그 경우에는 올바른 답을 유도하는 정도는 거의 없고, 이전에 올바른 답에 오류를 추가하는 경우도 있다. 개별 분석에서, 부족한 의료 지식이 성능 진보의 주요 한계로 인식되어, 데이터 크기의 확장, 데이터 품질의 향상, 모델 용량의 확장은 의료 지식을 기반으로 강화하고, 특히 어려운 의료 벤치마크에서 작은 모델이 성공을 달성하며, 성능의 지속적인 발전이 입증되었다. 이러한 발견은 LLM의 의료와 수학의 논리의 기본적인 차이를 명확히 하고, 의료 지식을 풍부화는 이유의 깊이 증가뿐만 아니라 검증 시간 스케일링의 효과 실현을 필수적인 요소임을 강조한다.",
      "upvotes": 3,
      "discussionId": "67ecb60376900f68cd1df550",
      "githubRepo": "https://github.com/UCSC-VLAA/m1",
      "ai_keywords": [
        "test-time scaling",
        "large language models",
        "medical reasoning",
        "knowledge representation",
        "decision-making processes",
        "lightweight fine-tuned models",
        "state-of-the-art performance",
        "reasoning token budget",
        "budget forcing",
        "iterative prompts",
        "medical QA performance",
        "medical knowledge",
        "data scale",
        "data quality",
        "model capacity",
        "medical knowledge grounding",
        "challenging medical benchmarks",
        "reasoning depth"
      ]
    },
    "publishedAt": "2025-04-01T10:57:43.000Z",
    "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning\n  with Large Language Models",
    "summary": "Test-time scaling has emerged as a powerful technique for enhancing the\nreasoning capabilities of large language models. However, its effectiveness in\nmedical reasoning remains uncertain, as the medical domain fundamentally\ndiffers from mathematical tasks in terms of knowledge representation and\ndecision-making processes. In this paper, we provide the first comprehensive\ninvestigation of test-time scaling for medical reasoning and present m1, a\nsimple yet effective approach that increases a model's medical reasoning\ncapability at inference. Our evaluation across diverse medical tasks\ndemonstrates that test-time scaling consistently enhances medical reasoning,\nenabling lightweight fine-tuned models under 10B parameters to establish new\nstate-of-the-art performance, while our 32B model rivals previous 70B-scale\nmedical LLMs. However, we identify an optimal reasoning token budget of\napproximately 4K, beyond which performance may degrade due to overthinking.\nBudget forcing, which extends test-time computation through iterative prompts,\nhelps models double-check answers but does not necessarily improve the overall\nmedical QA performance and, in some cases, even introduces errors into\npreviously correct responses. Our case-by-case analysis identifies insufficient\nmedical knowledge as a key bottleneck that prevents further performance gains\nthrough test-time scaling. We find that increasing data scale, improving data\nquality, and expanding model capacity consistently enhance medical knowledge\ngrounding, enabling continued performance improvements, particularly on\nchallenging medical benchmarks where smaller models reach saturation. These\nfindings underscore fundamental differences between medical and mathematical\nreasoning in LLMs, highlighting that enriched medical knowledge, other than\nincreased reasoning depth alone, is essential for realizing the benefits of\ntest-time scaling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00869.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63318b2349a9563915469f3b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg",
      "fullname": "Xiaoke Huang",
      "name": "xk-huang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23434",
      "authors": [
        {
          "_id": "67ecf47dd0f4d6684e0fb7e4",
          "name": "Yucheng Shi",
          "hidden": false
        },
        {
          "_id": "67ecf47dd0f4d6684e0fb7e5",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "67ecf47dd0f4d6684e0fb7e6",
          "name": "Wenlin Yao",
          "hidden": false
        },
        {
          "_id": "67ecf47dd0f4d6684e0fb7e7",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "67ecf47dd0f4d6684e0fb7e8",
          "name": "Ninghao Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T13:26:00.000Z",
      "submittedOnDailyAt": "2025-04-02T06:59:17.200Z",
      "title": "デプロイドガイドバイダーズ向けの挑戦：サーチ",
      "submittedOnDailyBy": {
        "_id": "64beb6b6140491ca9f803ebf",
        "avatarUrl": "/avatars/0daa2e813a13668b8b708cd8c12763d9.svg",
        "isPro": false,
        "fullname": "Yucheng SHi",
        "user": "YuchengShi",
        "type": "user"
      },
      "summary": "GUI アガント는 대규모의 기초 모델을 통해 힘을 입히고 있으며, 디지털 인터페이스와 상호작용할 수 있어 웹 자동화, 모바일 네비게이션, 소프트웨어 테스트 등 다양한 애플리케이션을 가능하게 합니다. 그러나 이러한 확장된 자동성으로 인해, 보안, 프라이버시, 안전성에 대한 중요한 우려가 제기되었습니다. 이 조사는 GUI アガント의 신뢰성을 구성하는 5가지 중요한 차원에 대해 검토하고 있습니다: 보안의 취약성, 동적인 환경에서의 신뢰성, 투명성과 설명성, 윤리적인 고려, 평가 방법. 또한, 공격에 취약한 점, 연속적 실패 모드, 실제 평가 벤치마크의 부족 등 큰 문제점을 밝혀냅니다. 이러한 문제를 해결하기 위해서는 실제 세계적인 기능에 장애를 가해, 그리고 더 광범위한 전체적인 대책의 필요성을 요구합니다. GUI アガント의 확산으로 인해, 강력한 보안 규격과 책임적인 개발 프로세스의 구축이 중요합니다. 이 조사는 신뢰성 있는 GUI アガント의 발전을 촉진하기 위한 기초를 제공하며, 미래의 연구로 시스템적인 이해를 깊게 도모하고자 합니다.",
      "upvotes": 3,
      "discussionId": "67ecf480d0f4d6684e0fb855",
      "githubRepo": "https://github.com/sycny/Awesome-Trustworthy-GUI-Agents",
      "ai_keywords": [
        "foundation models",
        "GUI agents",
        "web automation",
        "mobile navigation",
        "software testing",
        "security vulnerabilities",
        "reliability in dynamic environments",
        "transparency and explainability",
        "ethical considerations",
        "evaluation methodologies",
        "adversarial attacks",
        "cascading failure modes",
        "sequential decision-making",
        "realistic evaluation benchmarks"
      ]
    },
    "publishedAt": "2025-03-30T09:26:00.000Z",
    "title": "Towards Trustworthy GUI Agents: A Survey",
    "summary": "GUI agents, powered by large foundation models, can interact with digital\ninterfaces, enabling various applications in web automation, mobile navigation,\nand software testing. However, their increasing autonomy has raised critical\nconcerns about their security, privacy, and safety. This survey examines the\ntrustworthiness of GUI agents in five critical dimensions: security\nvulnerabilities, reliability in dynamic environments, transparency and\nexplainability, ethical considerations, and evaluation methodologies. We also\nidentify major challenges such as vulnerability to adversarial attacks,\ncascading failure modes in sequential decision-making, and a lack of realistic\nevaluation benchmarks. These issues not only hinder real-world deployment but\nalso call for comprehensive mitigation strategies beyond task success. As GUI\nagents become more widespread, establishing robust safety standards and\nresponsible development practices is essential. This survey provides a\nfoundation for advancing trustworthy GUI agents through systematic\nunderstanding and future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23434.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64beb6b6140491ca9f803ebf",
      "avatarUrl": "/avatars/0daa2e813a13668b8b708cd8c12763d9.svg",
      "fullname": "Yucheng SHi",
      "name": "YuchengShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00698",
      "authors": [
        {
          "_id": "67ecc2eb077a9bc63e7ba1a2",
          "name": "Team Cohere",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a3",
          "name": "Aakanksha",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a4",
          "name": "Arash Ahmadian",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a5",
          "name": "Marwan Ahmed",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a6",
          "name": "Jay Alammar",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a7",
          "name": "Yazeed Alnumay",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a8",
          "name": "Sophia Althammer",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a9",
          "name": "Arkady Arkhangorodsky",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1aa",
          "name": "Viraat Aryabumi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ab",
          "name": "Dennis Aumiller",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ac",
          "name": "Raphaël Avalos",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ad",
          "name": "Zahara Aviv",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ae",
          "name": "Sammie Bae",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1af",
          "name": "Saurabh Baji",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b0",
          "name": "Alexandre Barbet",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b1",
          "name": "Max Bartolo",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b2",
          "name": "Björn Bebensee",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b3",
          "name": "Neeral Beladia",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b4",
          "name": "Walter Beller-Morales",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b5",
          "name": "Alexandre Bérard",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b6",
          "name": "Andrew Berneshawi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b7",
          "name": "Anna Bialas",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b8",
          "name": "Phil Blunsom",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b9",
          "name": "Matt Bobkin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ba",
          "name": "Adi Bongale",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1bb",
          "name": "Sam Braun",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1bc",
          "name": "Maxime Brunet",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1bd",
          "name": "Samuel Cahyawijaya",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1be",
          "name": "David Cairuz",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1bf",
          "name": "Jon Ander Campos",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c0",
          "name": "Cassie Cao",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c1",
          "name": "Kris Cao",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c2",
          "name": "Roman Castagné",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c3",
          "name": "Julián Cendrero",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c4",
          "name": "Leila Chan Currie",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c5",
          "name": "Yash Chandak",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c6",
          "name": "Diane Chang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c7",
          "name": "Giannis Chatziveroglou",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c8",
          "name": "Hongyu Chen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c9",
          "name": "Claire Cheng",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ca",
          "name": "Alexis Chevalier",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1cb",
          "name": "Justin T. Chiu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1cc",
          "name": "Eugene Cho",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1cd",
          "name": "Eugene Choi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ce",
          "name": "Eujeong Choi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1cf",
          "name": "Tim Chung",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d0",
          "name": "Volkan Cirik",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d1",
          "name": "Ana Cismaru",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d2",
          "name": "Pierre Clavier",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d3",
          "name": "Henry Conklin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d4",
          "name": "Lucas Crawhall-Stein",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d5",
          "name": "Devon Crouse",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d6",
          "name": "Andres Felipe Cruz-Salinas",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d7",
          "name": "Ben Cyrus",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d8",
          "name": "Daniel D'souza",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d9",
          "name": "Hugo Dalla-Torre",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1da",
          "name": "John Dang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1db",
          "name": "William Darling",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1dc",
          "name": "Omar Darwiche Domingues",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1dd",
          "name": "Saurabh Dash",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1de",
          "name": "Antoine Debugne",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1df",
          "name": "Théo Dehaze",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e0",
          "name": "Shaan Desai",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e1",
          "name": "Joan Devassy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e2",
          "name": "Rishit Dholakia",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e3",
          "name": "Kyle Duffy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e4",
          "name": "Ali Edalati",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e5",
          "name": "Ace Eldeib",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e6",
          "name": "Abdullah Elkady",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e7",
          "name": "Sarah Elsharkawy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e8",
          "name": "Irem Ergün",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e9",
          "name": "Beyza Ermis",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ea",
          "name": "Marzieh Fadaee",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1eb",
          "name": "Boyu Fan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ec",
          "name": "Lucas Fayoux",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ed",
          "name": "Yannis Flet-Berliac",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ee",
          "name": "Nick Frosst",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ef",
          "name": "Matthias Gallé",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f0",
          "name": "Wojciech Galuba",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f1",
          "name": "Utsav Garg",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f2",
          "name": "Matthieu Geist",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f3",
          "name": "Mohammad Gheshlaghi Azar",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f4",
          "name": "Seraphina Goldfarb-Tarrant",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f5",
          "name": "Tomas Goldsack",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f6",
          "name": "Aidan Gomez",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f7",
          "name": "Victor Machado Gonzaga",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f8",
          "name": "Nithya Govindarajan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f9",
          "name": "Manoj Govindassamy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fa",
          "name": "Nathan Grinsztajn",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fb",
          "name": "Nikolas Gritsch",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fc",
          "name": "Patrick Gu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fd",
          "name": "Shangmin Guo",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fe",
          "name": "Kilian Haefeli",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ff",
          "name": "Rod Hajjar",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba200",
          "name": "Tim Hawes",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba201",
          "name": "Jingyi He",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba202",
          "name": "Sebastian Hofstätter",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba203",
          "name": "Sungjin Hong",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba204",
          "name": "Sara Hooker",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba205",
          "name": "Tom Hosking",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba206",
          "name": "Stephanie Howe",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba207",
          "name": "Eric Hu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba208",
          "name": "Renjie Huang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba209",
          "name": "Hemant Jain",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20a",
          "name": "Ritika Jain",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20b",
          "name": "Nick Jakobi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20c",
          "name": "Madeline Jenkins",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20d",
          "name": "JJ Jordan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20e",
          "name": "Dhruti Joshi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20f",
          "name": "Jason Jung",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba210",
          "name": "Trushant Kalyanpur",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba211",
          "name": "Siddhartha Rao Kamalakara",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba212",
          "name": "Julia Kedrzycki",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba213",
          "name": "Gokce Keskin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba214",
          "name": "Edward Kim",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba215",
          "name": "Joon Kim",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba216",
          "name": "Wei-Yin Ko",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba217",
          "name": "Tom Kocmi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba218",
          "name": "Michael Kozakov",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba219",
          "name": "Wojciech Kryściński",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21a",
          "name": "Arnav Kumar Jain",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21b",
          "name": "Komal Kumar Teru",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21c",
          "name": "Sander Land",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21d",
          "name": "Michael Lasby",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21e",
          "name": "Olivia Lasche",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21f",
          "name": "Justin Lee",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba220",
          "name": "Patrick Lewis",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba221",
          "name": "Jeffrey Li",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba222",
          "name": "Jonathan Li",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba223",
          "name": "Hangyu Lin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba224",
          "name": "Acyr Locatelli",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba225",
          "name": "Kevin Luong",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba226",
          "name": "Raymond Ma",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba227",
          "name": "Lukas Mach",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba228",
          "name": "Marina Machado",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba229",
          "name": "Joanne Magbitang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22a",
          "name": "Brenda Malacara Lopez",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22b",
          "name": "Aryan Mann",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22c",
          "name": "Kelly Marchisio",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22d",
          "name": "Olivia Markham",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22e",
          "name": "Alexandre Matton",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22f",
          "name": "Alex McKinney",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba230",
          "name": "Dominic McLoughlin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba231",
          "name": "Jozef Mokry",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba232",
          "name": "Adrien Morisot",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba233",
          "name": "Autumn Moulder",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba234",
          "name": "Harry Moynehan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba235",
          "name": "Maximilian Mozes",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba236",
          "name": "Vivek Muppalla",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba237",
          "name": "Lidiya Murakhovska",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba238",
          "name": "Hemangani Nagarajan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba239",
          "name": "Alekhya Nandula",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23a",
          "name": "Hisham Nasir",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23b",
          "name": "Shauna Nehra",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23c",
          "name": "Josh Netto-Rosen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23d",
          "name": "Daniel Ohashi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23e",
          "name": "James Owers-Bardsley",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23f",
          "name": "Jason Ozuzu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba240",
          "name": "Dennis Padilla",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba241",
          "name": "Gloria Park",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba242",
          "name": "Sam Passaglia",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba243",
          "name": "Jeremy Pekmez",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba244",
          "name": "Laura Penstone",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba245",
          "name": "Aleksandra Piktus",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba246",
          "name": "Case Ploeg",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba247",
          "name": "Andrew Poulton",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba248",
          "name": "Youran Qi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba249",
          "name": "Shubha Raghvendra",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24a",
          "name": "Miguel Ramos",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24b",
          "name": "Ekagra Ranjan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24c",
          "name": "Pierre Richemond",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24d",
          "name": "Cécile Robert-Michon",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24e",
          "name": "Aurélien Rodriguez",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24f",
          "name": "Sudip Roy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba250",
          "name": "Laura Ruis",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba251",
          "name": "Louise Rust",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba252",
          "name": "Anubhav Sachan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba253",
          "name": "Alejandro Salamanca",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba254",
          "name": "Kailash Karthik Saravanakumar",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba255",
          "name": "Isha Satyakam",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba256",
          "name": "Alice Schoenauer Sebag",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba257",
          "name": "Priyanka Sen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba258",
          "name": "Sholeh Sepehri",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba259",
          "name": "Preethi Seshadri",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25a",
          "name": "Ye Shen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25b",
          "name": "Tom Sherborne",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25c",
          "name": "Sylvie Chang Shi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25d",
          "name": "Sanal Shivaprasad",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25e",
          "name": "Vladyslav Shmyhlo",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25f",
          "name": "Anirudh Shrinivason",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba260",
          "name": "Inna Shteinbuk",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba261",
          "name": "Amir Shukayev",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba262",
          "name": "Mathieu Simard",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba263",
          "name": "Ella Snyder",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba264",
          "name": "Ava Spataru",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba265",
          "name": "Victoria Spooner",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba266",
          "name": "Trisha Starostina",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba267",
          "name": "Florian Strub",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba268",
          "name": "Yixuan Su",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba269",
          "name": "Jimin Sun",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26a",
          "name": "Dwarak Talupuru",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26b",
          "name": "Eugene Tarassov",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26c",
          "name": "Elena Tommasone",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26d",
          "name": "Jennifer Tracey",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26e",
          "name": "Billy Trend",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26f",
          "name": "Evren Tumer",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba270",
          "name": "Ahmet Üstün",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba271",
          "name": "Bharat Venkitesh",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba272",
          "name": "David Venuto",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba273",
          "name": "Pat Verga",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba274",
          "name": "Maxime Voisin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba275",
          "name": "Alex Wang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba276",
          "name": "Donglu Wang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba277",
          "name": "Shijian Wang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba278",
          "name": "Edmond Wen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba279",
          "name": "Naomi White",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27a",
          "name": "Jesse Willman",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27b",
          "name": "Marysia Winkels",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27c",
          "name": "Chen Xia",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27d",
          "name": "Jessica Xie",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27e",
          "name": "Minjie Xu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27f",
          "name": "Bowen Yang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba280",
          "name": "Tan Yi-Chern",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba281",
          "name": "Ivan Zhang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba282",
          "name": "Zhenyu Zhao",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba283",
          "name": "Zhoujie Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T12:08:07.000Z",
      "submittedOnDailyAt": "2025-04-02T03:24:20.179Z",
      "title": "企業용의 대규모 언어 모델",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "이 보고서에서 Command A의 개발을 설명합니다. Command A는 현실적인 기업 사례에 뛰어난 강력한 대규모 언어 모델로 특화된 모델입니다. Command A는 에이전트 최적화된 모델이며, 23 언어의 비즈니스를 지원하고 새로운 하이브리드 아키텍처를 가지고 효율성과 최고 수준의 성능을 균형을 이루는 모델입니다. 이 모델의 능력은 복잡한 비즈니스 프로세스를 자동화하기 위한 가장 선진적인 리뷰어링 어셈블리 디지제스(RAG) 기능과 도구 사용으로 실현됩니다. 이러한 능력은 분산된 훈련 접근법, 자동 최적화 알고리즘 및 모델 병합 기술로 구현됩니다. 또한, Command R7B의 결과를 포함합니다. Command R7B는 Command A와 능력 및 아키텍처의 유사성을 가지고 있습니다. 두 모델의 가중치는 연구의 목적로 릴리즈되었습니다. 이 기술 보고서에서는 원형 훈련 파이프라인을 상세히 설명하고, 기업과 관련된 태스크 및 공개 벤치마크에서 다양한 평가 결과를 제공하여 더 좋은 성능과 효율성을 보여주고 있습니다.",
      "upvotes": 2,
      "discussionId": "67ecc2ec077a9bc63e7ba2bd",
      "ai_keywords": [
        "agent-optimised",
        "multilingual-capable",
        "hybrid architecture",
        "Retrieval Augmented Generation (RAG)",
        "grounding",
        "tool use",
        "decentralised training",
        "self-refinement algorithms",
        "model merging techniques"
      ]
    },
    "publishedAt": "2025-04-01T08:08:07.000Z",
    "title": "Command A: An Enterprise-Ready Large Language Model",
    "summary": "In this report we describe the development of Command A, a powerful large\nlanguage model purpose-built to excel at real-world enterprise use cases.\nCommand A is an agent-optimised and multilingual-capable model, with support\nfor 23 languages of global business, and a novel hybrid architecture balancing\nefficiency with top of the range performance. It offers best-in-class Retrieval\nAugmented Generation (RAG) capabilities with grounding and tool use to automate\nsophisticated business processes. These abilities are achieved through a\ndecentralised training approach, including self-refinement algorithms and model\nmerging techniques. We also include results for Command R7B which shares\ncapability and architectural similarities to Command A. Weights for both models\nhave been released for research purposes. This technical report details our\noriginal training pipeline and presents an extensive evaluation of our models\nacross a suite of enterprise-relevant tasks and public benchmarks,\ndemonstrating excellent performance and efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00698.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6566
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00072",
      "authors": [
        {
          "_id": "67eccbca1006da75eca94d24",
          "name": "Lucas Ventura",
          "hidden": false
        },
        {
          "_id": "67eccbca1006da75eca94d25",
          "name": "Antoine Yang",
          "hidden": false
        },
        {
          "_id": "67eccbca1006da75eca94d26",
          "name": "Cordelia Schmid",
          "hidden": false
        },
        {
          "_id": "67eccbca1006da75eca94d27",
          "name": "Gül Varol",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:41:29.000Z",
      "submittedOnDailyAt": "2025-04-02T04:02:28.798Z",
      "title": "Chapter-Llama: 긴 비디오의 효율적인 섹션 분할에 대한 LLM",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "ビデオチャプターリング의 과제를 다루고, 긴 비디오 시간라인을 의미적인 단위로 분할하고, 대응하는 チャプター 타이틀을 생성합니다. 이 작업은 상대적으로 조사가 적지만, 자동 チャプター 리닝은 긴 비디오에서 효율적인 네비게이션 및 콘텐츠 검색에 잠재력이 있습니다. 본 논문에서는 'Chapter-Llama' 프레임워크를 사용하여, 텍스트 영역에서 효율적으로 문제를 해결하고, 1시간의 비디오에 강력한 チャプター 리닝 성능을 달성했습니다. 특히, 규모가 큰 언어 모델(LLM)을 활용하여, 비디오 프레임을 설명하는 캡션과 그 시간 스탬프를 입력합니다. 모든 프레임을 엄격히 캡션하는 것은 효율이 낮기 때문에, 비디오 트랜스ク립트의 내용을 기반으로 가벼운 비디오 프레임 선택 전략을 제안하고, 실험적으로 뚜렷한 우세를 보여주었습니다. LLM을 학습시켜, チャプター의 경계의 시간 스탬프와 자유 형식의 チャプター 타이틀을 출력하는 것을 목표로 합니다. 이 간단하지만 강력한 접근 방식은 1시간의 비디오를 하나의 forward pass로 처리할 수 있습니다. 우리의 결과를 최근 VidChapters-7M 벤치마크의 최신 수준과 비교하여 크게 향상시켰습니다(예를 들어, F1 스코어 45.3 vs 26.7). 프로젝트 페이지에서 코드와 모델을 공개하고, 더 나은 연구를 위해 더 많은 연구를 촉진합니다.",
      "upvotes": 2,
      "discussionId": "67eccbce1006da75eca94e68",
      "ai_keywords": [
        "large language model (LLM)",
        "context window",
        "speech transcripts",
        "captions",
        "timestamps",
        "frame selection strategy",
        "chapter boundaries",
        "F1 score"
      ]
    },
    "publishedAt": "2025-03-31T13:41:29.000Z",
    "title": "Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs",
    "summary": "We address the task of video chaptering, i.e., partitioning a long video\ntimeline into semantic units and generating corresponding chapter titles. While\nrelatively underexplored, automatic chaptering has the potential to enable\nefficient navigation and content retrieval in long-form videos. In this paper,\nwe achieve strong chaptering performance on hour-long videos by efficiently\naddressing the problem in the text domain with our 'Chapter-Llama' framework.\nSpecifically, we leverage a pretrained large language model (LLM) with large\ncontext window, and feed as input (i) speech transcripts and (ii) captions\ndescribing video frames, along with their respective timestamps. Given the\ninefficiency of exhaustively captioning all frames, we propose a lightweight\nspeech-guided frame selection strategy based on speech transcript content, and\nexperimentally demonstrate remarkable advantages. We train the LLM to output\ntimestamps for the chapter boundaries, as well as free-form chapter titles.\nThis simple yet powerful approach scales to processing one-hour long videos in\na single forward pass. Our results demonstrate substantial improvements (e.g.,\n45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M\nbenchmark. To promote further research, we release our code and models at our\nproject page.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00072.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6566
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23733",
      "authors": [
        {
          "_id": "67ec99788088196efd062021",
          "name": "Yiyang Du",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062022",
          "name": "Xiaochen Wang",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062023",
          "user": {
            "_id": "642086ed290342c5df85662d",
            "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
            "isPro": false,
            "fullname": "Chi Chen",
            "user": "carboncoo",
            "type": "user"
          },
          "name": "Chi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:49.411Z",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062024",
          "name": "Jiabo Ye",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062025",
          "name": "Yiru Wang",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062026",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062027",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062028",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062029",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd06202a",
          "name": "Zhifang Sui",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd06202b",
          "name": "Maosong Sun",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd06202c",
          "name": "Yang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T05:13:02.000Z",
      "submittedOnDailyAt": "2025-04-02T00:32:49.651Z",
      "title": "AdaMMS: ホモジュール 다 모델에 대한 모델 통합 및 무 서브바이버 코엔트리티 최적화",
      "submittedOnDailyBy": {
        "_id": "642086ed290342c5df85662d",
        "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
        "isPro": false,
        "fullname": "Chi Chen",
        "user": "carboncoo",
        "type": "user"
      },
      "summary": "최근, 모델 통합手法는 많은 규모의 언어 모델(LLMs)에서 다양한 태스크의 능력을 통합하는 강력한 장점을 보여주고 있습니다. 이전의 모델 통합手法는 주로 구조가 같은 Homogeneous 모델을 통합하는 데 초점을 두고, 고유한 Heterogeneous의 특성을 가진 다모둠 언어 모델(MLLMs)에 대해 도전을 받았다. 이 구조에서, Heterogeneous의 MLLM에 적합한 새로운 모델 통합手法 AdaMMS를 제안합니다. 우리 방법은 3단계로 도전을 해결합니다: 매핑, 통합, 탐색. 특히, 먼저, 다른 구조의 MLLM에 모델 통합을 적용하기 위해 매핑 함수를 설계합니다. 다음으로, Heterogeneous의 MLLM의 파라미터 공간의 불균형을 主动的으로 조정하기 위해 선형 이터플레이션을 적용합니다. 마지막으로, 라벨付け 데이터를 사용하지 않는 모델 통합을 위한 무생존 하이퍼 파라미터 선택 방법을 제안합니다. 처음의 Heterogeneous의 MLLM을 통합할 수 있는 모델 통합手法이기 때문에, 다양한 모델 조합에 대한 확장적인 실험에서 AdaMMS는 이전의 모델 통합手法보다 다양한 시각 언어 벤치마크에서 높은 평가를 받았습니다.",
      "upvotes": 2,
      "discussionId": "67ec99798088196efd062081",
      "ai_keywords": [
        "model merging",
        "Large Language Models (LLMs)",
        "Multimodal Large Language Models (MLLMs)",
        "heterogeneous property",
        "model architecture",
        "parameter space",
        "AdaMMS",
        "mapping function",
        "linear interpolation",
        "hyper-parameter searching",
        "unsupervised hyper-parameter selection",
        "vision-language benchmarks"
      ]
    },
    "publishedAt": "2025-03-31T01:13:02.000Z",
    "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization",
    "summary": "Recently, model merging methods have demonstrated powerful strengths in\ncombining abilities on various tasks from multiple Large Language Models\n(LLMs). While previous model merging methods mainly focus on merging\nhomogeneous models with identical architecture, they meet challenges when\ndealing with Multimodal Large Language Models (MLLMs) with inherent\nheterogeneous property, including differences in model architecture and the\nasymmetry in the parameter space. In this work, we propose AdaMMS, a novel\nmodel merging method tailored for heterogeneous MLLMs. Our method tackles the\nchallenges in three steps: mapping, merging and searching. Specifically, we\nfirst design mapping function between models to apply model merging on MLLMs\nwith different architecture. Then we apply linear interpolation on model\nweights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in\nthe hyper-parameter searching step, we propose an unsupervised hyper-parameter\nselection method for model merging. As the first model merging method capable\nof merging heterogeneous MLLMs without labeled data, extensive experiments on\nvarious model combinations demonstrated that AdaMMS outperforms previous model\nmerging methods on various vision-language benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23733.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642086ed290342c5df85662d",
      "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
      "fullname": "Chi Chen",
      "name": "carboncoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21860",
      "authors": [
        {
          "_id": "67ec97f65d9c75ff46de2974",
          "name": "Kailin Li",
          "hidden": false
        },
        {
          "_id": "67ec97f65d9c75ff46de2975",
          "name": "Puhao Li",
          "hidden": false
        },
        {
          "_id": "67ec97f65d9c75ff46de2976",
          "name": "Tengyu Liu",
          "hidden": false
        },
        {
          "_id": "67ec97f65d9c75ff46de2977",
          "name": "Yuyang Li",
          "hidden": false
        },
        {
          "_id": "67ec97f65d9c75ff46de2978",
          "name": "Siyuan Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:50:30.000Z",
      "submittedOnDailyAt": "2025-04-02T00:21:41.585Z",
      "title": "残差 학습을 이용한 효율적인 양손동시 동작의 멕시피러션 전송",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "인간의 손은 상호작용에 중심적인 역할을 수행하며, 이를 위해 디커트블 로봇의 조작에 대한 연구가 증가하고 있습니다. 데이터 주도의 구체적인 AI 알고리즘은 실제 강화학습이나 실세계의 테레오프레레이에서 얻는 것이 어렵지만, 인간처럼 정밀한 대규모 동작열을 요구합니다. 이를 해결하기 위해, 우리는 새로운 2단계 방법인 ManipTrans를 소개합니다. ManipTrans는 손의 동작을 모방하는 일반적인 트래지컬 이미지 생성기를 사전 학습시키고, 상호작용의 제약 아래 특정 잔차 모듈을 미세 조정하여 복잡한 두 손의 일을 효율적으로 학습하고 정확한 실행을 가능하게 합니다. 실험은 ManipTrans가 성공률, 정확도, 효율성에서 가장 先端한 방법을 초월하는 것을 보여주었습니다. ManipTrans를 활용하여, 우리는 로봇의 손으로 다수의 손의 객체 데이터 세트를 전달하고 DexManipNet을 만들었습니다. DexManipNet은 이전에 시도되지 않은 태스크처럼, 펜의 컵의 마스터나 부트ル의 윌드링 등 큰 데이터 세트를 다루는 것입니다. DexManipNet은 3.3K의 로봇의 동작의 에피소드를 포함하며, 간단하게 확장할 수 있으며, 디커트블한 손의 정책의 훈련을 촉진하고 실세계의 구현을 가능하게 합니다.",
      "upvotes": 2,
      "discussionId": "67ec97f85d9c75ff46de29f0",
      "projectPage": "https://maniptrans.github.io/",
      "ai_keywords": [
        "dexterous robotic manipulation",
        "data-driven embodied AI",
        "trajectory imitator",
        "bimanual skills",
        "interaction constraints",
        "fine-tuning",
        "parameter space",
        "DexManipNet",
        "pen capping",
        "bottle unscrewing",
        "episodes",
        "policy training"
      ]
    },
    "publishedAt": "2025-03-27T13:50:30.000Z",
    "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning",
    "summary": "Human hands play a central role in interacting, motivating increasing\nresearch in dexterous robotic manipulation. Data-driven embodied AI algorithms\ndemand precise, large-scale, human-like manipulation sequences, which are\nchallenging to obtain with conventional reinforcement learning or real-world\nteleoperation. To address this, we introduce ManipTrans, a novel two-stage\nmethod for efficiently transferring human bimanual skills to dexterous robotic\nhands in simulation. ManipTrans first pre-trains a generalist trajectory\nimitator to mimic hand motion, then fine-tunes a specific residual module under\ninteraction constraints, enabling efficient learning and accurate execution of\ncomplex bimanual tasks. Experiments show that ManipTrans surpasses\nstate-of-the-art methods in success rate, fidelity, and efficiency. Leveraging\nManipTrans, we transfer multiple hand-object datasets to robotic hands,\ncreating DexManipNet, a large-scale dataset featuring previously unexplored\ntasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K\nepisodes of robotic manipulation and is easily extensible, facilitating further\npolicy training for dexterous hands and enabling real-world deployments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21860.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6566
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.24210",
      "authors": [
        {
          "_id": "67ece4b12511c2aab09b84ca",
          "name": "Seungjun Lee",
          "hidden": false
        },
        {
          "_id": "67ece4b12511c2aab09b84cb",
          "name": "Gim Hee Lee",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/662f5b1e5aeedf55d209741a/CYuJcfz3_px75zb9IqBTP.png"
      ],
      "publishedAt": "2025-03-31T15:27:07.000Z",
      "submittedOnDailyAt": "2025-04-02T05:49:14.777Z",
      "title": "DiET-GS: 확산 앞선과 이벤트 스트리밍을 지원하는 3D 이동의 변형 분산화 3D Gaussian Splatting",
      "submittedOnDailyBy": {
        "_id": "662f5b1e5aeedf55d209741a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662f5b1e5aeedf55d209741a/YdsbnRJPj_6PRfWbCNWfX.jpeg",
        "isPro": false,
        "fullname": "Seungjun Lee",
        "user": "onandon",
        "type": "user"
      },
      "summary": "ブラーミュービウス 이미지에서 짙은 3D 표현을 재구성하는 것은 컴퓨터 비전의 오랜 문제입니다. 최근의 연구는, 이벤트 기반 카메라를 사용하여, 이동 중인 블러로부터 고품질의 새로운 시각을 합성하고, 고동적 범위와 마이크로 수준의 시간 분해능을 활용하여 이점을 얻습니다. 그러나, 이는 색의 부적절한 복원과 미세한 세부 정보의 손실로 인해 시각적 품질이 최적값을 달성하지 않습니다. 본 논문에서는, DiET-GS(Diffusion Prior and Event Stream-Assisted Motion Deblurring 3DGS)를 소개합니다. 우리의 프레임워크는, 블러 없는 이벤트 스트리밍과 분산 우선을 두 단계의 훈련 전략에 따라 효과적으로 활용합니다. 특히, 우리는, 이벤트의 이중 적분으로 3DGS를 제약하는 새로운 프레임워크를 제안하고, 정확한 색과 명확한 세부 정보를 구현합니다. 또한, 분산 우선을 사용하여, 경계의 세부 정보를 더욱 강화하는 간단한 기술도 제안합니다. 합성 데이터와 실세계 데이터 모두에서, 양질의 및 양량적인 결과를 보여주며, 우리의 DiET-GS가 기존 기준과 비교하여 새로운 시각의 품질이 크게 향상되어 있음을 보여줍니다. 프로젝트 페이지는 https://diet-gs.github.io입니다.",
      "upvotes": 1,
      "discussionId": "67ece4b62511c2aab09b8660",
      "projectPage": "https://diet-gs.github.io",
      "githubRepo": "https://github.com/DiET-GS/DiET-GS",
      "ai_keywords": [
        "diffusion prior",
        "event stream",
        "motion deblurring",
        "3DGS (3D Geometry Synthesis)",
        "blur-free event streams",
        "event double integral",
        "edge details",
        "novel view synthesis"
      ]
    },
    "publishedAt": "2025-03-31T11:27:07.000Z",
    "title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D\n  Gaussian Splatting",
    "summary": "Reconstructing sharp 3D representations from blurry multi-view images are\nlong-standing problem in computer vision. Recent works attempt to enhance\nhigh-quality novel view synthesis from the motion blur by leveraging\nevent-based cameras, benefiting from high dynamic range and microsecond\ntemporal resolution. However, they often reach sub-optimal visual quality in\neither restoring inaccurate color or losing fine-grained details. In this\npaper, we present DiET-GS, a diffusion prior and event stream-assisted motion\ndeblurring 3DGS. Our framework effectively leverages both blur-free event\nstreams and diffusion prior in a two-stage training strategy. Specifically, we\nintroduce the novel framework to constraint 3DGS with event double integral,\nachieving both accurate color and well-defined details. Additionally, we\npropose a simple technique to leverage diffusion prior to further enhance the\nedge details. Qualitative and quantitative results on both synthetic and\nreal-world data demonstrate that our DiET-GS is capable of producing\nsignificantly better quality of novel views compared to the existing baselines.\nOur project page is https://diet-gs.github.io",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/662f5b1e5aeedf55d209741a/CYuJcfz3_px75zb9IqBTP.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24210.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662f5b1e5aeedf55d209741a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662f5b1e5aeedf55d209741a/YdsbnRJPj_6PRfWbCNWfX.jpeg",
      "fullname": "Seungjun Lee",
      "name": "onandon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]