[
  {
    "paper": {
      "id": "2505.24120",
      "authors": [
        {
          "_id": "683fc08da33aeee1124887c4",
          "name": "Ai Jian",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c5",
          "user": {
            "_id": "660aab2c878289c5b34f9e97",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660aab2c878289c5b34f9e97/yxx1-lR8x5o6KaEpZDXQq.jpeg",
            "isPro": false,
            "fullname": "weijie qiu",
            "user": "qiuwj",
            "type": "user"
          },
          "name": "Weijie Qiu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:56:39.772Z",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c6",
          "user": {
            "_id": "62be9b5aae56e75e4d689e7c",
            "avatarUrl": "/avatars/6772bc09d6eeb4e86b1210481be91720.svg",
            "isPro": false,
            "fullname": "wangxiaokun",
            "user": "shawn0wang",
            "type": "user"
          },
          "name": "Xiaokun Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:01.766Z",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c7",
          "name": "Peiyu Wang",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c8",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c9",
          "name": "Jiangbo Pei",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887ca",
          "name": "Yichen Wei",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887cb",
          "name": "Yi Peng",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887cc",
          "name": "Xuchen Song",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/620f5a1c3f76c50e6458a9b6/l5zN70u8jAQBCjl41TWoi.png"
      ],
      "publishedAt": "2025-05-30T01:34:25.000Z",
      "submittedOnDailyAt": "2025-06-04T05:57:36.826Z",
      "title": "CSVQA: 중국의 다모렐 모델 벤치마크에서 VLM의 이론과학 추론 능력 평가",
      "submittedOnDailyBy": {
        "_id": "620f5a1c3f76c50e6458a9b6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
        "isPro": false,
        "fullname": "Peiyu Wang",
        "user": "OrlandoHugBot",
        "type": "user"
      },
      "summary": "시각 언어 모델(VLMs)은 다양한 이해에 대해 놀라운 진보를 보여주고 있지만, 과학적인 논리력의 능력은 충분히 평가되지 않았습니다. 현재 다양한 벤치마크는 일반적인 이미지 이해 또는 텍스트를 기반으로 하는 논리력 평가를 수행하며, 실제 과학적인 맥락을 갖지 않는 문제로, 시각적 증거 분석과 분야 고유의 지식을 통합하는 경우 평가가 부족하게 됩니다. 이러한 부족점을 보완하기 위해, CSVQA(카테고리 정렬 버전의 가상 QA)를 소개합니다. CSVQA는 과학적인 논리력을 평가하기 위해 영역에 기반한 시각적인 질문에 답을 통해 설계된 진단적인 다양한 벤치마크입니다. 벤치마크는 1,378개의 잘 구축된 질문에 답 쌍을 특징으로 하며, 다양한 STEM 분야를 확장하며, 각 문제에는 영역 지식, 시각적 증거의 통합, 그리고 고차원의 논리력이 필요합니다. 기존의 다양한 벤치마크에 비해, CSVQA는 현실적인 과학적인 콘텐츠와 복잡한 논리력을 중점적으로 두는 데 집중됩니다. 또한, 모델의 예측이 보충적인 논리력 단계에 기반하여 정당화되어 있는지를 체계적으로 평가하기 위해 엄격한 평가 프로토콜을 제안합니다. 15개의 모델의 컴퓨터 라인 테스트의 상세한 평가 결과로, 뚜렷한 성능 차이를 확인했습니다. 특히, 가장 높은 수준의 프로퍼티 모델도 그대로 49.6%의 정확도를 달성하지 못했습니다. 이 실험적 증거는 VLMs의 과학적인 논리력의 발전이 우선되어야 하는 것을 강조합니다. CSVQA는 https://huggingface.co/datasets/Skywork/CSVQA에서 공개되어 있습니다.",
      "upvotes": 41,
      "discussionId": "683fc091a33aeee1124888a8",
      "ai_summary": "A new benchmark, CSVQA, evaluates scientific reasoning in vision-language models through domain-specific visual question answering, highlighting the need for improvement in these models.",
      "ai_keywords": [
        "Vision-Language Models",
        "multimodal benchmark",
        "scientific reasoning",
        "domain-grounded",
        "visual question answering",
        "domain-specific knowledge",
        "higher-order reasoning",
        "evaluation protocol",
        "intermediate reasoning steps",
        "curated explanations"
      ]
    },
    "publishedAt": "2025-05-29T21:34:25.000Z",
    "title": "CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning\n  Capabilities of VLMs",
    "summary": "Vision-Language Models (VLMs) have demonstrated remarkable progress in\nmultimodal understanding, yet their capabilities for scientific reasoning\nremains inadequately assessed. Current multimodal benchmarks predominantly\nevaluate generic image comprehension or text-driven reasoning, lacking\nauthentic scientific contexts that require domain-specific knowledge\nintegration with visual evidence analysis. To fill this gap, we present CSVQA,\na diagnostic multimodal benchmark specifically designed for evaluating\nscientific reasoning through domain-grounded visual question answering.Our\nbenchmark features 1,378 carefully constructed question-answer pairs spanning\ndiverse STEM disciplines, each demanding domain knowledge, integration of\nvisual evidence, and higher-order reasoning. Compared to prior multimodal\nbenchmarks, CSVQA places greater emphasis on real-world scientific content and\ncomplex reasoning.We additionally propose a rigorous evaluation protocol to\nsystematically assess whether model predictions are substantiated by valid\nintermediate reasoning steps based on curated explanations. Our comprehensive\nevaluation of 15 VLMs on this benchmark reveals notable performance\ndisparities, as even the top-ranked proprietary model attains only 49.6\\%\naccuracy.This empirical evidence underscores the pressing need for advancing\nscientific reasoning capabilities in VLMs. Our CSVQA is released at\nhttps://huggingface.co/datasets/Skywork/CSVQA.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620f5a1c3f76c50e6458a9b6/l5zN70u8jAQBCjl41TWoi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24120.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620f5a1c3f76c50e6458a9b6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
      "fullname": "Peiyu Wang",
      "name": "OrlandoHugBot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02387",
      "authors": [
        {
          "_id": "683fa95ea0770843560c7ae3",
          "user": {
            "_id": "653a5b0f7c01c693a16dd184",
            "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
            "isPro": false,
            "fullname": "Zelai Xu",
            "user": "zelaix",
            "type": "user"
          },
          "name": "Zelai Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-04T02:03:11.372Z",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae4",
          "name": "Zhexuan Xu",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae5",
          "name": "Xiangmin Yi",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae6",
          "name": "Huining Yuan",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae7",
          "name": "Xinlei Chen",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae8",
          "name": "Yi Wu",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae9",
          "name": "Chao Yu",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7aea",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T02:57:38.000Z",
      "submittedOnDailyAt": "2025-06-04T00:58:29.506Z",
      "title": "VS-Bench: 다 에이전트 환경에서 전략적 논리와 결정의 VLMs 평가",
      "submittedOnDailyBy": {
        "_id": "653a5b0f7c01c693a16dd184",
        "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
        "isPro": false,
        "fullname": "Zelai Xu",
        "user": "zelaix",
        "type": "user"
      },
      "summary": "최근의 비전 언어 모델(VLMs)의 발전은 인터랙티브な 에이전트 태스크에 대한 기능에 확장되었지만, 현재의 벤치마크는 단일 에이전트 또는 텍스트만 포함하는 환경에 제한되어 있습니다. 반면, 실제 세계적인 스케너는 다양한 에이전트가 풍부한 비전과 언어 컨텍스트 내에서 상호작용하며, 다 모델 관측과 전략적인 상호작용을 통해 문제를 해결하고 있습니다. 이를 해결하기 위해, 비전 전략 벤치(VS-Bench)을 소개합니다. VS-Bench는 다 모델 벤치마크로, 다 에이전트 환경에서 전략적인 논리와 결정을 평가하는 VLMs을 평가하기 위해 사용됩니다. VS-Bench는 협동적이고 경쟁적이고, 또는 혼합적인 상호작용을 포함하는 8개의 비전 그래프 지정된 환경을 구성하고, 에이전트가 다른 에이전트의 미래 이동을 예측하고 장기적 목표를 최적화하는 능력을 평가하는 데 목적이 있습니다. 우리는 전략적인 논리의 오프라인 평가와 정규화 에피소드 리턴의 온라인 평가의 두 가지 보완적인 평가 차원을 검토하고 있습니다. 14개의 선도적인 VLMs의 확장 실험을 통해, 현재의 모델과 최적의 성능 사이에 큰 차이를 명확히 할 수 있었습니다. 최고의 모델은 47.8%의 예측 정확도와 24.3%의 정규화 리턴을 달성했습니다. 또한, 다 모델 관측, 테스트 시 스케일링, 사회적인 행동, VLM 에이전트의 실패 사례에 대한 상세한 분석을 수행하여, 현재의 모델의 한계를 밝혀 VS-Bench는 전략적인 다 모델 에이전트의 미래 연구의 기초를 보입니다. 코드와 데이터는 https://vs-bench.github.io에서 사용 가능합니다.",
      "upvotes": 34,
      "discussionId": "683fa95fa0770843560c7b3d",
      "projectPage": "https://vs-bench.github.io",
      "githubRepo": "https://github.com/zelaix/VS-Bench",
      "ai_summary": "VS-Bench is a multimodal benchmark designed to evaluate Vision Language Models' strategic reasoning and decision-making in complex multi-agent environments.",
      "ai_keywords": [
        "Vision Language Models",
        "VS-Bench",
        "multimodal benchmark",
        "strategic reasoning",
        "decision-making",
        "multi-agent environments",
        "vision-grounded environments",
        "cooperative",
        "competitive",
        "mixed-motive interactions",
        "next-action prediction",
        "normalized episode return",
        "multimodal observations",
        "test-time scaling",
        "social behaviors",
        "failure cases"
      ]
    },
    "publishedAt": "2025-06-02T22:57:38.000Z",
    "title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments",
    "summary": "Recent advancements in Vision Language Models (VLMs) have expanded their\ncapabilities to interactive agent tasks, yet existing benchmarks remain limited\nto single-agent or text-only environments. In contrast, real-world scenarios\noften involve multiple agents interacting within rich visual and linguistic\ncontexts, posing challenges with both multimodal observations and strategic\ninteractions. To bridge this gap, we introduce Visual Strategic Bench\n(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning\nand decision-making in multi-agent environments. VS-Bench comprises eight\nvision-grounded environments spanning cooperative, competitive, and\nmixed-motive interactions, designed to assess agents' ability to predict\nothers' future moves and optimize for long-term objectives. We consider two\ncomplementary evaluation dimensions, including offline evaluation of strategic\nreasoning by next-action prediction accuracy and online evaluation of\ndecision-making by normalized episode return. Extensive experiments of fourteen\nleading VLMs reveal a significant gap between current models and optimal\nperformance, with the best models attaining 47.8% prediction accuracy and 24.3%\nnormalized return. We further conduct in-depth analyses on multimodal\nobservations, test-time scaling, social behaviors, and failure cases of VLM\nagents. By standardizing the evaluation and highlighting the limitations of\nexisting models, we envision VS-Bench as a foundation for future research on\nstrategic multimodal agents. Code and data are available at\nhttps://vs-bench.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653a5b0f7c01c693a16dd184",
      "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
      "fullname": "Zelai Xu",
      "name": "zelaix",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03147",
      "authors": [
        {
          "_id": "683fae55c6b71c5994ccd4fe",
          "user": {
            "_id": "6367a8175bb06007ea099b8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6367a8175bb06007ea099b8f/IjG7HyWyWRlVt_XwRbxRW.jpeg",
            "isPro": false,
            "fullname": "linbin",
            "user": "LanguageBind",
            "type": "user"
          },
          "name": "Bin Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:49.923Z",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd4ff",
          "name": "Zongjian Li",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd500",
          "name": "Xinhua Cheng",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd501",
          "name": "Yuwei Niu",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd502",
          "name": "Yang Ye",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd503",
          "name": "Xianyi He",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd504",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:52.748Z",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd505",
          "name": "Wangbo Yu",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd506",
          "name": "Shaodong Wang",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd507",
          "name": "Yunyang Ge",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd508",
          "name": "Yatian Pang",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd509",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:59:33.000Z",
      "submittedOnDailyAt": "2025-06-04T00:55:35.016Z",
      "title": "UniWorld: 고해상도 세ман틱 엔코더의 통합 시각 이해 및 생성",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "当然，以下是英文文本的韩语翻译：\n\n「물론, 현재의 통합 모델은 시각 언어 이해와 문에서 이미지 생성에 강력한 성능을 제공하지만, 이미지 인식과 작업 태스크에서 사용자가 급히 요구하는 것들에 대해서는 제한이 있습니다. 최근, OpenAI는 시각 언어 모델에서 제공되는 의미적 특징을 활용한 모델인 GPT-4o-Image를 발표하고, 시각 인식과 작업의 광범위한 기능을 실현하여 커뮤니티의 흥미를 계속 끌고 있습니다. 우리의 세부적인 실험에서 GPT-4o-Image의 성능을 확인하다 보면, GPT-4o-Image는 VAE(Variational Autoencoder)이 아니라, 의미적 엔코더에서 추출된 특징을 사용하고 있다는 것을 추측합니다. 이러한 흥미로운 사실로 고무되어, 우리는 강력한 시각 언어 모델과 대비적인 의미 엔코더로부터 제공되는 의미적 특징에 기반한 통합 생성 프레임워크를 제안합니다. 그 결과, BAGEL의 데이터의 1%만 사용하여 강력한 통합 모델을 구축하고, BAGEL을 초과하는 성능을 이미지 편집 벤치마크에서 얻었습니다. UniWorld는 강력한 이미지 이해와 생성 기능을 유지하고 있으며, 여러 이미지 인식 태스크에서도 강력한 성능을 발휘합니다. 우리는 모델의 가중치, 학습과 평가 스크립트, 데이터 세트를 완전히 오픈 소스로 합니다.」",
      "upvotes": 33,
      "discussionId": "683fae56c6b71c5994ccd548",
      "githubRepo": "https://github.com/PKU-YuanGroup/UniWorld-V1",
      "ai_summary": "A unified generative framework called UniWorld uses semantic features from visual-language models for image perception and manipulation, outperforming BAGEL with reduced data.",
      "ai_keywords": [
        "GPT-4o-Image",
        "semantic encoders",
        "VAE",
        "UniWorld",
        "visual-language models",
        "contrastive semantic encoders",
        "image editing benchmarks",
        "image perception tasks"
      ]
    },
    "publishedAt": "2025-06-03T13:59:33.000Z",
    "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation",
    "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03147.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00123",
      "authors": [
        {
          "_id": "683e709a3a4c2c3b2750fc32",
          "name": "Gen Luo",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc33",
          "user": {
            "_id": "6565d7149afd51867e55520b",
            "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
            "isPro": false,
            "fullname": "Ganlin Yang",
            "user": "ganlinyang",
            "type": "user"
          },
          "name": "Ganlin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:03.857Z",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc34",
          "user": {
            "_id": "660691330be1fbe3b9e4c33d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660691330be1fbe3b9e4c33d/TxrDFH_cRu3AlpMC3xmhv.jpeg",
            "isPro": false,
            "fullname": "ZiYang Gong",
            "user": "Cusyoung",
            "type": "user"
          },
          "name": "Ziyang Gong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:00.364Z",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc35",
          "name": "Guanzhou Chen",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc36",
          "user": {
            "_id": "66ab30dfd456f0408b93f27b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ab30dfd456f0408b93f27b/nps4Kni_eOExO5Z92RiiF.jpeg",
            "isPro": false,
            "fullname": "Haonan Duan",
            "user": "robot-haonan",
            "type": "user"
          },
          "name": "Haonan Duan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:03:03.236Z",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc37",
          "name": "Erfei Cui",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc38",
          "name": "Ronglei Tong",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc39",
          "name": "Zhi Hou",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3a",
          "name": "Tianyi Zhang",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3b",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3c",
          "name": "Shenglong Ye",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3d",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3e",
          "name": "Jingbo Wang",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3f",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc40",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc41",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc42",
          "name": "Rongrong Ji",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc43",
          "name": "Xizhou Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T18:00:34.000Z",
      "submittedOnDailyAt": "2025-06-04T03:43:23.019Z",
      "title": "스페이스 내 시각, 听聴, 사고, 제어를 수행하는 다모뎀 대뇌 언어 모델\n\n(注意：此处的“听聴”是根据原文“聴”的直译，但在韩语中通常使用“청취”来表示听觉。如果需要更符合韩语习惯的表达，可以改为“청취”。)",
      "submittedOnDailyBy": {
        "_id": "6565d7149afd51867e55520b",
        "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
        "isPro": false,
        "fullname": "Ganlin Yang",
        "user": "ganlinyang",
        "type": "user"
      },
      "summary": "다모달 대 언어 모델(MLLMs)의 상당한 진보는腿式 로봇과 같은 물리적实体에 확장될 수 있는 것이 더욱 주목을 받았다. 이는 MLLMs가 시각 공간 추론과 물리적 상호작용 능력까지 포함해야 하는 다모달 이해 능력을 갖추어야 하는 데에 부릅니다. 그러나 이러한 능력의 근본적인 차이로 인해, 기존 방법은 이러한 능력들을 통합하는 데 어려움을 겪고 있습니다. 본 논문에서는 \"시각적 몸체인 대뇌\"(VeBrain)을 제안하여 현실 세계의 감각, 추론 및 제어를 통합한 통일된 프레임워크를 제시합니다. VeBrain은 로봇 제어를 2D 시각 공간의 일반 텍스트 기반 MLLM 작업으로 재구성하여 다양한 작업의 목표와 매핑 공간을 통일합니다. 그리고 MLLM의 텍스트 제어 신호를 실제 로봇의 운동 전략으로 변환하는 새로운 로봇 적응기를 제안합니다. 데이터 측면에서, 우리는 VeBrain의 다양한 능력을 포함하는 고품질 명령 데이터셋인 VeBrain-600k를 도입합니다. VeBrain-600k에서 우리는 수백 시간 동안 데이터를 수집, 선택 및 레이블링하며, 다모달 사고 체인(CoT)을 사용하여 다양한 능력을 단일 대화에 섞어줍니다. 13개의 다모달 기준 테스트와 5개의 공간 지능 기준 테스트를 통해 광범위한 실험을 통해, VeBrain은 기존의 MLLMs(예: Qwen2.5-VL)에 비해 뛰어난 성능을 보여주었습니다. 다리형 로봇과 기계 팔을 활용할 때, VeBrain은 기존 방법과 비교하여 강력한 적응성, 유연성과 조합력을 보여주었습니다. 예를 들어, Qwen2.5-VL과 비교했을 때, VeBrain은 MMVet에서 상당한 향상을 달성하였으며, 다리형 로봇 작업에서 평균 50%의 향상을 보였습니다.",
      "upvotes": 23,
      "discussionId": "683e70a13a4c2c3b2750fd76",
      "projectPage": "https://internvl.github.io/blog/2025-05-26-VeBrain/",
      "githubRepo": "https://github.com/OpenGVLab/VeBrain",
      "ai_summary": "VeBrain is a unified framework that integrates multimodal understanding, visual-spatial reasoning, and physical interaction for legged robots, demonstrating superior performance compared to existing methods across various benchmarks.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "VeBrain",
        "Visual Embodied Brain",
        "text-based MLLM tasks",
        "robotic adapter",
        "VeBrain-600k",
        "multimodal chain-of-thought",
        "MMVet",
        "compositional capabilities"
      ]
    },
    "publishedAt": "2025-05-30T14:00:34.000Z",
    "title": "Visual Embodied Brain: Let Multimodal Large Language Models See, Think,\n  and Control in Spaces",
    "summary": "The remarkable progress of Multimodal Large Language Models (MLLMs) has\nattracted increasing attention to extend them to physical entities like legged\nrobot. This typically requires MLLMs to not only grasp multimodal understanding\nabilities, but also integrate visual-spatial reasoning and physical interaction\ncapabilities. Nevertheless,existing methods struggle to unify these\ncapabilities due to their fundamental differences.In this paper, we present the\nVisual Embodied Brain (VeBrain), a unified framework for perception, reasoning,\nand control in real world. VeBrain reformulates robotic control into common\ntext-based MLLM tasks in the 2D visual space, thus unifying the objectives and\nmapping spaces of different tasks. Then, a novel robotic adapter is proposed to\nconvert textual control signals from MLLMs to motion policies of real robots.\nFrom the data perspective, we further introduce VeBrain-600k, a high-quality\ninstruction dataset encompassing various capabilities of VeBrain. In\nVeBrain-600k, we take hundreds of hours to collect, curate and annotate the\ndata, and adopt multimodal chain-of-thought(CoT) to mix the different\ncapabilities into a single conversation. Extensive experiments on 13 multimodal\nbenchmarks and 5 spatial intelligence benchmarks demonstrate the superior\nperformance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to\nlegged robots and robotic arms, VeBrain shows strong adaptability, flexibility,\nand compositional capabilities compared to existing methods. For example,\ncompared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by\n+5.6%, but also excels in legged robot tasks with +50% average gains.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00123.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6565d7149afd51867e55520b",
      "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
      "fullname": "Ganlin Yang",
      "name": "ganlinyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03135",
      "authors": [
        {
          "_id": "683fe55868402c738a8e5ee4",
          "name": "Mengdi Jia",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee5",
          "user": {
            "_id": "63c3e8abc7d7f4c63a515a02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
            "isPro": false,
            "fullname": "Zekun Qi",
            "user": "qizekun",
            "type": "user"
          },
          "name": "Zekun Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:22.042Z",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee6",
          "name": "Shaochen Zhang",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee7",
          "name": "Wenyao Zhang",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee8",
          "name": "Xinqiang Yu",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee9",
          "name": "Jiawei He",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5eea",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5eeb",
          "name": "Li Yi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63c3e8abc7d7f4c63a515a02/ZZsfC9We_mrTAc2s-h5ng.mp4"
      ],
      "publishedAt": "2025-06-03T17:58:29.000Z",
      "submittedOnDailyAt": "2025-06-04T05:47:51.969Z",
      "title": "OmniSpatial: 완전한 공간 논리 벤치마크를 위한 광범위한 공간 논리\n\n(注意：此处的翻译假设“幅広い空間論理”指的是“광범위한 공간 논리”，这是根据上下文进行的合理推断。如果“幅広い空間論理”有其他特定含义，请提供更多上下文以便进行更准确的翻译。)",
      "submittedOnDailyBy": {
        "_id": "63c3e8abc7d7f4c63a515a02",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
        "isPro": false,
        "fullname": "Zekun Qi",
        "user": "qizekun",
        "type": "user"
      },
      "summary": "공간 추론은 인지심리학의 중요한 분야 중 하나이며, 현재의 시각 언어 모델(VLMs)의 주요 균형을 이루고 있습니다. 과거의 연구는 기본적인 공간 관계의 이해를 평가하거나 개선하기 위해 주로 사용되었습니다. 그러나 이러한 작업은 공간 추론의 가장 기본적인 수준만 다루고 있습니다. 본 논문에서는 인지심리학에 기반한 상세한 공간 추론 평가 기준인 OmniSpatial을 소개합니다. OmniSpatial은 동적인 추론, 복잡한 공간 로직, 공간적인 상호작용, 그리고 점의 검토의 4가지 주요 카테고리를 포함하며, 50가지의 세부 카테고리를 가지고 있습니다. 인터넷 데이터의 크로닝과 엄정한 손으로 작성된 注釈를 통해 1,500개 이상의 질문・답변 쌍을 구축했습니다. 광범위한 실험을 통해 개방형 및 클로즈드 소스의 VLMs, 그리고 현재의 논리론과 공간 이해 모델은 상세한 공간 이해의 결함이 밝혀졌습니다. 또한 실패 사례를 분석하고 향후 연구의 잠재적인 방향을 제안했습니다.",
      "upvotes": 22,
      "discussionId": "683fe55c68402c738a8e5ff4",
      "ai_summary": "A comprehensive benchmark called OmniSpatial evaluates vision-language models' understanding of advanced spatial reasoning tasks, revealing significant limitations across various models.",
      "ai_keywords": [
        "vision-language models",
        "spatial reasoning",
        "cognitive psychology",
        "dynamic reasoning",
        "complex spatial logic",
        "spatial interaction",
        "perspective-taking",
        "question-answer pairs"
      ]
    },
    "publishedAt": "2025-06-03T13:58:29.000Z",
    "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for\n  Vision Language Models",
    "summary": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63c3e8abc7d7f4c63a515a02/ZZsfC9We_mrTAc2s-h5ng.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03135.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c3e8abc7d7f4c63a515a02",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
      "fullname": "Zekun Qi",
      "name": "qizekun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01674",
      "authors": [
        {
          "_id": "683faa6515abeae85e13336b",
          "name": "Yipeng Du",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336c",
          "name": "Tiehan Fan",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336d",
          "name": "Kepan Nan",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336e",
          "name": "Rui Xie",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336f",
          "name": "Penghao Zhou",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133370",
          "name": "Xiang Li",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133371",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133372",
          "name": "Zhenheng Yang",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133373",
          "user": {
            "_id": "65734004769f3ee9bde1af10",
            "avatarUrl": "/avatars/d6310ed861972fd691687d8f47413f33.svg",
            "isPro": false,
            "fullname": "Ying Tai",
            "user": "yingtai",
            "type": "user"
          },
          "name": "Ying Tai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:59.253Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T13:44:56.000Z",
      "submittedOnDailyAt": "2025-06-04T00:38:03.935Z",
      "title": "MotionSight: 다양한 모드에서 미세한 움직임을 이해하는 능력을 향상시키기",
      "submittedOnDailyBy": {
        "_id": "65927f3b754092f6b1e187a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65927f3b754092f6b1e187a7/gUrNvIQHmsl1vLwSUxpmL.jpeg",
        "isPro": false,
        "fullname": "tiehan fan",
        "user": "AnonMegumi",
        "type": "user"
      },
      "summary": "진보된 Multimodal Large Language Models (MLLMs)에도 불구하고, 미세한 운동을 이해하는 데 제한되어 있습니다. 그들은 일반적으로 시간계열 차분을 부족하며, 사진 간 시각적인 카운터가 평균화되거나 무시되는 경우가 많습니다. 또한, 고정 이미지에서의 시각적 프롬프트는 잠재적인 가능성을 보여주었지만, 이를 시간계열의 복잡성을 위해 동영상에 적용하는 것은 미세한 운동을 이해하는 데는 아직 크게 탐색되지 않았습니다. 우리는 MLLMs의 운동 인식을 향상시키고, 물체의 움직임과 카메라의 움직임을 카운터로 구분하는 데 필요한 특수한 시각적 신호를 생성할 수 있는지 조사하고 있습니다. 본 연구에서는, 새로운 zero-shot 방법인 \"MotionSight\"을 소개하고, 물체 중심의 시각적 스플래이트와 운동의 브레이를 시각적 프롬프트로, 학습을 필요로 하지 않고 미세한 운동을 효과적으로 이해하도록 시도하고 있습니다. 이를 유익한 데이터셋으로 변환하기 위해, 우리는 처음으로 큰 규모의 데이터셋인 MotionVid-QA를 만들었습니다. 이 데이터셋은 SFT와 선호 데이터를 포함하는 수리적 어노테이션을 가지고 있으며, 약 40K의 비디오 클롭과 87K의 QA를 구성하고 있습니다. 실험 결과는, MotionSight가 가장 선진한 오픈 소스 성능을 달성하고, 상업 모델과의 경쟁력을 보여주고 있음을 보여줍니다. 특히, 미세한 운동을 이해하는 데는 새로운 zero-shot 기술과 큰 규모의 고품질 데이터셋을 제공하고 있습니다. 모든 코드와 어노테이션은 공개적으로 제공됩니다.",
      "upvotes": 19,
      "discussionId": "683faa6615abeae85e1333c2",
      "projectPage": "https://nju-pcalab.github.io/projects/MotionSight/",
      "githubRepo": "https://github.com/NJU-PCALab/MotionSight",
      "ai_summary": "MotionSight, a zero-shot method using object-centric visual spotlight and motion blur as prompts, enhances fine-grained video motion understanding and achieves state-of-the-art performance on MotionVid-QA, a large-scale dataset with hierarchical annotations.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "fine-grained video motion understanding",
        "inter-frame differencing",
        "visual prompting",
        "temporal complexities",
        "MotionSight",
        "object-centric visual spotlight",
        "motion blur",
        "MotionVid-QA",
        "hierarchical annotations",
        "SFT",
        "preference data",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-06-02T09:44:56.000Z",
    "title": "MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal\n  LLMs",
    "summary": "Despite advancements in Multimodal Large Language Models (MLLMs), their\nproficiency in fine-grained video motion understanding remains critically\nlimited. They often lack inter-frame differencing and tend to average or ignore\nsubtle visual cues. Furthermore, while visual prompting has shown potential in\nstatic images, its application to video's temporal complexities, particularly\nfor fine-grained motion understanding, remains largely unexplored. We\ninvestigate whether inherent capability can be unlocked and boost MLLMs' motion\nperception and enable distinct visual signatures tailored to decouple object\nand camera motion cues. In this study, we introduce MotionSight, a novel\nzero-shot method pioneering object-centric visual spotlight and motion blur as\nvisual prompts to effectively improve fine-grained motion understanding without\ntraining. To convert this into valuable data assets, we curated MotionVid-QA,\nthe first large-scale dataset for fine-grained video motion understanding, with\nhierarchical annotations including SFT and preference data, {\\Theta}(40K) video\nclips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves\nstate-of-the-art open-source performance and competitiveness with commercial\nmodels. In particular, for fine-grained motion understanding we present a novel\nzero-shot technique and a large-scale, high-quality dataset. All the code and\nannotations will be publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65927f3b754092f6b1e187a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65927f3b754092f6b1e187a7/gUrNvIQHmsl1vLwSUxpmL.jpeg",
      "fullname": "tiehan fan",
      "name": "AnonMegumi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03065",
      "authors": [
        {
          "_id": "683fc6241de14546d5e02775",
          "user": {
            "_id": "64c9bac33cfe45b07179568d",
            "avatarUrl": "/avatars/4a8206cdb1770a8cdaae0d0a2b7b59f2.svg",
            "isPro": false,
            "fullname": "Pengtao Chen",
            "user": "PengtaoChen",
            "type": "user"
          },
          "name": "Pengtao Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:45.116Z",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02776",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02777",
          "name": "Maosen Zhao",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02778",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02779",
          "name": "Mingzhu Shen",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e0277a",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:42.268Z",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e0277b",
          "user": {
            "_id": "63417332c5565a4b8d43a0d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
            "isPro": false,
            "fullname": "Gang Yu",
            "user": "skicy",
            "type": "user"
          },
          "name": "Gang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:39.068Z",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e0277c",
          "name": "Tao Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/iewcVFqJgelW9EQTlH_LH.png"
      ],
      "publishedAt": "2025-06-03T16:42:37.000Z",
      "submittedOnDailyAt": "2025-06-04T04:37:13.352Z",
      "title": "Sparse-vDiT: 얕은 Attention의 힘으로 Video Diffusion Transformer를 가속화합니다.",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "DiTs은 영화 생성 분야에서 발전을 달성했지만, 긴 시퀀스 생성의 과제는 2차원 복잡한 어텐션 구조로 제한되어 있으며, 큰 추론 시간과 함께 있습니다. 영화 디퓨저 트랜스포머 (vDiT)의 어텐션 맵을 분석하여 3가지의 재현 가능한 희소성 패턴을 특정했습니다: 대각선, 다대각선, 수직 스트라이프 구조. 이러한 패턴은 강한 층의 깊이와 헤드의 위치 관계에 따라, 입력 콘텐츠에 대한 의존 관계가 제한되어 있습니다. 이러한 발견을 활용하여 희소성 가속 프레임워크인 Sparse-vDiT를 제안했습니다. Sparse-vDiT는 1) 패턴 최적화된 희소 커널을 각 특정한 희소성 패턴에 대해 계산적으로 효율적인 구현으로 밀집 어텐션을 대체합니다. 2) 하드웨어에 의한 비용 모델링을 통해 각 층과 헤드에 대해 최적의 희소 계산 전략을 선택하는 오프라인 희소 디퓨저 탐색 알고리즘을 포함합니다. 최적의 설정이 결정된 후, 같은 층 내에서 같은 어텐션 전략을 공유하는 헤드를 통합하여 추론 효율을 향상시킵니다. 최신의 vDiT 모델 (CogVideoX1.5, HunyuanVideo, Wan2.1)에 통합하여 Sparse-vDiT는 이론적인 FLOP 감소율 2.09배, 2.38배, 1.67배, 실제 추론 속도 향상 1.76배, 1.85배, 1.58배, 높은 시각적 품질을 유지하며 PSNR 값 24.13, 27.09, 22.59를 달성합니다. 우리의 연구는 vDiT의 잠재적인 구조적 희소성이 긴 영화 합성에서 체계적으로 활용될 수 있음을 보여줍니다.",
      "upvotes": 18,
      "discussionId": "683fc62b1de14546d5e02931",
      "githubRepo": "https://github.com/Peyton-Chen/Sparse-vDiT",
      "ai_summary": "Sparse-vDiT accelerates Video Diffusion Transformer (vDiT) by leveraging sparsity patterns in attention maps, reducing theoretical FLOPs and improving inference speed without significant loss in visual quality.",
      "ai_keywords": [
        "Diffusion Transformers",
        "DiTs",
        "Video Diffusion Transformer",
        "vDiT",
        "sparsity patterns",
        "diagonal",
        "multi-diagonal",
        "vertical-stripe structures",
        "sparse kernels",
        "sparse diffusion search algorithm",
        "FLOP reduction",
        "inference speedups",
        "PSNR",
        "latent structural sparsity"
      ]
    },
    "publishedAt": "2025-06-03T12:42:37.000Z",
    "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers",
    "summary": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09times, 2.38times, and 1.67times theoretical\nFLOP reduction, and actual inference speedups of 1.76times, 1.85times,\nand 1.58times, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/iewcVFqJgelW9EQTlH_LH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03065.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02096",
      "authors": [
        {
          "_id": "683fa36f7ed99d0040761114",
          "user": {
            "_id": "626d268d5f7327906f05cad1",
            "avatarUrl": "/avatars/18bda74612a3ee63a17f991bcc695106.svg",
            "isPro": true,
            "fullname": "Zijian Wu",
            "user": "Jakumetsu",
            "type": "user"
          },
          "name": "Zijian Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:57:09.765Z",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761115",
          "name": "Jinjie Ni",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761116",
          "name": "Xiangyan Liu",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761117",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761118",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761119",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:45:16.000Z",
      "submittedOnDailyAt": "2025-06-04T00:44:38.612Z",
      "title": "SynthRL: 데이터의 합성에 의한 증명 가능한 시각적 추론의 확장",
      "submittedOnDailyBy": {
        "_id": "6486b09e8315b19342f0bf5e",
        "avatarUrl": "/avatars/bc5f22f231c884146d373fe1042d81bd.svg",
        "isPro": false,
        "fullname": "Xiangyan Liu",
        "user": "xyliu6",
        "type": "user"
      },
      "summary": "VISION-LANGUAGE MODELS (VLMs) ARE CONSTRUCTED THROUGH REINFORCEMENT LEARNING WITH VERIFIABLE REWARDS (RLVR) AND HAVE SHOWN SIGNIFICANT ADVANCES IN EFFICIENT SCALING DURING TEST TIME. THIS PAPER INVESTIGATES HOW SYNTHESIZED RL DATA CAN FURTHER IMPROVE RLVR. TO THIS END, A SCALABLE AND GUARANTEED PIPELINE FOR AUTOMATIC DATA SCALING IN LOGICAL RL TRAINING IS PROPOSED. SynthRL IS COMPOSED OF THREE KEY STAGES: (1) SELECTION OF SEED QUESTIONS WITH APPROPRIATE DISTRIBUTIONS, (2) ENHANCING THEM TO MORE CHALLENGING VARIANTS WHILE PRESERVING THE ORIGINAL ANSWERS, AND (3) A GUARANTEED VERIFICATION STAGE TO ENSURE NEAR-PERFECT ACCURACY AND INCREASED DIFFICULTY. EXPERIMENTAL RESULTS DEMONSTRATE THE SCALABILITY AND EFFECTIVENESS OF SynthRL. WHEN APPLIED TO THE MMK12 DATASET, SynthRL SYNTHESIZES ABOUT 3.3K ADDITIONAL VERIFIABLE HARD QUESTIONS FROM APPROXIMATELY 8K SEED SAMPLES. THE MODEL TRAINED ON SYNTHESIZED DATA CONTINUOUSLY YIELDS IMPROVEMENTS ACROSS FIVE DIFFERENT AREAS OF VISUAL MATHEMATICAL LANGUAGE TEST BENCHMARKS AND SHOWS SIGNIFICANT ADVANCES OVER THE BASELINE MODEL TRAINED ON SEED DATA ALONE. NOTABLY, DETAILED ANALYSIS SHOWS THAT THE GAIN IS SIGNIFICANTLY MORE PRONOUNCED IN THE MOST CHALLENGING EVALUATION SAMPLES AND THAT SynthRL EFFECTIVELY EXTRACTS COMPLEX PATTERNS OF DEEP REASONING.",
      "upvotes": 18,
      "discussionId": "683fa3707ed99d0040761154",
      "ai_summary": "SynthRL, a scalable pipeline for data synthesis in reinforcement learning with verifiable rewards, enhances visual math reasoning VLMs by generating challenging, verifiable questions.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable reward",
        "RLVR",
        "SynthRL",
        "seed questions",
        "data augmentation",
        "verification stage",
        "MMK12 dataset",
        "visual math reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-06-02T13:45:16.000Z",
    "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis",
    "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose SynthRL-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02096.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6486b09e8315b19342f0bf5e",
      "avatarUrl": "/avatars/bc5f22f231c884146d373fe1042d81bd.svg",
      "fullname": "Xiangyan Liu",
      "name": "xyliu6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03143",
      "authors": [
        {
          "_id": "683fc5599363b50c19f17d42",
          "user": {
            "_id": "63ef330b1e695b35aa484e11",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ef330b1e695b35aa484e11/bXwpGy0dl8JXeJwJ--ilr.jpeg",
            "isPro": false,
            "fullname": "Qianhui WU",
            "user": "qianhuiwu",
            "type": "user"
          },
          "name": "Qianhui Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T04:21:31.171Z",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d43",
          "user": {
            "_id": "63340dbbd92c5842ae71d1e9",
            "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
            "isPro": false,
            "fullname": "Kanzhi Cheng",
            "user": "cckevinn",
            "type": "user"
          },
          "name": "Kanzhi Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:48.476Z",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d44",
          "user": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "isPro": false,
            "fullname": "Rui Yang",
            "user": "Ray2333",
            "type": "user"
          },
          "name": "Rui Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:51.070Z",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d45",
          "user": {
            "_id": "654dbac9938fbf1e696be8aa",
            "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
            "isPro": false,
            "fullname": "Chaoyun Zhang",
            "user": "vyokky",
            "type": "user"
          },
          "name": "Chaoyun Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:54.267Z",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d46",
          "name": "Jianwei Yang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d47",
          "name": "Huiqiang Jiang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d48",
          "name": "Jian Mu",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d49",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4a",
          "name": "Bo Qiao",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4b",
          "name": "Reuben Tan",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4c",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4d",
          "name": "Lars Liden",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4e",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4f",
          "name": "Huan Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d50",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d51",
          "name": "Jianbing Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d52",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d53",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:59:08.000Z",
      "submittedOnDailyAt": "2025-06-04T02:44:41.730Z",
      "title": "GUI 액터: 좌표없는 시각화 그룹핑의 GUI 에이전트",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "VLMプローティングGUIアガント의 구축 중 주요한 문제 중 하나는 시각화 젝터, 즉 시각화 콘텐츠와 맥락 계획에 기반하여 적절한 스크린 영역을 탐지하는 것입니다. 현재 많은 연구에서는 이 문제를 텍스트 기반의 좌표 생성 태스크로 취급하지만, 이러한 접근 방식은 다음과 같은 여러 제한을 가지고 있습니다: 공간적 및 문법적 aligment의 약점, 불확실한 서브 젝터 타겟 처리 불가, 스크린 좌표의 밀도와 Vision Transformers 등 모델에서 추출되는 시각적 특징의 거대성으로 인한 불적절한 매칭. 본 논문에서는 좌표 없는 GUI 젝터를 제안합니다. GUI-Actor는 텍스트 기반의 좌표 생성 태스크로 취급되어 있는 제한을 극복하기 위해, 부착 기반의 행동 헤드를 도입하고, 특정 <ACTOR> 토큰을 모든 관련 시각 패치 토큰과 일관시키면 모델이 한 번의 전방 패스를 통해 한 개 이상의 행동 영역을 제안할 수 있게 됩니다. 이를 따라 제안된 행동 영역 중에서 가장 가능한 행동 영역을 평가하여 선택하기 위해, 젝터 검증 기능을 설계합니다. 광범위한 실험은 GUI-Actor는 지난주의 최상위 방법보다 여러 가지 GUI 행동 젝터 벤치마크에서 뛰어넘으며, 새로운 스크린 해상度和 레이아웃에 대한 일반화 성능을 향상시켰습니다. 특히, GUI-Actor-7B는 ScreenSpot-Pro에서 UI-TARS-72B를 초월하며, Qwen2-VL과 Qwen2.5-VL을 백드롭으로 사용하여 40.7과 44.6의 점수를 달성했습니다. 또한 젝터 검증 기능을 도입하여, 기존의 최상위 모델과 동일한 성능을 달성할 수 있는 새로운 행동 헤드의 微調節만 수행하는 VLM 백드롭을 고정함으로써, 이전의 최상위 모델과 동일한 성능을 달성할 수 있음을 발견했습니다. 이는 GUI-Actor는 기본 VLM에 효과적인 젝터 능력을 부여할 수 있으며, 그 일반적인 강점을 잃지 않는 것을 증명합니다.",
      "upvotes": 17,
      "discussionId": "683fc55d9363b50c19f17e6b",
      "projectPage": "https://microsoft.github.io/GUI-Actor/",
      "githubRepo": "https://github.com/microsoft/GUI-Actor",
      "ai_summary": "GUI-Actor, a VLM-based method using attention for coordinate-free GUI grounding, outperforms existing methods with better generalization and efficient fine-tuning.",
      "ai_keywords": [
        "visual grounding",
        "text-based coordinate generation",
        "spatial-semantic alignment",
        "Vision Transformers",
        "attention-based action head",
        "grounding verifier",
        "GUI action grounding benchmarks",
        "GUI-Actor",
        "GUI-Actor-7B",
        "UI-TARS-72B",
        "ScreenSpot-Pro",
        "Qwen2-VL",
        "Qwen2.5-VL",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-06-03T13:59:08.000Z",
    "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
    "summary": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03131",
      "authors": [
        {
          "_id": "683fb2786a2b978ca4e62493",
          "user": {
            "_id": "64b7aa374df206a3ed1947d2",
            "avatarUrl": "/avatars/a7c7e703ccf8824259fc5a8a90a25746.svg",
            "isPro": false,
            "fullname": "wzd",
            "user": "GoodEnough",
            "type": "user"
          },
          "name": "Zidong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:13.762Z",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62494",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62495",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62496",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62497",
          "name": "Yiyuan Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63176933b58b0184630d2c74/VS2G-SBuSY5ltQmCLAaq3.png"
      ],
      "publishedAt": "2025-06-03T17:57:33.000Z",
      "submittedOnDailyAt": "2025-06-04T01:13:44.155Z",
      "title": "네비트 레지젝션 이미지 합성",
      "submittedOnDailyBy": {
        "_id": "63176933b58b0184630d2c74",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
        "isPro": false,
        "fullname": "Yiyuan Zhang",
        "user": "Yiyuan",
        "type": "user"
      },
      "summary": "노트오랜스 이미지 합성을 소개합니다. 이것은 새로운 생성 모델링 패러다임이며, 임의의 해상도와 가로비율을 사용하여 이미지 합성이 가능합니다. 이 접근법은 단일 고정 해상도, 정사각형 이미지의 방법의 제한을 극복하고, 변동 길이의 시각 토큰을 원生地적으로 처리합니다. 이로 인해 전통적인 방법의 주요 문제점을 해결합니다. 이에 따라, Native-resolution diffusion Transformer (NiT)를 도입합니다. 이 아키텍처는 노이즈 처리 과정에서 명시적으로 변하는 해상도와 가로비를 모델링하는 것을 목표로 합니다. 고정 포맷의 제약으로부터 개방되어, NiT는 다양한 해상도와 가로비율을 포함하는 이미지에서 고유의 시각 분포를 학습합니다. 특히, 하나의 NiT 모델은 동시에 ImageNet-256x256과 512x512 벤치마크에서 가장 先端의 성능을 달성합니다. 놀라울 정도로, 先端의 대규모 언어 모델의 강력한 0-shot 능력과 같은, ImageNet에서만 학습된 NiT는 0-shot 확장 성능을 나타냅니다. 이는 이전에 보지 않은 고해상도 (예: 1536 x 1536)과 다양한 가로비율 (예: 16:9, 3:1, 4:3)에서 고품질 이미지의 생성을 성공적으로 수행합니다. 이러한 발견은 노트오랜스 이미지 모델링이 시각 생성 모델링과 先端의 LLM 재료물 사이에서 중요한 잠재력을 보여주고 있습니다.",
      "upvotes": 13,
      "discussionId": "683fb27e6a2b978ca4e625bd",
      "projectPage": "https://wzdthu.github.io/NiT/",
      "githubRepo": "https://github.com/WZDTHU/NiT",
      "ai_summary": "A novel generative model, Native-resolution diffusion Transformer (NiT), synthesizes high-resolution and varied aspect ratio images with state-of-the-art performance and zero-shot generalization capabilities.",
      "ai_keywords": [
        "native-resolution image synthesis",
        "generative modeling paradigm",
        "variable-length visual tokens",
        "diffusion Transformer",
        "denoising process",
        "ImageNet-256x256",
        "ImageNet-512x512",
        "high-fidelity images",
        "aspect ratios",
        "zero-shot generalization"
      ]
    },
    "publishedAt": "2025-06-03T13:57:33.000Z",
    "title": "Native-Resolution Image Synthesis",
    "summary": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63176933b58b0184630d2c74/VS2G-SBuSY5ltQmCLAaq3.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03131.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63176933b58b0184630d2c74",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
      "fullname": "Yiyuan Zhang",
      "name": "Yiyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00070",
      "authors": [
        {
          "_id": "683fd7d79d4fb703271ad9ac",
          "name": "Dongyoung Kim",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9ad",
          "name": "Sumin Park",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9ae",
          "name": "Huiwon Jang",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9af",
          "name": "Jinwoo Shin",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9b0",
          "name": "Jaehyung Kim",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9b1",
          "name": "Younggyo Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:41:12.000Z",
      "submittedOnDailyAt": "2025-06-04T03:53:04.651Z",
      "title": "로봇 R1: 로봇공학의 목적에 대한 강화학습에 의한 기계화 로직의 향상",
      "submittedOnDailyBy": {
        "_id": "658d79663a5202a485a76d9b",
        "avatarUrl": "/avatars/1893384f28a7cb82d4588576c4c264f1.svg",
        "isPro": false,
        "fullname": "dongyoung kim",
        "user": "vangard703",
        "type": "user"
      },
      "summary": "대규모 비전 언어 모델(LVLMs)은 기계화 로봇 제어에 있어 통계적 학습을 활용한 로봇 제어에 관련된 기계화 로직을 조합하여 로봇 기술의 발전에 큰 가능성을 보여주고 있다. 일반적인 접근 방식은 주로 Supervised Fine-Tuning(SFT)을 이용한 기계화 로직 태스크의 훈련을 포함하지만, SFT 데이터 세트는 일반적으로 휴리스틱으로 구축되어 로봇 제어의 향상에 명시적으로 최적화되지 않은 경우가 많습니다. 또한, SFT는 카타스토피 Forget과 일반화 성능의 저하 등 문제들을 동반하는 경우가 많습니다. 이러한 제한을 해결하기 위해 로봇 제어에 특화된 기계화 로직을 강화하기 위한 새로운 프레임워크를 통해 강화 학습을 활용하고 있습니다. 로봇 R1은 전문적인 데모에서 얻을 수 있는 현재의 영상과 환경 메타 데이터에 기반하여 태스크의 완료에 필요한 다음 키 포인트 상태를 예측하는 것을 학습하고 있습니다. DeepSeek-R1의 학습 접근 방식에 의존하여, 로봇 R1은 이유에 기반한 응답을 샘플링하고 더 정확한 예측을 실현하도록 강화합니다. 실험 결과를 통해, 로봇 R1에서 훈련된 모델은 SFT 메소드보다 기계화 로직 태스크에 대한 성능이 뛰어납니다. 7B 파라미터를 보유한 것은 한 가지 제한이지만, 저급 액션 제어에 관련된 이유 태스크에서 GPT-4o를 초월할 수 있습니다.",
      "upvotes": 13,
      "discussionId": "683fd7d79d4fb703271ad9d9",
      "ai_summary": "Robot-R1, a reinforcement learning framework, enhances embodied reasoning for robotics by predicting keypoint states, outperforming supervised fine-tuning methods and even surpassing GPT-4o in low-level action control tasks.",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "embodied reasoning",
        "robot control",
        "Supervised Fine-Tuning (SFT)",
        "catastrophic forgetting",
        "generalization performance",
        "reinforcement learning",
        "keypoint state",
        "scene image",
        "environment metadata",
        "expert demonstrations",
        "DeepSeek-R1",
        "reasoning-based responses",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-05-29T12:41:12.000Z",
    "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in\n  Robotics",
    "summary": "Large Vision-Language Models (LVLMs) have recently shown great promise in\nadvancing robotics by combining embodied reasoning with robot control. A common\napproach involves training on embodied reasoning tasks related to robot control\nusing Supervised Fine-Tuning (SFT). However, SFT datasets are often\nheuristically constructed and not explicitly optimized for improving robot\ncontrol. Furthermore, SFT often leads to issues such as catastrophic forgetting\nand reduced generalization performance. To address these limitations, we\nintroduce Robot-R1, a novel framework that leverages reinforcement learning to\nenhance embodied reasoning specifically for robot control. Robot-R1 learns to\npredict the next keypoint state required for task completion, conditioned on\nthe current scene image and environment metadata derived from expert\ndemonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples\nreasoning-based responses and reinforces those that lead to more accurate\npredictions. Our experiments show that models trained with Robot-R1 outperform\nSFT methods on embodied reasoning tasks. Despite having only 7B parameters,\nRobot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action\ncontrol, such as spatial and primitive movement reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00070.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658d79663a5202a485a76d9b",
      "avatarUrl": "/avatars/1893384f28a7cb82d4588576c4c264f1.svg",
      "fullname": "dongyoung kim",
      "name": "vangard703",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03136",
      "authors": [
        {
          "_id": "683fa2ddbde0ae60c2f16183",
          "name": "Yinjie Wang",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16184",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16185",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16186",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16187",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:58:42.000Z",
      "submittedOnDailyAt": "2025-06-04T00:39:07.151Z",
      "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning\n\n이 문장은 \"RL 기반의 LLM Coder와 Unit Tester의 동시 진화\"로 번역될 수 있습니다. 이 논문의 주제는 강화 학습(Reinforcement Learning)을 사용하여 코드러(Coder)와 단위 테스트러(Unit Tester)를 동시 진화시켜 효율적인 소프트웨어 개발을 지원하는 기술입니다. 이 기술은 코드러가 코드를 생성하고 단위 테스트러가 생성된 코드를 테스트하는 과정을 반복하며, 두 모델이 상호작용하여 성능을 향상시키는데 사용됩니다. 이 방법론은 소프트웨어 개발 과정에서 코드의 품질과 효율성을 향상시키고, 개발 속도를 향상시킬 수 있습니다.",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "우리는 CURE라는 새로운 강화학습 프레임워크를 제안합니다. 이 프레임워크는 코드와 단위 테스트 생성 능력을 상호작용 결과에 기반하여 동시 진화시키는 데专用 보상 설계를 사용하며, 실제 코드를 통해 지도 받지 않습니다. 이러한 접근 방식은 유연하고 확장 가능한 훈련을 가능하게 하며, 단위 테스트러가 코드러의 실수를 직접 학습할 수 있게 합니다. Qwen2.5-Instruct 모델을 기반으로 최적화한 후, 우리가 개발한 ReasonFlux-Coder-7B와 14B 모델은 코드 생성 정확도를 5.3% 증가시키고, Best-of-N 정확도를 9.0% 증가시켰습니다. 이와 같은 크기의 Qwen-Coder, DeepSeek-Coder, Seed-Coder를 초과합니다. 이들은 테스트 시간 조정 및 에이전트 코드 등 하류 작업을 자연스럽게 확장할 수 있으며, 기본 모델보다 8.1% 향상을 달성합니다. 또한, LONG-COT 모델을 위한 ReasonFlux-Coder-4B는 단위 테스트 생성에서 64.8%의 추론 효율성을 달성하면서 Qwen3-4B를 초과합니다. 특히, 우리의 모델은 기본 모델의 강화 학습에서 효과적인 보상 모델로 사용될 수 있다는 점을 주목합니다. 프로젝트: https://github.com/Gen-Verse/CURE",
      "upvotes": 12,
      "discussionId": "683fa2debde0ae60c2f161cb",
      "projectPage": "https://huggingface.co/collections/Gen-Verse/reasonflux-coder-6833109ed9300c62deb32c6b",
      "githubRepo": "https://github.com/Gen-Verse/CURE",
      "ai_summary": "CURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "reward design",
        "coding",
        "unit test generation",
        "ReasonFlux-Coder",
        "Qwen2.5-Instruct",
        "Qwen-Coder",
        "DeepSeek-Coder",
        "Seed-Coder",
        "test-time scaling",
        "agentic coding",
        "long-CoT",
        "inference efficiency",
        "reward model"
      ]
    },
    "publishedAt": "2025-06-03T13:58:42.000Z",
    "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
    "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03136.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23061",
      "authors": [
        {
          "_id": "683fc4028de3ffc5838c3fa8",
          "name": "Tarun Suresh",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3fa9",
          "name": "Debangshu Banerjee",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3faa",
          "name": "Shubham Ugare",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3fab",
          "name": "Sasa Misailovic",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3fac",
          "name": "Gagandeep Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T04:04:54.000Z",
      "submittedOnDailyAt": "2025-06-04T02:27:09.092Z",
      "title": "DINGO: ディフェンションLLM의 제약된 추론",
      "submittedOnDailyBy": {
        "_id": "65e7bb35e5e78134ab049942",
        "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
        "isPro": false,
        "fullname": "Tarun Suresh",
        "user": "tarsur909",
        "type": "user"
      },
      "summary": "Diffusion LLMs는 전통적인 autoregressive LLMs와 비교하여 실행시간 효율화에 큰 가능성을 보여주고 있습니다. 그러나 현재의 Diffusion 모델은 정규 표현식과 같은 사용자가 지정한 공식적인 제약을 증명적으로 강제할 수 없기 때문에 고정된 스키마의 JSON이나 같은 구조화된 출력을 필요로 하는 작업에 신뢰도가 낮은 문제점이 있습니다. autoregressive 모델과 달리 Diffusion LLMs는 전체적으로 토큰 블록을 예측합니다. 이 병렬화는 토큰 순서 예측에 설계된 전통적인 제약付き 디코딩 알고리즘이 실제 출력 분포를 유지할 수 없게 됩니다. 이러한 제한을 해결하기 위해, DINGO라는 효율적이고 동시에 분포를 유지할 수 있는 증명적인 Dynamic Programming에 기반한 제약付き 디코딩 전략을 제안하고 있습니다. DINGO는 모델의 예측 분포의 가장 높은 확률로 출력 문자열을 샘플링할 수 있게 하며, 사용자가 지정한 정규 표현식을 엄격히 만족시킬 수 있습니다. 표준의 기호적인 수학 및 JSON 생성 벤치마크에서, DINGO는 무제한 추론보다 최대 68%의 점의 개선을 달성합니다.",
      "upvotes": 12,
      "discussionId": "683fc4038de3ffc5838c3fd8",
      "ai_summary": "DINGO, a dynamic programming-based decoding strategy, enhances diffusion language models by enforcing structured output constraints, significantly improving performance on symbolic math and JSON generation tasks.",
      "ai_keywords": [
        "diffusion LLMs",
        "autoregressive LLMs",
        "formal constraints",
        "regular expressions",
        "sequential token prediction",
        "parallel token prediction",
        "dynamic programming",
        "constrained decoding",
        "output distribution",
        "symbolic math generation",
        "JSON generation"
      ]
    },
    "publishedAt": "2025-05-29T00:04:54.000Z",
    "title": "DINGO: Constrained Inference for Diffusion LLMs",
    "summary": "Diffusion LLMs have emerged as a promising alternative to conventional\nautoregressive LLMs, offering significant potential for improved runtime\nefficiency. However, existing diffusion models lack the ability to provably\nenforce user-specified formal constraints, such as regular expressions, which\nmakes them unreliable for tasks that require structured outputs, such as\nfixed-schema JSON generation. Unlike autoregressive models that generate tokens\nsequentially, diffusion LLMs predict a block of tokens in parallel. This\nparallelism makes traditional constrained decoding algorithms, which are\ndesigned for sequential token prediction, ineffective at preserving the true\noutput distribution. To address this limitation, we propose DINGO, a dynamic\nprogramming-based constrained decoding strategy that is both efficient and\nprovably distribution-preserving. DINGO enables sampling of output strings with\nthe highest probability under the model's predicted distribution, while\nstrictly satisfying any user-specified regular expression. On standard symbolic\nmath and JSON generation benchmarks, DINGO achieves up to a 68 percentage point\nimprovement over unconstrained inference",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23061.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e7bb35e5e78134ab049942",
      "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
      "fullname": "Tarun Suresh",
      "name": "tarsur909",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02528",
      "authors": [
        {
          "_id": "683fb8bc3bcb592f18f5b866",
          "name": "Yan Gong",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b867",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b868",
          "name": "Yicheng Li",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b869",
          "name": "Chenglin Li",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b86a",
          "name": "Yin Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T07:06:35.000Z",
      "submittedOnDailyAt": "2025-06-04T01:39:07.675Z",
      "title": "RelationAdapter: Difyujon Transfomaer를 사용한 시각적 관계 학습 및 전달",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)의 학습 구조를 참고하여, 일반화된 이미지 편집의 새로운 패러다임이 등장하고 있습니다. 현재의 단일 레퍼런스 방법은 일반적으로 스타일이나 외모에 초점을 맞추고, 고정형 변환에 어려움을 겪는 경우가 많습니다. 이러한 제한을 해결하기 위해, 소스 이미지와 타겟 이미지의 쌍을 활용하여, 새로운 커리 이미지에 관련한 콘텐츠 편집 의도를 추출하고 전달하는 방법을 제안합니다. 이를 위해, RelationAdapter라는 가벼운 모듈을 도입하여, Diffusion Transformer(DiT) 기반 모델로 최소한의 예에서도 시각화 변환을 감지하고 적용할 수 있도록 합니다. 또한, Relation252K라는, 218종의 편집 태스크를 포함하는 세부적인 데이터 세트를 도입하여, 모델의 일반화能力和 적용성을 평가하는 것을 목표로 합니다. Relation252K에서 수행된 실험은 RelationAdapter가 편집 의도의 이해와 전달 능력에 대해 큰 향상을示し, 전체적으로 생성 품질과 편집 성능에 대해 눈에 띄는 효과를 나타내어 있습니다.",
      "upvotes": 11,
      "discussionId": "683fb8bd3bcb592f18f5b89f",
      "ai_summary": "RelationAdapter, a lightweight module, enhances Diffusion Transformer models to capture and apply visual transformations effectively using source-target image pairs, improving editing performance and generalization on diverse tasks.",
      "ai_keywords": [
        "RelationAdapter",
        "Diffusion Transformer",
        "DiT",
        "visual prompt-based image editing",
        "content-aware editing intent",
        "Relation252K",
        "visual transformations",
        "editing intent",
        "generation quality",
        "overall editing performance"
      ]
    },
    "publishedAt": "2025-06-03T03:06:35.000Z",
    "title": "RelationAdapter: Learning and Transferring Visual Relation with\n  Diffusion Transformers",
    "summary": "Inspired by the in-context learning mechanism of large language models\n(LLMs), a new paradigm of generalizable visual prompt-based image editing is\nemerging. Existing single-reference methods typically focus on style or\nappearance adjustments and struggle with non-rigid transformations. To address\nthese limitations, we propose leveraging source-target image pairs to extract\nand transfer content-aware editing intent to novel query images. To this end,\nwe introduce RelationAdapter, a lightweight module that enables Diffusion\nTransformer (DiT) based models to effectively capture and apply visual\ntransformations from minimal examples. We also introduce Relation252K, a\ncomprehensive dataset comprising 218 diverse editing tasks, to evaluate model\ngeneralization and adaptability in visual prompt-driven scenarios. Experiments\non Relation252K show that RelationAdapter significantly improves the model's\nability to understand and transfer editing intent, leading to notable gains in\ngeneration quality and overall editing performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02528.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24714",
      "authors": [
        {
          "_id": "683d04c751706d12b2c262ea",
          "user": {
            "_id": "642da1cd99f3110ac27caca5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
            "isPro": false,
            "fullname": "junyu",
            "user": "luojunyu",
            "type": "user"
          },
          "name": "Junyu Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:03:26.483Z",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262eb",
          "user": {
            "_id": "65a9c8652bf3e0cbbfcab2c8",
            "avatarUrl": "/avatars/fc690a78b5f2e94e08a40059ae40625c.svg",
            "isPro": false,
            "fullname": "Alan KOU",
            "user": "alan1027",
            "type": "user"
          },
          "name": "Zhizhuo Kou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:03:22.133Z",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ec",
          "user": {
            "_id": "6434c115a5aed21dd11981c5",
            "avatarUrl": "/avatars/d51e6e384cfc3affe578e7816bcebb35.svg",
            "isPro": false,
            "fullname": "Yang Liming",
            "user": "chunfenri",
            "type": "user"
          },
          "name": "Liming Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:03:24.346Z",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ed",
          "name": "Xiao Luo",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ee",
          "name": "Jinsheng Huang",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ef",
          "name": "Zhiping Xiao",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f0",
          "name": "Jingshu Peng",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f1",
          "name": "Chengzhong Liu",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f2",
          "name": "Jiaming Ji",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f3",
          "name": "Xuanzhe Liu",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f4",
          "name": "Sirui Han",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f5",
          "name": "Ming Zhang",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f6",
          "name": "Yike Guo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/ilkQ2K5LD4SPQ5oCcV2aB.png"
      ],
      "publishedAt": "2025-05-30T15:36:19.000Z",
      "submittedOnDailyAt": "2025-06-04T01:04:36.480Z",
      "title": "FinMME: 재무 모델 데이터 세트의 평가용 벤치마크 데이터 세트",
      "submittedOnDailyBy": {
        "_id": "642da1cd99f3110ac27caca5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
        "isPro": false,
        "fullname": "junyu",
        "user": "luojunyu",
        "type": "user"
      },
      "summary": "다모둠 대언어 모둠(MLLMs)은 최근 급격히 발전하고 있습니다. 그러나 재무 분야에서 효과적이고 전문적인 다모둠 평가 데이터셋이 부족합니다. 재무 분야의 MLLM 개발을 촉진하기 위해, 우리는 FinMME를 소개합니다. FinMME는 18가지의 재무 분야와 6가지의 자산 클래스를 확장하고, 10가지의 주요 차트 타입과 21가지의 서브 타입을 다루며, 11,000개 이상의 고품질 재무 연구 샘플을 포함합니다. 데이터 품질을 보장하기 위해, 20명의 어노테이터와 신중하게 설계된 검증 구조를 사용합니다. 또한 FinScore를 개발했습니다. FinScore는 훼셈 페널티와 다차원 능력 평가를 도입하여 편향적인 평가를 제공합니다. 확장된 실험 결과를 통해, 최신 모델인 GPT-4o도 FinMME에서 불만족스러운 성능을 나타내며, 그 어려움을 드러냅니다. 벤치마크는 다른 프로ン퓰트에 따른 예측 변이가 1% 이하임을 보여주며, 현재 데이터셋과 비교하여 높은 신뢰성을 나타냅니다. 우리의 데이터셋과 평가 프로토콜은 https://huggingface.co/datasets/luojunyu/FinMME 와 https://github.com/luo-junyu/FinMME 에서 사용 가능합니다.",
      "upvotes": 11,
      "discussionId": "683d04c951706d12b2c26367",
      "projectPage": "https://huggingface.co/datasets/luojunyu/FinMME",
      "githubRepo": "https://github.com/luo-junyu/FinMME",
      "ai_summary": "FinMME is a comprehensive multimodal dataset for financial research and FinScore is an evaluation system that highlights the challenges faced by even advanced models like GPT-4o in the finance domain.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "FinMME",
        "financial research samples",
        "high-quality dataset",
        "financial domains",
        "asset classes",
        "chart types",
        "data quality",
        "FinScore",
        "hallucination penalties",
        "multi-dimensional capability assessment",
        "benchmark dataset",
        "prediction robustness"
      ]
    },
    "publishedAt": "2025-05-30T11:36:19.000Z",
    "title": "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation",
    "summary": "Multimodal Large Language Models (MLLMs) have experienced rapid development\nin recent years. However, in the financial domain, there is a notable lack of\neffective and specialized multimodal evaluation datasets. To advance the\ndevelopment of MLLMs in the finance domain, we introduce FinMME, encompassing\nmore than 11,000 high-quality financial research samples across 18 financial\ndomains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We\nensure data quality through 20 annotators and carefully designed validation\nmechanisms. Additionally, we develop FinScore, an evaluation system\nincorporating hallucination penalties and multi-dimensional capability\nassessment to provide an unbiased evaluation. Extensive experimental results\ndemonstrate that even state-of-the-art models like GPT-4o exhibit\nunsatisfactory performance on FinMME, highlighting its challenging nature. The\nbenchmark exhibits high robustness with prediction variations under different\nprompts remaining below 1%, demonstrating superior reliability compared to\nexisting datasets. Our dataset and evaluation protocol are available at\nhttps://huggingface.co/datasets/luojunyu/FinMME and\nhttps://github.com/luo-junyu/FinMME.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/ilkQ2K5LD4SPQ5oCcV2aB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24714.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "642da1cd99f3110ac27caca5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
      "fullname": "junyu",
      "name": "luojunyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03126",
      "authors": [
        {
          "_id": "683fb3719f37285365b080c9",
          "user": {
            "_id": "672a037c19f1f942483f680c",
            "avatarUrl": "/avatars/a48464044e9eb11a2bc062be05d9aa9a.svg",
            "isPro": false,
            "fullname": "qiulu",
            "user": "qiulu66",
            "type": "user"
          },
          "name": "Lu Qiu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:04.988Z",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080ca",
          "user": {
            "_id": "630b094f8b327c7b8b94d24c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
            "isPro": false,
            "fullname": "Yizhuo Li",
            "user": "liyz",
            "type": "user"
          },
          "name": "Yizhuo Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:02.456Z",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080cb",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080cc",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080cd",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080ce",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:55:18.000Z",
      "submittedOnDailyAt": "2025-06-04T03:58:52.500Z",
      "title": "アニメシューター: 데이터셋의 비디오 생성\n\nAnimeShooter: 참고 가이드를 안내한 단계별 애니메이션 데이터셋\n\n(Note: The translation is provided as requested, adhering to the format and content of the original text.)",
      "submittedOnDailyBy": {
        "_id": "630b094f8b327c7b8b94d24c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
        "isPro": false,
        "fullname": "Yizhuo Li",
        "user": "liyz",
        "type": "user"
      },
      "summary": "최근 AI 생성 콘텐츠(AIGC)의 발전은 애니메이션의 생산을 크게 가속화했습니다. 흥미로운 애니메이션을 만들려면 노트 리프와 캐릭터 리ン크를 포함하는 코라시한 다샷 비디오 클립을 생성하는 것이 중요합니다. 그러나 현재 공개 데이터 세트는 주로 세계적인 실세계 시나리오를 중점적으로 다루고 있으며, 캐릭터의 일관성을 보장하기 위한 리퍼런스 이미지가 부족합니다. 이를 보완하기 위해, 우리는AnimeShooter, 리퍼런스를 안내하는 다샷 애니메이션 데이터 세트를 소개합니다.AnimeShooter는 자동화 프로세스에서 세부적인 휴리스틱 Annotation과 shot 간 강한 시각적 일관성을 특징으로 합니다. 스토리리 레벨의 Annotation은 노트 리프의 요약을 제공하며, 스토리리 라인, 키 시인, 메인 캐릭터 프로필과 리퍼런스 이미지를 포함합니다. 반면, shot 레벨의 Annotation은 스토리리를 연속된 shot에 분해하고, 시나, 캐릭터, 노트 리프와 설명적인 시각적 캡션을 포함하여 Annotation합니다. 또한, 특별한 서브셋인AnimeShooter-audio는 각 shot에 동기화된 오디오 트랙, 음성 설명과 음원을 제공합니다.AnimeShooter의 효과를 보여주기 위해, 리퍼런스를 안내하는 다샷 비디오 생성 태스크의 기준을确立하기 위해, 우리는AnimeShooterGen을 소개합니다. 이는 다모달 대언어 모델(MLLM)과 비디오 디퓨전 모델을 확장한 것입니다. 리퍼런스 이미지와 이전에 생성된 shot는 MLLM에서 처음으로 처리되며, 리퍼런스와 컨텍스트에 대한 인식을 가지는 표현을 생성합니다. 이러한 표현은 디퓨전 모델의 조건으로 사용되며, 다음 shot를 결정합니다. 실험 결과에 따르면,AnimeShooter에서 훈련된 모델은 shot 간 시각적 일관성과 리퍼런스 시각적 가이드에 따라 동작하며, 우리의 데이터 세트가 코라시한 애니메이션 비디오 생성에 대한 가치를 밝혀줍니다.",
      "upvotes": 10,
      "discussionId": "683fb3749f37285365b08167",
      "projectPage": "https://qiulu66.github.io/animeshooter",
      "githubRepo": "https://github.com/qiulu66/Anime-Shooter",
      "ai_summary": "AnimeShooter, a reference-guided multi-shot animation dataset, enhances coherent animated video generation by incorporating comprehensive hierarchical annotations and visual consistency, and AnimeShooterGen leverages MLLMs and video diffusion models to achieve superior results.",
      "ai_keywords": [
        "AnimeShooter",
        "reference-guided",
        "multimodal large language models (MLLMs)",
        "video diffusion models",
        "hierarchical annotations",
        "visual consistency",
        "cross-shot visual consistency",
        "reference visual guidance"
      ]
    },
    "publishedAt": "2025-06-03T13:55:18.000Z",
    "title": "AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video\n  Generation",
    "summary": "Recent advances in AI-generated content (AIGC) have significantly accelerated\nanimation production. To produce engaging animations, it is essential to\ngenerate coherent multi-shot video clips with narrative scripts and character\nreferences. However, existing public datasets primarily focus on real-world\nscenarios with global descriptions, and lack reference images for consistent\ncharacter guidance. To bridge this gap, we present AnimeShooter, a\nreference-guided multi-shot animation dataset. AnimeShooter features\ncomprehensive hierarchical annotations and strong visual consistency across\nshots through an automated pipeline. Story-level annotations provide an\noverview of the narrative, including the storyline, key scenes, and main\ncharacter profiles with reference images, while shot-level annotations\ndecompose the story into consecutive shots, each annotated with scene,\ncharacters, and both narrative and descriptive visual captions. Additionally, a\ndedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each\nshot, along with audio descriptions and sound sources. To demonstrate the\neffectiveness of AnimeShooter and establish a baseline for the reference-guided\nmulti-shot video generation task, we introduce AnimeShooterGen, which leverages\nMultimodal Large Language Models (MLLMs) and video diffusion models. The\nreference image and previously generated shots are first processed by MLLM to\nproduce representations aware of both reference and context, which are then\nused as the condition for the diffusion model to decode the subsequent shot.\nExperimental results show that the model trained on AnimeShooter achieves\nsuperior cross-shot visual consistency and adherence to reference visual\nguidance, which highlight the value of our dataset for coherent animated video\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630b094f8b327c7b8b94d24c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
      "fullname": "Yizhuo Li",
      "name": "liyz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00910",
      "authors": [
        {
          "_id": "683fcd9956d0f1cfb34f1d51",
          "name": "Seongjae Kang",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d52",
          "name": "Dong Bok Lee",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d53",
          "name": "Hyungjoon Jang",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d54",
          "name": "Dongseop Kim",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d55",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T08:54:37.000Z",
      "submittedOnDailyAt": "2025-06-04T03:10:05.743Z",
      "title": "PCoreSet: 시각 언어 모델에서의 지식 수용에 의한 효과적인 활성 학습",
      "submittedOnDailyBy": {
        "_id": "6357a08f8ed056fa1ccd3b38",
        "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
        "isPro": false,
        "fullname": "erjui",
        "user": "erjui",
        "type": "user"
      },
      "summary": "知識전달(KD)는 교사 모델의 지식을 활용하여 가벼운 모델이거나 특정 작업에 특화된 모델을 훈련시키기 위해 광범위하게 사용되고 있는 프레임워크입니다. 그러나 KD의 응용 분야로서의 능동 학습(AL)에서, 반복적인 샘플 선택을 통해 표기 비용 최소화하는 것을 목표로 하는 것은 아직 조사가 심도가 shallow한 상태입니다. 이 공간은 KD가 일반적으로 충분한 라벨付け 데이터의 접근을 가정하고 있으며, AL은 데이터 부족의 시나리오로, 특정 작업에 특화된 교사 모델이 일반적으로 사용할 수 없는 데서 생기는 공간입니다. 이 논문에서는, 대규모 시각 언어 모델(VLMs)의 0 shot 및 조금 shot 능력을 활용하여, AL과 KD를 통합하는 ActiveKD 프레임워크를 소개합니다. ActiveKD의 중요한 점은 VLMs의 구조화된 예측 편향이며, 이 예측이 확률 공간에서 클러스터로 형성되는 것입니다. 이 구조는 교사 모델의 추론 편향으로 볼 수 있으며, 학생의 학습에 도움이 되는 일반화된 출력 패턴을 이해하는 것입니다. 이 편향을 활용하기 위해, 확률적 CoreSet(PCoreSet)을 제안합니다. PCoreSet은 특징 공간에서 커버를 최대화하는 것이 아니라, 확률 공간에서 커버를 최대화하는 선택 전략으로, 표기 관리 백제트의 한계 내에서도 교사의 지식을 효율적으로 전달하는 것을 촉진합니다. 11개의 데이터 세트를 평가한 결과, PCoreSet은 현재의 선택 방법에 일치하며, AL과 KD의 교차점에서의 연구를 촉진하는 것을 보여줍니다.",
      "upvotes": 9,
      "discussionId": "683fcd9d56d0f1cfb34f1eb1",
      "githubRepo": "https://github.com/erjui/PCoreSet",
      "ai_summary": "ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.",
      "ai_keywords": [
        "knowledge distillation",
        "active learning",
        "task-specific models",
        "teacher models",
        "zero-shot",
        "few-shot",
        "large vision-language models",
        "structured prediction bias",
        "inductive bias",
        "Probabilistic CoreSet",
        "PCoreSet",
        "probability space",
        "categorically diverse samples"
      ]
    },
    "publishedAt": "2025-06-01T04:54:37.000Z",
    "title": "PCoreSet: Effective Active Learning through Knowledge Distillation from\n  Vision-Language Models",
    "summary": "Knowledge distillation (KD) is a widely used framework for training compact,\ntask-specific models by leveraging the knowledge of teacher models. However,\nits application to active learning (AL), which aims to minimize annotation\ncosts through iterative sample selection, remains underexplored. This gap stems\nfrom the fact that KD typically assumes access to sufficient labeled data,\nwhereas AL operates in data-scarce scenarios where task-specific teacher models\nare often unavailable. In this paper, we introduce ActiveKD, a framework that\nintegrates AL with KD by leveraging the zero- and few-shot capabilities of\nlarge vision-language models (VLMs). A key aspect of ActiveKD is the structured\nprediction bias of VLMs -- i.e., their predictions form clusters in the\nprobability space. We regard this structure as an inductive bias of the teacher\nmodel, capturing generalizable output patterns beneficial to student learning.\nTo exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection\nstrategy that maximizes coverage in the probability space rather than the\nfeature space. PCoreSet strategically selects categorically diverse unlabeled\nsamples, facilitating more efficient transfer of teacher knowledge under\nlimited annotation budgets. Evaluations on 11 datasets show that PCoreSet\nconsistently outperforms existing selection methods within the ActiveKD\nframework, advancing research at the intersection of AL and KD.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00910.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6357a08f8ed056fa1ccd3b38",
      "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
      "fullname": "erjui",
      "name": "erjui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02497",
      "authors": [
        {
          "_id": "683fbc32917306517315589f",
          "name": "Jiahao Chen",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a0",
          "name": "Hangjie Yuan",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a1",
          "name": "Yichen Qian",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a2",
          "name": "Jingyun Liang",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a3",
          "name": "Jiazheng Xing",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a4",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a5",
          "name": "Weihua Chen",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a6",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a7",
          "name": "Bing Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T06:25:00.000Z",
      "submittedOnDailyAt": "2025-06-04T01:55:34.274Z",
      "title": "LumosFlow: 동작을 안내받는 긴 비디오 생성",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "장 비디오 생성은 엔터테인먼트, 시뮬레이션 등 다양한 분야에서 광범위하게 활용되고 있으며, 주목을 받고 있습니다. 발전이 있는 반면, 시간에 걸쳐 일관된 시각적인 긴 시퀀스를 합성하는 것은 어려운 도전입니다. 기존의 접근 방식은 짧은 클립을 순차적으로 생성하여 결합하거나, 키 프레임을 생성하여 그 사이의 프레임을 휴리스틱으로 interpolation하여 장 비디오를 합성하는 것입니다. 그러나 둘 다 큰 도전이 남아 있으며, 시간적인 반복과 비자연적인 트랜지션 등 문제점이 있습니다. 본 논문에서는 장 비디오 생성 파이프라인을 재검토하고 LumosFlow라는 구조를 도입합니다. LumosFlow는 명시적인 움직임의 가이드를 채택합니다. 구체적으로는, 먼저 큰 움직임의 간격으로 키 프레임을 생성하기 위한 큰 움직임의 텍스트로부터 비디오의 확산 모델(LMTV-DM)을 사용합니다. 이로 인해 생성되는 장 비디오의 콘텐츠의 다양성을 보장합니다. 키 프레임 간의 맥락적인 트랜지션의 보간이 복잡하므로, 중간의 프레임의 보간을 움직임의 생성과 후처리로 분해합니다. 키 프레임의 쌍별로, 잠재적인 광학 흐름의 확산 모델(LOF-DM)을 사용하여 복잡한 큰 움직임의 광학 흐름을 합성하고, 그 후, MotionControlNet을 사용하여 프레임의 생성을 가이드하여 질을 향상시킵니다. 기존의 비디오 프레임의 보간과 비교하여, 15배의 보간을 실현하며, 인접한 프레임 사이의 적절한 연속적인 움직임을 보장합니다. 실험은 우리의 방법대로, 일관된 움직임과 외관을 가진 장 비디오를 생성할 수 있음을 보여줍니다. 코드와 모델은 승인 후 공개됩니다. 프로젝트 페이지는 https://jiahaochen1.github.io/LumosFlow/ 입니다.",
      "upvotes": 7,
      "discussionId": "683fbc36917306517315597d",
      "projectPage": "https://jiahaochen1.github.io/LumosFlow/",
      "ai_summary": "LumosFlow uses LMTV-DM for key frame generation and LOF-DM followed by MotionControlNet for smooth intermediate frame interpolation, ensuring temporally coherent long video generation.",
      "ai_keywords": [
        "Large Motion Text-to-Video Diffusion Model",
        "LMTV-DM",
        "Latent Optical Flow Diffusion Model",
        "LOF-DM",
        "MotionControlNet",
        "optical flows",
        "frame interpolation",
        "key frames",
        "long video generation",
        "motion guidance",
        "synthetic long videos"
      ]
    },
    "publishedAt": "2025-06-03T02:25:00.000Z",
    "title": "LumosFlow: Motion-Guided Long Video Generation",
    "summary": "Long video generation has gained increasing attention due to its widespread\napplications in fields such as entertainment and simulation. Despite advances,\nsynthesizing temporally coherent and visually compelling long sequences remains\na formidable challenge. Conventional approaches often synthesize long videos by\nsequentially generating and concatenating short clips, or generating key frames\nand then interpolate the intermediate frames in a hierarchical manner. However,\nboth of them still remain significant challenges, leading to issues such as\ntemporal repetition or unnatural transitions. In this paper, we revisit the\nhierarchical long video generation pipeline and introduce LumosFlow, a\nframework introduce motion guidance explicitly. Specifically, we first employ\nthe Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames\nwith larger motion intervals, thereby ensuring content diversity in the\ngenerated long videos. Given the complexity of interpolating contextual\ntransitions between key frames, we further decompose the intermediate frame\ninterpolation into motion generation and post-hoc refinement. For each pair of\nkey frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes\ncomplex and large-motion optical flows, while MotionControlNet subsequently\nrefines the warped results to enhance quality and guide intermediate frame\ngeneration. Compared with traditional video frame interpolation, we achieve 15x\ninterpolation, ensuring reasonable and continuous motion between adjacent\nframes. Experiments show that our method can generate long videos with\nconsistent motion and appearance. Code and models will be made publicly\navailable upon acceptance. Our project page:\nhttps://jiahaochen1.github.io/LumosFlow/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02497.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01144",
      "authors": [
        {
          "_id": "683fca92c1e51fea3a470e93",
          "name": "Ariel Shaulov",
          "hidden": false
        },
        {
          "_id": "683fca92c1e51fea3a470e94",
          "user": {
            "_id": "64972b8d27e41e26a32835d4",
            "avatarUrl": "/avatars/00c76991b5421f592d632a750ec8b998.svg",
            "isPro": false,
            "fullname": "Itay Hazan",
            "user": "itayhzn",
            "type": "user"
          },
          "name": "Itay Hazan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T06:31:03.556Z",
          "hidden": false
        },
        {
          "_id": "683fca92c1e51fea3a470e95",
          "name": "Lior Wolf",
          "hidden": false
        },
        {
          "_id": "683fca92c1e51fea3a470e96",
          "user": {
            "_id": "6181c72cdcc1df2c9de8a4d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
            "isPro": false,
            "fullname": "Hila Chefer",
            "user": "Hila",
            "type": "user"
          },
          "name": "Hila Chefer",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:36.782Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6181c72cdcc1df2c9de8a4d8/z3VoHZmxOtL3agHCaWWg4.mp4"
      ],
      "publishedAt": "2025-06-01T19:55:33.000Z",
      "submittedOnDailyAt": "2025-06-04T02:58:51.483Z",
      "title": "FlowMo: 흐름 지도에 기반한 동영상의 연속적인 이동 기록",
      "submittedOnDailyBy": {
        "_id": "6181c72cdcc1df2c9de8a4d8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
        "isPro": false,
        "fullname": "Hila Chefer",
        "user": "Hila",
        "type": "user"
      },
      "summary": "文脈 애니메이션의 분포 모델은 시간적인 모습(예: 움직임, 물리, 동적인 상호작용)을 모델화하는 능력이 기록적으로 제한되어 있습니다. 현재의 접근 방식은 이 제한을 해결하기 위해 모델의 재학습이나 외부 조건 부여 신호를 도입하여 시간의 일관성을 강제하는 것입니다. 본 연구에서는 추가적인 학습이나 보조 입력을 포함하지 않고, 사전 학습된 모델의 예측으로부터 의미 있는 시간의 표현을 추출할 수 있는지 조사했습니다. FlowMo라는 새로운 학습 필요 없는 가이드 메소드를 도입하고, 각 분포 단계에서 모델의 예측을 활용하여 동작의 일관성을 향상시킵니다. FlowMo는 연속적인 프레임에 대응하는 잠재 변수 사이의 거리를 측정하여 외관 무관한 시간의 표현을 얻습니다. 이로써 모델이 예측하는 잠재적인 시간의 구조를 명확히 합니다. 다음으로, 시간의 차원의 패치별로 분산을 측정하여 동작의 일관성을 평가하고 이를 동적으로 줄이는 방향으로 모델을 가이드합니다. 다양한 문맥 애니메이션 모델의 광범위한 실험에서, FlowMo는 동작의 일관성을 크게 향상시키고, 화질 또는 프로ン퓰트의 대응을 잃지 않고, 사전 학습된 애니메이션 분포 모델의 시간의 정확성을 향상시키는 효과적인 \"플레이와 플레이\" 솔루션을 제공했습니다.",
      "upvotes": 7,
      "discussionId": "683fca94c1e51fea3a470eee",
      "projectPage": "https://arielshaulov.github.io/FlowMo/",
      "githubRepo": "https://github.com/arielshaulov/FlowMo",
      "ai_summary": "FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.",
      "ai_keywords": [
        "text-to-video diffusion models",
        "temporal aspects",
        "motion",
        "physics",
        "dynamic interactions",
        "pre-trained model",
        "FlowMo",
        "guidance method",
        "appearance-debiased",
        "temporal representation",
        "latents",
        "patch-wise variance",
        "sampling",
        "temporal fidelity"
      ]
    },
    "publishedAt": "2025-06-01T15:55:33.000Z",
    "title": "FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video\n  Generation",
    "summary": "Text-to-video diffusion models are notoriously limited in their ability to\nmodel temporal aspects such as motion, physics, and dynamic interactions.\nExisting approaches address this limitation by retraining the model or\nintroducing external conditioning signals to enforce temporal consistency. In\nthis work, we explore whether a meaningful temporal representation can be\nextracted directly from the predictions of a pre-trained model without any\nadditional training or auxiliary inputs. We introduce FlowMo, a novel\ntraining-free guidance method that enhances motion coherence using only the\nmodel's own predictions in each diffusion step. FlowMo first derives an\nappearance-debiased temporal representation by measuring the distance between\nlatents corresponding to consecutive frames. This highlights the implicit\ntemporal structure predicted by the model. It then estimates motion coherence\nby measuring the patch-wise variance across the temporal dimension and guides\nthe model to reduce this variance dynamically during sampling. Extensive\nexperiments across multiple text-to-video models demonstrate that FlowMo\nsignificantly improves motion coherence without sacrificing visual quality or\nprompt alignment, offering an effective plug-and-play solution for enhancing\nthe temporal fidelity of pre-trained video diffusion models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6181c72cdcc1df2c9de8a4d8/z3VoHZmxOtL3agHCaWWg4.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01144.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6181c72cdcc1df2c9de8a4d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
      "fullname": "Hila Chefer",
      "name": "Hila",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01274",
      "authors": [
        {
          "_id": "683fe4caf8916dcd6d1c936a",
          "user": {
            "_id": "673060959e631f353ae1b5e0",
            "avatarUrl": "/avatars/d4b1e23de90ff1d02c38186a259b8d1e.svg",
            "isPro": false,
            "fullname": "Hosu Lee",
            "user": "lakelee",
            "type": "user"
          },
          "name": "Hosu Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:26.062Z",
          "hidden": false
        },
        {
          "_id": "683fe4caf8916dcd6d1c936b",
          "user": {
            "_id": "653238bed0f5a9e537ed966d",
            "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
            "isPro": false,
            "fullname": "Junho Kim",
            "user": "arkimjh",
            "type": "user"
          },
          "name": "Junho Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:30.239Z",
          "hidden": false
        },
        {
          "_id": "683fe4caf8916dcd6d1c936c",
          "name": "Hyunjun Kim",
          "hidden": false
        },
        {
          "_id": "683fe4caf8916dcd6d1c936d",
          "name": "Yong Man Ro",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T03:08:07.000Z",
      "submittedOnDailyAt": "2025-06-04T04:50:22.651Z",
      "title": "ReFoCUS: 강화학습에 의한 컨텍스트 이해를 통해 프레임 최적화 가이드\n\n(Note: The original text \"ReFoCUS: フレーム最適化をガイドする強化学習によるコンテキスト的理解\" is a direct translation from Japanese to Korean, but the provided translation above is a direct translation from Japanese to English, and then from English to Korean. If you need the translation from Japanese to Korean directly, please let me know.)",
      "submittedOnDailyBy": {
        "_id": "653238bed0f5a9e537ed966d",
        "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
        "isPro": false,
        "fullname": "Junho Kim",
        "user": "arkimjh",
        "type": "user"
      },
      "summary": "최근의 대형 다중 모델(LMMs)의 발전으로 시각 언어 추론이 효과적으로 가능해졌지만, 영상 콘텐츠 이해 능력은 프레임 선택 전략의 적절성 문제로 제한되어 있습니다. 현재의 접근 방식은 주로静적 휴리스틱 또는 외부 검색 모듈을 사용하여 영상-LLM에 프레임 정보를 제공하며, 이는 관련 정보를 제공하지 못합니다. 본 논문에서는 ReFoCUS(Reinforcement-guided Frame Optimization for Contextual UnderStanding)을 소개합니다. ReFoCUS는 프레임 수준의 정책 최적화 프레임워크이며, 최적화 목표를 문자열의 대답에서 이미지 입력 선택으로 옮깁니다. ReFoCUS는 强化学習을 사용하여, 참조 LMM에서 얻을 수 있는 보상 신호를 사용하여, 시간 순차적인 대답을 지원하는 최적의 프레임의 내적 선호도를 반영한 프레임 선택 정책을 학습합니다. 효율적으로 탐색하기 위해, 시간 순차적인 연속성을 보장하면서 복잡도를 줄이기 위해 자동 회귀적 조건부 선택 아키텍처를 사용합니다. 우리의 접근 방식은 프레임 수준에서 명시된 제어가 필요하지 않지만, 여러 영상 QA 벤치마크에서 논리 성능을 일관적으로 향상시키고, 프레임 선택과 모델 내부의 유틸리티의 일치를 beta로 밝혀줍니다.",
      "upvotes": 4,
      "discussionId": "683fe4ccf8916dcd6d1c93b6",
      "ai_summary": "ReFoCUS uses reinforcement learning to optimize frame selection for video-LLM, enhancing reasoning performance in video QA by aligning with model preferences.",
      "ai_keywords": [
        "Large Multi-modal Models",
        "vision-language reasoning",
        "frame selection strategies",
        "reinforcement learning",
        "frame selection policy",
        "reference LMM",
        "autoregressive architecture",
        "conditional selection architecture",
        "temporal coherence",
        "video QA benchmarks"
      ]
    },
    "publishedAt": "2025-06-01T23:08:07.000Z",
    "title": "ReFoCUS: Reinforcement-guided Frame Optimization for Contextual\n  Understanding",
    "summary": "Recent progress in Large Multi-modal Models (LMMs) has enabled effective\nvision-language reasoning, yet the ability to understand video content remains\nconstrained by suboptimal frame selection strategies. Existing approaches often\nrely on static heuristics or external retrieval modules to feed frame\ninformation into video-LLMs, which may fail to provide the query-relevant\ninformation. In this work, we introduce ReFoCUS (Reinforcement-guided Frame\nOptimization for Contextual UnderStanding), a novel frame-level policy\noptimization framework that shifts the optimization target from textual\nresponses to visual input selection. ReFoCUS learns a frame selection policy\nvia reinforcement learning, using reward signals derived from a reference LMM\nto reflect the model's intrinsic preferences for frames that best support\ntemporally grounded responses. To efficiently explore the large combinatorial\nframe space, we employ an autoregressive, conditional selection architecture\nthat ensures temporal coherence while reducing complexity. Our approach does\nnot require explicit supervision at the frame-level and consistently improves\nreasoning performance across multiple video QA benchmarks, highlighting the\nbenefits of aligning frame selection with model-internal utility.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01274.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653238bed0f5a9e537ed966d",
      "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
      "fullname": "Junho Kim",
      "name": "arkimjh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24726",
      "authors": [
        {
          "_id": "683ffca568402c738a947f4e",
          "name": "Shelly Bensal",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f4f",
          "name": "Umar Jamil",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f50",
          "name": "Christopher Bryant",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f51",
          "user": {
            "_id": "60e61b3969bd0df25c9375da",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
            "isPro": false,
            "fullname": "Melisa Russak",
            "user": "melisa",
            "type": "user"
          },
          "name": "Melisa Russak",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:53:45.047Z",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f52",
          "name": "Kiran Kamble",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f53",
          "name": "Dmytro Mozolevskyi",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f54",
          "name": "Muayad Ali",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f55",
          "name": "Waseem AlShikh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T15:49:42.000Z",
      "submittedOnDailyAt": "2025-06-04T06:34:02.563Z",
      "title": "Reflect, Retry, Reward: 자기 개선을 강화 학습에 의한 LLMs에 적용하기\n\n(注意：虽然任务要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，这里提供了一个更自然的表达方式。如果严格遵循任务要求，可以直接使用原始翻译结果。)",
      "submittedOnDailyBy": {
        "_id": "60e61b3969bd0df25c9375da",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
        "isPro": false,
        "fullname": "Melisa Russak",
        "user": "melisa",
        "type": "user"
      },
      "summary": "우리는 자기 자신을 반성하는 것을 통해 큰 언어 모델의 성능을 향상시키는 방법을 검토하고 있습니다. 모델이 잘못 답하는 경우, 더 좋은 자기 자신을 반성하도록 동기를 부여하고, 합성 데이터의 생성이 불가능하고, 이진의 피드백만 있는 경우에도, 모델이 복잡한 검증 가능한 작업의 해결 능력을 향상시킬 수 있음을 보여주었습니다. 우리의 프레임워크는 2단계로 작동합니다: 첫째, 작업이 실패하는 경우, 모델은 이전 시도를 분석하기 위해 자기 자신을 반성하는 댓글을 생성합니다. 둘째, 그 자기 자신을 반성하는 댓글을 바탕으로 작업을 다시 시도합니다. 만약 그 후의 시도가 성공하는 경우, 자기 자신을 반성하는 단계에서 생성된 토큰은 보상됩니다. 실험 결과를 따르면 다양한 모델 아키텍처에서 큰 성능 향상이 관찰되었으며, 수학식 표현에서 34.7%의 향상, 함수 호출에서 18.1%의 향상이 증명되었습니다. 특히, 1.5억에서 7억 파라미터의 작은 미세 조정 모델은 같은 가족의 10배 큰 모델을 초과합니다. 우리의 새로운 패러다임은 제한된 외부 피드백을 동반하여 어려운 작업에서 자동으로 향상되는 더 유용하고 안정적인 언어 모델을 개발하기 위한 흥미로운 경로입니다.",
      "upvotes": 4,
      "discussionId": "683ffca568402c738a947f7b",
      "ai_summary": "A method using self-reflection and reinforcement learning improves the performance of large language models, especially with limited feedback, by rewarding self-reflections that lead to better task performance.",
      "ai_keywords": [
        "self-reflection",
        "reinforcement learning",
        "self-reflective commentary",
        "performance gains",
        "math equation writing",
        "function calling",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-05-30T11:49:42.000Z",
    "title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
    "summary": "We explore a method for improving the performance of large language models\nthrough self-reflection and reinforcement learning. By incentivizing the model\nto generate better self-reflections when it answers incorrectly, we demonstrate\nthat a model's ability to solve complex, verifiable tasks can be enhanced even\nwhen generating synthetic data is infeasible and only binary feedback is\navailable. Our framework operates in two stages: first, upon failing a given\ntask, the model generates a self-reflective commentary analyzing its previous\nattempt; second, the model is given another attempt at the task with the\nself-reflection in context. If the subsequent attempt succeeds, the tokens\ngenerated during the self-reflection phase are rewarded. Our experimental\nresults show substantial performance gains across a variety of model\narchitectures, as high as 34.7% improvement at math equation writing and 18.1%\nimprovement at function calling. Notably, smaller fine-tuned models (1.5\nbillion to 7 billion parameters) outperform models in the same family that are\n10 times larger. Our novel paradigm is thus an exciting pathway to more useful\nand reliable language models that can self-improve on challenging tasks with\nlimited external feedback.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24726.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60e61b3969bd0df25c9375da",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
      "fullname": "Melisa Russak",
      "name": "melisa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03079",
      "authors": [
        {
          "_id": "683fee0a179d710da07d4352",
          "user": {
            "_id": "634aab35dcf125e4dafc87b1",
            "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
            "isPro": false,
            "fullname": "YangXiuyu",
            "user": "gzzyyxy",
            "type": "user"
          },
          "name": "Xiuyu Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:13.216Z",
          "hidden": true
        },
        {
          "_id": "683fee0a179d710da07d4353",
          "name": "Bohan Li",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4354",
          "name": "Shaocong Xu",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4355",
          "name": "Nan Wang",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4356",
          "name": "Chongjie Ye",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4357",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4358",
          "name": "Minghan Qin",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4359",
          "name": "Yikang Ding",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d435a",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d435b",
          "name": "Hang Zhao",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d435c",
          "name": "Hao Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:00:32.000Z",
      "submittedOnDailyAt": "2025-06-04T05:27:07.241Z",
      "title": "ORV: 4D 운영센터 로봇 비디오 생성",
      "submittedOnDailyBy": {
        "_id": "634aab35dcf125e4dafc87b1",
        "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
        "isPro": false,
        "fullname": "YangXiuyu",
        "user": "gzzyyxy",
        "type": "user"
      },
      "summary": "テレオプション에서 실제 세계의 로봇 시뮬레이션 데이터를 얻는 것은 시간과 노력이 필요합니다. 최근, 액션 주도 기노리티 모델은 안전성을 우려하지 않고 유지 관리의 노력을 줄여 로봇 학습과 시뮬레이션에 광범위하게 도입되었습니다. 그러나 이러한 방법론에서 사용하는 액션 시퀀스는 글로벌적으로 코어가 없는 데서 제어 정밀도와 일반화 능력을 제한적으로 가진 문제점이 있습니다. 이러한 제한을 해결하기 위해 로봇 비디오 생성 프레임워크 ORV를 제안합니다. ORV는 4D 의미적인 옥키니티 시퀀스를 선형 표현으로 활용하여 더 정확한 의미적 및 기오메트리적 가이드를 제공합니다. 옥키니티 기반 표현을 변환함으로써, ORV는 현실적인 로봇 비디오로 시뮬레이션 데이터를 쉽게 변환하고 높은 시간적 일관성과 정밀한 제어 가능성을 보장합니다. 또한 로봇의 미세 조작을 위한 다점 비디오의 동시 생성을 지원하여 로봇 학습 태스크에 필요한 중요한 능력을 제공합니다. 확장된 실험 결과는 ORV는 현재의 기본 방법보다 다양한 데이터셋과 서브 태스크에서 일관되고 뛰어넘습니다. демо, 코드 및 모델: https://orangesodahub.github.io/ORV",
      "upvotes": 3,
      "discussionId": "683fee12179d710da07d45f4",
      "projectPage": "https://orangesodahub.github.io/ORV/",
      "githubRepo": "https://github.com/OrangeSodahub/ORV",
      "ai_summary": "ORV, an Occupancy-centric Robot Video generation framework, uses 4D semantic occupancy sequences to produce photorealistic, temporally consistent, and precisely controllable robot videos, enhancing existing methods.",
      "ai_keywords": [
        "action-driven generative models",
        "occupancy-centric",
        "4D semantic occupancy sequences",
        "video generation",
        "photorealistic robot videos",
        "temporal consistency",
        "precise controllability",
        "multi-view videos"
      ]
    },
    "publishedAt": "2025-06-03T13:00:32.000Z",
    "title": "ORV: 4D Occupancy-centric Robot Video Generation",
    "summary": "Acquiring real-world robotic simulation data through teleoperation is\nnotoriously time-consuming and labor-intensive. Recently, action-driven\ngenerative models have gained widespread adoption in robot learning and\nsimulation, as they eliminate safety concerns and reduce maintenance efforts.\nHowever, the action sequences used in these methods often result in limited\ncontrol precision and poor generalization due to their globally coarse\nalignment. To address these limitations, we propose ORV, an Occupancy-centric\nRobot Video generation framework, which utilizes 4D semantic occupancy\nsequences as a fine-grained representation to provide more accurate semantic\nand geometric guidance for video generation. By leveraging occupancy-based\nrepresentations, ORV enables seamless translation of simulation data into\nphotorealistic robot videos, while ensuring high temporal consistency and\nprecise controllability. Furthermore, our framework supports the simultaneous\ngeneration of multi-view videos of robot gripping operations - an important\ncapability for downstream robotic learning tasks. Extensive experimental\nresults demonstrate that ORV consistently outperforms existing baseline methods\nacross various datasets and sub-tasks. Demo, Code and Model:\nhttps://orangesodahub.github.io/ORV",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634aab35dcf125e4dafc87b1",
      "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
      "fullname": "YangXiuyu",
      "name": "gzzyyxy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01789",
      "authors": [
        {
          "_id": "683fb99c7c8d720be438000a",
          "name": "Genta Indra Winata",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000b",
          "name": "David Anugraha",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000c",
          "name": "Emmy Liu",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000d",
          "name": "Alham Fikri Aji",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000e",
          "name": "Shou-Yi Hung",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000f",
          "name": "Aditya Parashar",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380010",
          "user": {
            "_id": "64d1e3a87e20ec9ea0020d03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d1e3a87e20ec9ea0020d03/xm-afh1AaqS0e9qpaPo7y.jpeg",
            "isPro": false,
            "fullname": "Patrick Amadeus Irawan",
            "user": "patrickamadeus",
            "type": "user"
          },
          "name": "Patrick Amadeus Irawan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:30.937Z",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380011",
          "name": "Ruochen Zhang",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380012",
          "name": "Zheng-Xin Yong",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380013",
          "name": "Jan Christian Blaise Cruz",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380014",
          "name": "Niklas Muennighoff",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380015",
          "user": {
            "_id": "6469949654873f0043b09c22",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6469949654873f0043b09c22/Lk7IJAR16Wa_sGJ2g81AQ.jpeg",
            "isPro": false,
            "fullname": "Seungone Kim",
            "user": "seungone",
            "type": "user"
          },
          "name": "Seungone Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:22.158Z",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380016",
          "name": "Hanyang Zhao",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380017",
          "user": {
            "_id": "63139ff6b46fc4e24332fa84",
            "avatarUrl": "/avatars/ee6923a7cb218f22535064a87761e497.svg",
            "isPro": false,
            "fullname": "Sudipta Kar",
            "user": "cryptexcode",
            "type": "user"
          },
          "name": "Sudipta Kar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:28.286Z",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380018",
          "name": "Kezia Erina Suryoraharjo",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380019",
          "name": "M. Farid Adilazuarda",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001a",
          "name": "En-Shiun Annie Lee",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001b",
          "name": "Ayu Purwarianti",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001c",
          "name": "Derry Tanti Wijaya",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001d",
          "name": "Monojit Choudhury",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T15:31:52.000Z",
      "submittedOnDailyAt": "2025-06-04T01:42:44.251Z",
      "title": "데이터 시트는 충분하지 않습니다：데이터 리빌리즘과 자동화 장르의 측정 및 책임",
      "submittedOnDailyBy": {
        "_id": "5f5c4b20e56d546cd6233098",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
        "isPro": false,
        "fullname": "Genta Indra Winata",
        "user": "gentaiscool",
        "type": "user"
      },
      "summary": "고품질 데이터셋은 기계 학습 모델의 훈련과 평가에 기초적인 존재로, 그 생성, 특히 정확한 인간의 어노테이션에 의한 생성은 큰 문제로 간주된다. 여러 데이터셋 논문의 제출은 독특성, 다样성, 또는 엄격한 품질 관리에 부족하며, 이러한 결점을 평가 시 자주 놓치고 있다. 제출도 데이터셋의 구축과 특성에 대한 중요한 세부 사항이 많이 생략되어 있다. 기존의 도구처럼 datasheets는 시각화를 촉진하기 위해 준비되어 있지만, 주로 설명적이고 표준화된 측정 가능한 데이터의 품질 평가의 방법 제공하지 않는다. 마찬가지로, 회의에서 메타 데이터의 요구는 책임에 대한 촉발을 하지만, 불균등하게 강제되어 있다. 이러한 제한을 해결하기 위해, 이 논문은 데이터셋의 평가 과정에서 체계적인, 검토 포인트에 기반한 평가 지표를 통합하는 것을 주장하고 있다. 또한, 이러한 결점을 해결하기 위해, 합성 데이터의 생성에 적절한 도구와 LLM-as-a-judge의 접근법을 채택한 scalable, 비용 효율적인 방법을 검토하고 있다. 행동의 촉발로, DataRubrics를 소개하며, 이는 인간이나 모델에서 생성된 데이터셋의 품질을 평가하기 위한 구조화된 프레임워크이다. LLM 기반의 평가에서 최근의 진보를 활용한 데이터 Rubrics는 데이터셋의 품질 평가에 대한 재현 가능한, scalable, 행동 가능한 해결책을 제공하며, 이를 통해 어떤 러너나 평가자가 데이터 중심의 연구에서 높은 표준을 유지할 수 있도록 한다. 또한, LLM 기반의 평가의 재현성을 지원하기 위한 코드는 https://github.com/datarubrics/datarubrics 에서 릴리즈되어 있다.",
      "upvotes": 3,
      "discussionId": "683fb99e7c8d720be4380090"
    },
    "publishedAt": "2025-06-02T11:31:52.000Z",
    "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and\n  Accountability",
    "summary": "High-quality datasets are fundamental to training and evaluating machine\nlearning models, yet their creation-especially with accurate human\nannotations-remains a significant challenge. Many dataset paper submissions\nlack originality, diversity, or rigorous quality control, and these\nshortcomings are often overlooked during peer review. Submissions also\nfrequently omit essential details about dataset construction and properties.\nWhile existing tools such as datasheets aim to promote transparency, they are\nlargely descriptive and do not provide standardized, measurable methods for\nevaluating data quality. Similarly, metadata requirements at conferences\npromote accountability but are inconsistently enforced. To address these\nlimitations, this position paper advocates for the integration of systematic,\nrubric-based evaluation metrics into the dataset review process-particularly as\nsubmission volumes continue to grow. We also explore scalable, cost-effective\nmethods for synthetic data generation, including dedicated tools and\nLLM-as-a-judge approaches, to support more efficient evaluation. As a call to\naction, we introduce DataRubrics, a structured framework for assessing the\nquality of both human- and model-generated datasets. Leveraging recent advances\nin LLM-based evaluation, DataRubrics offers a reproducible, scalable, and\nactionable solution for dataset quality assessment, enabling both authors and\nreviewers to uphold higher standards in data-centric research. We also release\ncode to support reproducibility of LLM-based evaluations at\nhttps://github.com/datarubrics/datarubrics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01789.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f5c4b20e56d546cd6233098",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
      "fullname": "Genta Indra Winata",
      "name": "gentaiscool",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03096",
      "authors": [
        {
          "_id": "684002718bd5bff9918ce018",
          "user": {
            "_id": "6310a6bb0a43f97f6c5567d3",
            "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
            "isPro": false,
            "fullname": "Christian Schlarmann",
            "user": "chs20",
            "type": "user"
          },
          "name": "Christian Schlarmann",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:53:34.919Z",
          "hidden": false
        },
        {
          "_id": "684002718bd5bff9918ce019",
          "name": "Francesco Croce",
          "hidden": false
        },
        {
          "_id": "684002718bd5bff9918ce01a",
          "name": "Nicolas Flammarion",
          "hidden": false
        },
        {
          "_id": "684002718bd5bff9918ce01b",
          "name": "Matthias Hein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:27:12.000Z",
      "submittedOnDailyAt": "2025-06-04T08:12:15.755Z",
      "title": "FuseLIP: 초기 토큰의 이산 융합에 의한 다모달 엔베디닝",
      "submittedOnDailyBy": {
        "_id": "6310a6bb0a43f97f6c5567d3",
        "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
        "isPro": false,
        "fullname": "Christian Schlarmann",
        "user": "chs20",
        "type": "user"
      },
      "summary": "Contrastive language-image pre-training은 각 모델에서 다른 인코더를 사용하며, 공통적인 잠재 공간에서 문과 이미지 페어의 특징량을 조정합니다. 이 접근 방식은 많은 zero-shot 태스크에서 놀라울만한 성능을 보입니다が, 원래는 다중 모델 입력을 처리할 수 없기 때문에, 이미지와 텍스트를 하나의 특징 벡터로 인코딩할 수 없습니다. 대처로, 추가 모듈을 사용하여 추출된 단일 모델에서 추출된 특징을 통합하는 것이 일반적인 실습입니다. 본 논문에서는, FuseLIP라는 다중 모델 인코더의 대체적인 아키텍처를 제안합니다. 최근의 이산 이미지 토큰나이저의 발전을 활용하여, 텍스트와 이미지 토큰나이저의 확장 어휘에 효과적인 변환 모델을 제안합니다. 이 초기 융합 접근 방식에서, 서로 다른 모델이 서로 상호작용하며, 일반적인 지연 융합보다 풍부한 표현을 얻을 수 있습니다. 새로운 데이터셋을 수집하고, 다중 모델 인코더 모델의 어려운 태스크를 설계합니다. FuseLIP는 VQA나 텍스트 가이드 이미지 변환 검색 등 다중 모델 인코더 태스크에서 다른 접근 방식보다 뛰어납니다が, 단일 모델 태스크에서는 기준과 비교하여 비슷한 성능을 나타냅니다.",
      "upvotes": 2,
      "discussionId": "684002758bd5bff9918ce109"
    },
    "publishedAt": "2025-06-03T13:27:12.000Z",
    "title": "FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens",
    "summary": "Contrastive language-image pre-training aligns the features of text-image\npairs in a common latent space via distinct encoders for each modality. While\nthis approach achieves impressive performance in several zero-shot tasks, it\ncannot natively handle multimodal inputs, i.e., encoding image and text into a\nsingle feature vector. As a remedy, it is common practice to use additional\nmodules to merge the features extracted by the unimodal encoders. In this work,\nwe present FuseLIP, an alternative architecture for multimodal embedding.\nLeveraging recent progress in discrete image tokenizers, we propose to use a\nsingle transformer model which operates on an extended vocabulary of text and\nimage tokens. This early fusion approach allows the different modalities to\ninteract at each depth of encoding and obtain richer representations compared\nto common late fusion. We collect new datasets for multimodal pre-training and\nevaluation, designing challenging tasks for multimodal encoder models. We show\nthat FuseLIP outperforms other approaches in multimodal embedding tasks such as\nVQA and text-guided image transformation retrieval, while being comparable to\nbaselines on unimodal tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03096.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310a6bb0a43f97f6c5567d3",
      "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
      "fullname": "Christian Schlarmann",
      "name": "chs20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02454",
      "authors": [
        {
          "_id": "683fb5d592425f86f2be5c40",
          "name": "Zhaorui Yang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c41",
          "name": "Bo Pan",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c42",
          "name": "Han Wang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c43",
          "name": "Yiyao Wang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c44",
          "name": "Xingyu Liu",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c45",
          "name": "Minfeng Zhu",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c46",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c47",
          "name": "Wei Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a568f5764b1dce366f9fd2/Jup3eQX_IL2nKhCQVaSWS.mp4"
      ],
      "publishedAt": "2025-06-03T05:18:19.000Z",
      "submittedOnDailyAt": "2025-06-04T01:28:03.959Z",
      "title": "멀티모달 딥러치러：텍스트와 차트를 교차하여 작성한 보고서의 생성\n  효과적인 프레임워크를 사용하여 스크래치에서 시작\n\n이 번역은 전문성과 정확성을 유지하였습니다.",
      "submittedOnDailyBy": {
        "_id": "64a568f5764b1dce366f9fd2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a568f5764b1dce366f9fd2/9THW2AJhEVmzltEAm9mMY.jpeg",
        "isPro": false,
        "fullname": "Zhaorui Yang",
        "user": "zhaoruiyang",
        "type": "user"
      },
      "summary": "ビジュアライゼーション은 개념과 정보의 효과적인 전달에 중요한 역할을 수행하고 있습니다. 최근의 이유와 검색어워드 생성의 발전에 따라, 대규모 언어 모델(LLMs)은 깊은 연구를 수행하고, 상세한 보고서를 생성할 수 있습니다. 그러나 현재의 깊은 연구 프레임워크는 주로 텍스트만 생성하는 것을 중심으로, 자동적인 교차 텍스트와 시각화의 생성은 조사되지 않았습니다. 이 새로운 태스크는 정보적인 시각화의 설계와 텍스트 보고서의 유효한 통합에 중요한 문제를 제기하고 있습니다. 이러한 문제를 해결하기 위해, 우리는 시각화의 공식적인 설명(FDV)을 제안합니다. FDV는 구조화된 텍스트 표현이며, LLMs가 학습하고 다양한, 고품질의 시각화를 생성할 수 있도록 합니다. 이 표현에 기반하여, 우리는 Multimodal DeepResearcher라는 에이전트 프레임워크를 통해 다음 4단계로 분해합니다: (1) 연구, (2) 샘플 보고서의 텍스트화, (3) 계획, (4) 모델 버전 보고서의 생성. 생성된 모델 버전 보고서의 평가에 있어서, MultimodalReportBench를 개발했습니다. 이는 100개의 다양한 테마를 포함하고, 5개의 전문적인 메트릭스를 사용하며 제공됩니다. 모델과 평가 방법의 확장 실험은 Multimodal DeepResearcher의 효과성을 보여주었습니다. 특히, 동일한 Claude 3.7 Sonnet 모델을 사용하였을 때, Multimodal DeepResearcher는 82%의 전체적인 승률을 달성하여 베이스라인 방법보다 뛰어납니다.",
      "upvotes": 2,
      "discussionId": "683fb5d792425f86f2be5c78",
      "projectPage": "https://rickyang1114.github.io/multimodal-deepresearcher/",
      "ai_summary": "A new framework, Multimodal DeepResearcher, enables Large Language Models to generate high-quality multimodal reports combining text and diverse visualizations through structured textual representations.",
      "ai_keywords": [
        "Formal Description of Visualization",
        "FDV",
        "Multimodal DeepResearcher",
        "researching",
        "exemplar report textualization",
        "planning",
        "multimodal report generation",
        "MultimodalReportBench",
        "multimodal reports"
      ]
    },
    "publishedAt": "2025-06-03T01:18:19.000Z",
    "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports\n  From Scratch with Agentic Framework",
    "summary": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a568f5764b1dce366f9fd2/Jup3eQX_IL2nKhCQVaSWS.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a568f5764b1dce366f9fd2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a568f5764b1dce366f9fd2/9THW2AJhEVmzltEAm9mMY.jpeg",
      "fullname": "Zhaorui Yang",
      "name": "zhaoruiyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02338",
      "authors": [
        {
          "_id": "683fbc730ffa93c1611d513b",
          "user": {
            "_id": "64c8f4cec547ed5243ebd0a8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
            "isPro": false,
            "fullname": "Hyungjoo Chae",
            "user": "hyungjoochae",
            "type": "user"
          },
          "name": "Hyungjoo Chae",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:14.169Z",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513c",
          "name": "Dongjin Kang",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513d",
          "name": "Jihyuk Kim",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513e",
          "name": "Beong-woo Kwak",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513f",
          "name": "Sunghyun Park",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5140",
          "name": "Haeju Park",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5141",
          "name": "Jinyoung Yeo",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5142",
          "name": "Moontae Lee",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5143",
          "name": "Kyungjae Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T00:29:15.000Z",
      "submittedOnDailyAt": "2025-06-04T01:55:22.453Z",
      "title": "Open-Source Reasoning モデル의 한 단점: RL을 사용하여 짙은 시작을 피하기 위한 짧은 컨텍스트 LLM의 데이터 세트",
      "submittedOnDailyBy": {
        "_id": "64c8f4cec547ed5243ebd0a8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
        "isPro": false,
        "fullname": "Hyungjoo Chae",
        "user": "hyungjoochae",
        "type": "user"
      },
      "summary": "R1의 공개로 더 큰 규모의 논리 모델(LRM)을 사용할 수 있게 되었고, 연구자들은 일반적으로 R1의 긴 Chain-of-Thought(CoT) 추론을 사용하여 새로운 LRM을 훈련하고 있습니다. 선행 연구는 LRM의 능력을 재현할 수 있는 직접적인 훈련을 보여줍니다. 그러나 현재의 모델(예: R1)의 의존성이 발전의 중요한 제약으로 작용하는 것을 밝혀 봅니다. 독립적인 LRM 개발의 첫 단계로, 이 논문은 훈련되지 않은 LLM을 사용하여 추론 시 스케일링에 대응하는 긴 CoT 데이터 세트를 구축할 가능성을 검토합니다. 이를 위해, 100K의 CoT 이유를 포함하는 Long CoT Collection 데이터 세트를 제공합니다. 여기서는 o1의 새로운 논리 전략을 짧은 CoT LLM에 도입하고, 더 긴 추측을 할 수 있도록, 추측을 더 제어할 수 있도록, 과도한 생각 문제에 더 나은 관리를 할 수 있도록 합니다. 확장된 분석은 데이터 세트가 R1과 같은 질 또는 약간 낮은 질을 달성하는 것을 증명합니다. 또한, 실험은 데이터 세트에서의 훈련은 일반적인 논리 능력 강화를 통해 RLVR에서 2-3배의 이점을 얻는 것을 보여줍니다.",
      "upvotes": 2,
      "discussionId": "683fbc760ffa93c1611d51cc",
      "ai_summary": "The Long CoT Collection dataset, generated by short CoT LLMs, enhances general reasoning skills and provides a strong foundation for reinforcement learning, achieving quality comparable to R1.",
      "ai_keywords": [
        "long chain-of-thought",
        "CoT inferences",
        "LRMs",
        "direct distillation",
        "inference-time scaling",
        "CoT rationales",
        "short CoT LLMs",
        "reasoning strategies",
        "thought budget",
        "overthinking",
        "reinforcement learning",
        "RLVR"
      ]
    },
    "publishedAt": "2025-06-02T20:29:15.000Z",
    "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to\n  Mitigate Cold-Starting Short CoT LLMs in RL",
    "summary": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c8f4cec547ed5243ebd0a8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
      "fullname": "Hyungjoo Chae",
      "name": "hyungjoochae",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00413",
      "authors": [
        {
          "_id": "683e703c33ac56778c2e51cd",
          "user": {
            "_id": "630139f1f6bea7dd15bdaf4e",
            "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
            "isPro": false,
            "fullname": "Daniel Israel",
            "user": "danielmisrael",
            "type": "user"
          },
          "name": "Daniel Israel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:22.988Z",
          "hidden": false
        },
        {
          "_id": "683e703c33ac56778c2e51ce",
          "name": "Guy Van den Broeck",
          "hidden": false
        },
        {
          "_id": "683e703c33ac56778c2e51cf",
          "name": "Aditya Grover",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T06:10:10.000Z",
      "submittedOnDailyAt": "2025-06-04T03:04:01.663Z",
      "title": "Adaptive Parallel Decoding를 사용하여 Diffusion LLMs를 가속화합니다.",
      "submittedOnDailyBy": {
        "_id": "630139f1f6bea7dd15bdaf4e",
        "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
        "isPro": false,
        "fullname": "Daniel Israel",
        "user": "danielmisrael",
        "type": "user"
      },
      "summary": "LLMs의 생성 속도는 자동 복원 디코딩에 의해 붉은 낱줄로 되어서, 토큰이 한 개씩 순차적으로 예측되는 낱줄로 되어 있습니다. 대신, 확산 대 언어 모델(dLLMs)은 이론적으로 토큰의 병렬 생성이 가능한 것으로 밝혀졌지만, 실제적으로, 품질을 크게 감점하지 않는 한, 자동 복원 모델의 속도를 달성하는 것은 어려워졌습니다. 이에 따라, 우리는 토큰의 병렬 추출 수를 동적으로 조정하는 새로운 방법인 자동 조정 병렬 디코딩(APD)을 소개합니다. 이를 위해, dLLM의 경계 확률과 작은 보조 자동 복원 모델의 배열의 통합 확률의 곱을 정의합니다. 이는 예측 디코딩의 표준 설정을 역전시키고, 큰 자동 복원 모델에서 작은 모델까지 디코딩을 시도하는 것을 목표로 합니다. 또한, APD를 발전시키기 위해, KV 캐치와 마스크된 입력의 크기의 제한을 설정합니다. 이러한 방법은 트랜스포머와 품질을 유연하게 조정하기 위해 3개의 조정 파라미터를 제공합니다. APD는 하류 벤치마크에서 품질 손실이 최소화되고, 명확히 높은 트랜스포머를 제공합니다.",
      "upvotes": 2,
      "discussionId": "683e703d33ac56778c2e51fe",
      "ai_summary": "Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.",
      "ai_keywords": [
        "autoregressive decoding",
        "diffusion large language models",
        "dLLMs",
        "adaptive parallel decoding",
        "APD",
        "marginal probabilities",
        "joint probability",
        "speculative decoding",
        "KV caching",
        "masked input"
      ]
    },
    "publishedAt": "2025-05-31T02:10:10.000Z",
    "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
    "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00413.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630139f1f6bea7dd15bdaf4e",
      "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
      "fullname": "Daniel Israel",
      "name": "danielmisrael",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16994",
      "authors": [
        {
          "_id": "683fb5d3a09aefea70733fa3",
          "user": {
            "_id": "64e14c5b12a5504dda70e60d",
            "avatarUrl": "/avatars/944b486bb037364ef7d9d2c826526708.svg",
            "isPro": false,
            "fullname": "Runyang",
            "user": "dd101bb",
            "type": "user"
          },
          "name": "Runyang You",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T07:34:53.226Z",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa4",
          "user": {
            "_id": "674038313ccfb67446ae2b35",
            "avatarUrl": "/avatars/8a3c0fdf971363988731f9eb8b13658c.svg",
            "isPro": false,
            "fullname": "tensorslow",
            "user": "tensorslow",
            "type": "user"
          },
          "name": "Yongqi Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-04T02:56:21.008Z",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa5",
          "name": "Xinyu Lin",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa6",
          "user": {
            "_id": "63b6dbc8ccebeadccc888456",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
            "isPro": false,
            "fullname": "Xin Zhang",
            "user": "izhx",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:59.107Z",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa7",
          "name": "Wenjie Wang",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa8",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa9",
          "name": "Liqiang Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:55:43.000Z",
      "submittedOnDailyAt": "2025-06-04T01:27:54.567Z",
      "title": "R^2ec: 이유를 지닌 큰 추천 모듈로의 방향법\n\n(注意：R^2ec 是一个专有名词，通常在机器学习或推荐系统中使用，翻译时保持原样。)",
      "submittedOnDailyBy": {
        "_id": "63b6dbc8ccebeadccc888456",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
        "isPro": false,
        "fullname": "Xin Zhang",
        "user": "izhx",
        "type": "user"
      },
      "summary": "대규모 모델을 추천 모델로 확장하여 강력한 추천 모델로 활용하고, LLM의 추론의 최근 발전은 추천의 추론을 검토하는 데 협력하고 있습니다. 현재의 연구는 추천 프로세스를 강화하기 위해 LLM을 외부 추론 모듈로 배치하는 중입니다. 그러나 이러한 분리된 설계는 중요한 리소스 비용과 최적의 공통 최적화의 결함이 있습니다. 이러한 문제를 해결하기 위해, 우리는 고유의 추론 능력을 가진 통합된 대규모 모델로 \\name\\을 제안합니다. 처음으로, 모델 아키텍처를 재검토하고 자동 협조적인 추론과 추천을 촉진하는 것을 목표로 합니다. 그 후, RecPO라는 대응하는 강화 학습 프레임워크를 제안하고, 동시에 추론과 추천의 능력을 최적화하기 위해 하나의 정책 업데이트로 \\name\\을 최적화합니다. RecPO는 추천 레이블을 모두 활용하여 추론 능력을 시뮬레이션하고, 특정화된 추론 어노테이션에 의존하지 않도록 합니다. 3개의 데이터셋에 대해 다양한 기준과 함께 실험을 수행하고, \\name\\의 효과를 입증했습니다. Hit@5에서 상대적인 향상률이 68.67%, NDCG@20에서 45.21%를 나타냅니다. 코드는 https://github.com/YRYangang/RRec에 접근할 수 있습니다.",
      "upvotes": 2,
      "discussionId": "683fb5d5a09aefea70734001",
      "githubRepo": "https://github.com/YRYangang/RRec",
      "ai_summary": "A unified large recommender model with intrinsic reasoning capabilities is proposed, facilitating interleaved reasoning and recommendation using a reinforcement learning framework called RecPO.",
      "ai_keywords": [
        "recommender models",
        "LLMs",
        "intrinsic reasoning",
        "autoregressive process",
        "reinforcement learning",
        "RecPO",
        "fused reward scheme",
        "Hit@5",
        "NDCG@20"
      ]
    },
    "publishedAt": "2025-05-22T13:55:43.000Z",
    "title": "R^2ec: Towards Large Recommender Models with Reasoning",
    "summary": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16994.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b6dbc8ccebeadccc888456",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
      "fullname": "Xin Zhang",
      "name": "izhx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03144",
      "authors": [
        {
          "_id": "684015f08bd5bff99191dff4",
          "user": {
            "_id": "644b71ddb2e7823a76abcf91",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
            "isPro": false,
            "fullname": "zhou wei",
            "user": "WeiChow",
            "type": "user"
          },
          "name": "Wei Chow",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:56:45.423Z",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff5",
          "name": "Yuan Gao",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff6",
          "name": "Linfeng Li",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff7",
          "name": "Xian Wang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff8",
          "name": "Qi Xu",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff9",
          "name": "Hang Song",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffa",
          "name": "Lingdong Kong",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffb",
          "name": "Ran Zhou",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffc",
          "name": "Yi Zeng",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffd",
          "name": "Yidong Cai",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffe",
          "name": "Botian Jiang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dfff",
          "name": "Shilin Xu",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e000",
          "name": "Jiajun Zhang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e001",
          "name": "Minghui Qiu",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e002",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e003",
          "name": "Tianshu Yang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e004",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e005",
          "name": "Juncheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:59:14.000Z",
      "submittedOnDailyAt": "2025-06-04T08:16:40.362Z",
      "title": "우대점: 교차된 다언어 문맥 탐색에 의한 다조건 쿼리",
      "submittedOnDailyBy": {
        "_id": "644b71ddb2e7823a76abcf91",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
        "isPro": false,
        "fullname": "zhou wei",
        "user": "WeiChow",
        "type": "user"
      },
      "summary": "セマンティック・リテライブラリー는 현대의 애플리케이션에 매우 중요하지만, 현재의 연구에서는 아직 조사가 부족하다. 기존의 데이터셋은 단일 언어, 단일 이미지, 또는 단일 리テライブ 조건에 제한되어 있으며, 이미지가 캡션으로 대체된 경우 성능의 지속성을 고려하면 시각정보의 표현력을 완전히 발휘할 수 없다는 점을 명확히 밝혀졌다. 그러나 실용적인 리テライブ 사례는 여러 이미지가 포함된 교차된 다조건 クエリ로 구성되어 있다. 따라서, 본 논문에서는 MERIT(교차된 다조건 세럿 리テライブ)라는 최초의 다언어 데이터셋을 소개하고, 5언어로 7종의 다른 제품 카테고리를 포함하며 320,000건의 クエリ와 135,000건의 제품을 포함하는 데이터셋을 소개한다. MERIT에서 수행한 전환적인 실험은 현재의 모델의 한계를 명확히 보여주었다: 특정 조건 요소를 과거에 고려하고, 글로벌 세럿 정보를 집중하는 것이다. 이에 따라, 새로운 미세 조정 프레임워크인 Coral을 제안하고, 미리 훈련된 MLLM을 적용하여, 인코딩 재구성을 포함하고, 微小 조건 요소를 보존하고, 대비 학습을 포함하여, 상세한 글로벌 세럿 정보를 추출하도록 하였다. 실험은 MERIT에서 45.9%의 성능 향상을 나타내며, 8개의 기존 리テライブ 벤치마크에서 강한 일반화 능력을 증명했다. 이러한 기여는 새로운 데이터셋, 현재 접근법의 중요한 한계를 인식, 새로운 미세 조정 프레임워크의 창출으로, 교차된 다조건 세럿 리テライブ의 미래 연구의 기초를 구축하고 있다.",
      "upvotes": 1,
      "discussionId": "684015f38bd5bff99191e158"
    },
    "publishedAt": "2025-06-03T13:59:14.000Z",
    "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition\n  Query",
    "summary": "Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03144.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644b71ddb2e7823a76abcf91",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
      "fullname": "zhou wei",
      "name": "WeiChow",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02510",
      "authors": [
        {
          "_id": "683faa31f0564d1fb4b9ffc6",
          "user": {
            "_id": "642656cbad1e3b0e6e91b752",
            "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
            "isPro": false,
            "fullname": "Jie Zhu",
            "user": "amazingj",
            "type": "user"
          },
          "name": "Jie Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:57:01.885Z",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffc7",
          "name": "Junhui Li",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffc8",
          "name": "Yalong Wen",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffc9",
          "name": "Xiandong Li",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffca",
          "name": "Lifan Guo",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffcb",
          "name": "Feng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T06:41:09.000Z",
      "submittedOnDailyAt": "2025-06-04T00:38:17.377Z",
      "title": "M^3FinMeeting: 다언어, 다산업, 다업무의 재무회의 이해 평가 데이터셋",
      "submittedOnDailyBy": {
        "_id": "642656cbad1e3b0e6e91b752",
        "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
        "isPro": false,
        "fullname": "Jie Zhu",
        "user": "amazingj",
        "type": "user"
      },
      "summary": "최근의 대규모 언어 모델(LLMs)의 발전으로 금융 분야에서의 성능 평가를 위한 새로운 벤치마크가 개발되었습니다. 그러나 현재의 금융 벤치마크는 주로 뉴스 기사, 수익보고서, 또는 발표에 기반하여 있으며, 실제로 금융 회의의 동향을 쉽게 파악하는 데 부족합니다. 이를填ぐために, 우리는 금융 회의 이해에 대한 새로운 벤치마크를 제안します. 이는 다언어, 다산업 분야, 다 태스크의 데이터 세트입니다. 먼저, M^3FinMeeting은 영어, 중국어, 일본어를 지원하며, 다양한 언어 컨텍스트에서 금융 토론의 이해를 강화합니다. 이어서, 글로벌 산업 분류 표준(GICS)에 따라 정의된 다양한 산업 분야를 수록하고, 벤치마크는广泛的金融活动을 커버합니다. 마지막으로, M^3FinMeeting은邀賢、문제응답(QA) 페어의 추출, 문제응답의 3가지의 태스크를 포함하며, 더 현실적이고 상세한 이해를 평가하기 위해 촉발합니다. 7개의 인기 LLMs와의 실험결과로부터, 가장 선진한 긴 문맥 모델도 큰 개선 여지가 있는 것을 명확히 한 반면, M^3FinMeeting은 LLMs의 금융 회의 이해 능력을 평가하는 데 필요한 벤치마크의 효과에 입증되어 있습니다.",
      "upvotes": 1,
      "discussionId": "683faa32f0564d1fb4ba0005",
      "projectPage": "https://github.com/aliyun/qwen-dianjin",
      "githubRepo": "https://github.com/aliyun/qwen-dianjin",
      "ai_summary": "A new multilingual, multi-sector, and multi-task benchmark, M³FinMeeting, evaluates large language models' performance in understanding financial meetings across different languages and industries.",
      "ai_keywords": [
        "large language models",
        "multilingual",
        "multi-sector",
        "multi-task",
        "benchmark",
        "financial meeting understanding",
        "Global Industry Classification Standard (GICS)",
        "summarization",
        "question-answer pair extraction",
        "question answering"
      ]
    },
    "publishedAt": "2025-06-03T02:41:09.000Z",
    "title": "M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial\n  Meeting Understanding Evaluation Dataset",
    "summary": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called M^3FinMeeting, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, M^3FinMeeting supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\nM^3FinMeeting includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\nM^3FinMeeting as a benchmark for assessing LLMs' financial meeting\ncomprehension skills.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02510.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642656cbad1e3b0e6e91b752",
      "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
      "fullname": "Jie Zhu",
      "name": "amazingj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24362",
      "authors": [
        {
          "_id": "68401045dd25841d998788cc",
          "user": {
            "_id": "61fbe8d2c5e6410373a76b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61fbe8d2c5e6410373a76b2a/m4yM6m2uagJxMselzEZfo.jpeg",
            "isPro": false,
            "fullname": "Anum Afzal",
            "user": "anumafzal94",
            "type": "user"
          },
          "name": "Anum Afzal",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T09:25:18.174Z",
          "hidden": false
        },
        {
          "_id": "68401045dd25841d998788cd",
          "name": "Florian Matthes",
          "hidden": false
        },
        {
          "_id": "68401045dd25841d998788ce",
          "user": {
            "_id": "6493393f357b252af72196c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6493393f357b252af72196c5/EWSy18XRcMRa_4XMM3Fu-.jpeg",
            "isPro": false,
            "fullname": "Gal Chechik",
            "user": "galchechik",
            "type": "user"
          },
          "name": "Gal Chechik",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-04T09:22:13.900Z",
          "hidden": false
        },
        {
          "_id": "68401045dd25841d998788cf",
          "user": {
            "_id": "66810e5877ed01ba880a4b40",
            "avatarUrl": "/avatars/3068f4b16f03a51772e652d76b37f9c3.svg",
            "isPro": false,
            "fullname": "Yftah Ziser",
            "user": "yziser",
            "type": "user"
          },
          "name": "Yftah Ziser",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-04T09:22:13.900Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T08:54:28.000Z",
      "submittedOnDailyAt": "2025-06-04T07:54:00.025Z",
      "title": "知りながら言う：LLM의 표현은 완성되기 전에 완료되기 전의 생각과 인식에 대한 성공적인 정보가 포함되어 있습니다.",
      "submittedOnDailyBy": {
        "_id": "61fbe8d2c5e6410373a76b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61fbe8d2c5e6410373a76b2a/m4yM6m2uagJxMselzEZfo.jpeg",
        "isPro": false,
        "fullname": "Anum Afzal",
        "user": "anumafzal94",
        "type": "user"
      },
      "summary": "우리는 Chain-of-Thought (CoT)의 0차원 프로세스가 완료되기 전에 예측이 가능한지 조사하는 것입니다. 우리는 LLM의 표현에 기반한 탐색 분류기가 토큰이 생성되기 전에도 우수한 성능을 보여주는 것을 발견하였으며, 이는 초기 단계의 표현에 이미 关于 reason의 과정에 대한 중요한 정보를 포함하고 있음을 보여줍니다. 반면, 강력한 BERT 기반의 baseline은 토큰을 생성하는 데만 의존하므로, 단순한 언어 코드에 의존하여, 더 깊은 reason의 동적에 기반한 더 낮은 성능을 보여주고 있습니다. 놀라울 정도로, 후속 reason의 단계를 사용하더라도 분류를 개선하는 것은 불가능합니다. 추가적인 컨텍스트가 도움이 되지 않을 경우, 초기 표현은 후속 표현과 유사하며, LLM은 초기에 키 정보를 인코딩하는 것을 보여줍니다. 이는 reason의 과정이 빨리 종료해도 손실이 없는 것을 의미합니다. 이를 검증하기 위해, 우리는 초기 중단 실험을 수행하여 CoT의 reason의 과정이 종료될 때까지 성능이 향상되는 것을 보여주지만, 완전한 reason의 과정에 비해 오류가 남아 있습니다. 그러나, CoT의锁를 줄이기 위한 서브젝트 훈련이나 강화학습과 같은 접근法是 우리의 분류기를 활용하여, 초기 중단이 효과적인지 인식할 수 있는 것을 가능하게 합니다. 우리가 발견한 것은 이러한 방법을 지원하는 것이 가능하며, CoT의 효율을 최적화하는 동시에 그 이익을 유지하는 데 도움을 줍니다.",
      "upvotes": 1,
      "discussionId": "68401045dd25841d998788f9"
    },
    "publishedAt": "2025-05-30T04:54:28.000Z",
    "title": "Knowing Before Saying: LLM Representations Encode Information About\n  Chain-of-Thought Success Before Completion",
    "summary": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT)\nprocess can be predicted before completion. We discover that a probing\nclassifier, based on LLM representations, performs well even before a\nsingle token is generated, suggesting that crucial information about the\nreasoning process is already present in the initial steps representations. In\ncontrast, a strong BERT-based baseline, which relies solely on the generated\ntokens, performs worse, likely because it depends on shallow linguistic cues\nrather than deeper reasoning dynamics. Surprisingly, using later reasoning\nsteps does not always improve classification. When additional context is\nunhelpful, earlier representations resemble later ones more, suggesting LLMs\nencode key information early. This implies reasoning can often stop early\nwithout loss. To test this, we conduct early stopping experiments, showing that\ntruncating CoT reasoning still improves performance over not using CoT at all,\nthough a gap remains compared to full reasoning. However, approaches like\nsupervised learning or reinforcement learning designed to shorten CoT chains\ncould leverage our classifier's guidance to identify when early stopping is\neffective. Our findings provide insights that may support such methods, helping\nto optimize CoT's efficiency while preserving its benefits.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24362.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61fbe8d2c5e6410373a76b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61fbe8d2c5e6410373a76b2a/m4yM6m2uagJxMselzEZfo.jpeg",
      "fullname": "Anum Afzal",
      "name": "anumafzal94",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24273",
      "authors": [
        {
          "_id": "683f21ed6ba11d78e3e383f6",
          "user": {
            "_id": "65f7c56fc6356b5cc5ab8245",
            "avatarUrl": "/avatars/ff8d3f3526e915f269274cc0dc5ca8ef.svg",
            "isPro": false,
            "fullname": "James Cai",
            "user": "jamescai20",
            "type": "user"
          },
          "name": "Hongyi James Cai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:01:32.571Z",
          "hidden": false
        },
        {
          "_id": "683f21ed6ba11d78e3e383f7",
          "name": "Junlin Wang",
          "hidden": false
        },
        {
          "_id": "683f21ed6ba11d78e3e383f8",
          "user": {
            "_id": "65d66cb2b06abf924b07ff76",
            "avatarUrl": "/avatars/de94e2fe07040b7dc3053bcaafa64ffe.svg",
            "isPro": false,
            "fullname": "Xiaoyin Chen",
            "user": "chenyn66",
            "type": "user"
          },
          "name": "Xiaoyin Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T16:25:17.851Z",
          "hidden": false
        },
        {
          "_id": "683f21ed6ba11d78e3e383f9",
          "name": "Bhuwan Dhingra",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T06:49:00.000Z",
      "submittedOnDailyAt": "2025-06-04T08:05:26.080Z",
      "title": "어떤 정도의 역산이 충분한가? LLM의 이유론을 향상시키기 위해 SFT와 RL의 상호작용을 고려하는 연구",
      "submittedOnDailyBy": {
        "_id": "65f7c56fc6356b5cc5ab8245",
        "avatarUrl": "/avatars/ff8d3f3526e915f269274cc0dc5ca8ef.svg",
        "isPro": false,
        "fullname": "James Cai",
        "user": "jamescai20",
        "type": "user"
      },
      "summary": "최근의 대언어 모델(LLMs)의 발전은 수학적 및 논리적 문제를 해결하는 능력을 크게 향상시키고, 정답이 명확히 있는 문제를 효과적으로 처리하는 데 특히 효과적이다. 특히, 정규적 미세 조정(SFT)과 강화학습(RL)의 기술이 사용되고 있다. 기존 연구에 따르면, RL은 탐색 전략을 포함하고, 긴 체인 오사인 소스(CoT)의 추론을 가능하게 하며, 백트래킹은 자연스럽게 학습되는 능력으로 나타난다. 그러나, 백트래킹의 구체적인 이점, 특히 이를 추론의 향상에 미치는 영향과 가장 적절한 사용 범위는 아직 이해되지 않는다. 본 연구에서는, SFT와 RL 사이의 동적인 변화를 조사하고, 8가지 이유 태스크(Countdown, Sudoku, Arc 1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, Self Reference)에 대한 실험을 수행하였다. 결과적으로, SFT에서 짧은 CoT 시퀀스는 냉정한 RL과 비교하여 중度的 기여를 제공하지만, 이 기여는 태스크가 어려워질 때 감소함을 알 수 있었다. 이 전망에 기반하여, 백트래킹의 단계수를 체계적으로 변경한 합성 데이터 세트를 구축하고, 정확성(내용)이나 구조(백트래킹의 빈도)의 영향을 개별적으로 조사하였다. 결과적으로, 다음과 같은 것을 알 수 있었다. (1) 백트래킹을 포함하는 긴 CoT은 일반적으로 더 좋은 및 안정적인 RL의 학습을 불러 일으킨다. (2) 더 어려운 문제 및 큰 탐색 공간을 가진 문제는 SFT 단계에서 백트래킹의 수가 높게 되는 경우가 많습니다. 또한, 제한된 데이터에 대한 실험에서, RL의 학습은 긴 CoT 시퀀스의 정확성에 크게 영향을 받지 않고, RL은 구조적 패턴보다 내용의 정확성을 우선시하는 것을 알 수 있었다. 총적으로, 본 연구의 결과는 LLMs의 추론의 효과적인 스케일링에 가장 적합한 훈련 전략의 설계에 실질적인 조언을 제공함을 알 수 있다.",
      "upvotes": 1,
      "discussionId": "683f21ed6ba11d78e3e38434",
      "ai_summary": "This study investigates the interplay between supervised fine-tuning and reinforcement learning in large language models, focusing on the role of backtracking in enhancing reasoning capabilities across various tasks.",
      "ai_keywords": [
        "supervised finetuning",
        "reinforcement learning",
        "chain-of-thought",
        "backtracking",
        "countdown",
        "sudoku",
        "arc 1d",
        "geometry",
        "color cube rotation",
        "list functions",
        "zebra puzzles",
        "self reference",
        "synthetic datasets",
        "distilled data"
      ]
    },
    "publishedAt": "2025-05-30T02:49:00.000Z",
    "title": "How Much Backtracking is Enough? Exploring the Interplay of SFT and RL\n  in Enhancing LLM Reasoning",
    "summary": "Recent breakthroughs in large language models (LLMs) have effectively\nimproved their reasoning abilities, particularly on mathematical and logical\nproblems that have verifiable answers, through techniques such as supervised\nfinetuning (SFT) and reinforcement learning (RL). Prior research indicates that\nRL effectively internalizes search strategies, enabling long chain-of-thought\n(CoT) reasoning, with backtracking emerging naturally as a learned capability.\nHowever, the precise benefits of backtracking, specifically, how significantly\nit contributes to reasoning improvements and the optimal extent of its use,\nremain poorly understood. In this work, we systematically investigate the\ndynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc\n1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self\nReference. Our findings highlight that short CoT sequences used in SFT as a\nwarm-up do have moderate contribution to RL training, compared with cold-start\nRL; however such contribution diminishes when tasks become increasingly\ndifficult. Motivated by this observation, we construct synthetic datasets\nvarying systematically in the number of backtracking steps and conduct\ncontrolled experiments to isolate the influence of either the correctness\n(content) or the structure (i.e., backtrack frequency). We find that (1) longer\nCoT with backtracks generally induce better and more stable RL training, (2)\nmore challenging problems with larger search space tend to need higher numbers\nof backtracks during the SFT stage. Additionally, we demonstrate through\nexperiments on distilled data that RL training is largely unaffected by the\ncorrectness of long CoT sequences, suggesting that RL prioritizes structural\npatterns over content correctness. Collectively, our results offer practical\ninsights into designing optimal training strategies to effectively scale\nreasoning in LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24273.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f7c56fc6356b5cc5ab8245",
      "avatarUrl": "/avatars/ff8d3f3526e915f269274cc0dc5ca8ef.svg",
      "fullname": "James Cai",
      "name": "jamescai20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18079",
      "authors": [
        {
          "_id": "68354eec0830dfc6782ba1c4",
          "name": "Xiaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c5",
          "name": "Zhaoyang Jia",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c6",
          "name": "Zongyu Guo",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c7",
          "name": "Jiahao Li",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c8",
          "name": "Bin Li",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c9",
          "name": "Houqiang Li",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1ca",
          "name": "Yan Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T16:37:36.000Z",
      "submittedOnDailyAt": "2025-06-04T07:20:42.262Z",
      "title": "딥 비디오 탐색: Agentic Search for Long-form Video including Tool Usage\n\nUnderstanding\n\n이 번역은 전문성과 정확성을 유지하며 제공됩니다.",
      "submittedOnDailyBy": {
        "_id": "6582b79aafc6b50a2cbaa5c8",
        "avatarUrl": "/avatars/65fa21ca144177f232347084b0e057c5.svg",
        "isPro": false,
        "fullname": "Xiaoyi Zhang",
        "user": "xyzhang626",
        "type": "user"
      },
      "summary": "장기 비디오 이해는 시간적, 공간적 복잡성과 장기적인 컨텍스트에서 질문에 대한 답변의 어려움으로 인해 중대한 문제를 직면하고 있습니다. 비디오 분석 능력과 장기 컨텍스트 처리에 상당한 진전을 보여주는 대규모 언어 모델(LLM)은 정보 밀도가 높은 1시간 길이의 비디오 처리에서도 한계가 있습니다. 이러한 한계를 극복하기 위해, 우리는 Deep Video Discovery agent를 제안합니다. 이전 비디오 에이전트는 刚性한 작업 흐름을 설계하여手動으로 작업하는 반면, 우리의 접근 방식은 에이전트의 자율성을 강조합니다. 다중粒성의 비디오 데이터베이스에서 검색 중심의 도구를 제공함으로써, DVD agent는 LLM의 先進的推理能力을 활용하여, 현재의 관측 상태로부터 계획을 세우고, 전략적으로 도구를 선택하고, 행동의 적절한 파라미터를 설정하고, 수집된 정보에 기반하여 내부적인 추론을 반복적으로 조정합니다. 장기 비디오 이해 벤치마크에서 세부적인 평가를 수행하여 시스템 전체의 설계의 우수점을 보여주고 있습니다. DVD agent는 어려운 LVBench 데이터 세트에서 先駆的한 성능을 달성하고, 기존 연구를 크게 초월합니다. 또한, 세부적인 소멸 테스트와 도구 분석을 제공하여, 장기 비디오 이해 태스크에 적합한 스마트 에이전트의 발전에 연결된 피드백을 제공합니다. 코드는 이후에 릴리즈됩니다.",
      "upvotes": 1,
      "discussionId": "68354eed0830dfc6782ba1fe",
      "ai_summary": "The Deep Video Discovery agent uses an autonomous agentic search strategy with large language models to overcome limitations in long-form video understanding, achieving state-of-the-art results on benchmarks like LVBench.",
      "ai_keywords": [
        "Deep Video Discovery agent",
        "agentic search strategy",
        "segmented video clips",
        "multi-granular video database",
        "advanced reasoning capability",
        "LLM",
        "autonomous nature",
        "observation state",
        "search-centric tools",
        "internal reasoning",
        "long video understanding benchmarks",
        "LVBench",
        "ablation studies",
        "tool analyses"
      ]
    },
    "publishedAt": "2025-05-23T12:37:36.000Z",
    "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding",
    "summary": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code will be released later.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6582b79aafc6b50a2cbaa5c8",
      "avatarUrl": "/avatars/65fa21ca144177f232347084b0e057c5.svg",
      "fullname": "Xiaoyi Zhang",
      "name": "xyzhang626",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02138",
      "authors": [
        {
          "_id": "683fe27c4f32bd7bbca087fc",
          "name": "Yarden Bakish",
          "hidden": false
        },
        {
          "_id": "683fe27c4f32bd7bbca087fd",
          "name": "Itamar Zimerman",
          "hidden": false
        },
        {
          "_id": "683fe27c4f32bd7bbca087fe",
          "name": "Hila Chefer",
          "hidden": false
        },
        {
          "_id": "683fe27c4f32bd7bbca087ff",
          "name": "Lior Wolf",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T18:07:55.000Z",
      "submittedOnDailyAt": "2025-06-04T04:51:22.070Z",
      "title": "대체된 LRP：위치付属性화는 트랜스포머의 설명성에 부족한 결함의 보완 역할을 합니다.",
      "submittedOnDailyBy": {
        "_id": "65376feed325b3f02fb92c69",
        "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
        "isPro": false,
        "fullname": "Itamar Zimerman",
        "user": "ItamarZ",
        "type": "user"
      },
      "summary": "Transformers의 효율적인 설명성 도구의 개발은 심층 학습 연구의 중요한 과제입니다. 이 분야에서 가장 바람직한 접근 방식 중 하나는 레이어 웨이스 리레바ン스 프로パ게션 (LRP)입니다. LRP는 사전 정의된 규칙에 따라 활성 값을 재배분하고, 관련 점수를 네트워크에서 입력 공간으로 역전파하는 것입니다. 그러나 현재 Transformer의 설명성에 대한 LRP 기반의 방법들은 Transformer 아키텍처의 중요한 구성 요소인 위치 인코딩 (PE)을 완전히 무시하고 있습니다. 이는 보존성 파괴와 구조적 및 위치적 특성에 대한 중요한 관련성을 잃는 것입니다. 이러한 제한을 해결하기 위해, Transformer의 설명성을 입력 공간으로 위치와 토큰의 쌍의 집합으로 재설정합니다. 이를 통해, Rotary, Learnable, Absolute PE 등 다양한 위치 인코딩 방법에 대한 특성의 전파를 목표로 하는 특화된 이론적인 LRP 규칙을 제안할 수 있습니다. 미세 조정된 분류기 및 0-shot 기반 모델 (예: LLaMA 3)에 대한 확장된 실험은 우리의 방법론이 시각 및 NLP 설명성 작업의 두 분야에서 가장 선진한 방법과 비교하여 뚜렷하게 뛰어넘는 것을 보여주고 있습니다. 우리의 코드는 공개적으로 사용 가능합니다.",
      "upvotes": 0,
      "discussionId": "683fe27d4f32bd7bbca08867",
      "githubRepo": "https://github.com/YardenBakish/PE-AWARE-LRP",
      "ai_summary": "A specialized LRP method for Transformer explainability considers positional encoding, improving relevance propagation and outperforming existing methods.",
      "ai_keywords": [
        "Layer-wise Relevance Propagation (LRP)",
        "Transformers",
        "positional encoding (PE)",
        "Rotary",
        "Learnable",
        "Absolute PE",
        "vision",
        "NLP explainability tasks"
      ]
    },
    "publishedAt": "2025-06-02T14:07:55.000Z",
    "title": "Revisiting LRP: Positional Attribution as the Missing Ingredient for\n  Transformer Explainability",
    "summary": "The development of effective explainability tools for Transformers is a\ncrucial pursuit in deep learning research. One of the most promising approaches\nin this domain is Layer-wise Relevance Propagation (LRP), which propagates\nrelevance scores backward through the network to the input space by\nredistributing activation values based on predefined rules. However, existing\nLRP-based methods for Transformer explainability entirely overlook a critical\ncomponent of the Transformer architecture: its positional encoding (PE),\nresulting in violation of the conservation property, and the loss of an\nimportant and unique type of relevance, which is also associated with\nstructural and positional features. To address this limitation, we reformulate\nthe input space for Transformer explainability as a set of position-token\npairs. This allows us to propose specialized theoretically-grounded LRP rules\ndesigned to propagate attributions across various positional encoding methods,\nincluding Rotary, Learnable, and Absolute PE. Extensive experiments with both\nfine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,\ndemonstrate that our method significantly outperforms the state-of-the-art in\nboth vision and NLP explainability tasks. Our code is publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02138.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65376feed325b3f02fb92c69",
      "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
      "fullname": "Itamar Zimerman",
      "name": "ItamarZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01565",
      "authors": [
        {
          "_id": "684009f8a33aeee1125b5765",
          "name": "Li Zhou",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b5766",
          "name": "Lutong Yu",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b5767",
          "name": "Dongchu Xie",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b5768",
          "name": "Shaohuan Cheng",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b5769",
          "name": "Wenyan Li",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b576a",
          "name": "Haizhou Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T11:43:46.000Z",
      "submittedOnDailyAt": "2025-06-04T07:35:11.966Z",
      "title": "한프바넨치： 시대를 초월하는 문화 이해와 다모형 번역 재제작의 벤치마크\n\n(注意：此处的翻译假设“한프바넨치”是正确的韩文转写，如果原文中的“汉弗巴尼奇”是正确的英文拼写，则应相应地调整为“한프바니치”)",
      "submittedOnDailyBy": {
        "_id": "619b506f70d03780cbec5806",
        "avatarUrl": "/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg",
        "isPro": false,
        "fullname": "wenyan li",
        "user": "lyan62",
        "type": "user"
      },
      "summary": "문화는 지역과 시간의 양쪽 방향으로 변화하며 풍부하고 동적인 영역입니다. 그러나 현재의 Vision Language Models (VLMs)를 기반으로 한 문화 이해 연구는 주로 지역적 다양성을 강조하지만, 시간적 중요성을 간과하고 있습니다. 이러한 공백을 메우기 위해, 우리는 \"한복 벤치\"라는 새로운, 전문가가 편집한 다양성 데이터 세트를 소개합니다. 한복은 고대 중국 시대를 거친 전통적인 옷で, 중국의 시간적 문화적 면을 반영하고, 현대 중국 사회에서도 높은 인기를 유지하고 있습니다. 한복 벤치는 문화적 시각적 이해와 문화적 이미지의 재설계의 두 가지 핵심 태스크를 포함합니다. 첫 번째 태스크는 단일 또는 복수 이미지의 입력에 기반하여 시간적·문화적 특징 인식을 평가하고, 시각적 질문에 따라 선택을 수행하여 조사됩니다. 두 번째는 문화적 요소를 이어가며 현대적 맥락에 적응하여 전통적인 옷을 현대의 디자인으로 전환하는 데 중점을 두고 있습니다. 우리의 평가에 따르면, 폐쇄된 VLMs은 시각적 문화 이해에서 비전문가와 비교하여 상대적으로 낮은 성능을 보입니다. 비전문가보다 더 낮은 성능을 보입니다. 개방된 VLMs은 비전문가보다 더 낮은 성능을 보입니다. 태스크의 재설계는 다양한 인간 평가에 따라, 가장 높은 성능을 나타내는 모델의 성공률이 42%였습니다. 우리의 벤치마크는 시간적 문화 이해와 창의적 적응의 새로운 방향에 대한 중요한 테스트를 제공하며, 이 분야에서 큰 도전을 명확히 합니다.",
      "upvotes": 0,
      "discussionId": "684009faa33aeee1125b57bd",
      "githubRepo": "https://github.com/lizhou21/TemporalCulture"
    },
    "publishedAt": "2025-06-02T07:43:46.000Z",
    "title": "Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural\n  Understanding and Transcreation",
    "summary": "Culture is a rich and dynamic domain that evolves across both geography and\ntime. However, existing studies on cultural understanding with vision-language\nmodels (VLMs) primarily emphasize geographic diversity, often overlooking the\ncritical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a\nnovel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning\nancient Chinese dynasties, serves as a representative cultural heritage that\nreflects the profound temporal aspects of Chinese culture while remaining\nhighly popular in Chinese contemporary society. Hanfu-Bench comprises two core\ntasks: cultural visual understanding and cultural image transcreation.The\nformer task examines temporal-cultural feature recognition based on single- or\nmulti-image inputs through multiple-choice visual question answering, while the\nlatter focuses on transforming traditional attire into modern designs through\ncultural element inheritance and modern context adaptation. Our evaluation\nshows that closed VLMs perform comparably to non-experts on visual cutural\nunderstanding but fall short by 10\\% to human experts, while open VLMs lags\nfurther behind non-experts. For the transcreation task, multi-faceted human\nevaluation indicates that the best-performing model achieves a success rate of\nonly 42\\%. Our benchmark provides an essential testbed, revealing significant\nchallenges in this new direction of temporal cultural understanding and\ncreative adaptation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01565.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619b506f70d03780cbec5806",
      "avatarUrl": "/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg",
      "fullname": "wenyan li",
      "name": "lyan62",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]