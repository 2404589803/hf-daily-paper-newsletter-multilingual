[
  {
    "paper": {
      "id": "2505.21115",
      "authors": [
        {
          "_id": "68372d97e4af3c39dcec8e65",
          "user": {
            "_id": "5dfa8e07da6d0311fd3d5430",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651090418656-5dfa8e07da6d0311fd3d5430.png",
            "isPro": false,
            "fullname": "Sergey Pletenev",
            "user": "memyprokotow",
            "type": "user"
          },
          "name": "Sergey Pletenev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:55.604Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e66",
          "user": {
            "_id": "660ee18e2dcd816ad14b3739",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
            "isPro": false,
            "fullname": "Maria Marina",
            "user": "zlatamaria",
            "type": "user"
          },
          "name": "Maria Marina",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:59.278Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e67",
          "user": {
            "_id": "6682607ece294ddc5e72f4fb",
            "avatarUrl": "/avatars/2a304bc3eb56ec7d13297d28fbb062ae.svg",
            "isPro": false,
            "fullname": "Ivanov",
            "user": "VirVen",
            "type": "user"
          },
          "name": "Nikolay Ivanov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:48:42.811Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e68",
          "name": "Daria Galimzianova",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e69",
          "user": {
            "_id": "643010b2ff56d6c2004699a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/OYsf1Hp-KAievw_M8XBG9.jpeg",
            "isPro": false,
            "fullname": "Krayko Nikita",
            "user": "nakrayko",
            "type": "user"
          },
          "name": "Nikita Krayko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:53.523Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6a",
          "name": "Mikhail Salnikov",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6b",
          "name": "Vasily Konovalov",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6c",
          "name": "Alexander Panchenko",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6d",
          "user": {
            "_id": "63bbfd74141c7d395c471768",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
            "isPro": false,
            "fullname": "Viktor Moskvoretskii",
            "user": "VityaVitalich",
            "type": "user"
          },
          "name": "Viktor Moskvoretskii",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:57.325Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T12:35:13.000Z",
      "submittedOnDailyAt": "2025-06-09T07:27:12.232Z",
      "title": "내일도 똑같을까요? 다언어 영극 문제\n신뢰성 있는 QA의 개선을 위한 분류\n\n이 번역은 전문성과 정확성을 유지하였습니다.",
      "submittedOnDailyBy": {
        "_id": "660ee18e2dcd816ad14b3739",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
        "isPro": false,
        "fullname": "Maria Marina",
        "user": "zlatamaria",
        "type": "user"
      },
      "summary": "대 언어 모델（LLMs）는 자주 '하로케샹'（即，输出与事实或信息不同的内容）를 수행합니다. 이 현상은 중요한 요소로만 아니라, 아직 조사가 부족한 원인 중 하나는 문제를 파악하는 시간적 특성입니다. 이는 답이 시간이 지나도 변하지 않는 '영록형'（evergreen）과 답이 시간에 따라 변하는 '변형형'（mutable）으로 나뉩니다. 본 연구에서는, EverGreenQA라는 첫 번째 다언어 QA 데이터 세트를 소개하고, 영록형 라벨을 지원하여 평가와 훈련을 수행할 수 있습니다. EverGreenQA를 사용하여, 12개의 현대 LLMs를 벤치마크로 평가하고, 이들이 문제를 명확히 표현하는 시간적 특성을 (어미적 판단에 따라) 또는 숨겨서 (불확실성 신호에 따라) 표현하는지 평가합니다. 또한, EG-E5라는 가벼운 다언어 분류기를 훈련하여, 이 태스크에서 가장 先端의 성능을 달성합니다. 마지막으로, 영록형 분류의 실용적인 효용을 보여주는 3가지 애플리케이션으로, 自知覚의 추정을 개선하고, QA 데이터 세트를 필터링하고, GPT-4o의 검색 행동을 설명합니다.",
      "upvotes": 41,
      "discussionId": "68372d98e4af3c39dcec8e88",
      "githubRepo": "https://github.com/s-nlp/Evergreen-classification",
      "ai_summary": "EverGreenQA, a multilingual QA dataset with evergreen labels, is introduced to benchmark LLMs on temporality encoding and assess their performance through verbalized judgments and uncertainty signals.",
      "ai_keywords": [
        "Large Language Models",
        "QA",
        "evergreen",
        "mutable",
        "temporality",
        "Multilingual QA dataset",
        "EG-E5",
        "lightweight multilingual classifier",
        "SoTA performance",
        "self-knowledge estimation",
        "filtering QA datasets",
        "GPT-4o retrieval behavior"
      ]
    },
    "publishedAt": "2025-05-27T08:35:13.000Z",
    "title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question\n  Classification to Improve Trustworthy QA",
    "summary": "Large Language Models (LLMs) often hallucinate in question answering (QA)\ntasks. A key yet underexplored factor contributing to this is the temporality\nof questions -- whether they are evergreen (answers remain stable over time) or\nmutable (answers change). In this work, we introduce EverGreenQA, the first\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\nencode question temporality explicitly (via verbalized judgments) or implicitly\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\nthe practical utility of evergreen classification across three applications:\nimproving self-knowledge estimation, filtering QA datasets, and explaining\nGPT-4o retrieval behavior.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21115.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "660ee18e2dcd816ad14b3739",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
      "fullname": "Maria Marina",
      "name": "zlatamaria",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01111",
      "authors": [
        {
          "_id": "6845b6a33ec10bdd8ab4da1b",
          "name": "Shunian Chen",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1c",
          "user": {
            "_id": "66440e86bfe15e84d369cb03",
            "avatarUrl": "/avatars/d15b3b3831bc74138206071612169f64.svg",
            "isPro": false,
            "fullname": "Xinyuan Xie",
            "user": "SatsukiVie",
            "type": "user"
          },
          "name": "Xinyuan Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:42.759Z",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1d",
          "name": "Zheshu Chen",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1e",
          "name": "Liyan Zhao",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1f",
          "name": "Owen Lee",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da20",
          "name": "Zhan Su",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da21",
          "name": "Qilin Sun",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da22",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T18:29:17.000Z",
      "submittedOnDailyAt": "2025-06-09T01:59:54.914Z",
      "title": "FusionAudio-1.2M: 다중모달 컨텍스트 합치기를 통해 미세한 음성 캡처를 위한 대안\n\n(注意：虽然要求不添加额外的文本，但为了确保翻译的准确性和专业性，这里对原文中的“向こうへ”进行了适当的调整，以符合韩语表达习惯。)",
      "submittedOnDailyBy": {
        "_id": "623be9e1d1eb227788764959",
        "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
        "isPro": false,
        "fullname": "Shunian Chen",
        "user": "Shunian",
        "type": "user"
      },
      "summary": "고품질의 대형 음성 캡처는 음성 이해의 발전에 중요하지만, 현재의 자동화 방법은 미세한 세부 사항과 맥락의 정확도를 떨어뜨리는 많은 캡처를 생성합니다. 이는 제한된 단일 모드나 표면적인 다중 모드 정보의 관계에 기인합니다. 인간의 청각 인식으로부터 영감을 얻고, 다양한 코스를 적절히 통합하여 복잡한 음성 스케너를 분석하는 고급 청각 인식을 실현하기 위해, 새로운 2단계의 자동화 파이프라인을 도입합니다. 이 파이프라인은 특별한 학습된 모델을 사용하여 다양한 맥락 코스(예: 대화음, 음악, 일반적인 소리, 관련 영상으로부터의 시각 정보)를 추출합니다. 그 후, 대형 언어 모델(LLM)은 이러한 풍부한 다중 모드 입력을 조합하여, 상세한 맥락에 관련된 음성 캡처를 생성합니다. 이 연구의 주요 기여는 다음과 같습니다: (1) 제안된 scalable한 미세한 음성 캡처 생성 방법, (2) 기능 추가, 120만 개의 이러한 세부적인 캡처를 포함하는 새로운 대형 데이터 세트, 600만 쌍의 QA 페어를 결합한 것, (3) 기능 추가를 사용하여 음성 모델의 향상, 특히 CLAP 기반의 음성 인코더의 음성-문서의 대응과 지시 따라임을 개선한 것입니다. 이 논문은 복잡한 음성 환경에서 정확한 자동화 이해에 대한 플레이어를 제시합니다. 코드와 데이터는 https://github.com/satsuki2486441738/FusionAudio에 찾을 수 있습니다.",
      "upvotes": 21,
      "discussionId": "6845b6a43ec10bdd8ab4da23",
      "githubRepo": "https://github.com/FreedomIntelligence/FusionAudio",
      "ai_summary": "A novel two-stage pipeline using specialized pretrained models and a large language model enhances audio caption quality by integrating diverse multimodal cues and contextual information.",
      "ai_keywords": [
        "audio captioning",
        "auditory perception",
        "auditory scene analysis",
        "pretrained models",
        "large language model",
        "FusionAudio",
        "CLAP-based audio encoder",
        "audio-text alignment",
        "instruction following"
      ]
    },
    "publishedAt": "2025-06-01T14:29:17.000Z",
    "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion",
    "summary": "High-quality, large-scale audio captioning is crucial for advancing audio\nunderstanding, yet current automated methods often generate captions that lack\nfine-grained detail and contextual accuracy, primarily due to their reliance on\nlimited unimodal or superficial multimodal information. Drawing inspiration\nfrom human auditory perception, which adeptly integrates cross-modal cues and\nperforms sophisticated auditory scene analysis, we introduce a novel two-stage\nautomated pipeline. This pipeline first employs specialized pretrained models\nto extract diverse contextual cues (e.g., speech, music, general sounds, and\nvisual information from associated video). A large language model (LLM) then\nsynthesizes these rich, multimodal inputs to generate detailed and\ncontext-aware audio captions. Key contributions of this work include: (1) the\nproposed scalable method for fine-grained audio caption generation; (2)\nFusionAudio, a new large-scale dataset comprising 1.2 million such detailed\ncaptions, combined with 6 million QA pairs; and (3) enhanced audio models\ndeveloped using FusionAudio, specifically a CLAP-based audio encoder with\nsuperior audio-text alignment and instruction following. This paper paves the\nway for more nuanced and accurate automated understanding of complex audio\nenvironments. Code and data can be found in\nhttps://github.com/satsuki2486441738/FusionAudio.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01111.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623be9e1d1eb227788764959",
      "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
      "fullname": "Shunian Chen",
      "name": "Shunian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05984",
      "authors": [
        {
          "_id": "68463ee43ec10bdd8ab4da6f",
          "user": {
            "_id": "622326ae0129f2097d69a3e2",
            "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
            "isPro": false,
            "fullname": "Cheng-Han Chiang",
            "user": "dcml0714",
            "type": "user"
          },
          "name": "Cheng-Han Chiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:26.041Z",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da70",
          "user": {
            "_id": "64dc191bc307ee5369fbcb04",
            "avatarUrl": "/avatars/5a8a0db63a187e85d4ae2fff93a838f0.svg",
            "isPro": false,
            "fullname": "Xiaofei Wang",
            "user": "xiaofei-wang",
            "type": "user"
          },
          "name": "Xiaofei Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T01:54:46.319Z",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da71",
          "name": "Chung-Ching Lin",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da72",
          "name": "Kevin Lin",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da73",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da74",
          "name": "Radu Kopetz",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da75",
          "name": "Yao Qian",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da76",
          "name": "Zhendong Wang",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da77",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da78",
          "name": "Hung-yi Lee",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da79",
          "name": "Lijuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T11:05:48.000Z",
      "submittedOnDailyAt": "2025-06-09T00:28:11.753Z",
      "title": "음성 관련 큰 언어 모델이 대화 방식의 판결 역할을 수행합니다.",
      "submittedOnDailyBy": {
        "_id": "622326ae0129f2097d69a3e2",
        "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
        "isPro": false,
        "fullname": "Cheng-Han Chiang",
        "user": "dcml0714",
        "type": "user"
      },
      "summary": "음성 관련의 대규모 언어 모델(ALLMs)은 음성 입력의 문자적 및 비문자적 정보에 대해 이해할 수 있습니다. 본 논문에서는 ALLMs를 자동 판단자로 하여 연설의 표현을 평가하는 것을 조사합니다. SLMs가 생성된 연설을 평가하기 위해 ALLM의 판단자를 사용합니다. 이 평가에는 음성의 스타일의 수행과 역할 연기의 2가지 태스크를 사용합니다. 평가하는 표현은 감정, 음량, 말속도, 단어의 중량, 피치의 제어, 비언어 요소 등 포함됩니다. 4가지 언어 모델(SLMs)을 사용하여 이러한 2가지 태스크를 수행하고, 인간과 ALLMs가 SLMs의 응답을 판단합니다. GPT-4o-audio와 Gemini-2.5-pro의 2가지 ALLM의 판단자를 인간의 평가 결과를 비교하며, Gemini와 인간의 판단자의 일치율은 인간 평가자 간의 일치율과 비교하여 상대적으로 높습니다. 이러한 기대되는 결과를 통해 ALLMs는 SLMs의 평가에 사용할 수 있는 판단자로 사용할 수 있음을 보여주었습니다. 또한 현재의 SLMs, 특히 GPT-4o-audio에서도 표현의 제어와 자연스러운 디ア로기 생성에 개선의 여지가 분명히 나타납니다.",
      "upvotes": 9,
      "discussionId": "68463ee43ec10bdd8ab4da7a",
      "ai_summary": "Audio-aware large language models can assess speaking styles in audio inputs, demonstrating performance comparable to human judges in evaluating synthesized speech along dimensions like emotion, volume, and pitch.",
      "ai_keywords": [
        "audio-aware large language models",
        "ALLMs",
        "speaking styles",
        "SLMs",
        "voice style instruction",
        "role-playing",
        "emotion",
        "volume",
        "speaking pace",
        "word emphasis",
        "pitch control",
        "non-verbal elements",
        "GPT-4o-audio",
        "Gemini-2.5-pro",
        "human evaluation",
        "agreement",
        "speaking style control",
        "natural dialogues"
      ]
    },
    "publishedAt": "2025-06-06T07:05:48.000Z",
    "title": "Audio-Aware Large Language Models as Judges for Speaking Styles",
    "summary": "Audio-aware large language models (ALLMs) can understand the textual and\nnon-textual information in the audio input. In this paper, we explore using\nALLMs as an automatic judge to assess the speaking styles of speeches. We use\nALLM judges to evaluate the speeches generated by SLMs on two tasks: voice\nstyle instruction following and role-playing. The speaking style we consider\nincludes emotion, volume, speaking pace, word emphasis, pitch control, and\nnon-verbal elements. We use four spoken language models (SLMs) to complete the\ntwo tasks and use humans and ALLMs to judge the SLMs' responses. We compare two\nALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and\nshow that the agreement between Gemini and human judges is comparable to the\nagreement between human evaluators. These promising results show that ALLMs can\nbe used as a judge to evaluate SLMs. Our results also reveal that current SLMs,\neven GPT-4o-audio, still have room for improvement in controlling the speaking\nstyle and generating natural dialogues.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622326ae0129f2097d69a3e2",
      "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
      "fullname": "Cheng-Han Chiang",
      "name": "dcml0714",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05629",
      "authors": [
        {
          "_id": "68464b273ec10bdd8ab4da86",
          "user": {
            "_id": "64a6518132cf858d6386ac52",
            "avatarUrl": "/avatars/4cabf3dab8b1ba06245ad8024f334181.svg",
            "isPro": false,
            "fullname": "Ananth Muppidi",
            "user": "ananthmuppidi",
            "type": "user"
          },
          "name": "Ananth Muppidi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T02:47:03.971Z",
          "hidden": false
        },
        {
          "_id": "68464b273ec10bdd8ab4da87",
          "user": {
            "_id": "5f89da6c5d083370c711f37c",
            "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
            "isPro": false,
            "fullname": "Abhilash Nandy",
            "user": "abhi1nandy2",
            "type": "user"
          },
          "name": "Abhilash Nandy",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-09T03:49:29.446Z",
          "hidden": false
        },
        {
          "_id": "68464b273ec10bdd8ab4da88",
          "user": {
            "_id": "65238ea295df08170c93933d",
            "avatarUrl": "/avatars/8364301e324274a550d12f2b184ea10e.svg",
            "isPro": false,
            "fullname": "Sambaran Bandyopadhyay",
            "user": "sambaran",
            "type": "user"
          },
          "name": "Sambaran Bandyopadhyay",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T02:47:03.971Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T23:13:22.000Z",
      "submittedOnDailyAt": "2025-06-09T01:27:41.831Z",
      "title": "环评生态化推广计划",
      "submittedOnDailyBy": {
        "_id": "5f89da6c5d083370c711f37c",
        "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
        "isPro": false,
        "fullname": "Abhilash Nandy",
        "user": "abhi1nandy2",
        "type": "user"
      },
      "summary": "특정 분야의 태스크에서 수행할 수 있는 대규모 언어 모델의 성능에 필요한 미세 조정은 계산적으로 비용이 높고 기술적으로 어려워서이다. 본 논문은 작은 파라미터 세트를 학습하여 프리트레이드 모델을 다운스트리م 태스크에 적응하기 위해 가능한 소프트 프로닝을 활용한 파라미터 효율적인 미세 조정에 초점을 맞추고 있다. 우리는 입력 토큰에 기반한 소프트 프로닝을 생성하고 중요도가 다른 토큰에 대해 다른 처리를 수행하기 위해 새로운 입력 의존 소프트 프로닝 방법(ID-SPAM)을 제안한다. 우리의 방법은 훈련 파라미터의 수를 줄이면서 간단하고 효율적이다. 우리는 제안된 방법과 비교하여 최신 기술에 대한 이점을 보여주고, 개선된 0 shot 영역 이동 능력을 보여주고 있다.",
      "upvotes": 9,
      "discussionId": "68464b273ec10bdd8ab4da89",
      "ai_summary": "A new method using input-dependent soft prompting with a self-attention mechanism improves parameter-efficient fine-tuning for large language models, enhancing zero-shot domain transfer.",
      "ai_keywords": [
        "soft prompting",
        "parameter-efficient fine-tuning",
        "pre-trained models",
        "downstream tasks",
        "Input Dependent Soft Prompting technique",
        "self-Attention Mechanism",
        "zero shot domain transfer"
      ]
    },
    "publishedAt": "2025-06-05T19:13:22.000Z",
    "title": "Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs",
    "summary": "The performance of large language models in domain-specific tasks\nnecessitates fine-tuning, which is computationally expensive and technically\nchallenging. This paper focuses on parameter-efficient fine-tuning using soft\nprompting, a promising approach that adapts pre-trained models to downstream\ntasks by learning a small set of parameters. We propose a novel Input Dependent\nSoft Prompting technique with a self-Attention Mechanism (ID-SPAM) that\ngenerates soft prompts based on the input tokens and attends different tokens\nwith varying importance. Our method is simple and efficient, keeping the number\nof trainable parameters small. We show the merits of the proposed approach\ncompared to state-of-the-art techniques on various tasks and show the improved\nzero shot domain transfer capability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05629.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f89da6c5d083370c711f37c",
      "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
      "fullname": "Abhilash Nandy",
      "name": "abhi1nandy2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01872",
      "authors": [
        {
          "_id": "683e77d41417d107337abf6e",
          "user": {
            "_id": "643f9e2288d9d4488fd81c52",
            "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
            "isPro": false,
            "fullname": "Tinghui Zhu",
            "user": "DarthZhu",
            "type": "user"
          },
          "name": "Tinghui Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:47.572Z",
          "hidden": false
        },
        {
          "_id": "683e77d41417d107337abf6f",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "683e77d41417d107337abf70",
          "name": "Muhao Chen",
          "hidden": false
        },
        {
          "_id": "683e77d41417d107337abf71",
          "name": "Yu Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:01:40.000Z",
      "submittedOnDailyAt": "2025-06-09T07:51:42.399Z",
      "title": "확장 모델 디시젼은 모든 모델 디시젼에 대한 길을 맞는지 정확한가?",
      "submittedOnDailyBy": {
        "_id": "643f9e2288d9d4488fd81c52",
        "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
        "isPro": false,
        "fullname": "Tinghui Zhu",
        "user": "DarthZhu",
        "type": "user"
      },
      "summary": "다모달 언어 모델(OLMs)는 텍스트, 이미지, 영상, 음성 등 다양한 입력 모달리티에 대해 통합하여 설명을 제공하며, 강력한 언어 능력을 유지하는 것을 목표로 합니다. 최근의 발전에도 불구하고, 현재의 모델, 특히 오픈 소스 모델은 진정한 다모달성을 달성하기까지 멀리 떨어져 있습니다. 특히, 특정 모달리티 쌍에 대한 훈련으로 배워진 모달리티 쌍을 초과하여 일반화하거나, 다모달 입력을 처리하여 강력한 성능을 달성하는 것이 어려워질 때도 있습니다. 여기서는, 훈련 모델의 주요 방법 중 모달리티 확장의 효과를 조사합니다. 구체적으로, 모달리티 확장이 핵심적인 언어 능력을 보완하는지, 독립적으로 훈련된 모달리티专用 모델을 효과적으로 통합할 수 있는지, 그리고 이러한 효과가 순차적 확장에 비해 더 좋은 지식 공유와 일반화에서 실현되는지 조사합니다. 상세한 실험을 통해, 이러한 트레이드오프를 분석하고, 현재의 접근 방식에서 진정한 다모달성을 달성할 가능성에 대한 피드백을 제공합니다.",
      "upvotes": 9,
      "discussionId": "683e77d41417d107337abf8f",
      "projectPage": "https://darthzhu.github.io/lm-extend-page/",
      "githubRepo": "https://github.com/DarthZhu/lm-extend",
      "ai_summary": "Research investigates the impact of extending modality and model merging on maintaining language abilities and generalization in omni-modal language models.",
      "ai_keywords": [
        "omni-modal language models",
        "modality extension",
        "fine-tuning",
        "language abilities",
        "model merging",
        "generalization",
        "true omni-modality"
      ]
    },
    "publishedAt": "2025-06-02T13:01:40.000Z",
    "title": "Is Extending Modality The Right Path Towards Omni-Modality?",
    "summary": "Omni-modal language models (OLMs) aim to integrate and reason over diverse\ninput modalities--such as text, images, video, and audio--while maintaining\nstrong language capabilities. Despite recent advancements, existing models,\nespecially open-source ones, remain far from true omni-modality, struggling to\ngeneralize beyond the specific modality pairs they are trained on or to achieve\nstrong performance when processing multi-modal inputs. We study the effect of\nextending modality, the dominant technique for training multimodal models,\nwhere an off-the-shelf language model is fine-tuned on target-domain and\nlanguage data. Specifically, we investigate three key questions: (1) Does\nmodality extension compromise core language abilities? (2) Can model merging\neffectively integrate independently fine-tuned modality-specific models to\nachieve omni-modality? (3) Does omni-modality extension lead to better\nknowledge sharing and generalization compared to sequential extension? Through\nextensive experiments, we analyze these trade-offs and provide insights into\nthe feasibility of achieving true omni-modality using current approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643f9e2288d9d4488fd81c52",
      "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
      "fullname": "Tinghui Zhu",
      "name": "DarthZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06276",
      "authors": [
        {
          "_id": "68466dfb3ec10bdd8ab4dae2",
          "name": "Jiatao Gu",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae3",
          "name": "Tianrong Chen",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae4",
          "name": "David Berthelot",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae5",
          "name": "Huangjie Zheng",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae6",
          "name": "Yuyang Wang",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae7",
          "name": "Ruixiang Zhang",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae8",
          "name": "Laurent Dinh",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae9",
          "name": "Miguel Angel Bautista",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4daea",
          "name": "Josh Susskind",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4daeb",
          "name": "Shuangfei Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:58:39.000Z",
      "submittedOnDailyAt": "2025-06-09T03:58:52.022Z",
      "title": "STARFlow: 고해상도 이미지의 합성에 대한 잠재적 정규화 폼의 스케일링",
      "submittedOnDailyBy": {
        "_id": "6164e72d73996c363c52e66d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png",
        "isPro": false,
        "fullname": "Jiatao Gu",
        "user": "thomagram",
        "type": "user"
      },
      "summary": "STARFlow는 스케일러블한 생성 모델입니다. 이 모델은 정규화 흐름에 기반하여 구축되어 있으며, 고해상도 이미지 합성에서 강력한 성능을 보여주고 있습니다. STARFlow의 핵심은 Transformer Autoregressive Flow (TARFlow)입니다. TARFlow는 정규화 흐름의 표현력과, 자동 협조 모니터링 채널의 구조화된 모델링 능력을 결합한 것입니다. 먼저, TARFlow의 이론적 일반성을 증명합니다. 이 기초에 따라, 다음 3가지 핵심적인 아키텍처와 알고리즘의 혁신을 도입하여, 스케일러블성을 크게 향상시킵니다. 1) 깊은 스로워드 설계로, 깊은 Transformer 블록이 모델의 표현력을 최대로捉え, 계산적으로 효율적인shallow Transformer 블록이 그 대신 가장 벤이피엔트입니다. 2) 학습된 자동 엔코더의 잠재 공간에서 모델링을 수행하여, 픽셀 레벨 모델링보다 더 효과적입니다. 3) 새로운 가이드 알고리즘을 도입하여, 샘플의 품질을 크게 향상시킵니다. 중요한 점은, 모델은 정규화 흐름으로 끝에서 끝까지 일관되어, 연속 공간에서 정확한 최대 확률 훈련을 가능하게 합니다. STARFlow는 클래스 조건과 문맥 조건付き 이미지 생성 태스크에서 우수한 성능을 보여주며, 샘플의 품질에 있어서 가장 선진적인 Difuë 모델과 비교하여 비슷한 성능을 달성합니다. 우리 지식에 따르면, 이 연구는 이 스케일과 해상도에서 정규화 흐름이 효과적으로 작동하는 첫 번째 성공 사례입니다.",
      "upvotes": 5,
      "discussionId": "68466dfb3ec10bdd8ab4daec",
      "ai_summary": "STARFlow, a generative model combining normalizing flows with autoregressive Transformers, achieves competitive image synthesis performance with innovations in architecture and latent space modeling.",
      "ai_keywords": [
        "normalizing flows",
        "Transformer Autoregressive Flow",
        "TARFlow",
        "theoretical universality",
        "deep-shallow design",
        "pretrained autoencoders",
        "latent space",
        "guidance algorithm",
        "end-to-end normalizing flow",
        "exact maximum likelihood training",
        "class-conditional",
        "text-conditional image generation",
        "state-of-the-art diffusion models"
      ]
    },
    "publishedAt": "2025-06-06T13:58:39.000Z",
    "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\n  Synthesis",
    "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06276.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6164e72d73996c363c52e66d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png",
      "fullname": "Jiatao Gu",
      "name": "thomagram",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06253",
      "authors": [
        {
          "_id": "68469b773ec10bdd8ab4db88",
          "name": "Yuping He",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db89",
          "name": "Yifei Huang",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8a",
          "user": {
            "_id": "6392c73390b8e99a6779a7b0",
            "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
            "isPro": false,
            "fullname": "Guo Chen",
            "user": "cg1177",
            "type": "user"
          },
          "name": "Guo Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:02.757Z",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8b",
          "name": "Lidong Lu",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8c",
          "name": "Baoqi Pei",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8d",
          "name": "Jilan Xu",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8e",
          "name": "Tong Lu",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8f",
          "name": "Yoichi Sato",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:25:48.000Z",
      "submittedOnDailyAt": "2025-06-09T07:00:04.801Z",
      "title": "바이럴디에이션 포어트레이드: 에크스코컨트리비시션과 에크스코컨트리비시션의 시각을 결합: 콜로보라레레이션 시네지셰이크 지능 조사",
      "submittedOnDailyBy": {
        "_id": "6392c73390b8e99a6779a7b0",
        "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
        "isPro": false,
        "fullname": "Guo Chen",
        "user": "cg1177",
        "type": "user"
      },
      "summary": "세계를 자신의 가상 중심에서(첫째로) 그리고 그 가상 중심에서(삼째로) 두 가지 시각으로 보는 것이 인간의 인식의 기초이며, 동적인 환경의 풍부한 보완적인 이해를 가능하게 합니다. 최근, 머신이 이러한 두 가지 시각의 보완적 기능을 활용하는 연구 방향이 영상 이해 분야의 흥미로운 연구 과제로 등장했습니다. 본 조사에서는 두 가지 시각을 통해 영상 이해를 자세히 조사합니다. 먼저, 두 가상 중심의 방법론을 통합한 실용적인 적용 사례를 특징적으로 제시하고, 그 잠재적인 데이터 마이닝을 예상합니다. 이어서 이러한 적용 사례를 실현하기 위한 중요한 연구 과제를 특정합니다. 그 후, 최근의 진척을 3가지의 주요 연구 방향에 그룹화하고, 각 방향의 다양한 태스크와 관련된 연구를 분석합니다. 또한, 두 가지 시각의 연구를 지원하는 벤치마크 데이터 세트에 대해 논의하고, 그 범위, 다양성, 적용 가능성에 대해 평가합니다. 마지막으로, 현재의 연구의 한계를 논의하고, 미래의 연구 방향을 제안합니다. 두 가지 시각의 통찰을 통합하여, 영상 이해 및 인공지능의 발전을 촉진하고, 머신이 인간처럼 세계를 보는 것을 목표로 합니다. 관련 연구를 조사하기 위한 GitHub 리포지토리는 https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision에 있습니다.",
      "upvotes": 5,
      "discussionId": "68469b773ec10bdd8ab4db90",
      "projectPage": "https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision",
      "githubRepo": "https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision",
      "ai_summary": "A survey on leveraging both egocentric and exocentric video understanding for enhancing complementary tasks with a focus on three research directions and benchmark datasets.",
      "ai_keywords": [
        "egocentric",
        "exocentric",
        "video understanding",
        "research tasks",
        "benchmark datasets"
      ]
    },
    "publishedAt": "2025-06-06T13:25:48.000Z",
    "title": "Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence\n  with Egocentric-Exocentric Vision",
    "summary": "Perceiving the world from both egocentric (first-person) and exocentric\n(third-person) perspectives is fundamental to human cognition, enabling rich\nand complementary understanding of dynamic environments. In recent years,\nallowing the machines to leverage the synergistic potential of these dual\nperspectives has emerged as a compelling research direction in video\nunderstanding. In this survey, we provide a comprehensive review of video\nunderstanding from both exocentric and egocentric viewpoints. We begin by\nhighlighting the practical applications of integrating egocentric and\nexocentric techniques, envisioning their potential collaboration across\ndomains. We then identify key research tasks to realize these applications.\nNext, we systematically organize and review recent advancements into three main\nresearch directions: (1) leveraging egocentric data to enhance exocentric\nunderstanding, (2) utilizing exocentric data to improve egocentric analysis,\nand (3) joint learning frameworks that unify both perspectives. For each\ndirection, we analyze a diverse set of tasks and relevant works. Additionally,\nwe discuss benchmark datasets that support research in both perspectives,\nevaluating their scope, diversity, and applicability. Finally, we discuss\nlimitations in current works and propose promising future research directions.\nBy synthesizing insights from both perspectives, our goal is to inspire\nadvancements in video understanding and artificial intelligence, bringing\nmachines closer to perceiving the world in a human-like manner. A GitHub repo\nof related works can be found at\nhttps://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6392c73390b8e99a6779a7b0",
      "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
      "fullname": "Guo Chen",
      "name": "cg1177",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05523",
      "authors": [
        {
          "_id": "6846657d3ec10bdd8ab4daca",
          "user": {
            "_id": "630bc5ae86b8b9904c33e94b",
            "avatarUrl": "/avatars/b176d9b1691c05cc941409dd6c2b2228.svg",
            "isPro": false,
            "fullname": "Zikui Cai",
            "user": "Zikui",
            "type": "user"
          },
          "name": "Zikui Cai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:17.551Z",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacb",
          "name": "Andrew Wang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacc",
          "name": "Anirudh Satheesh",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacd",
          "name": "Ankit Nakhawa",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dace",
          "name": "Hyunwoo Jae",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacf",
          "name": "Keenan Powell",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad0",
          "name": "Minghui Liu",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad1",
          "name": "Neel Jay",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad2",
          "name": "Sungbin Oh",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad3",
          "name": "Xiyao Wang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad4",
          "name": "Yongyuan Liang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad5",
          "name": "Tom Goldstein",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad6",
          "name": "Furong Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T19:12:45.000Z",
      "submittedOnDailyAt": "2025-06-09T03:10:23.755Z",
      "title": "MORSE-500: 프로그래밍적으로 제어 가능한 비디오 벤치마크で, 다모달 로직을 타입 테스트 합니다.",
      "submittedOnDailyBy": {
        "_id": "655fed9fdef5905d38b84af3",
        "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
        "isPro": false,
        "fullname": "Xiyao Wang",
        "user": "russwang",
        "type": "user"
      },
      "summary": "TV 비디오 모델(VLMs)의 급격한 발전에도 불구하고, 현재의 다형 논리의 벤치마크는 3가지 중요한 차원에서 부족합니다. 첫째, 주로 정적 이미지에 의존하고, 실제 세계의 시간적 복잡성을 이해하지 않습니다. 둘째, 수학 문제 해결에 초점을 맞추고, 추상적이고 물리적이고 계획적이고 공간적 및 시간적인 논리 기술의 광범위한 영역을 무시합니다. 셋째, 많은 벤치마크는 빠르게 쉼아웃하고, 실패 모드 진단 및 진보 평가에 제한되어 있습니다. 이러한 문제를 해결하기 위해, MORSE-500(다형 논리의 스트레스 테스트 환경)을 소개합니다. 이는 6가지의 보간 논리 카테고리를 확장한 500개의 비디오 벤치마크입니다. 각 인스턴스는确定的な Python 스크립트를 사용하여, MANIM, Matplotlib, MoviePy, 생성 비디오 모델 및 편집된 실생활 음식을 사용합니다. 이 스크립트 Drove 설계에서, 시각적 복잡성, 디트랙터 밀도, 시간적 동작을 미세하게 제어할 수 있으며, 모델의 향상에 따라 난이도를 체계적으로 스케일할 수 있습니다. 정적 벤치마크와 달리, MORSE-500은 발전을 목표로 설계되어 있습니다: 제어 가능한 생성 파이프라인은 새로운 인스턴스 생성을 가능하게 하며, 다음 세대의 모델의 스트레스 테스트에 최적화됩니다. 초기 실험은 가장 강력한 모델인 GEMINI 2.5 Pro, OpenAI o3, 그리고 강력한 오픈소스 모델을 포함하여 수행되었으며, 모든 카테고리에서 큰 성능 차이를 보였으며, 특히 추상적이고 계획적인 태스크에서 큰 결함이 나타났습니다. 데이터셋, 생성 스크립트, 평가 도구를 공개하여 투명하고 재현 가능한 발전적인 다형 논리 연구를 지원합니다.",
      "upvotes": 5,
      "discussionId": "6846657d3ec10bdd8ab4dad7",
      "projectPage": "https://morse-500.github.io/",
      "githubRepo": "https://github.com/morse-benchmark/morse-500",
      "ai_summary": "MORSE-500, a video benchmark with 500 scripted clips, evaluates multimodal reasoning across six categories, highlighting performance gaps in abstract and planning tasks.",
      "ai_keywords": [
        "Vision-language models",
        "MORSE-500",
        "multimodal reasoning",
        "video benchmark",
        "scripted clips",
        "reasoning categories",
        "Manim",
        "Matplotlib",
        "MoviePy",
        "generative video models",
        "controllable generation",
        "spatial capabilities",
        "temporal capabilities",
        "abstract reasoning",
        "planning tasks"
      ]
    },
    "publishedAt": "2025-06-05T15:12:45.000Z",
    "title": "MORSE-500: A Programmatically Controllable Video Benchmark to\n  Stress-Test Multimodal Reasoning",
    "summary": "Despite rapid advances in vision-language models (VLMs), current benchmarks\nfor multimodal reasoning fall short in three key dimensions. First, they\noverwhelmingly rely on static images, failing to capture the temporal\ncomplexity of real-world environments. Second, they narrowly focus on\nmathematical problem-solving, neglecting the broader spectrum of reasoning\nskills -- including abstract, physical, planning, spatial, and temporal\ncapabilities -- required for robust multimodal intelligence. Third, many\nbenchmarks quickly saturate, offering limited headroom for diagnosing failure\nmodes or measuring continued progress. We introduce MORSE-500 (Multimodal\nReasoning Stress-test Environment), a video benchmark composed of 500 fully\nscripted clips with embedded questions spanning six complementary reasoning\ncategories. Each instance is programmatically generated using deterministic\nPython scripts (via Manim, Matplotlib, MoviePy), generative video models, and\ncurated real footage. This script-driven design allows fine-grained control\nover visual complexity, distractor density, and temporal dynamics -- enabling\ndifficulty to be scaled systematically as models improve. Unlike static\nbenchmarks that become obsolete once saturated, MORSE-500 is built to evolve:\nits controllable generation pipeline supports the creation of arbitrarily\nchallenging new instances, making it ideally suited for stress-testing\nnext-generation models. Initial experiments with state-of-the-art systems --\nincluding various Gemini 2.5 Pro and OpenAI o3 which represent the strongest\navailable at the time, alongside strong open-source models -- reveal\nsubstantial performance gaps across all categories, with particularly large\ndeficits in abstract and planning tasks. We release the full dataset,\ngeneration scripts, and evaluation harness to support transparent,\nreproducible, and forward-looking multimodal reasoning research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05523.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655fed9fdef5905d38b84af3",
      "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
      "fullname": "Xiyao Wang",
      "name": "russwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05573",
      "authors": [
        {
          "_id": "6846902a3ec10bdd8ab4db61",
          "name": "Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db62",
          "user": {
            "_id": "62e18206926f4892a4c782bd",
            "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
            "isPro": false,
            "fullname": "Chenguo Lin",
            "user": "chenguolin",
            "type": "user"
          },
          "name": "Chenguo Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:09.640Z",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db63",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db64",
          "name": "Honglei Yan",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db65",
          "name": "Yiqiang Feng",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db66",
          "name": "Yadong Mu",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db67",
          "name": "Katerina Fragkiadaki",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/iNc42ij-Kj0Z4V0jD5lCp.mp4"
      ],
      "publishedAt": "2025-06-05T20:30:28.000Z",
      "submittedOnDailyAt": "2025-06-09T06:14:01.450Z",
      "title": "PartCrafter: 구조화된 3D 메쉬 생성에 의한 구성적 잠재적 디퓨저 트랜스포머",
      "submittedOnDailyBy": {
        "_id": "62e18206926f4892a4c782bd",
        "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
        "isPro": false,
        "fullname": "Chenguo Lin",
        "user": "chenguolin",
        "type": "user"
      },
      "summary": "PartCrafter은, 단일의 RGB 이미지로부터 기호적으로 의미 있는 및 기하학적으로 다른 3D 메쉬를 동시에 합성하는 최초의 구조화된 3D 생성 모델입니다. 기존의 방법과 달리, 단일의 3D 형태를 생성하거나 이미지를 분할하여 각 분할을 재구성하는 2단계 프로세스를 선택하는 것이 아니라, PartCrafter는 사전 분할된 입력에 의존하지 않는 통일된, 구조적인 생성 아키텍처를 채택하고 있습니다. 단일의 이미지에 기반하여, 복수의 3D 파트를 동시에 노이즈를 제거하고, 개별의 물체나 복잡한 다물체 공간의 파트 관계 인식적인 생성을 가능하게 합니다. PartCrafter는, 전체의 물체를 학습한 3D 메쉬DIFUージョントランスフォーマー(DiT)의 학습된 가중치, 인코더, 디코더를 상속받고, 두 가지의 혁신을 도입하고 있습니다: (1) 구조화된 잠재 공간, 여기서 각 3D 파트는 분리된 잠재 토큰의 집합으로 표현됩니다; (2) 구조화된 정보 흐름을 가능하게 하는 계층적 注意 구조, 생성 시 각 파트 내 및 모든 파트 간에, 글로벌적인 일치성을 보장하고 파트 수준의 세부 사항을 유지합니다. 파트 수준의 슈퍼비전을 지원하기 위해, 큰 규모의 3D 물체 데이터 세트에서 파트 수준의 注釈을 발굴하여 새로운 데이터 세트를 カレーレード했습니다. 실험은, PartCrafter가 기존의 방법보다 3D 메쉬의 분해 가능한 생성을 수행하고, 입력 이미지에서 직접 볼 수 없는 파트도 포함하는 것을 보여주고, 3D 이해와 합성에 대한 파트 관계 인식적인 생성 선두를 보여주고 있습니다. 코드와 훈련 데이터는 릴리즈됩니다.",
      "upvotes": 4,
      "discussionId": "6846902a3ec10bdd8ab4db68",
      "projectPage": "https://wgsxm.github.io/projects/partcrafter",
      "githubRepo": "https://github.com/wgsxm/PartCrafter",
      "ai_summary": "PartCrafter is a unified 3D generative model that synthesizes multiple semantically meaningful 3D meshes from a single image using a compositional latent space and hierarchical attention mechanism.",
      "ai_keywords": [
        "3D generative model",
        "multiple 3D meshes",
        "RGB image",
        "unified compositional generation architecture",
        "denoising",
        "3D diffusion transformer (DiT)",
        "compositional latent space",
        "disentangled latent tokens",
        "hierarchical attention mechanism",
        "part-level supervision",
        "part-aware generative priors"
      ]
    },
    "publishedAt": "2025-06-05T16:30:28.000Z",
    "title": "PartCrafter: Structured 3D Mesh Generation via Compositional Latent\n  Diffusion Transformers",
    "summary": "We introduce PartCrafter, the first structured 3D generative model that\njointly synthesizes multiple semantically meaningful and geometrically distinct\n3D meshes from a single RGB image. Unlike existing methods that either produce\nmonolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an\nimage and then reconstructing each segment, PartCrafter adopts a unified,\ncompositional generation architecture that does not rely on pre-segmented\ninputs. Conditioned on a single image, it simultaneously denoises multiple 3D\nparts, enabling end-to-end part-aware generation of both individual objects and\ncomplex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh\ndiffusion transformer (DiT) trained on whole objects, inheriting the pretrained\nweights, encoder, and decoder, and introduces two key innovations: (1) A\ncompositional latent space, where each 3D part is represented by a set of\ndisentangled latent tokens; (2) A hierarchical attention mechanism that enables\nstructured information flow both within individual parts and across all parts,\nensuring global coherence while preserving part-level detail during generation.\nTo support part-level supervision, we curate a new dataset by mining part-level\nannotations from large-scale 3D object datasets. Experiments show that\nPartCrafter outperforms existing approaches in generating decomposable 3D\nmeshes, including parts that are not directly visible in input images,\ndemonstrating the strength of part-aware generative priors for 3D understanding\nand synthesis. Code and training data will be released.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/iNc42ij-Kj0Z4V0jD5lCp.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05573.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e18206926f4892a4c782bd",
      "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
      "fullname": "Chenguo Lin",
      "name": "chenguolin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06199",
      "authors": [
        {
          "_id": "684637733ec10bdd8ab4da66",
          "user": {
            "_id": "674b2406591d7232820252cd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
            "isPro": false,
            "fullname": "Hongyan Zhi",
            "user": "Hoyard",
            "type": "user"
          },
          "name": "Hongyan Zhi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:27.821Z",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da67",
          "name": "Peihao Chen",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da68",
          "name": "Siyuan Zhou",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da69",
          "name": "Yubo Dong",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6a",
          "name": "Quanxi Wu",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6b",
          "name": "Lei Han",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6c",
          "name": "Mingkui Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T16:00:31.000Z",
      "submittedOnDailyAt": "2025-06-09T01:42:52.611Z",
      "title": "3DFlowAction: 3D 흐름 월드에서의 크로스・에모디멘트 연산 학습 모델",
      "submittedOnDailyBy": {
        "_id": "674b2406591d7232820252cd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
        "isPro": false,
        "fullname": "Hongyan Zhi",
        "user": "Hoyard",
        "type": "user"
      },
      "summary": "操作은 로봇에게 장기적으로 어려운 과제였지만, 인간은 복잡한 물체와 상호작용을 쉽게 수행할 수 있습니다. 예를 들어, 컵을 미크스레치에 붙일 수 있습니다. 이 어려움의 원인은 로봇에게 동작 스킬을 가르치는 데 필요한 규모가 큰 고유한 데이터 세트의 부족입니다. 현재의 로봇 데이터 세트는 간단한 스케인 내의 다른 액션 공간에서 로봇의 액션을 기록하고 있습니다. 이는 다른 스케인 내의 다른 로봇에서 연속적인 강력한 액션 표현의 학습을 방해하고 있습니다. 인간이 동작 태스크를 이해하는 방법과 물체가 3D 공간에서 어떻게 움직이는지 이해하는 것을 동작을 지도하는 중요한 커뮤디로 밝혀졌습니다. 이 커뮤디는 기계체에 의존하지 않고, 인간과 다른 로봇에도 적합합니다. 이에 따라 인간과 로봇의 동작 데이터로부터 3D 플로우 월드 모델을 학습하는 것을 목표로 합니다. 이 모델은 3D 공간에서 상호작용하는 물체의 미래의 움직임을 예측하고 동작의 액션 계획을 지도합니다. 특히, 이동 물체의 자동 검출 프로리프를 통해 큰 규모의 3D 광학 플로우 데이터 세트를 합성합니다. 그 후, 비디오 디퓨저 기반의 월드 모델은 이러한 데이터로부터 동작 물리를 학습하고 언어 지시에 기반한 3D 광학 플로우 트레이너를 생성합니다. 생성된 3D 물체 광학 플로우를 통해 흐름 가이드 렌더링 구조를 제안하고, 예측된 최종 상태를 렌더링하고 GPT-4o를 사용하여 예측된 흐름이 작업 설명과 일치하는지 평가합니다. 이로 인해 로봇은 클로즈드 로딩 계획 능력을 갖게 됩니다. 최종적으로, 예측된 3D 광학 플로우를 최적화 정책의 제약으로, 작업에 필요한 로봇 액션의 챗을 결정하는 것을 고려합니다. 확장된 실험은 다양한 로봇 동작 태스크에서의 강인한 일반화와 특정 하드웨어에 의한 훈련이 필요하지 않은 신뢰성 있는 클로즈드 포맷 情绪 적응을 보여줍니다.",
      "upvotes": 3,
      "discussionId": "684637733ec10bdd8ab4da6d",
      "githubRepo": "https://github.com/Hoyyyaard/3DFlowAction/",
      "ai_summary": "A 3D flow world model learned from human and robot manipulation data, using video diffusion and GPT-4o, enables robots to perform diverse manipulation tasks with strong generalization and cross-embodiment adaptation.",
      "ai_keywords": [
        "3D flow world model",
        "moving object auto-detect pipeline",
        "video diffusion-based world model",
        "3D optical flow dataset",
        "ManiFlow-110k",
        "3D optical flow trajectories",
        "flow-guided rendering mechanism",
        "GPT-4o",
        "closed-loop planning",
        "optimization policy",
        "cross-embodiment adaptation"
      ]
    },
    "publishedAt": "2025-06-06T12:00:31.000Z",
    "title": "3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\n  Model",
    "summary": "Manipulation has long been a challenging task for robots, while humans can\neffortlessly perform complex interactions with objects, such as hanging a cup\non the mug rack. A key reason is the lack of a large and uniform dataset for\nteaching robots manipulation skills. Current robot datasets often record robot\naction in different action spaces within a simple scene. This hinders the robot\nto learn a unified and robust action representation for different robots within\ndiverse scenes. Observing how humans understand a manipulation task, we find\nthat understanding how the objects should move in the 3D space is a critical\nclue for guiding actions. This clue is embodiment-agnostic and suitable for\nboth humans and different robots. Motivated by this, we aim to learn a 3D flow\nworld model from both human and robot manipulation data. This model predicts\nthe future movement of the interacting objects in 3D space, guiding action\nplanning for manipulation. Specifically, we synthesize a large-scale 3D optical\nflow dataset, named ManiFlow-110k, through a moving object auto-detect\npipeline. A video diffusion-based world model then learns manipulation physics\nfrom these data, generating 3D optical flow trajectories conditioned on\nlanguage instructions. With the generated 3D object optical flow, we propose a\nflow-guided rendering mechanism, which renders the predicted final state and\nleverages GPT-4o to assess whether the predicted flow aligns with the task\ndescription. This equips the robot with a closed-loop planning ability.\nFinally, we consider the predicted 3D optical flow as constraints for an\noptimization policy to determine a chunk of robot actions for manipulation.\nExtensive experiments demonstrate strong generalization across diverse robotic\nmanipulation tasks and reliable cross-embodiment adaptation without\nhardware-specific training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06199.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674b2406591d7232820252cd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
      "fullname": "Hongyan Zhi",
      "name": "Hoyard",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05433",
      "authors": [
        {
          "_id": "68469df13ec10bdd8ab4db92",
          "name": "Zikang Liu",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db93",
          "name": "Tongtian Yue",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db94",
          "name": "Yepeng Tang",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db95",
          "name": "Longteng Guo",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db96",
          "name": "Junxian Cai",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db97",
          "name": "Qingbin Liu",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db98",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db99",
          "name": "Jing Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T09:13:37.000Z",
      "submittedOnDailyAt": "2025-06-09T07:17:09.252Z",
      "title": "Prefix Grouper: 공통 밴드Prefix를 활용한 효율적인 GRPO 훈련",
      "submittedOnDailyBy": {
        "_id": "6448dcf1b6ac93fe6512e342",
        "avatarUrl": "/avatars/a6441f89eabd156181bafc47c0b2f8c8.svg",
        "isPro": false,
        "fullname": "Zikang Liu",
        "user": "JohnCage",
        "type": "user"
      },
      "summary": "그룹 상대 정책 최적화 (GRPO)은 후보 출력이 공통된 입력 접두사를 공유하는 상대적인 비교로부터 경사를 계산하여, 가치 계산을 효율적으로 개선합니다. GRPO는 효과적이지만, 긴 공통된 입력 접두사를 처리할 때 각 그룹 멤버마다冗長하게 인코딩해야 하며, 계산량의 부담이 커집니다. 이 불리한 점은 긴 코텍스트 학습의 경우 주요한 스케일러빌리티 버틀넥으로 작용합니다. 우리는 Shared-Prefix Forward 전략을 사용하여冗長한 접두사 계산을 제거하기 위해, Prefix Grouper라는 효율적인 GRPO 훈련 알고리즘을 제안합니다. 특히, 자동 주의를 두 부분으로 재구성하여, 우리 방법은 공통된 접두사를 한번만 인코딩할 수 있도록 하며, 모든 미분 가능성과 종결점으로부터 시작하는 훈련의 호환성을 유지합니다. Prefix Grouper는 표준 GRPO와 훈련 등등임을 이론적으로 및 실험적으로 증명합니다: 같은 전방향 출력과 후방향 경사를 얻으며, 최종적인 가치 계산의 동적 및 최종적인 가치 계산의 성능이 변하지 않도록 보장합니다. 실험적으로는, 우리의 실험은 Prefix Grouper가 일치하는 결과를 얻으며, 특히 긴 접두사인 경우 훈련의 계산 비용을 크게 줄이는 것을 확인했습니다. 제안된 방법은 완전히 플러그인입니다: 현재 GRPO 기반의 아키텍처와 일치하며, 구조적인 변경이 필요하지 않아, 입력 구성 및 주의 계산에 최소한의 변경이 필요합니다. Prefix Grouper는 같은 계산 버지 내에서도 큰 그룹 크기를 사용하게 하여, GRPO의 스케일러빌리티를 복잡한 작업이나 큰 모델에 대시하여 향상시킵니다. 코드는 현재 https://github.com/johncaged/PrefixGrouper에서 사용 가능합니다.",
      "upvotes": 2,
      "discussionId": "68469df13ec10bdd8ab4db9a",
      "githubRepo": "https://github.com/johncaged/PrefixGrouper",
      "ai_summary": "Prefix Grouper reduces computational overhead in GRPO by encoding shared prefixes only once, improving scalability in long-context scenarios without altering training dynamics or policy performance.",
      "ai_keywords": [
        "Group Relative Policy Optimization (GRPO)",
        "self-attention",
        "Shared-Prefix Forward strategy",
        "computational overhead",
        "long-context learning scenarios",
        "differentiability",
        "end-to-end training",
        "training-equivalent"
      ]
    },
    "publishedAt": "2025-06-05T05:13:37.000Z",
    "title": "Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward",
    "summary": "Group Relative Policy Optimization (GRPO) enhances policy learning by\ncomputing gradients from relative comparisons among candidate outputs that\nshare a common input prefix. Despite its effectiveness, GRPO introduces\nsubstantial computational overhead when processing long shared prefixes, which\nmust be redundantly encoded for each group member. This inefficiency becomes a\nmajor scalability bottleneck in long-context learning scenarios. We propose\nPrefix Grouper, an efficient GRPO training algorithm that eliminates redundant\nprefix computation via a Shared-Prefix Forward strategy. In particular, by\nrestructuring self-attention into two parts, our method enables the shared\nprefix to be encoded only once, while preserving full differentiability and\ncompatibility with end-to-end training. We provide both theoretical and\nempirical evidence that Prefix Grouper is training-equivalent to standard GRPO:\nit yields identical forward outputs and backward gradients, ensuring that the\noptimization dynamics and final policy performance remain unchanged.\nEmpirically, our experiments confirm that Prefix Grouper achieves consistent\nresults while significantly reducing the computational cost of training,\nparticularly in long-prefix scenarios. The proposed method is fully\nplug-and-play: it is compatible with existing GRPO-based architectures and can\nbe seamlessly integrated into current training pipelines as a drop-in\nreplacement, requiring no structural modifications and only minimal changes to\ninput construction and attention computation. Prefix Grouper enables the use of\nlarger group sizes under the same computational budget, thereby improving the\nscalability of GRPO to more complex tasks and larger models. Code is now\navailable at https://github.com/johncaged/PrefixGrouper",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05433.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6448dcf1b6ac93fe6512e342",
      "avatarUrl": "/avatars/a6441f89eabd156181bafc47c0b2f8c8.svg",
      "fullname": "Zikang Liu",
      "name": "JohnCage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04255",
      "authors": [
        {
          "_id": "684312988f9ec8394c514883",
          "user": {
            "_id": "65c43d6d2b723dbc4ddc29d2",
            "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
            "isPro": false,
            "fullname": "Kunal Pai",
            "user": "guineapig",
            "type": "user"
          },
          "name": "Kunal Pai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-07T05:45:12.032Z",
          "hidden": false
        },
        {
          "_id": "684312988f9ec8394c514884",
          "user": {
            "_id": "62a0dbe7bff710e3fb05f9ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a0dbe7bff710e3fb05f9ae/uZK0Zkv7YG7jWbweh5tQb.png",
            "isPro": false,
            "fullname": "Parth Shah",
            "user": "helloparthshah",
            "type": "user"
          },
          "name": "Parth Shah",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T16:08:57.423Z",
          "hidden": false
        },
        {
          "_id": "684312988f9ec8394c514885",
          "name": "Harshil Patel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T17:33:16.000Z",
      "submittedOnDailyAt": "2025-06-09T03:49:09.306Z",
      "title": "하시루: 하이라어 키 에이전트 시스템 하이브리드 인텔리전트 리소스의 활용",
      "submittedOnDailyBy": {
        "_id": "65c43d6d2b723dbc4ddc29d2",
        "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
        "isPro": false,
        "fullname": "Kunal Pai",
        "user": "guineapig",
        "type": "user"
      },
      "summary": "Rapid Large Language Model (LLM)의 발전은 자동 변환 다중 에이전트 시스템 (MAS)의 개발을 촉진하고 있습니다. 그러나 현재의 프레임워크는 일반적으로 유연성, 자원 인식, 모델 다양성, 자동 변환 도구의 개발에 부족합니다. 본 논문에서는 \"HASHIRU\" (하이브리드 인텔리전스 자원 활용을 위한 휴리스틱 에이전트 시스템)이라는 새로운 MAS 프레임워크를 소개합니다. 이 프레임워크는 유연성, 자원 효율성, 적응성을 향상시키기 위해 설계되었습니다. HASHIRU는 작업의 필요성과 자원의 제한 (비용, 메모리)에 따라 전문적인 \"EMPOI\" 에이전트를 동적으로 관리하는 \"SEO\" 에이전트를 특징으로 합니다. 이 하이브리드 인텔리전스는 작은, 지역적인 LLM을 우선하여, 필요에 따라 외부 API와 큰 모델을 유연하게 사용하는 것을 특징으로 하며, Ollama를 통해 제공됩니다. 고용/해임 비용에 대한 경제적 모델은 팀의 안정성과 자원의 효율적인 분배를 촉진합니다. 이 시스템은 자동 변환 API 도구의 제작과 메모리 함수를 포함합니다. 학술 논문 평가 (58%의 성공률), 보안 평가 (JailbreakBench의 일부에서 100%의 성공률), 복잡한 이유 평가 (GSM8K에서 96% vs. 61%; JEEBench에서 80% vs. 68.3%; SVAMP에서 92% vs. 84%) 등 다양한 태스크에서 평가는 HASHIRU의 능력을 보여주고 있습니다. 사례 연구는 자동 변환 비용 모델의 생성, 도구의 통합, 버지어트 관리에 의한 자동 개선을 보여주고 있습니다. HASHIRU는 동적인 휴리스틱 제어, 자원 인식의 하이브리드 인텔리전스, 자동 변환 기능 확장을 통해 더 강건, 효율적이고 적응적인 MAS를 제공하고 있습니다. 소스 코드와 벤치마크는 https://github.com/HASHIRU-AI/HASHIRU와 https://github.com/HASHIRU-AI/HASHIRUBench에 제공됩니다. 라이브 데모는 https://hashiruagentx-hashiruai.hf.space에 문의하여 사용 가능합니다.",
      "upvotes": 2,
      "discussionId": "684312998f9ec8394c514886",
      "githubRepo": "https://github.com/HASHIRU-AI/HASHIRU",
      "ai_summary": "HASHIRU, a novel MAS framework, enhances flexibility, resource efficiency, and adaptability by dynamically managing specialized agents and using a hybrid intelligence approach with smaller, local LLMs and external APIs.",
      "ai_keywords": [
        "Hierarchical Agent System",
        "Hybrid Intelligent Resource Utilization",
        "HASHIRU",
        "CEO agent",
        "employee agents",
        "Ollama",
        "external APIs",
        "economic model",
        "hiring/firing costs",
        "autonomous API tool creation",
        "academic paper review",
        "safety assessments",
        "GSM8K",
        "JEEBench",
        "SVAMP",
        "Gemini 2.0 Flash",
        "self-improvement",
        "autonomous cost model generation",
        "tool integration",
        "budget management"
      ]
    },
    "publishedAt": "2025-06-01T13:33:16.000Z",
    "title": "HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\n  Utilization",
    "summary": "Rapid Large Language Model (LLM) advancements are fueling autonomous\nMulti-Agent System (MAS) development. However, current frameworks often lack\nflexibility, resource awareness, model diversity, and autonomous tool creation.\nThis paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent\nResource Utilization), a novel MAS framework enhancing flexibility, resource\nefficiency, and adaptability. HASHIRU features a \"CEO\" agent dynamically\nmanaging specialized \"employee\" agents, instantiated based on task needs and\nresource constraints (cost, memory). Its hybrid intelligence prioritizes\nsmaller, local LLMs (via Ollama) while flexibly using external APIs and larger\nmodels when necessary. An economic model with hiring/firing costs promotes team\nstability and efficient resource allocation. The system also includes\nautonomous API tool creation and a memory function. Evaluations on tasks like\nacademic paper review (58% success), safety assessments (100% on a\nJailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash\non GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate\nHASHIRU's capabilities. Case studies illustrate its self-improvement via\nautonomous cost model generation, tool integration, and budget management.\nHASHIRU offers a promising approach for more robust, efficient, and adaptable\nMAS through dynamic hierarchical control, resource-aware hybrid intelligence,\nand autonomous functional extension. Source code and benchmarks are available\nat https://github.com/HASHIRU-AI/HASHIRU and\nhttps://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is\navailable at https://hashiruagentx-hashiruai.hf.space upon request.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04255.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c43d6d2b723dbc4ddc29d2",
      "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
      "fullname": "Kunal Pai",
      "name": "guineapig",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]