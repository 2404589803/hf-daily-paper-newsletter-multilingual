[
  {
    "paper": {
      "id": "2507.01006",
      "authors": [
        {
          "_id": "68649318d59a9eda59024a6f",
          "user": {
            "_id": "62ecd24cb8764c7738ef2793",
            "avatarUrl": "/avatars/c1b80b5c55f9d652c1aaac7919e1fa32.svg",
            "isPro": false,
            "fullname": "Wenyi Hong",
            "user": "wenyi",
            "type": "user"
          },
          "name": "Wenyi Hong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-02T08:33:49.277Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a70",
          "user": {
            "_id": "649d3d271bafbcc83acec930",
            "avatarUrl": "/avatars/0c42aabf4c6601686c22cc1308c318de.svg",
            "isPro": false,
            "fullname": "Wenmeng Yu",
            "user": "iyuge2",
            "type": "user"
          },
          "name": "Wenmeng Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:01:31.023Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a71",
          "user": {
            "_id": "6438bca4a5e10f6d58694b47",
            "avatarUrl": "/avatars/3aeb25fbc73c5cab1265e13d11adfb76.svg",
            "isPro": false,
            "fullname": "XG",
            "user": "xgeric",
            "type": "user"
          },
          "name": "Xiaotao Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:24:12.081Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a72",
          "name": "Guo Wang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a73",
          "user": {
            "_id": "6752e1774d9c048280780bc5",
            "avatarUrl": "/avatars/a4e47645f898b69afba2744ef1e64bf9.svg",
            "isPro": false,
            "fullname": "GuobingGan",
            "user": "bigganbing",
            "type": "user"
          },
          "name": "Guobing Gan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:34:24.248Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a74",
          "user": {
            "_id": "6864fee46534c596c47483cd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8pBis_JAOXOOPZuO_sRyp.png",
            "isPro": false,
            "fullname": "Haomiao Tang",
            "user": "tanghme0www",
            "type": "user"
          },
          "name": "Haomiao Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:44:29.600Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a75",
          "user": {
            "_id": "627626d42d26ac639e56f565",
            "avatarUrl": "/avatars/805c5f909f52656345b8bde486c9fa8f.svg",
            "isPro": false,
            "fullname": "Jiale Cheng",
            "user": "CCCCCC",
            "type": "user"
          },
          "name": "Jiale Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:39:50.429Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a76",
          "user": {
            "_id": "6864b9f90a269bace8c92164",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zSeWHZRCZtzZ13CzFrr5u.png",
            "isPro": false,
            "fullname": "Ji Qi",
            "user": "miracle11121",
            "type": "user"
          },
          "name": "Ji Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:24:35.989Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a77",
          "user": {
            "_id": "649cffffe89a765015a243a4",
            "avatarUrl": "/avatars/20753e7ed68d1fad6ff633101b0ee2e4.svg",
            "isPro": false,
            "fullname": "Junhui Ji",
            "user": "jasonnoy",
            "type": "user"
          },
          "name": "Junhui Ji",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:32:44.176Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a78",
          "user": {
            "_id": "65d421895ab888c9d4a0d333",
            "avatarUrl": "/avatars/3c278eac6eb3296337863de96d120d18.svg",
            "isPro": false,
            "fullname": "kinnplh",
            "user": "kinnplh",
            "type": "user"
          },
          "name": "Lihang Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:13:43.126Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a79",
          "user": {
            "_id": "64004a2d261cfa61f39ab8a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64004a2d261cfa61f39ab8a0/cr7fmheqwXkPh6p78gVui.jpeg",
            "isPro": false,
            "fullname": "ShuaiqiDuan",
            "user": "ShayDuane",
            "type": "user"
          },
          "name": "Shuaiqi Duan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:32:29.607Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a7a",
          "name": "Weihan Wang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a7b",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a7c",
          "user": {
            "_id": "65acc5afe2a2c8635614de43",
            "avatarUrl": "/avatars/c5fce792792cc0b52ed7475d72460c58.svg",
            "isPro": false,
            "fullname": "Yean Cheng",
            "user": "LiquidAmmonia",
            "type": "user"
          },
          "name": "Yean Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:54:31.993Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a7d",
          "user": {
            "_id": "64f5c2f19eaf9d8fb74d2b47",
            "avatarUrl": "/avatars/2623c5fc9bff757f20b66f7626065d52.svg",
            "isPro": false,
            "fullname": "Zehai He",
            "user": "he-zh22",
            "type": "user"
          },
          "name": "Zehai He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:32:38.491Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a7e",
          "name": "Zhe Su",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a7f",
          "user": {
            "_id": "650c0f7fdab7deefd4631109",
            "avatarUrl": "/avatars/3d0683d1113c3bd530b5e8b1499b17f6.svg",
            "isPro": false,
            "fullname": "ZhenYang21",
            "user": "ZhenYang21",
            "type": "user"
          },
          "name": "Zhen Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:55:04.067Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a80",
          "user": {
            "_id": "64a4fe94c6e1167c3c67053a",
            "avatarUrl": "/avatars/00cb76622d6afe5786a512abe04f6d6e.svg",
            "isPro": false,
            "fullname": "Ziyang Pan",
            "user": "soupsheep",
            "type": "user"
          },
          "name": "Ziyang Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:55:18.341Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a81",
          "user": {
            "_id": "62dc173789b4cf157d36ebee",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659772051636-62dc173789b4cf157d36ebee.jpeg",
            "isPro": false,
            "fullname": "Zeng Aohan",
            "user": "Sengxian",
            "type": "user"
          },
          "name": "Aohan Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:54:46.680Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a82",
          "user": {
            "_id": "6864fbe0f1a19f5474f6d66a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tSoyjsw2E8z0DqZa6n7D0.png",
            "isPro": false,
            "fullname": "wangbaoxu",
            "user": "wangbaoxu",
            "type": "user"
          },
          "name": "Baoxu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:30:50.007Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a83",
          "user": {
            "_id": "6864f3023f1637b315c7a94b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1LaoqhDJhA5_ZucwotiMt.png",
            "isPro": false,
            "fullname": "Boyan Shi",
            "user": "Boyan118",
            "type": "user"
          },
          "name": "Boyan Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:54:13.128Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a84",
          "user": {
            "_id": "65d9878ce4d11d592e37949b",
            "avatarUrl": "/avatars/51b35b1f33c526ac8e1b737254d79898.svg",
            "isPro": false,
            "fullname": "Pcy",
            "user": "huohuohuoha",
            "type": "user"
          },
          "name": "Changyu Pang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:54:34.155Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a85",
          "user": {
            "_id": "657bc6df504b90a3c59c91f8",
            "avatarUrl": "/avatars/d0d99192204adf3da935c5336de238b5.svg",
            "isPro": false,
            "fullname": "Chenhui Zhang",
            "user": "zhangch",
            "type": "user"
          },
          "name": "Chenhui Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:54:15.146Z",
          "hidden": true
        },
        {
          "_id": "68649318d59a9eda59024a86",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a87",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a88",
          "name": "Guoqing Chen",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a89",
          "user": {
            "_id": "62d7b131f6e8ba66107af761",
            "avatarUrl": "/avatars/f1c5df47aef69c824fd166722df8f670.svg",
            "isPro": false,
            "fullname": "Jiazheng Xu",
            "user": "xujz0703",
            "type": "user"
          },
          "name": "Jiazheng Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:13:37.291Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a8a",
          "user": {
            "_id": "629b5af8ba19b79d3542e755",
            "avatarUrl": "/avatars/ce7aefa86d8ab865789bb0ba7add16c5.svg",
            "isPro": false,
            "fullname": "Gary Chen",
            "user": "Garygedegege",
            "type": "user"
          },
          "name": "Jiali Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:32:36.660Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a8b",
          "name": "Jing Chen",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a8c",
          "user": {
            "_id": "6604160b8d50be909bea5ff6",
            "avatarUrl": "/avatars/04c17cbe98fec21aec73bb6e208cb4e8.svg",
            "isPro": false,
            "fullname": "Jinhao Chen",
            "user": "DexterChan",
            "type": "user"
          },
          "name": "Jinhao Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:13:48.911Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a8d",
          "name": "Jinghao Lin",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a8e",
          "name": "Jinjiang Wang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a8f",
          "name": "Junjie Chen",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a90",
          "user": {
            "_id": "652e814fc22d404ebf7af84a",
            "avatarUrl": "/avatars/66407dba33beba8f381f1fd18f30ebac.svg",
            "isPro": false,
            "fullname": "LEI Le-qi",
            "user": "le-qi",
            "type": "user"
          },
          "name": "Leqi Lei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:32:33.140Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a91",
          "name": "Leyi Pan",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a92",
          "name": "Mingzhi Zhang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a93",
          "name": "Qinkai Zheng",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a94",
          "user": {
            "_id": "64ba1a106a7c71f00472a053",
            "avatarUrl": "/avatars/fcf8b0c0ff7cabf3d456e1f72d5b218d.svg",
            "isPro": false,
            "fullname": "Sheng Yang",
            "user": "SuunnYang",
            "type": "user"
          },
          "name": "Sheng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:32:34.866Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a95",
          "name": "Shi Zhong",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a96",
          "user": {
            "_id": "6406db5cd684369027166986",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6406db5cd684369027166986/Zl-orrGcbY0RbfjfKszn1.jpeg",
            "isPro": false,
            "fullname": "Shiyu Huang",
            "user": "ShiyuHuang",
            "type": "user"
          },
          "name": "Shiyu Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:54:29.870Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a97",
          "user": {
            "_id": "66d9148ec2433bcb235bfd9d",
            "avatarUrl": "/avatars/edcea7d3066c9bcbe3060862b0a41e91.svg",
            "isPro": false,
            "fullname": "ZhaoShuyuan",
            "user": "Cyrus0246",
            "type": "user"
          },
          "name": "Shuyuan Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:30:55.730Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a98",
          "user": {
            "_id": "6604d5b94b0ad9bc40b17cea",
            "avatarUrl": "/avatars/aefdc6c6056c14e8622d9b3b639a4494.svg",
            "isPro": false,
            "fullname": "Sean Xue",
            "user": "xue4y",
            "type": "user"
          },
          "name": "Siyan Xue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:30:51.968Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a99",
          "user": {
            "_id": "648c48d8c0ddeee6df5b6d22",
            "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
            "isPro": false,
            "fullname": "Shangqing Tu",
            "user": "tsq2000",
            "type": "user"
          },
          "name": "Shangqin Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:39:08.904Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a9a",
          "name": "Shengbiao Meng",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a9b",
          "user": {
            "_id": "64697af7dcbb937d56b98237",
            "avatarUrl": "/avatars/167b1a52ea74d6d61e54053742412fde.svg",
            "isPro": false,
            "fullname": "Tianshu Zhang",
            "user": "huoyuezaiyuan",
            "type": "user"
          },
          "name": "Tianshu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:13:45.161Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a9c",
          "user": {
            "_id": "6739b8405d4d88ae26140940",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/l_5rD6EVQ9vDui0UFDEJq.png",
            "isPro": false,
            "fullname": "罗天蔚",
            "user": "Cyan666",
            "type": "user"
          },
          "name": "Tianwei Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:54:56.978Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a9d",
          "user": {
            "_id": "67fc7ba101fde81f68836d0a",
            "avatarUrl": "/avatars/482657bba0dc0226df2098b89f915cd9.svg",
            "isPro": false,
            "fullname": "Tianxiang Hao",
            "user": "beyondhtx",
            "type": "user"
          },
          "name": "Tianxiang Hao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:54:17.165Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a9e",
          "name": "Tianle Gong",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a9f",
          "user": {
            "_id": "66bd7103cc58b899916f954a",
            "avatarUrl": "/avatars/a1fe24fe35105613c24ea9db6e5487c5.svg",
            "isPro": false,
            "fullname": "liwenkai",
            "user": "lwk21",
            "type": "user"
          },
          "name": "Wenkai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:54:51.083Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa0",
          "user": {
            "_id": "673c2fcff31343826f64081b",
            "avatarUrl": "/avatars/0ffb6502d2caf35a8c6b03ea2140a441.svg",
            "isPro": false,
            "fullname": "Wei Jia",
            "user": "WeiJia1220",
            "type": "user"
          },
          "name": "Wei Jia",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:01:34.637Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa1",
          "user": {
            "_id": "648c64829c935db2b527a764",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648c64829c935db2b527a764/AnTu1dvpac3hUxLQo8A8K.jpeg",
            "isPro": false,
            "fullname": "Xin Lv",
            "user": "davidlvxin",
            "type": "user"
          },
          "name": "Xin Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:54:54.640Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa2",
          "user": {
            "_id": "677dffd82b763404dd896c6e",
            "avatarUrl": "/avatars/044225ddeafaeec51e098530f481adbd.svg",
            "isPro": false,
            "fullname": "Huang Xuancheng",
            "user": "xchuang17",
            "type": "user"
          },
          "name": "Xuancheng Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:13:47.245Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa3",
          "user": {
            "_id": "67cd327432668b04f4555270",
            "avatarUrl": "/avatars/15e2cef976cbe05c4c5858c88dccf4af.svg",
            "isPro": false,
            "fullname": "Yanling Wang",
            "user": "WYLing",
            "type": "user"
          },
          "name": "Yanling Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:54:49.022Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa4",
          "user": {
            "_id": "64d4b5808b65d477e68f2fba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/upUtVjLXZ9lAulngeFh_i.jpeg",
            "isPro": false,
            "fullname": "Xue Yadong",
            "user": "ataraxy3",
            "type": "user"
          },
          "name": "Yadong Xue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:32:40.396Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa5",
          "user": {
            "_id": "6864fe5a3063b37e17135486",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/aJYirqgL9qBgGS0YSLyzH.png",
            "isPro": false,
            "fullname": "Yanfeng Wang",
            "user": "wRonG118",
            "type": "user"
          },
          "name": "Yanfeng Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:44:20.930Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa6",
          "user": {
            "_id": "67a479dc15b391c3d467d202",
            "avatarUrl": "/avatars/af3587515e3b3bde306a5fffb593293f.svg",
            "isPro": false,
            "fullname": "A1phaN",
            "user": "anyifan",
            "type": "user"
          },
          "name": "Yifan An",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-02T09:43:27.504Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa7",
          "user": {
            "_id": "66eae378aed2e0ec9689e9df",
            "avatarUrl": "/avatars/0ed27107e803cae70f8080d4ec54bfa0.svg",
            "isPro": false,
            "fullname": "Evan Du",
            "user": "EvanDu037",
            "type": "user"
          },
          "name": "Yifan Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:32:42.389Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa8",
          "user": {
            "_id": "642ebcefd09a9c63c2124bd2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642ebcefd09a9c63c2124bd2/HFSkoaxIM6nTq1CrY6jyF.jpeg",
            "isPro": false,
            "fullname": "Yiming Shi",
            "user": "Shiym",
            "type": "user"
          },
          "name": "Yiming Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:39:52.595Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa9",
          "user": {
            "_id": "6865031df35853980850da4f",
            "avatarUrl": "/avatars/2e496e6ac24d508ad1b4549bf4cb6bc1.svg",
            "isPro": false,
            "fullname": "yiheng huang",
            "user": "hyheng",
            "type": "user"
          },
          "name": "Yiheng Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T10:05:42.668Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aaa",
          "name": "Yilin Niu",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aab",
          "user": {
            "_id": "6864ef64d4dde09fbabad95a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sbEmJxiJg4-8rrXDqUuFy.jpeg",
            "isPro": false,
            "fullname": "Yuan Wang",
            "user": "traveler2333",
            "type": "user"
          },
          "name": "Yuan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:55:14.252Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aac",
          "user": {
            "_id": "68248c7c2fa630b9a0b803c5",
            "avatarUrl": "/avatars/6f5692457b0dbcad03d0a08707ee304c.svg",
            "isPro": false,
            "fullname": "Yuanchang Yue",
            "user": "yueyuanchang",
            "type": "user"
          },
          "name": "Yuanchang Yue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:55:16.579Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aad",
          "user": {
            "_id": "65af5363101482afcc788e9d",
            "avatarUrl": "/avatars/e58daff222c847557b136d26b70e3342.svg",
            "isPro": false,
            "fullname": "Yuchen Li",
            "user": "comrade007134",
            "type": "user"
          },
          "name": "Yuchen Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:55:21.496Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aae",
          "user": {
            "_id": "655ecdea19fd101f14afc65e",
            "avatarUrl": "/avatars/e596b4c65a11ad2c4494e31ee2d61c76.svg",
            "isPro": false,
            "fullname": "zyt",
            "user": "zyt1024",
            "type": "user"
          },
          "name": "Yutao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:24:13.857Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aaf",
          "user": {
            "_id": "643507d1ce04fdb57e9d7e05",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643507d1ce04fdb57e9d7e05/QuCFAO3v7G3LrVGusqFj1.png",
            "isPro": false,
            "fullname": "zR",
            "user": "ZAHNGYUXUAN",
            "type": "user"
          },
          "name": "Yuxuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T08:02:44.813Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab0",
          "user": {
            "_id": "63033dc4e1e7f0e03a5e1a31",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg",
            "isPro": false,
            "fullname": "Zhengxiao Du",
            "user": "zxdu20",
            "type": "user"
          },
          "name": "Zhanxiao Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:30:53.686Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab1",
          "user": {
            "_id": "62b196646d3d059f40c3df19",
            "avatarUrl": "/avatars/dbddf54ae949437223f3a438d30ef653.svg",
            "isPro": false,
            "fullname": "Zhenyu Hou",
            "user": "think2try",
            "type": "user"
          },
          "name": "Zhenyu Hou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T09:01:33.136Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab2",
          "user": {
            "_id": "6865034aabd541be6b66c5f7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6lOkw7oiy7H8M7YwSYE2c.png",
            "isPro": false,
            "fullname": "xuezhao",
            "user": "xuezhao998",
            "type": "user"
          },
          "name": "Zhao Xue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T10:05:44.994Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab3",
          "name": "Zhengxiao Du",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab4",
          "name": "Zihan Wang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab5",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab6",
          "name": "Debing Liu",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab7",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab8",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab9",
          "name": "Minlie Huang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aba",
          "name": "Yuxiao Dong",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024abb",
          "name": "Jie Tang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62ecd24cb8764c7738ef2793/zDkNTP70lk6UrrLAk_Sdt.jpeg"
      ],
      "publishedAt": "2025-07-01T17:55:04.000Z",
      "submittedOnDailyAt": "2025-07-02T03:07:41.410Z",
      "title": "GLM-4.1V-Thinking: 스케일러블한 강화학습을 활용한 다양한 모노모ー달 논리로 향하는 방법론\n\n(Note: The original text was in Japanese, not English, which was translated to Korean. If you intended to have the text translated from English to Korean, please provide the correct English text for translation.)",
      "submittedOnDailyBy": {
        "_id": "62ecd24cb8764c7738ef2793",
        "avatarUrl": "/avatars/c1b80b5c55f9d652c1aaac7919e1fa32.svg",
        "isPro": false,
        "fullname": "Wenyi Hong",
        "user": "wenyi",
        "type": "user"
      },
      "summary": "GLM-4.1V-Thinking는 일반용의 다양한 언어를 처리하는 모델을 개발하기 위해 설계된 시각 언어 모델(VLM)입니다. 본 보고서에서는, 논리 중심의 훈련 프레임워크 개발 과정에서 중요한 발견을 공유합니다. 먼저, 큰 규모의 사전 훈련을 통해 강력한 시각 기반 모델을 개발하고, 이는 최종적인 성능의 상한을 설정합니다. 이어서, Curriculum Sampling을 사용한 강화 학습(RLCS)을 사용하여 모델의 모든 잠재력을 발휘시키고, STEM 문제 해결, 영상 이해, 콘텐츠 인식, 코딩, 기초화, GUI 기반의 에이전트, 긴 문장 이해 등 다양한 태스크에 대한 세부적인 능력 향상을 실현합니다. 이 분야의 연구를 지원하기 위해, GLM-4.1V-9B-Thinking을 오픈 소스로 공개하고, 상대적으로 작은 규모의 모델 중 가장 선진적인 성능을 달성합니다. 28개의 공개 벤치마크에서 상세한 평가 결과, 우리 모델은 Qwen2.5-VL-7B에 대해 약 80%의 태스크에서 우위를 차지하며, Qwen2.5-VL-72B에 대해 18개의 벤치마크에서 상대적으로나 더 우수한 성능을 달성합니다. 특히, 긴 문장 이해와 STEM 논리적인 어려운 태스크에서도 GPT-4o와 달리, 더 강력한 능력을 보여주며, 이를 통해 그 강력한 능력을 더욱 강조합니다. 코드, 모델 및 추가적인 정보는 https://github.com/THUDM/GLM-4.1V-Thinking에서 릴리즈 됩니다.",
      "upvotes": 111,
      "discussionId": "68649319d59a9eda59024abc",
      "githubRepo": "https://github.com/THUDM/GLM-4.1V-Thinking",
      "ai_summary": "A vision-language model, GLM-4.1V-Thinking, enhances general-purpose multimodal reasoning through large-scale pre-training and reinforcement learning, achieving state-of-the-art performance across various tasks.",
      "ai_keywords": [
        "vision-language model",
        "reasoning-centric training framework",
        "reinforcement learning",
        "curriculum sampling",
        "vision foundation model",
        "STEM problem solving",
        "video understanding",
        "content recognition",
        "coding",
        "grounding",
        "GUI-based agents",
        "long document understanding"
      ],
      "githubStars": 224
    },
    "publishedAt": "2025-07-01T13:55:04.000Z",
    "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable\n  Reinforcement Learning",
    "summary": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to\nadvance general-purpose multimodal reasoning. In this report, we share our key\nfindings in the development of the reasoning-centric training framework. We\nfirst develop a capable vision foundation model with significant potential\nthrough large-scale pre-training, which arguably sets the upper bound for the\nfinal performance. Reinforcement Learning with Curriculum Sampling (RLCS) then\nunlocks the full potential of the model, leading to comprehensive capability\nenhancement across a diverse range of tasks, including STEM problem solving,\nvideo understanding, content recognition, coding, grounding, GUI-based agents,\nand long document understanding, among others. To facilitate research in this\nfield, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art\nperformance among models of comparable size. In a comprehensive evaluation\nacross 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all\ntasks and achieves comparable or even superior performance on 18 benchmarks\nrelative to the significantly larger Qwen2.5-VL-72B. Notably,\nGLM-4.1V-9B-Thinking also demonstrates competitive or superior performance\ncompared to closed-source models such as GPT-4o on challenging tasks including\nlong document understanding and STEM reasoning, further underscoring its strong\ncapabilities. Code, models and more information are released at\nhttps://github.com/THUDM/GLM-4.1V-Thinking.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62ecd24cb8764c7738ef2793/zDkNTP70lk6UrrLAk_Sdt.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01006.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62ecd24cb8764c7738ef2793",
      "avatarUrl": "/avatars/c1b80b5c55f9d652c1aaac7919e1fa32.svg",
      "fullname": "Wenyi Hong",
      "name": "wenyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23115",
      "authors": [
        {
          "_id": "68634959588cea0da970c8a5",
          "user": {
            "_id": "66add675c7a575aa0e03d5f3",
            "avatarUrl": "/avatars/b72b18130664c1de197c1f8df371aa70.svg",
            "isPro": true,
            "fullname": "Haonan Chen",
            "user": "Haon-Chen",
            "type": "user"
          },
          "name": "Haonan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:35.715Z",
          "hidden": false
        },
        {
          "_id": "68634959588cea0da970c8a6",
          "user": {
            "_id": "648ba0d68e7f7a927675d4a3",
            "avatarUrl": "/avatars/d82ced93656e03d60c8b55010694f908.svg",
            "isPro": false,
            "fullname": "Hong Liu",
            "user": "hongliu9903",
            "type": "user"
          },
          "name": "Hong Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:40.207Z",
          "hidden": false
        },
        {
          "_id": "68634959588cea0da970c8a7",
          "user": {
            "_id": "64ed68ce60f6345da7014b38",
            "avatarUrl": "/avatars/adfc156482ef5570dc69329aa53975e6.svg",
            "isPro": false,
            "fullname": "Yuping Luo",
            "user": "roosephu",
            "type": "user"
          },
          "name": "Yuping Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:37.881Z",
          "hidden": false
        },
        {
          "_id": "68634959588cea0da970c8a8",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "68634959588cea0da970c8a9",
          "name": "Nan Yang",
          "hidden": false
        },
        {
          "_id": "68634959588cea0da970c8aa",
          "name": "Furu Wei",
          "hidden": false
        },
        {
          "_id": "68634959588cea0da970c8ab",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-29T06:41:00.000Z",
      "submittedOnDailyAt": "2025-07-02T00:57:35.541Z",
      "title": "모카: 모델링에 관련된 지속적인 예측 학습을 통해 더 좋은 양방향 모델 합성기호를 생성합니다.",
      "submittedOnDailyBy": {
        "_id": "66add675c7a575aa0e03d5f3",
        "avatarUrl": "/avatars/b72b18130664c1de197c1f8df371aa70.svg",
        "isPro": true,
        "fullname": "Haonan Chen",
        "user": "Haon-Chen",
        "type": "user"
      },
      "summary": "다모달 엔베딩 모뎀은 원인에 대한 시각 언어 모뎀(VLMs)에 기반하여 구축되어 있으며, 다양한 태스크에서 효과적으로 작동하는 것을 보여주고 있습니다. 그러나 현재의 접근 방식은 세 가지 주요한 제한을 가지고 있습니다: VLM 백본에서의 원인에 대한 어텐션의 사용은 엔베딩 태스크에 최적이 아닙니다; 고품질의 기준付き 데이터에 의존한 스케일라빌리티 문제; 훈련 객체와 데이터의 다양성 한계입니다. 이러한 문제를 해결하기 위해, 우리는 MoCa(모카)를 제안합니다. MoCa는 2단계의 프레임워크로, 이미 학습된 VLMs를 효과적인 양방향 다모달 엔베딩 모뎀으로 변환하는 방법을 제공합니다. 첫 번째 단계는 모델 시각에 대한 지속된 사전 학습으로, 간접 결합된 텍스트와 이미지의 입력을 동시에 디자이너로 공통적인 재구성 객체를 도입하고 양방향적인 컨텍스트 인식을 강화합니다. 두 번째 단계는 서로 다른, 의미적으로 풍부한 다모달 데이터를 초월하고, 간단한 이미지 캡션 페어를 제외한 확장성과 어레이먼트를 강화하기 위해 비교적인 최종 조정을 사용합니다. 우리의 방법은 연속된 사전 학습을 통해 양방향적인 어텐션을 도입하고, 대규모의 비기준付き 데이터 세트에서 효과적인 스케일라빌리티를 실현하고, 다양한 다모달 데이터를 사용하여 표현의 강건성을 강화함으로써, 위의 제한을 해결합니다. 실험은 MoCa가 MMEB와 VidRe-v2 벤치마크에서 일관된 성능 향상을 보여주고, 새로운 최첨단 결과를 구현하고, 모델 크기와 훈련 데이터 모두에서 강한 스케일라빌리티를 보여주었습니다.",
      "upvotes": 29,
      "discussionId": "6863495a588cea0da970c8ac",
      "projectPage": "https://haon-chen.github.io/MoCa",
      "githubRepo": "https://github.com/haon-chen/MoCa",
      "ai_summary": "MoCa, a two-stage framework, enhances pre-trained causal vision-language models for multimodal embedding by introducing bidirectional attention, scaling with unlabeled data, and diverse training objectives.",
      "ai_keywords": [
        "Vision Language Models",
        "VLMs",
        "bidirectional multimodal embedding models",
        "modality-aware continual pre-training",
        "joint reconstruction objective",
        "heterogeneous contrastive fine-tuning",
        "bidirectional attention",
        "massive unlabeled datasets",
        "MMEB",
        "ViDoRe-v2"
      ],
      "githubStars": 25
    },
    "publishedAt": "2025-06-29T02:41:00.000Z",
    "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional\n  Multimodal Embeddings",
    "summary": "Multimodal embedding models, built upon causal Vision Language Models (VLMs),\nhave shown promise in various tasks. However, current approaches face three key\nlimitations: the use of causal attention in VLM backbones is suboptimal for\nembedding tasks; scalability issues due to reliance on high-quality labeled\npaired data for contrastive learning; and limited diversity in training\nobjectives and data. To address these issues, we propose MoCa, a two-stage\nframework for transforming pre-trained VLMs into effective bidirectional\nmultimodal embedding models. The first stage, Modality-aware Continual\nPre-training, introduces a joint reconstruction objective that simultaneously\ndenoises interleaved text and image inputs, enhancing bidirectional\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\nimage-caption pairs to enhance generalization and alignment. Our method\naddresses the stated limitations by introducing bidirectional attention through\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\njoint reconstruction objectives, and utilizing diverse multimodal data for\nenhanced representation robustness. Experiments demonstrate that MoCa\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\nachieving new state-of-the-art results, and exhibits strong scalability with\nboth model size and training data on MMEB.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23115.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66add675c7a575aa0e03d5f3",
      "avatarUrl": "/avatars/b72b18130664c1de197c1f8df371aa70.svg",
      "fullname": "Haonan Chen",
      "name": "Haon-Chen",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01001",
      "authors": [
        {
          "_id": "6864a4ddd59a9eda59024aea",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun Zhao",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:38:54.448Z",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024aeb",
          "user": {
            "_id": "676ce7767fff9075b5d526fa",
            "avatarUrl": "/avatars/bf6697163b91564a8d4b773d3f6420bf.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "maxzky",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:38:52.477Z",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024aec",
          "user": {
            "_id": "67492b9e347c3876f22b3684",
            "avatarUrl": "/avatars/0d80d23f7b10ce8bac689f6e8317a014.svg",
            "isPro": false,
            "fullname": "Tiansheng Hu",
            "user": "HughieHu",
            "type": "user"
          },
          "name": "Tiansheng Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:38:50.408Z",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024aed",
          "name": "Sihong Wu",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024aee",
          "name": "Ronan Le Bras",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024aef",
          "name": "Taira Anderson",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af0",
          "name": "Jonathan Bragg",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af1",
          "name": "Joseph Chee Chang",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af2",
          "name": "Jesse Dodge",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af3",
          "name": "Matt Latzke",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af4",
          "name": "Yixin Liu",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af5",
          "name": "Charles McGrady",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af6",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af7",
          "name": "Zihang Wang",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af8",
          "name": "Chen Zhao",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af9",
          "name": "Hannaneh Hajishirzi",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024afa",
          "name": "Doug Downey",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024afb",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-01T17:51:59.000Z",
      "submittedOnDailyAt": "2025-07-02T01:48:24.934Z",
      "title": "과학어댑터: 과학 문헌 태스크에서 기초 모형의 개방된 평가 플랫폼",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "SciArena는 과학 문헌 태스크에서 기초 모델의 평가를 수행하는 오픈 플랫폼입니다. 전통적인 과학 문헌 이해와 합성에 대한 벤치마크와 달리, SciArena는 연구 커뮤니티를 직접 초청하고, Chatbot Arena의 커뮤니티 투표에 기반한 모델 비교 평가 방식을 채택하고 있습니다. 집중된 지식을 활용하여, SciArena는 문헌 기반의 긴 문장을 요구하는 개방된 과학 태스크에서 모델의 성능을 커뮤니티 주도 평가로 제공합니다. 현재, 23개의 오픈 소스 모델과 유료 모델을 지원하고, 다양한 과학 분야의 신뢰된 연구자로부터 13,000점 이상의 투표를 모으고 있습니다. 지금까지의 데이터를 분석하여, 제시된 질문의 다양성, 현실적인 문헌의 필요에 맞는 것, 참여하는 연구자가 강한 자기 일관성과 상호 평가자의 일치성을 보여주는 것을 확인했습니다. 모델의 등급 순위 리스트에 기반하여 결과를 논의하고 통찰을 얻습니다. 과학 문헌 태스크에 대한 모델 기반의 자동 평가 시스템 연구를 촉진하기 위해, SciArena-Eval이라는 메타 평가 벤치마크를 릴리즈합니다. 이 벤치마크는 모델의 답변의 품질을 판단하는 정확도를 측정하기 위해, 모델의 비교 평가와 사람의 투표를 비교합니다. 실험은 벤치마크의 문제를 명확히하고 신뢰할 수 있는 자동 평가 방법의 필요성을 강조합니다.",
      "upvotes": 25,
      "discussionId": "6864a4ded59a9eda59024afc",
      "projectPage": "https://sciarena.allen.ai/",
      "ai_summary": "SciArena is a community-driven platform for evaluating foundation models on scientific literature tasks, using collective voter judgments to rank models and address the need for reliable automated evaluation.",
      "ai_keywords": [
        "Chatbot Arena",
        "collective intelligence",
        "open-ended scientific tasks",
        "literature-grounded",
        "long-form responses",
        "meta-evaluation benchmark",
        "automated evaluation systems"
      ]
    },
    "publishedAt": "2025-07-01T13:51:59.000Z",
    "title": "SciArena: An Open Evaluation Platform for Foundation Models in\n  Scientific Literature Tasks",
    "summary": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.00432",
      "authors": [
        {
          "_id": "686490e9d59a9eda59024a64",
          "name": "Maggie Huan",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a65",
          "name": "Yuetai Li",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a66",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tuney Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:39:59.058Z",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a67",
          "name": "Xiaoyu Xu",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a68",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a69",
          "name": "Minxin Du",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a6a",
          "name": "Radha Poovendran",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a6b",
          "name": "Graham Neubig",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a6c",
          "name": "Xiang Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-01T05:23:05.000Z",
      "submittedOnDailyAt": "2025-07-02T00:24:45.275Z",
      "title": "수학논리가 일반적인 LLM의 능력을 향상시킬 수 있을까? LLM의 논리의 전달성을 이해하기 위한 연구",
      "submittedOnDailyBy": {
        "_id": "6230d750d93e84e233882dbc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg",
        "isPro": false,
        "fullname": "Xiang Yue",
        "user": "yuexiang96",
        "type": "user"
      },
      "summary": "수학 추론은 대형 언어 모델(LLMs)의 발전의 예시로, MATH와 AIME 등 기준 테스트에서 인간 수준을 빠르게 초월하는 새로운 모델이 등장했다. 그러나 수학 순위每周가 진화하는 가운데, 이러한 진화는 더 넓은 문제를 해결하는 능력에 반영되어 있는지, 또는 단순히 좁은 과적합인지를 묻는 것이 중요하다. 이를 대답하기 위해, 우리는 수학, 과학 질문, 에이전트 계획, 코드 작성, 표준 명령 따르기 등 다양한 태스크를 포함하여 20개 이상의 오픈 가중치의 추론 미세 조정 모델을 평가했다. 놀랍게도, 대부분의 수학에서 성공한 모델은 다른 분야로 이점을 옮기는 데 실패했다. 이를 엄격히 연구하기 위해, 우리는 Qwen3-14B 모델을 수만 수학 데이터와 다른 미세 조정 방법으로 제어 실험을 수행했다. 우리는 강화 학습(RL) 미세 조정 모델이 여러 분야에서 좋은 일반화 능력을 발휘하는 반면, 감독 미세 조정(SFT) 미세 조정 모델은 일반적으로 일반적인 능력을 잊는 것을 발견했다. 잠재 공간 표현과 토큰 공간 분포 변화 분석에 따르면, SFT는 상당한 표현과 출력 변동을 초래하지만, RL은 일반적인 영역 구조를 유지한다. 우리의 연구 결과는 표준의 후 훈련 레시피를 재검토하고, 특히 SFT 분식 데이터의 의존성을 개선하기 위한 데 필요한 것이다.",
      "upvotes": 22,
      "discussionId": "686490e9d59a9eda59024a6d",
      "githubRepo": "https://github.com/ReasoningTransfer/Transferability-of-LLM-Reasoning",
      "ai_summary": "Reinforcement learning-tuned models outperform supervised fine-tuned models in generalizing mathematical problem-solving abilities to other domains, indicating a need to re-evaluate training methods for reasoning models.",
      "ai_keywords": [
        "reinforcement learning",
        "supervised fine-tuning",
        "latent-space representation",
        "token-space distribution shift",
        "general-domain structure"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-07-01T01:23:05.000Z",
    "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning",
    "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00432.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6230d750d93e84e233882dbc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg",
      "fullname": "Xiang Yue",
      "name": "yuexiang96",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19852",
      "authors": [
        {
          "_id": "685d790d61ef876fd500e925",
          "name": "Xingyang Li",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e926",
          "user": {
            "_id": "63129589bbaa385279d1826e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg",
            "isPro": false,
            "fullname": "Muyang Li",
            "user": "Lmxyy",
            "type": "user"
          },
          "name": "Muyang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-27T08:57:19.218Z",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e927",
          "name": "Tianle Cai",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e928",
          "name": "Haocheng Xi",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e929",
          "name": "Shuo Yang",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e92a",
          "name": "Yujun Lin",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e92b",
          "name": "Lvmin Zhang",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e92c",
          "name": "Songlin Yang",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e92d",
          "name": "Jinbo Hu",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e92e",
          "name": "Kelly Peng",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e92f",
          "name": "Maneesh Agrawala",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e930",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e931",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e932",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63129589bbaa385279d1826e/X6ojt3M2m0VZGqlL3M-my.mp4"
      ],
      "publishedAt": "2025-06-24T17:59:59.000Z",
      "submittedOnDailyAt": "2025-07-02T02:22:22.027Z",
      "title": "라디アル 아텐션: O(nlog n) 스패르스 아텐션을 이용한 에너지 감쇠를 활용한 장기 비디오 생성",
      "submittedOnDailyBy": {
        "_id": "63129589bbaa385279d1826e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg",
        "isPro": false,
        "fullname": "Muyang Li",
        "user": "Lmxyy",
        "type": "user"
      },
      "summary": "최근의 Dif- ference 모델의 발전은 고품질의 비디오 생성이 가능해지는 데 기여하지만, 추가된 시간 차원은 계산 비용에 크게 영향을 미치고, 긴 비디오의 훈련과 추론이 부담이 커졌습니다. 본 논문에서는 비디오 Dif- ference 모델에서 공간-시간 에너지 감쇠(Spatiotemporal Energy Decay) 현상을 식별했습니다: 태그 간의 공간 및 시간의 거리가 멀어질수록 소프트맥스 후의 注意 스코어가 감쇠되며, 자연의 신호 또는 파동의 공간과 시간의 물리적 감쇠에 비슷합니다. 이를 기반으로 O(n log n) 복잡도의 Scalable Sparse Attention 구조인 Radial Attention을 제안합니다. Radial Attention은 에너지 감쇠를 지수적으로 감쇠하는 계산 밀도로 변환하여, 표준의 O(n^2)의 밀접한 注意보다 크게 효율적이고, 선형 注意보다 표현력이 높습니다. Radial Attention은 간단한静的 注意 마스크를 사용하며, 각 태그는 공간적으로 가까운 태그에 주의를 분배하고, 주의 윈도우 크기는 시간의 거리에 따라 축소됩니다. 또한, 사전 학습된 비디오 Dif- ference 모델이 LoRA를 기반으로 효율적인 미세 조정으로 생성력을 확장하는 것을 가능하게 합니다. 확장된 실험 결과에 따르면, Radial Attention은 Wan2.1-14B, HunyuanVideo, Mochi 1에서 비디오의 품질을 유지하면서, 원의 밀접한 注意보다 1.9배의 속도 향상을 실현했습니다. 최소한의 조정으로, 비디오 생성을 4배로 확장하고, 직접적인 미세 조정보다 4.4배의 훈련 비용 절감과 밀접한 注意 추론보다 3.7배의 추론 가속을 실현할 수 있습니다.",
      "upvotes": 14,
      "discussionId": "685d790d61ef876fd500e933",
      "projectPage": "https://hanlab.mit.edu/projects/radial-attention",
      "githubRepo": "https://github.com/mit-han-lab/radial-attention/",
      "ai_summary": "Radial Attention, a scalable sparse attention mechanism, improves efficiency and preserves video quality in diffusion models by leveraging spatiotemporal energy decay.",
      "ai_keywords": [
        "Spatiotemporal Energy Decay",
        "diffusion models",
        "radial attention",
        "attention scores",
        "dense attention",
        "linear attention",
        "attention mask",
        "LoRA-based fine-tuning",
        "video quality",
        "Wan2.1-14B",
        "HunyuanVideo",
        "Mochi 1"
      ],
      "githubStars": 211
    },
    "publishedAt": "2025-06-24T13:59:59.000Z",
    "title": "Radial Attention: O(nlog n) Sparse Attention with Energy Decay for\n  Long Video Generation",
    "summary": "Recent advances in diffusion models have enabled high-quality video\ngeneration, but the additional temporal dimension significantly increases\ncomputational costs, making training and inference on long videos prohibitively\nexpensive. In this paper, we identify a phenomenon we term Spatiotemporal\nEnergy Decay in video diffusion models: post-softmax attention scores diminish\nas spatial and temporal distance between tokens increase, akin to the physical\ndecay of signal or waves over space and time in nature. Motivated by this, we\npropose Radial Attention, a scalable sparse attention mechanism with O(n log\nn) complexity that translates energy decay into exponentially decaying compute\ndensity, which is significantly more efficient than standard O(n^2) dense\nattention and more expressive than linear attention. Specifically, Radial\nAttention employs a simple, static attention mask where each token attends to\nspatially nearby tokens, with the attention window size shrinking with temporal\ndistance. Moreover, it allows pre-trained video diffusion models to extend\ntheir generation length with efficient LoRA-based fine-tuning. Extensive\nexperiments show that Radial Attention maintains video quality across\nWan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9times speedup\nover the original dense attention. With minimal tuning, it enables video\ngeneration up to 4times longer while reducing training costs by up to\n4.4times compared to direct fine-tuning and accelerating inference by up to\n3.7times compared to dense attention inference.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63129589bbaa385279d1826e/X6ojt3M2m0VZGqlL3M-my.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19852.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63129589bbaa385279d1826e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg",
      "fullname": "Muyang Li",
      "name": "Lmxyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.20639",
      "authors": [
        {
          "_id": "685d2773696820ba1f28f38c",
          "name": "Shansan Gong",
          "hidden": false
        },
        {
          "_id": "685d2773696820ba1f28f38d",
          "name": "Ruixiang Zhang",
          "hidden": false
        },
        {
          "_id": "685d2773696820ba1f28f38e",
          "name": "Huangjie Zheng",
          "hidden": false
        },
        {
          "_id": "685d2773696820ba1f28f38f",
          "name": "Jiatao Gu",
          "hidden": false
        },
        {
          "_id": "685d2773696820ba1f28f390",
          "name": "Navdeep Jaitly",
          "hidden": false
        },
        {
          "_id": "685d2773696820ba1f28f391",
          "name": "Lingpeng Kong",
          "hidden": false
        },
        {
          "_id": "685d2773696820ba1f28f392",
          "name": "Yizhe Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T17:35:47.000Z",
      "submittedOnDailyAt": "2025-07-02T01:51:58.159Z",
      "title": "DiffuCoder: 코드 생성을 위한 마스크付き 디퓨션 모델의 이해와 개선\n\n(注意：原文中的“コード生成のためのマスク付きディフュージョンモデルの理解と改善”在翻译时，考虑到“DiffuCoder”可能是一个专有名词，因此直接保留了“DiffuCoder”，而非将其翻译成“디퓨션 모델을 위한 코드 생성을 위한 마스크付き 디퓨션 모델의 이해와 개선”。如果需要完全翻译，可以考虑将“DiffuCoder”翻译为“DiffuCoder: 코드 생성을 위한 마스크付き 디퓨션 모델의 이해와 개선”。)",
      "submittedOnDailyBy": {
        "_id": "628c83d186fc004b14e1ed48",
        "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg",
        "isPro": false,
        "fullname": "Shansan Gong",
        "user": "Sansa",
        "type": "user"
      },
      "summary": "Diffusion large language models (dLLMs)는 autoregressive (AR) 모델에 대한 유효한 대체로 나타났으며, 이는 그 모델이 전체 시퀀스를 대상으로 하는 데에 있다. dLLMs의 글로벌 계획 및 반복적인 리핏 기능은 코드 생성에 특히 도움이 됩니다. 그러나 현재의 dLLMs의 코드 관련 훈련 및 추론 구조는 조사가 부족합니다. dLLMs의 디코딩 행동을 이해하고 코드의 가능성 개발하기 위해, 우리는 그 디노이징 프로세스와 강화학습 (RL) 방법론을 체계적으로 조사합니다. 130B 토큰의 코드를 사용하여 7B dLLM 모델인 DiffuCoder를 훈련합니다. 이 모델을 테스트 벤치marks로 사용하여, 그 디코딩 행동을 분석하고 AR 모델과의 차이를 밝혀봅니다: (1) dLLMs는 반현대 효과적인 디코딩에 의존하지 않고, 카우시컬한 생성을 결정할 수 있습니다. (2) 샘플링 온도를 증가시키면, 태그의 선택 및 생성 순서가 다양화됩니다. 이 다양성은 RL의 로드아웃의 풍부한 탐색 공간을 만들어줍니다. RL 훈련을 위해, 토큰의 로그 라이버 분산을 줄이고 훈련 효율성을 유지하기 위해, 우리는 coupled-GRPO라는 새로운 샘플링 스키ー밍을 제안합니다. coupled-GRPO는 훈련 중 사용된 완료를 컴파일러적 마스크 노이즈를 구축함으로써 분산을 줄입니다. 실험에서, coupled-GRPO는 EvalPlus 벤치마크의 코드 성능을 뚜렷이 향상시키고, 디코딩 시 AR 카우시컬 의존성을 줄입니다. 이 연구는 dLLM의 기계를 깊게 이해하고 효과적인, diffusion 노트 기반의 RL 훈련 프레임워크를 제공합니다. https://github.com/apple/ml-diffucoder.",
      "upvotes": 12,
      "discussionId": "685d2774696820ba1f28f393",
      "githubRepo": "https://github.com/apple/ml-diffucoder",
      "ai_summary": "Diffusion large language models are applied to code generation, revealing their unique denoising processes and benefiting from a novel reinforcement learning sampling scheme.",
      "ai_keywords": [
        "diffusion large language models",
        "autoregressive models",
        "denoising models",
        "global planning",
        "iterative refinement",
        "code generation",
        "decoding behavior",
        "causal generation",
        "sampling temperature",
        "coupled-GRPO",
        "token log-likelihood estimates",
        "RL rollouts"
      ],
      "githubStars": 58
    },
    "publishedAt": "2025-06-25T13:35:47.000Z",
    "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation",
    "summary": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, DiffuCoder, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose coupled-GRPO, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR causal during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "628c83d186fc004b14e1ed48",
      "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg",
      "fullname": "Shansan Gong",
      "name": "Sansa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21277",
      "authors": [
        {
          "_id": "68634a2c588cea0da970c8ae",
          "user": {
            "_id": "66a097801a26a2350395edc7",
            "avatarUrl": "/avatars/1e7e127cb7222df7d56e5bfda6bab519.svg",
            "isPro": false,
            "fullname": "Qize Yang",
            "user": "PhilipC",
            "type": "user"
          },
          "name": "Qize Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:40:53.951Z",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8af",
          "name": "Shimin Yao",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b0",
          "name": "Weixuan Chen",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b1",
          "user": {
            "_id": "67067633351e0c16a5c27497",
            "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
            "isPro": false,
            "fullname": "Shenghao Fu",
            "user": "fushh7",
            "type": "user"
          },
          "name": "Shenghao Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:40:56.838Z",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b2",
          "name": "Detao Bai",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b3",
          "name": "Jiaxing Zhao",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b4",
          "user": {
            "_id": "66ef2611fcc1c455f8dce832",
            "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
            "isPro": false,
            "fullname": "Boyuan Sun",
            "user": "BBBBCHAN",
            "type": "user"
          },
          "name": "Boyuan Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:33.669Z",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b5",
          "name": "Bowen Yin",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b6",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b7",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T14:01:03.000Z",
      "submittedOnDailyAt": "2025-07-02T00:57:00.933Z",
      "title": "HumanOmniV2: \"텍스트를 포함한 모든 측면의 이해에서 모든 측면의 논리로\"",
      "submittedOnDailyBy": {
        "_id": "67067633351e0c16a5c27497",
        "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
        "isPro": false,
        "fullname": "Shenghao Fu",
        "user": "fushh7",
        "type": "user"
      },
      "summary": "다모달 대언어 모뎀의 급속한 발전에 따라, 깊은 인간 의도를 이해하고 해석하는 능력이 중요하게 인정되어, 구체적인 내용을 고려하는 것이 필요합니다. 최근의 연구에서는, 강화학습(RL)이 대언어 모뎀(LLM)의 의사결정 능력 향상에 가능성을 보여주지만, RL을 다모달 데이터와 형식에 적용할 때 발생하는 문제들은 아직도 해결되지 않고 있습니다. 본 논문에서는, 현재의 다모달 의사결정 모뎀에서 두 가지 주요 문제를 밝혀 있습니다: 전역적인 컨텍스트 이해의 부족과 스로트 쇼프 문제. 컨텍스트 이해의 부족은 모델이 다모달 컨텍스트를 잘못 해석하여 부정확한 답을 제시하는 경우가 있습니다. 스로트 쇼프 문제는 모델이 다모달 입력의 중요한 퀴즈를 낚이지 않고, 질문을 직접 해결하면서 다모달 정보를 고려하지 않는 현상으로 인해 발생합니다. 이러한 문제를 해결하기 위해, 모델이 다모달 입력에서 명확한 전역적인 컨텍스트를 이해하는 필요성을 강조하고 있습니다. 이러한 전역적인 컨텍스트 이해는 모델이 중요한 다모달 퀴즈를 낚이지 않도록, 구체적인 의사결정 과정이 보장될 수 있습니다. 다모달 컨텍스트 정보를 정확한 해석하기 위해, 대언어 모뎀에 의한 컨텍스트 보상, 형식 보상 및 정확도 보상을 계산하여 구현하고 있습니다. 또한, 복잡한 의사결정 능력을 향상시키기 위해, LLM을 사용하여 의사결정 과정이 로지컬한 방법으로 다모달 정보를 성공적으로 통합하는지 판단하는 로지컬 보상을 계산하고 있습니다. 또한, 복잡한 인간 의도와 감정을 이해하는 모뎀의 평가에 대한 문제에 대해, 의사결정 Omni-Modal 벤치마크, IntentBench를 소개하고 있습니다. 제안된 방법은 다른 오픈 소스 Omni-Modal 모뎀과 비교하여 다수의 Omni-Modal 벤치마크에서 선진적인 성능을 보여주고 있습니다.",
      "upvotes": 8,
      "discussionId": "68634a2c588cea0da970c8b8",
      "githubRepo": "https://github.com/HumanMLLM/HumanOmniV2",
      "ai_summary": "A reinforcement learning-based approach enhances multimodal reasoning by addressing context understanding and shortcut problems, using context, format, accuracy, and logical rewards, and achieving superior performance on the IntentBench benchmark.",
      "ai_keywords": [
        "Reinforcement Learning",
        "Large Language Models",
        "multimodal reasoning",
        "global context understanding",
        "shortcut problem",
        "context reward",
        "logical reward",
        "IntentBench",
        "multimodal benchmark"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-06-26T10:01:03.000Z",
    "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context",
    "summary": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21277.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67067633351e0c16a5c27497",
      "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
      "fullname": "Shenghao Fu",
      "name": "fushh7",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.00951",
      "authors": [
        {
          "_id": "6864a0cdd59a9eda59024ad4",
          "name": "Rizwan Qureshi",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ad5",
          "name": "Ranjan Sapkota",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ad6",
          "name": "Abbas Shah",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ad7",
          "name": "Amgad Muneer",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ad8",
          "name": "Anas Zafar",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ad9",
          "name": "Ashmal Vayani",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ada",
          "name": "Maged Shoman",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024adb",
          "name": "Abdelrahman B. M. Eldaly",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024adc",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024add",
          "name": "Ferhat Sadak",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ade",
          "user": {
            "_id": "5e466ca12d2efc729dc309ad",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1601989543860-5e466ca12d2efc729dc309ad.png",
            "isPro": false,
            "fullname": "shaina",
            "user": "shainaraza",
            "type": "user"
          },
          "name": "Shaina Raza",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:38:56.857Z",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024adf",
          "name": "Xinqi Fan",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae0",
          "name": "Ravid Shwartz-Ziv",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae1",
          "name": "Hong Yan",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae2",
          "name": "Vinjia Jain",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae3",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:38:59.004Z",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae4",
          "name": "Manoj Karkee",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae5",
          "name": "Jia Wu",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae6",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae7",
          "name": "Seyedali Mirjalili",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/sRonQRwdqad3gk75XnJAO.png"
      ],
      "publishedAt": "2025-07-01T16:52:25.000Z",
      "submittedOnDailyAt": "2025-07-02T01:32:29.326Z",
      "title": "토큰을 초월하여 생각하기: 뇌를 모델로 하는 지능으로부터, 인공 일반 지능의 인지적 기초와 사회적 영향",
      "submittedOnDailyBy": {
        "_id": "67ddd80896ac367438d400a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
        "isPro": false,
        "fullname": "Ranjan Sapkota",
        "user": "RanjanSapkota",
        "type": "user"
      },
      "summary": "머신이 진정으로 기억하고 이유를 제시하며, 인간처럼 특정 영역에서 사고하고 행동할 수 있는지는 지속된 질문으로, 이 질문은 인공지능(AGI)의 추구에 연결되어 있다. GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, Grok 3 등 모델의 기능이 늘어날 때에도, 이러한 시스템은 토큰 수준의 예측에 의존하며, 지역적 지능을 갖지 않기 때문에, 본질적으로 한계가 있다. 본 연구에서는 인공지능, 인지 신경과학, 심리학, 생성 모델, 에이전트 기반 시스템 등 광범위한 분야에서 AGI의 개발을 가로지르는 합성을 수행하고, 일반적인 지능의 아키텍처와 인지적 기초를 분석하며, 모듈러적인 이유, 지속적인 기억, 다 에이전트의 협조의 역할 등을 밝혀내는 데 중점을 둔다. 특히, 에이전트 프레임워크의 발전에 중점을 두고, 흡수, 계획, 동적인 도구의 조합을 사용하여 적응적인 행동을 가능하게 할 수 있도록 강조한다. 일반화 전략, 정보 압축, 테스트 시의 적응, 훈련이 필요 없는 방법 등을 검토하고, 유연한, 영역과 관계없는 지능을 위해 중요한 패스워드로 여기며, 시각 언어 모델(VLMs)은 시각 모듈의 시각적인 관점을 바꾼다. 시각의 이해와 협력의 작업 완료 인터페이스로 재검토되고, 진정한 지능은 규모만이 아니라, 기억과 이유의 통합에서 생겨나며, 모듈러, 상호작용, 자동 개선의 컴포넌트의 오케스트레이터로, 압축이 적응적인 행동을 가능하게 하는 데 중점을 둔다. 신경符号 시스템, 강화 학습, 인지 스키프 디닝의 발전을 기반으로, 통계적 학습과 목표 지향적 인지 사이에서의 간극을 연결하는 최근의 아키텍처를 조사한다. 마지막으로, AGI로의 과학적, 기술적, 윤리적 문제를 명확히 한다.",
      "upvotes": 7,
      "discussionId": "6864a0ced59a9eda59024ae8",
      "ai_summary": "The paper synthesizes the interdisciplinary approach to achieving Artificial General Intelligence, emphasizing modular reasoning, memory, multi-agent coordination, and the integration of neurosymbolic systems and reinforcement learning to overcome current model limitations.",
      "ai_keywords": [
        "Artificial General Intelligence (AGI)",
        "Agentic RAG frameworks",
        "retrieval",
        "planning",
        "dynamic tool use",
        "generalization strategies",
        "information compression",
        "test-time adaptation",
        "training-free methods",
        "Vision-Language Models (VLMs)",
        "neurosymbolic systems",
        "reinforcement learning",
        "cognitive scaffolding"
      ]
    },
    "publishedAt": "2025-07-01T12:52:25.000Z",
    "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive\n  Foundations for Artificial General Intelligence and its Societal Impact",
    "summary": "Can machines truly think, reason and act in domains like humans? This\nenduring question continues to shape the pursuit of Artificial General\nIntelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,\nDeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal\nfluency and partial reasoning, these systems remain fundamentally limited by\ntheir reliance on token-level prediction and lack of grounded agency. This\npaper offers a cross-disciplinary synthesis of AGI development, spanning\nartificial intelligence, cognitive neuroscience, psychology, generative models,\nand agent-based systems. We analyze the architectural and cognitive foundations\nof general intelligence, highlighting the role of modular reasoning, persistent\nmemory, and multi-agent coordination. In particular, we emphasize the rise of\nAgentic RAG frameworks that combine retrieval, planning, and dynamic tool use\nto enable more adaptive behavior. We discuss generalization strategies,\nincluding information compression, test-time adaptation, and training-free\nmethods, as critical pathways toward flexible, domain-agnostic intelligence.\nVision-Language Models (VLMs) are reexamined not just as perception modules but\nas evolving interfaces for embodied understanding and collaborative task\ncompletion. We also argue that true intelligence arises not from scale alone\nbut from the integration of memory and reasoning: an orchestration of modular,\ninteractive, and self-improving components where compression enables adaptive\nbehavior. Drawing on advances in neurosymbolic systems, reinforcement learning,\nand cognitive scaffolding, we explore how recent architectures begin to bridge\nthe gap between statistical learning and goal-directed cognition. Finally, we\nidentify key scientific, technical, and ethical challenges on the path to AGI.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/sRonQRwdqad3gk75XnJAO.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ddd80896ac367438d400a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
      "fullname": "Ranjan Sapkota",
      "name": "RanjanSapkota",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21545",
      "authors": [
        {
          "_id": "6864cbbad59a9eda59024b40",
          "name": "Yalun Dai",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b41",
          "name": "Yangyu Huang",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b42",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b43",
          "name": "Wenshan Wu",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b44",
          "name": "Chong Li",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b45",
          "name": "Wenhui Lu",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b46",
          "name": "Shijie Cao",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b47",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b48",
          "name": "Scarlett Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T17:59:07.000Z",
      "submittedOnDailyAt": "2025-07-02T04:34:17.870Z",
      "title": "언어 모델의 데이터 효과성 훈련",
      "submittedOnDailyBy": {
        "_id": "64d98ef7a4839890b25eb78b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
        "isPro": true,
        "fullname": "Fangyuan Yu",
        "user": "Ksgk-fy",
        "type": "user"
      },
      "summary": "데이터는 언어 모델(LM)의 훈련의 기초로 존재합니다. 최근의 연구는 데이터의 효율성에 초점을 맞추고 있습니다. 이는 훈련 데이터의 최소한이나 최적의 서브셋을 선택하여 성능을 최대화하는 것을 목표로 합니다. 데이터의 필터링, 샘플링, 선택 등 기술이 이 분야에서 중요한 역할을 수행하고 있습니다. 이를 보완하기 위해, 데이터의 효율성을 최대화하는 것을 목표로 하는 Data Efficacy를 정의했습니다. 이는 훈련 데이터의 조직 최적화를 중점적으로 다루며, 조사가 적은 영역에 집중합니다. 이 연구에서, LM의 훈련에 데이터의 효율성을 고려하는 일반적인 패러다임인 DELT를 소개합니다. 이는 훈련 데이터의 조직의 중요성을 강조합니다. DELT는 3가지 요소로 구성되어 있습니다: 데이터 스코어, 데이터 선택, 데이터 순서. 이러한 요소 중 하나인 학습 가능성과 품질을 경사 일치의 관점에서 검토하는 새로운 데이터 스코어의 예로, Learnability-Quality Scoring(LQS)를 설계했습니다. 또한 모델의 잊혀짐이나 데이터 분포 편향 등 문제를 해결하기 위해, 데이터 순서를 설계했습니다. 데이터의 효율성을 증명하기 위해 LM의 훈련에서 데이터의 효율성을 고려하는 구체적인 실험을 진행했습니다. 이를 통해 다음과 같은 결과를 얻었습니다. 첫째, 제안된 DELT의 각 예는 데이터 크기와 모델 크기를 늘리지 않고 LM의 성능을 다양한 수준으로 향상시킬 수 있습니다. 둘째, 이러한 예들 중 가장 눈에 띄는 개선을 얻는 것이 제안된 LQS와 Folding의 데이터 순서를 결합한 것입니다. 셋째, 데이터 선택을 적용하면 데이터의 효율성과 효율성을 동시에 달성할 수 있습니다. 따라서, 데이터의 효율성은 LM의 훈련의 기초적인 영역으로 인정할 수 있습니다.",
      "upvotes": 4,
      "discussionId": "6864cbbbd59a9eda59024b49",
      "ai_summary": "DELT, a paradigm for enhancing language model performance through data efficacy, consists of data scoring, selection, and ordering, demonstrating significant improvements without increasing data scale or model size.",
      "ai_keywords": [
        "data efficiency",
        "language models",
        "data efficacy",
        "DELT",
        "Data Scoring",
        "Data Selection",
        "Data Ordering",
        "Learnability-Quality Scoring",
        "LQS",
        "Folding Ordering",
        "model forgetting",
        "data distribution bias"
      ]
    },
    "publishedAt": "2025-06-26T13:59:07.000Z",
    "title": "Data Efficacy for Language Model Training",
    "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21545.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d98ef7a4839890b25eb78b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
      "fullname": "Fangyuan Yu",
      "name": "Ksgk-fy",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22960",
      "authors": [
        {
          "_id": "6864a811d59a9eda59024afe",
          "name": "Shreyas Dixit",
          "hidden": false
        },
        {
          "_id": "6864a811d59a9eda59024aff",
          "name": "Ashhar Aziz",
          "hidden": false
        },
        {
          "_id": "6864a811d59a9eda59024b00",
          "name": "Shashwat Bajpai",
          "hidden": false
        },
        {
          "_id": "6864a811d59a9eda59024b01",
          "name": "Vasu Sharma",
          "hidden": false
        },
        {
          "_id": "6864a811d59a9eda59024b02",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:38:48.204Z",
          "hidden": false
        },
        {
          "_id": "6864a811d59a9eda59024b03",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "6864a811d59a9eda59024b04",
          "name": "Amitava Das",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T17:34:08.000Z",
      "submittedOnDailyAt": "2025-07-02T02:11:36.000Z",
      "title": "Pekaoi: 시각적인 파라프레이즈 공격 안전에 대한 이상 없는 이미지 마킹 방법 (AI 생성 이미지에 대한)",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "EU의 법무청 관련 기관의 보고서에 따르면, 2026년에는 90%의 온라인 콘텐츠가 합성적으로 생성될 수 있는 가능성이 있으며, 정책 제정자도 이러한 가능성을 우려하고 있습니다. 이 경고는 \"Generative AI는 정치적 부정정보의 힘을 강화할 수 있습니다. 생성된 텍스트, 이미지, 영상, 음성의 복합적 영향은 어떤 단일 모델도 초월할 수 있는 가능성이 있습니다\"라고 합니다. 이에 반해, 캘리포니아의 Bill AB 3211은 AI 생성 이미지, 영상, 음성의 워마킹을 강제하고 있습니다. 그러나 무시 가능한 워마킹 방법의 취약성과, 악의 있는 제작자가 워마킹을 완전히 회피할 가능성에 대한 우려는 남아 있습니다. Generative AI에 의한 디이터마킹 공격, 특히 신규로 도입된 시각적인 재언어화 공격은 원본 이미지의 워마킹을 완전히 제거하고, 원본 이미지의 재언어화 능력을 보여주고 있습니다. 본 논문에서는, 시각적인 재언어화 공격에서 안전하고 파괴되지 않는 이미지 워마킹 방법인 PECCAVI를 소개합니다. 시각적인 재언어화 공격에서, 이미지의 핵심적인 의미 영역을 유지하면서 이미지를 변경하고, 이 영역을 Non-Melting Points (NMPs)라고 부르는 것입니다. PECCAVI는 이러한 NMPs 내부에 전략적으로 워마킹을 넣으며, 다채널 주파수 영역 워마킹을 사용합니다. 역공학의 노력에 대한 NMPs의 위치를 특정하는 것을 방해하기 위해 노이즈 부닝도 사용하며, 워마킹의 견고성을 향상시킵니다. PECCAVI는 모델 독립입니다. 모든 관련 리소스와 코드는 오픈 소스로 공개됩니다.",
      "upvotes": 1,
      "discussionId": "6864a811d59a9eda59024b05",
      "ai_summary": "PECCAVI is a robust image watermarking technique that is resistant to visual paraphrase attacks and distortions, utilizing NMPs and multi-channel frequency domain watermarking.",
      "ai_keywords": [
        "Generative AI",
        "visual paraphrase attack",
        "Non-Melting Points (NMPs)",
        "multi-channel frequency domain watermarking",
        "noisy burnishing",
        "model-agnostic"
      ]
    },
    "publishedAt": "2025-06-28T13:34:08.000Z",
    "title": "Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image\n  Watermarking Technique for AI-Generated Images",
    "summary": "A report by the European Union Law Enforcement Agency predicts that by 2026,\nup to 90 percent of online content could be synthetically generated, raising\nconcerns among policymakers, who cautioned that \"Generative AI could act as a\nforce multiplier for political disinformation. The combined effect of\ngenerative text, images, videos, and audio may surpass the influence of any\nsingle modality.\" In response, California's Bill AB 3211 mandates the\nwatermarking of AI-generated images, videos, and audio. However, concerns\nremain regarding the vulnerability of invisible watermarking techniques to\ntampering and the potential for malicious actors to bypass them entirely.\nGenerative AI-powered de-watermarking attacks, especially the newly introduced\nvisual paraphrase attack, have shown an ability to fully remove watermarks,\nresulting in a paraphrase of the original image. This paper introduces PECCAVI,\nthe first visual paraphrase attack-safe and distortion-free image watermarking\ntechnique. In visual paraphrase attacks, an image is altered while preserving\nits core semantic regions, termed Non-Melting Points (NMPs). PECCAVI\nstrategically embeds watermarks within these NMPs and employs multi-channel\nfrequency domain watermarking. It also incorporates noisy burnishing to counter\nreverse-engineering efforts aimed at locating NMPs to disrupt the embedded\nwatermark, thereby enhancing durability. PECCAVI is model-agnostic. All\nrelevant resources and codes will be open-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22960.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.00162",
      "authors": [
        {
          "_id": "6864d9fad59a9eda59024b7e",
          "name": "Yu Lu",
          "hidden": false
        },
        {
          "_id": "6864d9fad59a9eda59024b7f",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T18:11:21.000Z",
      "submittedOnDailyAt": "2025-07-02T07:48:03.620Z",
      "title": "FreeLong++: 훈련 없이 긴 비디오 생성에 의한 다 band 스펙트럼 융합",
      "submittedOnDailyBy": {
        "_id": "639ae93c3786549794e97c69",
        "avatarUrl": "/avatars/65f7c0b641145f68f22072a5f77e086d.svg",
        "isPro": true,
        "fullname": "YL",
        "user": "Simase",
        "type": "user"
      },
      "summary": "최근의 이미지 생성 모델의 발전으로 텍스트 기반에서 고품질의 짧은 이미지의 생성이 가능해졌습니다. 그러나 이러한 모델을 긴 이미지에 확장하는 것은 주로 시간적 일관성과 시각적 품질의 저하로 큰 문제를 초래합니다. 우리 초기의 관찰에서 짧은 이미지 생성 모델을 단순히 긴 시퀀스로 적용하면 품질의 저하가 확인될 수 있습니다. 심화된 분석에서는 이미지의 길이가 증가함에 따라 고주파 성분이 순차적으로 왜곡되는 체계적인 경향을 발견하고, 이를 고주파 왜곡이라고 합니다. 이를 해결하기 위해 FreeLong라는 훈련없이 사용할 수 있는 프레임워크를 제안합니다. 이 프레임워크는 긴 이미지의 특징량의 주파수 분포를 노이즈 처리 중 균형을 맞추기 위해 설계되었습니다. FreeLong은 전체적인 저주파 특징량(전체적인 의미를 이해)과 짧은 시간 윈도우에서 추출되는 고주파 특징량을 융합하여 미세한 세부 사항을 유지함으로써 이 목표를 달성합니다. 이를 기반으로 FreeLong++는 FreeLong의 두 가지 브랜치 설계를 여러 개의 어텐션 브랜치의 다중 브랜치 아키텍처로 확장합니다. 이들은 서로 다른 시간 스케일에서 동작합니다. 글로벌에서 로컬의 프레임 크기를 병렬로 배치하여 FreeLong++는 저주파부터 고주파까지 다밴드 주파수의 융합을 가능하게 하며, 긴 이미지 시퀀스에서도 의미의 일관성과 미세한 동적을 유지합니다. 추가적인 훈련이 필요하지 않으므로, FreeLong++는 현재의 이미지 생성 모델(예: Wan2.1과 LTX-Video)에 플러그인으로 설치할 수 있으며, 시간적 일관성과 시각적 품질을 크게 향상시켜 긴 이미지 생성이 가능합니다. 우리들은 이 접근법이 이전의 방법보다 더 높은 수준으로 긴 이미지 생성 작업에 적용할 수 있음을 보여주고, 연결된 다플랜트 이미지 생성을 지원하고, 깊은 깊이와 포즈 시퀀스를 사용한 제어 가능한 이미지 생성을 가능하게 합니다.",
      "upvotes": 0,
      "discussionId": "6864d9fad59a9eda59024b80"
    },
    "publishedAt": "2025-06-30T14:11:21.000Z",
    "title": "FreeLong++: Training-Free Long Video Generation via Multi-band\n  SpectralFusion",
    "summary": "Recent advances in video generation models have enabled high-quality short\nvideo generation from text prompts. However, extending these models to longer\nvideos remains a significant challenge, primarily due to degraded temporal\nconsistency and visual fidelity. Our preliminary observations show that naively\napplying short-video generation models to longer sequences leads to noticeable\nquality degradation. Further analysis identifies a systematic trend where\nhigh-frequency components become increasingly distorted as video length grows,\nan issue we term high-frequency distortion. To address this, we propose\nFreeLong, a training-free framework designed to balance the frequency\ndistribution of long video features during the denoising process. FreeLong\nachieves this by blending global low-frequency features, which capture holistic\nsemantics across the full video, with local high-frequency features extracted\nfrom short temporal windows to preserve fine details. Building on this,\nFreeLong++ extends FreeLong dual-branch design into a multi-branch architecture\nwith multiple attention branches, each operating at a distinct temporal scale.\nBy arranging multiple window sizes from global to local, FreeLong++ enables\nmulti-band frequency fusion from low to high frequencies, ensuring both\nsemantic continuity and fine-grained motion dynamics across longer video\nsequences. Without any additional training, FreeLong++ can be plugged into\nexisting video generation models (e.g. Wan2.1 and LTX-Video) to produce longer\nvideos with substantially improved temporal consistency and visual fidelity. We\ndemonstrate that our approach outperforms previous methods on longer video\ngeneration tasks (e.g. 4x and 8x of native length). It also supports coherent\nmulti-prompt video generation with smooth scene transitions and enables\ncontrollable video generation using long depth or pose sequences.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00162.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639ae93c3786549794e97c69",
      "avatarUrl": "/avatars/65f7c0b641145f68f22072a5f77e086d.svg",
      "fullname": "YL",
      "name": "Simase",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]