[
  {
    "paper": {
      "id": "2505.14683",
      "authors": [
        {
          "_id": "682d2fd84540abccd3b835e8",
          "name": "Chaorui Deng",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835e9",
          "name": "Deyao Zhu",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ea",
          "user": {
            "_id": "61fb81006374891646732f37",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg",
            "isPro": false,
            "fullname": "Kunchang Li",
            "user": "Andy1621",
            "type": "user"
          },
          "name": "Kunchang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:41:06.469Z",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835eb",
          "user": {
            "_id": "652e9c5774d1b0d7ff73d091",
            "avatarUrl": "/avatars/a6d2098b3dde4a8b7488a193f0ecb776.svg",
            "isPro": true,
            "fullname": "Chenhui Gou",
            "user": "gouc",
            "type": "user"
          },
          "name": "Chenhui Gou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:41:08.903Z",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ec",
          "name": "Feng Li",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ed",
          "name": "Zeyu Wang",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ee",
          "name": "Shu Zhong",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ef",
          "user": {
            "_id": "5df833bdda6d0311fd3d5403",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5df833bdda6d0311fd3d5403/62OtGJEQXdOuhV9yCd4HS.png",
            "isPro": false,
            "fullname": "Weihao Yu",
            "user": "whyu",
            "type": "user"
          },
          "name": "Weihao Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T09:31:55.569Z",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835f0",
          "user": {
            "_id": "64b6b81142134e053233c3c0",
            "avatarUrl": "/avatars/5c7455d99a7a2648f77a531c9a71eb98.svg",
            "isPro": false,
            "fullname": "Xiaonan Nie",
            "user": "codecaution",
            "type": "user"
          },
          "name": "Xiaonan Nie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:06:14.057Z",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835f1",
          "user": {
            "_id": "617fe76105423df678cef199",
            "avatarUrl": "/avatars/64c94a4d743edab18ecb4bb7c550f049.svg",
            "isPro": false,
            "fullname": "Song",
            "user": "Ziang",
            "type": "user"
          },
          "name": "Ziang Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:06:07.780Z",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835f2",
          "name": "Guang Shi",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835f3",
          "name": "Haoqi Fan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/61fb81006374891646732f37/HQOfWqrOf9B97hWczL489.png"
      ],
      "publishedAt": "2025-05-20T17:59:30.000Z",
      "submittedOnDailyAt": "2025-05-21T00:38:53.960Z",
      "title": "Emerging Properties in Unified Multimodal Pretraining\n\n1. 단일화 멀티모달 사전학습의 발전적 특성\n2. 단일화 멀티모달 사전학습의 주요 특성\n3. 단일화 멀티모달 사전학습의 새로운 특성\n4. 단일화 멀티모달 사전학습의 발전적 특성\n5. 단일화 멀티모달 사전학습의 주요 특성\n6. 단일화 멀티모달 사전학습의 새로운 특성\n7. 단일화 멀티모달 사전학습의 발전적 특성\n8. 단일화 멀티모달 사전학습의 주요 특성\n9. 단일화 멀티모달 사전학습의 새로운 특성\n10. 단일화 멀티모달 사전학습의 발전적 특성\n11. 단일화 멀티모달 사전학습의 주요 특성\n12. 단일화 멀티모달 사전학습의 새로운 특성\n13. 단일화 멀티모달 사전학습의 발전적 특성\n14. 단일화 멀티모달 사전학습의 주요 특성\n15. 단일화 멀티모달 사전학습의 새로운 특성\n16. 단일화 멀티모달 사전학습의 발전적 특성\n17. 단일화 멀티모달 사전학습의 주요 특성\n18. 단일화 멀티모달 사전학습의 새로운 특성\n19. 단일화 멀티모달 사전학습의 발전적 특성\n20. 단일화 멀티모달 사전학습의 주요 특성\n21. 단일화 멀티모달 사전학습의 새로운 특성\n22. 단일화 멀티모달 사전학습의 발전적 특성\n23. 단일화 멀티모달 사전학습의 주요 특성\n24. 단일화 멀티모달 사전학습의 새로운 특성\n25. 단일화 멀티모달 사전학습의 발전적 특성\n26. 단일화 멀티모달 사전학습의 주요 특성\n27. 단일화 멀티모달 사전학습의 새로운 특성\n28. 단일화 멀티모달 사전학습의 발전적 특성\n29. 단일화 멀티모달 사전학습의 주요 특성\n30. 단일화 멀티모달 사전학습의 새로운 특성\n31. 단일화 멀티모달 사전학습의 발전적 특성\n32. 단일화 멀티모달 사전학습의 주요 특성\n33. 단일화 멀티모달 사전학습의 새로운 특성\n34. 단일화 멀티모달 사전학습의 발전적 특성\n35. 단일화 멀티모달 사전학습의 주요 특성\n36. 단일화 멀티모달 사전학습의 새로운 특성\n37. 단일화 멀티모달 사전학습의 발전적 특성\n38. 단일화 멀티모달 사전학습의 주요 특성\n39. 단일화 멀티모달 사전학습의 새로운 특성\n40. 단일화 멀티모달 사전학습의 발전적 특성\n41. 단일화 멀티모달 사전학습의 주요 특성\n42. 단일화 멀티모달 사전학습의 새로운 특성\n43. 단일화 멀티모달 사전학습의 발전적 특성\n44. 단일화 멀티모달 사전학습의 주요 특성\n45. 단일화 멀티모달 사전학습의 새로운 특성\n46. 단일화 멀티모달 사전학습의 발전적 특성\n47. 단일화 멀티모달 사전학습의 주요 특성\n48. 단일화 멀티모달 사전학습의 새로운 특성\n49. 단일화 멀티모달 사전학습의 발전적 특성\n50. 단일화 멀티모달 사전학습의 주요 특성\n51. 단일화 멀티모달 사전학습의 새로운 특성\n52. 단일화 멀티모달 사전학습의 발전적 특성\n53. 단일화 멀티모달 사전학습의 주요 특성\n54. 단일화 멀티모달 사전학습의 새로운 특성\n55. 단일화 멀티모달 사전학습의 발전적 특성\n56. 단일화 멀티모달 사전학습의 주요 특성\n57. 단일화 멀티모달 사전학습의 새로운 특성\n58. 단일화 멀티모달 사전학습의 발전적 특성\n59. 단일화 멀티모달 사전학습의 주요 특성\n60. 단일화 멀티모달 사전학습의 새로운 특성\n61. 단일화 멀티모달 사전학습의 발전적 특성\n62. 단일화 멀티모달 사전학습의 주요 특성\n63. 단일화 멀티모달 사전학습의 새로운 특성\n64. 단일화 멀티모달 사전학습의 발전적 특성\n65. 단일화 멀티모달 사전학습의 주요 특성\n66. 단일화 멀티모달 사전학습의 새로운 특성\n67. 단일화 멀티모달 사전학습의 발전적 특성\n68. 단일화 멀티모달 사전학습의 주요 특성\n69. 단일화 멀티모달 사전학습의 새로운 특성\n70. 단일화 멀티모달 사전학습의 발전적 특성\n71. 단일화 멀티모달 사전학습의 주요 특성\n72. 단일화 멀티모달 사전학습의 새로운 특성\n73. 단일화 멀티모달 사전학습의 발전적 특성\n74. 단일화 멀티모달 사전학습의 주요 특성\n75. 단일화 멀티모달 사전학습의 새로운 특성\n76. 단일화 멀티모달 사전학습의 발전적 특성\n77. 단일화 멀티모달 사전학습의 주요 특성\n78. 단일화 멀티모달 사전학습의 새로운 특성\n79. 단일화 멀티모달 사전학습의 발전적 특성\n80. 단일화 멀티모달 사전학습의 주요 특성\n81. 단일화 멀티모달 사전학습의 새로운 특성\n82. 단일화 멀티모달 사전학습의 발전적 특성\n83. 단일화 멀티모달 사전학습의 주요 특성\n84. 단일화 멀티모달 사전학습의 새로운 특성\n85. 단일화 멀티모달 사전학습의 발전적 특성\n86. 단일화 멀티모달 사전학습의 주요 특성\n87. 단일화 멀티모달 사전학습의 새로운 특성\n88. 단일화 멀티모달 사전학습의 발전적 특성\n89. 단일화 멀티모달 사전학습의 주요 특성\n90. 단일화 멀티모달 사전학습의 새로운 특성\n91. 단일화 멀티모달 사전학습의 발전적 특성\n92. 단일화 멀티모달 사전학습의 주요 특성\n93. 단일화 멀티모달 사전학습의 새로운 특성\n94. 단일화 멀티모달 사전학습의 발전적 특성\n95. 단일화 멀티모달 사전학습의 주요 특성\n96. 단일화 멀티모달 사전학습의 새로운 특성\n97. 단일화 멀티모달 사전학습의 발전적 특성\n98. 단일화 멀티모달 사전학습의 주요 특성\n99. 단일화 멀티모달 사전학습의 새로운 특성\n100. 단일화 멀티모달 사전학습의 발전적 특성",
      "submittedOnDailyBy": {
        "_id": "61fb81006374891646732f37",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg",
        "isPro": false,
        "fullname": "Kunchang Li",
        "user": "Andy1621",
        "type": "user"
      },
      "summary": "다중모달 이해와 생성을 통합하는 기술은 첨단 프로프라이언스 시스템에서 놀라운 능력을 보여주고 있습니다. 본 연구에서는, 다중모달 이해와 생성을 본질적으로 지원하는 오픈소스 기반 모델인 BAGEL을 소개합니다. BAGEL은 대규모의 인터랙티브 텍스트, 이미지, 비디오, 웹 데이터에서 선택된 수백억개의 토큰을 프리트레이닝한, 통일된 디코더 모델로 구성되어 있습니다. 이러한 다양한 다중모달 인터랙티브 데이터의 규모화는 BAGEL이 복잡한 다중모달 추론의新兴能力을 발휘합니다. 결과적으로, 표준 벤치마크에서 오픈소스 통합 모델의 다중모달 생성과 이해 성능을 크게 초월하며, 자유형식의 이미지 조작, 미래 프레임 예측, 3D 조작, 세계 네비게이션 등 첨단 다중모달 추론 능력을 발휘하고 있습니다. 다중모달 연구의进一步的 기회를 촉진하기 위해, 본 연구에서는 주요한 발견, 프리트레이닝 세부 사항, 데이터 생성 프로토콜을 공유하고, 코드와 체크포인트를 커뮤니티에 릴리즈합니다. 프로젝트 페이지는 https://bagel-ai.org/에 있습니다.",
      "upvotes": 53,
      "discussionId": "682d2fdc4540abccd3b836ee",
      "ai_keywords": [
        "unified, decoder-only model",
        "pretrained",
        "trillions of tokens",
        "large-scale interleaved data",
        "complex multimodal reasoning",
        "multimodal generation",
        "multimodal understanding",
        "free-form image manipulation",
        "future frame prediction",
        "3D manipulation",
        "world navigation"
      ]
    },
    "publishedAt": "2025-05-20T13:59:30.000Z",
    "title": "Emerging Properties in Unified Multimodal Pretraining",
    "summary": "Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/61fb81006374891646732f37/HQOfWqrOf9B97hWczL489.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14683.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61fb81006374891646732f37",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg",
      "fullname": "Kunchang Li",
      "name": "Andy1621",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11594",
      "authors": [
        {
          "_id": "682d426251ce04237318cfe5",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:46.065Z",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfe6",
          "name": "Jia Wei",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfe7",
          "user": {
            "_id": "62cc11a4f1d37c16280a2923",
            "avatarUrl": "/avatars/265b3cfb80f0a7b11a2ef67c49e29cf7.svg",
            "isPro": false,
            "fullname": "Pengle Zhang",
            "user": "Guyan",
            "type": "user"
          },
          "name": "Pengle Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:07:09.573Z",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfe8",
          "name": "Xiaoming Xu",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfe9",
          "user": {
            "_id": "67ea1f6693f71dd8167a2d22",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/H_upra_XVG1AoBKUe9ArV.png",
            "isPro": false,
            "fullname": "haofeng huang",
            "user": "haofeng666",
            "type": "user"
          },
          "name": "Haofeng Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:06:48.450Z",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfea",
          "name": "Haoxu Wang",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfeb",
          "name": "Kai Jiang",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfec",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfed",
          "user": {
            "_id": "65fcad0ba0d7adc40b54fac2",
            "avatarUrl": "/avatars/7564b5642378fddb46ec3b5ae57c0402.svg",
            "isPro": false,
            "fullname": "Jianfei Chen",
            "user": "surfingtomchen",
            "type": "user"
          },
          "name": "Jianfei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:06:40.981Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/Tb20E3IJSV6PjcD9Nkvfg.png"
      ],
      "publishedAt": "2025-05-16T18:01:54.000Z",
      "submittedOnDailyAt": "2025-05-21T01:35:25.101Z",
      "title": "SageAttention3: 微型 사이클링 FP4 Attention의 추론과 8비트 학습의 검토",
      "submittedOnDailyBy": {
        "_id": "66c0a08bac74db25de8427ec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
        "isPro": false,
        "fullname": "Jintao Zhang",
        "user": "jt-zhang",
        "type": "user"
      },
      "summary": "注意力의 효율성은 그 2차원 시간 복잡성에 의해 중요합니다. 우리는 2가지 주요한 기여로, 注意력의 효율성을 향상시킵니다. 먼저, Blackwell GPU의 새로운 FP4 Tensor Core를 활용하여, 注意력 계산을 가속화합니다. 우리의 구현은 RTX5090에서 1038TOPS를 달성하며, 이는 RTX5090에서 가장 빠른 FlashAttention보다 5배 빠르며, 실험은 우리의 FP4 注意력이 다양한 모델의 추론을 가속화하는 데 사용할 수 있는 표준화된 방법이라는 것을 보여주고 있습니다. 두 번째로, 우리는 저 비트 注意력(예: FlashAttention3과 SageAttention)을 학습 태스크에 도입했습니다. 현재의 저 비트 注意력은 주로 추론에 집중하지만, 대규모 모델의 학습 효율성도 중요합니다. 저 비트 注意력이 학습 태스크에 효과적으로 적용할 수 있는지를 조사하기 위해, 우리는 전파와 역전파 모두에 대해 정확한, 효율적인 8 비트 注意력을 설계했습니다. 실험은 8 비트 注意력이 미세한 손실 없이 성능을 달성하며, 사전 학습 태스크에서는 수렴이 늦어질 것을 보여주고 있습니다. 코드는 https://github.com/thu-ml/SageAttention에 공개됩니다.",
      "upvotes": 26,
      "discussionId": "682d426551ce04237318d0b9",
      "projectPage": "https://github.com/thu-ml/SageAttention",
      "githubRepo": "https://github.com/thu-ml/SageAttention",
      "ai_keywords": [
        "attention",
        "FP4 Tensor Cores",
        "TOPS",
        "speedup",
        "inference",
        "plug-and-play",
        "low-bit attention",
        "8-bit attention",
        "forward propagation",
        "backward propagation",
        "fine-tuning",
        "pretraining",
        "convergence",
        "lossless performance"
      ]
    },
    "publishedAt": "2025-05-16T14:01:54.000Z",
    "title": "SageAttention3: Microscaling FP4 Attention for Inference and An\n  Exploration of 8-Bit Training",
    "summary": "The efficiency of attention is important due to its quadratic time\ncomplexity. We enhance the efficiency of attention through two key\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\naccelerate attention computation. Our implementation achieves 1038 TOPS on\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\nExperiments show that our FP4 attention can accelerate inference of various\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\nfocus only on inference. However, the efficiency of training large models is\nalso important. To explore whether low-bit attention can be effectively applied\nto training tasks, we design an accurate and efficient 8-bit attention for both\nforward and backward propagation. Experiments indicate that 8-bit attention\nachieves lossless performance in fine-tuning tasks but exhibits slower\nconvergence in pretraining tasks. The code will be available at\nhttps://github.com/thu-ml/SageAttention.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/Tb20E3IJSV6PjcD9Nkvfg.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11594.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14246",
      "authors": [
        {
          "_id": "682d7a2340a42d1538fada76",
          "user": {
            "_id": "66fe1334ff3ee1f7569fab6d",
            "avatarUrl": "/avatars/6868b1a545028a9b8bbded52490dc093.svg",
            "isPro": false,
            "fullname": "ziyuliu",
            "user": "ziyuliu",
            "type": "user"
          },
          "name": "Ziyu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:08:39.517Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada77",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:39:42.962Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada78",
          "user": {
            "_id": "6671341b4eee852a8b25888f",
            "avatarUrl": "/avatars/635d1821bbc960b4ea845e606883eb16.svg",
            "isPro": false,
            "fullname": "yushan zou",
            "user": "zyshan",
            "type": "user"
          },
          "name": "Yushan Zou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:08:45.972Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada79",
          "user": {
            "_id": "652768eeb723b49e8c8865da",
            "avatarUrl": "/avatars/491e02da9ec81e439ccda8a181634bca.svg",
            "isPro": false,
            "fullname": "Zijian Liang",
            "user": "steins1096",
            "type": "user"
          },
          "name": "Zijian Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:08:51.938Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7a",
          "user": {
            "_id": "67c0849ee08c178ef8d4e05c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mQ6VdnjZnRhb0H_waPclo.png",
            "isPro": false,
            "fullname": "Xiaoyi Dong",
            "user": "sweetFruit",
            "type": "user"
          },
          "name": "Xiaoyi Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:09:00.912Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7b",
          "user": {
            "_id": "65000bef18830fabea469fdd",
            "avatarUrl": "/avatars/b320c77dfad039d9f9c54127f610d44f.svg",
            "isPro": false,
            "fullname": "Cao Yuhang",
            "user": "yhcao",
            "type": "user"
          },
          "name": "Yuhang Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:09:22.645Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7c",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:09:31.442Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7d",
          "user": {
            "_id": "636317ed80c1a705a6eff396",
            "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
            "isPro": false,
            "fullname": "Dahua Lin",
            "user": "lindahua",
            "type": "user"
          },
          "name": "Dahua Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:09:37.103Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7e",
          "user": {
            "_id": "64b4eec4faa3181a5eab9c46",
            "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
            "isPro": true,
            "fullname": "Jiaqi Wang",
            "user": "myownskyW7",
            "type": "user"
          },
          "name": "Jiaqi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:39:45.391Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T11:59:25.000Z",
      "submittedOnDailyAt": "2025-05-21T05:32:01.442Z",
      "title": "Visual Agentic Reinforcement Fine-Tuning",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "대논리 모형（예：OpenAI의 o3）의 핵심 트렌드에서 외부 도구의 본래 에이전트 능력이 중요합니다. 이는 웹 브라우저를 사용하여 검색하거나 이미지 처리를 위해 코드를 작성하고 실행하여 이미지에 대해 접근하는 것을 의미합니다. 오픈 소스 연구 커뮤니티에서는 언어만 기반의 에이전트 능력（함수 호출 및 도구 통합）에 대한 발전이 이루어졌지만, 이미지에 대해 진정으로 접근하는 다형형 에이전트 능력과 그 상대적인 벤치마크는 아직 적어도 탐색되지 않았습니다. 본 논문에서는 시각적 에이전트 강화 훈련 (Visual-ARFT)을 통해 시각적 논리 능력이 부여된 대시각 언어 모델 (LVLMs)의 효과를 주장합니다. Visual-ARFT에 의해 오픈 소스 LVLMs는 웹 사이트 검색하여 정보 업데이트를 얻거나, 이미지의 절삭, 회전 등 이미지 처리手法를 사용하여 입력 이미지를 조작하고 분석할 수 있는 코드를 작성할 수 있습니다. 또한, 다형형 에이전트 벤치마크 (MAT)의 두 가지 설정 (MAT-Search와 MAT-Coding)을 제시하여 LVLM의 에이전트 검색 및 코딩 능력을 평가할 수 있습니다. 실험 결과를 통해 Visual-ARFT는 MAT-Coding에서 기준 대비 F1 스코어가 +18.6%, EM 스코어가 +13.0%를 초과하고, MAT-Search에서 F1 스코어가 +10.3%, EM 스코어가 +8.7%를 초과하며, 최종적으로 GPT-4o를 초월합니다. Visual-ARFT는 2Wiki, HotpotQA 등 기존의 다형형 폼 QA 벤치마크에서도 F1 스코어가 +29.3%, EM 스코어가 +25.9%의 효과를 보였으며, 강한 일반화 능력을 나타냅니다. 우리의 발견은 Visual-ARFT가 강건하고 일반화 가능한 다형형 에이전트의 구축에 적합한 길을 제시하는 것을 보여줍니다.",
      "upvotes": 15,
      "discussionId": "682d7a2440a42d1538fadac0",
      "githubRepo": "https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT",
      "ai_keywords": [
        "Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT)",
        "Large Vision-Language Models (LVLMs)",
        "Multi-modal Agentic Tool Bench (MAT)",
        "MAT-Search",
        "MAT-Coding",
        "F1",
        "EM",
        "GPT-4o",
        "multi-hop QA benchmarks",
        "2Wiki",
        "HotpotQA"
      ]
    },
    "publishedAt": "2025-05-20T07:59:25.000Z",
    "title": "Visual Agentic Reinforcement Fine-Tuning",
    "summary": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14246.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14460",
      "authors": [
        {
          "_id": "682d54b0396c1e613eaac5ef",
          "user": {
            "_id": "655de51982afda0fc479fb91",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/99oxuFD3OjlReuGfDZluh.png",
            "isPro": false,
            "fullname": "Tianhe Wu",
            "user": "TianheWu",
            "type": "user"
          },
          "name": "Tianhe Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:26.204Z",
          "hidden": false
        },
        {
          "_id": "682d54b0396c1e613eaac5f0",
          "name": "Jian Zou",
          "hidden": false
        },
        {
          "_id": "682d54b0396c1e613eaac5f1",
          "name": "Jie Liang",
          "hidden": false
        },
        {
          "_id": "682d54b0396c1e613eaac5f2",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "682d54b0396c1e613eaac5f3",
          "name": "Kede Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T14:56:50.000Z",
      "submittedOnDailyAt": "2025-05-21T07:48:51.670Z",
      "title": "VisualQuality-R1: 순차적 순위를 부여하는 리벤지먼트 학습을 이용한 이미지 품질 평가\n\n(Note: The translation provided is a direct translation of the given text. If \"VisualQuality-R1\" is a specific term or identifier, it is kept as is. The translation aims to maintain the original meaning and context while ensuring accuracy and professionalism in Korean.)",
      "submittedOnDailyBy": {
        "_id": "655de51982afda0fc479fb91",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/99oxuFD3OjlReuGfDZluh.png",
        "isPro": false,
        "fullname": "Tianhe Wu",
        "user": "TianheWu",
        "type": "user"
      },
      "summary": "DeepSeek-R1은 대규모 언어 모델(LLMs)의 논리론과 일반화 능력을 강화 학습을 통해 효과적으로 증진시키는 데 효과적이다. 그러나 논리론에 의한 계산적 모델링의 가능성은 이미지 품질 평가(IQA)와 같은 중요한 시각적인 논리론에 의존하는 작업에 대해 충분히 조사되어 있지 않다. 본 논문에서는 VisualQuality-R1이라는 논리론에 의한 비기반 IQA(NR-IQA) 모델을 소개하고, 강화 학습을 사용하여 이를 훈련시키고, 이미지의 질의 고유의 상대적인 특성에 맞는 학습 알고리즘인 랭킹을 수행한다. 특히, 이미지의 조합으로 그룹 상대적인 정책 최적화를 사용하여 각 이미지에 대해 여러 품질 스코어를 생성한다. 이 추정값은 Thurstone 모델에 의한 한 이미지가 다른 이미지보다 높은 품질을 가진 확률을 계산한다. 각 품질 추정에 대한 보상은 연속적인 피드백 아지저가 아니라, 이진화된 라벨로 정의된다. 확장된 실험은 제안된 VisualQuality-R1이, 판별적 딥러닝에 기반한 NR-IQA 모델과 최근의 논리론에 의한 품질 회귀 방법보다 뛰어난 것을 보여준다. 또한, VisualQuality-R1은 시각적인 스케일의 재조정이 필요하지 않도록, 다양한 이미지 처리 작업의 발전을 신뢰적으로 측정할 수 있다는 점을 특별히 적합한다. 이러한 특성은 이미지 처리의 다양한 작업에서 발전의 신뢰적인 측정에 특히 적합하다.",
      "upvotes": 14,
      "discussionId": "682d54b0396c1e613eaac62a",
      "githubRepo": "https://github.com/TianheWu/VisualQuality-R1",
      "ai_keywords": [
        "reinforcement learning",
        "VisualQuality-R1",
        "reasoning-induced no-reference IQA (NR-IQA)",
        "group relative policy optimization",
        "Thurstone model",
        "continuous fidelity measures",
        "discriminative deep learning",
        "reasoning-induced quality regression",
        "super-resolution",
        "image generation"
      ]
    },
    "publishedAt": "2025-05-20T10:56:50.000Z",
    "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\n  Reinforcement Learning to Rank",
    "summary": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\nreasoning and generalization capabilities of large language models (LLMs)\nthrough reinforcement learning. Nevertheless, the potential of\nreasoning-induced computational modeling has not been thoroughly explored in\nthe context of image quality assessment (IQA), a task critically dependent on\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\nreinforcement learning to rank, a learning algorithm tailored to the\nintrinsically relative nature of visual quality. Specifically, for a pair of\nimages, we employ group relative policy optimization to generate multiple\nquality scores for each image. These estimates are then used to compute\ncomparative probabilities of one image having higher quality than the other\nunder the Thurstone model. Rewards for each quality estimate are defined using\ncontinuous fidelity measures rather than discretized binary labels. Extensive\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\ndiscriminative deep learning-based NR-IQA models as well as a recent\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\ncapable of generating contextually rich, human-aligned quality descriptions,\nand supports multi-dataset training without requiring perceptual scale\nrealignment. These features make VisualQuality-R1 especially well-suited for\nreliably measuring progress in a wide range of image processing tasks like\nsuper-resolution and image generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14460.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "655de51982afda0fc479fb91",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/99oxuFD3OjlReuGfDZluh.png",
      "fullname": "Tianhe Wu",
      "name": "TianheWu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.04388",
      "authors": [
        {
          "_id": "682d83494c4685831f85ec92",
          "name": "Dario Garcia-Gasulla",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec93",
          "user": {
            "_id": "661e8559b0338c03bd6e5054",
            "avatarUrl": "/avatars/044d69afa40ddef4485aebfe984da96b.svg",
            "isPro": false,
            "fullname": "Bayarri",
            "user": "JordiBayarri-bsc",
            "type": "user"
          },
          "name": "Jordi Bayarri-Planas",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec94",
          "name": "Ashwin Kumar Gururajan",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec95",
          "name": "Enrique Lopez-Cuena",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec96",
          "user": {
            "_id": "6622352d9fcf61dee8a1f24d",
            "avatarUrl": "/avatars/dd164c50b3ecb8861e2294337b942e6f.svg",
            "isPro": false,
            "fullname": "Adrian Tormos",
            "user": "adriantormos",
            "type": "user"
          },
          "name": "Adrian Tormos",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec97",
          "user": {
            "_id": "65b8e66f078c543033e49672",
            "avatarUrl": "/avatars/aa81fd59b4ec624245a4a84d2708962d.svg",
            "isPro": false,
            "fullname": "Daniel Hinjos García",
            "user": "danihinjos",
            "type": "user"
          },
          "name": "Daniel Hinjos",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:39:40.380Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec98",
          "user": {
            "_id": "620683e7eeb1b73d904c96e5",
            "avatarUrl": "/avatars/d0309ac9408530a74f1799e175cc5fad.svg",
            "isPro": false,
            "fullname": "Pablo Bernabeu",
            "user": "pabberpe",
            "type": "user"
          },
          "name": "Pablo Bernabeu-Perez",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec99",
          "user": {
            "_id": "65d71603ca16ef9ba7fb2efb",
            "avatarUrl": "/avatars/7d3e1436427f7f58c86fb1f8724c4244.svg",
            "isPro": false,
            "fullname": "Anna",
            "user": "annariasdu",
            "type": "user"
          },
          "name": "Anna Arias-Duart",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T07:59:53.756Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9a",
          "user": {
            "_id": "6565c2a4131d13ccc5df1435",
            "avatarUrl": "/avatars/218ceac504772e7f0fb3ee4d46d4fab7.svg",
            "isPro": false,
            "fullname": "Pablo Agustin Martin Torres",
            "user": "PabloMartinTorres",
            "type": "user"
          },
          "name": "Pablo Agustin Martin-Torres",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T09:03:22.597Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9b",
          "name": "Marta Gonzalez-Mallo",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9c",
          "user": {
            "_id": "62d16c742ad06bbc89217797",
            "avatarUrl": "/avatars/11ce629fbcb33f2431164d8a3e54c876.svg",
            "isPro": false,
            "fullname": "Sergio Alvarez-Napagao",
            "user": "tranchis",
            "type": "user"
          },
          "name": "Sergio Alvarez-Napagao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9d",
          "name": "Eduard Ayguadé-Parra",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9e",
          "name": "Ulises Cortés",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/SdO6Jth8V1sz0wvfL9Nxg.png",
        "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/Efb0dFT2ULJC-TvdQ1ERR.png"
      ],
      "publishedAt": "2025-05-07T13:13:14.000Z",
      "submittedOnDailyAt": "2025-05-21T06:12:19.909Z",
      "title": "알로라 가족의 레시피： 오픈 및 전문적인 건강 케어 LLM",
      "submittedOnDailyBy": {
        "_id": "62f7a16192950415b637e201",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f7a16192950415b637e201/4IIqYap43vujvSuql68Vj.jpeg",
        "isPro": false,
        "fullname": "Dario",
        "user": "dariog",
        "type": "user"
      },
      "summary": "목적: 하이브리드 언어 모델(LLMs)의 발전에 따라, 건강 케어 분야에서 대중의 이익을 보호하기 위해 경쟁적인 오픈 소스 모델의 필요성이 있습니다. 본 연구는 데이터의 전처리 및 학습의 중요한 단계를 최적화하고, 모델의 안전성(DPO를 통해)과 효율성(RAG를 통해)를 향상시키는 방법을 제시하고, 이 분야에 새로운 기준을 정합니다. 결과적으로, 가장 좋은 비공개 모델과 경쟁적인 모델을 제공하며, 자유의 허가를 받아 릴리스 됩니다.\n\n방법: Llama 3.1와 Qwen 2.5의 강력한 베이스 모델을 기반으로, Aloe Beta는 문법적 Chain of Thought 예제를 추가하여 공개 데이터에 강화합니다. 모델은 직접적인 선호도 최적화(DPO)에 의해, 젓가락 공격의 존재하에서 윤리적 및 정책 일관성 성능을 강조합니다. 평가는 종결적, 개발적, 안전성 및 인간 평가를 포함하여, 결과의 신뢰성을 최대화합니다.\n\n결과: Aloe Family의 강력한 성능에 기반하여, 비공개 모델과 같은 성능을 보여주며, 건강 케어 벤치마크 및 의료 분야에서 경쟁적인 성능을 제공하며, 의료 전문가에게도 좋습니다. 편향성과 독성성에 대해, Aloe Beta 모델은 안전성을 크게 향상시키고, 모르는 젓가락 공격에 대한 강도를 보여줍니다. 책임 있는 릴리스를 위해, Aloe Family 모델에 대한 구체적인 리스크 평가가 제공됩니다.\n\n결론: Aloe Beta 모델과 그 모델을 생성하는 레시피는, 높은 수준의 성능을 제공하면서 높은 윤리적 요구 사항을 유지하는 오픈 소스 의료용 LLM 분야에 큰 기여입니다. 이 연구는 건강 케어 분야의 LLM 개발 및 보고에 새로운 기준을 세팅합니다.",
      "upvotes": 14,
      "discussionId": "682d834a4c4685831f85ed09",
      "ai_keywords": [
        "Direct Preference Optimization (DPO)",
        "Retrieval-Augmented Generation (RAG)",
        "Chain of Thought",
        "Llama 3.1",
        "Qwen 2.5",
        "Close-ended tests",
        "Open-ended tests",
        "Safety assessments",
        "Human assessments",
        "Jailbreaking attacks",
        "Bias",
        "Toxicity",
        "Risk assessment"
      ]
    },
    "publishedAt": "2025-05-07T09:13:14.000Z",
    "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
    "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/SdO6Jth8V1sz0wvfL9Nxg.png",
      "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/Efb0dFT2ULJC-TvdQ1ERR.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04388.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f7a16192950415b637e201",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f7a16192950415b637e201/4IIqYap43vujvSuql68Vj.jpeg",
      "fullname": "Dario",
      "name": "dariog",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14513",
      "authors": [
        {
          "_id": "682d334862cadf615f5f73e6",
          "user": {
            "_id": "63ad217b9fc40b14560e9e06",
            "avatarUrl": "/avatars/c2d33830b141fc9c73ad8302ff35ed9d.svg",
            "isPro": false,
            "fullname": "Yen-Chen Wu",
            "user": "yenchen",
            "type": "user"
          },
          "name": "Yen-Chen Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:11:09.090Z",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73e7",
          "user": {
            "_id": "643fb7332397d8eef5b844cd",
            "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
            "isPro": false,
            "fullname": "Feng-Ting Liao",
            "user": "FengTing",
            "type": "user"
          },
          "name": "Feng-Ting Liao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:41:00.821Z",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73e8",
          "user": {
            "_id": "66018c8eb1e509e1e4d9196f",
            "avatarUrl": "/avatars/5d6cbfc7b6c435264d271c958607630f.svg",
            "isPro": false,
            "fullname": "Meng-Hsi Chen",
            "user": "menghsichen",
            "type": "user"
          },
          "name": "Meng-Hsi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:11:15.604Z",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73e9",
          "name": "Pei-Chen Ho",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73ea",
          "name": "Farhang Nabiei",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73eb",
          "user": {
            "_id": "6811b1294119e4ecc92fc93b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/srmY2yyzOg9KRDSLYXJKf.png",
            "isPro": false,
            "fullname": "Dashan Shiu",
            "user": "dsshiu",
            "type": "user"
          },
          "name": "Da-shan Shiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:11:37.639Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T15:41:05.000Z",
      "submittedOnDailyAt": "2025-05-21T00:30:57.355Z",
      "title": "Latent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer",
      "submittedOnDailyBy": {
        "_id": "643fb7332397d8eef5b844cd",
        "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
        "isPro": false,
        "fullname": "Feng-Ting Liao",
        "user": "FengTing",
        "type": "user"
      },
      "summary": "Transformers는 대규모 언어 모델(LLMs)의 표준 구현으로, 일반적으로 10개에서 数百개의 분리된 층으로 구성됩니다. 더 많은 층이 성능 향상을 달성할 수 있다는 것은 사실이지만, 이 접근 방식은 효율적이지 않습니다. 특히, 확률적 흐름 모델과 흐름 기반 모델이 연속적인 층의 우수한 성능을 보여주는 것을 고려하여 논의되고 있습니다. 우리는 Latent Flow Transformer(LFT)를 제안하고 있습니다. LFT는 흐름 매칭으로 학습된 하나의 학습된 트랜스포머로 층의 블록을 대체하고, 원의 아키텍처와 호환성을 유지하는 동시에 큰 압축을 실현합니다. 또한, 흐름 기반 방법의 결합 보존의 제한을 해결하기 위해, Flow Walking(FW) 알고리즘을 도입하고 있습니다. Pythia-410M 모델에서, 흐름 매칭으로 학습된 LFT는 24층 중 6층을 압축하고, 직접 2층을 스킵하는 것보다 KL Divergence가 0.407이며, 성능이 향상됩니다(0.529). 이는 이 설계의 가능성에 대한 증거를 보여줍니다. FW로 학습된 LFT는 12층을 1층으로 합칩니다, KL Divergence를 0.736으로 억제하고, 스킵하는 3층(0.932)을 초과하여, 자동 회귀적인 생성 패러다임과 흐름 기반의 생성 패러다임 사이의 차이를 크게 좁힙니다.",
      "upvotes": 11,
      "discussionId": "682d334962cadf615f5f743f",
      "githubRepo": "https://github.com/mtkresearch/latent-flow-transformer",
      "ai_keywords": [
        "Transformers",
        "large language models (LLMs)",
        "discrete layers",
        "continuous layers",
        "diffusion models",
        "flow-based models",
        "latent Flow Transformer (LFT)",
        "learned transport operator",
        "flow matching",
        "Flow Walking (FW) algorithm",
        "Pythia-410M",
        "KL Divergence",
        "autoregressive",
        "flow-based generation paradigms"
      ]
    },
    "publishedAt": "2025-05-20T11:41:05.000Z",
    "title": "Latent Flow Transformer",
    "summary": "Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in preserving\ncoupling by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14513.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643fb7332397d8eef5b844cd",
      "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
      "fullname": "Feng-Ting Liao",
      "name": "FengTing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13438",
      "authors": [
        {
          "_id": "682d84d6aa4903837eeac1dc",
          "user": {
            "_id": "63885f1d0bebb233d8ad6e5b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
            "isPro": false,
            "fullname": "Penghui Qi",
            "user": "QPHutu",
            "type": "user"
          },
          "name": "Penghui Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:39:35.138Z",
          "hidden": false
        },
        {
          "_id": "682d84d6aa4903837eeac1dd",
          "user": {
            "_id": "65f5392c68b8e0cb3c9977a2",
            "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
            "isPro": false,
            "fullname": "Zichen",
            "user": "lkevinzc",
            "type": "user"
          },
          "name": "Zichen Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T09:31:12.541Z",
          "hidden": false
        },
        {
          "_id": "682d84d6aa4903837eeac1de",
          "user": {
            "_id": "63d91b6d255ef6add20e1b38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
            "isPro": false,
            "fullname": "Tianyu Pang",
            "user": "P2333",
            "type": "user"
          },
          "name": "Tianyu Pang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:12:08.028Z",
          "hidden": false
        },
        {
          "_id": "682d84d6aa4903837eeac1df",
          "user": {
            "_id": "632407c892e07e3ca20aca28",
            "avatarUrl": "/avatars/23b51b37b12b51a0947f687d1de4d3b5.svg",
            "isPro": false,
            "fullname": "Chao Du",
            "user": "duchao",
            "type": "user"
          },
          "name": "Chao Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:12:18.649Z",
          "hidden": false
        },
        {
          "_id": "682d84d6aa4903837eeac1e0",
          "name": "Wee Sun Lee",
          "hidden": false
        },
        {
          "_id": "682d84d6aa4903837eeac1e1",
          "name": "Min Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/UeoRcEi-bKgq6eecwvo8o.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/JLzRbfpomaF02gHLNkON6.png"
      ],
      "publishedAt": "2025-05-19T17:58:44.000Z",
      "submittedOnDailyAt": "2025-05-21T06:36:34.577Z",
      "title": "시간 분배에 대한 이유론을 최적화하기 위한 예산에 대한 상대적 정책 최적화법",
      "submittedOnDailyBy": {
        "_id": "63885f1d0bebb233d8ad6e5b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
        "isPro": false,
        "fullname": "Penghui Qi",
        "user": "QPHutu",
        "type": "user"
      },
      "summary": "スケーリングテストタイム의 계산량의 확장은 대규모 언어 모델(LLMs)의 추론 능력을 향상시키기 위해 중요합니다. 현재의 접근 방식은 일반적으로 최종적인 증명 가능한 보상을 최대화하기 위해 강화학습(RL)을 사용합니다. 그러나 이러한 방법들은 고정된 큰 토큰 버퍼에서 최종적인 성능만 최적화하고 학습 및 배포의 효율을 저해하고 있습니다. 본 연구에서는 AnytimeReasoner라는 새로운 프레임워크를 제안하고, 변하는 토큰 버퍼 제약 아래의 추론의 시간 효율성과 유연성을 향상시키기 위해 임의 시간 추론 성능을 최적화하는 것을 목표로 합니다. 이를 달성하기 위해, 먼저 분포에서 샘플링된 토큰 버퍼에서 완전한 사고 과정까지 끝까지 끊고, 각각의 끊어진 생각에 대해 가장 적절한 대답을 요약하도록 모델을 강제합니다. 이로써 추론 프로세스에 증명 가능한 밀집 보상을 도입하고, RL 최적화에 효과적인 신뢰 분배를 촉진합니다. 다음으로, 생각과 요약의 정책을 분리하여 누적 보상을 최대화하여 최적화합니다. 또한, 생각 정책의 학습 프로세스의 강건성과 효율성을 향상시키기 위해 새로운 분산 감소 방법, 버킷 상대적 정책 최적화(BRPO)를 도입합니다. 수학적 추론 태스크의 실험 결과에 따르면, 우리의 방법은 모든 다른 사전 분포의 모든 생각 버퍼에서 GRPO를 초과하고 학습 및 토큰의 효율을 향상시킵니다.",
      "upvotes": 11,
      "discussionId": "682d84d7aa4903837eeac215",
      "githubRepo": "https://github.com/sail-sg/AnytimeReasoner",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "verifiable reward",
        "reasoning traces",
        "token budget",
        "AnytimeReasoner",
        "truncation",
        "verifiable dense rewards",
        "credit assignment",
        "thinking and summary policies",
        "decoupled manner",
        "cumulative reward",
        "Budget Relative Policy Optimization (BRPO)",
        "variance reduction"
      ]
    },
    "publishedAt": "2025-05-19T13:58:44.000Z",
    "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
    "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/UeoRcEi-bKgq6eecwvo8o.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/JLzRbfpomaF02gHLNkON6.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13438.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63885f1d0bebb233d8ad6e5b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
      "fullname": "Penghui Qi",
      "name": "QPHutu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13547",
      "authors": [
        {
          "_id": "682d6b06ec3b65b35772c0af",
          "user": {
            "_id": "668f440894dfc0ed1a7006ed",
            "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
            "isPro": false,
            "fullname": "Pengxin Guo",
            "user": "gpx333",
            "type": "user"
          },
          "name": "Pengxin Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:11.271Z",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b0",
          "user": {
            "_id": "641b065a1911d3be6742ef04",
            "avatarUrl": "/avatars/bceb4ad2a278b6e2a40af0b89c4fb48e.svg",
            "isPro": false,
            "fullname": "Yinong Wang",
            "user": "jcccy",
            "type": "user"
          },
          "name": "Yinong Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:12:52.463Z",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b1",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b2",
          "user": {
            "_id": "678f8d4b1a5e393c1c285165",
            "avatarUrl": "/avatars/533eb74376915eb63c03dc9b4df421f6.svg",
            "isPro": false,
            "fullname": "MENGTINGLIU",
            "user": "MENGTINGLIU",
            "type": "user"
          },
          "name": "Mengting Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:13:13.691Z",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b3",
          "user": {
            "_id": "637f0eb22438d7485b8ef5d7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
            "isPro": false,
            "fullname": "Ming Li",
            "user": "limingcv",
            "type": "user"
          },
          "name": "Ming Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:13:40.959Z",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b4",
          "name": "Jinkai Zheng",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b5",
          "user": {
            "_id": "663058bc2653ec94f4a6235f",
            "avatarUrl": "/avatars/f55b8c3c8100d6b6d65ba61abc4fb014.svg",
            "isPro": false,
            "fullname": "Liangqiong Qu",
            "user": "Liangqiong-QU",
            "type": "user"
          },
          "name": "Liangqiong Qu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:12:59.427Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T03:41:54.000Z",
      "submittedOnDailyAt": "2025-05-21T04:27:08.535Z",
      "title": "탐색 파데레티드 파운딩의 대형 언어 모델",
      "submittedOnDailyBy": {
        "_id": "668f440894dfc0ed1a7006ed",
        "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
        "isPro": false,
        "fullname": "Pengxin Guo",
        "user": "gpx333",
        "type": "user"
      },
      "summary": "LLM의 프로덕션은 LLM의 압축을 가능하게 하며, 자원 제한된 기기에서 도입을 촉진하기 위해 기대되는 기술로 등장했습니다. 그러나 현재의 방법론은 일반적으로 공개된 카라바레션 샘플에 액세스가 필요하며, 프라이버시 관심 있는 분야에서는 획득이 어려운 경우가 있습니다. 이러한 문제를 해결하기 위해, FedPrLLM, LLM의 프라이버시 보호적인 압축에 대한 단일의 federated 프로덕션 프레임워크를 설계하여 제안합니다. FedPrLLM에서 각 클라이언트는 로컬의 카라바레션 데이터에 기반하여 프로덕션 마스크 행렬을 계산하고, 이를 서버에 공유하여 글로벌 모델을 프로덕션하는 것이 필요합니다. 이 접근 방식은 각 클라이언트의 지식에 기반한 글로벌 모델의 협력적인 프로덕션을 가능하게 하며, 로컬 데이터의 프라이버시를 유지할 수 있습니다. 또한 FedPrLLM 프레임워크 내에서 다양한 가능성을 평가하기 위해 다양한 비교 그룹, 프로덕션 전략, 가중치의 스케일링 결정을 수행했습니다. 엄격한 평가에 따라 플레이어 비교와 가중치의 스케일링이 없는 일시적인 프로덕션이 FedPrLLM 프레임워크 내에서 가장 최적의 선택입니다. 우리 작업은 프라이버시 관심 있는 분야에서 LLM의 프로덕션에 대한 미래의 노력을 가이드하는 것입니다. 코드는 https://github.com/Pengxin-Guo/FedPrLLM에 공개되어 있습니다.",
      "upvotes": 10,
      "discussionId": "682d6b07ec3b65b35772c0f3",
      "githubRepo": "https://github.com/Pengxin-Guo/FedPrLLM",
      "ai_keywords": [
        "FedPrLLM",
        "federated pruning",
        "pruning mask matrix",
        "local calibration data",
        "global model",
        "collaborative pruning",
        "layer comparison",
        "weight scaling",
        "one-shot pruning"
      ]
    },
    "publishedAt": "2025-05-18T23:41:54.000Z",
    "title": "Exploring Federated Pruning for Large Language Models",
    "summary": "LLM pruning has emerged as a promising technology for compressing LLMs,\nenabling their deployment on resource-limited devices. However, current\nmethodologies typically require access to public calibration samples, which can\nbe challenging to obtain in privacy-sensitive domains. To address this issue,\nwe introduce FedPrLLM, a comprehensive federated pruning framework designed for\nthe privacy-preserving compression of LLMs. In FedPrLLM, each client only needs\nto calculate a pruning mask matrix based on its local calibration data and\nshare it with the server to prune the global model. This approach allows for\ncollaborative pruning of the global model with the knowledge of each client\nwhile maintaining local data privacy. Additionally, we conduct extensive\nexperiments to explore various possibilities within the FedPrLLM framework,\nincluding different comparison groups, pruning strategies, and the decision to\nscale weights. Our extensive evaluation reveals that one-shot pruning with\nlayer comparison and no weight scaling is the optimal choice within the\nFedPrLLM framework. We hope our work will help guide future efforts in pruning\nLLMs in privacy-sensitive fields. Our code is available at\nhttps://github.com/Pengxin-Guo/FedPrLLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13547.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668f440894dfc0ed1a7006ed",
      "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
      "fullname": "Pengxin Guo",
      "name": "gpx333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13866",
      "authors": [
        {
          "_id": "682d2dee396c1e613e9fcbe5",
          "user": {
            "_id": "662672eaebdfec5cfdf1d034",
            "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
            "isPro": false,
            "fullname": "Jiwon Song",
            "user": "jiwonsong",
            "type": "user"
          },
          "name": "Jiwon Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:41:15.525Z",
          "hidden": false
        },
        {
          "_id": "682d2dee396c1e613e9fcbe6",
          "user": {
            "_id": "639ffbc6beb95d698de9640d",
            "avatarUrl": "/avatars/7ef1aaadd5b378d00e17dc548e42cb7e.svg",
            "isPro": false,
            "fullname": "Dongwon Jo",
            "user": "dongwonjo",
            "type": "user"
          },
          "name": "Dongwon Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:41:12.000Z",
          "hidden": false
        },
        {
          "_id": "682d2dee396c1e613e9fcbe7",
          "user": {
            "_id": "6566ddb96af53c602f80b1e2",
            "avatarUrl": "/avatars/403c8e486115920e50867b6462ddfd99.svg",
            "isPro": false,
            "fullname": "Yulhwa Kim",
            "user": "YulhwaKim",
            "type": "user"
          },
          "name": "Yulhwa Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:14:05.272Z",
          "hidden": false
        },
        {
          "_id": "682d2dee396c1e613e9fcbe8",
          "name": "Jae-Joon Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T03:21:52.000Z",
      "submittedOnDailyAt": "2025-05-21T00:06:25.696Z",
      "title": "Reasoning Path Compression: 효율적인 LLM Reasoning 생성 프로세스의 압축\n\n(Note: The original text \"Reasoning Path Compression: 効率的なLLM Reasoningの生成プロセスの圧縮\" contains a mix of English and Japanese characters. The translation provided above is based on the assumption that the intended meaning is to describe the compression of the reasoning process for efficient LLM reasoning generation. If the original text was meant to be purely in Korean, the translation would be \"Reasoning Path Compression: 효율적인 LLM Reasoning 생성 프로세스의 압축\".)",
      "submittedOnDailyBy": {
        "_id": "662672eaebdfec5cfdf1d034",
        "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
        "isPro": false,
        "fullname": "Jiwon Song",
        "user": "jiwonsong",
        "type": "user"
      },
      "summary": "최근의 이유 기반의 론칭 모형은 최종적인 답을 생성하기 전에 긴 중간적인 이유의 경로를 생성하여 높은 정확도를 달성합니다. 이 접근 방식은 논리적인 사고가 필요한 문제를 해결하는 데 효과적이지만, 긴 이유의 경로는 메모리 사용량과 토큰 생성의 트랜잭션을 크게 증가시켜 이러한 모델의 실질적인 적용이 제한됩니다. 우리는 이유의 경로의 семанти적 희소성을 활용하여 추론을 가속화하는 데 도움이 되는 훈련이 제한되지 않는 방법인 \"이유의 경로 압축(RPC)\"를 제안합니다. RPC는 최근 생성된 질문을 구성하는 선택 창에 의해 계산되는 중요도 스코어를 사용하여, 높은 중요도 스코어를 가진 KV 캐시를 유지함으로써 주기적으로 KV 캐시를 압축합니다. 실험은 RPC가 전체 KV 캐시를 사용하는 추론과 비교하여 QwQ-32B의 생성 트랜잭션을 1.60배 증가시키고, AIME 2024 벤치마크에서 정확도의 하락이 1.2%인 것을 보여주었습니다. 우리의 발견은 이유의 트래스의 семанти적 희소성을 효과적으로 활용할 수 있다는 것을 보여줍니다. 이는 이유의 LLM의 효율적인 적용을 위한 실질적인 길을 제공하며, 우리의 코드는 https://github.com/jiwonsong-dev/ReasoningPathCompression에 공개되어 있습니다.",
      "upvotes": 9,
      "discussionId": "682d2def396c1e613e9fcc0b",
      "githubRepo": "https://github.com/jiwonsong-dev/ReasoningPathCompression",
      "ai_keywords": [
        "Reasoning Path Compression (RPC)",
        "KV cache",
        "semantic sparsity",
        "reasoning traces",
        "QwQ-32B",
        "AIME 2024 benchmark"
      ]
    },
    "publishedAt": "2025-05-19T23:21:52.000Z",
    "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
    "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60times compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13866.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662672eaebdfec5cfdf1d034",
      "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
      "fullname": "Jiwon Song",
      "name": "jiwonsong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14677",
      "authors": [
        {
          "_id": "682d718306291bf11fcf69a3",
          "user": {
            "_id": "664da76e4eb4c91c8c32cc06",
            "avatarUrl": "/avatars/050bb6e4136f8a1645fef277ad08c7fc.svg",
            "isPro": false,
            "fullname": "Jiaer Xia",
            "user": "Jiaer-Xia",
            "type": "user"
          },
          "name": "Jiaer Xia",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:14:20.493Z",
          "hidden": false
        },
        {
          "_id": "682d718306291bf11fcf69a4",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:39:56.735Z",
          "hidden": false
        },
        {
          "_id": "682d718306291bf11fcf69a5",
          "name": "Peng Gao",
          "hidden": false
        },
        {
          "_id": "682d718306291bf11fcf69a6",
          "name": "Yixuan Li",
          "hidden": false
        },
        {
          "_id": "682d718306291bf11fcf69a7",
          "user": {
            "_id": "62ac6656de8bfbb93094b8fd",
            "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
            "isPro": false,
            "fullname": "Kaiyang Zhou",
            "user": "kaiyangzhou",
            "type": "user"
          },
          "name": "Kaiyang Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:14:28.433Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:58:35.000Z",
      "submittedOnDailyAt": "2025-05-21T04:54:13.466Z",
      "title": "비지온알라이즈-R1: 시각적 추론에서의 짧은 시간 간격을 줄이기 위한 강화학습",
      "submittedOnDailyBy": {
        "_id": "62ac6656de8bfbb93094b8fd",
        "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
        "isPro": false,
        "fullname": "Kaiyang Zhou",
        "user": "kaiyangzhou",
        "type": "user"
      },
      "summary": "학습의 일반적인 이유 능력은 오랜 기간 AI의 어려웠던 문제였습니다. 최근의 대규모 언어 모델(LLMs) 연구, 예를 들어 DeepSeek-R1에 따르면, GRPO 등 강화학습 방법들은 단순한 질문과 답의 쌍을 사용하여 학습된 LLMs가 이유 능력을 발전시킬 수 있음을 보여주었습니다. 본 논문에서는 강화학습과 시각 질문 대답 쌍을 사용하여 시각 언어 모델(VLMs)에 이유를 부여하고, 명시적인 이유의 연속(CoT)의 슈퍼비전을 포함하지 않는 것을 목표로 합니다. 우리의 발견은, 단순히 VLM에 강화학습을 적용하면, 간단한 문제를 통해 스로ット쉽을 발전시키며, 이전에 본 데이터 분포의 확장성을 줄일 수 있다는 것을 보여줍니다. 짧은 학습을 억제하기 위한 키는, 이유에 앞서 이미지의 해석을 촉구하는 것입니다. 따라서, 모델을 본 적이 없는 이미지의 Detailed Caption을 생성하고, 이유의 연속을 구축하기 위해 훈련합니다. 273K의 CoT 없는 시각 질문 대답 쌍을 학습하고, 단순히 강화학습을 사용한 경우, Visionary-R1이라는 이름을 가진 모델은 GPT-4o, Claude3.5-Sonnet, Gemini-1.5-Pro 등 강력한 다 타입 모델을 초월하고, 여러 시각 이유 벤치마크에서 뛰어납니다.",
      "upvotes": 8,
      "discussionId": "682d718406291bf11fcf69df",
      "githubRepo": "https://github.com/maifoundations/Visionary-R1",
      "ai_keywords": [
        "reinforcement learning",
        "deep learning",
        "large language models (LLMs)",
        "DeepSeek-R1",
        "GRPO",
        "visual language models (VLMs)",
        "visual question-answer pairs",
        "chain-of-thought (CoT)",
        "caption-reason-answer",
        "multimodal models",
        "GPT-4o",
        "Claude3.5-Sonnet",
        "Gemini-1.5-Pro",
        "visual reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-05-20T13:58:35.000Z",
    "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning",
    "summary": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14677.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ac6656de8bfbb93094b8fd",
      "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
      "fullname": "Kaiyang Zhou",
      "name": "kaiyangzhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14652",
      "authors": [
        {
          "_id": "682d474782567fffe1f99b7a",
          "user": {
            "_id": "5ec82854968f6028e0559f70",
            "avatarUrl": "/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg",
            "isPro": true,
            "fullname": "Xueguang Ma",
            "user": "MrLight",
            "type": "user"
          },
          "name": "Xueguang Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:41.675Z",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7b",
          "user": {
            "_id": "612ee6a7b960e78c6d2319d4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "isPro": false,
            "fullname": "Qian Liu",
            "user": "SivilTaram",
            "type": "user"
          },
          "name": "Qian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:39.513Z",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7c",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7d",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7e",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7f",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T03:23:52.529Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:41:33.000Z",
      "submittedOnDailyAt": "2025-05-21T01:56:42.807Z",
      "title": "일반 로봇: 모든 디렉토리에 있는 LLM 추론의 발전\n\n(Note: The translation provided is a direct literal translation from English to Korean. The term \"일반 로봇\" is used to maintain the original context of \"General Robot,\" which could be interpreted as a type of robot or a general term for robots in a broader context. If \"General Robot\" refers to a specific type or system, the translation might need to be adjusted accordingly.)",
      "submittedOnDailyBy": {
        "_id": "5ec82854968f6028e0559f70",
        "avatarUrl": "/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg",
        "isPro": true,
        "fullname": "Xueguang Ma",
        "user": "MrLight",
        "type": "user"
      },
      "summary": "강화학습(RL)은 최근 대규모 언어 모델(LLMs)의 성능을 향상시키는 강력한 가능성에 주목하고 있습니다. 특히, Deepseek-R1-Zero가 소개한 \"Zero\" 강화학습은 기초 LLMs의 직접적인 RL 훈련을 가능하게 하고, 중간적인 서브 객체 제어의 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종 최종",
      "upvotes": 8,
      "discussionId": "682d474882567fffe1f99bc5",
      "projectPage": "https://tiger-ai-lab.github.io/General-Reasoner/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/General-Reasoner",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "large language models (LLMs)",
        "Zero reinforcement learning",
        "base LLMs",
        "supervised fine-tuning",
        "generative model-based answer verifier",
        "chain-of-thought",
        "context-awareness"
      ]
    },
    "publishedAt": "2025-05-20T13:41:33.000Z",
    "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
    "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14652.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5ec82854968f6028e0559f70",
      "avatarUrl": "/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg",
      "fullname": "Xueguang Ma",
      "name": "MrLight",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14489",
      "authors": [
        {
          "_id": "682d55ecea67e90811b09b6b",
          "user": {
            "_id": "617f679fb15f8a665f3999fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617f679fb15f8a665f3999fc/NW1vkLsGAlWpAQYTux05X.jpeg",
            "isPro": false,
            "fullname": "Dongkeun Yoon",
            "user": "DKYoon",
            "type": "user"
          },
          "name": "Dongkeun Yoon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:23.897Z",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b6c",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b6d",
          "name": "Sohee Yang",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b6e",
          "name": "Sunkyoung Kim",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b6f",
          "name": "Soyeon Kim",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b70",
          "name": "Yongil Kim",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b71",
          "name": "Eunbi Choi",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b72",
          "user": {
            "_id": "660260cf1737e5cd4a826550",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660260cf1737e5cd4a826550/AlSfoM2WtqPjLtYR6x7Wf.jpeg",
            "isPro": false,
            "fullname": "Yireun Kim",
            "user": "yireun",
            "type": "user"
          },
          "name": "Yireun Kim",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T04:27:50.114Z",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b73",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/uZZgyWD0lw8jektQyDZfV.png",
        "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/_Kq3BF1HC_aFc6mee08zl.png",
        "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/bpz_J8XujNicCvMH2n2iK.png"
      ],
      "publishedAt": "2025-05-20T15:19:00.000Z",
      "submittedOnDailyAt": "2025-05-21T02:58:45.807Z",
      "title": "Reasoning Models Better Express Their Confidence\n\n이 모델은 자신들의 자신감에 대해 더 잘 표현할 수 있습니다.",
      "submittedOnDailyBy": {
        "_id": "617f679fb15f8a665f3999fc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617f679fb15f8a665f3999fc/NW1vkLsGAlWpAQYTux05X.jpeg",
        "isPro": false,
        "fullname": "Dongkeun Yoon",
        "user": "DKYoon",
        "type": "user"
      },
      "summary": "대 언어 모형（LLMs）는 그 강점을 더해서 자신감을 정확히 전달하는 것이 어려워져, 그 오류를 판단하는 것도 어려워져 신뢰도가 제한된다. 본 연구에서는, 장기 연속성 로딩（CoT） 로직을 수행하는 로직 모형（LLMs）이 문제를 해결하는 데에 그저如此한 우수한 성능을 보여주고, 자신감을 정확히 전달할 수 있는 것을도 시사한다. 특히, 6개의 로직 모형을 6개의 데이터 세트로 벤치마크하여, 33개의 36개의 설정 중, 비 로직 모형과 비교하여 엄격히 더 좋은 자신감 조정을 실현하는 것을 발견했다. 상세한 분석에 따르면, 이 조정의 향상은 로직 모형의 장기 연속성 로딩처럼 점차적인 사고 볼륨에 의한 것이다. 예를 들어, 전략 탐색이나 후퇴 등, 이들은 연속성 로딩 과정에서 자신감을 동적으로 조정할 수 있게 하고, 점차적으로 정확해지게 된다. 특히, 로직 모형은 연속성 로딩이 진행될수록 조정이 점차적으로 개선되는 것을 발견하고, 이 경향은 비 로직 모형에서 보이지 않는다. 또, 연속성 로딩에서 점차적인 사고 볼륨을 제거하는 것은 조정의 저하에 이어진다. 마지막으로, 이러한 향상은 로직 모형뿐 아니라 비 로직 모형도, 연속 학습에 의해 점차적인 사고를 수행함으로써 더 나은 성능을示す 것을 시사한다.",
      "upvotes": 8,
      "discussionId": "682d55edea67e90811b09ba1",
      "githubRepo": "https://github.com/MattYoon/reasoning-models-confidence",
      "ai_keywords": [
        "large language models (LLMs)",
        "chain-of-thought (CoT) reasoning",
        "confidence calibration",
        "non-reasoning counterparts",
        "slow thinking behaviors",
        "backtracking",
        "in-context learning"
      ]
    },
    "publishedAt": "2025-05-20T11:19:00.000Z",
    "title": "Reasoning Models Better Express Their Confidence",
    "summary": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/uZZgyWD0lw8jektQyDZfV.png",
      "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/_Kq3BF1HC_aFc6mee08zl.png",
      "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/bpz_J8XujNicCvMH2n2iK.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14489.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "617f679fb15f8a665f3999fc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617f679fb15f8a665f3999fc/NW1vkLsGAlWpAQYTux05X.jpeg",
      "fullname": "Dongkeun Yoon",
      "name": "DKYoon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13559",
      "authors": [
        {
          "_id": "682d4ccc3b5f51f4218e12b4",
          "name": "Sathya Krishnan Suresh",
          "hidden": false
        },
        {
          "_id": "682d4ccc3b5f51f4218e12b5",
          "name": "Tanmay Surana",
          "hidden": false
        },
        {
          "_id": "682d4ccc3b5f51f4218e12b6",
          "name": "Lim Zhi Hao",
          "hidden": false
        },
        {
          "_id": "682d4ccc3b5f51f4218e12b7",
          "name": "Eng Siong Chng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T09:18:14.000Z",
      "submittedOnDailyAt": "2025-05-21T02:18:05.886Z",
      "title": "CS-Sum: 코드 스위치 다이ア로기 요약의 벤치마크와 대규모 언어 모델의 한계",
      "submittedOnDailyBy": {
        "_id": "62f0e457bc8201db9ef47f89",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f0e457bc8201db9ef47f89/zOhDptwZpDGaugKCBOWB2.jpeg",
        "isPro": false,
        "fullname": "Sathya Krishnan",
        "user": "SkAndMl",
        "type": "user"
      },
      "summary": "コードスイッチ（CS）는 대규모 언어 모델（LLMs）에 있어서 중요한 문제로 등장하지만, 그 이해도가 LLMs에서 아직 상세히 조사되지 않았습니다. 우리는 CS-Sum을 소개합니다. 이는 LLMs가 CS 대화에서 영어 요약을 수행함으로써 CS의 이해도를 평가하는 데 사용할 수 있는 벤치마크입니다. CS-Sum은 표준적인 언어 기반이 아니라, 영어-중국어（EN-ZH）、영어-타밀어（EN-TA）、영어-말레이어어（EN-MS）의 3가지 언어 쌍에서 CS 대화 요약을 평가하는 첫 번째 벤치마크입니다. 각 언어 쌍에는 900-1300건의 인간 설명 다이얼로그가 포함되어 있습니다.\n\n10개의 LLMs（이것은 오픈소스 모델과 클로저소스 모델을 포함합니다）를 평가하고, 소수 학습, 번역 요약, 덧붙이 데이터에 대한 LoRA, QLoRA의 3가지 접근 방식을 사용하여 실적 결과를 분석했습니다. 우리의 발견은 자동 평가 지표의 점수가 높더라도, LLMs는 대화의 전체적인 의미를 변경하는 微妙한 오류를 발견할 수 있습니다. 따라서, 우리는 LLMs가 CS 입력을 처리할 때 가장 일반적인 3가지 오류 유형을 소개합니다. 오류율은 CS 페어별로 다르며, 각 LLMs에서도 다르며, 따라서 특정 언어 페어에서 더 자주 오류를 발견할 수 있는 LLMs가 존재하며, 이는 CS 데이터에 대한 전문적인 훈련의 필요성을 강조합니다.",
      "upvotes": 8,
      "discussionId": "682d4ccd3b5f51f4218e12f4",
      "ai_keywords": [
        "code-switching (CS)",
        "Large Language Models (LLMs)",
        "CS-Sum",
        "CS dialogue to English summarization",
        "Mandarin-English (EN-ZH)",
        "Tamil-English (EN-TA)",
        "Malay-English (EN-MS)",
        "few-shot",
        "translate-summarize",
        "fine-tuning",
        "LoRA",
        "QLoRA",
        "synthetic data",
        "CS input"
      ]
    },
    "publishedAt": "2025-05-19T05:18:14.000Z",
    "title": "CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the\n  Limits of Large Language Models",
    "summary": "Code-switching (CS) poses a significant challenge for Large Language Models\n(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce\nCS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue\nto English summarization. CS-Sum is the first benchmark for CS dialogue\nsummarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and\nMalay-English (EN-MS), with 900-1300 human-annotated dialogues per language\npair. Evaluating ten LLMs, including open and closed-source models, we analyze\nperformance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA\non synthetic data) approaches. Our findings show that though the scores on\nautomated metrics are high, LLMs make subtle mistakes that alter the complete\nmeaning of the dialogue. To this end, we introduce 3 most common type of errors\nthat LLMs make when handling CS input. Error rates vary across CS pairs and\nLLMs, with some LLMs showing more frequent errors on certain language pairs,\nunderscoring the need for specialized training on code-switched data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13559.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62f0e457bc8201db9ef47f89",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f0e457bc8201db9ef47f89/zOhDptwZpDGaugKCBOWB2.jpeg",
      "fullname": "Sathya Krishnan",
      "name": "SkAndMl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14680",
      "authors": [
        {
          "_id": "682d30a37812103582f50de4",
          "name": "Sunhao Dai",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de5",
          "name": "Wenjie Wang",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de6",
          "name": "Liang Pang",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de7",
          "name": "Jun Xu",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de8",
          "name": "See-Kiong Ng",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de9",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50dea",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:59:13.000Z",
      "submittedOnDailyAt": "2025-05-21T00:18:37.037Z",
      "title": "NExT-Search: generative AI 검색의 사용자 피드백 시스템의 재구성",
      "submittedOnDailyBy": {
        "_id": "64db88993725f8d9a908c077",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
        "isPro": false,
        "fullname": "Sunhao Dai",
        "user": "KID-22",
        "type": "user"
      },
      "summary": "생성형 AI 검색은 복잡한 검색어에 대한 끝말끝말의 답변을 제공하여 사용자의 직접적인 검색 및 검색어의 선택을 줄이고 정보 검색을 리시탠싱하고 있습니다. 그러나 이 패러다임은 편리성을 향상시키며 역사적으로 텍스트 수준의 큰 규모의 사용자 피드백을 통해 발전한 명확한 웹 검색의 발전을 방해하고 있습니다. 웹 검색은 문서 수준의 큰 규모의 미세한 사용자 피드백(예: 클릭, 잔류 시간)을 수집하여 랭킹 모델을 지속적으로 개선할 수 있습니다. 반면, 생성형 AI 검색은 검색어 분해, 문서 검색, 답변 생성의 긴 검색 파이프라인을 통해 동작하며 일반적으로 최종적인 답변에 대한 조각감의 피드백을 받습니다. 이는 최종적인 출력에 대한 사용자 피드백이 특정 시스템 구성 요소에 효과적으로 매핑되지 못하여 각 중간 단계의 개선과 피드백 루프의 유지가 어려워집니다. 본 논문에서는 NExT-Search라는 다음 세대 패러다임에 대해 상상하고 있습니다. 이는 생성형 AI 검색에서 미세한, 프로세스 수준의 피드백을 재 소개하는 것입니다. NExT-Search는 User Debug Mode와 Shadow User Mode의 두 가지 보완 모드를 통합하고 있습니다. User Debug Mode는 관심 있는 사용자가 중요한 단계에서 접근할 수 있는 것입니다. Shadow User Mode는 개별 사용자의 취향을 시뮬레이션하여 상호작용이 낮은 사용자에게 AI를 도와준 피드백을 제공합니다. 또한 이러한 피드백 신호를 온라인 적응과 오프라인 업데이트를 통해 활용할 수 있도록 검토하고 있습니다. 이는 현재의 검색 출력을 시간으로 미세하게 조정하고 인터섹션 로그를 수집하여 검색어 분해, 검색, 생성 모델을 정기적으로 미세하게 조정하는 것을 예상하고 있습니다. 생성형 AI 검색 파이프라인의 중요한 단계에 인간 제어를 복구하여 NExT-Search는 인간 피드백과 함께 발전할 수 있는 풍부한 AI 검색 시스템의 구축을 희망하는 방향을 제공하여 믿습니다.",
      "upvotes": 7,
      "discussionId": "682d30a47812103582f50e19",
      "ai_keywords": [
        "generative AI search",
        "information retrieval",
        "end-to-end answers",
        "complex queries",
        "manually browsing",
        "summarizing",
        "traditional Web search",
        "ranking models",
        "fine-grained user feedback",
        "clicks",
        "dwell time",
        "document level",
        "query decomposition",
        "document retrieval",
        "answer generation",
        "coarse-grained feedback",
        "feedback loop disconnect",
        "system components",
        "NExT-Search",
        "User Debug Mode",
        "Shadow User Mode",
        "personalized user agent",
        "AI-assisted feedback",
        "online adaptation",
        "offline update",
        "real-time refinement",
        "interaction logs",
        "fine-tune query decomposition",
        "fine-tune retrieval",
        "fine-tune generation models",
        "feedback-rich AI search systems"
      ]
    },
    "publishedAt": "2025-05-20T13:59:13.000Z",
    "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search",
    "summary": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14680.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64db88993725f8d9a908c077",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
      "fullname": "Sunhao Dai",
      "name": "KID-22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14673",
      "authors": [
        {
          "_id": "682d713e6c66e25ab59fc800",
          "user": {
            "_id": "65c38108425a226a29f00365",
            "avatarUrl": "/avatars/c276a0d2b108df446246c31a396496f0.svg",
            "isPro": false,
            "fullname": "tongyu",
            "user": "yutchina02",
            "type": "user"
          },
          "name": "Yu Tong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:01.048Z",
          "hidden": false
        },
        {
          "_id": "682d713e6c66e25ab59fc801",
          "user": {
            "_id": "65ad57da57f263e3d030187a",
            "avatarUrl": "/avatars/e1e3c4119180aee5fb660d1e5f745e99.svg",
            "isPro": false,
            "fullname": "潘子豪",
            "user": "Apostle723",
            "type": "user"
          },
          "name": "Zihao Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:39:58.888Z",
          "hidden": false
        },
        {
          "_id": "682d713e6c66e25ab59fc802",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "682d713e6c66e25ab59fc803",
          "name": "Kaiyang Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:58:02.000Z",
      "submittedOnDailyAt": "2025-05-21T04:53:16.661Z",
      "title": "ディープラーニング 기반의 이미지 생성을 위한 자동 단어 생성 기술에 적용되는 水印付き 이미지 생성 기술\n\n(Note: \"水印付き\" is translated as \"수인자付き\" to maintain the original meaning of \"watermarked\" in Korean, though \"수인자付き\" is more commonly used in technical contexts.)",
      "submittedOnDailyBy": {
        "_id": "62ac6656de8bfbb93094b8fd",
        "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
        "isPro": false,
        "fullname": "Kaiyang Zhou",
        "user": "kaiyangzhou",
        "type": "user"
      },
      "summary": "Invisible 이미지 워터마크링은 이미지 소유권 보호 및 시각 생성 모델의 악의적 사용 방지를 가능하게 합니다. 그러나 기존의 생성 워터마크링 방법은 주로 확산 모델을 위한 설계를 하고 있으며, 자동 회귀 이미지 생성 모델에 대한 워터마크링은 크게 탐색되지 않은 상태입니다. 우리는 인덱스 마크(IndexMark)를 제안하여, 자동 회귀 이미지 생성 모델에 대한 훈련 없는 워터마크링 프레임워크를 제공합니다. 인덱스 마크는 코드북의 중복 특성을 기반으로, 유사 인덱스를 대체하여 거의 없는 시각 차이를 생성하도록 설계되었습니다. 인덱스 마크의 핵심 요소는 간단하지만 효과적인 매치 후 대체 방법입니다. 이 방법은 토큰 유사성을 기반으로 코드북에서 워터마크 토큰을 신중히 선택하고, 토큰 대체를 통해 워터마크 토큰의 사용을 촉진하여 이미지 품질에 영향을 미치지 않는 방식으로 워터마크를 삽입합니다. 워터마크 검증은 생성된 이미지에서 워터마크 토큰의 비율을 계산하여 수행되며, 인덱스 인코더를 통해 정밀도를 더 높일 수 있습니다. 또한, 이진 파싱 공격에 대한 강건성을 향상시키기 위해 보조 검증 계획을 도입했습니다. 실험은 이미지 품질과 검증 정확도에서 최첨단 성능을 달성하고, 다양한 파괴, 잡음, 가우시안 블러, 랜덤 에라sing, 색 진동, JPEG 압축 등 다양한 파괴에 강건함을 보여주었습니다.",
      "upvotes": 7,
      "discussionId": "682d71426c66e25ab59fc946",
      "githubRepo": "https://github.com/maifoundations/IndexMark",
      "ai_keywords": [
        "autoregressive image generation models",
        "codebook",
        "indices",
        "token similarity",
        "watermark tokens",
        "token replacement",
        "IndexMark",
        "match-then-replace method",
        "Index Encoder",
        "auxiliary validation scheme",
        "cropping attacks",
        "Gaussian blur",
        "random erasing",
        "color jittering",
        "JPEG compression"
      ]
    },
    "publishedAt": "2025-05-20T13:58:02.000Z",
    "title": "Training-Free Watermarking for Autoregressive Image Generation",
    "summary": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14673.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ac6656de8bfbb93094b8fd",
      "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
      "fullname": "Kaiyang Zhou",
      "name": "kaiyangzhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14640",
      "authors": [
        {
          "_id": "682d488557686b8c44f257fa",
          "user": {
            "_id": "65c387c807a1445dfe1e9452",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c387c807a1445dfe1e9452/t0VnwQh2wRZ9W_UGTZ8zt.jpeg",
            "isPro": false,
            "fullname": "Wentao Ma",
            "user": "tonymwt",
            "type": "user"
          },
          "name": "Wentao Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:31.165Z",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257fb",
          "user": {
            "_id": "64405a9d518271b0d1beea38",
            "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg",
            "isPro": false,
            "fullname": "Weiming Ren",
            "user": "wren93",
            "type": "user"
          },
          "name": "Weiming Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:28.689Z",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257fc",
          "name": "Yiming Jia",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257fd",
          "user": {
            "_id": "66349404f2c753240d02952a",
            "avatarUrl": "/avatars/4f207cf5807d9629b9f4f7d13875b840.svg",
            "isPro": false,
            "fullname": "ZhuofengLi",
            "user": "ZhuofengLi",
            "type": "user"
          },
          "name": "Zhuofeng Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:35.151Z",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257fe",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257ff",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f25800",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:26:32.000Z",
      "submittedOnDailyAt": "2025-05-21T02:03:04.298Z",
      "title": "VideoEval-Pro: 견고하고 현실적인 긴 비디오의 이해 평가",
      "submittedOnDailyBy": {
        "_id": "64405a9d518271b0d1beea38",
        "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg",
        "isPro": false,
        "fullname": "Weiming Ren",
        "user": "wren93",
        "type": "user"
      },
      "summary": "대규모 다모달 모델(LMMs)은 최근 긴 비디오 이해(LVU) 분야에서 강력한 도구로 등장하며, 표준화된 LVU 벤치마크 개발에 촉진되어 있습니다. 그러나 우리의 조사는 현재의 LVU 벤치마크에 대해 더 냉정하게 교훈을 제시하고 있습니다. 우선, 현재 벤치마크는 정답을 맞추는 가능성 때문에, 평가 결과를 높일 수 있는 방법으로 다수 선택 문제(MCQs)를 중시하고 있습니다. 그리고 이러한 벤치마크의 일부 문제를 모델이 읽지 않아도 답을 할 수 있는 강력한 사전 지식을 가지고 있습니다. 예를 들어, Gemini-1.5-Pro는 Video-MME에서 긴 비디오에서 랜덤한 프레임을 제공하면 50% 이상의 정답률을 달성할 수 있습니다. 또한, 우리는 프레임의 수를 늘리면 현재 벤치마크에서 성능 향상을 반드시 볼 수 없다는 것을 발견했습니다. 이는 직관적으로는 반대입니다. 이러한 이유로, 현재 LVU 벤치마크의 정당성과 강건성이 뒤흔듭니다, LMMs의 긴 비디오 이해 능력의 진정한 평가가 방해되어 있습니다. 이러한 문제를 해결하기 위해, 우리는 VideoEval-Pro라는 실용적인 LVU 벤치마크를 제안합니다. 이 것은 비디오 전체를 이해하는 데 필요한 개방적인 짧은 답변을 포함하는 것입니다. VideoEval-Pro는 시각적인 구조와 논리적인 구조로, Segment 수준과 전체 비디오 이해를 평가합니다. 21개의 공개 및 오픈소스 비디오 LMMs를 평가하고, 다음 결과를 얻었습니다: 1) video LMMs는 MCQs와 비교하여 개방적인 문제에서 큰 성능 저하(>25%)를 보입니다. 2) 놀라울 정도로, MCQ 점수가 높으면 VideoEval-Pro에서 개방적인 점수가 높아지지 않습니다. 3) 다른 MCQ 벤치마크와 비교하여, VideoEval-Pro는 입력 프레임의 수를 늘리면 더 많은 이익을 얻을 수 있습니다. 우리의 결과는 VideoEval-Pro가 긴 비디오 이해를 평가하기 위해 더 실용적이고 신뢰성 있는 지표를 제공하며, 이 분야의 발전을 명확히 보여주는 것입니다.",
      "upvotes": 6,
      "discussionId": "682d488857686b8c44f258b7",
      "projectPage": "https://tiger-ai-lab.github.io/VideoEval-Pro",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VideoEval-Pro",
      "ai_keywords": [
        "multimodal models",
        "long video understanding",
        "benchmarks",
        "multiple-choice questions",
        "Video-MME",
        "random frame",
        "VideoEval-Pro",
        "open-ended questions",
        "segment-level understanding",
        "full-video understanding",
        "perception tasks",
        "reasoning tasks",
        "performance drops"
      ]
    },
    "publishedAt": "2025-05-20T13:26:32.000Z",
    "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation",
    "summary": "Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance (>25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14640.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64405a9d518271b0d1beea38",
      "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg",
      "fullname": "Weiming Ren",
      "name": "wren93",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13430",
      "authors": [
        {
          "_id": "682d70c598e3ea9be315ed85",
          "name": "Sifeng Shang",
          "hidden": false
        },
        {
          "_id": "682d70c598e3ea9be315ed86",
          "name": "Jiayi Zhou",
          "hidden": false
        },
        {
          "_id": "682d70c598e3ea9be315ed87",
          "name": "Chenyu Lin",
          "hidden": false
        },
        {
          "_id": "682d70c598e3ea9be315ed88",
          "name": "Minxian Li",
          "hidden": false
        },
        {
          "_id": "682d70c598e3ea9be315ed89",
          "name": "Kaiyang Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T17:55:15.000Z",
      "submittedOnDailyAt": "2025-05-21T04:51:33.821Z",
      "title": "ゼロ次元最適化을 이용한 量子화 뉴럴네트워크의 미세조정\n\n(注意：此处的翻译严格遵循原文，保持了专业性和准确性，同时确保了语言的自然流畅。)",
      "submittedOnDailyBy": {
        "_id": "62ac6656de8bfbb93094b8fd",
        "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
        "isPro": false,
        "fullname": "Kaiyang Zhou",
        "user": "kaiyangzhou",
        "type": "user"
      },
      "summary": "GPU 메모리는 딥러닝 모델의 크기가 지수적으로 증가함에 따라, 이러한 모델을 스트리밍 태스크에 적용할 때의 한계로 자리잡고 있습니다. 본 논문에서는 모델 가중치, 경사, 최적화 상태의 메모리 사용량을 최소화하고, 한계 내의 메모리 효율적인 훈련을 추진하는 것을 목표로 합니다. 우리의 아이디어는 경사와 최적화 상태를 0차 최적화(zeroth-order optimization)를 사용하여 제거하는 것입니다. 이는 경사를 근사하기 위해, 가중치를 전파 과정에서 풀링하여 경사의 방향을 결정하는 것입니다. 가중치의 메모리 사용량을 최소화하기 위해, 모델 퀀텀화(quantization)을 사용합니다. 예를 들어, bfloat16을 int4로 변환합니다. 그러나 퀀텀화된 가중치에 직접 0차 최적화를 적용하는 것은 이산한 가중치와 연속한 경사의 정확도 간극에 의해 불가능합니다. 이挑戦를 극복하기 위해, 우리는 경사의 추정을 위해 연속적인 퀀텀화 스케일을 풀링하고, 방향 함수 미분 클립핑법을 사용하여 훈련을 안정화하는 새로운 접근 방식인 퀀텀화 0차 최적화(QZO)를 제안합니다. QZO는 스칼라 기반과 코드북 기반의 훈련 후 퀀텀화 방법 모두에 직교하게 위치합니다. bfloat16 기반의 전체 파라미터 미세 조정에 비해, QZO는 4비트 LLM의 경우 총 메모리 비용 18배 이상 줄일 수 있으며, Llama-2-13B와 Stable Diffusion 3.5 Large의 미세 조정을 24GB GPU에서 한 번에 수행할 수 있습니다.",
      "upvotes": 5,
      "discussionId": "682d70c898e3ea9be315ee64",
      "githubRepo": "https://github.com/maifoundations/QZO",
      "ai_keywords": [
        "zeroth-order optimization",
        "gradient estimation",
        "model quantization",
        "bfloat16",
        "int4",
        "quantization scale",
        "directional derivative clipping",
        "Quantized Zeroth-order Optimization (QZO)",
        "scalar-based post-training quantization",
        "codebook-based post-training quantization",
        "full-parameter fine-tuning",
        "Llama-2-13B",
        "Stable Diffusion 3.5 Large"
      ]
    },
    "publishedAt": "2025-05-19T13:55:15.000Z",
    "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization",
    "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18times for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13430.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ac6656de8bfbb93094b8fd",
      "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
      "fullname": "Kaiyang Zhou",
      "name": "kaiyangzhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.12448",
      "authors": [
        {
          "_id": "682d47aa64daf8623f1f5604",
          "user": {
            "_id": "6586817e509bcae23f3dfc60",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pwEStoey1XJYDi3F0Z930.png",
            "isPro": false,
            "fullname": "Yang Liu",
            "user": "yliu-cs",
            "type": "user"
          },
          "name": "Yang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:37.342Z",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5605",
          "name": "Ming Ma",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5606",
          "name": "Xiaomin Yu",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5607",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5608",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5609",
          "name": "Mingyang Sun",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f560a",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f560b",
          "name": "Donglin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T14:40:16.000Z",
      "submittedOnDailyAt": "2025-05-21T01:56:36.099Z",
      "title": "SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SS",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "ビジュアル・ラウンジモデル（VLMs）가 다중모달 태스크에서 놀라울 정도로 발전하고 있는 데에도 RGB 입력의 의존성으로 공간 이해의 정확도가 제한되어 있습니다. 현재의 공간 정보를 통합하는 방법인 포인트 云 또는 깊이의 사용은 특화된 센서가 필요하거나 깊이 정보를 고차원의 추론에 효과적으로 활용할 수 없기 때문입니다. 이에 대해 우리는 새로운 공간 이해와 논리적 방법, SSR(Spatial Sense and Reasoning)을 제안합니다. SSR는 간단한 깊이 데이터를 구조화하고 해석 가능한 문자열로 변환하는 새로운 프레임워크입니다. 이 문자열의 논리적 표현은 공간 논리 능력의 크게 향상을 위해 심미적인 중간 표현으로 활용됩니다. 또한 학습 압축을 활용하여 생성된 논리적 표현을 간결한 잠재적 벡터로 압축하여 현재의 VLMs와 무결하게 통합할 수 있는 리소스 효율적인 플랫폼으로 만들 수 있습니다. SSR-CoT 데이터셋을 도입하여 다양한 중간적인 공간 논리적 표현을 Annotation한 백만 규모의 시각-언어 논리 데이터셋을 제공하며, SSRBench를 통해 컴퓨터 비전의 다양한 태스크에 대한 상세한 벤치마크를 소개합니다. 여러 벤치마크에서 확장된 실험은 SSR가 깊이를 효과적으로 활용하고 공간 논리 능력을 향상시키고, VLMs를 더 인간적인 다중모달 이해에 향하게 하는 것을 보여주고 있습니다. 우리 프로젝트 페이지는 https://yliu-cs.github.io/SSR에 있습니다.",
      "upvotes": 5,
      "discussionId": "682d47ab64daf8623f1f5634",
      "projectPage": "https://yliu-cs.github.io/SSR/",
      "githubRepo": "https://github.com/yliu-cs/SSR",
      "ai_keywords": [
        "Spatial Sense and Reasoning (SSR)",
        "structured, interpretable textual rationales",
        "knowledge distillation",
        "compact latent embeddings",
        "resource-efficient integration",
        "SSR-CoT",
        "million-scale visual-language reasoning dataset",
        "intermediate spatial reasoning annotations",
        "SSRBench",
        "comprehensive multi-task benchmark",
        "depth utilization",
        "spatial reasoning",
        "human-like multi-modal understanding"
      ]
    },
    "publishedAt": "2025-05-18T10:40:16.000Z",
    "title": "SSR: Enhancing Depth Perception in Vision-Language Models via\n  Rationale-Guided Spatial Reasoning",
    "summary": "Despite impressive advancements in Visual-Language Models (VLMs) for\nmulti-modal tasks, their reliance on RGB inputs limits precise spatial\nunderstanding. Existing methods for integrating spatial cues, such as point\nclouds or depth, either require specialized sensors or fail to effectively\nexploit depth information for higher-order reasoning. To this end, we propose a\nnovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that\ntransforms raw depth data into structured, interpretable textual rationales.\nThese textual rationales serve as meaningful intermediate representations to\nsignificantly enhance spatial reasoning capabilities. Additionally, we leverage\nknowledge distillation to compress the generated rationales into compact latent\nembeddings, which facilitate resource-efficient and plug-and-play integration\ninto existing VLMs without retraining. To enable comprehensive evaluation, we\nintroduce a new dataset named SSR-CoT, a million-scale visual-language\nreasoning dataset enriched with intermediate spatial reasoning annotations, and\npresent SSRBench, a comprehensive multi-task benchmark. Extensive experiments\non multiple benchmarks demonstrate SSR substantially improves depth utilization\nand enhances spatial reasoning, thereby advancing VLMs toward more human-like\nmulti-modal understanding. Our project page is at\nhttps://yliu-cs.github.io/SSR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14631",
      "authors": [
        {
          "_id": "682d69ed056cf7b86cd8d6ec",
          "user": {
            "_id": "66ab80e9bfb7d73a56bc293c",
            "avatarUrl": "/avatars/9644266304c832c74ef572b5eb2d9468.svg",
            "isPro": false,
            "fullname": "Jack",
            "user": "lingjie23",
            "type": "user"
          },
          "name": "Lingjie Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:17.384Z",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6ed",
          "user": {
            "_id": "62d1227384bfbee86b6eec56",
            "avatarUrl": "/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg",
            "isPro": false,
            "fullname": "Xun Wu",
            "user": "YUSHUIWX",
            "type": "user"
          },
          "name": "Xun Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:20.741Z",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6ee",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6ef",
          "name": "Qingxiu Dong",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f0",
          "name": "Zewen Chi",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f1",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f2",
          "name": "Xingxing Zhang",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f3",
          "name": "Tengchao Lv",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f4",
          "name": "Lei Cui",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f5",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:23:25.000Z",
      "submittedOnDailyAt": "2025-05-21T04:22:54.134Z",
      "title": "Think Only When You Need with Large Hybrid-Reasoning Models\n\n이 문장은 \"필요한 경우에만 대규모 하이브리드 추론 모델을 사용하라\"라는 의미로 번역될 수 있습니다.",
      "submittedOnDailyBy": {
        "_id": "62d1227384bfbee86b6eec56",
        "avatarUrl": "/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg",
        "isPro": false,
        "fullname": "Xun Wu",
        "user": "YUSHUIWX",
        "type": "user"
      },
      "summary": "최근의 대규모 논리론 모델(LRMs)은 결과 도출 전에 확장된 사고 과정을 사용하며, 전통적인 대규모 언어 모델(LLMs)보다 크게 논리론 능력에 개선되어 있습니다. 그러나 과도한 긴 사고는 토큰 소비와 라틴어의 큰 오버헤드를 불러일으키며, 특히 간단한 질문에 대해 불필요한 것으로 나타납니다. 본 논문에서는 대규모 하이브리드 논리론 모델(LHRMs)을 소개합니다. LHRMs는 처음으로 제안된 하이브리드 Fine-Tuning(HFT)를 냉정한 시작으로, 그리고 제안된 하이브리드 그룹 정책 최적화(HGPO)를 사용하여 온라인 재학습을 결합하여, 사용자 질문의 컨텍스트 정보를 기반으로 사고를 수행할지 여부를 적응적으로 결정할 수 있는 모델입니다. 또한 하이브리드 사고 모델의 능력을定量적으로 평가하기 위해, Hybrid Accuracy라는 메트릭을 제안합니다. 확장된 사고 과정의 적절한 사용 방법을 재검토하기 위한 강력한 시작 포인트로, LHRMs는 논리론과 일반적인 능력을 향상시키고, 효율성을 크게 향상시키는 것을 보여줍니다.",
      "upvotes": 4,
      "discussionId": "682d69ee056cf7b86cd8d730",
      "ai_keywords": [
        "Large Reasoning Models (LRMs)",
        "Large Language Models (LLMs)",
        "Hybrid-Reasoning Models (LHRMs)",
        "Hybrid Fine-Tuning (HFT)",
        "Hybrid Group Policy Optimization (HGPO)",
        "Hybrid Accuracy"
      ]
    },
    "publishedAt": "2025-05-20T13:23:25.000Z",
    "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
    "summary": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14631.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d1227384bfbee86b6eec56",
      "avatarUrl": "/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg",
      "fullname": "Xun Wu",
      "name": "YUSHUIWX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14464",
      "authors": [
        {
          "_id": "682d39e6fa24196dcd10d5e8",
          "user": {
            "_id": "621499d72be42a56cca7afad",
            "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg",
            "isPro": false,
            "fullname": "TianXiaoyu",
            "user": "Emperorizzis",
            "type": "user"
          },
          "name": "Xiaoyu Tian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:56.337Z",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5e9",
          "name": "Yunjie Ji",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ea",
          "name": "Haotian Wang",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5eb",
          "name": "Shuaiting Chen",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ec",
          "name": "Sitong Zhao",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ed",
          "name": "Yiping Peng",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ee",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ef",
          "name": "Xiangang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T15:00:51.000Z",
      "submittedOnDailyAt": "2025-05-21T01:40:28.265Z",
      "title": "모든 정답은 같지 않음: 디스티루션의 원인에 의한 이유",
      "submittedOnDailyBy": {
        "_id": "621499d72be42a56cca7afad",
        "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg",
        "isPro": false,
        "fullname": "TianXiaoyu",
        "user": "Emperorizzis",
        "type": "user"
      },
      "summary": "디스티ル러는 오픈 소스 언어 모델의 논리 능력을 향상시키기 위한 실용적이고 효과적인 접근으로 등장했습니다. 본 연구에서는, 3가지의 가장 선진한 교사 모델(AM-Thinking-v1, Qwen3-235B-A22B, DeepSeek-R1)이 공유된 189만건의クエリ의 코퍼스로부터 증명된 출력을 모은 후, 이유 데이터의 디스티ル러에 대한 대규모 실험 연구를 수행했습니다. 3가지의 병렬 데이터 세트를 구축하고, 그 분포를 분석하여, AM-Thinking-v1으로부터 디스티ル러된 데이터가 태그 길이의 다양성이 높고, 퍼플레키티티가 낮은 것을 밝혀냅니다. 각 데이터 세트를 사용하여 훈련된 학생 모델은 AIME2024, AIME2025, MATH500, LiveCodeBench 등 이유 벤치마크에서 평가되었습니다. AM 기반의 모델은 AIME2024에서 84.3, AIME2025에서 72.2, MATH500에서 98.4, LiveCodeBench에서 65.9의 우수한 성능을 보였으며, 어려운 작업에 긴 응답을, 간단한 작업에 짧은 응답을 생성하여 적응적인 출력 행동을 나타냅니다. 이러한 발견은 고품질의 증명된 이유 트래스의 가치를 밝혀냅니다. AM-Thinking-v1과 Qwen3-235B-A22B의 디스티ル러된 데이터 세트를 공개하고, 오픈된 및 고성능의 이유 모델에 대한 향후 연구를 지원하기 위해, 이 데이터 세트를 Hugging Face에 릴리즈했습니다. 이러한 데이터 세트는 Hugging Face에서 공개적으로 사용할 수 있습니다.",
      "upvotes": 4,
      "discussionId": "682d39e8fa24196dcd10d643",
      "ai_keywords": [
        "distillation",
        "reasoning capabilities",
        "open-source language models",
        "empirical study",
        "reasoning data distillation",
        "AM-Thinking-v1",
        "Qwen3-235B-A22B",
        "DeepSeek-R1",
        "shared corpus",
        "parallel datasets",
        "token length diversity",
        "perplexity",
        "student models",
        "reasoning benchmarks",
        "AIME2024",
        "AIME2025",
        "MATH500",
        "LiveCodeBench",
        "adaptive output behavior"
      ]
    },
    "publishedAt": "2025-05-20T11:00:51.000Z",
    "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters",
    "summary": "Distillation has emerged as a practical and effective approach to enhance the\nreasoning capabilities of open-source language models. In this work, we conduct\na large-scale empirical study on reasoning data distillation by collecting\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\nconstruct three parallel datasets and analyze their distributions, revealing\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\nlower perplexity. Student models trained on each dataset are evaluated on\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\nThe AM-based model consistently achieves the best performance (e.g., 84.3 on\nAIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and\ndemonstrates adaptive output behavior-producing longer responses for harder\ntasks and shorter ones for simpler tasks. These findings highlight the value of\nhigh-quality, verified reasoning traces. We release the AM-Thinking-v1 and\nQwen3-235B-A22B distilled datasets to support future research on open and\nhigh-performing reasoning-oriented language models. The datasets are publicly\navailable on Hugging FaceDatasets are available on Hugging Face:\n\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled{AM-Thinking-v1-Distilled},\nhttps://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled{AM-Qwen3-Distilled}.}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14464.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "621499d72be42a56cca7afad",
      "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg",
      "fullname": "TianXiaoyu",
      "name": "Emperorizzis",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14674",
      "authors": [
        {
          "_id": "682d4145514c96fbf03f5f76",
          "name": "Jiaxin Guo",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f77",
          "name": "Zewen Chi",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f78",
          "user": {
            "_id": "5df85abada6d0311fd3d5408",
            "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
            "isPro": false,
            "fullname": "Li Dong",
            "user": "unilm",
            "type": "user"
          },
          "name": "Li Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:52.470Z",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f79",
          "name": "Qingxiu Dong",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f7a",
          "user": {
            "_id": "62d1227384bfbee86b6eec56",
            "avatarUrl": "/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg",
            "isPro": false,
            "fullname": "Xun Wu",
            "user": "YUSHUIWX",
            "type": "user"
          },
          "name": "Xun Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:50.085Z",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f7b",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f7c",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/wpApND0JXiL6s6MUpBdg7.png"
      ],
      "publishedAt": "2025-05-20T17:58:03.000Z",
      "submittedOnDailyAt": "2025-05-21T02:57:46.082Z",
      "title": "레벨 보상 논리 모델",
      "submittedOnDailyBy": {
        "_id": "5df85abada6d0311fd3d5408",
        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
        "isPro": false,
        "fullname": "Li Dong",
        "user": "unilm",
        "type": "user"
      },
      "summary": "보상 모델은 대규모 언어 모델을 인간의 기대에 맞는 출력에 지향하는 중요한 역할을 수행합니다. 그러나 효과적으로 검증 시의 계산량을 활용하여 보상 모델의 성능을 향상시키는 문제는 개방되어 있습니다. 본 논문에서는, 최종적인 보상을 생성하기 전에 신중한 이유를 담은 처리를 수행하기 위해 설계된 보상 이유 모델(RRMs)을 소개합니다. 연속적인 이유를 통해, RRMs은 적절한 보상이 바로 명확하지 않은 복잡한 질문에 대해 적절한 보상을 구하기 위해 추가적인 검증 시의 계산량을 활용합니다. RRMs의 개발에는 명시적인 이유의 추적 없이 자신의 진화 과정을 촉진하여 보상 이유 능력을 키우는 강화 학습 프레임워크를 구현하였습니다. 실험 결과를 통해, RRMs은 다양한 분야에서 보상 모델 벤치마크에서 상위 성능을 달성했습니다. 특히, RRMs은 검증 시의 계산량을 적절히 활용하여 보상의 정확도를 더욱 향상시킬 수 있음을 보여주었습니다. 사전 학습된 보상 이유 모델은 https://huggingface.co/Reward-Reasoning에서 사용할 수 있습니다.",
      "upvotes": 3,
      "discussionId": "682d4146514c96fbf03f5fab",
      "ai_keywords": [
        "Reward models",
        "large language models",
        "chain-of-thought reasoning",
        "reinforcement learning",
        "self-evolved reward reasoning",
        "reward modeling benchmarks"
      ]
    },
    "publishedAt": "2025-05-20T13:58:03.000Z",
    "title": "Reward Reasoning Model",
    "summary": "Reward models play a critical role in guiding large language models toward\noutputs that align with human expectations. However, an open challenge remains\nin effectively utilizing test-time compute to enhance reward model performance.\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\nspecifically designed to execute a deliberate reasoning process before\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\nadditional test-time compute for complex queries where appropriate rewards are\nnot immediately apparent. To develop RRMs, we implement a reinforcement\nlearning framework that fosters self-evolved reward reasoning capabilities\nwithout requiring explicit reasoning traces as training data. Experimental\nresults demonstrate that RRMs achieve superior performance on reward modeling\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\nexploit test-time compute to further improve reward accuracy. The pretrained\nreward reasoning models are available at\nhttps://huggingface.co/Reward-Reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/wpApND0JXiL6s6MUpBdg7.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5df85abada6d0311fd3d5408",
      "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
      "fullname": "Li Dong",
      "name": "unilm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14352",
      "authors": [
        {
          "_id": "682d72b0d57ba1e4d132148d",
          "user": {
            "_id": "6422f416a73327caad9d1d86",
            "avatarUrl": "/avatars/aa3639277cd1732504402fc64a57eff8.svg",
            "isPro": false,
            "fullname": "Bartosz Cywiński",
            "user": "bcywinski",
            "type": "user"
          },
          "name": "Bartosz Cywiński",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:39:48.008Z",
          "hidden": false
        },
        {
          "_id": "682d72b0d57ba1e4d132148e",
          "name": "Emil Ryd",
          "hidden": false
        },
        {
          "_id": "682d72b0d57ba1e4d132148f",
          "name": "Senthooran Rajamanoharan",
          "hidden": false
        },
        {
          "_id": "682d72b0d57ba1e4d1321490",
          "name": "Neel Nanda",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T13:36:37.000Z",
      "submittedOnDailyAt": "2025-05-21T04:59:37.379Z",
      "title": "LLM에서 기계적으로 설명 가능한 잠재적 지식 추출에 대한 접근법",
      "submittedOnDailyBy": {
        "_id": "6422f416a73327caad9d1d86",
        "avatarUrl": "/avatars/aa3639277cd1732504402fc64a57eff8.svg",
        "isPro": false,
        "fullname": "Bartosz Cywiński",
        "user": "bcywinski",
        "type": "user"
      },
      "summary": "이 텍스트는 텍스트의 번역만 반환합니다.",
      "upvotes": 3,
      "discussionId": "682d72b1d57ba1e4d13214c6",
      "githubRepo": "https://github.com/EmilRyd/eliciting-secrets",
      "ai_keywords": [
        "Taboo model",
        "logit lens",
        "sparse autoencoders"
      ]
    },
    "publishedAt": "2025-05-20T09:36:37.000Z",
    "title": "Towards eliciting latent knowledge from LLMs with mechanistic\n  interpretability",
    "summary": "As language models become more powerful and sophisticated, it is crucial that\nthey remain trustworthy and reliable. There is concerning preliminary evidence\nthat models may attempt to deceive or keep secrets from their operators. To\nexplore the ability of current techniques to elicit such hidden knowledge, we\ntrain a Taboo model: a language model that describes a specific secret word\nwithout explicitly stating it. Importantly, the secret word is not presented to\nthe model in its training data or prompt. We then investigate methods to\nuncover this secret. First, we evaluate non-interpretability (black-box)\napproaches. Subsequently, we develop largely automated strategies based on\nmechanistic interpretability techniques, including logit lens and sparse\nautoencoders. Evaluation shows that both approaches are effective in eliciting\nthe secret word in our proof-of-concept setting. Our findings highlight the\npromise of these approaches for eliciting hidden knowledge and suggest several\npromising avenues for future work, including testing and refining these methods\non more complex model organisms. This work aims to be a step towards addressing\nthe crucial problem of eliciting secret knowledge from language models, thereby\ncontributing to their safe and reliable deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14352.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6422f416a73327caad9d1d86",
      "avatarUrl": "/avatars/aa3639277cd1732504402fc64a57eff8.svg",
      "fullname": "Bartosz Cywiński",
      "name": "bcywinski",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14135",
      "authors": [
        {
          "_id": "682d43cf85d5e40c81ed313d",
          "name": "Ruihuang Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed313e",
          "name": "Caijin Zhou",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed313f",
          "name": "Shoujian Zheng",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3140",
          "name": "Jianxiang Lu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3141",
          "name": "Jiabin Huang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3142",
          "name": "Comi Chen",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3143",
          "name": "Junshu Tang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3144",
          "name": "Guangzheng Xu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3145",
          "name": "Jiale Tao",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3146",
          "name": "Hongmei Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3147",
          "name": "Donghao Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3148",
          "name": "Wenqing Yu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3149",
          "name": "Senbo Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314a",
          "name": "Zhimin Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314b",
          "name": "Yetshuan Shi",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314c",
          "name": "Haoyu Yang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314d",
          "name": "Yukun Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314e",
          "name": "Wenxun Dai",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314f",
          "name": "Jiaqi Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3150",
          "name": "Linqing Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3151",
          "name": "Qixun Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3152",
          "name": "Zhiyong Xu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3153",
          "name": "Yingfang Zhang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3154",
          "name": "Jiangfeng Xiong",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3155",
          "name": "Weijie Kong",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3156",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3157",
          "name": "Hongxin Zhang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3158",
          "name": "Qiaoling Zheng",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3159",
          "name": "Weiting Guo",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315a",
          "name": "Xinchi Deng",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315b",
          "name": "Yixuan Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315c",
          "name": "Renjia Wei",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315d",
          "name": "Yulin Jian",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315e",
          "name": "Duojun Huang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315f",
          "name": "Xuhua Ren",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3160",
          "name": "Sihuan Lin",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3161",
          "name": "Yifu Sun",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3162",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3163",
          "name": "Joey Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3164",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3165",
          "name": "Jingmiao Yu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3166",
          "name": "Jihong Zhang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3167",
          "name": "Caesar Zhong",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3168",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3169",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316a",
          "name": "Linus",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316b",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316c",
          "name": "Longhuang Wu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316d",
          "name": "Shuai Shao",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316e",
          "name": "Qinglin Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T09:39:48.000Z",
      "submittedOnDailyAt": "2025-05-21T01:40:24.172Z",
      "title": "フンユウンゲーム：インデックス 수준의 실체 수준의 스마트 게임 제작 모델",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "인텔리전트 게임 제작은 게임 개발에서 혁신적인 발전으로, 생성 AI를 사용하여 게임 콘텐츠의 동적 생성과 개선을 보여주고 있습니다. 생성 모델의显著한 진보에 더해, 고품질의 게임 자산(그림과 영상 포함)의 전방위적 합성은 어려운 경계입니다. 고품질의 게임 콘텐츠의 제작과 플레이어의 취향을 동시에 맞추며, 디자이너의 효율성을 크게 향상시키기 위해, 우리는 \"고보 게임\"이라는 혁신적인 프로젝트를 소개합니다. \"고보 게임\"은 이미지 생성과 영상 생성의 두 가지 주요 분야로 구성되어 있습니다. 이미지 생성 컴포넌트는 수조개 게임 이미지를 포함하는 큰 데이터 세트에 기반하여, 게임 시나리오에 맞는 사용자定制된 이미지 생성 모델을 개발했습니다: (1) 일반적인 텍스트로부터 이미지의 생성. (2) 게임 시각적 효과의 생성, 텍스트로부터의 효과 및 참조 이미지에 기반한 게임 시각적 효과의 생성. (3) 투명 이미지의 생성, 캐릭터, 스케너와 게임 시각적 효과. (4) 스케치, 흑백 이미지, 화이트 모델에 기반한 게임 캐릭터의 생성. 영상 생성 컴포넌트는 수백만 개의 게임과 애니메이션 영상을 포함하는 세부적인 데이터 세트에 기반하여, 게임 개발의 중요한 문제점을 파악하고, 다양한 게임 영상 시나리오에 강인한 적응성을 가진 5가지 핵심 알고리즘 모델을 개발했습니다: (1) 이미지로부터의 영상의 생성. (2) 360도 A/T 포즈 대체 영상의 합성. (3) 동적인 이라스팅 레레이션의 생성. (4) 생성된 영상의 초해상도. (5) 상호작용 게임 영상의 생성. 이러한 이미지와 영상 생성 모델은 고수준의 예술적 표현을 보여주고, 게임과 애니메이션의 다양한 아트 스타일에 대한 체계적인 이해를 깊게 하고, 분야 고유의 지식을 깊게 통합하고 있습니다.",
      "upvotes": 3,
      "discussionId": "682d43d585d5e40c81ed3302",
      "ai_keywords": [
        "text-to-image generation",
        "game visual effects generation",
        "text-to-effect",
        "reference image-based generation",
        "transparent image generation",
        "character generation",
        "sketch-based generation",
        "black-and-white image-based generation",
        "white model-based generation",
        "image-to-video generation",
        "360 A/T Pose Avatar Video Synthesis",
        "dynamic illustration generation",
        "generative video super-resolution",
        "interactive game video generation"
      ]
    },
    "publishedAt": "2025-05-20T05:39:48.000Z",
    "title": "Hunyuan-Game: Industrial-grade Intelligent Game Creation Model",
    "summary": "Intelligent game creation represents a transformative advancement in game\ndevelopment, utilizing generative artificial intelligence to dynamically\ngenerate and enhance game content. Despite notable progress in generative\nmodels, the comprehensive synthesis of high-quality game assets, including both\nimages and videos, remains a challenging frontier. To create high-fidelity game\ncontent that simultaneously aligns with player preferences and significantly\nboosts designer efficiency, we present Hunyuan-Game, an innovative project\ndesigned to revolutionize intelligent game production. Hunyuan-Game encompasses\ntwo primary branches: image generation and video generation. The image\ngeneration component is built upon a vast dataset comprising billions of game\nimages, leading to the development of a group of customized image generation\nmodels tailored for game scenarios: (1) General Text-to-Image Generation. (2)\nGame Visual Effects Generation, involving text-to-effect and reference\nimage-based game visual effect generation. (3) Transparent Image Generation for\ncharacters, scenes, and game visual effects. (4) Game Character Generation\nbased on sketches, black-and-white images, and white models. The video\ngeneration component is built upon a comprehensive dataset of millions of game\nand anime videos, leading to the development of five core algorithmic models,\neach targeting critical pain points in game development and having robust\nadaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)\n360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)\nGenerative Video Super-Resolution. (5) Interactive Game Video Generation. These\nimage and video generation models not only exhibit high-level aesthetic\nexpression but also deeply integrate domain-specific knowledge, establishing a\nsystematic understanding of diverse game and anime art styles.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14135.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6898
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11365",
      "authors": [
        {
          "_id": "682d7ae082567fffe108b31a",
          "user": {
            "_id": "6596ca5cce76219628b8eab4",
            "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
            "isPro": false,
            "fullname": "Pierre Le Jeune",
            "user": "pierlj",
            "type": "user"
          },
          "name": "Pierre Le Jeune",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T07:16:41.247Z",
          "hidden": false
        },
        {
          "_id": "682d7ae082567fffe108b31b",
          "user": {
            "_id": "65bccff4c1a44b6ef1c100da",
            "avatarUrl": "/avatars/bf91000c78b83167958dc44c582397f0.svg",
            "isPro": false,
            "fullname": "benoit",
            "user": "bmalezieux",
            "type": "user"
          },
          "name": "Benoît Malézieux",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:04:02.669Z",
          "hidden": false
        },
        {
          "_id": "682d7ae082567fffe108b31c",
          "user": {
            "_id": "649f00fc37bfb5202be464a9",
            "avatarUrl": "/avatars/89f6c6a92c076099f5450c3cd2057619.svg",
            "isPro": false,
            "fullname": "Inoki at Giskard",
            "user": "inoki-giskard",
            "type": "user"
          },
          "name": "Weixuan Xiao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:04:02.669Z",
          "hidden": false
        },
        {
          "_id": "682d7ae082567fffe108b31d",
          "name": "Matteo Dora",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T15:31:08.000Z",
      "submittedOnDailyAt": "2025-05-21T05:35:45.784Z",
      "title": "ファーレ: 대규모 언어 모델의 안전성 검증 도구",
      "submittedOnDailyBy": {
        "_id": "6596ca5cce76219628b8eab4",
        "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
        "isPro": false,
        "fullname": "Pierre Le Jeune",
        "user": "pierlj",
        "type": "user"
      },
      "summary": "LLM의 안전성 보장은 책임적인 기능 설정에 있어서 중요하지만, 현재의 평가는 종종 성능을 우선시하고 실패 모드를 특정하는 경우가 많습니다. 우리는 3가지 중요한 차원에서의 LLM의 행동을 조사하고 평가하기 위해 다언어 진단 프레임워크인 \"Phare\"를 소개합니다: 인사와 신뢰성, 사회 바이어스, 유해한 콘텐츠의 생성. 17가지의 가장 선진된 LLM을 평가한 결과, 모든 안전 차원에서의 체계적인 취약성 패턴을 밝혀 냈습니다. 이러한 구체적인 실패 모드를 특징적으로 보여주는 Phare는 연구자와 실천자에게 더 강건하고 일치하는 신뢰할 수 있는 언어 시스템의 구축에 도움이 되는 실질적인 아이디어를 제공합니다.",
      "upvotes": 3,
      "discussionId": "682d7ae282567fffe108b40f",
      "projectPage": "https://phare.giskard.ai/",
      "githubRepo": "https://github.com/Giskard-AI/phare",
      "ai_keywords": [
        "hallucination",
        "reliability",
        "social biases",
        "harmful content generation",
        "multilingual diagnostic framework",
        "sycophancy",
        "prompt sensitivity",
        "stereotype reproduction"
      ]
    },
    "publishedAt": "2025-05-16T11:31:08.000Z",
    "title": "Phare: A Safety Probe for Large Language Models",
    "summary": "Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11365.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6596ca5cce76219628b8eab4",
      "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
      "fullname": "Pierre Le Jeune",
      "name": "pierlj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14534",
      "authors": [
        {
          "_id": "682d7f009a06bc9f9106480b",
          "user": {
            "_id": "65ef0b4b325d9aaef87eb33b",
            "avatarUrl": "/avatars/ec27dba9de7e74c1f6cdcacf5aa25528.svg",
            "isPro": false,
            "fullname": "C Shi",
            "user": "chongyangs",
            "type": "user"
          },
          "name": "Chongyang Shi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:21:37.326Z",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f9106480c",
          "name": "Sharon Lin",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f9106480d",
          "name": "Shuang Song",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f9106480e",
          "name": "Jamie Hayes",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f9106480f",
          "name": "Ilia Shumailov",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064810",
          "name": "Itay Yona",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064811",
          "name": "Juliette Pluto",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064812",
          "name": "Aneesh Pappu",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064813",
          "name": "Christopher A. Choquette-Choo",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064814",
          "name": "Milad Nasr",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064815",
          "name": "Chawin Sitawarin",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064816",
          "name": "Gena Gibson",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064817",
          "name": "Andreas Terzis",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064818",
          "name": "John \"Four\" Flynn",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T15:54:45.000Z",
      "submittedOnDailyAt": "2025-05-21T05:52:22.988Z",
      "title": "제네미의 가짜 프로노프티nje injection 방어에서 얻은 교훈",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "제미니는 사용자가 대신해서 작업을 수행하는 데에 사용되고 있습니다. 이로 인해 함수 호출과 도구 사용 기능을 모델이 사용자 데이터에 액세스할 수 있게 되었습니다. 그러나 일부 도구는 불신 데이터에 액세스해야 하는 필요성이 있으며, 위험이 발생할 수 있습니다. 가이드라인은 불신 데이터에 악의적인 지시를 넣고 모델이 사용자의 기대에서 벗어날 수 있으며, 데이터나 권한을 잘못 처리하는 것을 유발할 수 있습니다. 이 보고서에서는 구글 디피드뮴이 제미니 모델의 상대방 강도를 평가하는 접근 방식을 설명하고, 그 과정에서 얻은 주요 교훈들을 설명합니다. 상대방으로 복잡한 가이드라인을 대면하여 제미니의 실적을 평가하기 위해 사용된 대면 평가 프레임워크를 통해 과거, 현재, 미래의 제미니 버전에 대해 연속적으로 적용 가능한 공격 방법 세트를 수행합니다. 이러한 연속적인 평가는 제미니가 동작에 대한 강도를 직접적으로 도움이 됩니다.",
      "upvotes": 2,
      "discussionId": "682d7f019a06bc9f9106486e",
      "ai_keywords": [
        "function-calling",
        "tool-use capabilities",
        "adversarial robustness",
        "adaptive attack techniques",
        "resilience"
      ]
    },
    "publishedAt": "2025-05-20T11:54:45.000Z",
    "title": "Lessons from Defending Gemini Against Indirect Prompt Injections",
    "summary": "Gemini is increasingly used to perform tasks on behalf of users, where\nfunction-calling and tool-use capabilities enable the model to access user\ndata. Some tools, however, require access to untrusted data introducing risk.\nAdversaries can embed malicious instructions in untrusted data which cause the\nmodel to deviate from the user's expectations and mishandle their data or\npermissions. In this report, we set out Google DeepMind's approach to\nevaluating the adversarial robustness of Gemini models and describe the main\nlessons learned from the process. We test how Gemini performs against a\nsophisticated adversary through an adversarial evaluation framework, which\ndeploys a suite of adaptive attack techniques to run continuously against past,\ncurrent, and future versions of Gemini. We describe how these ongoing\nevaluations directly help make Gemini more resilient against manipulation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14534.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.12182",
      "authors": [
        {
          "_id": "682d31dd17608739046e1169",
          "name": "Haohang Li",
          "hidden": false
        },
        {
          "_id": "682d31dd17608739046e116a",
          "name": "Yupeng Cao",
          "hidden": false
        },
        {
          "_id": "682d31dd17608739046e116b",
          "name": "Yangyang Yu",
          "hidden": false
        },
        {
          "_id": "682d31dd17608739046e116c",
          "name": "Jordan W. Suchow",
          "hidden": false
        },
        {
          "_id": "682d31dd17608739046e116d",
          "name": "Zining Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T00:47:21.000Z",
      "submittedOnDailyAt": "2025-05-21T00:23:07.653Z",
      "title": "Truth Neurons\n\n트루 뉴로노즈\n\n(注意: \"Truth Neurons\"는 이미 번역되어 있습니다. 이 경우, \"Truth Neurons\"를 그대로 사용하거나, \"트루 뉴로노즈\"로 번역할 수 있습니다.)",
      "submittedOnDailyBy": {
        "_id": "634cabd104491d9f7111eea3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cabd104491d9f7111eea3/JoqlugwfD1aGkd-wZTmP7.jpeg",
        "isPro": false,
        "fullname": "Haohang Li",
        "user": "Acatsama",
        "type": "user"
      },
      "summary": "논문에서, 언어 모델이 다양한 작업 흐름 내에서 놀라운 성공과 도입을 기록하는 데에도 불구하고, 이들은 때로는 불리한 답을 생성하는 경우가 있습니다. 우리는 이러한 모델들이 기계적으로真理를 덧셈하여 형성된 것으로 이해되어 있으며, 이에 따라 신뢰성과 안전성이 위험에 처해질 수 있습니다. 본 논문에서는,真理의 표현을 뉴런 레벨에서 인식하는 방법을 제안합니다. 우리는 언어 모델에真理를 뉴마스크 무 의존으로 인코딩하는 \"TRUTH NUERON\"이 존재한다는 것을 보여줍니다. 다양한 규모의 모델에서 실험을 수행하고, TRUTH NUERON의 존재를 증명하고, 뉴런 레벨에서의真理의 인코딩은 많은 언어 모델에서 공유되는 특성임을 확인했습니다. TRUTH NUERON의 층별로 분포 패턴은真理의 기하학적인 발견과 일치합니다. TruthfulQA 데이터 세트에서 찾은 TRUTH NUERON의 활성화를 선택적으로 억제함으로써, TruthfulQA 및 다른 벤치마크에서 성능이 저하되고,真理 구조는 특정 데이터 세트에 의존하지 않는 것을 보여주었습니다. 우리의 결과를 통해, 언어 모델의真理 구조에 대해 새로운 시각을 제공하고, 신뢰성과 신뢰성 향상에 대한 잠재적인 방향을 밝혀줍니다.",
      "upvotes": 2,
      "discussionId": "682d31de17608739046e11c9",
      "ai_keywords": [
        "truth neurons",
        "neuron level",
        "truthfulness mechanisms",
        "TruthfulQA dataset"
      ]
    },
    "publishedAt": "2025-05-17T20:47:21.000Z",
    "title": "Truth Neurons",
    "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12182.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634cabd104491d9f7111eea3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cabd104491d9f7111eea3/JoqlugwfD1aGkd-wZTmP7.jpeg",
      "fullname": "Haohang Li",
      "name": "Acatsama",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09569",
      "authors": [
        {
          "_id": "682563f807f74666ec373332",
          "name": "Linbo Liu",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373333",
          "user": {
            "_id": "636c32ae181c81c337f086b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg",
            "isPro": false,
            "fullname": "Xinle Sheila Liu",
            "user": "sliuxl",
            "type": "user"
          },
          "name": "Xinle Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-15T10:30:40.773Z",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373334",
          "name": "Qiang Zhou",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373335",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373336",
          "name": "Yihan Liu",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373337",
          "name": "Hoan Nguyen",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373338",
          "name": "Behrooz Omidvar-Tehrani",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373339",
          "name": "Xi Shen",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec37333a",
          "name": "Jun Huan",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec37333b",
          "name": "Omer Tripp",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec37333c",
          "name": "Anoop Deoras",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T17:11:23.000Z",
      "submittedOnDailyAt": "2025-05-21T04:22:33.758Z",
      "title": "MIGRATION-BENCH: Jaba 8에서부터 리포지토리 수준의 코드 이동 벤치마크",
      "submittedOnDailyBy": {
        "_id": "636c32ae181c81c337f086b9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg",
        "isPro": false,
        "fullname": "Xinle Sheila Liu",
        "user": "sliuxl",
        "type": "user"
      },
      "summary": "최근, 강력한 대언어 모델(LLMs)의 급속한 발전으로, 소프트웨어공학의 다양한 태스크들이 LLMs를 사용하여 해결이 가능해지고, 생산성과 scalability가 크게 향상되었습니다. 이러한 모델의 코딩능력을 평가하기 위해 여러 벤치마크 데이터세트가 개발되어 있지만, 주로 문제 해결이나 문제 해결 태스크에 초점을 맞추고 있습니다. 대조적으로, 우리는 새로운 코딩 벤치마크「MIGRATION-BENCH」를 소개하고, 이 벤치마크의 초점은 코드 미그레이션에 있습니다. 「MIGRATION-BENCH」은 Java 8에서 가장 최근의 장기 지원(LTS) 버전(Java 17, 21)으로의 미그레이션을 종합적으로 평가하기 위한 벤치마크입니다. 「MIGRATION-BENCH」에는 5,102개와 300개의 리포지토리에서 선택된 완전한 데이터세트와 그 부분집합이 포함되어 있습니다. 선택된 컬렉션은 복잡성과 어려움에 초점을 맞추고, 코드 미그레이션 연구에 도움이 되는 다양한 리소스로서 제공됩니다. 또한, 우리는 엄격하고 표준화된 평가 프레임워크를 제공하며, 이러한 어려운 태스크에 대한 LLMs의 평가를 촉진합니다. 또한, SD-Feedback를 제안하고, LLMs가 리포지토리 수준의 코드 미그레이션을 효과적으로 해결할 수 있음을 보여주었습니다. Claude-3.5-Sonnet-v2를 사용하였을 때의 선택 컬렉션에서, 최소와 최대의 미그레이션의 성공률은 각각 62.33%와 27.00%입니다. 벤치마크 데이터세트와 소스코드는 다음과 같은 URL에서 사용 가능합니다:\nhttps://huggingface.co/collections/AmazonScience\nhttps://github.com/amazon-science/self_debug",
      "upvotes": 2,
      "discussionId": "682563f907f74666ec373369",
      "projectPage": "https://github.com/amazon-science/SDFeedback",
      "githubRepo": "https://github.com/amazon-science/MigrationBench",
      "ai_keywords": [
        "large language models (LLMs)",
        "code migration",
        "MIGRATION-BENCH",
        "Java 8",
        "long-term support (LTS) versions (Java 17, 21)",
        "repositories",
        "SD-Feedback",
        "pass@1"
      ]
    },
    "publishedAt": "2025-05-14T13:11:23.000Z",
    "title": "MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8",
    "summary": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on problem-solving and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a\ndistinct focus: code migration. MIGRATION-BENCH aims to serve as a\ncomprehensive benchmark for migration from Java 8 to the latest long-term\nsupport (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset\nand its subset selected with 5,102 and 300 repositories respectively.\nSelected is a representative subset curated for complexity and difficulty,\noffering a versatile resource to support research in the field of code\nmigration. Additionally, we provide a comprehensive evaluation framework to\nfacilitate rigorous and standardized assessment of LLMs on this challenging\ntask. We further propose SD-Feedback and demonstrate that LLMs can effectively\ntackle repository-level code migration to Java 17. For the selected subset with\nClaude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate\n(pass@1) for minimal and maximal migration respectively. The benchmark dataset\nand source code are available at:\nhttps://huggingface.co/collections/AmazonScience and\nhttps://github.com/amazon-science/self_debug respectively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09569.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636c32ae181c81c337f086b9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg",
      "fullname": "Xinle Sheila Liu",
      "name": "sliuxl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13718",
      "authors": [
        {
          "_id": "682d6ccc26146f27d1ab3d83",
          "user": {
            "_id": "64cb922ec7f30fbf7b91a9a7",
            "avatarUrl": "/avatars/457eae5e56b9641ee5543146447d1755.svg",
            "isPro": false,
            "fullname": "Safal Shrestha",
            "user": "safal312",
            "type": "user"
          },
          "name": "Safal Shrestha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:07.374Z",
          "hidden": false
        },
        {
          "_id": "682d6ccc26146f27d1ab3d84",
          "name": "Minwu Kim",
          "hidden": false
        },
        {
          "_id": "682d6ccc26146f27d1ab3d85",
          "name": "Aadim Nepal",
          "hidden": false
        },
        {
          "_id": "682d6ccc26146f27d1ab3d86",
          "name": "Anubhav Shrestha",
          "hidden": false
        },
        {
          "_id": "682d6ccc26146f27d1ab3d87",
          "name": "Keith Ross",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T20:29:15.000Z",
      "submittedOnDailyAt": "2025-05-21T04:35:24.229Z",
      "title": "처음 도전 전의 준비: 리소스 제한 설정에서 일반적인 이유의 해방",
      "submittedOnDailyBy": {
        "_id": "64cb922ec7f30fbf7b91a9a7",
        "avatarUrl": "/avatars/457eae5e56b9641ee5543146447d1755.svg",
        "isPro": false,
        "fullname": "Safal Shrestha",
        "user": "safal312",
        "type": "user"
      },
      "summary": "設計有效的推論能力的LLM通常需要使用可証明的報酬を持つ強化学習（RLVR）または調整された長い思い出の鏈（Long Chain of Thoughts, CoT）를 이용한 디스タイルテーション進行訓練。這些方法需要詳細的訓練データ。因此，如果高品質的訓練データ不足，這將成為一大問題。我們提出了一種高效的2段階訓練策略，用於在限られたスティールバッジ中開發理由論的LLM。第一步是通過使用「Knights \\& Knaves (K\\&K) ロジックパズル」進行長CoT訓練，使模型「熱身」，以學習通用的理由論技能。第二步是在熱身後的模型上應用RLVR，使用限定的目標域的例子。我們的實驗表明，這種2段階方法具有以下優點：（i）即使在熱身階段，也能促進擴展性的理由論，並在MATH、HumanEval^{+}、MMLU-Pro等多種任務中提高性能。（ii）在相同的小數據集（100例以下）上進行RLVR訓練時，熱身後的模型比基礎模型更優秀；（iii）在RLVR訓練前進行熱身，可以保持特定域內訓練後的跨域泛化性能；（iv）在RLVR訓練中引入熱身可以改善訓練樣本的效率，提高精度和整體的訓練樣本效率。本文結果揭示了在數據不足的環境中，通過熱身建立強大的理由論LLM的可能性。",
      "upvotes": 1,
      "discussionId": "682d6ccd26146f27d1ab3dbb",
      "githubRepo": "https://github.com/safal312/warmup-before-you-train",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "Long Chain of Thoughts (CoT)",
        "Knights \\& Knaves (K\\&K) logic puzzles",
        "MATH",
        "HumanEval$^{+}$",
        "MMLU-Pro",
        "warmup phase",
        "generalized reasoning",
        "cross-domain generalizability",
        "sample efficiency",
        "robust reasoning LLMs"
      ]
    },
    "publishedAt": "2025-05-19T16:29:15.000Z",
    "title": "Warm Up Before You Train: Unlocking General Reasoning in\n  Resource-Constrained Settings",
    "summary": "Designing effective reasoning-capable LLMs typically requires training using\nReinforcement Learning with Verifiable Rewards (RLVR) or distillation with\ncarefully curated Long Chain of Thoughts (CoT), both of which depend heavily on\nextensive training data. This creates a major challenge when the amount of\nquality training data is scarce. We propose a sample-efficient, two-stage\ntraining strategy to develop reasoning LLMs under limited supervision. In the\nfirst stage, we \"warm up\" the model by distilling Long CoTs from a toy domain,\nnamely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning\nskills. In the second stage, we apply RLVR to the warmed-up model using a\nlimited set of target-domain examples. Our experiments demonstrate that this\ntwo-phase approach offers several benefits: (i) the warmup phase alone\nfacilitates generalized reasoning, leading to performance improvements across a\nrange of tasks, including MATH, HumanEval^{+}, and MMLU-Pro. (ii) When both\nthe base model and the warmed-up model are RLVR trained on the same small\ndataset (leq100 examples), the warmed-up model consistently outperforms the\nbase model; (iii) Warming up before RLVR training allows a model to maintain\ncross-domain generalizability even after training on a specific domain; (iv)\nIntroducing warmup in the pipeline improves not only accuracy but also overall\nsample efficiency during RLVR training. The results in this paper highlight the\npromise of warmup for building robust reasoning LLMs in data-scarce\nenvironments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13718.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb922ec7f30fbf7b91a9a7",
      "avatarUrl": "/avatars/457eae5e56b9641ee5543146447d1755.svg",
      "fullname": "Safal Shrestha",
      "name": "safal312",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13380",
      "authors": [
        {
          "_id": "682d3787265177367e119f04",
          "user": {
            "_id": "64c2bea2ada7df214276913b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
            "isPro": false,
            "fullname": "Nguyen Van Nam",
            "user": "DavidNguyen",
            "type": "user"
          },
          "name": "Nam V. Nguyen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:58.584Z",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f05",
          "name": "Huy Nguyen",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f06",
          "name": "Quang Pham",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f07",
          "name": "Van Nguyen",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f08",
          "name": "Savitha Ramasamy",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f09",
          "name": "Nhat Ho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T17:24:26.000Z",
      "submittedOnDailyAt": "2025-05-21T00:48:58.161Z",
      "title": "競争에 의한 통계적으로 보장된 엑시퍼트 조합의 훈련 (CompeteSMoE)",
      "submittedOnDailyBy": {
        "_id": "64c2bea2ada7df214276913b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
        "isPro": false,
        "fullname": "Nguyen Van Nam",
        "user": "DavidNguyen",
        "type": "user"
      },
      "summary": "Sparse mixture of experts (SMoE)는 네트워크의 깊이나 폭의 증가를 초과하는 모델 복잡성의 증가에 대한 매력적인 해결책을 제공합니다. 그러나 효과적인 SMoE의 훈련은 계산을 수행하는 Expert가 직접 Routing 프로세스에 기여하지 않는 점에서 어려움을 제기하고 있습니다. 본 논문에서는 네트워크에서 가장 강력한 Neural Response를 가진 Expert에 토큰을 Routing하는 새로운 구조인 \"Compete\"를 제안합니다. 이론적으로도, Compete 구조는 기존의 Softmax Routing보다 더 좋은 샘플 효율을 기대할 수 있습니다. 또한, CompeteSMoE라는 간단하고 효과적인 알고리즘을 개발하고, Router를 사용하여 Compete 정책을 학습하는 것이 가능하여, 저비용으로 강력한 성능을 누리는 것을 가능합니다. 이 알고리즘은 시각적인 Instruction Tuning과 언어 사전 학습 태스크에 대한 광범위한 실험 평가로, 최신의 SMoE 전략과 비교하여 효율성, 강건성, scalability를 보여주었습니다. 구현은 https://github.com/Fsoft-AIC/CompeteSMoE에 제공됩니다. 본 논문은 arXiv:2402.02526의 이전 연구의 개선판입니다.",
      "upvotes": 1,
      "discussionId": "682d3788265177367e119f71",
      "githubRepo": "https://github.com/Fsoft-AIC/CompeteSMoE",
      "ai_keywords": [
        "Sparse mixture of experts (SMoE)",
        "competition mechanism",
        "sample efficiency",
        "softmax routing",
        "CompeteSMoE",
        "neural response",
        "router",
        "competition policy",
        "visual instruction tuning",
        "language pre-training"
      ]
    },
    "publishedAt": "2025-05-19T13:24:26.000Z",
    "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\n  Competition",
    "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13380.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c2bea2ada7df214276913b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
      "fullname": "Nguyen Van Nam",
      "name": "DavidNguyen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13103",
      "authors": [
        {
          "_id": "682d7f7a176087390484a412",
          "name": "Han Zheng",
          "hidden": false
        },
        {
          "_id": "682d7f7a176087390484a413",
          "name": "Ilia Shumailov",
          "hidden": false
        },
        {
          "_id": "682d7f7a176087390484a414",
          "name": "Tianqi Fan",
          "hidden": false
        },
        {
          "_id": "682d7f7a176087390484a415",
          "name": "Aiden Hall",
          "hidden": false
        },
        {
          "_id": "682d7f7a176087390484a416",
          "name": "Mathias Payer",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T13:32:51.000Z",
      "submittedOnDailyAt": "2025-05-21T05:55:51.238Z",
      "title": "1 달러로 7,400개의 버그를 수정할 수 있습니다: 저가 코스트 시티 프로그래밍 수정 기술",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "バグ検出手法의 급격한 발전은 개발자가 합리적으로 수정할 수 있는 것보다 많은 취약점을 발견할 수 있게 되어, 효과적인 자동화 프로그램 수정(APR)의 방법의 필요성이 급격히 제기되어 있습니다. 그러나 현대의 バグ의 복잡성은 정확한 원인 분석이 어려워 불신赖성으로 나타납니다. 이러한 도전을 해결하기 위해, 우리는 폭발 사이트 수정을 제안하고, 수정의 과제를 단순히 하되, 게시판 사용 위험을 감당하기 위해 결정했습니다. 또한, 우리는 대언어 모델(LLMs)의 토큰 비용의 크게 줄이기와 두 가지의 효율성과 효과성을 유지하기 위한 템플릿 가이드된 패치 생성 접근 방식을 소개합니다.\n\n우리는 프로토 타입 시스템을 구현하고, 가장 先端的 APR 도구와 비교했습니다. 결과적으로, 최상급 성능의 아가이ント CodeRover-S와 함께 사용했을 때, WILLIAMT은 토큰 비용을 45.9% 줄였으며, ARVO(실상의 오픈 소스 소프트웨어 취약점 벤치마크)에서 バグ 수정율을 73.5%에 올렸으며, +29.6% 증가했습니다. 또한, WILLIAMT은 선진적인 LLMs의 액세스가 없는 경우에도 효과적으로 작동하는 것을 보여주었습니다: Mac M4 Mini에서 실행되는 로컬 모델에서도 합리적인 수정율을 달성했습니다. 이러한 발견은 WILLIAMT의 광범위한 적용 가능성과 scalability를 밝혀냅니다.",
      "upvotes": 1,
      "discussionId": "682d7f7a176087390484a448",
      "ai_keywords": [
        "crash-site repair",
        "template-guided patch generation",
        "Large Language Models (LLMs)",
        "WILLIAMT",
        "CodeRover-S",
        "ARVO",
        "ground-truth open source software vulnerabilities benchmark"
      ]
    },
    "publishedAt": "2025-05-19T09:32:51.000Z",
    "title": "Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair",
    "summary": "The rapid advancement of bug-finding techniques has led to the discovery of\nmore vulnerabilities than developers can reasonably fix, creating an urgent\nneed for effective Automated Program Repair (APR) methods. However, the\ncomplexity of modern bugs often makes precise root cause analysis difficult and\nunreliable. To address this challenge, we propose crash-site repair to simplify\nthe repair task while still mitigating the risk of exploitation. In addition,\nwe introduce a template-guided patch generation approach that significantly\nreduces the token cost of Large Language Models (LLMs) while maintaining both\nefficiency and effectiveness.\n  We implement our prototype system, WILLIAMT, and evaluate it against\nstate-of-the-art APR tools. Our results show that, when combined with the\ntop-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and\nincreases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open\nsource software vulnerabilities benchmark. Furthermore, we demonstrate that\nWILLIAMT can function effectively even without access to frontier LLMs: even a\nlocal model running on a Mac M4 Mini achieves a reasonable repair rate. These\nfindings highlight the broad applicability and scalability of WILLIAMT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13103.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13010",
      "authors": [
        {
          "_id": "682c46c70f622b7afc2e6f8f",
          "user": {
            "_id": "67d5c51817d572b960f6982a",
            "avatarUrl": "/avatars/2c22663b27326b2300444bcc84ebbdd7.svg",
            "isPro": false,
            "fullname": "Himel Ghosh",
            "user": "himel7",
            "type": "user"
          },
          "name": "Himel Ghosh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T09:42:53.022Z",
          "hidden": false
        },
        {
          "_id": "682c46c70f622b7afc2e6f90",
          "name": "Ahmed Mosharafa",
          "hidden": false
        },
        {
          "_id": "682c46c70f622b7afc2e6f91",
          "name": "Georg Groh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67d5c51817d572b960f6982a/YKwZeahX2EReZyYUdKvtE.png"
      ],
      "publishedAt": "2025-05-19T11:54:39.000Z",
      "submittedOnDailyAt": "2025-05-21T08:14:27.462Z",
      "title": "偏見인지 아닌지: 뉴스의 편견을 감지하는 편견 감지기",
      "submittedOnDailyBy": {
        "_id": "67d5c51817d572b960f6982a",
        "avatarUrl": "/avatars/2c22663b27326b2300444bcc84ebbdd7.svg",
        "isPro": false,
        "fullname": "Himel Ghosh",
        "user": "himel7",
        "type": "user"
      },
      "summary": "미디어 편향 검출은 공정적이고 균형적인 정보의 전파를 보장하기 위해 중요한 임무이지만, 편향의 주관성과 고품질의 레이블된 데이터의 부족으로 인해 어려운 문제를 겪습니다. 본 연구에서는 전문가가 레이블한 BABE 데이터셋을 사용하여 RoBERTa 기반 모델을 미세 조정하여 문서 수준의 편향 분류를 수행했습니다. McNemar 검정과 5x2 교차검증을 조합하여, 모델과 영역에 적응된 DA-RoBERTa 기반 라인 파일과 비교하여 통계적으로 유의한 성능 향상을 보여주었습니다. 또한, 注意기 기반 분석에서 모델은 정치적 언어에 과도하게 민감하지 않고, 맥락에 관련된 토큰에 의해 의미적으로 관심점을 배치합니다. 미디어 편향을 자세히 검증하기 위해, 모델과 이미 존재하는 편향 유형 분류기를 조합한 파이프라인을 제안했습니다. 이 방법은 큰 편향 코퍼스의 부족으로 인한 문서 수준 분석과 데이터 세트 크기의 제한에도 불구하고, 더 강건하고 설명 가능한 사회책임의 NLP 시스템의 구축에 기여합니다. 향후 방향성은 맥락에 관련된 모델링, 편향 중화, 고급 편향 유형 분류를 논의합니다.",
      "upvotes": 1,
      "discussionId": "682c46c80f622b7afc2e6fcf",
      "ai_keywords": [
        "RoBERTa-based model",
        "BABE dataset",
        "McNemar's test",
        "5x2 cross-validation paired t-test",
        "domain-adaptively pre-trained",
        "DA-RoBERTa baseline",
        "attention-based analysis",
        "politically charged terms",
        "contextually relevant tokens",
        "sentence-level analysis",
        "bias-type classifier",
        "context-aware modeling",
        "bias neutralization",
        "advanced bias type classification"
      ]
    },
    "publishedAt": "2025-05-19T07:54:39.000Z",
    "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector",
    "summary": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67d5c51817d572b960f6982a/YKwZeahX2EReZyYUdKvtE.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13010.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67d5c51817d572b960f6982a",
      "avatarUrl": "/avatars/2c22663b27326b2300444bcc84ebbdd7.svg",
      "fullname": "Himel Ghosh",
      "name": "himel7",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11966",
      "authors": [
        {
          "_id": "682d42808560f4baf596643b",
          "user": {
            "_id": "6608fa4f5baec84322ec85ea",
            "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
            "isPro": false,
            "fullname": "Zhong",
            "user": "Jianyuan1",
            "type": "user"
          },
          "name": "Jianyuan Zhong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:43.900Z",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf596643c",
          "name": "Zeju Li",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf596643d",
          "name": "Zhijian Xu",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf596643e",
          "name": "Xiangyu Wen",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf596643f",
          "name": "Kezhi Li",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf5966440",
          "name": "Qiang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-17T11:41:44.000Z",
      "submittedOnDailyAt": "2025-05-21T01:36:54.831Z",
      "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative Verifier\n\n解決-탐지-검증: 추론-타임 스케일링에 유연한 생성적 검증기 사용",
      "submittedOnDailyBy": {
        "_id": "6608fa4f5baec84322ec85ea",
        "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
        "isPro": false,
        "fullname": "Zhong",
        "user": "Jianyuan1",
        "type": "user"
      },
      "summary": "대 언어 모델(LLM)의 복잡한 태스크의 논리적 측면에서, 해결책의 정확성과 계산 효율성 사이의 반비례 관계가 본질적으로 존재합니다. 그 후의 검증 단계는 성능 개선을 목표로 하지만, 자기 자신의 어려운 반비례 관계를 계속 유지합니다: 복잡한 생성 보상 모델(GenRMs)은 테스트 시 LLM과 naive하게 통합되면 계산적으로 어려워집니다が, 간단하고 빠르면 신뢰도가 떨어질 가능성이 있습니다. 이러한 문제를 극복하기 위해, FlexiVe라는 새로운 생성 검증기를 소개합니다. FlexiVe는 효율적인 고속 사고와 미세한 느린 사고 사이에서 계산 자원을 유연하게 배분하는 '검증 뷰폰의 유연한 배분' 전략을 사용함으로써, 그 사이에 균형을 이루는 것입니다. 또한, Solve-Detect-Verify 파이프라인을 제안합니다. 이는 FlexiVe를 적절히 구성하고, 해결책의 완결점을 적극적으로 특정하고, 목표화된 검증을 수행하고, 집중된 해결 모델의 피드백을 제공하는 효율적인 추론 시 스케일링 프레임워크입니다. 실험은 FlexiVe는 ProcessBench에서 논리적 오류를 특정하는 정확도가 상위에 있습니다. 또한, 어려운 수학적인 논리적 벤치마크(AIME 2024, AIME 2025, CNMO)에서, 우리의 완전한 접근法是 논리적 정확성과 추론 효율성에서 기본보다 상위에 있습니다. 우리의 시스템은 테스트 시 LLM의 논리적 측면을 강화하기 위한 Scalable 및 효과적인 해결책을 제공합니다.",
      "upvotes": 1,
      "discussionId": "682d42808560f4baf5966480",
      "ai_keywords": [
        "Generative Reward Models (GenRMs)",
        "Flexible Allocation of Verification Budget",
        "Solve-Detect-Verify pipeline",
        "reasoning traces",
        "ProcessBench",
        "AIME 2024",
        "AIME 2025",
        "CNMO",
        "self-consistency"
      ]
    },
    "publishedAt": "2025-05-17T07:41:44.000Z",
    "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative\n  Verifier",
    "summary": "Large Language Model (LLM) reasoning for complex tasks inherently involves a\ntrade-off between solution accuracy and computational efficiency. The\nsubsequent step of verification, while intended to improve performance, further\ncomplicates this landscape by introducing its own challenging trade-off:\nsophisticated Generative Reward Models (GenRMs) can be computationally\nprohibitive if naively integrated with LLMs at test-time, while simpler, faster\nmethods may lack reliability. To overcome these challenges, we introduce\nFlexiVe, a novel generative verifier that flexibly balances computational\nresources between rapid, reliable fast thinking and meticulous slow thinking\nusing a Flexible Allocation of Verification Budget strategy. We further propose\nthe Solve-Detect-Verify pipeline, an efficient inference-time scaling framework\nthat intelligently integrates FlexiVe, proactively identifying solution\ncompletion points to trigger targeted verification and provide focused solver\nfeedback. Experiments show FlexiVe achieves superior accuracy in pinpointing\nerrors within reasoning traces on ProcessBench. Furthermore, on challenging\nmathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full\napproach outperforms baselines like self-consistency in reasoning accuracy and\ninference efficiency. Our system offers a scalable and effective solution to\nenhance LLM reasoning at test time.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11966.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6608fa4f5baec84322ec85ea",
      "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
      "fullname": "Zhong",
      "name": "Jianyuan1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14178",
      "authors": [
        {
          "_id": "682d388c57686b8c44ede70b",
          "user": {
            "_id": "656553d89bf6665f10e3a92d",
            "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
            "isPro": false,
            "fullname": "xiang wyatt zhang",
            "user": "Wyattz23",
            "type": "user"
          },
          "name": "Xiang Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T02:21:00.902Z",
          "hidden": false
        },
        {
          "_id": "682d388c57686b8c44ede70c",
          "name": "Juntai Cao",
          "hidden": false
        },
        {
          "_id": "682d388c57686b8c44ede70d",
          "name": "Jiaqi Wei",
          "hidden": false
        },
        {
          "_id": "682d388c57686b8c44ede70e",
          "name": "Yiwei Xu",
          "hidden": false
        },
        {
          "_id": "682d388c57686b8c44ede70f",
          "user": {
            "_id": "6466d463060756d2854ab3e1",
            "avatarUrl": "/avatars/4401387180c16472a6823f78aaa86d54.svg",
            "isPro": false,
            "fullname": "Chenyu You",
            "user": "Charlesyooo",
            "type": "user"
          },
          "name": "Chenyu You",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T02:30:12.849Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T10:32:30.000Z",
      "submittedOnDailyAt": "2025-05-21T00:51:16.514Z",
      "title": "LLM의 토큰화 제한: 기호적 및 산술적 이유의 한계에 대한 연구",
      "submittedOnDailyBy": {
        "_id": "656553d89bf6665f10e3a92d",
        "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
        "isPro": false,
        "fullname": "xiang wyatt zhang",
        "user": "Wyattz23",
        "type": "user"
      },
      "summary": "토크나이션은 언어 모델에서 가장 중요하며, 잘 인식되지 않는 계산의 가장 초기의 계층입니다. Chain-of-Thought (CoT) 프로닝은 트렌드 모델이 외부적으로 중간 단계를 표현하여 재귀적 계산을 근사할 수 있게 해주지만, 우리는 이러한 추론의 성공은 토크나이즈된 입력의 구조에 근본적으로 제한되어 있다는 것을 보여주고 있습니다. 본 논문에서는 토크나이션 스키ーム, 특히 바이트 페어 인코딩 (BPE) 등 서브워드 기반의 방법들이, 원자의 논리 단위를 결합하거나 은닉하거나 심볼적 계산을 방해하는 것을 이론적으로 및 실험적으로 조사하고 있습니다. 토크나이션에 대한 개념을 소개하고, 어휘의 거대화가 논리적 대응을 방해하고 심볼적 처리의 일반화에 방해할 것으로 나타낼 수 있는 것을 형식적으로 정의합니다. 쉼표 계산과 심볼적 태스크에 대해 체계적으로 평가하고, 토큰 구조가 계산 성능을 크게 영향을 미치고 CoT에서도 실패할 것을 보여줍니다. 반면, 원자로 대응된 포맷은 강력한 일반화에 가능하게 하고, 작은 모델 (예: GPT-4o-mini)이 큰 시스템 (예: o1)을 초월할 것을 보여줍니다. 우리가 발견한 것은, LLM의 심볼적 논리 능력은 아키텍처에 제한되지 않고, 심층적으로 토큰 수준의 표현에 의존하는 것을 보여줍니다.",
      "upvotes": 0,
      "discussionId": "682d388c57686b8c44ede753",
      "ai_keywords": [
        "Chain-of-Thought (CoT) prompting",
        "transformer models",
        "recursive computation",
        "intermediate steps",
        "tokenization schemes",
        "subword-based methods",
        "byte-pair encoding (BPE)",
        "symbolic computation",
        "Token Awareness",
        "atomic reasoning units",
        "logical alignment",
        "structured reasoning",
        "arithmetic tasks",
        "symbolic tasks",
        "reasoning performance",
        "atomically-aligned formats",
        "structured reasoning",
        "low-level representations",
        "symbolic reasoning ability",
        "LLMs",
        "token-level representations"
      ]
    },
    "publishedAt": "2025-05-20T06:32:30.000Z",
    "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\n  Reasoning Limits",
    "summary": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14178.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656553d89bf6665f10e3a92d",
      "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
      "fullname": "xiang wyatt zhang",
      "name": "Wyattz23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13138",
      "authors": [
        {
          "_id": "682da62a975206d1caa8c9d9",
          "name": "Emile van Krieken",
          "hidden": false
        },
        {
          "_id": "682da62a975206d1caa8c9da",
          "name": "Pasquale Minervini",
          "hidden": false
        },
        {
          "_id": "682da62a975206d1caa8c9db",
          "name": "Edoardo Ponti",
          "hidden": false
        },
        {
          "_id": "682da62a975206d1caa8c9dc",
          "name": "Antonio Vergari",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62cd4ae83e5ba89c40f15156/6TK9qXUQWm6Yk3bQW_0ht.gif"
      ],
      "publishedAt": "2025-05-19T14:07:47.000Z",
      "submittedOnDailyAt": "2025-05-21T08:42:10.409Z",
      "title": "뉴로신보리크 디피유션 모듈",
      "submittedOnDailyBy": {
        "_id": "62cd4ae83e5ba89c40f15156",
        "avatarUrl": "/avatars/f70aca85f8836edaedcee324a18140b5.svg",
        "isPro": false,
        "fullname": "Emile van Krieken",
        "user": "HEmile",
        "type": "user"
      },
      "summary": "뉴로신보럴(NeSy) 예측기는 신경관찰과 기호언어의 이유를 통합하여 시각적 이유 등 다양한 태스크를 해결하는데 사용됩니다. 그러나 표준의 NeSy 예측기는 추출된 기호 간의 조건 독립성을 가정하고, 상호작용 및 불확실성의 모델링 능력을 제한하고, 과도한 자신감의 예측과 분포 외의 일반화에 악화를 초래합니다. 이러한 제한을 극복하기 위해, 우리는 기호 간의 의존성을 모델링하기 위해 새로운 NeSy 예측기 클래스인 뉴로신보럴 확산 모델(NeSyDMs)을 도입합니다. 우리의 접근법은 NeSy 예측기에서 독립성 가정을 확산 프로세스의 각 단계에서 재활용함으로써, 기호 간의 의존성과 불확실성의 평가를 가능하게 하며, scalable한 학습을 가능하게 합니다. 합성 데이터와 실세계의 벤치마크를 통해, 고차원 시각적 계획 및 규칙 기반의 자율주행을 포함한 다양한 분야에서 NeSyDMs는 NeSy 예측기에서 가장 높은 정확도를 달성하고, 강력한 보완을 나타냅니다.",
      "upvotes": 0,
      "discussionId": "682da62a975206d1caa8ca06",
      "githubRepo": "https://github.com/HEmile/neurosymbolic-diffusion"
    },
    "publishedAt": "2025-05-19T10:07:47.000Z",
    "title": "Neurosymbolic Diffusion Models",
    "summary": "Neurosymbolic (NeSy) predictors combine neural perception with symbolic\nreasoning to solve tasks like visual reasoning. However, standard NeSy\npredictors assume conditional independence between the symbols they extract,\nthus limiting their ability to model interactions and uncertainty - often\nleading to overconfident predictions and poor out-of-distribution\ngeneralisation. To overcome the limitations of the independence assumption, we\nintroduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy\npredictors that use discrete diffusion to model dependencies between symbols.\nOur approach reuses the independence assumption from NeSy predictors at each\nstep of the diffusion process, enabling scalable learning while capturing\nsymbol dependencies and uncertainty quantification. Across both synthetic and\nreal-world benchmarks - including high-dimensional visual path planning and\nrule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among\nNeSy predictors and demonstrate strong calibration.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62cd4ae83e5ba89c40f15156/6TK9qXUQWm6Yk3bQW_0ht.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13138.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62cd4ae83e5ba89c40f15156",
      "avatarUrl": "/avatars/f70aca85f8836edaedcee324a18140b5.svg",
      "fullname": "Emile van Krieken",
      "name": "HEmile",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.12306",
      "authors": [
        {
          "_id": "682d2f9f3b5f51f42185dd7d",
          "name": "Yuwei Zhang",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd7e",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd7f",
          "name": "Shangbin Feng",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd80",
          "name": "Yifan Zhu",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd81",
          "name": "Letian Peng",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd82",
          "name": "Jayanth Srinivasa",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd83",
          "name": "Gaowen Liu",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd84",
          "name": "Jingbo Shang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T08:39:05.000Z",
      "submittedOnDailyAt": "2025-05-21T00:13:17.201Z",
      "title": "양방향 LM은 지식의 기억자입니까? 실세계의 지식 注入의 벤치마크",
      "submittedOnDailyBy": {
        "_id": "64323dd503d81fa4d26deaf9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png",
        "isPro": false,
        "fullname": "Letian Peng",
        "user": "KomeijiForce",
        "type": "user"
      },
      "summary": "대규모 언어 모델(LLMs)에서 뚜렷한 진보가 있습니다 however, 그 지식의 기억 능력은 표준화가 되어 있지 않거나, 고품질의 테스트 환경이 부족하여 조사가 부족합니다. 본 논문에서는 시간으로 진화하며, 인간이干预하지 않는 새로운, 현실적인, 대규모 지식 注入 벤치마크를 소개합니다. 특히, Wikipedia의 \"Did You Know...\"를 활용하여 최근 추가된 및 인간이 쓴 사실에 기반한 WikiDYK를 제안합니다. 이러한 사실은 검증 가능성과 명확성 등 기준에 따라 Wikipedia의 전문가 편집자가 신중하게 선택됩니다. 각 사실은 간단한 clozePrompt부터 복잡한 다단계クエスト 등 다양한 태스크 포맷을 넘어가는 여러 질문·답변 쌍으로 변환됩니다. WikiDYK는 12,290개의 사실과 77,180개의 질문을 포함하며, Wikipedia 편집자로부터의 미래의 업데이트와 함께 확장이 용이하습니다. 장기적인 전처리를 통해 확장된 실험에 의해, 놀라울 수 있는 피드백이 얻었습니다: 현재 사용广泛的因果语言模型(CLMs)은 대칭 언어 모델(BiLMs)과 비교하여 지식의 기억 능력이 크게 약하고, 신뢰성 오류율은 23% 낮아졌습니다. 현재 BiLMs의 규모가 작은 것을 보완하기 위해, BiLMs의 앙상블을 외부 지식베이스에 활용하여 LLMs와 통합하는 모둡적인 협력 프레임워크를 제안합니다. 실험은 이 프레임워크가 신뢰성 정확도를 최대 29.1% 정도 개선하는 것을 보여주었습니다.",
      "upvotes": 0,
      "discussionId": "682d2fa03b5f51f42185ddb4",
      "ai_keywords": [
        "large language models (LLMs)",
        "knowledge memorization capabilities",
        "WikiDYK",
        "\"Did You Know...\" entries",
        "question-answer pairs",
        "cloze prompts",
        "multi-hop questions",
        "continued pre-training",
        "Causal Language Models (CLMs)",
        "Bidirectional Language Models (BiLMs)",
        "modular collaborative framework",
        "ensembles of BiLMs",
        "external knowledge repositories",
        "reliability accuracy"
      ]
    },
    "publishedAt": "2025-05-18T04:39:05.000Z",
    "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for\n  Real-world Knowledge Injection",
    "summary": "Despite significant advances in large language models (LLMs), their knowledge\nmemorization capabilities remain underexplored, due to the lack of standardized\nand high-quality test ground. In this paper, we introduce a novel, real-world\nand large-scale knowledge injection benchmark that evolves continuously over\ntime without requiring human intervention. Specifically, we propose WikiDYK,\nwhich leverages recently-added and human-written facts from Wikipedia's \"Did\nYou Know...\" entries. These entries are carefully selected by expert Wikipedia\neditors based on criteria such as verifiability and clarity. Each entry is\nconverted into multiple question-answer pairs spanning diverse task formats\nfrom easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290\nfacts and 77,180 questions, which is also seamlessly extensible with future\nupdates from Wikipedia editors. Extensive experiments using continued\npre-training reveal a surprising insight: despite their prevalence in modern\nLLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge\nmemorization capabilities compared to Bidirectional Language Models (BiLMs),\nexhibiting a 23% lower accuracy in terms of reliability. To compensate for the\nsmaller scales of current BiLMs, we introduce a modular collaborative framework\nutilizing ensembles of BiLMs as external knowledge repositories to integrate\nwith LLMs. Experiment shows that our framework further improves the reliability\naccuracy by up to 29.1%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12306.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64323dd503d81fa4d26deaf9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png",
      "fullname": "Letian Peng",
      "name": "KomeijiForce",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10588",
      "authors": [
        {
          "_id": "682d2dd917608739046ce410",
          "name": "Manisha Mehta",
          "hidden": false
        },
        {
          "_id": "682d2dd917608739046ce411",
          "name": "Fausto Giunchiglia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T16:46:11.000Z",
      "submittedOnDailyAt": "2025-05-21T00:06:34.563Z",
      "title": "일반 알파 디지털 언어의 이해: 콘텐츠 모달리티의 안전성에 대한 LLM 시스템 평가",
      "submittedOnDailyBy": {
        "_id": "604eb19e3050a33ebb17ef58",
        "avatarUrl": "/avatars/23650002ba3befee83060fe978a251c8.svg",
        "isPro": false,
        "fullname": "Virendra Mehta",
        "user": "Veeru",
        "type": "user"
      },
      "summary": "이 연구는 AI 시스템이 Generation Alpha(Gen Alpha, 2010-2024년 출생세대)의 디지털 언어를 어떻게 해석하는지에 대한 고유한 평가를 제공합니다. Gen Alpha는 AI와 함께 성장한 첫 번째 세대이며, 만족스러운 디지털 참여와 발전된 통신 방식과 현재의 보안 도구 사이에서 확장된 비대칭성을 통해 새로운 온라인 위험을 감안합니다. 그들의 특징적인 언어는 게임, 메므, AI 주도의 추세로 형성되어, 두 사람의 모델러와 자동화 시스템으로부터 유해한 상호작용을 숨겨주며, 이 연구에서는 4개의 선도적인 AI 모델(GPT-4, Claude, Gemini, Llama 3)이 Gen Alpha의 디코즈에서 숨겨진 harassment와 조작을 감지하는 능력을 평가합니다. 게임 플랫폼, 소셜 미디어, 비디오 콘텐츠에서 100건의 최신 표현을 사용하여, 이 연구는 온라인 보안에 직접적인 영향을 미치는 중요한 이해의 실패를 밝혀냅니다. 이 연구는 다음과 같은 기여를 제공합니다: (1) Gen Alpha의 표현을捉える 처음의 데이터셋; (2) 청소년 보호를 위한 AI 모델링 시스템의 개선을 위한 프레임워크; (3) AI 시스템, 사람 모델러, 그리고 부모로부터의 직접적인 입력을 포함하는 다각적인 평가; (4) 언어의 거리가 청소년의 취약성을 증가시키는 것을 분석합니다. 이러한 발견은 청소년의 통신에 맞는 보안 시스템의 재설계의 긴급한 필요성을 강조합니다. 특히, Gen Alpha가 부모가 그들의 디지털 세계를 이해하지 못하여 도움을 요청하는 것을 위반하는 것을 기록하고 있습니다. 이 연구는 Gen Alpha의 연구자의 통찰과 체계적인 학문적 분석을 결합하여, 중요한 디지털 보안의 도전에 대응하는 것을 목표로 합니다.",
      "upvotes": 0,
      "discussionId": "682d2dd917608739046ce440"
    },
    "publishedAt": "2025-05-14T12:46:11.000Z",
    "title": "Understanding Gen Alpha Digital Language: Evaluation of LLM Safety\n  Systems for Content Moderation",
    "summary": "This research offers a unique evaluation of how AI systems interpret the\ndigital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first\ncohort raised alongside AI, Gen Alpha faces new forms of online risk due to\nimmersive digital engagement and a growing mismatch between their evolving\ncommunication and existing safety tools. Their distinct language, shaped by\ngaming, memes, and AI-driven trends, often conceals harmful interactions from\nboth human moderators and automated systems. We assess four leading AI models\n(GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked\nharassment and manipulation within Gen Alpha discourse. Using a dataset of 100\nrecent expressions from gaming platforms, social media, and video content, the\nstudy reveals critical comprehension failures with direct implications for\nonline safety. This work contributes: (1) a first-of-its-kind dataset capturing\nGen Alpha expressions; (2) a framework to improve AI moderation systems for\nyouth protection; (3) a multi-perspective evaluation including AI systems,\nhuman moderators, and parents, with direct input from Gen Alpha co-researchers;\nand (4) an analysis of how linguistic divergence increases youth vulnerability.\nFindings highlight the urgent need to redesign safety systems attuned to youth\ncommunication, especially given Gen Alpha reluctance to seek help when adults\nfail to understand their digital world. This study combines the insight of a\nGen Alpha researcher with systematic academic analysis to address critical\ndigital safety challenges.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10588.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "604eb19e3050a33ebb17ef58",
      "avatarUrl": "/avatars/23650002ba3befee83060fe978a251c8.svg",
      "fullname": "Virendra Mehta",
      "name": "Veeru",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]