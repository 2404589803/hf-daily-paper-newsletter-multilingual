[
  {
    "paper": {
      "id": "2506.04308",
      "authors": [
        {
          "_id": "68424dc48d0422fce0273e99",
          "user": {
            "_id": "63f08dc79cf89c9ed1bb89cd",
            "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
            "isPro": false,
            "fullname": "Zhoues",
            "user": "Zhoues",
            "type": "user"
          },
          "name": "Enshen Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:21.339Z",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9a",
          "name": "Jingkun An",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9b",
          "name": "Cheng Chi",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9c",
          "name": "Yi Han",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9d",
          "name": "Shanyu Rong",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9e",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9f",
          "name": "Pengwei Wang",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273ea0",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273ea1",
          "name": "Tiejun Huang",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273ea2",
          "name": "Lu Sheng",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273ea3",
          "name": "Shanghang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:59:27.000Z",
      "submittedOnDailyAt": "2025-06-06T00:41:30.786Z",
      "title": "RoboRefer: 공간참조를 이유로 하는 시각언어 모델의 반대편으로\n\n(Note: The original text \"RoboRefer\" is a proper noun and is kept in its original form in Korean as well, with the rest of the text translated into Korean.)",
      "submittedOnDailyBy": {
        "_id": "63f08dc79cf89c9ed1bb89cd",
        "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
        "isPro": false,
        "fullname": "Zhoues",
        "user": "Zhoues",
        "type": "user"
      },
      "summary": "공간참조는 구체적인 로봇이 3차원 물리적 세계와 상호작용하기 위해 기본적인 능력입니다. 그러나 최근의 접근법은 강력한 사전 학습된 비전 언어 모델(VLMs)을 사용하더라도, 복잡한 3차원 스케인에 정확한 이해하고, 지시된 장소를 동적으로 논리적으로 생각할 수 없습니다. 따라서, 우리는 SFT(Subjekt Feedback Tuning)를 통해 분리된 것을 전문적으로 깊은 2Encoder를 통계적으로 결합하여, 정밀한 공간 이해를 구현할 수 있는 3D 적응 비전 언어 모델(VLM)을 제안하고 있습니다. 또한, RoboRefer는 RFT(Reinforcement Learning Tuning)을 통해, 메트릭에 관련된 프로세스 보상 함수를 사용하여 확장된 다스텝 공간 논리론을 진행합니다. SFT와 RFT의 훈련을 지원하기 위해, 우리는 20M QA 페어(초반의 두 배)를 가진 큰 규모의 데이터셋 RefSpatial을 도입했습니다. 이 데이터셋은 31가지의 공간 관계(초반의 15가지)를 커버하고, 복잡한 논리론 프로세스(5스텝까지)를 지원합니다. 또한, 우리는 RefSpatial-Bench라는 어려운 벤치마크를 도입하고, 다스텝 논리론에서 공간참조의 평가의 부족점을 메꿔줍니다. 실험은 SFT 훈련된 RoboRefer가 상태의 가장 선진적인 공간 이해를 구현하고, 평균 성공률이 89.6%였습니다. RFT 훈련된 RoboRefer는 평균 정확도에서 17.4% 이상 젊은-2.5-Pro를 초과하고, 모든 베이스라인을 크게 초과했습니다. 특히, RoboRefer는 다양한 로봇(예를 들어 UR5, G1 Humanoid)에서 장기적인, 동적인 태스크를 수행하기 위해, 다른 제어 정책과 통합할 수 있습니다.",
      "upvotes": 30,
      "discussionId": "68424dc88d0422fce0273fb5",
      "githubRepo": "https://github.com/Zhoues/RoboRefer",
      "ai_summary": "RoboRefer, a 3D-aware vision language model, enhances spatial understanding and multi-step reasoning in embodied robots through supervised and reinforcement fine-tuning, using the RefSpatial dataset and RefSpatial-Bench benchmark.",
      "ai_keywords": [
        "3D-aware VLM",
        "disentangled depth encoder",
        "supervised fine-tuning (SFT)",
        "reinforcement fine-tuning (RFT)",
        "metric-sensitive reward functions",
        "RefSpatial",
        "RefSpatial-Bench",
        "spatial referring tasks",
        "multi-step reasoning",
        "state-of-the-art spatial understanding",
        "long-horizon",
        "dynamic tasks"
      ]
    },
    "publishedAt": "2025-06-04T13:59:27.000Z",
    "title": "RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language\n  Models for Robotics",
    "summary": "Spatial referring is a fundamental capability of embodied robots to interact\nwith the 3D physical world. However, even with the powerful pretrained vision\nlanguage models (VLMs), recent approaches are still not qualified to accurately\nunderstand the complex 3D scenes and dynamically reason about the\ninstruction-indicated locations for interaction. To this end, we propose\nRoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding\nby integrating a disentangled but dedicated depth encoder via supervised\nfine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial\nreasoning via reinforcement fine-tuning (RFT), with metric-sensitive process\nreward functions tailored for spatial referring tasks. To support SFT and RFT\ntraining, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x\nprior), covering 31 spatial relations (vs. 15 prior) and supporting complex\nreasoning processes (up to 5 steps). In addition, we introduce\nRefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial\nreferring with multi-step reasoning. Experiments show that SFT-trained\nRoboRefer achieves state-of-the-art spatial understanding, with an average\nsuccess rate of 89.6%. RFT-trained RoboRefer further outperforms all other\nbaselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average\naccuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various\ncontrol policies to execute long-horizon, dynamic tasks across diverse robots\n(e,g., UR5, G1 humanoid) in cluttered real-world scenes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04308.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63f08dc79cf89c9ed1bb89cd",
      "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
      "fullname": "Zhoues",
      "name": "Zhoues",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05301",
      "authors": [
        {
          "_id": "68428f675738dda052f724d3",
          "name": "Jianyi Wang",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d4",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d5",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d6",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d7",
          "name": "Meng Wei",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d8",
          "name": "Zongsheng Yue",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d9",
          "name": "Shangchen Zhou",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724da",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724db",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724dc",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724dd",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724de",
          "name": "Chen Change Loy",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724df",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63043db17373aacccd89f49d/OfOznZMmLTV2VmgcLA0fG.mp4"
      ],
      "publishedAt": "2025-06-05T17:51:05.000Z",
      "submittedOnDailyAt": "2025-06-06T06:38:37.104Z",
      "title": "SeedVR2: 1단계 비디오 리피메이션에 의한 적대적인 분화 후 학습",
      "submittedOnDailyBy": {
        "_id": "63043db17373aacccd89f49d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63043db17373aacccd89f49d/jzP_fPCFXeYJvAD8uA_N7.jpeg",
        "isPro": false,
        "fullname": "JIANYI WANG",
        "user": "Iceclear",
        "type": "user"
      },
      "summary": "최근의 분기 기반의 비디오 리스트레션(VR)의 발전은 시각적 품질의 큰 향상을 보여주지만, 추론 시의 계산 비용이 높은 문제를 제기하고 있습니다. 반면에, 많은 스타일 기반의 접근 방식은 한 단계 이미지 리스트레션의 가능성을 보여주고 있지만, 현재의 접근 방식을 VR에 확장하는 것은 어려워하며, 특히 현실적인 고해상도 비디오 처리에 있어서는 아직 조사가 부족한 상태입니다. 본 논문에서는, 대결적인 VR 훈련을 수행하는 한 단계 분기 기반의 VR 모델을 제안하고, 이를 SeedVR2라고 부르도록 합니다. 고해상도의 VR를 한 단계로 처리하기 위해, 모델 구조와 훈련 순서에도 많은 개선을 도입했습니다. 특히, 적응적인 윈도우 어텐션 구조를 제안하고, 출력 해상도에 맞게 윈도우 크기를 동적으로 조정하여, 고해상도의 VR에서 미리 정의된 윈도우 크기를 사용하는 윈도우 어텐션으로 인해 미치지 못하는 윈도우의 불확실성을 피할 수 있습니다. 대결적인 훈련의 안정化和 향상을 위해, 손실 함수의 효과를 확인하고, 특히 훈련 효율을 크게 저해하지 않는 한의 제안된 특징 매칭 손실의 효과를 확인했습니다. 확장된 실험은, SeedVR2는 현재의 VR 접근 방식을 비교하여 상대적으로 나은 또는 더 좋은 성능을 구현할 수 있음을 보여줍니다.",
      "upvotes": 28,
      "discussionId": "68428f6a5738dda052f72569",
      "projectPage": "https://iceclear.github.io/projects/seedvr2/",
      "githubRepo": "https://github.com/IceClear/SeedVR2",
      "ai_summary": "SeedVR2 is a one-step diffusion-based video restoration model that uses adaptive window attention and feature matching loss to achieve high visual quality with reduced computational cost compared to existing methods.",
      "ai_keywords": [
        "diffusion-based video restoration",
        "VR",
        "adversarial VR training",
        "adaptive window attention",
        "feature matching loss"
      ]
    },
    "publishedAt": "2025-06-05T13:51:05.000Z",
    "title": "SeedVR2: One-Step Video Restoration via Diffusion Adversarial\n  Post-Training",
    "summary": "Recent advances in diffusion-based video restoration (VR) demonstrate\nsignificant improvement in visual quality, yet yield a prohibitive\ncomputational cost during inference. While several distillation-based\napproaches have exhibited the potential of one-step image restoration,\nextending existing approaches to VR remains challenging and underexplored,\nparticularly when dealing with high-resolution video in real-world settings. In\nthis work, we propose a one-step diffusion-based VR model, termed as SeedVR2,\nwhich performs adversarial VR training against real data. To handle the\nchallenging high-resolution VR within a single step, we introduce several\nenhancements to both model architecture and training procedures. Specifically,\nan adaptive window attention mechanism is proposed, where the window size is\ndynamically adjusted to fit the output resolutions, avoiding window\ninconsistency observed under high-resolution VR using window attention with a\npredefined window size. To stabilize and improve the adversarial post-training\ntowards VR, we further verify the effectiveness of a series of losses,\nincluding a proposed feature matching loss without significantly sacrificing\ntraining efficiency. Extensive experiments show that SeedVR2 can achieve\ncomparable or even better performance compared with existing VR approaches in a\nsingle step.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63043db17373aacccd89f49d/OfOznZMmLTV2VmgcLA0fG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05301.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63043db17373aacccd89f49d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63043db17373aacccd89f49d/jzP_fPCFXeYJvAD8uA_N7.jpeg",
      "fullname": "JIANYI WANG",
      "name": "Iceclear",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05284",
      "authors": [
        {
          "_id": "6842929c46106f29d78635ad",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635ae",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635af",
          "name": "Ryan Po",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635b0",
          "name": "Yinghao Xu",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635b1",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635b2",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635b3",
          "name": "Gordon Wetzstein",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b4eec4faa3181a5eab9c46/58qSYbX-_UzJE5ubWpM9W.mp4"
      ],
      "publishedAt": "2025-06-05T17:42:34.000Z",
      "submittedOnDailyAt": "2025-06-06T05:48:31.006Z",
      "title": "Video World Models with Long-term Spatial Memory",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "새로운 세계 모델은 카메라의 이동이나 텍스트 플롯 등 제어 신호에 대해 순차적으로 비디오 프레임을 생성합니다. 시간적 컨텍스트 윈도우 크기가 제한되어 있기 때문에, 이러한 모델은 재방문 시 시장의 일관성을 유지하기 위해 많은 노력을 기울이고, 이전에 생성한 환경이 쉽게 잊히는 경우가 많습니다. 인간의 기억 구조를 모델링한 것을 참고하여, 장기간의 일관성을 높일 수 있도록 비디오 모델의 장기간의 공간적 기억을 실현하기 위한 새로운 프레임워크를 제안합니다. 이 프레임워크는 장기간의 공간적 기억에서 정보를 저장하고 꺼내는 구조를 포함하고, 3D 메모리 구조를 명확히 하여 세계 모델을 훈련 및 평가하기 위한 사용자 정의 데이터 세트를 선택합니다. 평가 결과는 관련한 기준과 비교하여, 질의 개선, 일관성, 컨텍스트의 길이가 개선되어 장기간의 일관성의 세계 생성로의 길을 열립니다.",
      "upvotes": 24,
      "discussionId": "684292a046106f29d7863732",
      "ai_summary": "A new framework enhances video world models' long-term consistency by integrating a geometry-grounded long-term spatial memory mechanism.",
      "ai_keywords": [
        "world models",
        "autoregressive generation",
        "video frames",
        "control signals",
        "temporal context window",
        "scene consistency",
        "long-term spatial memory",
        "custom datasets",
        "3D memory mechanisms"
      ]
    },
    "publishedAt": "2025-06-05T13:42:34.000Z",
    "title": "Video World Models with Long-term Spatial Memory",
    "summary": "Emerging world models autoregressively generate video frames in response to\nactions, such as camera movements and text prompts, among other control\nsignals. Due to limited temporal context window sizes, these models often\nstruggle to maintain scene consistency during revisits, leading to severe\nforgetting of previously generated environments. Inspired by the mechanisms of\nhuman memory, we introduce a novel framework to enhancing long-term consistency\nof video world models through a geometry-grounded long-term spatial memory. Our\nframework includes mechanisms to store and retrieve information from the\nlong-term spatial memory and we curate custom datasets to train and evaluate\nworld models with explicitly stored 3D memory mechanisms. Our evaluations show\nimproved quality, consistency, and context length compared to relevant\nbaselines, paving the way towards long-term consistent world generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b4eec4faa3181a5eab9c46/58qSYbX-_UzJE5ubWpM9W.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05284.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05010",
      "authors": [
        {
          "_id": "6842632d542c9011f1bebf46",
          "user": {
            "_id": "639c379cdb7c5f35004066cb",
            "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
            "isPro": false,
            "fullname": "Zhenran Xu",
            "user": "imryanxu",
            "type": "user"
          },
          "name": "Zhenran Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:31.325Z",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf47",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf48",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf49",
          "name": "Qingli Hu",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4a",
          "name": "Zijiao Wu",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4b",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4c",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4d",
          "name": "Kaifu Zhang",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4e",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4f",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/wG9VdZ8vQGyG76QYWS4NS.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/VrBvS8a9t6I462AYXI2uc.png"
      ],
      "publishedAt": "2025-06-05T13:20:50.000Z",
      "submittedOnDailyAt": "2025-06-06T02:21:39.406Z",
      "title": "ComfyUI-Copilot: 자동화 작업 흐름의 지능형 보조자의 개발",
      "submittedOnDailyBy": {
        "_id": "639c379cdb7c5f35004066cb",
        "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
        "isPro": false,
        "fullname": "Zhenran Xu",
        "user": "imryanxu",
        "type": "user"
      },
      "summary": "ComfyUI-Copilot를 소개합니다. 이것은 AI를 기반으로 하는 아트 창작의 오픈 소스 플랫폼인 ComfyUI를 강화하기 위해, 대규모 언어 모델을 기능으로 가지는 플러그인이 됩니다. ComfyUI는 유연한 프레임워크와 간단한 UI를 특징으로 하지만, 초보자에게는 설명서 부족, 모델의 잘못된 설정, 작업 흐름 설계의 복잡성 등 문제점이 있습니다. ComfyUI-Copilot는 이러한 문제를 해결하기 위해, 지능적인 노드와 모델 추천, 작업 흐름의 자동화 작업 흐름 구축을 제공하여 있습니다. 핵심은 작업의 위탁을 수행하는 중심적인 보조 에이전트와 다양한 사용에 대응하는 전문적인 작업자 에이전트를 구성한 휴리스틱 다 에이전트 프레임워크입니다. 이러한 에이전트는 우리의 사용자 정의된 ComfyUI 지식 베이스를 기반으로, 더블 체크와 데플로yment를 스트리밍화하여 있습니다. ComfyUI-Copilot의 효율성은 오프라인定量 평가와 온라인 사용자의 피드백을 통해 증명되어 있습니다. 이로 인해, 노드의 정확한 추천과 작업 흐름의 개발을 가속화합니다. 또한, 사용 예시는 ComfyUI-Copilot가 초보자의 입문 장벽을 낮추고, 경험자의 작업 흐름의 효율화를 촉진하는 것을 보여줍니다. ComfyUI-Copilot의 설치 패키지와 데모 비디오는 https://github.com/AIDC-AI/ComfyUI-Copilot에서 이용할 수 있습니다.",
      "upvotes": 23,
      "discussionId": "6842632e542c9011f1bebfa3",
      "projectPage": "https://x.com/wangly0229/status/1923515826713526583",
      "githubRepo": "https://github.com/AIDC-AI/ComfyUI-Copilot",
      "ai_summary": "ComfyUI-Copilot uses a large language model and multi-agent system to enhance the usability and efficiency of the AI-driven art creation platform ComfyUI.",
      "ai_keywords": [
        "large language model",
        "multi-agent framework",
        "central assistant agent",
        "specialized worker agents",
        "knowledge bases"
      ]
    },
    "publishedAt": "2025-06-05T09:20:50.000Z",
    "title": "ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development",
    "summary": "We introduce ComfyUI-Copilot, a large language model-powered plugin designed\nto enhance the usability and efficiency of ComfyUI, an open-source platform for\nAI-driven art creation. Despite its flexibility and user-friendly interface,\nComfyUI can present challenges to newcomers, including limited documentation,\nmodel misconfigurations, and the complexity of workflow design. ComfyUI-Copilot\naddresses these challenges by offering intelligent node and model\nrecommendations, along with automated one-click workflow construction. At its\ncore, the system employs a hierarchical multi-agent framework comprising a\ncentral assistant agent for task delegation and specialized worker agents for\ndifferent usages, supported by our curated ComfyUI knowledge bases to\nstreamline debugging and deployment. We validate the effectiveness of\nComfyUI-Copilot through both offline quantitative evaluations and online user\nfeedback, showing that it accurately recommends nodes and accelerates workflow\ndevelopment. Additionally, use cases illustrate that ComfyUI-Copilot lowers\nentry barriers for beginners and enhances workflow efficiency for experienced\nusers. The ComfyUI-Copilot installation package and a demo video are available\nat https://github.com/AIDC-AI/ComfyUI-Copilot.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/wG9VdZ8vQGyG76QYWS4NS.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/VrBvS8a9t6I462AYXI2uc.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05010.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639c379cdb7c5f35004066cb",
      "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
      "fullname": "Zhenran Xu",
      "name": "imryanxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02865",
      "authors": [
        {
          "_id": "683fefbb7ed0da422d1ab676",
          "name": "Mathieu Andreux",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab677",
          "name": "Breno Baldas Skuk",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab678",
          "user": {
            "_id": "6808a8cf6b8c599b583d0fe9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6808a8cf6b8c599b583d0fe9/QPgERtP3Nl2n0BCBlp89D.jpeg",
            "isPro": false,
            "fullname": "Hamza Benchekroun",
            "user": "hamza-hcompany",
            "type": "user"
          },
          "name": "Hamza Benchekroun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T15:03:30.496Z",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab679",
          "name": "Emilien Biré",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67a",
          "name": "Antoine Bonnet",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67b",
          "name": "Riaz Bordie",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67c",
          "name": "Matthias Brunel",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67d",
          "name": "Pierre-Louis Cedoz",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67e",
          "name": "Antoine Chassang",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67f",
          "name": "Mickaël Chen",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab680",
          "name": "Alexandra D. Constantinou",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab681",
          "name": "Antoine d'Andigné",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab682",
          "name": "Hubert de La Jonquière",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab683",
          "name": "Aurélien Delfosse",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab684",
          "name": "Ludovic Denoyer",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab685",
          "name": "Alexis Deprez",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab686",
          "name": "Augustin Derupti",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab687",
          "name": "Michael Eickenberg",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab688",
          "name": "Mathïs Federico",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab689",
          "name": "Charles Kantor",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68a",
          "name": "Xavier Koegler",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68b",
          "name": "Yann Labbé",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68c",
          "name": "Matthew C. H. Lee",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68d",
          "name": "Erwan Le Jumeau de Kergaradec",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68e",
          "name": "Amir Mahla",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68f",
          "name": "Avshalom Manevich",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab690",
          "name": "Adrien Maret",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab691",
          "name": "Charles Masson",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab692",
          "name": "Rafaël Maurin",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab693",
          "name": "Arturo Mena",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab694",
          "name": "Philippe Modard",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab695",
          "name": "Axel Moyal",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab696",
          "name": "Axel Nguyen Kerbel",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab697",
          "name": "Julien Revelle",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab698",
          "name": "Mats L. Richter",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab699",
          "name": "María Santos",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69a",
          "name": "Laurent Sifre",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69b",
          "name": "Maxime Theillard",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69c",
          "name": "Marc Thibault",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69d",
          "name": "Louis Thiry",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69e",
          "name": "Léo Tronchon",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69f",
          "name": "Nicolas Usunier",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab6a0",
          "user": {
            "_id": "6264f9655f6f2e14d6ac981c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650784534234-noauth.png",
            "isPro": false,
            "fullname": "Tony Wu",
            "user": "tonywu71",
            "type": "user"
          },
          "name": "Tony Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:28:00.518Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T13:29:03.000Z",
      "submittedOnDailyAt": "2025-06-06T07:18:18.574Z",
      "title": "Surfer-H는 Open Weights를 기반으로 한 저비용의 웹 아웃로드 에이전트입니다. Holo1과의 결합으로 더욱 효율적인 서비스를 제공합니다.",
      "submittedOnDailyBy": {
        "_id": "6808a8cf6b8c599b583d0fe9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6808a8cf6b8c599b583d0fe9/QPgERtP3Nl2n0BCBlp89D.jpeg",
        "isPro": false,
        "fullname": "Hamza Benchekroun",
        "user": "hamza-hcompany",
        "type": "user"
      },
      "summary": "Surfer-H는 비용 효율적인 웹 에이전트입니다. 이는 사용자가 정의한 태스크를 수행하기 위해 시각 언어 모델(VLM)을 통합하고 있습니다. Surfer-H는 새로운 오픈 웨이트 컬렉션의 VLM, Holo1과 결합되어 있습니다. Holo1은 웹 탐색과 정보 추출에 특화된 것입니다. Holo1은 신중하게 선택된 데이터 소스로 훈련되었습니다. 여기에는 오픈 액세스 웹 콘텐츠, 합성 예시, 시그널 프로덕트 데이터 등이 포함됩니다. Holo1은 일반적인 사용자 인터페이스(UI) 벤치마크와 새로운 웹 UI 로케일 벤치마크, WebClick에서 가장 우수한 성능을 나타냅니다. Holo1을 포치로, Surfer-H는 WebVoyager에서 92.2%의 가장 先端의 성능을 달성하고 정확도와 비용 효율의 파로트 최적의 균형을 조정하고 있습니다. 웹 에이전트 시스템의 연구 진척을 가속화하기 위해, WebClick의 평가 데이터 세트와 Holo1 모델 웨이트를 공개하고 있습니다.",
      "upvotes": 22,
      "discussionId": "683fefbd7ed0da422d1ab718",
      "ai_summary": "Surfer-H, paired with Holo1, an open-weight collection of Vision-Language Models, achieves top performance in web navigation tasks with high cost-efficiency.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLM",
        "web navigation",
        "information extraction",
        "generalist User Interface",
        "UI",
        "WebClick",
        "WebVoyager",
        "open-sourcing",
        "model weights"
      ]
    },
    "publishedAt": "2025-06-03T09:29:03.000Z",
    "title": "Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights",
    "summary": "We present Surfer-H, a cost-efficient web agent that integrates\nVision-Language Models (VLM) to perform user-defined tasks on the web. We pair\nit with Holo1, a new open-weight collection of VLMs specialized in web\nnavigation and information extraction. Holo1 was trained on carefully curated\ndata sources, including open-access web content, synthetic examples, and\nself-produced agentic data. Holo1 tops generalist User Interface (UI)\nbenchmarks as well as our new web UI localization benchmark, WebClick. When\npowered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on\nWebVoyager, striking a Pareto-optimal balance between accuracy and\ncost-efficiency. To accelerate research advancement in agentic systems, we are\nopen-sourcing both our WebClick evaluation dataset and the Holo1 model weights.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02865.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6808a8cf6b8c599b583d0fe9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6808a8cf6b8c599b583d0fe9/QPgERtP3Nl2n0BCBlp89D.jpeg",
      "fullname": "Hamza Benchekroun",
      "name": "hamza-hcompany",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23656",
      "authors": [
        {
          "_id": "6842520f05049fa51eed0e9f",
          "user": {
            "_id": "656d8d4b1f8d9b618de91369",
            "avatarUrl": "/avatars/884dba9e56936241034b179d11a513b9.svg",
            "isPro": false,
            "fullname": "Xiangdong Zhang",
            "user": "aHapBean",
            "type": "user"
          },
          "name": "Xiangdong Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:15.290Z",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea0",
          "name": "Jiaqi Liao",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea1",
          "name": "Shaofeng Zhang",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea2",
          "name": "Fanqing Meng",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea3",
          "name": "Xiangpeng Wan",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea4",
          "name": "Junchi Yan",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea5",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:06:44.000Z",
      "submittedOnDailyAt": "2025-06-06T00:59:51.401Z",
      "title": "VideoREPA: 비디오 생성에 대한 관련적인 어레이멘트를 기초 모델과 통합하여 물리학을 배우는 방법\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "63a2a51ef30c464227924fc6",
        "avatarUrl": "/avatars/e109e85abd25b97bb29dbbe007119e34.svg",
        "isPro": false,
        "fullname": "Haoyu Sun",
        "user": "Mikivis",
        "type": "user"
      },
      "summary": "최근의 텍스트에서 이미지로의 (T2V) Difu̇̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈",
      "upvotes": 18,
      "discussionId": "6842521205049fa51eed0f67",
      "projectPage": "https://videorepa.github.io/",
      "githubRepo": "https://github.com/aHapBean/VideoREPA",
      "ai_summary": "VideoREPA enhances text-to-video synthesis by aligning token-level relations and distilling physics understanding from foundation models into T2V models.",
      "ai_keywords": [
        "T2V diffusion models",
        "physics understanding",
        "video self-supervised learning",
        "Token Relation Distillation (TRD) loss",
        "spatio-temporal alignment",
        "representation alignment (REPA)",
        "CogVideoX",
        "physics commonsense",
        "intuitive physics"
      ]
    },
    "publishedAt": "2025-05-29T13:06:44.000Z",
    "title": "VideoREPA: Learning Physics for Video Generation through Relational\n  Alignment with Foundation Models",
    "summary": "Recent advancements in text-to-video (T2V) diffusion models have enabled\nhigh-fidelity and realistic video synthesis. However, current T2V models often\nstruggle to generate physically plausible content due to their limited inherent\nability to accurately understand physics. We found that while the\nrepresentations within T2V models possess some capacity for physics\nunderstanding, they lag significantly behind those from recent video\nself-supervised learning methods. To this end, we propose a novel framework\ncalled VideoREPA, which distills physics understanding capability from video\nunderstanding foundation models into T2V models by aligning token-level\nrelations. This closes the physics understanding gap and enable more\nphysics-plausible generation. Specifically, we introduce the Token Relation\nDistillation (TRD) loss, leveraging spatio-temporal alignment to provide soft\nguidance suitable for finetuning powerful pre-trained T2V models, a critical\ndeparture from prior representation alignment (REPA) methods. To our knowledge,\nVideoREPA is the first REPA method designed for finetuning T2V models and\nspecifically for injecting physical knowledge. Empirical evaluations show that\nVideoREPA substantially enhances the physics commonsense of baseline method,\nCogVideoX, achieving significant improvement on relevant benchmarks and\ndemonstrating a strong capacity for generating videos consistent with intuitive\nphysics. More video results are available at https://videorepa.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a2a51ef30c464227924fc6",
      "avatarUrl": "/avatars/e109e85abd25b97bb29dbbe007119e34.svg",
      "fullname": "Haoyu Sun",
      "name": "Mikivis",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05240",
      "authors": [
        {
          "_id": "684249e23fb0b2ecb854594a",
          "user": {
            "_id": "630b094f8b327c7b8b94d24c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
            "isPro": false,
            "fullname": "Yizhuo Li",
            "user": "liyz",
            "type": "user"
          },
          "name": "Yizhuo Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:23.836Z",
          "hidden": false
        },
        {
          "_id": "684249e23fb0b2ecb854594b",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "684249e23fb0b2ecb854594c",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "684249e23fb0b2ecb854594d",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "684249e23fb0b2ecb854594e",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T16:59:53.000Z",
      "submittedOnDailyAt": "2025-06-06T00:26:53.631Z",
      "title": "잠시 스페이스의 대응과 흐름의 시작 대응",
      "submittedOnDailyBy": {
        "_id": "630b094f8b327c7b8b94d24c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
        "isPro": false,
        "fullname": "Yizhuo Li",
        "user": "liyz",
        "type": "user"
      },
      "summary": "이 논문은 임의의 목표 분포에 대해 학습 가능한 잠재 공간 조정을 위한 새로운 프레임워크를 제안합니다. 이 방법은 사전 학습된 폼 기반의 생성 모델을 목표 특성에 의해潜在 공간을 정규화합니다. 이 고정 폼 모델은 최적화 목표로潜在 공간 조정을 위해 조정 손실을 사용하여 정규화합니다. 우리는 이 조정 손실의 최소화가 목표 분포의 잠재 공간의 로그 확률의 분산 하한을 최대화하는 계산적으로 시간이 드는 대체 객체를 설정하는 것을 공식적으로 증명합니다. 특히, 제안된 방법은 계산적으로 고 비용의 확률 평가를 제외하고 최적화 중의 ODE 해결을 피합니다. 전문적인 증명에서 조정 손실의 함수면은 목표 분포의 음대수 확률을 근사하는 것을 보여줍니다. 또한, ImageNet 상의 다양한 목표 분포에 대한 대규모 이미지 생성 실험을 통해 우리의 접근 방식의 효과를 평가하고 세부적인 논의 및 소멸 실험을 수행합니다. 이론적 및 실험적 증명을 통해 우리의 프레임워크는 잠재 공간 조정에 새로운 길을 개척합니다.",
      "upvotes": 16,
      "discussionId": "684249e73fb0b2ecb8545afb",
      "projectPage": "https://liyizhuo.com/align/",
      "githubRepo": "https://github.com/liyz15/Aligning-Latent-Spaces-with-Flow-Priors",
      "ai_summary": "A novel framework using flow-based generative models aligns learnable latent spaces to target distributions, reducing computational expense and improving log-likelihood maximization.",
      "ai_keywords": [
        "flow-based generative models",
        "latent spaces",
        "alignment loss",
        "flow matching objective",
        "variational lower bound",
        "log-likelihood",
        "ImageNet"
      ]
    },
    "publishedAt": "2025-06-05T12:59:53.000Z",
    "title": "Aligning Latent Spaces with Flow Priors",
    "summary": "This paper presents a novel framework for aligning learnable latent spaces to\narbitrary target distributions by leveraging flow-based generative models as\npriors. Our method first pretrains a flow model on the target features to\ncapture the underlying distribution. This fixed flow model subsequently\nregularizes the latent space via an alignment loss, which reformulates the flow\nmatching objective to treat the latents as optimization targets. We formally\nprove that minimizing this alignment loss establishes a computationally\ntractable surrogate objective for maximizing a variational lower bound on the\nlog-likelihood of latents under the target distribution. Notably, the proposed\nmethod eliminates computationally expensive likelihood evaluations and avoids\nODE solving during optimization. As a proof of concept, we demonstrate in a\ncontrolled setting that the alignment loss landscape closely approximates the\nnegative log-likelihood of the target distribution. We further validate the\neffectiveness of our approach through large-scale image generation experiments\non ImageNet with diverse target distributions, accompanied by detailed\ndiscussions and ablation studies. With both theoretical and empirical\nvalidation, our framework paves a new way for latent space alignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630b094f8b327c7b8b94d24c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
      "fullname": "Yizhuo Li",
      "name": "liyz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05176",
      "authors": [
        {
          "_id": "6842521939f41e76fd96ae38",
          "name": "Yanzhao Zhang",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae39",
          "name": "Mingxin Li",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3a",
          "user": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "/avatars/bf5c04a6032709f35e3fb48e1be6976f.svg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "name": "Dingkun Long",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:13.249Z",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3b",
          "user": {
            "_id": "63b6dbc8ccebeadccc888456",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
            "isPro": false,
            "fullname": "Xin Zhang",
            "user": "izhx",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:10.698Z",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3c",
          "name": "Huan Lin",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3d",
          "name": "Baosong Yang",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3e",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3f",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae40",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae41",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae42",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae43",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T15:49:48.000Z",
      "submittedOnDailyAt": "2025-06-06T01:01:32.740Z",
      "title": "Qwen3 임베딩: 토큰 임베딩 모델을 통해 문장 임베딩 및 재검색의 발전",
      "submittedOnDailyBy": {
        "_id": "616adb8578833ce5997e441a",
        "avatarUrl": "/avatars/bf5c04a6032709f35e3fb48e1be6976f.svg",
        "isPro": false,
        "fullname": "Dingkun Long",
        "user": "thenlper",
        "type": "user"
      },
      "summary": "이 작품에서 Qwen3 Embedding 시리즈를 소개합니다. 이 시리즈는 이전의 GTE-Qwen 시리즈와 비교하여 문맥을 이해하고 재검색의 능력을 크게 발전시켰습니다. Qwen3 기반 모델을 통해 구축되었습니다. Qwen3 LLMs의 강력한 다언어 문맥 이해와 생성 능력에 활용하여 새로운 다단계 훈련 프로파일을 도입했습니다. 이는 대규모 무제한 사전 훈련과 고품질 데이터 세트에 기반한 규제된 미세 조정을 조합하여 수행됩니다. 또한 효과적인 모델 통합 전략은 Qwen3 Embedding 시리즈의 강건성과 적응성을 보장하고 있습니다. 훈련 프로세스 중 Qwen3 LLMs는 백준 모델의 역할을 수행하며, 또한 다양한 영역과 언어의 광범위한 고품질, 풍부한, 다양한 훈련 데이터의 합성에 중요한 역할을 합니다. Qwen3 Embedding 시리즈는 문맥을 이해하고 재검색의 작업에 대해 모델 크기 범위(0.6B, 4B, 8B)를 제공하며, 사용자가 다양한 사용 시나리오를 선택하여 효율성과 효과성을 최적화할 수 있습니다. 실험적인 평가에 따라 Qwen3 Embedding 시리즈는 다양한 벤치마크에서 가장 선진적인 결과를 얻었습니다. 특히, MTEB의 다언어 평가 벤치마크에서 문맥을 이해하고, 코드 검색, 크로스 언어 검색, 다언어 검색 등의 검색 태스크에서 뛰어난 성능을 보입니다. 재현성 증진과 커뮤니티 주도 연구 개발의 추진을 위해 Qwen3 Embedding 모델은 Apache 2.0 라이선스 하에 공개되었습니다.",
      "upvotes": 16,
      "discussionId": "6842521a39f41e76fd96ae6f",
      "projectPage": "https://qwenlm.github.io/blog/qwen3-embedding/",
      "githubRepo": "https://github.com/QwenLM/Qwen3-Embedding",
      "ai_summary": "The Qwen3 Embedding series, built on Qwen3 foundation models, offers advanced text embedding and reranking capabilities through a multi-stage training pipeline, achieving state-of-the-art performance across multilingual and retrieval benchmarks.",
      "ai_keywords": [
        "Qwen3 Embedding series",
        "GTE-Qwen series",
        "Qwen3 LLMs",
        "multilingual text understanding",
        "unsupervised pre-training",
        "supervised fine-tuning",
        "model merging",
        "embedding",
        "reranking",
        "MTEB",
        "code retrieval",
        "cross-lingual retrieval",
        "multilingual retrieval"
      ]
    },
    "publishedAt": "2025-06-05T11:49:48.000Z",
    "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models",
    "summary": "In this work, we introduce the Qwen3 Embedding series, a significant\nadvancement over its predecessor, the GTE-Qwen series, in text embedding and\nreranking capabilities, built upon the Qwen3 foundation models. Leveraging the\nQwen3 LLMs' robust capabilities in multilingual text understanding and\ngeneration, our innovative multi-stage training pipeline combines large-scale\nunsupervised pre-training with supervised fine-tuning on high-quality datasets.\nEffective model merging strategies further ensure the robustness and\nadaptability of the Qwen3 Embedding series. During the training process, the\nQwen3 LLMs serve not only as backbone models but also play a crucial role in\nsynthesizing high-quality, rich, and diverse training data across multiple\ndomains and languages, thus enhancing the training pipeline. The Qwen3\nEmbedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both\nembedding and reranking tasks, addressing diverse deployment scenarios where\nusers can optimize for either efficiency or effectiveness. Empirical\nevaluations demonstrate that the Qwen3 Embedding series achieves\nstate-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmark MTEB for text embedding, as well as in\nvarious retrieval tasks, including code retrieval, cross-lingual retrieval and\nmultilingual retrieval. To facilitate reproducibility and promote\ncommunity-driven research and development, the Qwen3 Embedding models are\npublicly available under the Apache 2.0 license.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05176.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "616adb8578833ce5997e441a",
      "avatarUrl": "/avatars/bf5c04a6032709f35e3fb48e1be6976f.svg",
      "fullname": "Dingkun Long",
      "name": "thenlper",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 96
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04633",
      "authors": [
        {
          "_id": "68426296b8d07a60074e866a",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866b",
          "name": "Mahtab Bigverdi",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866c",
          "user": {
            "_id": "645b4819f9d4ec91fdd54852",
            "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
            "isPro": false,
            "fullname": "Jiawei Gu",
            "user": "kuvvi",
            "type": "user"
          },
          "name": "Jiawei Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:38.249Z",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866d",
          "name": "Zixian Ma",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866e",
          "name": "Yinuo Yang",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866f",
          "name": "Ziang Li",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e8670",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e8671",
          "name": "Ranjay Krishna",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T05:09:46.000Z",
      "submittedOnDailyAt": "2025-06-06T02:08:57.401Z",
      "title": "스펙트럼 인식의 발전: 시각 시뮬레이션에서 다중모달 모델의 평가",
      "submittedOnDailyBy": {
        "_id": "645b4819f9d4ec91fdd54852",
        "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
        "isPro": false,
        "fullname": "Jiawei Gu",
        "user": "kuvvi",
        "type": "user"
      },
      "summary": "공간 인식은 인간의 지능의 중요한 부분으로, 시각적 시뮬레이션을 통해 문제를 해결하는 데에 주로 의존하지 않고, 언어적 추론에 의존하는 것과 비슷하게 작동하는 것으로 이루어져 있습니다. 그러나 현재의 AI 벤치마크는 주로 언어적 추론을 평가하고, 비언어적, 다단계의 시각적 시뮬레이션의 복잡성을 잘못 평가하고 있습니다. 우리는 STARE(공간 변환과 이유론의 평가)를 소개합니다. STARE는 시각적 시뮬레이션을 통해 해결할 수 있는 다양한 타입의 복잡한 태스크를 통해 엄격하게 평가하는 벤치마크입니다. STARE는 기본적인 기하학적 변환(2차원 및 3차원), 통합적인 공간적인 이유론(큐브네트트 접기 및 唐詰ぼし 퍼즐), 실세계적인 공간적인 이유론(점과 시간의 이유론)을 포함하는 4K 태스크를 특징으로 합니다. 이러한 태스크는 물체 조립, 기계도 해석, 일상적인 공간 이동 등 실용적인 인지적 문제를 반영하고 있습니다. 우리의 평가에 따르면, 모델은 간단한 2차원 변환의 이유론에 능숙하지만, 3차원 큐브네트트 접기 및 唐詰ぼし 퍼즐과 같은 복잡한 태스크에서는 여러 단계의 시각적 시뮬레이션이 필요할 때 그 근사적인 운명을 같이 행동합니다. 인간은 거의 정확한 정확도를 달성하지만, 복잡한 태스크에서는 상당한 시간(최대 28.9초)을 필요로 하지만, 중간적인 시각적 시뮬레이션을 사용함으로써 평균 7.5초가 빨라집니다. 반면, 모델은 시각적 시뮬레이션으로부터 성능의 향상은 불확실하며, 많은 태스크에서 개선하지만, 특정한 경우, 唐詰ぼし 퍼즐(GPT-4o, o1)과 큐브네트트 접기(Claude-3.5, Gemini-2.0 Flash)에서 감소합니다. 이는 모델이 중간적인 시각적 정보를 효과적으로 활용하는 방법이 없는 것을 보여줍니다.",
      "upvotes": 15,
      "discussionId": "68426298b8d07a60074e86eb",
      "projectPage": "https://huggingface.co/datasets/kuvvi/STARE",
      "githubRepo": "https://github.com/STARE-bench/STARE/",
      "ai_summary": "A new benchmark evaluates multimodal models on visual simulation tasks, revealing varying model performances compared to human accuracy and the impact of intermediate visual simulations.",
      "ai_keywords": [
        "spatial cognition",
        "visual simulations",
        "verbal reasoning",
        "multimodal large language models",
        "STARE",
        "spatial transformations",
        "geometric transformations",
        "integrated spatial reasoning",
        "real-world spatial reasoning",
        "visual reasoning",
        "intermediate visual simulations"
      ]
    },
    "publishedAt": "2025-06-05T01:09:46.000Z",
    "title": "Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual\n  Simulations",
    "summary": "Spatial cognition is essential for human intelligence, enabling\nproblem-solving through visual simulations rather than solely relying on verbal\nreasoning. However, existing AI benchmarks primarily assess verbal reasoning,\nneglecting the complexities of non-verbal, multi-step visual simulation. We\nintroduce STARE(Spatial Transformations and Reasoning Evaluation), a benchmark\ndesigned to rigorously evaluate multimodal large language models on tasks\nbetter solved through multi-step visual simulation. STARE features 4K tasks\nspanning foundational geometric transformations (2D and 3D), integrated spatial\nreasoning (cube net folding and tangram puzzles), and real-world spatial\nreasoning (perspective and temporal reasoning), reflecting practical cognitive\nchallenges like object assembly, mechanical diagram interpretation, and\neveryday spatial navigation. Our evaluations show that models excel at\nreasoning over simpler 2D transformations, but perform close to random chance\non more complex tasks like 3D cube net folding and tangram puzzles that require\nmulti-step visual simulations. Humans achieve near-perfect accuracy but take\nconsiderable time (up to 28.9s) on complex tasks, significantly speeding up\n(down by 7.5 seconds on average) with intermediate visual simulations. In\ncontrast, models exhibit inconsistent performance gains from visual\nsimulations, improving on most tasks but declining in specific cases like\ntangram puzzles (GPT-4o, o1) and cube net folding (Claude-3.5, Gemini-2.0\nFlash), indicating that models may not know how to effectively leverage\nintermediate visual information.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04633.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b4819f9d4ec91fdd54852",
      "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
      "fullname": "Jiawei Gu",
      "name": "kuvvi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05344",
      "authors": [
        {
          "_id": "68424fe9bdc448822b31beac",
          "name": "Jiahui Wang",
          "hidden": false
        },
        {
          "_id": "68424fe9bdc448822b31bead",
          "user": {
            "_id": "64f001bfabd9fb1914398bd5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
            "isPro": false,
            "fullname": "liuzuyan",
            "user": "Zuyan",
            "type": "user"
          },
          "name": "Zuyan Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:17.520Z",
          "hidden": false
        },
        {
          "_id": "68424fe9bdc448822b31beae",
          "name": "Yongming Rao",
          "hidden": false
        },
        {
          "_id": "68424fe9bdc448822b31beaf",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-06T00:48:21.379Z",
      "title": "SparseMM: 헤드의 희소성은 MLLM의 시각적 개념의 응답에서 나타난다.",
      "submittedOnDailyBy": {
        "_id": "64f001bfabd9fb1914398bd5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
        "isPro": false,
        "fullname": "liuzuyan",
        "user": "Zuyan",
        "type": "user"
      },
      "summary": "다모달 대언어 모델(MLLMs)는 시각적 능력이 추가된 사전 훈련된 대언어 모델(LLMs)에서 일반적으로 얻을 수 있습니다. 본 연구에서는, MLLMs가 시각적 입력을 어떻게 처리하는지를 조사하고, 注意기 구조를 분석하고 있습니다. 그 결과, 놀라울만한 희소성 현상이 밝혀졌습니다: LLMs의 注意헤드의 시각적 이해에 활발히 활동하는 헤더는 약 5% 미만의 작은 부분입니다. 이러한 헤더를 효율적으로 특정하기 위해, 우리는 특정의 응답 분석에 의한 헤드 수준의 시각적 관련성을 정량화하는 훈련 제한 프레임워크를 설계했습니다. 이 발견에 기반하여, 우리는 시각적 스코어에 기반하여 LLMs의 헤드에 불균형한 계산바이지션을 할당하는 KV-Cache 최적화 스틸라티, SparseMM을 소개했습니다. 이는 시각적 헤더의 희소성을 활용하여 MLLM의 추론을 가속화하는 것을 목표로 합니다. 선행의 KV-Cache 가속화 방법과 비교하여, SparseMM은 시각적 특성을 무시하고 있는 점에 주목하며, 해상도와 메모리 영역의 감소를 우선시하고 있습니다.主流의 다모달 벤치마크에서 확장된 평가에 따라, SparseMM은 정확성과 효율성의 균형을 잘 유지하고 있습니다. 특히, SparseMM은 성능의 등화성을 유지하는 동시에, 생성 시 1.38배의 실타임 가속과 52%의 메모리 감소를 실현했습니다. 우리의 프로젝트는 https://github.com/CR400AF-A/SparseMM 에서 오픈소스로 제공됩니다.",
      "upvotes": 14,
      "discussionId": "68424febbdc448822b31bf2c",
      "projectPage": "https://cr400af-a.github.io/SparseMM/",
      "githubRepo": "https://github.com/CR400AF-A/SparseMM",
      "ai_summary": "MLLLMs achieve enhanced efficiency through SparseMM, a KV-Cache optimization strategy that identifies and prioritizes visual heads, leading to significant real-time acceleration and memory reduction without compromising performance.",
      "ai_keywords": [
        "multimodal large language models",
        "LLMs",
        "visual capabilities",
        "attention mechanisms",
        "visual heads",
        "targeted response analysis",
        "KV-Cache optimization",
        "SparseMM",
        "head-level visual relevance",
        "visual semantics",
        "multimodal benchmarks"
      ]
    },
    "publishedAt": "2025-06-05T13:59:55.000Z",
    "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
    "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05344.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "64f001bfabd9fb1914398bd5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
      "fullname": "liuzuyan",
      "name": "Zuyan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05331",
      "authors": [
        {
          "_id": "684260765bfed1b94a9cc307",
          "user": {
            "_id": "647c7a4ed412b3b376572a00",
            "avatarUrl": "/avatars/9cc310fd3f9e3f211475816ed9b0cdaa.svg",
            "isPro": false,
            "fullname": "Xinyan Chen",
            "user": "xy06",
            "type": "user"
          },
          "name": "Xinyan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:42.546Z",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc308",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc309",
          "user": {
            "_id": "6349214f8146350b3a4c5cdf",
            "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
            "isPro": false,
            "fullname": "Dongzhi Jiang",
            "user": "CaraJ",
            "type": "user"
          },
          "name": "Dongzhi Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:40.308Z",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc30a",
          "name": "Aojun Zhou",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc30b",
          "name": "Shilin Yan",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc30c",
          "name": "Weifeng Lin",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc30d",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:59:02.000Z",
      "submittedOnDailyAt": "2025-06-06T02:07:44.094Z",
      "title": "MINT-CoT: 수학의 Chain-of-Thought Reasoning에서 간접 시각 토큰의 사용을 가능하게 합니다.",
      "submittedOnDailyBy": {
        "_id": "647c7a4ed412b3b376572a00",
        "avatarUrl": "/avatars/9cc310fd3f9e3f211475816ed9b0cdaa.svg",
        "isPro": false,
        "fullname": "Xinyan Chen",
        "user": "xy06",
        "type": "user"
      },
      "summary": "Chain-of-Thought (CoT)는 대규모 언어 모델(LLMs)의 수학적 추론을 통해 광범위하게 효과적으로 개선되어 왔지만, 여러 종류의 분야에 확장하는 것은 어려워집니다. 현재의 연구는 이미지 입력에 대한 유사한 맥락적 추론을 적용하거나, 수학 콘텐츠에 대한 시각적 인코더의 시각적 인식의 한계를 극복하기 위해 시각적 신호를 수학적 토큰 사이에 삽입하는 시도를 하고 있습니다. 그러나 수학 문제 해결에 있어서 3가지의 주요한 제한이 있습니다: 거대화의 이미지 영역의 의존관계, 시각적 인코더에 의한 수학 콘텐츠의 인식의 한계, 시각적 변경의 외부 능력의 의존관계입니다. 본 논문에서는 수학적 interleaved tokens을 이용한 Chain-of-Thought의 시각적 추론을 도입하고, MINT-CoT으로 소개합니다. MINT-CoT은 interleave token을 사용하여 수학 그래프 내의 임의의 모양의 시각적 영역을 동적으로 선택하여, 맥락적 추론 단계에 적절한 시각 토큰을 삽입합니다. 이를 실현하기 위해 MINT-CoT 데이터 세트를 구축하고, 54K의 수학 문제를 포함하며, 각 추론 단계에 대해 토큰 수준의 시각적 영역과 대응하여 엄격한 데이터 생성 프로세스를 동반합니다. 또한, MINT-CoT-7B 모델을 구축하기 위해, 문맥만의 CoT SFT, 삽입된 CoT SFT, 삽입된 CoT RL의 3단계의 훈련 전략을 제안합니다. 확장된 실험은 수학 분야에서 유효한 시각적 삽입 추론의 효과를 보여주고, MINT-CoT-7B는 MathVista에서 +34.08%, GeoQA에서 +28.78%, MMStar에서 +23.2%의 효과를 나타냅니다. 코드와 데이터는 https://github.com/xinyan-cxy/MINT-CoT에 공개되어 있습니다.",
      "upvotes": 12,
      "discussionId": "684260775bfed1b94a9cc346",
      "githubRepo": "https://github.com/xinyan-cxy/MINT-CoT",
      "ai_summary": "MINT-CoT enhances multimodal mathematical reasoning by interleaving visual tokens into textual chain-of-thought steps, enabling flexible visual perception and improved problem-solving.",
      "ai_keywords": [
        "Chain-of-Thought",
        "Large Language Models",
        "multimodal domains",
        "textual reasoning",
        "visual signals",
        "image input",
        "vision encoders",
        "math content",
        "visual modification",
        "Mathematical INterleaved Tokens",
        "Interleave Token",
        "visual regions",
        "token level",
        "MINT-CoT dataset",
        "text-only CoT SFT",
        "interleaved CoT SFT",
        "interleaved CoT RL",
        "MINT-CoT-7B",
        "MathVista",
        "GeoQA",
        "MMStar"
      ]
    },
    "publishedAt": "2025-06-05T13:59:02.000Z",
    "title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical\n  Chain-of-Thought Reasoning",
    "summary": "Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large\nLanguage Models (LLMs), but it still remains challenging for extending it to\nmultimodal domains. Existing works either adopt a similar textual reasoning for\nimage input, or seek to interleave visual signals into mathematical CoT.\nHowever, they face three key limitations for math problem-solving: reliance on\ncoarse-grained box-shaped image regions, limited perception of vision encoders\non math content, and dependence on external capabilities for visual\nmodification. In this paper, we propose MINT-CoT, introducing Mathematical\nINterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively\ninterleaves relevant visual tokens into textual reasoning steps via an\nInterleave Token, which dynamically selects visual regions of any shapes within\nmath figures. To empower this capability, we construct the MINT-CoT dataset,\ncontaining 54K mathematical problems aligning each reasoning step with visual\nregions at the token level, accompanied by a rigorous data generation pipeline.\nWe further present a three-stage MINT-CoT training strategy, progressively\ncombining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which\nderives our MINT-CoT-7B model. Extensive experiments demonstrate the\neffectiveness of our method for effective visual interleaved reasoning in\nmathematical domains, where MINT-CoT-7B outperforms the baseline model by\n+34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our\ncode and data are available at https://github.com/xinyan-cxy/MINT-CoT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05331.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647c7a4ed412b3b376572a00",
      "avatarUrl": "/avatars/9cc310fd3f9e3f211475816ed9b0cdaa.svg",
      "fullname": "Xinyan Chen",
      "name": "xy06",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05328",
      "authors": [
        {
          "_id": "68424822f0c91a7dcb64193b",
          "user": {
            "_id": "64a3de701698ad2985277148",
            "avatarUrl": "/avatars/09eebadbbea53ed2800591564ff5c931.svg",
            "isPro": false,
            "fullname": "lulidong",
            "user": "lulidong",
            "type": "user"
          },
          "name": "Lidong Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:34.755Z",
          "hidden": false
        },
        {
          "_id": "68424822f0c91a7dcb64193c",
          "user": {
            "_id": "6392c73390b8e99a6779a7b0",
            "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
            "isPro": false,
            "fullname": "Guo Chen",
            "user": "cg1177",
            "type": "user"
          },
          "name": "Guo Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:32.158Z",
          "hidden": false
        },
        {
          "_id": "68424822f0c91a7dcb64193d",
          "name": "Zhiqi Li",
          "hidden": false
        },
        {
          "_id": "68424822f0c91a7dcb64193e",
          "name": "Yicheng Liu",
          "hidden": false
        },
        {
          "_id": "68424822f0c91a7dcb64193f",
          "name": "Tong Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:58:33.000Z",
      "submittedOnDailyAt": "2025-06-06T00:16:44.777Z",
      "title": "AV-Reasoner: クールガイド를 따른 음성-시각의 수를 개선하고, MLLM에서 벤치마크를 수행합니다.",
      "submittedOnDailyBy": {
        "_id": "64a3de701698ad2985277148",
        "avatarUrl": "/avatars/09eebadbbea53ed2800591564ff5c931.svg",
        "isPro": false,
        "fullname": "lulidong",
        "user": "lulidong",
        "type": "user"
      },
      "summary": "電視 이해의 발전에 따라 현재의 MLLM은 숫자 세기 태스크에 많은 어려움을 겪고 있습니다. 기존의 벤치마크는 짧은 비디오, 근접 세트의 쿼리, 카테고리의 부족, 그리고 약한 다 모델 커버리지를 지속적으로 직면하고 있습니다. 본 논문에서는 CG-AV-Counting이라는 1,027건의 다 모델 쿼스트와 5,845건의 카테고리 지정된 컬러 기반 숫자 세기 벤치마크를 소개합니다. 이는 블랙박스 평가와 화이트박스 평가를 지원하고, 엔드 투 엔드 및 이유 기반의 숫자 세기를 위해 엄격한 테스트 벤치마크입니다. 숫자 세기 능력을 향상시키기 위해, GRPO와 클레클라러닝을 사용하여 훈련된 AV-Reasoner 모델을 제안합니다. AV-Reasoner는 여러 벤치마크에서 가장 최신의 결과를 기록하며, 강화 학습의 효과성을 보여주고 있습니다. 그러나 도메인 외의 벤치마크에서의 실험은 언어 공간에서 이유를 갖게 된 것이 성능 향상에 도움이 되는 것을 보여주지 않습니다. 코드와 벤치마크는 https://av-reasoner.github.io에 공개되어 있습니다.",
      "upvotes": 12,
      "discussionId": "68424823f0c91a7dcb6419c7",
      "projectPage": "https://AV-Reasoner.github.io",
      "githubRepo": "https://github.com/AV-Reasoner/AV-Reasoner",
      "ai_summary": "CG-AV-Counting is a new benchmark for video counting tasks that includes multimodal data and supports end-to-end and reasoning-based models. AV-Reasoner, trained with GRPO and curriculum learning, achieves top results but shows limitations on out-of-domain tasks.",
      "ai_keywords": [
        "MLLMs",
        "CG-AV-Counting",
        "multimodal questions",
        "clue-grounded",
        "black-box evaluation",
        "white-box evaluation",
        "AV-Reasoner",
        "GRPO",
        "curriculum learning",
        "reinforcement learning",
        "out-of-domain benchmarks"
      ]
    },
    "publishedAt": "2025-06-05T13:58:33.000Z",
    "title": "AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual\n  Counting for MLLMs",
    "summary": "Despite progress in video understanding, current MLLMs struggle with counting\ntasks. Existing benchmarks are limited by short videos, close-set queries, lack\nof clue annotations, and weak multimodal coverage. In this paper, we introduce\nCG-AV-Counting, a manually-annotated clue-grounded counting benchmark with\n1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It\nsupports both black-box and white-box evaluation, serving as a comprehensive\ntestbed for both end-to-end and reasoning-based counting. To explore ways to\nimprove model's counting capability, we propose AV-Reasoner, a model trained\nwith GRPO and curriculum learning to generalize counting ability from related\ntasks. AV-Reasoner achieves state-of-the-art results across multiple\nbenchmarks, demonstrating the effectiveness of reinforcement learning. However,\nexperiments show that on out-of-domain benchmarks, reasoning in the language\nspace fails to bring performance gains. The code and benchmark have been\nrealeased on https://av-reasoner.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05328.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a3de701698ad2985277148",
      "avatarUrl": "/avatars/09eebadbbea53ed2800591564ff5c931.svg",
      "fullname": "lulidong",
      "name": "lulidong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03077",
      "authors": [
        {
          "_id": "683fc07a1de14546d5decf19",
          "name": "Qijun Luo",
          "hidden": false
        },
        {
          "_id": "683fc07a1de14546d5decf1a",
          "user": {
            "_id": "65a521af90b5e87bcd343828",
            "avatarUrl": "/avatars/3bbe83c1ba47df17d9c05a049147e5cc.svg",
            "isPro": false,
            "fullname": "Mengqi Li",
            "user": "Kullpar",
            "type": "user"
          },
          "name": "Mengqi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:10.607Z",
          "hidden": false
        },
        {
          "_id": "683fc07a1de14546d5decf1b",
          "name": "Lei Zhao",
          "hidden": false
        },
        {
          "_id": "683fc07a1de14546d5decf1c",
          "name": "Xiao Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T16:54:15.000Z",
      "submittedOnDailyAt": "2025-06-06T02:05:55.529Z",
      "title": "StreamBP: 긴 시퀀스의 훈련에 있어서 메모리 효율적인 정확한 반복 계산 알고리즘",
      "submittedOnDailyBy": {
        "_id": "65a521af90b5e87bcd343828",
        "avatarUrl": "/avatars/3bbe83c1ba47df17d9c05a049147e5cc.svg",
        "isPro": false,
        "fullname": "Mengqi Li",
        "user": "Kullpar",
        "type": "user"
      },
      "summary": "장 시퀀스 데이터로 언어 모델을 훈련하는 것은 복잡한 태스크(예: 장锁 논리)에 대한 모델의 능력을 향상시키기 위한 요구사항입니다が 시퀀스의 길이가 늘어날수록 BP(Backpropagation) 과정 중 활성 값의 메모리 비용이 크게 증가합니다. 이 문제를 해결하기 위해 StreamBP라는 메모리 효율적인 엣지 BP 메소드를 제안합니다. StreamBP는 시퀀스 차원과 함께 쉘 로어의 선형 분해를 수행하고 활성 값과 로지ッ트의 메모리 비용을 크게 줄입니다. 제안된 방법은 SFT, GRPO, DPO 등 일반적인 객체에 적용할 수 있습니다. 구현 측면에서 StreamBP는 언어 모델의 원인 구조를 활용하여 계산 비용 FLOPs를 줄이고 BP 속도를 빠르게 합니다. Gradient Checkpointing과 비교하여 StreamBP는 BP의 최대 시퀀스 길이를 2.8~5.5배 늘리고 BP 시간은 상대적으로 적거나 줄입니다. StreamBP의 시퀀스 길이 스케일링 능력은 훈련을 가속화하기 위해 배치 크기 스케일링을 직접 적용할 수 있습니다. 또한 통신 효율적인 분산 StreamBP를 개발하여 효과적으로 다 GPU 훈련을 지원하고 적용 범위를 넓혔습니다. 우리의 코드는 임의의 Transformer 모델의 훈련 파이프라인을 간단히 통합할 수 있으며, https://github.com/Ledzy/StreamBP에서 사용 가능합니다.",
      "upvotes": 12,
      "discussionId": "683fc07e1de14546d5decfe2",
      "githubRepo": "https://github.com/Ledzy/StreamBP",
      "ai_summary": "StreamBP, a memory-efficient and exact backpropagation method, decomposes the chain rule to reduce memory costs, enabling longer sequence lengths and faster training speeds for language models compared to gradient checkpointing.",
      "ai_keywords": [
        "backpropagation (BP)",
        "memory-efficient",
        "exact BP",
        "gradient checkpointing",
        "chain rule",
        "sequence dimension",
        "layer-wise",
        "activation values",
        "logits",
        "SFT",
        "GRPO",
        "DPO",
        "computational FLOPs",
        "BP speed",
        "causal structure",
        "language model",
        "multi-GPU training",
        "distributed StreamBP"
      ]
    },
    "publishedAt": "2025-06-03T12:54:15.000Z",
    "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence\n  Training of LLMs",
    "summary": "Training language models on long sequence data is a demanding requirement for\nenhancing the model's capability on complex tasks, e.g., long-chain reasoning.\nHowever, as the sequence length scales up, the memory cost for storing\nactivation values becomes huge during the Backpropagation (BP) process, even\nwith the application of gradient checkpointing technique. To tackle this\nchallenge, we propose a memory-efficient and exact BP method called StreamBP,\nwhich performs a linear decomposition of the chain rule along the sequence\ndimension in a layer-wise manner, significantly reducing the memory cost of\nactivation values and logits. The proposed method is applicable to common\nobjectives such as SFT, GRPO, and DPO. From an implementation perspective,\nStreamBP achieves less computational FLOPs and faster BP speed by leveraging\nthe causal structure of the language model. Compared to gradient checkpointing,\nStreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,\nwhile using comparable or even less BP time. Note that StreamBP's sequence\nlength scaling ability can be directly transferred to batch size scaling for\naccelerating training. We further develop a communication-efficient distributed\nStreamBP to effectively support multi-GPU training and broaden its\napplicability. Our code can be easily integrated into the training pipeline of\nany transformer models and is available at https://github.com/Ledzy/StreamBP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03077.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65a521af90b5e87bcd343828",
      "avatarUrl": "/avatars/3bbe83c1ba47df17d9c05a049147e5cc.svg",
      "fullname": "Mengqi Li",
      "name": "Kullpar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05327",
      "authors": [
        {
          "_id": "6842591962047f5641b3b650",
          "user": {
            "_id": "661d1f83ea3df2195a7c2924",
            "avatarUrl": "/avatars/dec49fc1d79913b07b57ccbef079198f.svg",
            "isPro": false,
            "fullname": "dcshi",
            "user": "dc-walker",
            "type": "user"
          },
          "name": "Duochao Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:58.621Z",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b651",
          "user": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "name": "Weijie Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:00.966Z",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b652",
          "name": "Donny Y. Chen",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b653",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b654",
          "name": "Jia-Wang Bian",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b655",
          "name": "Bohan Zhuang",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b656",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:58:23.000Z",
      "submittedOnDailyAt": "2025-06-06T01:28:04.514Z",
      "title": "다시 깊이 표현을 사용한 Feed-Forward 3D Gaussian Spreading을 재확인합니다.",
      "submittedOnDailyBy": {
        "_id": "66699aa8a33847217b5a49c7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
        "isPro": false,
        "fullname": "Weijie Wang",
        "user": "lhmd",
        "type": "user"
      },
      "summary": "3DGS(3차원 가우스 스플릿팅) 파이프라인에서 널리 사용되고 있는 데프스맵은 새로운 시각 포인트 합성에 적합합니다. 이 방법론은 데프스맵을 3차원 포인트 云로 변환하여 새로운 시각 포인트 합성을 수행합니다. 이 접근법은 효율적인 훈련, 기존 카메라 자세의 사용, 정확한 기오메트리 추정 등 여러 장점을 제공합니다. 그러나 물체의 경계에 대한 데프스 불연속은 포인트 云의 파괴와 희소화를招引하고 렌더링의 품질을 저하시킵니다. 이러한 문제를 해결하기 위해, 우리는 사전 학습된 Transformer가 예측한 포인트 맵을 기반으로 새로운 정규화 손실인 PM-Loss를 소개합니다. 포인트 맵은 데프스맵보다 정확하지는 않지만, 특히 물체의 경계 주변에서는 기오메트리의 평활성을 강제하고 데프스맵을 개선합니다. 이 데프스맵의 개선으로, 우리의 방법은 다양한 아키텍처와 스케네에서 3DGS를 크게 향상시키고 일관된 좋은 렌더링 결과를 제공합니다. 우리의 프로젝트 페이지는 https://aim-uofa.github.io/PMLoss 입니다.",
      "upvotes": 10,
      "discussionId": "6842591a62047f5641b3b6bc",
      "projectPage": "https://aim-uofa.github.io/PMLoss",
      "githubRepo": "https://github.com/aim-uofa/PM-Loss",
      "ai_summary": "PM-Loss, a regularization technique using pointmaps from a pre-trained transformer, enhances feed-forward 3D Gaussian Splatting by improving depth map accuracy and rendering quality.",
      "ai_keywords": [
        "3D Gaussian Splatting",
        "3DGS",
        "depth maps",
        "point clouds",
        "novel view synthesis",
        "PM-Loss",
        "pre-trained transformer",
        "pointmap",
        "geometric smoothness"
      ]
    },
    "publishedAt": "2025-06-05T13:58:23.000Z",
    "title": "Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting",
    "summary": "Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS)\npipelines by unprojecting them into 3D point clouds for novel view synthesis.\nThis approach offers advantages such as efficient training, the use of known\ncamera poses, and accurate geometry estimation. However, depth discontinuities\nat object boundaries often lead to fragmented or sparse point clouds, degrading\nrendering quality -- a well-known limitation of depth-based representations. To\ntackle this issue, we introduce PM-Loss, a novel regularization loss based on a\npointmap predicted by a pre-trained transformer. Although the pointmap itself\nmay be less accurate than the depth map, it effectively enforces geometric\nsmoothness, especially around object boundaries. With the improved depth map,\nour method significantly improves the feed-forward 3DGS across various\narchitectures and scenes, delivering consistently better rendering results. Our\nproject page: https://aim-uofa.github.io/PMLoss",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05327.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66699aa8a33847217b5a49c7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
      "fullname": "Weijie Wang",
      "name": "lhmd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05349",
      "authors": [
        {
          "_id": "68424bed54a0d0e4b906baca",
          "name": "Hanoona Rasheed",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bacb",
          "name": "Abdelrahman Shaker",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bacc",
          "name": "Anqi Tang",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bacd",
          "name": "Muhammad Maaz",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bace",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bacf",
          "name": "Salman Khan",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bad0",
          "name": "Fahad Khan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64636b2551fa6e6306046293/rVaEoWuqnZnoewKuZDe09.mp4"
      ],
      "publishedAt": "2025-06-05T17:59:58.000Z",
      "submittedOnDailyAt": "2025-06-06T04:23:10.625Z",
      "title": "VideoMathQA: 비디오 기반의 다형식 이해에 의한 수학 논리 시험\n\n(Note: The original text \"VideoMathQA\" is a portmanteau of \"Video\" and \"MathQA,\" which is a term that combines \"Video\" with \"Mathematical Question Answering.\" The translation maintains this term's integrity while translating the rest of the text into Korean.)",
      "submittedOnDailyBy": {
        "_id": "64636b2551fa6e6306046293",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64636b2551fa6e6306046293/Uuz6z2MZb_LKLGM8uxF9s.jpeg",
        "isPro": false,
        "fullname": "Hanoona Rasheed",
        "user": "Hanoona",
        "type": "user"
      },
      "summary": "수학 비디오 데이터 세트에서 현실 세계에서의 수학적 추론은 정적 이미지나 텍스트에 비해 구조적으로 다른 문제입니다. 이는 미세한 시각 정보에 대한 해석, 손글씨나 디지털 텍스트를 정확히 읽는 것, 그리고 시간 경과에 따라 비선형적으로 분산된 어휘를 통합하는 필요로 합니다. 이러한 다모달의 컨텍스트에서의 성공은 시각적 인식뿐만 아니라 복잡한 뉴스 흐름에서 선택적으로 정확한 컨텍스트의 세부 사항을 특정하고 통합하는 데 의존합니다. 이에 대해 VIDEOMATHQA라는 벤치마크를 통해 모델이 비디오에서 이 시간적으로 확장된 크로스모달 추론을 수행할 수 있는지 평가하기 위한 프로젝트를 도입합니다. 이 벤치마크는 10가지의 수학 분야를 확장하고, 10초부터 1시간 이상의 비디오를 대상으로 합니다. 이는 구조화된 시각적 콘텐츠의 해석, 교육적인 나래션의 이해, 시각적, 음성, 텍스트의 모달 파일의 통합을 요구합니다. 이에 위해 대학원 수준의 전문가를 고용하여 고품질의 기록을 보장하고, 920 마인 허브 이상의 기록 시간을 보장했습니다. 현실적인 스캔을 반영하기 위해, 질문은 3가지의 핵심 추론 문제를 중심으로 설계되어 있습니다: 직접적인 문제 해결, 개념적인 트랜스폼, 그리고 깊은 교훈의 이해, 이는 확장된 설명과 일부의 해답을 포함하는 단계적인 이유를 통합하는 것입니다. 각 질문에는 단계적인 이유의 기록이 포함되어 있으며, 모델의 능력을 미세하게 진단할 수 있습니다. 이 벤치마크를 통해 현재의 접근 방식의 한계를 밝혀, 시간적으로 확장된 모달 풍부한 수학 문제 설정에서 이유를 수행하는 모델의 체계적인 평가 프레임워크를 구축합니다. VIDEOMATHQA의 벤치마크와 평가 코드는 https://mbzuai-oryx.github.io/VideoMathQA에서 이용할 수 있습니다.",
      "upvotes": 9,
      "discussionId": "68424bef54a0d0e4b906bb3e",
      "ai_summary": "VideoMathQA evaluates models' ability to perform temporally extended cross-modal reasoning across various mathematical domains in video settings, addressing direct problem solving, conceptual transfer, and deep instructional comprehension.",
      "ai_keywords": [
        "VideoMathQA",
        "temporally extended cross-modal reasoning",
        "structured visual content",
        "instructional narratives",
        "modality-rich",
        "multi-step reasoning",
        "partial solutions",
        "multi-step reasoning annotations",
        "system evaluation framework"
      ]
    },
    "publishedAt": "2025-06-05T13:59:58.000Z",
    "title": "VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal\n  Understanding in Videos",
    "summary": "Mathematical reasoning in real-world video settings presents a fundamentally\ndifferent challenge than in static images or text. It requires interpreting\nfine-grained visual information, accurately reading handwritten or digital\ntext, and integrating spoken cues, often dispersed non-linearly over time. In\nsuch multimodal contexts, success hinges not just on perception, but on\nselectively identifying and integrating the right contextual details from a\nrich and noisy stream of content. To this end, we introduce VideoMathQA, a\nbenchmark designed to evaluate whether models can perform such temporally\nextended cross-modal reasoning on videos. The benchmark spans 10 diverse\nmathematical domains, covering videos ranging from 10 seconds to over 1 hour.\nIt requires models to interpret structured visual content, understand\ninstructional narratives, and jointly ground concepts across visual, audio, and\ntextual modalities. We employ graduate-level experts to ensure high quality,\ntotaling over 920 man-hours of annotation. To reflect real-world scenarios,\nquestions are designed around three core reasoning challenges: direct problem\nsolving, where answers are grounded in the presented question; conceptual\ntransfer, which requires applying learned methods to new problems; and deep\ninstructional comprehension, involving multi-step reasoning over extended\nexplanations and partially worked-out solutions. Each question includes\nmulti-step reasoning annotations, enabling fine-grained diagnosis of model\ncapabilities. Through this benchmark, we highlight the limitations of existing\napproaches and establish a systematic evaluation framework for models that must\nreason, rather than merely perceive, across temporally extended and\nmodality-rich mathematical problem settings. Our benchmark and evaluation code\nare available at: https://mbzuai-oryx.github.io/VideoMathQA",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64636b2551fa6e6306046293/rVaEoWuqnZnoewKuZDe09.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05349.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64636b2551fa6e6306046293",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64636b2551fa6e6306046293/Uuz6z2MZb_LKLGM8uxF9s.jpeg",
      "fullname": "Hanoona Rasheed",
      "name": "Hanoona",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05345",
      "authors": [
        {
          "_id": "6842a3cb9393cafb521855aa",
          "name": "Adrian Łańcucki",
          "hidden": false
        },
        {
          "_id": "6842a3cb9393cafb521855ab",
          "name": "Konrad Staniszewski",
          "hidden": false
        },
        {
          "_id": "6842a3cb9393cafb521855ac",
          "name": "Piotr Nawrot",
          "hidden": false
        },
        {
          "_id": "6842a3cb9393cafb521855ad",
          "name": "Edoardo M. Ponti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-06T06:46:43.733Z",
      "title": "推論 시의 하이퍼 스케일링 및 KV 캐시 압축\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "640deb5d3c82bd463ee44735",
        "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
        "isPro": false,
        "fullname": "Piotr Nawrot",
        "user": "pnawrot",
        "type": "user"
      },
      "summary": "推論 시의 스케일링은 긴 또는 병렬적인 시퀀스의 생성으로 인하여 이유의 정확도를 향상시키고 효율성을 교환합니다. 그러나 Transformer LLMs에서 생성 비용은 KV 캐시의 크기에 의해 제한되어 생성된 토큰 수에 의한 제한이 아니라기 때문에 이 점을 조사하고 있습니다. 따라서, 추론 시의 하이퍼 스케일링을 조사합니다: KV 캐시를 압축하여 동일한 계산 버지 안에서 생성할 수 있는 토큰 수를 증가시키고 스케일링 추론의 정확도를 향상시킵니다. 그러나 이 접근법의 성공은 압축 메소드가 높은 압축 비율로도 정확도를 유지하는 것이 필요합니다. 하이퍼 스케일링의 실용성을 실현하기 위해 Dynamic Memory Sparsification (DMS)를 소개합니다. DMS는 새로운 방법で KV 캐시를 스패르시화하기 위한 것입니다. 1K 훈련 단계에서 8배의 압축을 달성하고 훈련이 없는 스패르시 배정보다 더 좋은 정확도를 유지합니다. DMS는 캐시된 토큰을 지연하여 제거하고 숨겨진 표현을 통합하여 중요한 정보를 유지합니다. DMS를 사용한 추론 시의 하이퍼 스케일링의 효과를 증명합니다. 여러 LLMs의 가족에 대해 상대적으로 추론 시간과 메모리 부담을 감소시키면서 정확도를 향상시킵니다. 예를 들어, Qwen-R1 32B는 AIME 24에서 평균 9.1점, GPQA에서 7.6점, LiveCodeBench에서 9.6점의 정확도 향상을 실현합니다.",
      "upvotes": 9,
      "discussionId": "6842a3cc9393cafb521855dd",
      "ai_summary": "Inference-time hyper-scaling with Dynamic Memory Sparsification in Transformer LLMs allows for increased token generation within the same compute budget by compressing the key-value cache, thereby enhancing accuracy.",
      "ai_keywords": [
        "inference-time hyper-scaling",
        "key-value (KV) cache",
        "Dynamic Memory Sparsification (DMS)",
        "token eviction",
        "representation merging",
        "AIME 24",
        "GPQA",
        "LiveCodeBench",
        "Qwen-R1 32B"
      ]
    },
    "publishedAt": "2025-06-05T13:59:55.000Z",
    "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
    "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8times compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05345.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640deb5d3c82bd463ee44735",
      "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
      "fullname": "Piotr Nawrot",
      "name": "pnawrot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05287",
      "authors": [
        {
          "_id": "68425719ba04d3ceff5bea29",
          "user": {
            "_id": "64a3fe3dde901eb01df12398",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
            "isPro": false,
            "fullname": "YuqianYuan",
            "user": "CircleRadon",
            "type": "user"
          },
          "name": "Yuqian Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:05.846Z",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2a",
          "name": "Ronghao Dang",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2b",
          "name": "Long Li",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2c",
          "name": "Wentong Li",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2d",
          "name": "Dian Jiao",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2e",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2f",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea30",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea31",
          "name": "Wenqiao Zhang",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea32",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea33",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/bFTB1kqlo-Oj0gQM7yzfe.png"
      ],
      "publishedAt": "2025-06-05T17:44:12.000Z",
      "submittedOnDailyAt": "2025-06-06T01:27:53.697Z",
      "title": "EOC-Bench: MLLMs는, 자기 중심의 세계에서 물체를 인식, 기억, 예측할 수 있는지?",
      "submittedOnDailyBy": {
        "_id": "64a3fe3dde901eb01df12398",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
        "isPro": false,
        "fullname": "YuqianYuan",
        "user": "CircleRadon",
        "type": "user"
      },
      "summary": "다모달 대언어 모델(MLLMs)의 등장은 중심적인 시각 애플리케이션에서 큰 진전을 촉발시켰습니다. 이러한 애플리케이션은 사용자가 동적으로 다양한 환경에서 도구와 상호작용하는 경우, 물체의 지속적이고 맥락에 관련된 이해를 필요로 합니다. 그러나 현재의 시각적 벤치마크는, 동적인 스케닝을 중심으로, 물체의 외관과 공간적 속성을 우선시하고, 사용자의 상호작용에 의한 동적인 변화를 평가하는 것을 잊어버렸습니다. 이러한 결점을 해결하기 위해, 우리는 EOC-Bench를 소개합니다. EOC-Bench는 동적인 중심적인 스케닝에서 물체 중심적인 시각적 인지를 체계적으로 평가하기 위해 설계된 혁신적인 벤치마크입니다. 특히, EOC-Bench는 과거, 현재, 미래의 3가지 시간계 분류로 나뉘어 3,277개의 세부적으로 설명된 QA 쌍을 특징으로 가지고 있으며, 11개의 세부적인 평가 차원과 3가지 시각적인 물체 참조를 덮어줍니다. 평가의 완벽성을 보장하기 위해, 우리는 4가지의微观 포맷의 사람이 로프 내의 Annotation 프레임워크를 개발하고, 오픈 엔드 포인트의 시간계 평가에 대한 새로운 다스케일 시간계 정확성 메트릭을 설계했습니다. EOC-Bench에 기반하여, 우리는 권리자의 프로프라이어, 오픈소스, 물체 수준의 MLLMs의 평가를 수행했습니다. EOC-Bench는 MLLMs의 시각적 물체 인지 능력의 발전을 촉발시키는 중요한 도구이며, 시각적 시스템의 신뢰성 있는 핵심 모델의 개발에 강력한 기초를 제공합니다.",
      "upvotes": 9,
      "discussionId": "6842571dba04d3ceff5beb34",
      "projectPage": "https://circleradon.github.io/EOCBench/",
      "githubRepo": "https://github.com/alibaba-damo-academy/EOCBench",
      "ai_summary": "EOC-Bench introduces a benchmark to evaluate dynamic object-centric cognition in egocentric vision applications, focusing on temporal and interactive aspects not covered by existing benchmarks.",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "egocentric vision",
        "embodied benchmarks",
        "object-centric embodied cognition",
        "QA pairs",
        "temporal accuracy metric"
      ]
    },
    "publishedAt": "2025-06-05T13:44:12.000Z",
    "title": "EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an\n  Egocentric World?",
    "summary": "The emergence of multimodal large language models (MLLMs) has driven\nbreakthroughs in egocentric vision applications. These applications necessitate\npersistent, context-aware understanding of objects, as users interact with\ntools in dynamic and cluttered environments. However, existing embodied\nbenchmarks primarily focus on static scene exploration, emphasizing object's\nappearance and spatial attributes while neglecting the assessment of dynamic\nchanges arising from users' interactions. To address this gap, we introduce\nEOC-Bench, an innovative benchmark designed to systematically evaluate\nobject-centric embodied cognition in dynamic egocentric scenarios. Specially,\nEOC-Bench features 3,277 meticulously annotated QA pairs categorized into three\ntemporal categories: Past, Present, and Future, covering 11 fine-grained\nevaluation dimensions and 3 visual object referencing types. To ensure thorough\nassessment, we develop a mixed-format human-in-the-loop annotation framework\nwith four types of questions and design a novel multi-scale temporal accuracy\nmetric for open-ended temporal evaluation. Based on EOC-Bench, we conduct\ncomprehensive evaluations of various proprietary, open-source, and object-level\nMLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object\ncognitive capabilities of MLLMs, establishing a robust foundation for\ndeveloping reliable core models for embodied systems.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/bFTB1kqlo-Oj0gQM7yzfe.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05287.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a3fe3dde901eb01df12398",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
      "fullname": "YuqianYuan",
      "name": "CircleRadon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02620",
      "authors": [
        {
          "_id": "68425ef33b5bb39c456487e0",
          "user": {
            "_id": "64049ae20ab5e22719f35103",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg",
            "isPro": false,
            "fullname": "Dongyu Yan",
            "user": "StarYDY",
            "type": "user"
          },
          "name": "Dongyu Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:56.223Z",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e1",
          "name": "Leyi Wu",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e2",
          "name": "Jiantao Lin",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e3",
          "name": "Luozhou Wang",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e4",
          "name": "Tianshuo Xu",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e5",
          "name": "Zhifei Chen",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e6",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e7",
          "name": "Lie Xu",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e8",
          "name": "Shunsi Zhang",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e9",
          "user": {
            "_id": "655cba1d87b67834000590e8",
            "avatarUrl": "/avatars/3bd43b7c9351f65b8f38f4c8237a0146.svg",
            "isPro": false,
            "fullname": "Yingcong Chen",
            "user": "yingcongchen",
            "type": "user"
          },
          "name": "Yingcong Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T03:22:29.750Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T08:36:03.000Z",
      "submittedOnDailyAt": "2025-06-06T02:21:59.705Z",
      "title": "FlexPainter: 유연하고 다각도 일관성 있는 테크스처 생성",
      "submittedOnDailyBy": {
        "_id": "64049ae20ab5e22719f35103",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg",
        "isPro": false,
        "fullname": "Dongyu Yan",
        "user": "StarYDY",
        "type": "user"
      },
      "summary": "3D 모델링의 중요한 부분 중 하나인 테크스틱 맵의 생성은 캐릭터의 렌더링 품질을 결정합니다. 최근, 확산 기반의 방법들은 테크스틱 생성의 새로운 길을 개척했습니다. 그러나 제어의 유연성과 제시 모델의 제한은 제작자가 원하는 결과를 만들 수 있는 것을 막고 있습니다. 또한 생성된 다점 이미지 사이의 불연속성은 테크스틱 생성의 품질을 저하시킵니다. 이러한 문제를 해결하기 위해, FlexPainter라는 새로운 테크스틱 생성 파이프라인을 소개합니다. 이 파이프라인은 유연한 다모델 조건付き 가이드를 가능하게 하며, 높은 일치성을 달성합니다. 공동 조건付き 매핑 공간을 구축하고, 서로 다른 입력 모델 사이에 유연한 통합을 수행합니다. 이 매핑 공간을 활용하여 이미지 기반의 CFG 방법을 제안하고, 구조적 및 스타일 정보를 분해하여 참조 이미지 기반의 스타일이식을 실현합니다. 이미지 확산에 포함되는 3D 지식을 활용하여, 처음으로 격자 표현을 사용하여 다점 이미지를 동시에 생성하고, 글로벌 이해를 강화합니다. 또한, 점차 샘플링의 때, 시점 동기화와 적응적인 가중치 모듈을 제안하여 로컬의 일치성을 보장합니다. 마지막으로, 3D 지식을 가진 테크스틱 완성 모델과 테크스틱 강화 모델을 조합하여, 연속적이고 고해상도의 테크스틱 맵을 생성합니다. 자세한 실험은, 우리의 프레임워크가 유연성과 생성 품질의 양면에서 대단히 최신의 방법보다 크게 뛰어넘는 것을 보여줍니다.",
      "upvotes": 8,
      "discussionId": "68425ef53b5bb39c4564888b",
      "projectPage": "https://starydy.xyz/FlexPainter/",
      "githubRepo": "https://github.com/StarRealMan/FlexPainter",
      "ai_summary": "FlexPainter, a novel texture generation pipeline, uses a shared conditional embedding space to enable flexible multi-modal guidance, ensuring high-quality and consistent texture map generation using image diffusion priors and a 3D-aware model.",
      "ai_keywords": [
        "diffusion-based methods",
        "texture generation",
        "flexible multi-modal conditional guidance",
        "conditional embedding space",
        "image-based CFG method",
        "structural information",
        "style information",
        "reference image-based stylization",
        "image diffusion prior",
        "grid representation",
        "view synchronization",
        "adaptive weighting module",
        "3D-aware texture completion model",
        "texture enhancement model"
      ]
    },
    "publishedAt": "2025-06-03T04:36:03.000Z",
    "title": "FlexPainter: Flexible and Multi-View Consistent Texture Generation",
    "summary": "Texture map production is an important part of 3D modeling and determines the\nrendering quality. Recently, diffusion-based methods have opened a new way for\ntexture generation. However, restricted control flexibility and limited prompt\nmodalities may prevent creators from producing desired results. Furthermore,\ninconsistencies between generated multi-view images often lead to poor texture\ngeneration quality. To address these issues, we introduce FlexPainter,\na novel texture generation pipeline that enables flexible multi-modal\nconditional guidance and achieves highly consistent texture generation. A\nshared conditional embedding space is constructed to perform flexible\naggregation between different input modalities. Utilizing such embedding space,\nwe present an image-based CFG method to decompose structural and style\ninformation, achieving reference image-based stylization. Leveraging the 3D\nknowledge within the image diffusion prior, we first generate multi-view images\nsimultaneously using a grid representation to enhance global understanding.\nMeanwhile, we propose a view synchronization and adaptive weighting module\nduring diffusion sampling to further ensure local consistency. Finally, a\n3D-aware texture completion model combined with a texture enhancement model is\nused to generate seamless, high-resolution texture maps. Comprehensive\nexperiments demonstrate that our framework significantly outperforms\nstate-of-the-art methods in both flexibility and generation quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02620.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64049ae20ab5e22719f35103",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg",
      "fullname": "Dongyu Yan",
      "name": "StarYDY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04209",
      "authors": [
        {
          "_id": "68413c8eb64ba498925da6a8",
          "user": {
            "_id": "65d45fbf9f087171b805c428",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d45fbf9f087171b805c428/bgxwcn2p_D9qEa5vynSxV.jpeg",
            "isPro": false,
            "fullname": "Jingfeng Yang",
            "user": "JingfengY",
            "type": "user"
          },
          "name": "Jingfeng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:26.842Z",
          "hidden": false
        },
        {
          "_id": "68413c8eb64ba498925da6a9",
          "user": {
            "_id": "64ea89932ca4ff1d53b77548",
            "avatarUrl": "/avatars/ce3df67ba3ea3197ebf74fbe5e2c0e48.svg",
            "isPro": false,
            "fullname": "Ziyang Wu",
            "user": "robinwuzy",
            "type": "user"
          },
          "name": "Ziyang Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:57.098Z",
          "hidden": false
        },
        {
          "_id": "68413c8eb64ba498925da6aa",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "68413c8eb64ba498925da6ab",
          "name": "Yi Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:51:56.000Z",
      "submittedOnDailyAt": "2025-06-06T00:43:44.611Z",
      "title": "언어-화상 대응을 고정한 텍스트 엔코더를 사용합니다.",
      "submittedOnDailyBy": {
        "_id": "65d45fbf9f087171b805c428",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d45fbf9f087171b805c428/bgxwcn2p_D9qEa5vynSxV.jpeg",
        "isPro": false,
        "fullname": "Jingfeng Yang",
        "user": "JingfengY",
        "type": "user"
      },
      "summary": "현재 언어와 이미지의 대응 관계를 확립하는 가장 우수한 접근 방식은 CLIP와 같은 비교 학습을 통해 문과 이미지 인코더를 함께 사전 학습하는 것입니다. 본 논문에서는 이러한 고가의 공통 학습이 필요할 수 있는지를 의심하고, 특히 고정된 규모의 언어 모델(LLM)이 더 좋은 문 인코더로 시각 표현 학습을 지도할 수 있는지를 검토합니다. 즉, LLM에서 고정된 문 인코더를 사용하여 언어-이미지의 대응 관계를 학습하기 위해, 이미지 인코더만 학습하는 LIFT(Language-Image alignment with a Fixed Text encoder)를 제안합니다. 이러한 방식은 구성적 이해와 긴 캡션에 대한 많은 시나리오에서 CLIP을 초과하는 높은 효율성을 나타내며, 계산 효율의 큰 향상을 얻을 수 있습니다. 본 논문은 LLM에서 문을 인코딩하여 시각 학습을 지도하는 방법을 체계적으로 조사하는 첫 번째 단계를 시작하며, 언어를 대응시키는 시각 표현의 학습의 설계 선택지를 제시합니다.",
      "upvotes": 7,
      "discussionId": "68413c8fb64ba498925da720",
      "projectPage": "https://jingfeng0705.github.io/LIFT/lift.html",
      "githubRepo": "https://github.com/Jingfeng0705/LIFT",
      "ai_summary": "Learning Language-Image alignment with a Fixed Text encoder (LIFT) using pre-trained large language models effectively guides visual representation learning, outperforming joint training methods like CLIP in compositional understanding and long captions.",
      "ai_keywords": [
        "contrastive learning",
        "CLIP",
        "pre-trained fixed large language model",
        "LLM",
        "Language-Image alignment",
        "LIFT",
        "image encoder",
        "compositional understanding",
        "long captions"
      ]
    },
    "publishedAt": "2025-06-04T13:51:56.000Z",
    "title": "Language-Image Alignment with Fixed Text Encoders",
    "summary": "Currently, the most dominant approach to establishing language-image\nalignment is to pre-train text and image encoders jointly through contrastive\nlearning, such as CLIP and its variants. In this work, we question whether such\na costly joint training is necessary. In particular, we investigate if a\npre-trained fixed large language model (LLM) offers a good enough text encoder\nto guide visual representation learning. That is, we propose to learn\nLanguage-Image alignment with a Fixed Text encoder (LIFT) from an LLM by\ntraining only the image encoder. Somewhat surprisingly, through comprehensive\nbenchmarking and ablation studies, we find that this much simplified framework\nLIFT is highly effective and it outperforms CLIP in most scenarios that involve\ncompositional understanding and long captions, while achieving considerable\ngains in computational efficiency. Our work takes a first step towards\nsystematically exploring how text embeddings from LLMs can guide visual\nlearning and suggests an alternative design choice for learning\nlanguage-aligned visual representations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04209.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65d45fbf9f087171b805c428",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d45fbf9f087171b805c428/bgxwcn2p_D9qEa5vynSxV.jpeg",
      "fullname": "Jingfeng Yang",
      "name": "JingfengY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01011",
      "authors": [
        {
          "_id": "6842746e8edd398d01b68e03",
          "name": "Siqi Hui",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e04",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e05",
          "name": "Sanping Zhou",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e06",
          "name": "Ye Deng",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e07",
          "name": "Wenli Huang",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e08",
          "name": "Jinjun Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T13:44:20.000Z",
      "submittedOnDailyAt": "2025-06-06T03:26:27.710Z",
      "title": "自動 재생 공격에 대응하는 언어 편향을 기반으로 한 이미지 마킹 방법: 접근법",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "自動회귀(AR) 이미지 생성 모델은 합성질의 발전으로 주목을 받았다. 이로 인하여, 水印을 숨겨 넣는 필요성이 높아졌고, 강력한 水印 기술이 필요하게 되었다. 그러나, 현재 생성 중의 水印 기술은 주로 디피션 모델을 위한 것으로, 水印은 디피션 잠재 상태 안에서 숨겨져 있습니다. 이 설계는 AR 모델에 대한 직접적인 적용에 큰 문제점을 불러일으키고 있습니다. 또한, 디피션 기반의 재생 공격은 잠재 상태의 변동으로 그 水印을 효과적으로 제거할 수 있습니다. 이러한 문제를 대처하기 위해, 우리는 Lexical Bias Watermarking(LBW)을 제안합니다. LBW는 AR 모델에 대한 설계를 하고, 재생 공격을 저항하는 것을 목표로 합니다. LBW는 생성 시에 토큰 선택을 특정의 「초록 리스트」에 바이어스하여 토큰 매핑에 직접 水印을 숨겨 넣습니다. 이 접근 방식은 현재의 AR 모델과 무간적 적합성을 보장하고, 후처리의 水印 기술에 자연스럽게 적용할 수 있습니다. 백박스 공격에 대한 안전성 향상을 위해, 디피션 기반의 재생 공격을 저항하기 위해, 「초록 리스트」를 사용하지 않고, 각 이미지에 대해 초록 리스트의 풀에서 랜덤하게 샘플링합니다. 水印 검출은 토큰 분포의定量화와 통계 분석에 의해 수행됩니다. 확장된 실험은 LBW가 재생 공격을 저항하는 것을 특히 보여, 水印의 강건성이 높은 것을 밝혀줍니다.",
      "upvotes": 7,
      "discussionId": "6842747b8edd398d01b69110",
      "ai_summary": "A novel watermarking technique, Lexical Bias Watermarking, enhances the security of autoregressive image generation models by embedding watermarks into token selection, demonstrating superior resistance to regeneration attacks.",
      "ai_keywords": [
        "autoregressive models",
        "in-generation watermarking",
        "diffusion models",
        "diffusion latent states",
        "token prediction",
        "regeneration attacks",
        "Lexical Bias Watermarking",
        "token maps",
        "green list",
        "watermark detection",
        "quantization",
        "statistical analysis",
        "token distribution"
      ]
    },
    "publishedAt": "2025-06-01T09:44:20.000Z",
    "title": "Autoregressive Images Watermarking through Lexical Biasing: An Approach\n  Resistant to Regeneration Attack",
    "summary": "Autoregressive (AR) image generation models have gained increasing attention\nfor their breakthroughs in synthesis quality, highlighting the need for robust\nwatermarking to prevent misuse. However, existing in-generation watermarking\ntechniques are primarily designed for diffusion models, where watermarks are\nembedded within diffusion latent states. This design poses significant\nchallenges for direct adaptation to AR models, which generate images\nsequentially through token prediction. Moreover, diffusion-based regeneration\nattacks can effectively erase such watermarks by perturbing diffusion latent\nstates. To address these challenges, we propose Lexical Bias Watermarking\n(LBW), a novel framework designed for AR models that resists regeneration\nattacks. LBW embeds watermarks directly into token maps by biasing token\nselection toward a predefined green list during generation. This approach\nensures seamless integration with existing AR models and extends naturally to\npost-hoc watermarking. To increase the security against white-box attacks,\ninstead of using a single green list, the green list for each image is randomly\nsampled from a pool of green lists. Watermark detection is performed via\nquantization and statistical analysis of the token distribution. Extensive\nexperiments demonstrate that LBW achieves superior watermark robustness,\nparticularly in resisting regeneration attacks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01011.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05209",
      "authors": [
        {
          "_id": "684247f35d537e0e5ecb724b",
          "name": "Nikhil Kandpal",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb724c",
          "name": "Brian Lester",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb724d",
          "name": "Colin Raffel",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb724e",
          "user": {
            "_id": "636071759ddc44e710e0f5ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636071759ddc44e710e0f5ce/-gmEhY5PidmSXIQPi2-QB.jpeg",
            "isPro": true,
            "fullname": "Sebastian Majstorovic",
            "user": "storytracer",
            "type": "user"
          },
          "name": "Sebastian Majstorovic",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:37.270Z",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb724f",
          "name": "Stella Biderman",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7250",
          "name": "Baber Abbasi",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7251",
          "name": "Luca Soldaini",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7252",
          "name": "Enrico Shippole",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7253",
          "name": "A. Feder Cooper",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7254",
          "name": "Aviya Skowron",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7255",
          "name": "John Kirchenbauer",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7256",
          "name": "Shayne Longpre",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7257",
          "name": "Lintang Sutawika",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7258",
          "name": "Alon Albalak",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7259",
          "name": "Zhenlin Xu",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725a",
          "name": "Guilherme Penedo",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725b",
          "name": "Loubna Ben Allal",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725c",
          "name": "Elie Bakouch",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725d",
          "name": "John David Pressman",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725e",
          "name": "Honglu Fan",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725f",
          "name": "Dashiell Stander",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7260",
          "name": "Guangyu Song",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7261",
          "name": "Aaron Gokaslan",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7262",
          "name": "Tom Goldstein",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7263",
          "name": "Brian R. Bartoldson",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7264",
          "name": "Bhavya Kailkhura",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7265",
          "name": "Tyler Murray",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T16:21:30.000Z",
      "submittedOnDailyAt": "2025-06-06T05:47:06.933Z",
      "title": "「공개 영역과 개방 许可 라이센스의 텍스트의 8TB 데이터 세트 Common Pile v0.1」",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 일반적으로 무허가 텍스트의 큰 양으로 훈련되어 있으며, 이러한 실습은 지적 재산권 침해의 가능성과 윤리적 우려로 조사되어 있습니다. LLMs에 개방된 허가 텍스트를 사용하여 훈련하는 것은 이러한 문제를 해결하는 첫 번째 단계이며, 선행의 데이터 수집의 노력을 통해 성능이 좋은 LLMs를 생성하기 위해 작은 크기 또는 낮은 품질의 데이터 세트를 생성했습니다. 이를 보완하기 위해, 8tev의 개방된 허가 텍스트의 컬렉션인 Common Pile v0.1을 수집, 준비, 릴리즈합니다. Common Pile은 연구 논문, 코드, 책, 엔지니어링, 교육 자료, 음성 번역 등 다양한 분야에서 30개의 소스로 구성됩니다. 중요하게도, 2개의 7억 파라미터의 LLMs를 Common Pile의 텍스트로 훈련하여 Comma v0.1-1T와 Comma v0.1-2T를 각각 1트릴리언과 2트릴리언의 토큰으로 훈련시켰습니다. 두 모델은 무허가 텍스트로 훈련된 LLMs와 같은 계산 바지움에서 상대적인 성능을 달성했습니다. Common Pile v0.1 자체를 릴리즈하는 것이 아니라, 그 제작에 사용된 코드 및 Comma v0.1 모델의 훈련 미션과 체크포인트도 함께 릴리즈합니다.",
      "upvotes": 6,
      "discussionId": "684247f85d537e0e5ecb73d3",
      "projectPage": "https://huggingface.co/common-pile",
      "githubRepo": "https://github.com/r-three/common-pile",
      "ai_summary": "The Common Pile v0.1 dataset, an 8-terabyte collection of openly licensed text, is used to train competitive 7 billion parameter LLMs.",
      "ai_keywords": [
        "Large language models",
        "LLMs",
        "openly licensed text",
        "Common Pile v0.1",
        "parameter-efficient fine-tuning",
        "Llama 1 and 2 7B",
        "training mixture",
        "checkpoints"
      ]
    },
    "publishedAt": "2025-06-05T12:21:30.000Z",
    "title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text",
    "summary": "Large language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. Training LLMs on\nopenly licensed text presents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performant LLMs. To address this gap, we collect, curate, and release\nthe Common Pile v0.1, an eight terabyte collection of openly licensed text\ndesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameter LLMs on text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance to LLMs trained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing the Common Pile v0.1 itself, we also release the\ncode used in its creation as well as the training mixture and checkpoints for\nthe Comma v0.1 models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05209.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2725
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04405",
      "authors": [
        {
          "_id": "6842454fbdc448822b2f1c03",
          "name": "Ran Xu",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c04",
          "name": "Yuchen Zhuang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c05",
          "name": "Yishan Zhong",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c06",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c07",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c08",
          "name": "Hang Wu",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c09",
          "name": "May D. Wang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0a",
          "name": "Peifeng Ruan",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0b",
          "name": "Donghan Yang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0c",
          "name": "Tao Wang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0d",
          "name": "Guanghua Xiao",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0e",
          "name": "Carl Yang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0f",
          "name": "Yang Xie",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c10",
          "user": {
            "_id": "65cae89119683f9817c049ea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg",
            "isPro": false,
            "fullname": "Wenqi Shi",
            "user": "wshi83",
            "type": "user"
          },
          "name": "Wenqi Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:39.845Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T19:38:55.000Z",
      "submittedOnDailyAt": "2025-06-06T00:21:54.285Z",
      "title": "MedAgentGym: MedAgentGym는 큰 규모의 코드 기반의 의료 논리 문제를 해결하기 위한 Large Language Model(LLM) 에이전트의 훈련을 위한 플랫폼입니다.",
      "submittedOnDailyBy": {
        "_id": "65cae89119683f9817c049ea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg",
        "isPro": false,
        "fullname": "Wenqi Shi",
        "user": "wshi83",
        "type": "user"
      },
      "summary": "MedAgentGYM은 최초로 공개되는 학습 환경입니다. 이 환경은 대규모 언어 모델(LLM) 에이전트의 의료 계획의 기초적인 강화를 위해 설계되었습니다. MedAgentGYM은 72,413개의 태스크 인스턴스를 포함하며, 129개 카테고리로 구성되어 있으며, 실제 의료 바이오 메디컬 시나리오에서 얻을 수 있습니다. 태스크는 실행 가능한 코딩 환경에 포함되고, 세부적인 태스크 설명, 상호작용 가능한 피드백 구조, 확인 가능한 실제 注釈, scalable한 학습 프로젝트 생성을 특징으로 합니다. 30개 이상의 LLM의 확장 검증에 의해, 비즈니스 API 기반 모델과 오픈 소스 컨트리뷰터 사이에서 뚜렷한 성능 차이를 확인했습니다. MedAgentGYM을 활용하여, Med-Copilot-7B는 감독 학습(+36.44%)과 지속적인 강화 학습(+42.47%)을 통해 큰 성능 향상을 얻으며, gpt-4o와 경쟁적인 가격대응으로 개인 정보 보호를 위해 선택肢로 등장했습니다. MedAgentGYM은 통일된 실행 환경 내의 카테고리별로 확장 가능한 세부적인 벤치마크와 접근 가능한 학습 툴킷을 제공하며, LLM 기반의 코딩 보조 프로그램 개발을 위한 통합 플랫폼으로 제공됩니다.",
      "upvotes": 4,
      "discussionId": "68424552bdc448822b2f1cd0",
      "githubRepo": "https://github.com/wshi83/MedAgentGym",
      "ai_summary": "MedAgentGYM, a training environment for coding-based medical reasoning in LLMs, enhances performance through supervised fine-tuning and reinforcement learning, providing a benchmark and expandable resource.",
      "ai_keywords": [
        "large language model",
        "MedAgentGYM",
        "task instances",
        "biomedical scenarios",
        "coding environments",
        "task descriptions",
        "interactive feedback",
        "ground-truth annotations",
        "training trajectories",
        "LLMs",
        "supervised fine-tuning",
        "reinforcement learning",
        "Med-Copilot-7B",
        "gpt-4o",
        "coding assistants",
        "biomedical research"
      ]
    },
    "publishedAt": "2025-06-04T15:38:55.000Z",
    "title": "MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at\n  Scale",
    "summary": "We introduce MedAgentGYM, the first publicly available training environment\ndesigned to enhance coding-based medical reasoning capabilities in large\nlanguage model (LLM) agents. MedAgentGYM comprises 72,413 task instances across\n129 categories derived from authentic real-world biomedical scenarios. Tasks\nare encapsulated within executable coding environments, each featuring detailed\ntask descriptions, interactive feedback mechanisms, verifiable ground-truth\nannotations, and scalable training trajectory generation. Extensive\nbenchmarking of over 30 LLMs reveals a notable performance disparity between\ncommercial API-based models and open-source counterparts. Leveraging\nMedAgentGYM, Med-Copilot-7B achieves substantial performance gains through\nsupervised fine-tuning (+36.44%) and continued reinforcement learning\n(+42.47%), emerging as an affordable and privacy-preserving alternative\ncompetitive with gpt-4o. By offering both a comprehensive benchmark and\naccessible, expandable training resources within unified execution\nenvironments, MedAgentGYM delivers an integrated platform to develop LLM-based\ncoding assistants for advanced biomedical research and practice.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04405.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cae89119683f9817c049ea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg",
      "fullname": "Wenqi Shi",
      "name": "wshi83",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20914",
      "authors": [
        {
          "_id": "68425a585738dda052ea4c91",
          "name": "Jianman Lin",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c92",
          "name": "Haojie Li",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c93",
          "name": "Chunmei Qing",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c94",
          "name": "Zhijing Yang",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c95",
          "name": "Liang Lin",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c96",
          "name": "Tianshui Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T09:05:28.000Z",
      "submittedOnDailyAt": "2025-06-06T01:33:43.629Z",
      "title": "거오메트리 변수이며, 외관 유지를 위한 물체 조합",
      "submittedOnDailyBy": {
        "_id": "6332e2689bf698ce68a22e8c",
        "avatarUrl": "/avatars/c1922acfda2e6d2fe7b03194a404eb10.svg",
        "isPro": false,
        "fullname": "JIANTAO LIN",
        "user": "LTT",
        "type": "user"
      },
      "summary": "일반 물체 구성(GOC)은 목표 물체를 背景 공간에 무간적으로 통합하고, 선택된 기하학적 특성을 유지하면서 그 微妙한 외관 세부 정보를 동시에 보존하는 것을 목표로 합니다. 최근의 접근법은 세ман틱 엔코딩을 구축하여, 이를 고급한 디피션 모델에 통합하여 기하학적인 에지렉션 가능한 생성을 가능하게 합니다. 그러나 이러한 높은 엔코딩은 그 상위 세ман틱 카테고리를만 설명하고, 微妙한 외관 세부 정보를 무용하게 버리는 실존합니다. 우리는 기하학적인 에지렉션과 외관을 보존할 수 있는 디세너드 기하학적 디피션(DGAD) 모델을 도입합니다. 이는 처음에, 세ман틱 엔코딩을 활용하여 선택된 기하학적 변형을 은닉적으로捉捉し, 다음으로, 微妙한 외관 특징을 기하학적인 에지렉션된 표현과 일치시키기 위한 교차 어텐션 검색 구조를 사용하여, 물체의 합성에서 정밀한 기하학적 에지렉션과 충실한 외관의 보존을 가능하게 합니다. 특히, DGAD는 CLIP/DINO로부터의 엔트리드 네트워크를 기반으로, 세ман틱 엔코딩과 외관 보존 표현을 추출하여, 이들은 분리되어 통합되며, 인코딩과 디코딩 파이프라인에 무간적으로 통합됩니다. 먼저, 강력한 공간 인식 능력을 가진 사전 학습된 디피션 모델에 세ман틱 엔코딩을 통합하여, 물체의 기하학을 은닉적으로捉捉し, 유연한 물체 조작을 가능하게 하며, 효과적인 편집 가능성을 보장합니다. 다음으로, 기하학적 에지렉션을 은닉적으로 학습한 물체의 엔트리를 활용하여, 외관 특징에 대응하는 영역과 공간적으로 일치시키기 위한 밀도있는 교차 어텐션 구조를 설계하여, 충실한 외관의 일관성을 보장합니다. 공개 벤치마크에서의 확장된 실험은 제안된 DGAD 프레임워크의 효과성을 보여주고 있습니다.",
      "upvotes": 4,
      "discussionId": "68425a595738dda052ea4ce4",
      "githubRepo": "https://github.com/jianmanlincjx/DGAD",
      "ai_summary": "The Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model effectively integrates target objects into background scenes by using semantic embeddings for geometry and cross-attention for appearance alignment.",
      "ai_keywords": [
        "disentangled geometry-editable",
        "appearance-preserving diffusion",
        "diffusion models",
        "cross-attention retrieval",
        "CLIP/DINO",
        "reference networks",
        "semantic embeddings",
        "appearance-preserving representations",
        "flexible object manipulation",
        "spatial reasoning capabilities",
        "dense cross-attention mechanism",
        "public benchmarks"
      ]
    },
    "publishedAt": "2025-05-27T05:05:28.000Z",
    "title": "Geometry-Editable and Appearance-Preserving Object Compositon",
    "summary": "General object composition (GOC) aims to seamlessly integrate a target object\ninto a background scene with desired geometric properties, while simultaneously\npreserving its fine-grained appearance details. Recent approaches derive\nsemantic embeddings and integrate them into advanced diffusion models to enable\ngeometry-editable generation. However, these highly compact embeddings encode\nonly high-level semantic cues and inevitably discard fine-grained appearance\ndetails. We introduce a Disentangled Geometry-editable and\nAppearance-preserving Diffusion (DGAD) model that first leverages semantic\nembeddings to implicitly capture the desired geometric transformations and then\nemploys a cross-attention retrieval mechanism to align fine-grained appearance\nfeatures with the geometry-edited representation, facilitating both precise\ngeometry editing and faithful appearance preservation in object composition.\nSpecifically, DGAD builds on CLIP/DINO-derived and reference networks to\nextract semantic embeddings and appearance-preserving representations, which\nare then seamlessly integrated into the encoding and decoding pipelines in a\ndisentangled manner. We first integrate the semantic embeddings into\npre-trained diffusion models that exhibit strong spatial reasoning capabilities\nto implicitly capture object geometry, thereby facilitating flexible object\nmanipulation and ensuring effective editability. Then, we design a dense\ncross-attention mechanism that leverages the implicitly learned object geometry\nto retrieve and spatially align appearance features with their corresponding\nregions, ensuring faithful appearance consistency. Extensive experiments on\npublic benchmarks demonstrate the effectiveness of the proposed DGAD framework.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20914.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6332e2689bf698ce68a22e8c",
      "avatarUrl": "/avatars/c1922acfda2e6d2fe7b03194a404eb10.svg",
      "fullname": "JIANTAO LIN",
      "name": "LTT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05348",
      "authors": [
        {
          "_id": "6842a994497e2b62234145d7",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145d8",
          "name": "Peishan Yang",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145d9",
          "name": "Zhen Xu",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145da",
          "name": "Jiaming Sun",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145db",
          "name": "Zhanhua Zhang",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145dc",
          "name": "Yong Chen",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145dd",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145de",
          "name": "Sida Peng",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145df",
          "name": "Xiaowei Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:59:57.000Z",
      "submittedOnDailyAt": "2025-06-06T07:18:31.313Z",
      "title": "무엇때 어디서든 자유로운 캄캄이 접근을 제공하는 솔루션, 동적인 스케쥴링 재구성에 적합하다.",
      "submittedOnDailyBy": {
        "_id": "6768fc1b75d8e8d042d26732",
        "avatarUrl": "/avatars/6e8c8b26effa41ba4073e69857b0c80a.svg",
        "isPro": false,
        "fullname": "Yifan Wang",
        "user": "wyf2020",
        "type": "user"
      },
      "summary": "이 논문은 복잡한 움직임을 갖는 동적인 3D 화면의 재구성에 대한 문제를 해결하는 데에 관한 것입니다. 최근의 연구에서는 표준 공간에서 3D 가우시안 특성을 정의하고, 변형 필드를 사용하여 표준 특성을 관측 공간으로 매핑하여 실시간 동적인 시각 합성을 실현했습니다. 그러나 이러한 방법들은 변형 필드의 최적화가 어려워, 복잡한 움직임을 갖는 화면을 처리하는 데 어려움을 겪습니다. 이러한 문제를 극복하기 위해, 우리는 FreeTimeGS를 제안합니다. 이것은 새로운 4D 표현이며, 가우시안 특성이 임의의 시간과 위치에서 나타날 수 있도록 합니다. 표준 가우시안 특성과 비교하여, 이 표현은 강한 유연성을 가지고 있으므로, 동적인 3D 화면의 모델링 능력이 향상됩니다. 또한 각 가우시안 특성에 동작 함수를 부여하고, 시간 동안 인접 영역으로 이동할 수 있도록 합니다. 여러 데이터 세트에 대한 실험 결과를 통해, 우리의 방법의 렌더링 품질은 최근의 방법과 비교하여 크게 초과합니다.",
      "upvotes": 3,
      "discussionId": "6842a996497e2b622341467e",
      "projectPage": "https://zju3dv.github.io/freetimegs/",
      "ai_summary": "A novel 4D representation, FreeTimeGS, enhances the modeling of dynamic 3D scenes by enabling Gaussian primitives to appear at arbitrary times and locations, improving rendering quality compared to existing methods.",
      "ai_keywords": [
        "3D Gaussian primitives",
        "canonical space",
        "deformation fields",
        "real-time dynamic view synthesis",
        "4D representation",
        "motion function",
        "temporal redundancy"
      ]
    },
    "publishedAt": "2025-06-05T13:59:57.000Z",
    "title": "FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene\n  Reconstruction",
    "summary": "This paper addresses the challenge of reconstructing dynamic 3D scenes with\ncomplex motions. Some recent works define 3D Gaussian primitives in the\ncanonical space and use deformation fields to map canonical primitives to\nobservation spaces, achieving real-time dynamic view synthesis. However, these\nmethods often struggle to handle scenes with complex motions due to the\ndifficulty of optimizing deformation fields. To overcome this problem, we\npropose FreeTimeGS, a novel 4D representation that allows Gaussian primitives\nto appear at arbitrary time and locations. In contrast to canonical Gaussian\nprimitives, our representation possesses the strong flexibility, thus improving\nthe ability to model dynamic 3D scenes. In addition, we endow each Gaussian\nprimitive with an motion function, allowing it to move to neighboring regions\nover time, which reduces the temporal redundancy. Experiments results on\nseveral datasets show that the rendering quality of our method outperforms\nrecent methods by a large margin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05348.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6768fc1b75d8e8d042d26732",
      "avatarUrl": "/avatars/6e8c8b26effa41ba4073e69857b0c80a.svg",
      "fullname": "Yifan Wang",
      "name": "wyf2020",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05282",
      "authors": [
        {
          "_id": "68425fce548d527097ac00bb",
          "name": "Tao Sun",
          "hidden": false
        },
        {
          "_id": "68425fce548d527097ac00bc",
          "name": "Liyuan Zhu",
          "hidden": false
        },
        {
          "_id": "68425fce548d527097ac00bd",
          "name": "Shengyu Huang",
          "hidden": false
        },
        {
          "_id": "68425fce548d527097ac00be",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "68425fce548d527097ac00bf",
          "name": "Iro Armeni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:36:03.000Z",
      "submittedOnDailyAt": "2025-06-06T01:56:27.167Z",
      "title": "Rectified Point Flow: 일반적인 점군 데이터의 자세 추정법",
      "submittedOnDailyBy": {
        "_id": "6503916e0905dd866fd129cb",
        "avatarUrl": "/avatars/818716460d9c9aaa056e0f1b43816c6a.svg",
        "isPro": false,
        "fullname": "Liyuan Zhu",
        "user": "liyzzz",
        "type": "user"
      },
      "summary": "Rectified Point Flow를 소개합니다. 이것은 1개의 조건부 생성 문제로 pairwise point cloud registration와 multi-part shape assembly를 통합한 파라미터화입니다. 무방향의 점 세트를 제공하면, 우리 방법은 연속적인 점별 속도장을 학습하여 노이즈를 포함하는 점을 목표 위치로 운반합니다. 이로써 부품의 자세가 복원됩니다. 기존 연구와 비교하여, 우리 방법은 각 부품의 자세 예측을 通用的な 대칭성 처리를 사용하였던 방식 대신, 내부적으로 대칭성을 학습하고 대칭성 레이블이 필요하지 않습니다. 자동 번역된 인코더와 결합하여, 우리 방법은 6개의 벤치마크에서 새로운 최고 수준의 성능을 달성합니다. 특히, 통일적인 공식화는 다양한 데이터 세트에 의한 효과적인 병렬 훈련을 가능하게 하고, 공통적인 기하학적 전파를 학습하여 정확도를 향상시킵니다. 프로젝트 페이지는 https://rectified-pointflow.github.io/입니다.",
      "upvotes": 3,
      "discussionId": "68425fcf548d527097ac011c",
      "projectPage": "https://rectified-pointflow.github.io/",
      "githubRepo": "https://github.com/GradientSpaces/Rectified-Point-Flow",
      "ai_summary": "Rectified Point Flow unifies pairwise point cloud registration and multi-part shape assembly through a continuous point-wise velocity field, achieving state-of-the-art performance on various benchmarks.",
      "ai_keywords": [
        "Rectified Point Flow",
        "pairwise point cloud registration",
        "multi-part shape assembly",
        "continuous point-wise velocity field",
        "self-supervised encoder",
        "overlapping points",
        "geometric priors"
      ]
    },
    "publishedAt": "2025-06-05T13:36:03.000Z",
    "title": "Rectified Point Flow: Generic Point Cloud Pose Estimation",
    "summary": "We introduce Rectified Point Flow, a unified parameterization that formulates\npairwise point cloud registration and multi-part shape assembly as a single\nconditional generative problem. Given unposed point clouds, our method learns a\ncontinuous point-wise velocity field that transports noisy points toward their\ntarget positions, from which part poses are recovered. In contrast to prior\nwork that regresses part-wise poses with ad-hoc symmetry handling, our method\nintrinsically learns assembly symmetries without symmetry labels. Together with\na self-supervised encoder focused on overlapping points, our method achieves a\nnew state-of-the-art performance on six benchmarks spanning pairwise\nregistration and shape assembly. Notably, our unified formulation enables\neffective joint training on diverse datasets, facilitating the learning of\nshared geometric priors and consequently boosting accuracy. Project page:\nhttps://rectified-pointflow.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05282.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6503916e0905dd866fd129cb",
      "avatarUrl": "/avatars/818716460d9c9aaa056e0f1b43816c6a.svg",
      "fullname": "Liyuan Zhu",
      "name": "liyzzz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00830",
      "authors": [
        {
          "_id": "684264d1a9584289f0053f5c",
          "name": "Zhengcong Fei",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f5d",
          "name": "Hao Jiang",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f5e",
          "user": {
            "_id": "65bef422fdb8d33cefeaccc3",
            "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
            "isPro": false,
            "fullname": "Qiu Di",
            "user": "diqiu7",
            "type": "user"
          },
          "name": "Di Qiu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:29.181Z",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f5f",
          "name": "Baoxuan Gu",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f60",
          "name": "Youqiang Zhang",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f61",
          "name": "Jiahua Wang",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f62",
          "name": "Jialin Bai",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f63",
          "name": "Debang Li",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f64",
          "name": "Mingyuan Fan",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f65",
          "name": "Guibin Chen",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f66",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T04:27:13.000Z",
      "submittedOnDailyAt": "2025-06-06T02:21:50.161Z",
      "title": "SkyReels-Audio: オーバニーエコーディションテーブルドラミングトランスフォーマーズ에 의한 영화 속 오디오 조건에 따라 표상된 테이블 토크 이미지",
      "submittedOnDailyBy": {
        "_id": "65bef422fdb8d33cefeaccc3",
        "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
        "isPro": false,
        "fullname": "Qiu Di",
        "user": "diqiu7",
        "type": "user"
      },
      "summary": "オーディオ条件付きのタイピングポートレットの生成と編集中、텍스트, 이미지, 영상 등 다양한 입력을 기반으로 안내하는 방법 아직까지 발견되지 않았습니다. 본 논문에서는, 고품질의 시퀀스적으로 일관된 タイピングポートレット 비디오의 합성을 목표로 하는 SkyReels-Audio의 연속적인 프레임워크를 제안합니다. 이 프레임워크는 사전 학습된 비디오 디퓨저 트랜스포머를 기반으로 구축되어 있으며, 무한 길이의 생성과 편집이 가능하며, 다양한 조건付け을 가능하게 합니다. 또한, 긴 비디오 시퀀스에서의 미세한 다양한 제어도 가능합니다. 이를 실현하기 위해, ファシールム의 움직임과 オーディオ의 동기를 발전적으로 조정하는 ダージュ 학습 전략을 사용합니다. 또한, ファシールム의 지역적 일관성을 향상시키기 위해, ファシールム 마스크 손실과 オーディオ 가이드 드라이버 프레임워크를 도입합니다. 더불어, 스ライディング ウィンドウ デノイズ 접근 방식을 사용하여, 시퀀스 간의 잠재 표현을 융합하여, 장기간 및 다양한 식별자에서 시각의 품질과 시퀀스 일관성을 보장합니다. 더욱 중요한 점은, 고품질의 サンクローナイション 된 オーディオ, 비디오, 텍스트를 포함하는 데이터 타입의 데이터 파이프라인을 구축했습니다. 세부적인 벤치마크 평가에 따라, SkyReels-Audio는, 입술 동기의 정확도, 식별자의 일관성, 사진의 현실감 등, 특히 복잡한 조건 하에서도 우수한 성능을 발휘합니다.",
      "upvotes": 3,
      "discussionId": "684264d2a9584289f0053fc9",
      "ai_summary": "SkyReels-Audio is a unified framework using pretrained video diffusion transformers for generating high-fidelity and coherent audio-conditioned talking portrait videos, supported by a hybrid curriculum learning strategy and advanced loss mechanisms.",
      "ai_keywords": [
        "video diffusion transformers",
        "infinite-length generation",
        "multimodal inputs",
        "hybrid curriculum learning",
        "facial mask loss",
        "classifier-free guidance mechanism",
        "sliding-window denoising",
        "lip-sync accuracy",
        "identity consistency",
        "realistic facial dynamics"
      ]
    },
    "publishedAt": "2025-06-01T00:27:13.000Z",
    "title": "SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video\n  Diffusion Transformers",
    "summary": "The generation and editing of audio-conditioned talking portraits guided by\nmultimodal inputs, including text, images, and videos, remains under explored.\nIn this paper, we present SkyReels-Audio, a unified framework for synthesizing\nhigh-fidelity and temporally coherent talking portrait videos. Built upon\npretrained video diffusion transformers, our framework supports infinite-length\ngeneration and editing, while enabling diverse and controllable conditioning\nthrough multimodal inputs. We employ a hybrid curriculum learning strategy to\nprogressively align audio with facial motion, enabling fine-grained multimodal\ncontrol over long video sequences. To enhance local facial coherence, we\nintroduce a facial mask loss and an audio-guided classifier-free guidance\nmechanism. A sliding-window denoising approach further fuses latent\nrepresentations across temporal segments, ensuring visual fidelity and temporal\nconsistency across extended durations and diverse identities. More importantly,\nwe construct a dedicated data pipeline for curating high-quality triplets\nconsisting of synchronized audio, video, and textual descriptions.\nComprehensive benchmark evaluations show that SkyReels-Audio achieves superior\nperformance in lip-sync accuracy, identity consistency, and realistic facial\ndynamics, particularly under complex and challenging conditions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00830.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65bef422fdb8d33cefeaccc3",
      "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
      "fullname": "Qiu Di",
      "name": "diqiu7",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04245",
      "authors": [
        {
          "_id": "68425054feb46a093178003f",
          "user": {
            "_id": "64ff4b1a0e8369f6a8c47c7e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ff4b1a0e8369f6a8c47c7e/X8ocOagj89gexSa253q_8.png",
            "isPro": false,
            "fullname": "Eric Lan",
            "user": "Eric-Lan",
            "type": "user"
          },
          "name": "Guangchen Lan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-06T02:20:41.949Z",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780040",
          "name": "Huseyin A. Inan",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780041",
          "user": {
            "_id": "65e88cdd95a27dfbf6b4e63b",
            "avatarUrl": "/avatars/3d2d270398f0824b392f99e158e94f26.svg",
            "isPro": false,
            "fullname": "Sahar Abdelnabi",
            "user": "sahar-abdelnabi",
            "type": "user"
          },
          "name": "Sahar Abdelnabi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T02:20:06.391Z",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780042",
          "name": "Janardhan Kulkarni",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780043",
          "user": {
            "_id": "6380a37a5c62156ce7dff8b9",
            "avatarUrl": "/avatars/fbe5a20869cb55ec43759c1b5f9c4135.svg",
            "isPro": false,
            "fullname": "Lukas Wutschitz",
            "user": "wulu",
            "type": "user"
          },
          "name": "Lukas Wutschitz",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-06T09:45:54.243Z",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780044",
          "name": "Reza Shokri",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780045",
          "name": "Christopher G. Brinton",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780046",
          "name": "Robert Sim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T21:26:21.000Z",
      "submittedOnDailyAt": "2025-06-06T00:52:31.028Z",
      "title": "LLM의 컨텍스트 필드의 일관성 문제에 대한 논리론과 강화학습",
      "submittedOnDailyBy": {
        "_id": "64ff4b1a0e8369f6a8c47c7e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ff4b1a0e8369f6a8c47c7e/X8ocOagj89gexSa253q_8.png",
        "isPro": false,
        "fullname": "Eric Lan",
        "user": "Eric-Lan",
        "type": "user"
      },
      "summary": "사용자를 대리하여 의사결정을 하는 자율적 에이전트의 시대가 시작되고, 이 분야의 중심적인 문제로 kontextual Integrity (CI)의 보장이 됩니다. CI는 특정한 작업 수행 시 적절한 정보를 공유할 수 있는지 판단하는 것입니다. 우리는 CI의 실현에는 에이전트가 수행 중인 kontextual 정보에 대해 이유를 제시하는 것이 필요하다고 주장합니다. 이를 검증하기 위해, 먼저 LLMs에 대해 CI와 관련된 이유를 명확히 할 수 있도록 유도하는 시도를 하였습니다. 그 후, 이 접근 방식을 확장하여 강화학습 (RL) 프레임워크를 개발하고, 모델에 필요한 이유를 더욱 깊게 제공하기 위해 노력했습니다. 합성된, 자동적으로 생성된 데이터 세트 (예: 700건의 데이터만 포함하며 다양한 kontextual 背景와 정보 공개 규칙을 포함)을 사용함으로써, 우리의 방법은 다양한 모델 크기와 가족에서 작업 성능을 유지하는 동시에, 부적절한 정보 공개를 크게 줄일 수 있음을 보여주었습니다. 중요한 점은, 이 합성된 데이터 세트에서 향상은 기존의 CI 벤치마크 (예: PrivacyLens)에 포함되는 인간 Annotation을 포함한 것으로, AI 보조 프로그램의 행동과 도구 호출에 대한 프라이버시 오류 평가에도 영향을 미칩니다.",
      "upvotes": 3,
      "discussionId": "68425056feb46a09317800d9",
      "ai_summary": "A reinforcement learning framework for LLMs enhances contextual integrity by reducing inappropriate information disclosure and maintaining task performance across various benchmarks.",
      "ai_keywords": [
        "LLMs",
        "reinforcement learning",
        "contextual integrity",
        "information disclosure",
        "synthetic dataset",
        "PrivacyLens",
        "privacy leakage",
        "AI assistants"
      ]
    },
    "publishedAt": "2025-05-29T17:26:21.000Z",
    "title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning",
    "summary": "As the era of autonomous agents making decisions on behalf of users unfolds,\nensuring contextual integrity (CI) -- what is the appropriate information to\nshare while carrying out a certain task -- becomes a central question to the\nfield. We posit that CI demands a form of reasoning where the agent needs to\nreason about the context in which it is operating. To test this, we first\nprompt LLMs to reason explicitly about CI when deciding what information to\ndisclose. We then extend this approach by developing a reinforcement learning\n(RL) framework that further instills in models the reasoning necessary to\nachieve CI. Using a synthetic, automatically created, dataset of only sim700\nexamples but with diverse contexts and information disclosure norms, we show\nthat our method substantially reduces inappropriate information disclosure\nwhile maintaining task performance across multiple model sizes and families.\nImportantly, improvements transfer from this synthetic dataset to established\nCI benchmarks such as PrivacyLens that has human annotations and evaluates\nprivacy leakage of AI assistants in actions and tool calls.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04245.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ff4b1a0e8369f6a8c47c7e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ff4b1a0e8369f6a8c47c7e/X8ocOagj89gexSa253q_8.png",
      "fullname": "Eric Lan",
      "name": "Eric-Lan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05278",
      "authors": [
        {
          "_id": "68426dfeb5f4d2d0f8fd098e",
          "user": {
            "_id": "60adfff0306d6873ec42d545",
            "avatarUrl": "/avatars/4a63f90638dbffebfeeee181a6d0220c.svg",
            "isPro": false,
            "fullname": "Nan",
            "user": "NanHUO",
            "type": "user"
          },
          "name": "Nan Huo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:24.419Z",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd098f",
          "name": "Jinyang Li",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0990",
          "name": "Bowen Qin",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0991",
          "name": "Ge Qu",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0992",
          "name": "Xiaolong Li",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0993",
          "name": "Xiaodong Li",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0994",
          "name": "Chenhao Ma",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0995",
          "name": "Reynold Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:33:02.000Z",
      "submittedOnDailyAt": "2025-06-06T02:57:34.041Z",
      "title": "미크로-액트:クエス트 대답에서 지식 충돌을 완화시키기 위한 행동 가능한 자기 사유 논리",
      "submittedOnDailyBy": {
        "_id": "6419435385030eca6ac94701",
        "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
        "isPro": false,
        "fullname": "Ge Qu",
        "user": "gq2138",
        "type": "user"
      },
      "summary": "レビュアル・アウゲーション(RAG) 시스템은 검색된 외부 지식이 대규모의 언어 모델(LLMs)의 고유의 파라미터 知識와 矛盾しているため、知識 コンフリクト に苦戦します。これは、問題回答(QA)などのダウンストラムタスクの性能に不利に影響します。現在のアプローチは、2つの知識ソースを並列で比較して コンフリクト を軽減することを試みていますが、これは過剰な コンテキスト をLLMsに負担させ、矛盾の識別と軽減において妨げることになります。この問題に対処するために、我々は、上下位の行動空間を持つ マイクロアクション(Micro-Act) フレームワークを提案します。このフレームワークは、コンテキスト の複雑さ を自動的に認識し、各知識ソースを細かい比較の順番に適応的に分解します。これらの比較は、行動可能なステップとして表現され、表面的な コンテキスト を超える推理により可能になります。5データセットの幅広い実験を通じて、Micro-Actはすべての5データセットと3コンフリクトタイプの上で、状態の最先端のベースラインと比較してQAの精度が顕著に向上します。特に、時間的や語義的なタイプでは、すべてのベースラインが顕著に失敗しているのに対して、Micro-Actは優れた性能を示します。より重要なのは、Micro-Actは非 コンフリクト の問題にも強固な性能を示し、実世界的なRAGアプリケーションでの実用的な価値を示しています。",
      "upvotes": 2,
      "discussionId": "68426dfeb5f4d2d0f8fd09c7",
      "ai_summary": "A framework called Micro-Act addresses Knowledge Conflicts in Retrieval-Augmented Generation by adaptively decomposing knowledge sources, leading to improved QA accuracy compared to existing methods.",
      "ai_keywords": [
        "Retrieval-Augmented Generation",
        "Knowledge Conflicts",
        "large language models",
        "parametric knowledge",
        "question answering",
        "hierarchical action space",
        "context complexity",
        "fine-grained comparisons",
        "actionable steps",
        "benchmark datasets",
        "QA accuracy",
        "conflict types",
        "temporal conflicts",
        "semantic conflicts",
        "non-conflict questions"
      ]
    },
    "publishedAt": "2025-06-05T13:33:02.000Z",
    "title": "Micro-Act: Mitigate Knowledge Conflict in Question Answering via\n  Actionable Self-Reasoning",
    "summary": "Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge\nConflicts, where retrieved external knowledge contradicts the inherent,\nparametric knowledge of large language models (LLMs). It adversely affects\nperformance on downstream tasks such as question answering (QA). Existing\napproaches often attempt to mitigate conflicts by directly comparing two\nknowledge sources in a side-by-side manner, but this can overwhelm LLMs with\nextraneous or lengthy contexts, ultimately hindering their ability to identify\nand mitigate inconsistencies. To address this issue, we propose Micro-Act a\nframework with a hierarchical action space that automatically perceives context\ncomplexity and adaptively decomposes each knowledge source into a sequence of\nfine-grained comparisons. These comparisons are represented as actionable\nsteps, enabling reasoning beyond the superficial context. Through extensive\nexperiments on five benchmark datasets, Micro-Act consistently achieves\nsignificant increase in QA accuracy over state-of-the-art baselines across all\n5 datasets and 3 conflict types, especially in temporal and semantic types\nwhere all baselines fail significantly. More importantly, Micro-Act exhibits\nrobust performance on non-conflict questions simultaneously, highlighting its\npractical value in real-world RAG applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6419435385030eca6ac94701",
      "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
      "fullname": "Ge Qu",
      "name": "gq2138",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05229",
      "authors": [
        {
          "_id": "6842bc6855574a112d5733cc",
          "name": "Danil Sivtsov",
          "hidden": false
        },
        {
          "_id": "6842bc6855574a112d5733cd",
          "name": "Ivan Rodkin",
          "hidden": false
        },
        {
          "_id": "6842bc6855574a112d5733ce",
          "name": "Gleb Kuzmin",
          "hidden": false
        },
        {
          "_id": "6842bc6855574a112d5733cf",
          "name": "Yuri Kuratov",
          "hidden": false
        },
        {
          "_id": "6842bc6855574a112d5733d0",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T16:43:48.000Z",
      "submittedOnDailyAt": "2025-06-06T08:35:33.323Z",
      "title": "대각 벡치 그리드에서 장기 컨텍스트의 재귀적 메모리 트랜스포머의 병렬화를 해제합니다.",
      "submittedOnDailyBy": {
        "_id": "618b9540682ec1c38327e586",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618b9540682ec1c38327e586/v_ZBkfh8O9Zh6C2YQpuBX.jpeg",
        "isPro": false,
        "fullname": "Yury Kuratov",
        "user": "yurakuratov",
        "type": "user"
      },
      "summary": "Transformer 모델은 긴 문맥 추론에 대해 2차원 시간과 선형 메모리 복잡성을 통해 어려움을 드러냅니다. Recurrent Memory Transformers (RMTs)는 이 문제를 해결하기 위해 아스イン 코스트를 선형 시간과 상수 메모리 사용으로 억제합니다. 그러나 그 메모리 업데이트 구조는 순차 실행에 의해 성능 백락을 일으킵니다.\n\n우리는 Segment별로 병렬성을 해제하면서 정확한 재귀를 유지하는 스케줄링 시나리오인 Diagonal Batching을 도입합니다. 이 접근법은 순차 제약을 제거하고 단일 긴 문맥 입력에서도 복잡한 배치와 파이프라인 기술이 필요하지 않도록 효율적인 GPU 추론을 가능하게 합니다. 이 방법은 모든 실행 시 계산 재배열이기 때문에 기존의 RMT 모델은 재학습이 필요하지 않습니다.\n\nDiagonal Batching이 적용된 LLaMA-1B ARMT 모델은 표준의 전체 주의 LLaMA-1B보다 3.3배의 속도업과 131,072 토큰 시퀀스의 순차 RMT 구현보다 1.8배의 속도업을 달성합니다. 순차 백락을 제거함으로써 Diagonal Batching은 추론 비용과 라틴 시퀀스를 줄이고 RMT의 실질적인 해결책으로의 강화를 실현합니다.",
      "upvotes": 2,
      "discussionId": "6842bc6955574a112d573421",
      "ai_summary": "Diagonal Batching enables parallel inference in Recurrent Memory Transformers, significantly improving speed and efficiency for long-context tasks.",
      "ai_keywords": [
        "Transformer models",
        "long-context inference",
        "quadratic time complexity",
        "linear memory complexity",
        "Recurrent Memory Transformers",
        "RMTs",
        "memory update mechanism",
        "sequential execution",
        "Diagonal Batching",
        "run-time computation reordering",
        "parallelism",
        "GPU inference",
        "LLaMA-1B ARMT model",
        "full-attention LLaMA-1B",
        "inference cost",
        "latency"
      ]
    },
    "publishedAt": "2025-06-05T12:43:48.000Z",
    "title": "Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers\n  for Long Contexts",
    "summary": "Transformer models struggle with long-context inference due to their\nquadratic time and linear memory complexity. Recurrent Memory Transformers\n(RMTs) offer a solution by reducing the asymptotic cost to linear time and\nconstant memory usage. However, their memory update mechanism leads to\nsequential execution, causing a performance bottleneck.\n  We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism\nacross segments in RMTs while preserving exact recurrence. This approach\neliminates the sequential constraint, enabling efficient GPU inference even for\nsingle long-context inputs without complex batching and pipelining techniques.\nBecause the technique is purely a run-time computation reordering, existing RMT\nmodels adopt it with no retraining.\n  Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup\nover standard full-attention LLaMA-1B and a 1.8x speedup over the sequential\nRMT implementation on 131,072-token sequences. By removing sequential\nbottleneck, Diagonal Batching reduces inference cost and latency, thereby\nstrengthening RMTs as a practical solution for real-world, long-context\napplications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05229.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "618b9540682ec1c38327e586",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618b9540682ec1c38327e586/v_ZBkfh8O9Zh6C2YQpuBX.jpeg",
      "fullname": "Yury Kuratov",
      "name": "yurakuratov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04734",
      "authors": [
        {
          "_id": "6842537f1c4f28a2031f499c",
          "user": {
            "_id": "632c30576bcb864974cc40a8",
            "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
            "isPro": false,
            "fullname": "sunlin",
            "user": "lincharliesun",
            "type": "user"
          },
          "name": "Lin Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:08.211Z",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f499d",
          "name": "Weihong Lin",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f499e",
          "name": "Jinzhu Wu",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f499f",
          "name": "Yongfu Zhu",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a0",
          "name": "Xiaoqi Jian",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a1",
          "name": "Guangxiang Zhao",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a2",
          "name": "Change Jia",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a3",
          "name": "Linglin Zhang",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a4",
          "name": "Sai-er Hu",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a5",
          "name": "Yuhan Wu",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a6",
          "name": "Xiangzheng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T08:09:11.000Z",
      "submittedOnDailyAt": "2025-06-06T01:04:27.438Z",
      "title": "평가는 모두 충분하다：평가 설계에 의한 LLM 추론 능력의 전략적 과평가",
      "submittedOnDailyBy": {
        "_id": "632c30576bcb864974cc40a8",
        "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
        "isPro": false,
        "fullname": "sunlin",
        "user": "lincharliesun",
        "type": "user"
      },
      "summary": "Deepseek-R1-Distill 시리즈의 모델은 수학, 과학, 프로그래밍, 기타 분야에서 강력한 성능을 보여주며 오픈 소스 커뮤니티에서 광범위하게 사용되고 있습니다. 그러나 우리의 연구에 따르면, 이들 모델의 벤치마크 평가 결과가 다양한 요인에 의해 큰 변동을 동반하고 있습니다. 평가 조건의 미묘한 차이는 결과가 크게 변화시킬 수 있습니다. 같은 현상은 Deepseek-R1-Distill 시리즈를 기반으로 微調節된 다른 오픈 소스 추론 모델, QwQ-32B 모델에도 보입니다. 이 모델의 주장된 성능 향상은 신뢰할 수 있는 재현이 되지 않습니다. 따라서, 우리는 모델의 성능 평가의 더 엄격한 패러다임의 구축을 주장하고, Deepseek-R1-Distill 시리즈 모델의 실험적 평가를 제공합니다.",
      "upvotes": 2,
      "discussionId": "684253811c4f28a2031f4a11",
      "ai_summary": "Empirical assessments reveal significant fluctuations in benchmark evaluation results of Deepseek-R1-Distill models, questioning the reliability of claimed performance improvements and advocating for a more rigorous evaluation paradigm.",
      "ai_keywords": [
        "reasoning models",
        "Deepseek-R1-Distill",
        "benchmark evaluation",
        "open-source inference models",
        "performance variations",
        "QwQ-32B model"
      ]
    },
    "publishedAt": "2025-06-05T04:09:11.000Z",
    "title": "Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning\n  Capabilities Through Evaluation Design",
    "summary": "Reasoning models represented by the Deepseek-R1-Distill series have been\nwidely adopted by the open-source community due to their strong performance in\nmathematics, science, programming, and other domains. However, our study\nreveals that their benchmark evaluation results are subject to significant\nfluctuations caused by various factors. Subtle differences in evaluation\nconditions can lead to substantial variations in results. Similar phenomena are\nobserved in other open-source inference models fine-tuned based on the\nDeepseek-R1-Distill series, as well as in the QwQ-32B model, making their\nclaimed performance improvements difficult to reproduce reliably. Therefore, we\nadvocate for the establishment of a more rigorous paradigm for model\nperformance evaluation and present our empirical assessments of the\nDeepseek-R1-Distill series models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04734.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632c30576bcb864974cc40a8",
      "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
      "fullname": "sunlin",
      "name": "lincharliesun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02751",
      "authors": [
        {
          "_id": "68425f88fa50fdb6ce3674b5",
          "user": {
            "_id": "67e6679e4b036872ccb9448d",
            "avatarUrl": "/avatars/04dff7f816046f6e82910c894db10ab1.svg",
            "isPro": false,
            "fullname": "fcyycf",
            "user": "fcy99",
            "type": "user"
          },
          "name": "Chuanyu Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:53.825Z",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674b6",
          "name": "Yuqi Zhang",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674b7",
          "name": "Kunbin Yao",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674b8",
          "name": "Guanying Chen",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674b9",
          "name": "Yuan Xiong",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674ba",
          "name": "Chuan Huang",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674bb",
          "name": "Shuguang Cui",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674bc",
          "name": "Xiaochun Cao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67e6679e4b036872ccb9448d/qy_cnBlPGQPb1e8qZyWGs.mp4"
      ],
      "publishedAt": "2025-06-03T11:13:48.000Z",
      "submittedOnDailyAt": "2025-06-06T06:39:38.846Z",
      "title": "RobustSplat: 밀도와 동력학에 의한 순간없는 3DGS\n\n(注意：原文中的\"密度と動力学の解離による瞬間無しの3DGS\"在翻译时，\"解離\"一词在韩语中没有直接对应的词汇，因此这里使用了\"에 의한\"来表达\"由...引起的\"，以保持句子的流畅性和准确性。)",
      "submittedOnDailyBy": {
        "_id": "67e6679e4b036872ccb9448d",
        "avatarUrl": "/avatars/04dff7f816046f6e82910c894db10ab1.svg",
        "isPro": false,
        "fullname": "fcyycf",
        "user": "fcy99",
        "type": "user"
      },
      "summary": "3D Gaussian Splatting (3DGS)는 새로운 시각을 합성하고 3D 모델링 분야에서 실시간 이미지 렌더링을 수행하기 위해 주목을 받고 있습니다. 그러나 현재의 방법들은 순간적인 물체로 영향을 받은 스케네를 정확하게 모델링하는 것이 어려워서 렌더링 이미지에 영향을 미칩니다. 우리는 Gaussian densification 프로세스가 스케네의 세부 사항을 효과적으로 추출할 때, 순간적인 디스텍뷰를 모델링하기 위해 추가적인 Gaussian를 생성하고 이 과정에서 영향을 미칠 것을 발견했습니다. 이를 해결하기 위해, 우리는 RobustSplat라는 강력한 해결책을 제안합니다. 이 해결책은 두 가지 중요한 설계에 기반합니다. 첫 번째로, Gaussian의 증식을 지연시키는 스테라지스트를 도입하고, 동적인 스케네 구조의 최적화를 우선시하여, 초기 최적화 단계에서 순간적인 물체로 인한 과적합을 억제합니다. 두 번째로, 스케일 연결 마스크 부스트루프 접근 방식을 설계하고, 저해상도의 특징 유사성 서브젝션을 사용하여 확실한 초기 순간 마스크의 추정을 수행하고, 그 강한 의미적 일관성과 노이즈의 견고성을 활용하여, 고해상도 서브젝션을 사용하여 더욱 정확한 마스크 예측을 실현하는 것을 목표로 합니다. 여러 어려운 데이터 세트에 대한 확장된 실험에 따라, 우리의 방법들은 현재의 방법들을 초월하고, 강건성과 효과성을 명확히 보여주었습니다. 우리의 프로젝트 페이지는 https://fcyycf.github.io/RobustSplat/ 입니다.",
      "upvotes": 2,
      "discussionId": "68425f8afa50fdb6ce367531",
      "projectPage": "https://fcyycf.github.io/RobustSplat/",
      "githubRepo": "https://github.com/fcyycf/RobustSplat",
      "ai_summary": "RobustSplat addresses artifacts in 3D Gaussian Splatting caused by transient objects through delayed Gaussian growth and scale-cascaded mask bootstrapping.",
      "ai_keywords": [
        "Gaussian Splatting",
        "novel-view synthesis",
        "3D modeling",
        "Gaussian densification",
        "transient objects",
        "Gaussian growth",
        "delayed Gaussian growth",
        "scale-cascaded mask bootstrapping",
        "feature similarity",
        "mask prediction"
      ]
    },
    "publishedAt": "2025-06-03T07:13:48.000Z",
    "title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free\n  3DGS",
    "summary": "3D Gaussian Splatting (3DGS) has gained significant attention for its\nreal-time, photo-realistic rendering in novel-view synthesis and 3D modeling.\nHowever, existing methods struggle with accurately modeling scenes affected by\ntransient objects, leading to artifacts in the rendered images. We identify\nthat the Gaussian densification process, while enhancing scene detail capture,\nunintentionally contributes to these artifacts by growing additional Gaussians\nthat model transient disturbances. To address this, we propose RobustSplat, a\nrobust solution based on two critical designs. First, we introduce a delayed\nGaussian growth strategy that prioritizes optimizing static scene structure\nbefore allowing Gaussian splitting/cloning, mitigating overfitting to transient\nobjects in early optimization. Second, we design a scale-cascaded mask\nbootstrapping approach that first leverages lower-resolution feature similarity\nsupervision for reliable initial transient mask estimation, taking advantage of\nits stronger semantic consistency and robustness to noise, and then progresses\nto high-resolution supervision to achieve more precise mask prediction.\nExtensive experiments on multiple challenging datasets show that our method\noutperforms existing methods, clearly demonstrating the robustness and\neffectiveness of our method. Our project page is\nhttps://fcyycf.github.io/RobustSplat/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67e6679e4b036872ccb9448d/qy_cnBlPGQPb1e8qZyWGs.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67e6679e4b036872ccb9448d",
      "avatarUrl": "/avatars/04dff7f816046f6e82910c894db10ab1.svg",
      "fullname": "fcyycf",
      "name": "fcy99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23115",
      "authors": [
        {
          "_id": "6842afaa6b5d1e675f254cf1",
          "name": "Yunshen Wang",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf2",
          "name": "Yicheng Liu",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf3",
          "name": "Tianyuan Yuan",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf4",
          "name": "Yucheng Mao",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf5",
          "name": "Yingshi Liang",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf6",
          "name": "Xiuyu Yang",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf7",
          "name": "Honggang Zhang",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf8",
          "name": "Hang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T05:34:22.000Z",
      "submittedOnDailyAt": "2025-06-06T07:37:16.625Z",
      "title": "3차원 차지율 예측을 위한 분산 기반의 자동주행 모델\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "634aab35dcf125e4dafc87b1",
        "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
        "isPro": false,
        "fullname": "YangXiuyu",
        "user": "gzzyyxy",
        "type": "user"
      },
      "summary": "3D 오케야녕 그리드의 결정을 시각 입력으로부터 정확하게 예측하는 것은 자동 운전에 중요하지만, 현재의 분류적인 방법들은 노이즈가 많은 데이터, 불완전한 관측, 3D 시에 있는 복잡한 구조에 대해 어려움을 겪습니다. 본 연구에서는, 3D 오케야녕 예측을 생성 모델링 태스크로 재구성하고, 잠재적인 데이터 분포를 학습하고, 3D 시의 전제 지식을 통합합니다. 이 접근 방식은 예측의 일관성, 노이즈의 견인도를 높이며, 3D 공간 구조의 복잡성을 더 잘 처리합니다. 확장된 실험에 따라, 3D 기반의 생성 모델은 가장 先端의 분류적인 접근을 초월하며, 더욱 현실적이고 정확한 오케야녕 예측을 제공하며, 특히 가려진거나 시각이 낮은 영역에서도 뛰어나습니다. 또한, 향상된 예측은 하류의 계획 태스크에 크게 이익을 줍니다. 우리의 방법의 실용적인 장점은 자동 운전의 실세계 엔드라인 애플리케이션에 명확히 나타납니다.",
      "upvotes": 2,
      "discussionId": "6842afac6b5d1e675f254db5",
      "ai_summary": "Diffusion models improve 3D occupancy prediction from visual inputs, enhancing accuracy and robustness in complex and occluded scenes, which benefits autonomous driving.",
      "ai_keywords": [
        "diffusion models",
        "generative modeling",
        "3D occupancy grids",
        "autonomous driving",
        "noise robustness",
        "3D scene priors",
        "downstream planning tasks"
      ]
    },
    "publishedAt": "2025-05-29T01:34:22.000Z",
    "title": "Diffusion-Based Generative Models for 3D Occupancy Prediction in\n  Autonomous Driving",
    "summary": "Accurately predicting 3D occupancy grids from visual inputs is critical for\nautonomous driving, but current discriminative methods struggle with noisy\ndata, incomplete observations, and the complex structures inherent in 3D\nscenes. In this work, we reframe 3D occupancy prediction as a generative\nmodeling task using diffusion models, which learn the underlying data\ndistribution and incorporate 3D scene priors. This approach enhances prediction\nconsistency, noise robustness, and better handles the intricacies of 3D spatial\nstructures. Our extensive experiments show that diffusion-based generative\nmodels outperform state-of-the-art discriminative approaches, delivering more\nrealistic and accurate occupancy predictions, especially in occluded or\nlow-visibility regions. Moreover, the improved predictions significantly\nbenefit downstream planning tasks, highlighting the practical advantages of our\nmethod for real-world autonomous driving applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23115.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634aab35dcf125e4dafc87b1",
      "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
      "fullname": "YangXiuyu",
      "name": "gzzyyxy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03238",
      "authors": [
        {
          "_id": "684158e2f11e4b2c51fce923",
          "user": {
            "_id": "6496eae78a7c70379a512e39",
            "avatarUrl": "/avatars/f5ab483ae93cc04b43e825dfd9440905.svg",
            "isPro": false,
            "fullname": "Ziheng Zhao",
            "user": "zzh99",
            "type": "user"
          },
          "name": "Ziheng Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T10:00:00.086Z",
          "hidden": false
        },
        {
          "_id": "684158e2f11e4b2c51fce924",
          "name": "Lisong Dai",
          "hidden": false
        },
        {
          "_id": "684158e2f11e4b2c51fce925",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "684158e2f11e4b2c51fce926",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "684158e2f11e4b2c51fce927",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:57:34.000Z",
      "submittedOnDailyAt": "2025-06-06T00:32:31.925Z",
      "title": "리셋된 전신 CT 영상 해석: 악성물 시노지시스의 접근법",
      "submittedOnDailyBy": {
        "_id": "6496eae78a7c70379a512e39",
        "avatarUrl": "/avatars/f5ab483ae93cc04b43e825dfd9440905.svg",
        "isPro": false,
        "fullname": "Ziheng Zhao",
        "user": "zzh99",
        "type": "user"
      },
      "summary": "CT 영상의 자동 번역 - 특히, 다 평면과 전신 스키م에서의 이상 검출의 위치와 설명 -은 임상 방사선의학에서 중대한 문제입니다. 본 연구는 다음과 같은 4가지 주요의 기여로 이 문제를 해결하는 것을 목표로 합니다: (i) 분류학에 대해, 고급 방사선 의사와 협력하여, 전체 체적 영역에 404건의 대표적인 이상 검출을 포함하는 한 개념의 분류 시스템을 제안합니다. (ii) 데이터에 대해, 다 평면과 전체 체적 영역에서 14,500건 이상의 CT 영상을 포함하는 데이터 세트를 제공하며, 19,000건 이상의 이상 검출에 대해 세부적인 게이팅 Annotation을 제공하며, 각 이상 검출에 대한 상세한 설명을 붙이고, 분류 시스템에 맞춤형 합니다. (iii) 모델 개발에 대해, OminiAbnorm-CT를 제안하며, 다 평면과 전신 CT 영상에서 이상 검출의 자동 게이팅과 설명을 수행할 수 있으며, 더불어 시각적 Prompt를 통해 유연한 상호작용을 가능하게 합니다. (iv) 벤치마크에 대해, 실제 임상 스크립트에 기반한 3가지의 대표적인 평가 태스크를 설정합니다. 확장된 실험을 통해, OminiAbnorm-CT는 모든 태스크와 메트릭에 대해 현재의 방법보다 유의적으로 우수함을 보여주는 것입니다.",
      "upvotes": 1,
      "discussionId": "684158e3f11e4b2c51fce9d7",
      "ai_summary": "OminiAbnorm-CT, a model for automated interpretation of CT images, outperforms existing methods in localizing and describing abnormalities across different body regions using text queries and visual prompts.",
      "ai_keywords": [
        "OminiAbnorm-CT"
      ]
    },
    "publishedAt": "2025-06-03T13:57:34.000Z",
    "title": "Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric\n  Approach",
    "summary": "Automated interpretation of CT images-particularly localizing and describing\nabnormal findings across multi-plane and whole-body scans-remains a significant\nchallenge in clinical radiology. This work aims to address this challenge\nthrough four key contributions: (i) On taxonomy, we collaborate with senior\nradiologists to propose a comprehensive hierarchical classification system,\nwith 404 representative abnormal findings across all body regions; (ii) On\ndata, we contribute a dataset containing over 14.5K CT images from multiple\nplanes and all human body regions, and meticulously provide grounding\nannotations for over 19K abnormalities, each linked to the detailed description\nand cast into the taxonomy; (iii) On model development, we propose\nOminiAbnorm-CT, which can automatically ground and describe abnormal findings\non multi-plane and whole-body CT images based on text queries, while also\nallowing flexible interaction through visual prompts; (iv) On benchmarks, we\nestablish three representative evaluation tasks based on real clinical\nscenarios. Through extensive experiments, we show that OminiAbnorm-CT can\nsignificantly outperform existing methods on all the tasks and metrics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03238.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6496eae78a7c70379a512e39",
      "avatarUrl": "/avatars/f5ab483ae93cc04b43e825dfd9440905.svg",
      "fullname": "Ziheng Zhao",
      "name": "zzh99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02587",
      "authors": [
        {
          "_id": "68428d18af4573dbb7cba864",
          "user": {
            "_id": "6526503e39fd3599e87c5c53",
            "avatarUrl": "/avatars/f45560d4f6bbb4f0dc06b27a46429726.svg",
            "isPro": false,
            "fullname": "Weiduo Yuan",
            "user": "Yewandou",
            "type": "user"
          },
          "name": "Weiduo Yuan",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T06:39:23.138Z",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba865",
          "name": "Jerry Li",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba866",
          "name": "Justin Yue",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba867",
          "name": "Divyank Shah",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba868",
          "name": "Konstantinos Karydis",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba869",
          "name": "Hang Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T08:07:18.000Z",
      "submittedOnDailyAt": "2025-06-06T05:17:25.580Z",
      "title": "BEVCALIB: ギャンブルビーストビュー에 의한 3D 기오메트리 가이드를 이용한 Lidar-카메라의 조정",
      "submittedOnDailyBy": {
        "_id": "6526503e39fd3599e87c5c53",
        "avatarUrl": "/avatars/f45560d4f6bbb4f0dc06b27a46429726.svg",
        "isPro": false,
        "fullname": "Weiduo Yuan",
        "user": "Yewandou",
        "type": "user"
      },
      "summary": "정확한 LiDAR-카메라 교정은 자율주행 시스템과 로봇 시스템의 다중모달 인식의 기본적인 중요성을 지닌다. 전통적인 교정 방법은 제어된 환경에서 다양한 데이터를 수집해야 하며, 차량/로봇의 동작 중 발생하는 변환 변화를 교정할 수 없다. 본 논문에서는 첫 번째 모델을 제안하고, 새비의 눈의 관점(BEV)의 특징을 사용하여 LiDAR-카메라 교정을 수행하는 것을 목표로 한다. 이를 실현하기 위해, 카메라의 BEV 특징과 LiDAR의 BEV 특징을 따로 추출하여 공통된 BEV 특징 공간에 융합시키는 것이 필요하다. BEV 특징으로부터의 구조 정보를 최대한 활용하기 위해, 새로운 특징 선택기를 도입하고, 가장 중요한 특징을 필터링하여 메모리 소비를 줄이고 효율적인 학습을 가능하게 한다. KITTI, NuScenes 및 우리의 데이터 세트에 대해 분산 평가를 수행하고, BEVCALIB가 새로운 최선으로 된 것을 보여주는 것을 보여준다. 다양한 노이즈 조건에서, KITTI 데이터 세트에서는 평균 (47.08%, 82.32%), NuScenes 데이터 세트에서는 평균 (78.17%, 68.29%)으로, 문헌에서 가장 좋은 기준과 비교하여 우위를 차지한다. 오픈 소스 분야에서, 최선의 재현 가능한 기준을 10배 이상 향상시킨다. 우리의 코드와 Demo 결과를 https://cisl.ucr.edu/BEVCalib에서 이용할 수 있다.",
      "upvotes": 1,
      "discussionId": "68428d1baf4573dbb7cba8f5",
      "projectPage": "https://cisl.ucr.edu/BEVCalib/",
      "githubRepo": "https://github.com/UCR-CISL/BEVCalib",
      "ai_summary": "BEVCALIB model uses bird's-eye view features for accurate LiDAR-camera calibration from raw data, demonstrating superior performance under various noise conditions.",
      "ai_keywords": [
        "bird's-eye view",
        "BEVCALIB",
        "camera BEV features",
        "LiDAR BEV features",
        "shared BEV feature space",
        "feature selector",
        "transformation decoder",
        "KITTI",
        "NuScenes",
        "reproducible baseline"
      ]
    },
    "publishedAt": "2025-06-03T04:07:18.000Z",
    "title": "BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View\n  Representations",
    "summary": "Accurate LiDAR-camera calibration is fundamental to fusing multi-modal\nperception in autonomous driving and robotic systems. Traditional calibration\nmethods require extensive data collection in controlled environments and cannot\ncompensate for the transformation changes during the vehicle/robot movement. In\nthis paper, we propose the first model that uses bird's-eye view (BEV) features\nto perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve\nthis, we extract camera BEV features and LiDAR BEV features separately and fuse\nthem into a shared BEV feature space. To fully utilize the geometric\ninformation from the BEV feature, we introduce a novel feature selector to\nfilter the most important features in the transformation decoder, which reduces\nmemory consumption and enables efficient training. Extensive evaluations on\nKITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes a\nnew state of the art. Under various noise conditions, BEVCALIB outperforms the\nbest baseline in the literature by an average of (47.08%, 82.32%) on KITTI\ndataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation,\nrotation), respectively. In the open-source domain, it improves the best\nreproducible baseline by one order of magnitude. Our code and demo results are\navailable at https://cisl.ucr.edu/BEVCalib.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02587.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6526503e39fd3599e87c5c53",
      "avatarUrl": "/avatars/f45560d4f6bbb4f0dc06b27a46429726.svg",
      "fullname": "Weiduo Yuan",
      "name": "Yewandou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04996",
      "authors": [
        {
          "_id": "68429955c49e8ad3f997b24a",
          "user": {
            "_id": "622dc11fe27c88667db093fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
            "isPro": false,
            "fullname": "Edoardo Bianchi",
            "user": "EdBianchi",
            "type": "user"
          },
          "name": "Edoardo Bianchi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T07:31:34.170Z",
          "hidden": false
        },
        {
          "_id": "68429955c49e8ad3f997b24b",
          "name": "Antonio Liotta",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T13:05:23.000Z",
      "submittedOnDailyAt": "2025-06-06T06:04:33.210Z",
      "title": "PATS: 스キル 인식에 기반한 시간계열 샘플링을 통한 다각도 스포츠 스キル 평가",
      "submittedOnDailyBy": {
        "_id": "622dc11fe27c88667db093fc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
        "isPro": false,
        "fullname": "Edoardo Bianchi",
        "user": "EdBianchi",
        "type": "user"
      },
      "summary": "자동화 스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스팸스",
      "upvotes": 0,
      "discussionId": "68429956c49e8ad3f997b288",
      "ai_summary": "PATS is a novel temporal sampling method that enhances video analysis of athletic skills by ensuring complete movement patterns are captured, outperforming existing methods across various domains.",
      "ai_keywords": [
        "Proficiency-Aware Temporal Sampling",
        "PATS",
        "EgoExo4D benchmark",
        "SkillFormer",
        "temporal continuity",
        "temporal coherence",
        "fundamental movement patterns",
        "dynamic sports",
        "sequential skills"
      ]
    },
    "publishedAt": "2025-06-05T09:05:23.000Z",
    "title": "PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill\n  Assessment",
    "summary": "Automated sports skill assessment requires capturing fundamental movement\npatterns that distinguish expert from novice performance, yet current video\nsampling methods disrupt the temporal continuity essential for proficiency\nevaluation. To this end, we introduce Proficiency-Aware Temporal Sampling\n(PATS), a novel sampling strategy that preserves complete fundamental movements\nwithin continuous temporal segments for multi-view skill assessment. PATS\nadaptively segments videos to ensure each analyzed portion contains full\nexecution of critical performance components, repeating this process across\nmultiple segments to maximize information coverage while maintaining temporal\ncoherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses\nthe state-of-the-art accuracy across all viewing configurations (+0.65% to\n+3.05%) and delivers substantial gains in challenging domains (+26.22%\nbouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that\nPATS successfully adapts to diverse activity characteristics-from\nhigh-frequency sampling for dynamic sports to fine-grained segmentation for\nsequential skills-demonstrating its effectiveness as an adaptive approach to\ntemporal sampling that advances automated skill assessment for real-world\napplications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622dc11fe27c88667db093fc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
      "fullname": "Edoardo Bianchi",
      "name": "EdBianchi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  }
]