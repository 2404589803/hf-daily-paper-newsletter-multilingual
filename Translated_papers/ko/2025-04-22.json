[
  {
    "paper": {
      "id": "2504.14945",
      "authors": [
        {
          "_id": "6806fdeda296cac1cf860505",
          "name": "Jianhao Yan",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf860506",
          "name": "Yafu Li",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf860507",
          "name": "Zican Hu",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf860508",
          "name": "Zhi Wang",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf860509",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf86050a",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf86050b",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf86050c",
          "name": "Yue Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T08:09:13.000Z",
      "submittedOnDailyAt": "2025-04-22T00:57:31.276Z",
      "title": "학습을 위한 오피리시스 가이드를 따라하는 이유에 대한 이해",
      "submittedOnDailyBy": {
        "_id": "6086838b19137b3a6ba760e7",
        "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
        "isPro": false,
        "fullname": "Jianhao Yan",
        "user": "Elliott",
        "type": "user"
      },
      "summary": "최근의 대형 추론 모형(LRMs)의 발전은 복수 단계 추론과 자기 반성 등 복잡한 행동이 단순한 규칙 기반 보상을 가지는 강화 학습(RL)에서 나타나는 것을 보여주고 있습니다. 그러나 현재의 RL의 0 시작 접근 방식은 고유의 '정책'으로 학습을 모델의 자체 출력에 제한하고 초기 능력을 초과한 추론 능력을 얻을 수 없다고 되어 있습니다. 우리는 LUFFY(정책 가이드에서 추론을 학습하는)라는 프레임워크를 소개합니다. LUFFY는 0RL에 정책 추론 추적 추가하고 학습 중 정책 로어 아웃과 결합하여 모사과 탐색을 동적으로 균형을 맞추는 것입니다. 특히, 우리는 표면적인 刚성 的 모사를 피하기 위해 정규화 중요도 샘플링을 이용한 혼합 정책 학습을 제안하고 있습니다. 또한, LUFFY는 6개의 수학 벤치마크에서 평균 7.0 이상의 이익을 달성하고, 분포 외 태스크에서 6.2점 이상의 우위를 차지하고 있습니다. 또한, 모사 기준의 규범적인 조정을 크게 초월하여 일반화에 큰 도움을 줍니다. 분석은 LUFFY는 모사를 효과적으로 수행하는 것이 아니라, 지도를 초과한 탐색을 수행함으로써 정책 가이드에서 가능한 추론 모형의 훈련을 교환 가능한 패스를 제공하고 있음을 보여줍니다.",
      "upvotes": 37,
      "discussionId": "6806fdeea296cac1cf860553",
      "githubRepo": "https://github.com/ElliottYan/LUFFY",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "zero-RL",
        "on-policy",
        "off-policy",
        "LUFFY",
        "imitation learning",
        "on-policy rollouts",
        "policy shaping",
        "regularized importance sampling",
        "mixed-policy training",
        "math benchmarks",
        "out-of-distribution tasks",
        "imitation-based supervised fine-tuning (SFT)",
        "generalizable reasoning models"
      ]
    },
    "publishedAt": "2025-04-21T04:09:13.000Z",
    "title": "Learning to Reason under Off-Policy Guidance",
    "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14945.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6086838b19137b3a6ba760e7",
      "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
      "fullname": "Jianhao Yan",
      "name": "Elliott",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15257",
      "authors": [
        {
          "_id": "68070ee593d1301c2f2ade99",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9a",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9b",
          "name": "Yufei He",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9c",
          "user": {
            "_id": "6214e4ee1e35c843d42d1f88",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
            "isPro": false,
            "fullname": "Longxu Dou",
            "user": "dreamerdeo",
            "type": "user"
          },
          "name": "Longxu Dou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:46.818Z",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9d",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9e",
          "name": "Zhijie Deng",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9f",
          "name": "Bryan Hooi",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2adea0",
          "name": "Min Lin",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2adea1",
          "name": "Tianyu Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:35:42.000Z",
      "submittedOnDailyAt": "2025-04-22T02:12:27.161Z",
      "title": "FlowReasoner: 쿼리 레벨 메타 에이전트의 강화",
      "submittedOnDailyBy": {
        "_id": "6650c77a74664a42ddfb9187",
        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
        "isPro": false,
        "fullname": "yueliu1999",
        "user": "yueliu1999",
        "type": "user"
      },
      "summary": "이 논문에서는 ForceReasoning Maester라는 쿼리 수준 메타 에이전트를 제안하고, 쿼리 수준의 다 에이전트 시스템의 자동화 설계를 목표로 합니다. 핵심 아이디어는 외부의 실행 피드백을 기반으로 논리적인 메타 에이전트를 장려하는 것입니다. 구체적으로, DeepSeek R1를 디스틸하여 FlowReasoner에 다 에이전트 시스템의 생성에 대한 기본적인 논리적 능력이 부여됩니다. 다음으로, 외부의 실행 피드백을 이용한 강화 학습(RL)을 추가로 수행합니다. 성능, 복잡성, 효율성 측면에서 다기능적인 보상을 설계하여, FlowReasoner는 각 사용자 쿼리에 대해专用의 다 에이전트 시스템을 생성할 수 있습니다. 공학 벤치마크와 컴페 코더 벤치마크의 양쪽에서의 실험은 FlowReasoner의 우수한 성능을 보여주었습니다. 특히, 3개의 벤치마크에서 o1-mini를 10.52%의 정확도 초과했습니다. 코드는 https://github.com/sail-sg/FlowReasoner에서 접근할 수 있습니다.",
      "upvotes": 27,
      "discussionId": "68070ee693d1301c2f2aded1",
      "ai_keywords": [
        "DeepSeek R1",
        "reinforcement learning",
        "reward",
        "performance",
        "complexity",
        "efficiency",
        "deliberative reasoning"
      ]
    },
    "publishedAt": "2025-04-21T13:35:42.000Z",
    "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
    "summary": "This paper proposes a query-level meta-agent named FlowReasoner to automate\nthe design of query-level multi-agent systems, i.e., one system per user query.\nOur core idea is to incentivize a reasoning-based meta-agent via external\nexecution feedback. Concretely, by distilling DeepSeek R1, we first endow the\nbasic reasoning ability regarding the generation of multi-agent systems to\nFlowReasoner. Then, we further enhance it via reinforcement learning (RL) with\nexternal execution feedback. A multi-purpose reward is designed to guide the RL\ntraining from aspects of performance, complexity, and efficiency. In this\nmanner, FlowReasoner is enabled to generate a personalized multi-agent system\nfor each user query via deliberative reasoning. Experiments on both engineering\nand competition code benchmarks demonstrate the superiority of FlowReasoner.\nRemarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks.\nThe code is available at https://github.com/sail-sg/FlowReasoner.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15257.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6650c77a74664a42ddfb9187",
      "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
      "fullname": "yueliu1999",
      "name": "yueliu1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15271",
      "authors": [
        {
          "_id": "680748d088578d9444349293",
          "name": "Guo Chen",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349294",
          "user": {
            "_id": "6582d86e58df0a2e21db80b8",
            "avatarUrl": "/avatars/a8245b1644183bd3ee7dc06b218c6e47.svg",
            "isPro": true,
            "fullname": "ZhiqiLi",
            "user": "RealZhiqiLi",
            "type": "user"
          },
          "name": "Zhiqi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:20.873Z",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349295",
          "name": "Shihao Wang",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349296",
          "name": "Jindong Jiang",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349297",
          "name": "Yicheng Liu",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349298",
          "name": "Lidong Lu",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349299",
          "name": "De-An Huang",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929a",
          "name": "Wonmin Byeon",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929b",
          "name": "Matthieu Le",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929c",
          "name": "Tuomas Rintamaki",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929d",
          "name": "Tyler Poon",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929e",
          "name": "Max Ehrlich",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929f",
          "name": "Tuomas Rintamaki",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a0",
          "name": "Tyler Poon",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a1",
          "name": "Tong Lu",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a2",
          "name": "Limin Wang",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a3",
          "name": "Bryan Catanzaro",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a4",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a5",
          "name": "Andrew Tao",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a6",
          "name": "Zhiding Yu",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a7",
          "name": "Guilin Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:57:28.000Z",
      "submittedOnDailyAt": "2025-04-22T06:14:17.808Z",
      "title": "Eagle 2.5: 프론티어 비전 런그라우트 컨텍스트의 후처리 훈련을 강화합니다.",
      "submittedOnDailyBy": {
        "_id": "6392c73390b8e99a6779a7b0",
        "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
        "isPro": false,
        "fullname": "Guo Chen",
        "user": "cg1177",
        "type": "user"
      },
      "summary": "Eagle 2.5, 先端적인 시각 언어 모델(VLMs)의 가족을 소개합니다. 우리의 연구는 긴 비디오 이해와 고해상도 이미지 이해의 도전을 해결하고, 두 가지 작업에 적용 가능한 일반적인 프레임워크를 제안합니다. 제안된 훈련 프레임워크에는上下文의 보완과 시각적인 세부 사항을 유지하기 위해 자동적인 저해상도 샘플링과 이미지 영역 보존의 두 가지 기술이 내장되어 있습니다. 프레임워크는 긴 비디오 데이터의 훈련에 대한 파이프라인 내의 많은 효율화를 포함합니다. 마지막으로, Eagle-Video-110K라는 새로운 데이터 세트를 제안하고, 이는 비디오 수준과 클립 수준의 설명을 통합하여 긴 비디오 이해를 촉진하는 것입니다. Eagle 2.5는 긴 비디오 다모달 벤치마크에서 큰 개선을 나타내며, 현재의 VLMs의 한계를 해결하는 강력한 해결책을 제공합니다. 특히, 우리의 최고의 모델 Eagle 2.5-8B는 512 프레임의 입력을 사용하여 Video-MME에서 72.4%를 달성하고, GPT-4o, Qwen2.5-VL-72B, InternVL2.5-78B와 같은 최상위 기업 모델 및 큰 규모의 오픈 소스 모델의 결과를 초월했습니다.",
      "upvotes": 24,
      "discussionId": "680748d188578d9444349311",
      "projectPage": "https://nvlabs.github.io/EAGLE/",
      "githubRepo": "https://github.com/NVlabs/EAGLE",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "long-context multimodal learning",
        "long video comprehension",
        "high-resolution image understanding",
        "Automatic Degrade Sampling",
        "Image Area Preservation",
        "efficiency optimizations",
        "long-context data training",
        "Eagle-Video-110K",
        "story-level annotations",
        "clip-level annotations",
        "long-video understanding",
        "multimodal benchmarks",
        "Video-MME"
      ]
    },
    "publishedAt": "2025-04-21T13:57:28.000Z",
    "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier\n  Vision-Language Models",
    "summary": "We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)\nfor long-context multimodal learning. Our work addresses the challenges in long\nvideo comprehension and high-resolution image understanding, introducing a\ngeneralist framework for both tasks. The proposed training framework\nincorporates Automatic Degrade Sampling and Image Area Preservation, two\ntechniques that preserve contextual integrity and visual details. The framework\nalso includes numerous efficiency optimizations in the pipeline for\nlong-context data training. Finally, we propose Eagle-Video-110K, a novel\ndataset that integrates both story-level and clip-level annotations,\nfacilitating long-video understanding. Eagle 2.5 demonstrates substantial\nimprovements on long-context multimodal benchmarks, providing a robust solution\nto the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B\nachieves 72.4% on Video-MME with 512 input frames, matching the results of\ntop-tier commercial model such as GPT-4o and large-scale open-source models\nlike Qwen2.5-VL-72B and InternVL2.5-78B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15271.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6392c73390b8e99a6779a7b0",
      "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
      "fullname": "Guo Chen",
      "name": "cg1177",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13958",
      "authors": [
        {
          "_id": "6806fe1bec9fb3764b875ea0",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea1",
          "user": {
            "_id": "63888d3fd68e37abd599f428",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
            "isPro": true,
            "fullname": "emre can",
            "user": "emrecanacikgoz",
            "type": "user"
          },
          "name": "Emre Can Acikgoz",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:57.521Z",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea2",
          "name": "Qi He",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea3",
          "name": "Hongru Wang",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea4",
          "name": "Xiusi Chen",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea5",
          "name": "Dilek Hakkani-Tür",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea6",
          "name": "Gokhan Tur",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea7",
          "name": "Heng Ji",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/v-Pmd0mDnq-v5479tRcgC.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/pCPBgXm4IAZdhYjc-ZSrw.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/0c5XM8ZGXnieGz3IzuYTh.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/omlm8xwODvorHTUkfFOgR.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/1n73QmTy-PA5FfFQJMTej.png"
      ],
      "publishedAt": "2025-04-16T21:45:32.000Z",
      "submittedOnDailyAt": "2025-04-22T01:37:59.400Z",
      "title": "ToolRL: 보상은 모든 툴 학습에 필수적인 것입니다.",
      "submittedOnDailyBy": {
        "_id": "63888d3fd68e37abd599f428",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
        "isPro": true,
        "fullname": "emre can",
        "user": "emrecanacikgoz",
        "type": "user"
      },
      "summary": "현재의 대규모 언어 모델(LLMs)는 일반적으로 규칙적인 미세 조정(SFT)을 통해 툴 사용 능력을 얻는다. 그러나 SFT는 불숙하고 복잡한 툴 사용 시나리오에 대한 일반화는 어려워진다. 최근의 강화학습(RL)의 발전, 특히 R1 모델과 같은 모델에 있어서, 설명과 일반화 능력이 기대될 수 있다는 것을 보여주고 있다. 그러나 툴 사용의 보상 설계는 특별한 문제를 동반한다: 여러 툴이 다양한 파라미터로 호출되고, 답의 일치만 coarse-grained의 보상 신호로, 유효한 학습에 필요한 fine-grained의 피드백을 제공하지 못한다. 본 논문에서는 RL 패러다임 내에서 툴 선택과 적용 태스크의 보상 설계에 대한 초기의 상세한 연구를 수행한다. 보상 전략의 광범위한 범위를 체계적으로 탐색하고, 그 종류, 규모, 거니루티, 시간적인 동작을 분석한다. 이러한 통찰에 기초하여, 툴 사용 태스크에 적합한 원칙적인 보상 설계를 제안하고, 그룹 상대적 정책 최적화(GRPO)을 사용하여 LLMs를 훈련한다. 다양한 벤치마크에서의 실험 평가는 우리의 접근 방식이 충격적이고 scalable하며 안정적인 훈련을 실현하고, 기본 모델보다 17%의 향상과 SFT 모델보다 15%의 효과를 얻는 것을 보여준다. 이러한 결과를 통해, LLMs의 툴 사용 능력과 일반화 성능을 향상시키기 위한 고려한 보상 설계의 중요성을 명확히 한다. 모든 코드는 향후 연구를 촉진하기 위해 공개된다.",
      "upvotes": 19,
      "discussionId": "6806fe1dec9fb3764b875f0c",
      "githubRepo": "https://github.com/qiancheng0/ToolRL",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "R1-like models",
        "reasoning and generalization",
        "tool selection",
        "tool application",
        "reward design",
        "finegrained feedback",
        "reward strategies",
        "Group Relative Policy Optimization (GRPO)"
      ]
    },
    "publishedAt": "2025-04-16T17:45:32.000Z",
    "title": "ToolRL: Reward is All Tool Learning Needs",
    "summary": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/v-Pmd0mDnq-v5479tRcgC.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/pCPBgXm4IAZdhYjc-ZSrw.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/0c5XM8ZGXnieGz3IzuYTh.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/omlm8xwODvorHTUkfFOgR.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/1n73QmTy-PA5FfFQJMTej.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13958.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63888d3fd68e37abd599f428",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
      "fullname": "emre can",
      "name": "emrecanacikgoz",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15281",
      "authors": [
        {
          "_id": "68072d05362af0cf18fb4b0c",
          "name": "Cailin Zhuang",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b0d",
          "name": "Yaoqi Hu",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b0e",
          "name": "Xuanyang Zhang",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b0f",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:44.772Z",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b10",
          "name": "Jiacheng Bao",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b11",
          "name": "Shengqi Liu",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b12",
          "user": {
            "_id": "67da6acc05101e8e1d2c20a2",
            "avatarUrl": "/avatars/1cfa3a1f59687db58af4e1b4a8767bfd.svg",
            "isPro": false,
            "fullname": "Wang",
            "user": "Yiying12",
            "type": "user"
          },
          "name": "Yiying Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:42.183Z",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b13",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b14",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b15",
          "name": "Ming Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/EeggKsuKNbiiygcWDKxQZ.webm"
      ],
      "publishedAt": "2025-04-21T17:59:55.000Z",
      "submittedOnDailyAt": "2025-04-22T04:18:58.848Z",
      "title": "スタイルメ3D: 3D 가우시안 위의 복수 Encoder를 이용한 분리된 초기값을 이용한 스타일화",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "3D Gaussian Splatting (3DGS)는 현실적인 스케니팅의 재구성에 뛰어넘는 장점이 있지만, 캐릭터, 게임 등 스타일화된 콘텐츠에서는 테ク스처의 파괴, 의미적인 오차, 추상적인 미술적 특성에 대한 적응성 제한이 나타날 수 있습니다. 우리는 3D GS 스타일화의 종합적인 프레임워크인 StyleMe3D를 제안합니다. 이 프레임워크는 다모달 스타일 조건, 다레벨 의미적 조정, 시각적 품질 향상을 통합합니다. 우리의 주요 아이디어는 다음과 같습니다: (1) RGB 속성만 최적화하면 스타일화에서 기하학의 조화성을 유지합니다. (2) 저레벨, 중레벨, 고레벨의 의미를 분리하는 것은 스타일화의 일관성에 중요합니다. (3) 분리된 물체나 복잡한 스케니징의 스케일러블성을 실제적인 적용에 필요합니다. StyleMe3D는 네 개의 새로운 컴포넌트를 도입합니다: Dynamic Style Score Distillation (DSSD), 의미적인 조정을 위해 Stable Diffusion의 잠재 공간, Localized, Content-Aware Texture Transfer를 위한 Contrastive Style Descriptor (CSD), 그리고 스타일의 세부 사항과 구조적 일관성을 분리하기 위한 3D Gaussian Quality Assessment (3DG-QA), Simultaneously Optimized Scale (SOS), 그리고 인간 평가 데이터에 기반한 미분 가능한 예술적 선도를 사용하여 예술적인 스타일 조정과 시각적 조화 향상을 추구합니다. StyleMe3D는 NERF의 합성 데이터셋 (물체)과 tantdt db (스케니징) 데이터셋을 평가하여, 기하학의 세부 사항을 보존하고 전체 스케니징의 스타일의 일관성을 보장함으로써 최신 방법보다 뛰어넘는 성능을 보입니다. 이 연구는 현실적인 3D GS와 예술적인 스타일화의 결합을 통해 게임, 가상 세계, 디지털 아트의 응용을 개발합니다.",
      "upvotes": 18,
      "discussionId": "68072d08362af0cf18fb4c7b",
      "projectPage": "https://styleme3d.github.io/",
      "githubRepo": "https://github.com/AIGCResearch/styleme3d",
      "ai_keywords": [
        "Gaussian Splatting",
        "StyleMe3D",
        "multi-modal style conditioning",
        "multi-level semantic alignment",
        "perceptual quality enhancement",
        "Dynamic Style Score Distillation",
        "Stable Diffusion",
        "latent space",
        "Contrastive Style Descriptor",
        "localized, content-aware texture transfer",
        "Simultaneously Optimized Scale",
        "3D Gaussian Quality Assessment",
        "differentiable aesthetic prior",
        "NeRF synthetic dataset",
        "tandt db",
        "preserving geometric details",
        "stylistic consistency",
        "real-time rendering"
      ]
    },
    "publishedAt": "2025-04-21T13:59:55.000Z",
    "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on\n  3D Gaussians",
    "summary": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction\nbut struggles with stylized scenarios (e.g., cartoons, games) due to fragmented\ntextures, semantic misalignment, and limited adaptability to abstract\naesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer\nthat integrates multi-modal style conditioning, multi-level semantic alignment,\nand perceptual quality enhancement. Our key insights include: (1) optimizing\nonly RGB attributes preserves geometric integrity during stylization; (2)\ndisentangling low-, medium-, and high-level semantics is critical for coherent\nstyle transfer; (3) scalability across isolated objects and complex scenes is\nessential for practical deployment. StyleMe3D introduces four novel components:\nDynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent\nspace for semantic alignment; Contrastive Style Descriptor (CSD) for localized,\ncontent-aware texture transfer; Simultaneously Optimized Scale (SOS) to\ndecouple style details and structural coherence; and 3D Gaussian Quality\nAssessment (3DG-QA), a differentiable aesthetic prior trained on human-rated\ndata to suppress artifacts and enhance visual harmony. Evaluated on NeRF\nsynthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D\noutperforms state-of-the-art methods in preserving geometric details (e.g.,\ncarvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,\ncoherent lighting in landscapes), while maintaining real-time rendering. This\nwork bridges photorealistic 3D GS and artistic stylization, unlocking\napplications in gaming, virtual worlds, and digital art.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/EeggKsuKNbiiygcWDKxQZ.webm"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15281.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14396",
      "authors": [
        {
          "_id": "6806fcc4f349e60f6c1b928c",
          "user": {
            "_id": "630461624ec2dfa82a5ad7e7",
            "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
            "isPro": false,
            "fullname": "Minho Park",
            "user": "mpark",
            "type": "user"
          },
          "name": "Minho Park",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:59.880Z",
          "hidden": false
        },
        {
          "_id": "6806fcc4f349e60f6c1b928d",
          "name": "Taewoong Kang",
          "hidden": false
        },
        {
          "_id": "6806fcc4f349e60f6c1b928e",
          "user": {
            "_id": "6369f693bf21b20c5692937b",
            "avatarUrl": "/avatars/e937dc8234b3e456149882bfce34841f.svg",
            "isPro": false,
            "fullname": "Jooyeol Yun",
            "user": "YeolJoo",
            "type": "user"
          },
          "name": "Jooyeol Yun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:51:02.918Z",
          "hidden": false
        },
        {
          "_id": "6806fcc4f349e60f6c1b928f",
          "name": "Sungwon Hwang",
          "hidden": false
        },
        {
          "_id": "6806fcc4f349e60f6c1b9290",
          "name": "Jaegul Choo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-19T19:59:11.000Z",
      "submittedOnDailyAt": "2025-04-22T00:50:52.270Z",
      "title": "스フィアディフ：무튜닝된 모든 방향의 팰런트 샷프 션 탬포어 이미지와 비디오의 생성을 가능하게 하는 스フィア 탠포어 표현을 사용한 것",
      "submittedOnDailyBy": {
        "_id": "630461624ec2dfa82a5ad7e7",
        "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
        "isPro": false,
        "fullname": "Minho Park",
        "user": "mpark",
        "type": "user"
      },
      "summary": "AR/VR 앱의 수요가 증가함에 따라, 고품질의 360도 Panorama 콘텐츠의 필요성이 강조되어 있습니다. 그러나, 등각 투영(ERP)에 의한 엄격한 왜곡이 원인이 되어, 고품질의 360도 Panorama 이미지와 비디오의 생성은 어려운 과제입니다. 현재의 접근 방식은, 제한적인 ERP 데이터 세트에 훈련된 사전 학습된 Difu-sion 모델을 사용하거나, 무제한의 훈련 방법을 시도하지만, 이들은 POOL 근처에서 불연속성이 발생합니다. 본 논문에서는, 추가적인 훈련을 수행하지 않고, 최신 Difu-sion 모델을 사용하여, 순滑한 360도 Panorama 이미지와 비디오의 생성을 수행하는 새로운 접근 방식을 제시합니다. 球면 잠재 표현을 정의하고, 전체적인 관점에서 일관된 분포를 보장하며, ERP에 의한 왜곡을 완화합니다. MultiDiffusion을 球면 잠재 공간에 확장하고, 球면 잠재 샘플링 메소드를 제안하며, 사전 학습된 Difu-sion 모델의 직접 사용을 가능하게 합니다. 또한, 왜곡에 대한 가중 평균화를 도입하여, 투하 프로세스에서의 생성 품질을 더욱 향상시킵니다. 우리 방법은, 고품질을 유지하는 동시에, 360도 Panorama 콘텐츠의 생성에서 현재의 접근 방식을 초월하며, 만족스러운 AR/VR 앱에 대한 강력한 해결책으로 작용합니다. 코드는 여기를 참조할 수 있습니다. https://github.com/pmh9960/SphereDiff",
      "upvotes": 18,
      "discussionId": "6806fcc7f349e60f6c1b93ab",
      "projectPage": "https://pmh9960.github.io/research/SphereDiff/",
      "githubRepo": "https://github.com/pmh9960/SphereDiff",
      "ai_keywords": [
        "SphereDiff",
        "diffusion models",
        "equirectangular projection (ERP)",
        "spherical latent representation",
        "MultiDiffusion",
        "spherical latent space",
        "spherical latent sampling method",
        "distortion-aware weighted averaging",
        "high-fidelity",
        "immersive AR/VR applications"
      ]
    },
    "publishedAt": "2025-04-19T15:59:11.000Z",
    "title": "SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video\n  Generation via Spherical Latent Representation",
    "summary": "The increasing demand for AR/VR applications has highlighted the need for\nhigh-quality 360-degree panoramic content. However, generating high-quality\n360-degree panoramic images and videos remains a challenging task due to the\nsevere distortions introduced by equirectangular projection (ERP). Existing\napproaches either fine-tune pretrained diffusion models on limited ERP datasets\nor attempt tuning-free methods that still rely on ERP latent representations,\nleading to discontinuities near the poles. In this paper, we introduce\nSphereDiff, a novel approach for seamless 360-degree panoramic image and video\ngeneration using state-of-the-art diffusion models without additional tuning.\nWe define a spherical latent representation that ensures uniform distribution\nacross all perspectives, mitigating the distortions inherent in ERP. We extend\nMultiDiffusion to spherical latent space and propose a spherical latent\nsampling method to enable direct use of pretrained diffusion models. Moreover,\nwe introduce distortion-aware weighted averaging to further improve the\ngeneration quality in the projection process. Our method outperforms existing\napproaches in generating 360-degree panoramic content while maintaining high\nfidelity, making it a robust solution for immersive AR/VR applications. The\ncode is available here. https://github.com/pmh9960/SphereDiff",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14396.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630461624ec2dfa82a5ad7e7",
      "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
      "fullname": "Minho Park",
      "name": "mpark",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13203",
      "authors": [
        {
          "_id": "68070197c1bd0fc00c7d90a7",
          "name": "Salman Rahman",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90a8",
          "name": "Liwei Jiang",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90a9",
          "name": "James Shiffer",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90aa",
          "name": "Genglin Liu",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90ab",
          "name": "Sheriff Issaka",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90ac",
          "name": "Md Rizwan Parvez",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90ad",
          "name": "Hamid Palangi",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90ae",
          "name": "Kai-Wei Chang",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90af",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90b0",
          "name": "Saadia Gabriel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:11:28.000Z",
      "submittedOnDailyAt": "2025-04-22T01:13:32.158Z",
      "title": "X-Teaming: 다턴의 젓가락파괴와 적응적인 다 에이전트의 방어\n\n(注意：虽然要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，这里提供了一个更符合韩语表达习惯的版本。)",
      "submittedOnDailyBy": {
        "_id": "625913bd5f80a3c1aad074b6",
        "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
        "isPro": false,
        "fullname": "Salman Rahman",
        "user": "salmannyu",
        "type": "user"
      },
      "summary": "다언어 교환에서 언어 모델(LM)과의 다언어 교환은 전략적으로有害한 의도가 교환 간에 확산되는 위험성을 동반합니다. 그러나 선행 연구는 대부분 단어 교환 안전성에 초점을 맞추고 있으며, 다언어 교환 테스트의 적용성과 다양성은 주요 문제 중 하나입니다. 이러한 문제를 대처하기 위해, 우리는 X-Teaming이라는 scalable 프레임워크를 제안합니다. 이 프레임워크는 보이지 않는 무害한 교환이有害한 결과를 발전시키는 방식으로 체계적으로 탐색하고, 대응하는 공격 시나리오를 생성하는 것을 목표로 합니다. X-Teaming은 계획, 공격 최적화, 검증을 위해 협력 에이전트를 사용하며, 대표적인 최신 오픈 웨이트 모델과 클로즈드 소스 모델에 대해 98.1%의 성공률로 가장 先端의 다언어 교환 다운볼 효과성과 다양성을 달성합니다. 특히, X-Teaming은 최신의 Claude 3.7 Sonnet 모델에 대해 96.2%의 공격 성공률을 달성하며, 이는 단어 교환 공격에 근사적으로 면역되어 있던 것을 의미합니다. X-Teaming에 기반하여, 우리는 XGuard-Train이라는 오픈 소스의 다언어 교환 안전성 훈련 데이터셋을 소개합니다. 이 데이터셋은 이전의 최고의 리소스보다 20배 더 크며, 30K개의 인터랙티브 다운볼을 포함하며, LM의 강력한 다언어 교환 안전성 조정을 가능하게 합니다. 우리의 연구는 복잡한 대화 공격에 대한 대응에 필수적인 도구와 통찰을 제공하고, LM의 다언어 교환 안전성을 개선하는 것을 목표로 합니다.",
      "upvotes": 17,
      "discussionId": "68070198c1bd0fc00c7d9101",
      "projectPage": "https://x-teaming.github.io/",
      "githubRepo": "https://github.com/salman-lui/x-teaming",
      "ai_keywords": [
        "multi-turn interactions",
        "language models (LMs)",
        "safety risks",
        "harmful intent",
        "single-turn safety",
        "adaptability",
        "diversity",
        "X-Teaming",
        "collaborative agents",
        "attack scenarios",
        "jailbreak effectiveness",
        "jailbreak success rates",
        "Claude 3.7 Sonnet model",
        "multi-turn jailbreak",
        "XGuard-Train",
        "safety training dataset",
        "interactive jailbreaks",
        "multi-turn safety alignment"
      ]
    },
    "publishedAt": "2025-04-15T12:11:28.000Z",
    "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
    "summary": "Multi-turn interactions with language models (LMs) pose critical safety\nrisks, as harmful intent can be strategically spread across exchanges. Yet, the\nvast majority of prior work has focused on single-turn safety, while\nadaptability and diversity remain among the key challenges of multi-turn\nred-teaming. To address these challenges, we present X-Teaming, a scalable\nframework that systematically explores how seemingly harmless interactions\nescalate into harmful outcomes and generates corresponding attack scenarios.\nX-Teaming employs collaborative agents for planning, attack optimization, and\nverification, achieving state-of-the-art multi-turn jailbreak effectiveness and\ndiversity with success rates up to 98.1% across representative leading\nopen-weight and closed-source models. In particular, X-Teaming achieves a 96.2%\nattack success rate against the latest Claude 3.7 Sonnet model, which has been\nconsidered nearly immune to single-turn attacks. Building on X-Teaming, we\nintroduce XGuard-Train, an open-source multi-turn safety training dataset that\nis 20x larger than the previous best resource, comprising 30K interactive\njailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our\nwork offers essential tools and insights for mitigating sophisticated\nconversational attacks, advancing the multi-turn safety of LMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13203.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625913bd5f80a3c1aad074b6",
      "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
      "fullname": "Salman Rahman",
      "name": "salmannyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14603",
      "authors": [
        {
          "_id": "6806fab1b89f3c89c81afbe0",
          "user": {
            "_id": "654dbac9938fbf1e696be8aa",
            "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
            "isPro": false,
            "fullname": "Chaoyun Zhang",
            "user": "vyokky",
            "type": "user"
          },
          "name": "Chaoyun Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:51:25.217Z",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe1",
          "name": "He Huang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe2",
          "name": "Chiming Ni",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe3",
          "name": "Jian Mu",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe4",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe5",
          "name": "Shilin He",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe6",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe7",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe8",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe9",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbea",
          "name": "Liqun Li",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbeb",
          "name": "Yu Kang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbec",
          "name": "Zhao Jiang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbed",
          "name": "Suzhen Zheng",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbee",
          "name": "Rujia Wang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbef",
          "name": "Jiaxu Qian",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf0",
          "name": "Minghua Ma",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf1",
          "name": "Jian-Guang Lou",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf2",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf3",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf4",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-20T13:04:43.000Z",
      "submittedOnDailyAt": "2025-04-22T00:41:48.404Z",
      "title": "UFO2: 데스크탑 어우터 엔진 오시오스",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "최근의 컴퓨터 사용 에이전트(CUAs)는 복잡한 데스크톱 워크플로우를 자동화할 수 있는 가능성을 제시하고 있습니다. 그러나 현재의 CUAs는 OS의 약한 통합, 취약한 스크린샷 기반의 인터랙션, 그리고 파괴적인 실행으로 인해 개념적인 프로토타입으로 남아 있습니다.\n\nUFO2는 Windows 데스크톱을 위한 멀티 에이전트 에이전트 오시스(AgentOS)입니다. UFO2는 태스크 분해와 코디네이션을 위해 중심적인 호스트 에이전트, 네이티브 API, 도메인 전문 지식, 그리고 통합 GUI-API 액션 레이어를 갖춘 애플리케이션 전문화 에이전트의 집합을 특징으로 합니다. 이 아키텍처는 강건한 태스크 실행을 유지하면서 모듈화와 확장성을 유지합니다. 하이브리드 제어 감지 파이프라인은 윈도우 UI 자동화(UIA)와 시각 분석을 결합하여 다양한 인터페이스 스타일을 지원합니다. 실행 효율성은 예측적인 다 액션 계획을 통해 LLM의 비용 절감에 도움을 줍니다. 마지막으로, Picture-in-Picture(PiP) 인터페이스는 가상 데스크톱 내의 독립적인 뷰에서 자동화를 가능하게 하며, 에이전트와 사용자가 동시에 조작할 수 있도록 합니다.\n\nUFO2는 20개 이상의 실질적인 Windows 애플리케이션을 평가하고, 이전의 CUAs와 비교하여 강력한 신뢰성과 높은 실행 정확도를 보여주었습니다. 결과적으로, 깊은 OS 통합은 신뢰할 수 있는, 사용자 중심의 데스크톱 자동화에 대한 확장 가능한 경로를 열어줍니다.",
      "upvotes": 15,
      "discussionId": "6806fabdb89f3c89c81affb5",
      "projectPage": "https://microsoft.github.io/UFO/",
      "githubRepo": "https://github.com/microsoft/UFO",
      "ai_keywords": [
        "multimodal large language models",
        "task decomposition",
        "coordination",
        "application-specialized AppAgent",
        "native APIs",
        "domain-specific knowledge",
        "unified GUI--API action layer",
        "hybrid control detection pipeline",
        "Windows UI Automation (UIA)",
        "vision-based parsing",
        "speculative multi-action planning",
        "Picture-in-Picture (PiP) interface",
        "runtime efficiency"
      ]
    },
    "publishedAt": "2025-04-20T09:04:43.000Z",
    "title": "UFO2: The Desktop AgentOS",
    "summary": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14603.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15280",
      "authors": [
        {
          "_id": "680703e4c0ce7eea9ba1a548",
          "user": {
            "_id": "6499eca0685215f7247bd5ce",
            "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
            "isPro": false,
            "fullname": "Chun-Hsiao Yeh",
            "user": "danielchyeh",
            "type": "user"
          },
          "name": "Chun-Hsiao Yeh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:54.878Z",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a549",
          "user": {
            "_id": "641bd539aebaa27e07540613",
            "avatarUrl": "/avatars/000e8057e7280b9eba9bec5116d14718.svg",
            "isPro": false,
            "fullname": "chenyu wang",
            "user": "ch-chenyu",
            "type": "user"
          },
          "name": "Chenyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:52.696Z",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54a",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54b",
          "name": "Ta-Ying Cheng",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54c",
          "name": "Rouyu Wang",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54d",
          "name": "Tianzhe Chu",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54e",
          "name": "Yuexiang Zhai",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54f",
          "name": "Yubei Chen",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a550",
          "name": "Shenghua Gao",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a551",
          "name": "Yi Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:59:53.000Z",
      "submittedOnDailyAt": "2025-04-22T01:49:06.289Z",
      "title": "「다른 시각에서 보기: MLLM의 다관 이해 평가」",
      "submittedOnDailyBy": {
        "_id": "637c7420f219c71f93ec8f81",
        "avatarUrl": "/avatars/969b72bd4320423af89e6a5d0ffa03cc.svg",
        "isPro": false,
        "fullname": "frog",
        "user": "frog123123123123",
        "type": "user"
      },
      "summary": "다중시각 이해와 다중시각에서 시각정보의 통합이 기계화 에이전트에서 사용하는 다중모달 대언어 모델(MLLMs)에서 기본적인 문제입니다. 최근의 MLLMs는 고수준의 이유와 계획에 있어서 놀라운 진전을 보여주지만, 다중시각의 기하학적 일관성과 시각간의 대응관계에 대해서는 부족한점이 있습니다. MLLMs의 다중시각 공간의 이유에 대한 문제를 전적으로 평가하기 위해, 우리는 90가지의 다양한 실세계 공간에 대해 2,100개 이상의 사람이 주목하는 다중시각의 질문응답 페어 2,100개 이상의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어 2,100개의 질문응답 페어",
      "upvotes": 9,
      "discussionId": "680703e6c0ce7eea9ba1a63e",
      "projectPage": "https://danielchyeh.github.io/All-Angles-Bench/",
      "githubRepo": "https://github.com/Chenyu-Wang567/All-Angles-Bench/tree/main",
      "ai_keywords": [
        "Multi-Modal Large Language Models (MLLMs)",
        "multi-view geometric consistency",
        "cross-view correspondence",
        "All-Angles Bench",
        "Gemini-2.0-Flash",
        "Claude-3.7-Sonnet",
        "GPT-4o",
        "geometric correspondence",
        "camera pose estimation",
        "cross-view correspondence",
        "partially occluded views",
        "coarse camera poses",
        "multi-view awareness"
      ]
    },
    "publishedAt": "2025-04-21T13:59:53.000Z",
    "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in\n  MLLMs",
    "summary": "Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15280.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c7420f219c71f93ec8f81",
      "avatarUrl": "/avatars/969b72bd4320423af89e6a5d0ffa03cc.svg",
      "fullname": "frog",
      "name": "frog123123123123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14239",
      "authors": [
        {
          "_id": "6806e9e819a9fa609609fe65",
          "name": "Yuhang Liu",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe66",
          "user": {
            "_id": "64245f2c089d5fae56b4549a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
            "isPro": false,
            "fullname": "Pengxiang Li",
            "user": "pengxiang",
            "type": "user"
          },
          "name": "Pengxiang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:51:50.804Z",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe67",
          "user": {
            "_id": "629084f3a391a907d5e0484e",
            "avatarUrl": "/avatars/3a1d6d5aa90b3b8372505d13ccf8f2dd.svg",
            "isPro": false,
            "fullname": "unk",
            "user": "xieck13",
            "type": "user"
          },
          "name": "Congkai Xie",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-22T00:59:21.913Z",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe68",
          "name": "Xavier Hu",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe69",
          "user": {
            "_id": "650dde4ce14eeb01d42b37a1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dde4ce14eeb01d42b37a1/n5Yv24uofZ2XJjXdYCrKd.png",
            "isPro": false,
            "fullname": "Xiaotian Han",
            "user": "xiaotianhan",
            "type": "user"
          },
          "name": "Xiaotian Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:51:27.543Z",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe6a",
          "name": "Shengyu Zhang",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe6b",
          "name": "Hongxia Yang",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe6c",
          "name": "Fei Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-19T09:25:55.000Z",
      "submittedOnDailyAt": "2025-04-22T02:26:02.459Z",
      "title": "InfiGUI-R1: 반응성 액터로부터 측정적 리서치나어로 다모델 GUI 에이전트의 발전",
      "submittedOnDailyBy": {
        "_id": "64245f2c089d5fae56b4549a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
        "isPro": false,
        "fullname": "Pengxiang Li",
        "user": "pengxiang",
        "type": "user"
      },
      "summary": "다모달 대언어 모델(MLLMs)은 그래픽 사용자 인터페이스(GUI) 에이전트를 동력으로 하고, 계산 장치에서 작업의 자동화에 좋은 결과를 보여주고 있다. 최근의 연구는 GUI 작업에서 이유를 평가하고 평가적인 결과를 얻고 있다. 그러나 현재 많은 접근 방식은 복잡한 GUI 환경에 대해 충분한 강건하고 적응적인 이유를 얻을 수 있는 가능성은手工 설계된 이유 템플릿을 기반으로 하는 것이다. 반면, 기존의 에이전트는 주로 숨겨진 이유로 GUI 작업의 계획과 오류 수정에 필요한 충분한 깊이를 잃어 Reactive Actor로 계속 작동하고 있다. 우리는 이러한 에이전트의 발전에 Reactive Actor로부터의 결정적인 이유에 기반한 행동을 전환하는 것이 필요로 한다. 이러한 전환을 촉진하기 위해, MLLM 기반의 GUI 에이전트인 InfiGUI-R1을 소개하고, 우리의 Actor2Reasoner 프레임워크를 통해 개발한, 이유 중심의 두 단계의 훈련 접근 방식을 사용하여 진화시킬 에이전트를 개발하고 있다. 첫 번째 단계는 이유 주입으로, 기본적인 이유를 구축하는 것을 중점 삼고 있다. 공간 이유 디스탠스 리쉐일로, 교사 모델으로부터 MLLM의 크로스 모드 공간 이유 능력을 전파하기 위해 트랙을 사용하며, GUI 시각 정보와 로직 이유를 통합할 수 있게 하는 것이다. 두 번째 단계는 결정의 강화로, 강화 학습을 사용하여 기본적인 이유를 결정적인 이유로 훈련하는 것이다. 이 단계에서는 두 가지 접근 방식을 도입하고 있다: Sub-goal Guidance는 정확한 중간 Sub-goal을 생성하는 모델에 보상을 부여하고, Error Recovery Scenario Construction은 인식된 오류가 있는 단계에서 실패와 리카바이를 위한 훈련 시나리오를 생성하는 것이다. 실험 결과를 통해 InfiGUI-R1은 GUI 그리핑과 트랙 작업에 강한 성능을 보여고 있다. 리소스는 https://github.com/Reallm-Labs/InfiGUI-R1을 참조할 수 있다.",
      "upvotes": 7,
      "discussionId": "6806e9e919a9fa609609fea1",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Graphical User Interface (GUI) Agents",
        "Spatial Reasoning Distillation",
        "Actor2Reasoner framework",
        "Reasoning Injection",
        "Deliberation Enhancement",
        "Sub-goal Guidance",
        "Error Recovery Scenario Construction",
        "GUI grounding",
        "trajectory tasks"
      ]
    },
    "publishedAt": "2025-04-19T05:25:55.000Z",
    "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to\n  Deliberative Reasoners",
    "summary": "Multimodal Large Language Models (MLLMs) have powered Graphical User\nInterface (GUI) Agents, showing promise in automating tasks on computing\ndevices. Recent works have begun exploring reasoning in GUI tasks with\nencouraging results. However, many current approaches rely on manually designed\nreasoning templates, which may result in reasoning that is not sufficiently\nrobust and adaptive for complex GUI environments. Meanwhile, some existing\nagents continue to operate as Reactive Actors, relying primarily on implicit\nreasoning that may lack sufficient depth for GUI tasks demanding planning and\nerror recovery. We argue that advancing these agents requires a shift from\nreactive acting towards acting based on deliberate reasoning. To facilitate\nthis transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed\nthrough our Actor2Reasoner framework, a reasoning-centric, two-stage training\napproach designed to progressively evolve agents from Reactive Actors to\nDeliberative Reasoners. The first stage, Reasoning Injection, focuses on\nestablishing a basic reasoner. We employ Spatial Reasoning Distillation to\ntransfer cross-modal spatial reasoning capabilities from teacher models to\nMLLMs through trajectories with explicit reasoning steps, enabling models to\nintegrate GUI visual-spatial information with logical reasoning before action\ngeneration. The second stage, Deliberation Enhancement, refines the basic\nreasoner into a deliberative one using Reinforcement Learning. This stage\nintroduces two approaches: Sub-goal Guidance, which rewards models for\ngenerating accurate intermediate sub-goals, and Error Recovery Scenario\nConstruction, which creates failure-and-recovery training scenarios from\nidentified prone-to-error steps. Experimental results show InfiGUI-R1 achieves\nstrong performance in GUI grounding and trajectory tasks. Resources at\nhttps://github.com/Reallm-Labs/InfiGUI-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14239.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64245f2c089d5fae56b4549a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
      "fullname": "Pengxiang Li",
      "name": "pengxiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14655",
      "authors": [
        {
          "_id": "68070991b0cb768cae20c585",
          "name": "Yunhui Xia",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c586",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c587",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c588",
          "user": {
            "_id": "6731c04d5f55903a1d8c307c",
            "avatarUrl": "/avatars/704b1d628c55e3141194b08736f21267.svg",
            "isPro": false,
            "fullname": "Jason Klein Liu",
            "user": "jasonkleinlove",
            "type": "user"
          },
          "name": "Jason Klein Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-22T03:14:26.859Z",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c589",
          "name": "Huifeng Sun",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c58a",
          "name": "Siyue Wu",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c58b",
          "user": {
            "_id": "63f6c04ac96958470d1e9043",
            "avatarUrl": "/avatars/da46cdd9e21498e120ca91b67bfbfb5e.svg",
            "isPro": false,
            "fullname": "Jian Hu",
            "user": "chuyi777",
            "type": "user"
          },
          "name": "Jian Hu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-22T03:14:26.859Z",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c58c",
          "name": "Xiaolong Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-20T15:28:16.000Z",
      "submittedOnDailyAt": "2025-04-22T01:46:36.359Z",
      "title": "LeetCode 데이터셋: 효율적인 훈련과 견고한 평가의 목적에 적합한 시간열 데이터셋",
      "submittedOnDailyBy": {
        "_id": "6468823272d9180d4ac90bdf",
        "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
        "isPro": false,
        "fullname": "Wei Shen",
        "user": "Swtheking",
        "type": "user"
      },
      "summary": "LeetCodeDataset를 소개합니다. 이 데이터셋은 LLM 연구의 두 가지 중요한 문제를 해결하기 위한 고품질의 벤치마크입니다. 이는 합리적인 코드 벤치마크의 부족과 독립적인 훈련 테스트 베드의 부족을 해결하고 있습니다. LeetCode Python 문제를 자료원으로 삼으며, 풍부한 메타 데이터, 넓은 커버리지 범위, 각 문제마다 100점 이상의 테스트 케이스, 시간 순 분할(2024년 7월 전후)을 사용하여, 이 데이터셋은 비연결 평가와 효율적인 규칙付き조정(SFT)을 가능하게 합니다. 실험에 따르면 합리적인 모델은 비합리적인 모델보다 유의미하게 우수하며, 2.6K 모델을 생성한 SFT는 110K 샘플의 모델과 같은 성능을 달성합니다. 이 데이터셋과 평가 프레임워크는 Hugging Face와 GitHub에 제공됩니다.",
      "upvotes": 6,
      "discussionId": "68070992b0cb768cae20c5ca",
      "ai_keywords": [
        "LeetCodeDataset",
        "code-generation models",
        "LLM research",
        "evaluation",
        "supervised fine-tuning (SFT)",
        "reasoning models"
      ]
    },
    "publishedAt": "2025-04-20T11:28:16.000Z",
    "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient\n  Training of Code LLMs",
    "summary": "We introduce LeetCodeDataset, a high-quality benchmark for evaluating and\ntraining code-generation models, addressing two key challenges in LLM research:\nthe lack of reasoning-focused coding benchmarks and self-contained training\ntestbeds. By curating LeetCode Python problems with rich metadata, broad\ncoverage, 100+ test cases per problem, and temporal splits (pre/post July\n2024), our dataset enables contamination-free evaluation and efficient\nsupervised fine-tuning (SFT). Experiments show reasoning models significantly\noutperform non-reasoning counterparts, while SFT with only 2.6K model-generated\nsolutions achieves performance comparable to 110K-sample counterparts. The\ndataset and evaluation framework are available on Hugging Face and Github.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14655.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6468823272d9180d4ac90bdf",
      "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
      "fullname": "Wei Shen",
      "name": "Swtheking",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13805",
      "authors": [
        {
          "_id": "680662b97415e191e3579b9e",
          "user": {
            "_id": "64d761b98ebc40443831f82a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
            "isPro": false,
            "fullname": "lgy0404",
            "user": "lgy0404",
            "type": "user"
          },
          "name": "Guangyi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:52:04.194Z",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579b9f",
          "user": {
            "_id": "65a088f4300957620ba45c70",
            "avatarUrl": "/avatars/56ed45e10d3455531979f30881b2d3f9.svg",
            "isPro": false,
            "fullname": "pengxiang zhao",
            "user": "Pengxiangzhao",
            "type": "user"
          },
          "name": "Pengxiang Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:52:01.390Z",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba0",
          "name": "Liang Liu",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba1",
          "name": "Zhiming Chen",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba2",
          "name": "Yuxiang Chai",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba3",
          "name": "Shuai Ren",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba4",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba5",
          "name": "Shibo He",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba6",
          "name": "Wenchao Meng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T17:13:34.000Z",
      "submittedOnDailyAt": "2025-04-22T01:14:18.820Z",
      "title": "LearnAct: 소수 shot 모빌 GUI 에이전트와 통일된 데모нс트레이션의 벤치마크",
      "submittedOnDailyBy": {
        "_id": "6458ce236fa580137af5aa95",
        "avatarUrl": "/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg",
        "isPro": false,
        "fullname": "Yuxiang Chai",
        "user": "Yuxiang007",
        "type": "user"
      },
      "summary": "모바일 GUI 에이전트는 작업의 자동화에 좋은 효과를 보입니다が, 다양한 리알워어스케너리오에서 일반화하는 문제를 발견합니다. 사전 학습과 미세 조정을 사용하는 전통적인 접근 방식은 모바일 앱의 다양성과 사용자의 특정한 작업에 대해 어려움을 겪습니다. 우리는 인간의 예시를 통해 모바일 GUI 에이전트의 능력을 향상시키고,未见过的 시나리오에서 효과를 높일 것을 우선시하고, 큰 데이터 세트를 통해 일반화를 추구하는 것을 포기합니다. 이를 실현하기 위해, 우리는 학습을 위한 첫 번째 상세한 데이터 세트인 LearnGUI를 소개합니다. 이 데이터 세트는 2,252개의 오프라인 작업과 101개의 온라인 작업을 포함하며, 고품질의 인간의 예시를 포함합니다. 또한, 우리는 예시로부터 지식을 자동으로 추출하고, 작업의 완료를 향상시키기 위한 복잡한 다 에이전트 프레임워크인 LearnAct를 개발합니다. 이 프레임워크는 지식 추출을 위한 DemoParser, 연관 지식의 검색을 위한 KnowSeeker, 예시를 기반으로한 작업 실행을 위한 ActExecutor의 3개의 특수화된 에이전트를 조합하여 구성됩니다. 우리의 실험 결과에 따르면, 오프라인과 온라인 평가에서도 유의미한 성능 향상이 나타났습니다. 오프라인 평가에서, 하나의 예시는 모델의 성능을 향상시키고, Gemini-1.5-Pro의 정확도를 19.3%에서 51.7%로 높였습니다. 온라인 평가에서, UI-TARS-7B-SFT의 작업 성공률을 18.1%에서 32.8%로 높였습니다. LearnAct 프레임워크와 LearnGUI 벤치마크는, 예시 기반의 학습을 더욱 적응적이고, 플랫폼이나 개인화된 모바일 GUI 에이전트의 구현을 가능하게 하는 바람직한 방향을 인정받습니다.",
      "upvotes": 6,
      "discussionId": "680662bd7415e191e3579c7d",
      "ai_keywords": [
        "LearnGUI",
        "LearnAct",
        "DemoParser",
        "KnowSeeker",
        "ActExecutor",
        "demonstration-based learning",
        "mobile GUI agents",
        "human demonstrations",
        "multi-agent framework"
      ]
    },
    "publishedAt": "2025-04-18T13:13:34.000Z",
    "title": "LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration\n  Benchmark",
    "summary": "Mobile GUI agents show promise in automating tasks but face generalization\nchallenges in diverse real-world scenarios. Traditional approaches using\npre-training or fine-tuning with massive datasets struggle with the diversity\nof mobile applications and user-specific tasks. We propose enhancing mobile GUI\nagent capabilities through human demonstrations, focusing on improving\nperformance in unseen scenarios rather than pursuing universal generalization\nthrough larger datasets. To realize this paradigm, we introduce LearnGUI, the\nfirst comprehensive dataset specifically designed for studying\ndemonstration-based learning in mobile GUI agents, comprising 2,252 offline\ntasks and 101 online tasks with high-quality human demonstrations. We further\ndevelop LearnAct, a sophisticated multi-agent framework that automatically\nextracts knowledge from demonstrations to enhance task completion. This\nframework integrates three specialized agents: DemoParser for knowledge\nextraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for\ndemonstration-enhanced task execution. Our experimental results show\nsignificant performance gains in both offline and online evaluations. In\noffline assessments, a single demonstration improves model performance,\nincreasing Gemini-1.5-Pro's accuracy from 19.3% to 51.7%. In online\nevaluations, our framework enhances UI-TARS-7B-SFT's task success rate from\n18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish\ndemonstration-based learning as a promising direction for more adaptable,\npersonalized, and deployable mobile GUI agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13805.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6458ce236fa580137af5aa95",
      "avatarUrl": "/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg",
      "fullname": "Yuxiang Chai",
      "name": "Yuxiang007",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08902",
      "authors": [
        {
          "_id": "680730598ce2be7f5450bd77",
          "user": {
            "_id": "64e3950d9ec4cf50009ce960",
            "avatarUrl": "/avatars/8aade78bdde8423bd811ca8394cb08c5.svg",
            "isPro": false,
            "fullname": "Pascal Chang",
            "user": "pascalchang87",
            "type": "user"
          },
          "name": "Pascal Chang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:31.652Z",
          "hidden": false
        },
        {
          "_id": "680730598ce2be7f5450bd78",
          "user": {
            "_id": "66a38c9e053b7e9c907fba72",
            "avatarUrl": "/avatars/59588845e21ec6ad63631c892066f4ad.svg",
            "isPro": false,
            "fullname": "Sergio Sancho",
            "user": "ssancho",
            "type": "user"
          },
          "name": "Sergio Sancho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:28.796Z",
          "hidden": false
        },
        {
          "_id": "680730598ce2be7f5450bd79",
          "name": "Jingwei Tang",
          "hidden": false
        },
        {
          "_id": "680730598ce2be7f5450bd7a",
          "name": "Markus Gross",
          "hidden": false
        },
        {
          "_id": "680730598ce2be7f5450bd7b",
          "name": "Vinicius C. Azevedo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T18:12:01.000Z",
      "submittedOnDailyAt": "2025-04-22T04:59:10.950Z",
      "title": "로카リ닝 유리: 라플라시안 피라미드 워핑에 의한 생성 아나마르포시ㅋ",
      "submittedOnDailyBy": {
        "_id": "66863bd107019f3fe48a21ab",
        "avatarUrl": "/avatars/3297e18e43d40e902b9554a077a34a8a.svg",
        "isPro": false,
        "fullname": "Manuel Kansy",
        "user": "manuelkansy",
        "type": "user"
      },
      "summary": "Anamorphosis는 직접 볼 때 인식할 수 없는 것처럼 번역된 이미지의 클래스를 가리키는 것입니다. 그 본질적인 형태는 특정한 시각에서만 볼 수 있습니다. 이러한 수학적인 장치의 구성은 17세기까지追溯할 수 있습니다が, 특정한 시각에서 볼 필요가 있으며, 일반적인 방법으로 볼 때 의미를 잃습니다. 본 논문에서는 이러한 유명한 광학 이리유션을 재검토하고 생성적인 변형을 추가하여 새로운 방법을 제안합니다. 잠재적 정규화 플로 모형을 도움을 받아 직접 볼 때도 정당한 해석이 남아있는 아나모픽 이미지의 생성 방법을 제안합니다. 이를 위해 라플라시안 피라미드 워핑(Laplacian Pyramid Warping)을 도입하고, 고품질의 이미지를 생성하기 위한 주파수에 대한 이미지 워핑 방법의 요약을 합니다. 우리 연구는 Visual Anagrams(arXiv:2311.17919)을 잠재 공간 모델에 확장하여 광범위한 공간 변환에 대해 새로운 생성적인 시각 이리유션을 만들 수 있도록 합니다.",
      "upvotes": 6,
      "discussionId": "6807305f8ce2be7f5450bf69",
      "ai_keywords": [
        "latent rectified flow models",
        "Laplacian Pyramid Warping",
        "frequency-aware image warping",
        "generative perceptual illusions"
      ]
    },
    "publishedAt": "2025-04-11T14:12:01.000Z",
    "title": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping",
    "summary": "Anamorphosis refers to a category of images that are intentionally distorted,\nmaking them unrecognizable when viewed directly. Their true form only reveals\nitself when seen from a specific viewpoint, which can be through some\ncatadioptric device like a mirror or a lens. While the construction of these\nmathematical devices can be traced back to as early as the 17th century, they\nare only interpretable when viewed from a specific vantage point and tend to\nlose meaning when seen normally. In this paper, we revisit these famous optical\nillusions with a generative twist. With the help of latent rectified flow\nmodels, we propose a method to create anamorphic images that still retain a\nvalid interpretation when viewed directly. To this end, we introduce Laplacian\nPyramid Warping, a frequency-aware image warping technique key to generating\nhigh-quality visuals. Our work extends Visual Anagrams (arXiv:2311.17919) to\nlatent space models and to a wider range of spatial transforms, enabling the\ncreation of novel generative perceptual illusions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08902.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "66863bd107019f3fe48a21ab",
      "avatarUrl": "/avatars/3297e18e43d40e902b9554a077a34a8a.svg",
      "fullname": "Manuel Kansy",
      "name": "manuelkansy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15217",
      "authors": [
        {
          "_id": "68070e1895e06498588c4497",
          "user": {
            "_id": "63239bd492e07e3ca2068a16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63239bd492e07e3ca2068a16/OFESFFWQVHC1EtJpnGOWv.jpeg",
            "isPro": true,
            "fullname": "Yatong Bai",
            "user": "Bai-YT",
            "type": "user"
          },
          "name": "Yatong Bai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:50.514Z",
          "hidden": false
        },
        {
          "_id": "68070e1895e06498588c4498",
          "name": "Jonah Casebeer",
          "hidden": false
        },
        {
          "_id": "68070e1895e06498588c4499",
          "name": "Somayeh Sojoudi",
          "hidden": false
        },
        {
          "_id": "68070e1895e06498588c449a",
          "user": {
            "_id": "648119ee7a741c7f33f49f25",
            "avatarUrl": "/avatars/183d2143ba20e1cc8712c63c055aadd7.svg",
            "isPro": false,
            "fullname": "Nicholas J. Bryan",
            "user": "Njb",
            "type": "user"
          },
          "name": "Nicholas J. Bryan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-22T03:35:40.134Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/648119ee7a741c7f33f49f25/V_gAc821aOtnXsoaUFvCQ.mp4"
      ],
      "publishedAt": "2025-04-21T16:41:40.000Z",
      "submittedOnDailyAt": "2025-04-22T02:07:09.310Z",
      "title": "DRAGON: 분포 분배 최적화 분배 분배 리프 유전 생성 모델",
      "submittedOnDailyBy": {
        "_id": "648119ee7a741c7f33f49f25",
        "avatarUrl": "/avatars/183d2143ba20e1cc8712c63c055aadd7.svg",
        "isPro": false,
        "fullname": "Nicholas J. Bryan",
        "user": "Njb",
        "type": "user"
      },
      "summary": "DRAGON은 생성 모델을 원하는 출력에 맞추기 위해 다양한 방법을 활용하는 프레임워크입니다. 기존의 가치 함수를 평가하는 데만 집중하는 것이 아니라, 그 분포도 평가할 수 있어 개별 예시, 예시와 분포의 관계, 분포와 분포의 관계의 광범위한 범위의 값을 평가할 수 있습니다. 이러한 다양성을 활용하여, 인코더와 예시를 선택하여 샘플 분포를 만들고, 새로운 가치 함수를 구축합니다. Cross-modal 인코더의 경우, 예시는 다른 모달 데이터로 변합니다. 예를 들어, 텍스트와 오디오가 될 수 있습니다. 이후, DRAGON은 온라인 및 오버플로우 생성을 모아, 이들을 점수화하여 긍정 예시 데모nst레이션 세트와 부정 세트를 구축하고, 이러한 대비를 활용하여 보상을 최대화합니다. 평가에는 20가지의 가치 함수를 사용하여 오디오 영역의 텍스트로부터 음악 확장 모델을 미세 조정하고, 사용자 정의 음악 예술 평가 모델, CLAP 점수, Vendi 다양성, Frechet 오디오 거리(FAD)를 포함합니다. 또한, 개별 곡(per-song)과 전체 데이터 세트의 FAD 설정을 비교하고, 여러 FAD 인코더와 참조 세트를 생략합니다. 20개 목표 가치 함수에서 모두, DRAGON은 81.45%의 평균 승률을 달성합니다. 또한, 샘플 세트에 기반한 가치 함수는 확실히 생성을 향상시키고, 모델 기반의 가치 함수와 비교할 수 있습니다. 적절한 샘플 세트를 사용하여, DRAGON은 인종의 취향의 훈련 데이터에 의존하지 않고 60.95%의 인종 투표의 음악 질 승률을 달성합니다. 이러한 방식으로, DRAGON은 인간 관찰에서 질을 향상시키기 위한 가치 함수의 설계와 최적화에 새로운 접근을 제시하고 있습니다. 소리 예시는 https://ml-dragon.github.io/web 에서 다운로드 가능합니다.",
      "upvotes": 5,
      "discussionId": "68070e1f95e06498588c467f",
      "ai_keywords": [
        "Distributional RewArds for Generative OptimizatioN (DRAGON)",
        "fine-tuning",
        "media generation models",
        "reinforcement learning with human feedback (RLHF)",
        "direct preference optimization (DPO)",
        "reward functions",
        "exemplar distribution",
        "cross-modality encoders",
        "CLAP",
        "online and on-policy generations",
        "positive demonstration set",
        "negative set",
        "contrast",
        "audio-domain text-to-music diffusion model",
        "music aesthetics model",
        "Vendi diversity",
        "Frechet audio distance (FAD)",
        "full-dataset FAD",
        "human-voted music quality"
      ]
    },
    "publishedAt": "2025-04-21T12:41:40.000Z",
    "title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models",
    "summary": "We present Distributional RewArds for Generative OptimizatioN (DRAGON), a\nversatile framework for fine-tuning media generation models towards a desired\noutcome. Compared with traditional reinforcement learning with human feedback\n(RLHF) or pairwise preference approaches such as direct preference optimization\n(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate\neither individual examples or distributions of them, making it compatible with\na broad spectrum of instance-wise, instance-to-distribution, and\ndistribution-to-distribution rewards. Leveraging this versatility, we construct\nnovel reward functions by selecting an encoder and a set of reference examples\nto create an exemplar distribution. When cross-modality encoders such as CLAP\nare used, the reference examples may be of a different modality (e.g., text\nversus audio). Then, DRAGON gathers online and on-policy generations, scores\nthem to construct a positive demonstration set and a negative set, and\nleverages the contrast between the two sets to maximize the reward. For\nevaluation, we fine-tune an audio-domain text-to-music diffusion model with 20\ndifferent reward functions, including a custom music aesthetics model, CLAP\nscore, Vendi diversity, and Frechet audio distance (FAD). We further compare\ninstance-wise (per-song) and full-dataset FAD settings while ablating multiple\nFAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an\n81.45% average win rate. Moreover, reward functions based on exemplar sets\nindeed enhance generations and are comparable to model-based rewards. With an\nappropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality\nwin rate without training on human preference annotations. As such, DRAGON\nexhibits a new approach to designing and optimizing reward functions for\nimproving human-perceived quality. Sound examples at\nhttps://ml-dragon.github.io/web.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/648119ee7a741c7f33f49f25/V_gAc821aOtnXsoaUFvCQ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15217.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648119ee7a741c7f33f49f25",
      "avatarUrl": "/avatars/183d2143ba20e1cc8712c63c055aadd7.svg",
      "fullname": "Nicholas J. Bryan",
      "name": "Njb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15133",
      "authors": [
        {
          "_id": "68073a2d19a9fa6096218691",
          "name": "Ziwen Xu",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218692",
          "name": "Shuxun Wang",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218693",
          "name": "Kewei Xu",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218694",
          "name": "Haoming Xu",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218695",
          "name": "Mengru Wang",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218696",
          "name": "Xinle Deng",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218697",
          "name": "Yunzhi Yao",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218698",
          "name": "Guozhou Zheng",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218699",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa609621869a",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T14:33:55.000Z",
      "submittedOnDailyAt": "2025-04-22T05:12:13.220Z",
      "title": "EasyEdit2: 대 언어 모델의 편집에 대한 쉬운 작업 프레임워크",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "본 논문에서는 LLM(라ージェット ラングジャン モデル)의 행동을 제어할 수 있는 플러그인 및 패치 가능한 조정성을 제공하는 프레임워크 EasyEdit2를 소개합니다. EasyEdit2는 안전성, 감정, 성격, 이유의 패턴, 사실성, 언어특징 등 광범위한 범위의 테스트 시의 인터랙션을 지원합니다. 이전의 작업과는 달리, EasyEdit2는 무차별의 모델의 제어를 가능하게 하는 새로운 아키텍처를 특징적으로 가지고 있습니다. 이는 스팀링 벡터 생성기 및 스팀링 벡터 앱라이더와 같은 핵심 모듈을 포함하며, 모델의 파라미터를 변경하면서도 모델의 행동을 영향을 미칠 수 있는 자동적인 스팀링 벡터의 생성과 적용을 가능하게 합니다. EasyEdit2의 주요 장점 중 하나는 그 쉬운 정도입니다. 사용자가 엄격한 기술 지식이 필요하지 않는 것이 특징입니다. 한 예를 통해 모델의 응답을 효과적으로 가이드하고 조정할 수 있으며, 결정적인 제어가 효율적으로 접근할 수 있습니다. 실험적으로는 다양한 LLM에 대한 모델의 제어 성능을 보고, 이러한 기술의 효과성을 보여줍니다. GitHub의 https://github.com/zjunlp/EasyEdit에서 소스 코드를 릴리즈하고, 데모 노트북을 제공하고 있습니다. 또한, Quick Introduction을 위해 https://zjunlp.github.io/project/EasyEdit2/video에서 데모 비디오를 제공하고 있습니다.",
      "upvotes": 4,
      "discussionId": "68073a2e19a9fa60962186db",
      "projectPage": "https://zjunlp.github.io/project/EasyEdit2/",
      "githubRepo": "https://github.com/zjunlp/EasyEdit",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "test-time interventions",
        "steering vector generator",
        "steering vector applier",
        "seamless model steering",
        "model steering performance"
      ]
    },
    "publishedAt": "2025-04-21T10:33:55.000Z",
    "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models",
    "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15133.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14717",
      "authors": [
        {
          "_id": "680720e066b60d551b653f1b",
          "name": "Bowei Zhang",
          "hidden": false
        },
        {
          "_id": "680720e066b60d551b653f1c",
          "name": "Lei Ke",
          "hidden": false
        },
        {
          "_id": "680720e066b60d551b653f1d",
          "name": "Adam W. Harley",
          "hidden": false
        },
        {
          "_id": "680720e066b60d551b653f1e",
          "name": "Katerina Fragkiadaki",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/D3KLUoyXGD0mILRMznzGG.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/qsRjn3Gr351OY_A8w0tGc.gif"
      ],
      "publishedAt": "2025-04-20T19:09:43.000Z",
      "submittedOnDailyAt": "2025-04-22T03:25:44.916Z",
      "title": "TAPIP3D: 영원한 3차원 기하학에서 임의의 점의 추적\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "submittedOnDailyBy": {
        "_id": "6258a6455ea3a0a9b6de3f22",
        "avatarUrl": "/avatars/6eeed72a97fb24465e5e65583fbe50cf.svg",
        "isPro": false,
        "fullname": "Lei Ke",
        "user": "lkeab",
        "type": "user"
      },
      "summary": "TAPIP3D는 단일 RGB 및 RGB-D 비디오에서 장기간 3D 포인트 트래킹의 새로운 접근 방식을 소개합니다. TAPIP3D는 카메라의 안정된 공간 시간 특징량 클라우드로 비디오를 표현하고, 깊이 및 카메라의 움직임 정보를 사용하여 2D 비디오 특징량을 3D 월드 공간에 리프터밍하여 카메라의 움직임을 효과적으로 취소하여 실현합니다. TAPIP3D는 이 안정된 표현 내에서 장기간의 3D 움직임을 반복하여 정밀화하여 강력한 트래킹을 장기간 수행할 수 있습니다. 3D 포인트의 분포의 불규칙성을 관리하기 위해 Local Pair Attention 구조를 제안합니다. 이 3D 컨텍스트화 전략은 3D의 공간 관계를 효과적으로 활용하여 3D 트래지렉토리의 정확한 계산에 정보를 가진 특징량 네이브를 형성합니다. 우리 3D 중심적 접근은 현재의 3D 포인트 트래킹 방법보다 크게 우수하며, 정확한 깊이가 있는 경우, 전통적인 2D 픽셀 트래커와 비교하여 2D 트래킹의 정확도도 향상됩니다. 이 접근 방식은 카메라 좌표（즉, 안정되지 않은 좌표）와 세계 좌표 모두에서 추론을 지원하고 카메라의 움직임을 보정하여 트래킹 성능을 향상시킵니다. 우리 접근 방식은 이전의 2D 및 3D 트래커에서 사용된 전통적인 2D 정사각형 상관 네이브를 대체하여 다양한 3D 포인트 트래킹 벤치마크에서도 더 강건하고 정확한 결과를 얻을 수 있습니다. 프로젝트 페이지: https://tapip3d.github.io",
      "upvotes": 4,
      "discussionId": "680720e166b60d551b653f7d",
      "projectPage": "https://tapip3d.github.io/",
      "githubRepo": "https://github.com/zbw001/TAPIP3D",
      "ai_keywords": [
        "camera-stabilized",
        "spatio-temporal feature clouds",
        "depth and camera motion information",
        "2D video features",
        "3D world space",
        "multi-frame 3D motion estimates",
        "Local Pair Attention mechanism",
        "3D contextualization strategy",
        "spatial relationships in 3D",
        "feature neighborhoods",
        "3D trajectory estimation",
        "3D point tracking",
        "2D tracker",
        "inference in both camera coordinates",
        "world coordinates",
        "3D point tracking benchmarks",
        "2D square correlation neighborhoods"
      ]
    },
    "publishedAt": "2025-04-20T15:09:43.000Z",
    "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry",
    "summary": "We introduce TAPIP3D, a novel approach for long-term 3D point tracking in\nmonocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized\nspatio-temporal feature clouds, leveraging depth and camera motion information\nto lift 2D video features into a 3D world space where camera motion is\neffectively canceled. TAPIP3D iteratively refines multi-frame 3D motion\nestimates within this stabilized representation, enabling robust tracking over\nextended periods. To manage the inherent irregularities of 3D point\ndistributions, we propose a Local Pair Attention mechanism. This 3D\ncontextualization strategy effectively exploits spatial relationships in 3D,\nforming informative feature neighborhoods for precise 3D trajectory estimation.\nOur 3D-centric approach significantly outperforms existing 3D point tracking\nmethods and even enhances 2D tracking accuracy compared to conventional 2D\npixel trackers when accurate depth is available. It supports inference in both\ncamera coordinates (i.e., unstabilized) and world coordinates, and our results\ndemonstrate that compensating for camera motion improves tracking performance.\nOur approach replaces the conventional 2D square correlation neighborhoods used\nin prior 2D and 3D trackers, leading to more robust and accurate results across\nvarious 3D point tracking benchmarks. Project Page: https://tapip3d.github.io",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/D3KLUoyXGD0mILRMznzGG.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/qsRjn3Gr351OY_A8w0tGc.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14717.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6258a6455ea3a0a9b6de3f22",
      "avatarUrl": "/avatars/6eeed72a97fb24465e5e65583fbe50cf.svg",
      "fullname": "Lei Ke",
      "name": "lkeab",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15047",
      "authors": [
        {
          "_id": "6806fb8e27dabde0a109776d",
          "user": {
            "_id": "645b663eca5d8a297712f2e1",
            "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
            "isPro": false,
            "fullname": "Quy-Anh Dang",
            "user": "quyanh",
            "type": "user"
          },
          "name": "Quy-Anh Dang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:51:09.578Z",
          "hidden": false
        },
        {
          "_id": "6806fb8e27dabde0a109776e",
          "user": {
            "_id": "63105f7463b70252b4775783",
            "avatarUrl": "/avatars/89cfe71302cde6c2834f58d44bbcdbd5.svg",
            "isPro": false,
            "fullname": "Chris Ngo",
            "user": "tnngo2",
            "type": "user"
          },
          "name": "Chris Ngo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:51:07.511Z",
          "hidden": false
        },
        {
          "_id": "6806fb8e27dabde0a109776f",
          "name": "Truong-Son Hy",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/XbUmMPAitDuX-g7n50bTa.png",
        "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/a16jiYkgx_UtIyM9BiBc2.png"
      ],
      "publishedAt": "2025-04-21T12:04:57.000Z",
      "submittedOnDailyAt": "2025-04-22T00:47:26.080Z",
      "title": "레이보 플러스: 에보루치튜어리 카리테디바이지옹서니에 의한 전투적 프로노프토 생성 강화",
      "submittedOnDailyBy": {
        "_id": "645b663eca5d8a297712f2e1",
        "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
        "isPro": false,
        "fullname": "Quy-Anh Dang",
        "user": "quyanh",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 뛰어난 능력을 보여주지만, 대립적인 프롬프트에 취약성을 활용하여 불안정하거나 편향적인 출력을 생성하는 경우가 있습니다. 현재의 레드팀 믹스 법은, scalability의 문제, 자원 강조의 요구, 공격전략의 다양성 제한이 있습니다. 우리는 진화 계산에 기반한 새로운 레드팀 믹스 프레임워크 \"RainbowPlus\"를 제안합니다. 이 방법은 고전적인 진화 계산 알고리즘(예: MAP-Elites)에 적용된 혁신을 가지고, 적응적인 품질다양성(QD) 검색을 통해 대립적인 프롬프트의 생성을 강화합니다. 다요소 아카이브를 사용하여 다양한 고품질의 프롬프트를 저장하고, 다수의 프롬프트를 동시에 평가하는 평가 함수를 사용하여, RainbowPlus는 이전의 QD 방법(예: Rainbow Teaming)의 단일 프롬프트 아카이브와 두 가지 비교의 제약을 극복합니다. 6개의 벤치마크 데이터셋과 4개의 오픈소스 LLMs를 비교한 실험에서, RainbowPlus는 우수한 공격 성공률(ASR)과 다양성(Diverse-Score 약 0.84)을 나타내며, 100배 이상의 고유한 프롬프트(예: 10,418 vs. 100 for Ministral-8B-Instruct-2410)를 생성합니다. HarmBench 데이터셋의 9개의 최선 방법에 대해, 12개의 LLMs(10개의 오픈소스, 2개의 클로즈소스)를 사용하여, RainbowPlus는 평균 ASR 81.1%를 달성하며, AutoDAN-Turbo를 3.9% 초과하며, 1.45시간 동안 실행할 수 있는 반면, 13.50시간을 소요하여 9배 빠르며, LLM 안전성의 발전을 촉진하고 취약성 평가의 scalable한 도구를 제공합니다. 코드와 리소스는 https://github.com/knoveleng/rainbowplus에서 공개되어 있습니다.",
      "upvotes": 3,
      "discussionId": "6806fb9127dabde0a1097849",
      "githubRepo": "https://github.com/knoveleng/rainbowplus",
      "ai_keywords": [
        "adversarial prompts",
        "vulnerabilities",
        "unsafe outputs",
        "biased outputs",
        "red-teaming methods",
        "scalability challenges",
        "resource-intensive requirements",
        "quality-diversity (QD) search",
        "evolutionary computation",
        "MAP-Elites",
        "adaptive quality-diversity (QD) search",
        "multi-element archive",
        "fitness function",
        "single-prompt archives",
        "pairwise comparisons",
        "attack success rate (ASR)",
        "Diverse-Score",
        "superior attack success rate (ASR)",
        "diversity",
        "unique prompts",
        "benchmark datasets",
        "open-source LLMS",
        "HarmBench",
        "closed-source",
        "AutoDAN-Turbo",
        "vulnerability assessment"
      ]
    },
    "publishedAt": "2025-04-21T08:04:57.000Z",
    "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary\n  Quality-Diversity Search",
    "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score approx 0.84), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/XbUmMPAitDuX-g7n50bTa.png",
      "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/a16jiYkgx_UtIyM9BiBc2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15047.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "645b663eca5d8a297712f2e1",
      "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
      "fullname": "Quy-Anh Dang",
      "name": "quyanh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15270",
      "authors": [
        {
          "_id": "68074ea84a1c690663814e56",
          "name": "Ji Qi",
          "hidden": false
        },
        {
          "_id": "68074ea84a1c690663814e57",
          "name": "Yuan Yao",
          "hidden": false
        },
        {
          "_id": "68074ea84a1c690663814e58",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "68074ea84a1c690663814e59",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "68074ea84a1c690663814e5a",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "68074ea84a1c690663814e5b",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "68074ea84a1c690663814e5c",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:57:21.000Z",
      "submittedOnDailyAt": "2025-04-22T06:39:45.004Z",
      "title": "LMM를 활용하여 효율적인 이미지 이해를 실현하기 위한 강화된 구조화된 이미지 압축 기술",
      "submittedOnDailyBy": {
        "_id": "64ed568ccf6118a9379a61b8",
        "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
        "isPro": false,
        "fullname": "Yushi Bai",
        "user": "bys0318",
        "type": "user"
      },
      "summary": "대규모 다모둠 모델(LMMs)은, 연속된 TV 프레임을 인식하고, 시간적인 정보 밀도가 고유적으로 변화하는 TV에 대한 계산적 효과성을 발생시킵니다. 본 논문에서는, Quicksviewer를 소개합니다. Quicksviewer는, 갓벨・소프맥스를 사용하여 비균질 밀도의 TV를 갓볼로 분할하고, 각 갓볼에 대해 통일적인 리샘플링을 수행하여 효율적인 TV 이해를 실현하는 새로운 인식 패러다임인 LMM입니다. 이 간단하고 직관적인 접근 방식은, 시간적 밀도에 기반하여 온라인으로 동적으로 압축하여 공간-시간적冗長성(총 45배의 압축률)을 크게 줄이고, 큰 수용野를 활용한 효율적인 훈련을 가능하게 합니다. 텍스트 백보드에서 모델을 훈련시키고, 평균 420초/1fps의 긴 TV를 3단계로 단계적으로 처리하여 인식의 효율성을 높일 수 있습니다. 훈련용 텍스트와 비디오 샘플은 총 0.8M으로, 고정된 분할 전략을 사용한 직접적인 baseline보다 최대 8.72점의 정확도 향상을 달성하여 성능의 효과성을 보여주었습니다. Video-MME에서, Quicksviewer는 baseline에서 필요로 하는 프레임당 5% 미만의 토큰을 사용하며, 가벼운 긴 시퀀스으로 가장 선진한 성능을 발휘합니다. 이 패러다임에서, 입력 프레임의 수를 늘리면, 모델의 능력에 대해 명확한 파워 레이어의 법칙이 나타납니다. 또한, 갓볼 네트워크가 생성한 Segment는 TV의 연속적인 이벤트 분석에 도움을 줍니다.",
      "upvotes": 2,
      "discussionId": "68074eab4a1c690663814ef7",
      "projectPage": "https://quicksviewer.github.io/",
      "githubRepo": "https://github.com/quicksviewer/quicksviewer",
      "ai_keywords": [
        "LMMs (Large Multimodal Models)",
        "Quicksviewer",
        "Gumbel Softmax",
        "spatiotemporal redundancy",
        "large receptive field",
        "language backbone",
        "Video-MME",
        "power law",
        "cubing network"
      ]
    },
    "publishedAt": "2025-04-21T13:57:21.000Z",
    "title": "An LMM for Efficient Video Understanding via Reinforced Compression of\n  Video Cubes",
    "summary": "Large Multimodal Models (LMMs) uniformly perceive video frames, creating\ncomputational inefficiency for videos with inherently varying temporal\ninformation density. This paper present Quicksviewer, an LMM with new\nperceiving paradigm that partitions a video of nonuniform density into varying\ncubes using Gumbel Softmax, followed by a unified resampling for each cube to\nachieve efficient video understanding. This simple and intuitive approach\ndynamically compress video online based on its temporal density, significantly\nreducing spatiotemporal redundancy (overall 45times compression rate), while\nenabling efficient training with large receptive field. We train the model from\na language backbone through three progressive stages, each incorporating\nlengthy videos on average of 420s/1fps thanks to the perceiving efficiency.\nWith only 0.8M total video-text samples for training, our model outperforms the\ndirect baseline employing a fixed partitioning strategy by a maximum of 8.72 in\naccuracy, demonstrating the effectiveness in performance. On Video-MME,\nQuicksviewer achieves SOTA under modest sequence lengths using just up to 5\\%\nof tokens per frame required by baselines. With this paradigm, scaling up the\nnumber of input frames reveals a clear power law of the model capabilities. It\nis also empirically verified that the segments generated by the cubing network\ncan help for analyzing continuous events in videos.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ed568ccf6118a9379a61b8",
      "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
      "fullname": "Yushi Bai",
      "name": "bys0318",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14899",
      "authors": [
        {
          "_id": "68075a2bdadb28dddc13dfc6",
          "name": "Chenjie Cao",
          "hidden": false
        },
        {
          "_id": "68075a2bdadb28dddc13dfc7",
          "name": "Jingkai Zhou",
          "hidden": false
        },
        {
          "_id": "68075a2bdadb28dddc13dfc8",
          "name": "Shikai Li",
          "hidden": false
        },
        {
          "_id": "68075a2bdadb28dddc13dfc9",
          "name": "Jingyun Liang",
          "hidden": false
        },
        {
          "_id": "68075a2bdadb28dddc13dfca",
          "name": "Chaohui Yu",
          "hidden": false
        },
        {
          "_id": "68075a2bdadb28dddc13dfcb",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "68075a2bdadb28dddc13dfcc",
          "name": "Xiangyang Xue",
          "hidden": false
        },
        {
          "_id": "68075a2bdadb28dddc13dfcd",
          "name": "Yanwei Fu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64892bc82eafb7b91182bec5/ldOrEdwnbNQcTKZBcT4Oq.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/64892bc82eafb7b91182bec5/g5LyXAgsA-XhlQDd-q-_S.mp4"
      ],
      "publishedAt": "2025-04-21T07:10:41.000Z",
      "submittedOnDailyAt": "2025-04-22T07:30:21.972Z",
      "title": "Uni3C: 3D 강화 카메라와 인간 동작 제어의 정밀한 통합화된 비디오 생성 시스템",
      "submittedOnDailyBy": {
        "_id": "64892bc82eafb7b91182bec5",
        "avatarUrl": "/avatars/c9eed91e60f3ea227b35e111d3fc4200.svg",
        "isPro": false,
        "fullname": "chenjie cao",
        "user": "ewrfcas",
        "type": "user"
      },
      "summary": "카메라와 인간의 움직임의 제어는 비디오 생성 분야에서 광범위하게 연구되어 있습니다만, 현재의 방법들은 일반적으로 이들을 따로 처리하며, 둘 다 고품질의 어노테이션 데이터가 제한되어 있습니다. 이를 극복하기 위해, 비디오 생성에서 카메라와 인간의 움직임을 정밀하게 제어하는 통합적인 3D 확장 프레임워크인 웰3C(Uni3C)를 제안합니다. Uni3C에는 두 가지 주요 기여가 있습니다. 첫째, 비디오 생성의 백본이 동결된 상태에서 훈련된 플래그 앤 플레이 컨트롤러 PCDController를 제안하며, 단원 디피스로부터의 미투입 포인트 클러스터를 활용하여 정확한 카메라 제어를 실현합니다. 클러스터의 강력한 3D 선두와 비디오 fundament 모델의 강력한 능력을 활용하여, PCDController는 추론 백본이 동결되어 있거나 조정되어 있는지에 따라, 인상적인 일반화 능력을 보여주며, 이러한 유연성은 Uni3C의 다른 모듈이 특정 영역에서 훈련될 수 있으며, 각 모듈이 카메라 제어 또는 인간 움직임 제어에 초점을 맞추는 데 집중하여, 공통으로 어노테이션된 데이터의 의존성을 줄일 수 있습니다. 둘째, 추론 단계에서 공통으로 어레이된 3D 월드 가이드라인을 제안하며, 이는 스케니 포인트 클러스터와 SMPL-X 캐릭터를 무간적으로 통합하고, 각각의 카메라와 인간 움직임의 제어 신호를 통일하여, 비디오 생성의 백본이 조정되어 있는지에 따라 효과적인 제어를 실현합니다. 확장된 실험은 PCDController가 조정된 백본을 사용하여 카메라의 움직임을 구동할 때의 강력한 강건성을 확인합니다. Uni3C는 카메라의 제어 가능성과 인간 움직임의 품질 모두가 상대적으로 크게 향상되어 있습니다. 또한, 어려운 카메라의 움직임과 인간 동작을 특징으로 하는 적절한 검증 세트를 수집하여, 우리의 방법의 효과를 평가합니다.",
      "upvotes": 2,
      "discussionId": "68075a2edadb28dddc13e0ac",
      "projectPage": "https://ewrfcas.github.io/Uni3C/",
      "ai_keywords": [
        "plug-and-play control module",
        "PCDController",
        "unprojected point clouds",
        "monocular depth",
        "3D priors",
        "video foundational models",
        "jointly aligned 3D world guidance",
        "scenery point clouds",
        "SMPL-X characters",
        "camera controllability",
        "human motion quality"
      ]
    },
    "publishedAt": "2025-04-21T03:10:41.000Z",
    "title": "Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls\n  for Video Generation",
    "summary": "Camera and human motion controls have been extensively studied for video\ngeneration, but existing approaches typically address them separately,\nsuffering from limited data with high-quality annotations for both aspects. To\novercome this, we present Uni3C, a unified 3D-enhanced framework for precise\ncontrol of both camera and human motion in video generation. Uni3C includes two\nkey contributions. First, we propose a plug-and-play control module trained\nwith a frozen video generative backbone, PCDController, which utilizes\nunprojected point clouds from monocular depth to achieve accurate camera\ncontrol. By leveraging the strong 3D priors of point clouds and the powerful\ncapacities of video foundational models, PCDController shows impressive\ngeneralization, performing well regardless of whether the inference backbone is\nfrozen or fine-tuned. This flexibility enables different modules of Uni3C to be\ntrained in specific domains, i.e., either camera control or human motion\ncontrol, reducing the dependency on jointly annotated data. Second, we propose\na jointly aligned 3D world guidance for the inference phase that seamlessly\nintegrates both scenic point clouds and SMPL-X characters to unify the control\nsignals for camera and human motion, respectively. Extensive experiments\nconfirm that PCDController enjoys strong robustness in driving camera motion\nfor fine-tuned backbones of video generation. Uni3C substantially outperforms\ncompetitors in both camera controllability and human motion quality.\nAdditionally, we collect tailored validation sets featuring challenging camera\nmovements and human actions to validate the effectiveness of our method.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64892bc82eafb7b91182bec5/ldOrEdwnbNQcTKZBcT4Oq.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/64892bc82eafb7b91182bec5/g5LyXAgsA-XhlQDd-q-_S.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14899.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64892bc82eafb7b91182bec5",
      "avatarUrl": "/avatars/c9eed91e60f3ea227b35e111d3fc4200.svg",
      "fullname": "chenjie cao",
      "name": "ewrfcas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13941",
      "authors": [
        {
          "_id": "6806f6ff67a715240a5ab9f8",
          "name": "Syeda Nahida Akter",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9f9",
          "user": {
            "_id": "66980b9c9baa4382e1678809",
            "avatarUrl": "/avatars/1a516bb7aa7871834c19de708cdd853a.svg",
            "isPro": false,
            "fullname": "Shrimai Prabhumoye",
            "user": "shrimai19",
            "type": "user"
          },
          "name": "Shrimai Prabhumoye",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-22T01:55:12.561Z",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fa",
          "name": "Matvei Novikov",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fb",
          "name": "Seungju Han",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fc",
          "name": "Ying Lin",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fd",
          "name": "Evelina Bakhturi",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fe",
          "name": "Eric Nyberg",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9ff",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5aba00",
          "name": "Mostofa Patwary",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5aba01",
          "name": "Mohammad Shoeybi",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5aba02",
          "name": "Bryan Catanzaro",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T21:37:13.000Z",
      "submittedOnDailyAt": "2025-04-22T00:28:47.499Z",
      "title": "NEMOTRON-CROSSTHINK: 수학론보다 더 넓은 범위의 자동 학습의 스케일링",
      "submittedOnDailyBy": {
        "_id": "6338dd1776421c0543150467",
        "avatarUrl": "/avatars/4539dcec644e40be33f4a0d419fa66cb.svg",
        "isPro": false,
        "fullname": "Syeda Nahida Akter",
        "user": "SieraL",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)은 특히 강화 학습(RL)에 의해 강화된 경우 강력한 논리 능력을 보여주고 있습니다. 이전 연구에서 수학적인 논리에 대해서는 규칙과 정확성이 명확하여, RL을 성공적으로 적용했습니다. 그러나 데이터의 제한, 증명이 불가능한 보상 구조, 그리고 다양한 태스크의 요구로 인해 이러한 방법을 광범위한 논리 영역에 일반화하는 것은 어려웠습니다. 본 논문에서는 다양한 분야의 코퍼스를 체계적으로 RL의 훈련에 통합하여 다양한 논리 태스크의 확장성을 향상시키기 위한 프레임워크인 'NEMOTRON-CROSSTHINK'을 제안합니다. NEMOTRON-CROSSTHINK는 (1) STEM, 인문학, 사회과학 등 다양한 분야에서부터 데이터를 사용합니다. (2) 구조付き 템플릿(예: 선택肢와 개방형)을 적용하여 응답 공간의 복잡성을 제어합니다. (3) 증명이 있는 응답을 필터링합니다. (4) 다수의 분야에서부터 데이터를 효과적으로 활용하는 데이터 브릿딩 전략을 최적화함으로써 중요한 문제를 해결합니다. 우리의 접근 방식은 수학보다 증명이 있는 보상 모델링을 가능하게 하며, 수학(MATH-500: +30.1%, AMC23: +27.5%)과 수학이 아닌 논리 벤치마크(MMLU-PRO: +12.8%, GPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%)의 정확도를 향상시킵니다. 또한, NEMOTRON-CROSSTHINK은 28% 이상 토큰을 사용하여 올바른 응답을 보여주는 데 사용되어, 응답의 효율이 크게 향상되고, 더 집중적이고 효과적인 논리를 보여주고 있습니다. NEMOTRON-CROSSTHINK을 통해, 우리는 다양한 분야, 다양한 형식의 데이터를 RL에 통합하여 더 정확한, 효율적인, 일반화 가능한 LLMs를 실현할 수 있음을 보여줍니다.",
      "upvotes": 2,
      "discussionId": "6806f70067a715240a5aba4c",
      "ai_keywords": [
        "Reinforcement Learning",
        "NEMOTRON-CROSSTHINK",
        "multi-domain corpora",
        "structured templates",
        "answer-space complexity",
        "verifiable answers",
        "data blending strategies",
        "reward modeling",
        "MATH-500",
        "AMC23",
        "MMLU-PRO",
        "GPQA-DIAMOND",
        "AGIEVAL",
        "SUPERGPQA",
        "response efficiency",
        "tokens"
      ]
    },
    "publishedAt": "2025-04-15T17:37:13.000Z",
    "title": "NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning",
    "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities,\nparticularly when enhanced through Reinforcement Learning (RL). While prior\nwork has successfully applied RL to mathematical reasoning -- where rules and\ncorrectness are well-defined -- generalizing these methods to broader reasoning\ndomains remains challenging due to limited data, the lack of verifiable reward\nstructures, and diverse task requirements. In this work, we propose\nNEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain\ncorpora, including both synthetic and real-world question-answer pairs, into RL\ntraining to improve generalization across diverse reasoning tasks.\nNEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from\nvaried sources spanning STEM, humanities, social sciences, etc.; (2) applying\nstructured templates (e.g., multiple-choice and open-ended) to control\nanswer-space complexity; (3) filtering for verifiable answers; and (4)\noptimizing data blending strategies that utilizes data from multiple sources\neffectively. Our approach enables scalable and verifiable reward modeling\nbeyond mathematics and demonstrates improved accuracies on both math (MATH-500:\n+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,\nGPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,\nNEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --\nusing 28% fewer tokens for correct answers -- highlighting more focused and\neffective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that\nintegrating multi-domain, multi-format data in RL leads to more accurate,\nefficient, and generalizable LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13941.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6338dd1776421c0543150467",
      "avatarUrl": "/avatars/4539dcec644e40be33f4a0d419fa66cb.svg",
      "fullname": "Syeda Nahida Akter",
      "name": "SieraL",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.12186",
      "authors": [
        {
          "_id": "680764930b2f7fda706f6c03",
          "name": "Alejandro Newell",
          "hidden": false
        },
        {
          "_id": "680764930b2f7fda706f6c04",
          "name": "Peiyun Hu",
          "hidden": false
        },
        {
          "_id": "680764930b2f7fda706f6c05",
          "name": "Lahav Lipson",
          "hidden": false
        },
        {
          "_id": "680764930b2f7fda706f6c06",
          "name": "Stephan R. Richter",
          "hidden": false
        },
        {
          "_id": "680764930b2f7fda706f6c07",
          "name": "Vladlen Koltun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T15:40:15.000Z",
      "submittedOnDailyAt": "2025-04-22T08:13:12.160Z",
      "title": "CoMotion: 연속된 다기능 3D 동작",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "우리는 단일의 단일카라메카메라 스트리밍에서 다수의 사람의 상세한 3D 포즈를 검출하고 추적하는 방법을 소개합니다. 우리의 시스템은 복잡한 포즈와 가려진 혼잡한 스케네에서 시간적 일치를 유지합니다. 우리의 모델은 각 프레임에서 강력한 검출과 학습된 포즈 업데이트를 수행하고, 프레임에서 프레임으로 사람을 추적합니다. 검출을 시간적으로 매칭하는 것보다, 새로운 입력 이미지에서 포즈를 직접 업데이트하고, 가려지며 온라인 추적을 가능하게 합니다. 다수의 이미지 데이터셋과 비디오 데이터셋을 사용하여, 위상 라벨링된 어노테이션을 확장하여, 3D 포즈 추정의 최상위 시스템과 같은 정확도를 달성하고, 시간 경과에 따라 다수의 사람을 추적함으로써 더욱 빠르게 정확한 모델을 생성합니다. 코드와 가중치는 https://github.com/apple/ml-comotion에서 제공됩니다.",
      "upvotes": 0,
      "discussionId": "680764950b2f7fda706f6ca7",
      "ai_keywords": [
        "3D poses",
        "monocular camera",
        "temporally coherent predictions",
        "occlusions",
        "pose update",
        "online tracking",
        "pseudo-labeled annotations",
        "state-of-the-art systems",
        "3D pose estimation accuracy"
      ]
    },
    "publishedAt": "2025-04-16T11:40:15.000Z",
    "title": "CoMotion: Concurrent Multi-person 3D Motion",
    "summary": "We introduce an approach for detecting and tracking detailed 3D poses of\nmultiple people from a single monocular camera stream. Our system maintains\ntemporally coherent predictions in crowded scenes filled with difficult poses\nand occlusions. Our model performs both strong per-frame detection and a\nlearned pose update to track people from frame to frame. Rather than match\ndetections across time, poses are updated directly from a new input image,\nwhich enables online tracking through occlusion. We train on numerous image and\nvideo datasets leveraging pseudo-labeled annotations to produce a model that\nmatches state-of-the-art systems in 3D pose estimation accuracy while being\nfaster and more accurate in tracking multiple people through time. Code and\nweights are provided at https://github.com/apple/ml-comotion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12186.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 829
    },
    "isAuthorParticipating": false
  }
]