[
  {
    "paper": {
      "id": "2504.15376",
      "authors": [
        {
          "_id": "680bda9c34c8d0bd08e01a25",
          "user": {
            "_id": "64c170190bfb901b04399295",
            "avatarUrl": "/avatars/c30ce7566ae3497ddc989ec8918d37cc.svg",
            "isPro": false,
            "fullname": "Zhiqiu Lin",
            "user": "zhiqiulin",
            "type": "user"
          },
          "name": "Zhiqiu Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:01.030Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a26",
          "user": {
            "_id": "65f82fb0de5e636ca20184fa",
            "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
            "isPro": false,
            "fullname": "Alan",
            "user": "syCen",
            "type": "user"
          },
          "name": "Siyuan Cen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:05.915Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a27",
          "name": "Daniel Jiang",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a28",
          "name": "Jay Karhade",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a29",
          "user": {
            "_id": "67b2db158904ba09ca8feb79",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b2db158904ba09ca8feb79/faCKKdyroDNCcylEAQZKu.png",
            "isPro": false,
            "fullname": "Hewei Wang",
            "user": "Stephen624",
            "type": "user"
          },
          "name": "Hewei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:03.301Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2a",
          "name": "Chancharik Mitra",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2b",
          "name": "Tiffany Ling",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2c",
          "name": "Yuhan Huang",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2d",
          "name": "Sifan Liu",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2e",
          "name": "Mingyu Chen",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2f",
          "name": "Rushikesh Zawar",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a30",
          "name": "Xue Bai",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a31",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a32",
          "name": "Chuang Gan",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a33",
          "name": "Deva Ramanan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T18:34:57.000Z",
      "submittedOnDailyAt": "2025-04-28T00:10:15.204Z",
      "title": "Towards Understanding Camera Motions in Any Video",
      "submittedOnDailyBy": {
        "_id": "65f82fb0de5e636ca20184fa",
        "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
        "isPro": false,
        "fullname": "Alan",
        "user": "syCen",
        "type": "user"
      },
      "summary": "카메라벤치는 카메라의 동작을 이해하고 개선하기 위해 설계된 큰 규모의 데이터셋과 벤치마크입니다. 카메라벤치는 약 3,000개의 다양한 인터넷 비디오로 구성되며 전문가가 엄격한 다단계의 품질관리 프로세스를 통해 설명되어 있습니다. 우리의 기여 중 하나는 카메라 동작의 기본 요소인 탤러닝을 설계한 것입니다. 예를 들어, \"추적\" (또는 타락)과 같은 동작은 움직이는 주체의 내용을 이해해야 하는 것을 발견했습니다. 우리는 인간 설명 능력의 측정을 위해 규모적인 인간 조사를 수행하여, 영역의 전문 지식과 튜토리얼 기반의 훈련이 정확도를 크게 향상시키는 것을 밝혀 냈습니다. 예를 들어, 초보자는 \"존\" (인트린시크의 변화)와 \"진\" (엑스트린시크의 변화)를 혼동할 수 있지만, 이를 구분할 수 있도록 훈련을 받습니다. 카메라벤치를 사용하여 구조로부터의 동작 (SfM)과 비디오 언어 모델 (VLM)을 평가하고, SfM 모델은 스케ن 콘텐츠에 따라 의미적인 기본 요소를 쉽게 이해하게 되고, VLM은 정확도가 높은궤도 추정이 필요한 기오메트리 기본 요소를 쉽게 이해하게 됩니다. 그 후, 카메라벤치를 사용하여 생성된 VLM을 미세 조정하고, 두 가지 모두를 최적화하여 이동 추가된 서브 시즈, 비디오 질문의 답변, 비디오 텍스트의 검색 등 다양한 응용을 보여주었습니다. 우리는 탤러닝, 벤치마크, 튜토리얼을 사용하여 비디오 내의 카메라 동작을 이해하는 최종 목표를 향한 미래의 노력을 추진하는 것을 희망합니다.",
      "upvotes": 110,
      "discussionId": "680bda9e34c8d0bd08e01ae9",
      "projectPage": "https://linzhiqiu.github.io/papers/camerabench/",
      "githubRepo": "https://github.com/sy77777en/CameraBench",
      "ai_keywords": [
        "Structure-from-Motion (SfM)",
        "Video-Language Models (VLMs)",
        "semantic primitives",
        "geometric primitives",
        "generative VLM",
        "motion-augmented captioning",
        "video question answering",
        "video-text retrieval"
      ]
    },
    "publishedAt": "2025-04-21T14:34:57.000Z",
    "title": "Towards Understanding Camera Motions in Any Video",
    "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f82fb0de5e636ca20184fa",
      "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
      "fullname": "Alan",
      "name": "syCen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16656",
      "authors": [
        {
          "_id": "6809a4ac81a95c83f0c81c83",
          "name": "Chris",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c84",
          "name": "Yichen Wei",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c85",
          "name": "Yi Peng",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c86",
          "name": "Xiaokun Wang",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c87",
          "name": "Weijie Qiu",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c88",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c89",
          "name": "Tianyidan Xie",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8a",
          "name": "Jiangbo Pei",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8b",
          "name": "Jianhao Zhang",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8c",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8d",
          "user": {
            "_id": "6462b241b438438da3c25a5d",
            "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
            "isPro": false,
            "fullname": "Xuchen Song",
            "user": "xuchensong",
            "type": "user"
          },
          "name": "Xuchen Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:35:17.241Z",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8e",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8f",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T12:24:10.000Z",
      "submittedOnDailyAt": "2025-04-28T05:19:19.230Z",
      "title": "Skywork R1V2: 논리론에 대한 다모둠 하이브리드 강화학습을 활용한 연구",
      "submittedOnDailyBy": {
        "_id": "6462b241b438438da3c25a5d",
        "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
        "isPro": false,
        "fullname": "Xuchen Song",
        "user": "xuchensong",
        "type": "user"
      },
      "summary": "Skywork R1V2를 소개합니다. 이것은 다음 세대의 다 모델 인지 모델이며, 이전의 Skywork R1V와 대폭 발전되어 있습니다. R1V2의 핵심은 보상 모델의 가이드와 규칙 기반의 전략을 결합한 하이브리드 강화 학습 패러다임입니다. 이로 인해 복잡한 인지 능력과 광범위한 일반화에 대한 오랜 문제점을 해결할 수 있습니다. 또한, 훈련 효율화를 위해 선택적 샘플 버퍼(SSB) 구조를 제안하고 있습니다. 이것은 그룹 상대 정책 최적화(GRPO)에서 \"소멸하는 이득\" 문제를 해결하기 위해 최적화 과정에서 고가치 샘플을 우선시하는 것입니다. 특히, 과도한 강화 신호가 시각의 헛새를 불러일으키는 것을 관찰하고, 훈련 프로세스 중 조정된 보상 쉘더를 통해 이 현상을 체계적으로 관찰하고 완화하는 것입니다. 실험 결과를 통해 R1V2의 뛰어난 능력을 입증하고, OlympiadBench에서 62.6, AIME2024에서 79.0, LiveCodeBench에서 63.6, MMMU에서 74.0의 성능을 보여주고 있습니다. 이러한 결과는 현재의 오픈 소스 모델을 초과하는 우수한 성능을 보여주고, 젝미 2.5와 OpenAI o4-mini 등 선진 프로프라이터 시스템과의 성능 간격을 좁히는 발전을 나타냅니다. Skywork R1V2 모델의 가중치는 공개되어 있으며, 개방성과 재현성을 촉진하기 위해 공개되어 있습니다. 홈페이지는 https://huggingface.co/Skywork/Skywork-R1V2-38B입니다.",
      "upvotes": 38,
      "discussionId": "6809a4ae81a95c83f0c81cda",
      "githubRepo": "https://github.com/SkyworkAI/Skywork-R1V",
      "ai_keywords": [
        "reinforcement learning",
        "reward-model guidance",
        "rule-based strategies",
        "Selective Sample Buffer (SSB)",
        "Vanishing Advantages",
        "Group Relative Policy Optimization (GRPO)",
        "visual hallucinations",
        "calibrated reward thresholds",
        "benchmark-leading performances",
        "OlympiadBench",
        "AIME2024",
        "LiveCodeBench",
        "MMMU",
        "Skywork R1V2-38B"
      ]
    },
    "publishedAt": "2025-04-23T08:24:10.000Z",
    "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning",
    "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a\nmajor leap forward from its predecessor, Skywork R1V. At its core, R1V2\nintroduces a hybrid reinforcement learning paradigm that harmonizes\nreward-model guidance with rule-based strategies, thereby addressing the\nlong-standing challenge of balancing sophisticated reasoning capabilities with\nbroad generalization. To further enhance training efficiency, we propose the\nSelective Sample Buffer (SSB) mechanism, which effectively counters the\n``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization\n(GRPO) by prioritizing high-value samples throughout the optimization process.\nNotably, we observe that excessive reinforcement signals can induce visual\nhallucinations--a phenomenon we systematically monitor and mitigate through\ncalibrated reward thresholds throughout the training process. Empirical results\naffirm the exceptional capability of R1V2, with benchmark-leading performances\nsuch as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and\n74.0 on MMMU. These results underscore R1V2's superiority over existing\nopen-source models and demonstrate significant progress in closing the\nperformance gap with premier proprietary systems, including Gemini 2.5 and\nOpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to\npromote openness and reproducibility\nhttps://huggingface.co/Skywork/Skywork-R1V2-38B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6462b241b438438da3c25a5d",
      "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
      "fullname": "Xuchen Song",
      "name": "xuchensong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18415",
      "authors": [
        {
          "_id": "680ef1549cc294f617fb14b4",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "680ef1549cc294f617fb14b5",
          "name": "Shuming Ma",
          "hidden": false
        },
        {
          "_id": "680ef1549cc294f617fb14b6",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-25T15:17:52.000Z",
      "submittedOnDailyAt": "2025-04-28T01:39:22.422Z",
      "title": "BitNet v2: 지역 4비트 액티브 동작에 헤ида머드 변환을 적용한 1비트 LLM",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "1-bit Large Language Models (LLMs)의 효율적인 배치는 활성화 아웃라이어（activation outliers）로 인해 방해가 됩니다. 이러한 아웃라이어는 낮은 비트 폭에 대한 양식화（quantization）을 복잡하게 만들며 효율적인 배치가 어려워집니다. 우리는 1-bit LLMs의 원생 4비트 활성화를 양식화할 수 있는 새로운 프레임워크 BitNet v2를 소개합니다. 특히, 注意力 네트워크와 전방 네트워크의 활성화 아웃라이어 문제를 해결하기 위해, 온라인 Hademard 변환을 적용하는 H-BitLinear 모듈을 제안합니다. 이 변환은 활성화 분포를 가우시안 분포처럼 평활화하고, 낮은 비트 표현에 적합한 형태로 변환합니다. 실험 결과를 통해, 8비트 활성화를 사용한 BitNet v2의 학습은 BitNet b1.58과 같은 성능을 나타냅니다. 중요한 점은 BitNet v2는 4비트의 원생 활성화를 사용하여 학습하며, 최소한의 성능 저하를 감안하여, 배치 추론의 메모리 플리트와 계산 비용의 대폭 감소가 가능합니다.",
      "upvotes": 17,
      "discussionId": "680ef1559cc294f617fb1536",
      "ai_keywords": [
        "BitNet v2",
        "1-bit Large Language Models (LLMs)",
        "activation outliers",
        "quantization",
        "4-bit activation quantization",
        "H-BitLinear",
        "Hadamard transformation",
        "activation distributions",
        "Gaussian-like forms",
        "low-bit representation",
        "8-bit activations",
        "BitNet b1.58",
        "batched inference"
      ]
    },
    "publishedAt": "2025-04-25T11:17:52.000Z",
    "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs",
    "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17821",
      "authors": [
        {
          "_id": "680f56b8da9639d22c64443f",
          "name": "Xinyu Chen",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644440",
          "name": "Yunxin Li",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644441",
          "name": "Haoyuan Shi",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644442",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644443",
          "name": "Wenhan Luo",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644444",
          "name": "Yaowei Wang",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644445",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T13:47:30.000Z",
      "submittedOnDailyAt": "2025-04-28T09:15:13.533Z",
      "title": "VideoVista-CulturalLingo: 360°의 경계 - 영화 이해에서 문화, 언어, 분야를 연결하는 것\n\n(Note: The original title \"360°の境界\" is translated as \"360°의 경계\" to maintain the same meaning and context in Korean, where \"경계\" can be interpreted as \"경계\" or \"경계\" depending on the context. Here, \"경계\" is used to convey the sense of a comprehensive or all-encompassing perspective.)",
      "submittedOnDailyBy": {
        "_id": "62fdb01bc1588e1d4c6c1a7c",
        "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
        "isPro": false,
        "fullname": "Yunxin Li",
        "user": "YunxinLi",
        "type": "user"
      },
      "summary": "ビデオ 이해 능력을 평가하기 위한 다모달 AI 시스템의 평가는 그들의 이해와 이유론의 능력을 효과적으로 측정할 수 있습니다. 비디오 평가 벤치마크는 일반적으로 영어 중심이며, 주로 서양 문화의 배경에서 제작된 비디오를 특징으로 합니다. 본 논문에서는 VideoVista-CulturalLingo라는 첫 번째 비디오 평가 벤치마크를 소개합니다. 이 벤치마크는 비디오 이해의 문화적, 언어적, 영역적 간극을 연결하는 것입니다. 우리 연구는 현재 존재하는 벤치마크와 다음과 같은 차이점이 있습니다:\n\n1. 문화다양성: 중국, 미국, 유럽의 문화를 사용합니다.\n2. 다언어성: 중국어와 영어로 작성된 질문이 포함됩니다. 이들은 가장 많이 사용되는 언어 중 두 가지입니다.\n3. 광범위한 영역: 인간이 만든 수백 개의 영역에서 제작된 비디오를 사용합니다.\n\nVideoVista-CulturalLingo는 1,389개의 비디오와 3,134개의 QA 페어를 포함하며, 24개의 최근 오픈 소스 또는 소유 비디오 모델을 평가했습니다. 실험 결과를 통해 다음과 같은 것을 확인했습니다:\n\n1. 현재의 모델은 서양 중심의 질문에 비해 중국 중심의 질문에 더 나은 성능을 나타내지 않고, 특히 중국의 역사에 대한 질문에 대해 특히 나쁨을 나타냅니다.\n2. 현재의 오픈 소스 모델은 시간적인 이해에 특히 한계가 있으며, 특히 이벤트 로카라이즈 태스크에서 최대 45.2%의 점수를 달성할 수 없습니다.\n3.主流 모델은 일반적인 과학적인 질문에 강력한 성능을 보여주지만, 오픈 소스 모델은 수학에 약한 성능을 나타냅니다.",
      "upvotes": 14,
      "discussionId": "680f56bdda9639d22c64456b",
      "projectPage": "https://videovista-culturallingo.github.io/",
      "githubRepo": "https://github.com/HITsz-TMG/VideoVista"
    },
    "publishedAt": "2025-04-23T09:47:30.000Z",
    "title": "VideoVista-CulturalLingo: 360^circ Horizons-Bridging Cultures,\n  Languages, and Domains in Video Comprehension",
    "summary": "Assessing the video comprehension capabilities of multimodal AI systems can\neffectively measure their understanding and reasoning abilities. Most video\nevaluation benchmarks are limited to a single language, typically English, and\npredominantly feature videos rooted in Western cultural contexts. In this\npaper, we present VideoVista-CulturalLingo, the first video evaluation\nbenchmark designed to bridge cultural, linguistic, and domain divide in video\ncomprehension. Our work differs from existing benchmarks in the following ways:\n1) Cultural diversity, incorporating cultures from China, North America, and\nEurope; 2) Multi-linguistics, with questions presented in Chinese and\nEnglish-two of the most widely spoken languages; and 3) Broad domain, featuring\nvideos sourced from hundreds of human-created domains. VideoVista-CulturalLingo\ncontains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent\nopen-source or proprietary video large models. From the experiment results, we\nobserve that: 1) Existing models perform worse on Chinese-centric questions\nthan Western-centric ones, particularly those related to Chinese history; 2)\nCurrent open-source models still exhibit limitations in temporal understanding,\nespecially in the Event Localization task, achieving a maximum score of only\n45.2%; 3) Mainstream models demonstrate strong performance in general\nscientific questions, while open-source models demonstrate weak performance in\nmathematics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fdb01bc1588e1d4c6c1a7c",
      "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
      "fullname": "Yunxin Li",
      "name": "YunxinLi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16427",
      "authors": [
        {
          "_id": "680c48805ec65044c2861a6a",
          "user": {
            "_id": "669090c01e3f5b16ce22b535",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
            "isPro": false,
            "fullname": "Hanlei Zhang",
            "user": "HanleiZhang",
            "type": "user"
          },
          "name": "Hanlei Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:52:58.965Z",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6b",
          "name": "Zhuohang Li",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6c",
          "name": "Yeshuang Zhu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6d",
          "name": "Hua Xu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6e",
          "name": "Peiwu Wang",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6f",
          "name": "Haige Zhu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a70",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a71",
          "name": "Jinchao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T05:25:13.000Z",
      "submittedOnDailyAt": "2025-04-28T00:53:49.838Z",
      "title": "라르지 라ン지 모듈 모델은 다모달 언어 분석에 도움을 줄 수 있는지? MMLA: 복잡한 벤치마크",
      "submittedOnDailyBy": {
        "_id": "669090c01e3f5b16ce22b535",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
        "isPro": false,
        "fullname": "Hanlei Zhang",
        "user": "HanleiZhang",
        "type": "user"
      },
      "summary": "다형 언어 분석은 다양한 모듈을 활용하여, 인간 대화의 깊은 세ман틱을 이해하기 위해 급격히 발전 중인 분야입니다. 이러한 중요성에 대해, 다형 언어 모델(MLLMs)이 인지 수준의 세ман틱을 이해하는 능력에 대한 연구는 부족합니다. 본 논문에서는 이러한 빈대에 대한 해결책을 찾기 위해, 특히 설계된 세부적인 벤치마크인 MMLA를 소개합니다. MMLA는 61K 이상의 다형 대화를 포함하며, 단계화된 시나리오와 실세계의 시나리오로 구성되어 있으며, 다형 세ман틱의 6가지 핵심적인 차원을 덮습니다: 의도, 감정, 다이라구어 액트, 센티멘트, 대화방식, 커뮤니케이션의 행동입니다. LLMs와 MLLMs의 8가지 주요 분야를 3가지의 방법에서 평가합니다: 0-shot 추론, 서브프로젝트 조정, 매뉴얼 조정. 확장된 실험 결과에 따르면, 상당히 조정된 모델에서도 약 60%~70%의 정확도를 달성할 수 있다는 것을 알 수 있으며, 현재의 MLLMs가 복잡한 인간 언어를 이해하는 능력의 한계를 명확히 합니다. MMLA는 대 언어 모델의 다형 언어 분석의 가능성에 대한 강력한 기초를 제공하며, 이 분야의 진보에 기여하는 유익한 자원을 제공합니다. 데이터셋과 코드는 https://github.com/thuiar/MMLA에서 오픈 소스로 제공됩니다.",
      "upvotes": 9,
      "discussionId": "680c48825ec65044c2861ac4",
      "githubRepo": "https://github.com/thuiar/MMLA",
      "ai_keywords": [
        "multimodal language models (MLLMs)",
        "MMLA (Multimodal Language Analysis)",
        "multimodal utterances",
        "intent",
        "emotion",
        "dialogue act",
        "sentiment",
        "speaking style",
        "communication behavior",
        "zero-shot inference",
        "supervised fine-tuning",
        "instruction tuning",
        "large language models (LLMs)"
      ]
    },
    "publishedAt": "2025-04-23T01:25:13.000Z",
    "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
    "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669090c01e3f5b16ce22b535",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
      "fullname": "Hanlei Zhang",
      "name": "HanleiZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17768",
      "authors": [
        {
          "_id": "680f2668db85fd31cd5080ff",
          "name": "Piotr Nawrot",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508100",
          "name": "Robert Li",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508101",
          "name": "Renjie Huang",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508102",
          "name": "Sebastian Ruder",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508103",
          "name": "Kelly Marchisio",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508104",
          "name": "Edoardo M. Ponti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T17:39:25.000Z",
      "submittedOnDailyAt": "2025-04-28T05:26:26.185Z",
      "title": "Sparse Frontier: Transformer LLMs의 희소적 注意의 균형점\n\n(Note: The term \"注意\" is a direct translation of \"attention\" in Korean, but it might be more appropriate to use \"주의\" or \"관심\" depending on the context. For the sake of accuracy, I have used \"주의\" here.)",
      "submittedOnDailyBy": {
        "_id": "640deb5d3c82bd463ee44735",
        "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
        "isPro": false,
        "fullname": "Piotr Nawrot",
        "user": "pnawrot",
        "type": "user"
      },
      "summary": "Sparse attention은 Transformer LLM의 긴 문맥 처리 능력을 확장하는 잠재적으로 유용한 전략입니다が, 그 가능성, 효율성과 정확성 간의 트레이드오프, 그리고 시스템적 스케일링 연구는 아직 조사되지 않았습니다. 이 빈도를 메꾸기 위해, 다양한 긴 문맥 태스크의 집합(이 집합에는 자연어 기반의 새로운 태스크를 포함하지만, 제어 가능하고 평가가 간단한 것을 유지합니다)에 대해, 서로 다른 모델 크기, 문맥 길이, 스패르스 수준에 따른 훈련되지 않은 스패르스 어텐션 메소드를 보다 상세하게 비교합니다. 실험의 기초에 따라, 다음 주요한 발견을 보고합니다: 1) isoFLOPS 분석에 의해, 매우 긴 문맥의 경우, 큰고 높은 스패르스 모델은 작은고 밀한 모델보다 우수함을 명확히 알립니다. 2) 통계적으로 정확도를 유지하기 위한 스패르스 수준은, 디코딩 시보다 사전 처리 시에 더 높으며, 이는 디코딩 시에 이러한 수준이 모델 크기에 관계되어 있음을 나타냅니다. 3) 태스크 또는 전체 단계에서 최적의 스텝이 명확하지 않습니다, 대신, 다양한 스패르시fication 단위와 시각적 적응이 필요한 시나리오마다 다릅니다. 이로 인하여, 중度的 스패르스 수준에서는 적어도 한 가지 태스크에서 성능 저하가 크게 보입니다, 이는 스패르스 어텐션은 일반적인 해결책이 아님을 강조합니다. 4) 스패르스 어텐션에 특화된 스케일링 규칙을 제안하고, 이 규칙이 우리의 실험 범위를 초과해도 사실적으로 존재함을 증명합니다. 이러한 관점을 기반으로, 스패르스 어텐션은 Transformer LLM의 긴 문맥 처리 능력을 향상시키는 중요한 도구입니다, 성능 스핀팝의 애플리케이션에서 평가의 트레이드오프를 신중히 수행하는 것이 필요합니다.",
      "upvotes": 7,
      "discussionId": "680f2669db85fd31cd50815e",
      "ai_keywords": [
        "Sparse attention",
        "Transformer LLMs",
        "Training-free",
        "IsoFLOPS analysis",
        "Sequence lengths",
        "Sparsity levels",
        "Long-sequence tasks",
        "Natural language",
        "Accuracy preservation",
        "Decoding",
        "Prefilling",
        "Budget adaptivity",
        "Performance degradation",
        "Scaling laws"
      ]
    },
    "publishedAt": "2025-04-24T13:39:25.000Z",
    "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
    "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17768.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "640deb5d3c82bd463ee44735",
      "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
      "fullname": "Piotr Nawrot",
      "name": "pnawrot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17816",
      "authors": [
        {
          "_id": "680ed2679e529f7799a0689f",
          "user": {
            "_id": "636b20591340f879a2eb98d0",
            "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
            "isPro": false,
            "fullname": "Daneul Kim",
            "user": "carpedkm",
            "type": "user"
          },
          "name": "Daneul Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-28T07:38:37.502Z",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a0",
          "name": "Jingxu Zhang",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a1",
          "name": "Wonjoon Jin",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a2",
          "name": "Sunghyun Cho",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a3",
          "user": {
            "_id": "65115c00a588fdb36558b673",
            "avatarUrl": "/avatars/1f36263dc4bfaf696a4aa959a6aab1e1.svg",
            "isPro": false,
            "fullname": "Qi Dai",
            "user": "daiqi",
            "type": "user"
          },
          "name": "Qi Dai",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-28T00:57:12.914Z",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a4",
          "name": "Jaesik Park",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a5",
          "user": {
            "_id": "676a328148d749b7086782d0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Tt7u8l8f_1oVBWmBp7tkm.png",
            "isPro": false,
            "fullname": "Chong Luo",
            "user": "cluo-ms",
            "type": "user"
          },
          "name": "Chong Luo",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-28T00:57:12.914Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/636b20591340f879a2eb98d0/ZUDDcDv2cTWIFx65RIEbG.mp4"
      ],
      "publishedAt": "2025-04-23T06:48:31.000Z",
      "submittedOnDailyAt": "2025-04-28T07:02:51.113Z",
      "title": "주제 주도의 비디오 생성에서 차별화된 아이디нти티와 움직임\n\n(注意：虽然您要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，我进行了适当的调整，以确保翻译内容符合您的要求。)",
      "submittedOnDailyBy": {
        "_id": "636b20591340f879a2eb98d0",
        "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
        "isPro": false,
        "fullname": "Daneul Kim",
        "user": "carpedkm",
        "type": "user"
      },
      "summary": "우리는 추가 훈련 없이 0-shot 환경에서 주체별 학습과 시간적 동역학을 분리하여 사용자 정의 비디오 생성 모델을 훈련하는 방법을 제안합니다. 사용자 정의 비디오의 전통적인 튜닝 없는 방법은 큰, 라벨링 된 비디오 데이터셋을 기반으로 하며, 이는 계산적으로 비싸고 광범위한 라벨링이 필요합니다. 이전 접근 방식과 달리, 우리는 사용자 정의 이미지 데이터셋을 직접 비디오 사용자 정의 모델 훈련에 사용하는 방법을 소개합니다. 비디오 사용자 정의를 두 가지 부분으로 분할합니다: (1) 사용자 정의 이미지 데이터셋을 통한 인식 Injection과 (2) 작은, 라벨링되지 않은 비디오 집합을 통해 이미지-to-video 훈련 방법을 통해 시간적 모델링 보존. 또한, 이미지-to-video 미세 조정 중 랜덤 이미지 토큰 드롭과 랜덤 이미지 초기화를 사용하여 복사 및 붙여넣기 문제를 완화합니다. 학습을 더욱 향상시키기 위해, 주체별 및 시간적 특징의 공동 최적화 중 확률적 스위칭을 도입하여 치명적인 잊음을 완화합니다. 우리의 방법은 강력한 주체 일관성과 확장성으로, 기존 사용자 정의 비디오 모델보다 0-shot 환경에서 뛰어난 성능을 보입니다, 우리의 프레임워크의 효과성을 보여주는 데 기여합니다.",
      "upvotes": 5,
      "discussionId": "680ed2689e529f7799a06907",
      "projectPage": "https://carpedkm.github.io/projects/disentangled_sub/",
      "githubRepo": "https://github.com/carpedkm/disentangled-subject-to-vid",
      "ai_keywords": [
        "subject-specific learning",
        "temporal dynamics",
        "image customization dataset",
        "identity injection",
        "temporal modeling",
        "image-to-video training method",
        "random image token dropping",
        "randomized image initialization",
        "image-to-video fine-tuning",
        "stochastic switching",
        "joint optimization",
        "catastrophic forgetting",
        "subject consistency",
        "zero-shot settings"
      ]
    },
    "publishedAt": "2025-04-23T02:48:31.000Z",
    "title": "Subject-driven Video Generation via Disentangled Identity and Motion",
    "summary": "We propose to train a subject-driven customized video generation model\nthrough decoupling the subject-specific learning from temporal dynamics in\nzero-shot without additional tuning. A traditional method for video\ncustomization that is tuning-free often relies on large, annotated video\ndatasets, which are computationally expensive and require extensive annotation.\nIn contrast to the previous approach, we introduce the use of an image\ncustomization dataset directly on training video customization models,\nfactorizing the video customization into two folds: (1) identity injection\nthrough image customization dataset and (2) temporal modeling preservation with\na small set of unannotated videos through the image-to-video training method.\nAdditionally, we employ random image token dropping with randomized image\ninitialization during image-to-video fine-tuning to mitigate the copy-and-paste\nissue. To further enhance learning, we introduce stochastic switching during\njoint optimization of subject-specific and temporal features, mitigating\ncatastrophic forgetting. Our method achieves strong subject consistency and\nscalability, outperforming existing video customization models in zero-shot\nsettings, demonstrating the effectiveness of our framework.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/636b20591340f879a2eb98d0/ZUDDcDv2cTWIFx65RIEbG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17816.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636b20591340f879a2eb98d0",
      "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
      "fullname": "Daneul Kim",
      "name": "carpedkm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15716",
      "authors": [
        {
          "_id": "680dcc5d3478de07603a8036",
          "user": {
            "_id": "642656cbad1e3b0e6e91b752",
            "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
            "isPro": false,
            "fullname": "Jie Zhu",
            "user": "amazingj",
            "type": "user"
          },
          "name": "Jie Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-28T07:39:02.713Z",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8037",
          "name": "Qian Chen",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8038",
          "name": "Huaixia Dou",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8039",
          "name": "Junhui Li",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803a",
          "name": "Lifan Guo",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803b",
          "name": "Feng Chen",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803c",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T09:01:04.000Z",
      "submittedOnDailyAt": "2025-04-28T06:16:26.234Z",
      "title": "DianJin-R1: 대규모 언어 모델의 재무론 평가와 향상",
      "submittedOnDailyBy": {
        "_id": "642656cbad1e3b0e6e91b752",
        "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
        "isPro": false,
        "fullname": "Jie Zhu",
        "user": "amazingj",
        "type": "user"
      },
      "summary": "有效な理由論理는、대규모 언어 모델（LLMs）에 있어서 금융 분야에서 핵심적인 문제로 인식되어 있습니다. 이러한 문제를 해결하기 위해 영역 고유의 지식, 정밀한 수치 계산, 그리고 엄격한 규칙 위반 준수 요구가 있습니다. 우리는 reason-based 논리를 강화하는 프레임워크인 DianJin-R1을 제안하고, reason-based 논리를 강화함으로써 이러한 문제를 해결하고자 합니다. 우리의 접근 방식의 핵심은 DianJin-R1-Data입니다. DianJin-R1-Data는 CFLUE, FinQA, 그리고 개인적인 위반 코퍼스（중국 위반 체크, CCC）로부터 구축된 고품질 데이터셋입니다. 이 데이터셋은 다양한 금융 분야의 reason-based 논리 스케일와 검증된 注釈를 결합하여 구성되어 있습니다. 우리의 모델인 DianJin-R1-7B와 DianJin-R1-32B는 Qwen2.5-7B-Instruct와 Qwen2.5-32B-Instruct로부터 구축되어 있으며, 구조화된 포맷을 사용하여 reason-based 논리 단계와 최종적인 답을 생성하는 것을 통해 미세 조정되어 있습니다. reason-based 논리의 품질을 더 높일 수 있도록, 우리는 Group Relative Policy Optimization (GRPO)을 적용하고, 이는 강화 학습 방법론으로, 구조화된 출력을 촉구하는 하나의 보상 신호와 답의 정확성을 보상하는 다른 보상 신호를 포함합니다. 우리의 모델은 5개의 벤치마크에 평가됩니다: 3개의 금융 데이터셋 (CFLUE, FinQA, CCC)과 2개의 일반적인 reason-based 논리 벤치마크 (MATH-500, GPQA-Diamond)입니다. 실험 결과를 통해 DianJin-R1 모델이, 특히 복잡한 금융 태스크에서 reason-based 논리가 없는 것보다 일관적으로 뛰어넘는 것을 보여줍니다. 또한, 실제 세계적인 CCC 데이터셋에서, 우리 단일 호출의 reason-based 논리 모델은 계산 비용이 크게 증가하는 다 에이전트 시스템의 성능을 추월할 수 있습니다. 이러한 발견은 DianJin-R1이 구조화된 서브젝션과 보상에 맞는 학습을 통해 금융의 reason-based 논리를 강화하는 효과성을 보여주고, 실제 세계적인 애플리케이션에 대한 Scalable, 실용적인 해결책을 제공함을 보여줍니다.",
      "upvotes": 5,
      "discussionId": "680dcc5e3478de07603a807e",
      "ai_keywords": [
        "reasoning-enhanced framework",
        "reasoning-augmented supervision",
        "reinforcement learning",
        "DianJin-R1-Data",
        "CFLUE",
        "FinQA",
        "Chinese Compliance Check (CCC)",
        "high-quality dataset",
        "DianJin-R1-7B",
        "DianJin-R1-32B",
        "Qwen2.5-7B-Instruct",
        "Qwen2.5-32B-Instruct",
        "structured format",
        "reasoning steps",
        "Group Relative Policy Optimization (GRPO)",
        "dual reward signals",
        "structured outputs",
        "answer correctness",
        "MATH-500",
        "GPQA-Diamond",
        "financial datasets",
        "single-call reasoning models",
        "multi-agent systems",
        "real-world applications"
      ]
    },
    "publishedAt": "2025-04-22T05:01:04.000Z",
    "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large\n  Language Models",
    "summary": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15716.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642656cbad1e3b0e6e91b752",
      "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
      "fullname": "Jie Zhu",
      "name": "amazingj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12080",
      "authors": [
        {
          "_id": "680afc5f2c4b584e1d786eee",
          "name": "Mengshi Qi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786eef",
          "user": {
            "_id": "66a8c8e4f5cda7b8690205ef",
            "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
            "isPro": false,
            "fullname": "Pengfei Zhu",
            "user": "zaplm",
            "type": "user"
          },
          "name": "Pengfei Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:30.195Z",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef0",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef1",
          "name": "Xiaoyang Bi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef2",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef3",
          "name": "Huadong Ma",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef4",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T13:41:59.000Z",
      "submittedOnDailyAt": "2025-04-28T02:11:37.182Z",
      "title": "DC-SAM: 이미지와 비디오에서 컨텍스트 내의 분할을 수행하기 위한 이중 일치성에 의한 방법",
      "submittedOnDailyBy": {
        "_id": "66a8c8e4f5cda7b8690205ef",
        "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
        "isPro": false,
        "fullname": "Pengfei Zhu",
        "user": "zaplm",
        "type": "user"
      },
      "summary": "그래픽 데이터에 하나의 라벨 예시가 주어지면 in-context segmentation은 대응하는 물체의 분할을 목표로 합니다. 이 설정은 few-shot learning에서 owner-shot segmentation으로 알려져 있으며, 스케ن 이해, 이미지/비디오 편집 등 다양한 시각적 태스크에 적용되어 있습니다. 최근의 Segment Anything Models는 상호적인 segmentation을 통해 가장 최신의 결과를 달성했지만, 이러한 접근法是 직접 in-context segmentation에 적용되지 않습니다. 본 논문에서는 prompt-tuning 기반의 Dual Consistency SAM (DC-SAM) 방법을 제안하고, SAM과 SAM2를 이미지와 비디오의 두 가지 in-context segmentation에 적용합니다. 주요 아이디어는 SAM의 prompt encoder의 특징을 고품질의 시각적 prompt를 제공하여 강화합니다. 마스크 프로이드를 생성할 때 SAM의 특징량을 융합하여 prompt encoder와 더 잘 맞췄습니다. 다음으로, 융합된 특징량과 초기 prompt를 사용하여 반복적으로 일치하는 cross-attention을 설계합니다. 그 후, prompt encoder 내의 구분적인 긍정/부정 prompt를 사용하여 이분식 설계를 제공합니다. 또한 간단한 마스크 탓을 위한 훈련 전략을 설계하고 제안된 이중 일관성 방법을 마스크 탓에 적용합니다. DC-SAM은 주로 이미지에 적용되어 있지만, SAM2의 지원으로 비디오 영역에서도 무결하게 확장할 수 있습니다. 비디오 영역에서 in-context segmentation이 없기 때문에 IC-VOS라는 첫 번째 벤치마크를 자동으로 정비하고, 기존의 비디오 분할 데이터 세트를 사용하여 구축합니다. 이 벤치마크를 사용하여 확장된 실험은 COCO-20i에서 55.5 (+1.4) mIoU, PASCAL-5i에서 73.0 (+1.1) mIoU, 제안된 IC-VOS 벤치마크에서 J&F 스코어 71.52를 달성했습니다. 이 DC-SAM의 소스 코드와 벤치마크는 https://github.com/zaplm/DC-SAM에 공개됩니다.",
      "upvotes": 5,
      "discussionId": "680afc622c4b584e1d786f9e",
      "ai_keywords": [
        "prompt-tuning",
        "prompt encoder",
        "mask prior",
        "cycle-consistent cross-attention",
        "dual-branch design",
        "discriminative positive prompts",
        "negative prompts",
        "mask-tube",
        "In-Context Video Object Segmentation (IC-VOS)",
        "mIoU"
      ]
    },
    "publishedAt": "2025-04-16T09:41:59.000Z",
    "title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency",
    "summary": "Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12080.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a8c8e4f5cda7b8690205ef",
      "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
      "fullname": "Pengfei Zhu",
      "name": "zaplm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]