[
  {
    "paper": {
      "id": "2505.04410",
      "authors": [
        {
          "_id": "681d615fbd89ba9ceb5e94bc",
          "user": {
            "_id": "64a385281cbf675203fbb7df",
            "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
            "isPro": false,
            "fullname": "Junjie Wang",
            "user": "xiaomoguhzz",
            "type": "user"
          },
          "name": "Junjie Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:48.630Z",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94bd",
          "name": "Bin Chen",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94be",
          "name": "Yulin Li",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94bf",
          "name": "Bin Kang",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94c0",
          "name": "Yichi Chen",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94c1",
          "name": "Zhuotao Tian",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a385281cbf675203fbb7df/MpXgXINxg1RmSFnfeqXqU.mp4"
      ],
      "publishedAt": "2025-05-07T13:46:34.000Z",
      "submittedOnDailyAt": "2025-05-15T06:24:33.810Z",
      "title": "DeCLIP: 개방 박스의 분리된 학습에 의한 밀도 관측",
      "submittedOnDailyBy": {
        "_id": "64a385281cbf675203fbb7df",
        "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
        "isPro": false,
        "fullname": "Junjie Wang",
        "user": "xiaomoguhzz",
        "type": "user"
      },
      "summary": "밀집 시각 예측 태스크는预定의 카테고리에 의존하여 사진 개념이 제한적이며, 실제 세계의 스케너리로 적용할 수 있는 범위가 좁아지고, 실용적인 활용이 어려워졌습니다. CLIP 등 시각 언어 모델(VLMs)은 개방 박스 언어 태스크에서 뛰어난 성능을 보여주지만, 밀집 예측에 직접 적용할 때, 지역특징 표현의 제한으로 우수한 성능을 보여주는 것이 어렵습니다. 본 논문에서는 CLIP의 이미지 토큰이 공간적 또는 의미적으로 관련되어 있는 영역에서 정보를 효과적으로 수집할 수 없게 되는 것을 관찰하고, 이러한 문제를 해결하기 위해 DeCLIP 프레임워크를 제안하고 있습니다. DeCLIP는 자동 注意 모듈을 독립화하여 \"내용\"과 \"컨텍스트\"의 특징량을 얻는 방식으로 CLIP를 강화하고 있습니다. \"내용\"의 특징량은 이미지 크롭의 표현과 일치하며, 지역별 식별성을 향상시키고, \"컨텍스트\"의 특징량은 DINO 등 시각 Fundamental 모델의 지도 아래 공간적 관계성을 유지하는 것을 학습하고 있습니다. 확장된 실험은 DeCLIP이 현재의 방법보다 크게 개선된 결과를 보여줍니다. 코드는 https://github.com/xiaomoguhz/DeCLIP에 액세스할 수 있습니다.",
      "upvotes": 28,
      "discussionId": "681d6161bd89ba9ceb5e9571",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "CLIP",
        "dense prediction",
        "predefined categories",
        "open-vocabulary tasks",
        "spatially related regions",
        "semantically related regions",
        "local discriminability",
        "spatial consistency",
        "self-attention module",
        "content features",
        "context features",
        "image crop representations",
        "vision foundation models",
        "DINO",
        "object detection",
        "semantic segmentation"
      ]
    },
    "publishedAt": "2025-05-07T09:46:34.000Z",
    "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
    "summary": "Dense visual prediction tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense prediction often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. The ``content'' features are aligned with image crop\nrepresentations to improve local discriminability, while ``context'' features\nlearn to retain the spatial correlations under the guidance of vision\nfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIP\nsignificantly outperforms existing methods across multiple open-vocabulary\ndense prediction tasks, including object detection and semantic segmentation.\nCode is available at magenta{https://github.com/xiaomoguhz/DeCLIP}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a385281cbf675203fbb7df/MpXgXINxg1RmSFnfeqXqU.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a385281cbf675203fbb7df",
      "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
      "fullname": "Junjie Wang",
      "name": "xiaomoguhzz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09568",
      "authors": [
        {
          "_id": "68254419181d43c25d829239",
          "name": "Jiuhai Chen",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923a",
          "name": "Zhiyang Xu",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923b",
          "name": "Xichen Pan",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923c",
          "name": "Yushi Hu",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923d",
          "name": "Can Qin",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923e",
          "name": "Tom Goldstein",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923f",
          "name": "Lifu Huang",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829240",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829241",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829242",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829243",
          "name": "Le Xue",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829244",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829245",
          "name": "Ran Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T17:11:07.000Z",
      "submittedOnDailyAt": "2025-05-15T00:07:05.564Z",
      "title": "BLIP3-o: 완전 오픈 유니폼 멀티모달 모델의 가족 - 아키텍처, 훈련, 데이터셋",
      "submittedOnDailyBy": {
        "_id": "6393847e3e30234ae798b7be",
        "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
        "isPro": true,
        "fullname": "JiuhaiChen",
        "user": "jiuhai",
        "type": "user"
      },
      "summary": "최근의 다모달 모델 연구에서, 이미지 이해와 생성을 통합하는 데 주목을 받고 있습니다. 이미지 이해의 설계 선택은 광범위하게 연구되어 있지만, 이미지 생성과 통합 프레임워크의 최적의 모델 구조와 훈련 방식은 아직 조사가 부족합니다. 자동 복원 모델과 디퓨전 모델이 고품질의 생성과 scalability에 강한 가능성을 보여주는 데 기반하여, 이러한 모델의 통합 모델 설정에 대한 자세한 연구를 수행했습니다. 이러한 연구에 기초하여, 새로운 접근 방식을 제안했습니다. 이 접근 방식은 단순한 VAE 기반 표현과 다른 점에서, 의미적으로 풍부한 CLIP 이미지 특성을 생성하기 위해 디퓨전 트랜스포머를 사용합니다. 이 설계는 학습 효율성과 생성 품질의 향상을 실현합니다. 또한, 통합 모델의 순차적인 예측 훈련 전략을 제시하고, 이미지 이해 능력을 유지하면서 이미지 생성 능력을 강화하는 실질적인 장점을 나타냅니다. 마지막으로, GPT-4o에 다양한 캡처를 사용하여, 비디오 오브젝트, 인간의 손의 동작 등 다양한 공간으로 구성된 데이터 세트를 사용하며, 고품질의 명령 훈련 데이터 세트 BLIP3o-60k를 선정했습니다. 이 새로운 모델 설계, 훈련 방식, 데이터 세트를 기반으로, BLIP3-o라는 가장 선진적인 통합 모델 시스템을 개발했습니다. BLIP3-o는 이미지 이해와 생성의 두 가지 테스트에서 가장 선진적인 성능을 내는 것입니다. 향후 연구를 촉진하기 위해, 모델 코드, 모델 가중치, 훈련 스크립트, 예측 훈련 데이터 세트, 명령 훈련 데이터 세트를 완전히 공개합니다.",
      "upvotes": 13,
      "discussionId": "6825441a181d43c25d82927a",
      "ai_keywords": [
        "autoregressive models",
        "diffusion models",
        "semantically rich CLIP image features",
        "diffusion transformer",
        "VAE-based representations",
        "sequential pretraining strategy",
        "image understanding",
        "image generation",
        "instruction-tuning dataset",
        "GPT-4o",
        "state-of-the-art unified multimodal models"
      ]
    },
    "publishedAt": "2025-05-14T13:11:07.000Z",
    "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset",
    "summary": "Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09568.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6393847e3e30234ae798b7be",
      "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
      "fullname": "JiuhaiChen",
      "name": "jiuhai",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 27
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09343",
      "authors": [
        {
          "_id": "682578ca1b93095c061429ff",
          "name": "Chenggang Zhao",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a00",
          "name": "Chengqi Deng",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a01",
          "name": "Chong Ruan",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a02",
          "name": "Damai Dai",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a03",
          "name": "Huazuo Gao",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a04",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a05",
          "name": "Liyue Zhang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a06",
          "name": "Panpan Huang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a07",
          "name": "Shangyan Zhou",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a08",
          "name": "Shirong Ma",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a09",
          "name": "Wenfeng Liang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0a",
          "name": "Ying He",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0b",
          "name": "Yuqing Wang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0c",
          "name": "Yuxuan Liu",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0d",
          "name": "Y. X. Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T12:39:03.000Z",
      "submittedOnDailyAt": "2025-05-15T05:22:39.526Z",
      "title": "DeepSeek-V3의 견해: AI 아키텍처의 스케일링 문제와 하드웨어의 반성",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "대규모 언어 모델(LLMs)의 급격한 확장은 현재의 하드웨어 아키텍처에 있어 중요한 한계가 드러나고 있습니다. 이러한 한계는 메모리 용량, 계산 효율성, 그리고 인터네트 연결 속도의 제한을 포함합니다. DeepSeek-V3는 2,048대의 NVIDIA H800 GPU를 사용하여 훈련된 사실로 입증되어, 하드웨어에 대한 관심과 모델 공동 설계가 이러한 도전을 효과적으로 해결할 수 있음을 보여줍니다. 이 논문에서는 DeepSeek-V3/R1 모델 아키텍처와 그 AI 인프라에 대한 상세한 분석을 제공하며, Multi-head Latent Attention(MLA)를 사용하여 메모리 효율을 향상시키는 것, Mixture of Experts(MoE) 아키텍처를 사용하여 계산-통신의 trade-off를 최적화하는 것, FP8 혼합 정밀도 훈련을 사용하여 하드웨어의 기능을 최대화하는 것, 그리고 Multi-Plane 네트워크 토폴로지를 사용하여 클러스터 수준의 네트워크 오버헤드를 최소화하는 것 등 주요 혁신을 중점적으로 설명합니다. DeepSeek-V3 개발 과정에서 마주한 하드웨어 한계에 기반하여, 학계와 산업界的伙伴들과 함께, 고 정밀도 저 정밀도 계산 유닛, 규모 업과 규모 다운의 수렴, 그리고 저 라트 통신 팩토리의 혁신에 대한 브라우저적인 논의를 진행합니다. 이러한 통찰은 AI 작업로드가 증가하는 요구에 대응하는 데 필요한 하드웨어와 모델의 공동 설계의 중요성을 강조하고, 다음 세대 AI 시스템의 혁신에 대한 실질적인 계획을 제공합니다.",
      "upvotes": 10,
      "discussionId": "682578cb1b93095c06142a55",
      "ai_keywords": [
        "Multi-head Latent Attention (MLA)",
        "Mixture of Experts (MoE)",
        "FP8 mixed-precision training",
        "Multi-Plane Network Topology",
        "low-precision computation units",
        "scale-up and scale-out convergence",
        "low-latency communication fabrics"
      ]
    },
    "publishedAt": "2025-05-14T08:39:03.000Z",
    "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures",
    "summary": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09343.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 862
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.08455",
      "authors": [
        {
          "_id": "6824176351679cbc704daa88",
          "user": {
            "_id": "6483b3d52193a1768c00c5ff",
            "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
            "isPro": false,
            "fullname": "Pritam Sarkar",
            "user": "pritamqu",
            "type": "user"
          },
          "name": "Pritam Sarkar",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-14T04:09:09.283Z",
          "hidden": false
        },
        {
          "_id": "6824176351679cbc704daa89",
          "name": "Ali Etemad",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-13T11:35:58.000Z",
      "submittedOnDailyAt": "2025-05-15T04:58:35.875Z",
      "title": "VCRBench: 대규모 비디오 언어 모델의 긴 문맥因果 추론 능력에 대한 검토",
      "submittedOnDailyBy": {
        "_id": "6483b3d52193a1768c00c5ff",
        "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
        "isPro": false,
        "fullname": "Pritam Sarkar",
        "user": "pritamqu",
        "type": "user"
      },
      "summary": "최근의 이미지 이해의 발전에도 불구하고, 대규모 이미지 언어 모델(LVLMs)은 이미지 기반의 원인적 추론 능력을 가지고 있는 것을 확인하는 데 필요한 평가는 아직 존재하지 않는다. 이 결함이 있기 때문에, 우리는 새로운 평가 기준인 \"Video-based long-form Causal Reasoning(VCRBench)\"를 소개합니다. VCRBench는 일상적인 활동의 단계를 임의로 섞은 단계 영상으로 구성되어 있으며, 각 영상은 특정 원인적 이벤트를 촬영하고, LVLMs가 특정 목표를 달성하기 위해 필요한 이벤트를 인식하고, 이유를 설명하고, 올바른 순서를 파악할 수 있는지 테스트합니다. 또한, 이 평가 기준은 언어적인 슬롯을 사용하지 않고, 복수 선택 또는 이진형 QA 형식으로 언어적인 표현을 사용하지 않고, 개방형 QA 평가의 문제를 피하기 위해 설계되었습니다. VCRBench에서 가장 최신의 LVLMs 모델을 평가한 결과, 이 모델들은 이미지 기반의 긴 문장 원인적 추론에 어려움을 겪고 있음을 보여주고, 이 어려움은 직접적인 시각적 관찰으로부터 멀리 있는 원인적 관계의 모델링의 어려움에 기인합니다. 이를 위해, 우리는 간단한 단계를 제시하여 이 능력을 설정하는 데 도움이 될 수 있는 \"Recognition-Reasoning Decomposition(RRD)\"를 제안합니다. RRD는 이미지 기반의 원인적 추론을 이미지 인식과 원인적 추론의 두 가지 하위 태스크로 분해하는 모듈화된 접근법입니다. VCRBench에서의 실험은 RRD가 VCRBench의 정확도를 크게 향상시키고, 최대 25.2%의 향상을 보여주었습니다. 마지막으로, LVLMs가 복잡한 이미지 기반의 긴 문장 원인적 추론 태스크에서 주로 언어 지식에 의존하는 등, 흥미로운 지침을 제시했습니다.",
      "upvotes": 2,
      "discussionId": "6824176551679cbc704daafb",
      "projectPage": "https://pritamsarkar.com/VCRBench/",
      "githubRepo": "https://github.com/pritamqu/VCRBench",
      "ai_keywords": [
        "Large Video Language Models (LVLMs)",
        "causal reasoning",
        "benchmarks",
        "procedural videos",
        "causal dependencies",
        "video-based long-form causal reasoning",
        "VCRBench",
        "video recognition",
        "Recognition-Reasoning Decomposition (RRD)"
      ]
    },
    "publishedAt": "2025-05-13T07:35:58.000Z",
    "title": "VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\n  Video Language Models",
    "summary": "Despite recent advances in video understanding, the capabilities of Large\nVideo Language Models (LVLMs) to perform video-based causal reasoning remains\nunderexplored, largely due to the absence of relevant and dedicated benchmarks\nfor evaluating causal reasoning in visually grounded and goal-driven settings.\nTo fill this gap, we introduce a novel benchmark named Video-based long-form\nCausal Reasoning (VCRBench). We create VCRBench using procedural videos of\nsimple everyday activities, where the steps are deliberately shuffled with each\nclip capturing a key causal event, to test whether LVLMs can identify, reason\nabout, and correctly sequence the events needed to accomplish a specific goal.\nMoreover, the benchmark is carefully designed to prevent LVLMs from exploiting\nlinguistic shortcuts, as seen in multiple-choice or binary QA formats, while\nalso avoiding the challenges associated with evaluating open-ended QA. Our\nevaluation of state-of-the-art LVLMs on VCRBench suggests that these models\nstruggle with video-based long-form causal reasoning, primarily due to their\ndifficulty in modeling long-range causal dependencies directly from visual\nobservations. As a simple step toward enabling such capabilities, we propose\nRecognition-Reasoning Decomposition (RRD), a modular approach that breaks\nvideo-based causal reasoning into two sub-tasks of video recognition and causal\nreasoning. Our experiments on VCRBench show that RRD significantly boosts\naccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis\nreveals interesting insights, for instance, that LVLMs primarily rely on\nlanguage knowledge for complex video-based long-form causal reasoning tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08455.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6483b3d52193a1768c00c5ff",
      "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
      "fullname": "Pritam Sarkar",
      "name": "pritamqu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]