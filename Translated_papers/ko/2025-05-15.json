[
  {
    "paper": {
      "id": "2505.04410",
      "authors": [
        {
          "_id": "681d615fbd89ba9ceb5e94bc",
          "user": {
            "_id": "64a385281cbf675203fbb7df",
            "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
            "isPro": false,
            "fullname": "Junjie Wang",
            "user": "xiaomoguhzz",
            "type": "user"
          },
          "name": "Junjie Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:48.630Z",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94bd",
          "name": "Bin Chen",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94be",
          "name": "Yulin Li",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94bf",
          "name": "Bin Kang",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94c0",
          "name": "Yichi Chen",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94c1",
          "name": "Zhuotao Tian",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a385281cbf675203fbb7df/MpXgXINxg1RmSFnfeqXqU.mp4"
      ],
      "publishedAt": "2025-05-07T13:46:34.000Z",
      "submittedOnDailyAt": "2025-05-15T06:24:33.810Z",
      "title": "DeCLIP: 오픈 박스의 밀집 인식의 디코더 학습\n\n(Note: The translation provided is a direct literal translation. For a more natural and contextually appropriate translation, consider the following:\n\nDeCLIP: 밀집 인식의 오픈 박스 디코더 학습\n\nThis version maintains the technical accuracy while sounding more natural in Korean.)",
      "submittedOnDailyBy": {
        "_id": "64a385281cbf675203fbb7df",
        "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
        "isPro": false,
        "fullname": "Junjie Wang",
        "user": "xiaomoguhzz",
        "type": "user"
      },
      "summary": "밀집적인 시각 예측 태스크는 미리 정의된 카테고리에 의존하며, 시각 개념이 무제한의 실세계 시나리오에서 적용성을 제한하고 있습니다. CLIP 등 시각 언어 모델(VLMs)은 개방 박스 언어 태스크에서 원하는 결과를 보여주지만, 밀집 예측에 직접 적용할 때, 지역적 특징 표현의 제한으로 더 좋은 성능을 보여주는 것이 불가능합니다. 본 연구에서는 CLIP의 이미지 토큰이 공간적 또는 의미적으로 연관된 영역에서 정보를 효과적으로 모을 수 없게 된 것을 관찰하고, 이러한 문제를 해결하기 위해 DeCLIP라는 새로운 프레임워크를 제안합니다. DeCLIP은 자동 注意 모듈을 분리하고, \"콘텐츠\"와 \"콘텍스트\"의 특징량을 각각 얻는 방식으로 CLIP를 강화합니다. \"콘텐츠\"의 특징량은 이미지 크롭의 표현과 일치하며, 지역적 식별성을 향상시키고, \"콘텍스트\"의 특징량은 DINO 등 시각 기반 모델의 지도 아래 공간적 상관성을 유지합니다. 확장된 실험 결과에 따르면, DeCLIP은 물체 탐지, 의미 분할 등 다양한 개방 박스 언어 태스크에서 기존 방법보다 크게 뛰어난 성능을 보여주고 있습니다. 코드는 https://github.com/xiaomoguhz/DeCLIP에 접근할 수 있습니다.",
      "upvotes": 28,
      "discussionId": "681d6161bd89ba9ceb5e9571",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "CLIP",
        "dense prediction",
        "predefined categories",
        "open-vocabulary tasks",
        "spatially related regions",
        "semantically related regions",
        "local discriminability",
        "spatial consistency",
        "self-attention module",
        "content features",
        "context features",
        "image crop representations",
        "vision foundation models",
        "DINO",
        "object detection",
        "semantic segmentation"
      ]
    },
    "publishedAt": "2025-05-07T09:46:34.000Z",
    "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
    "summary": "Dense visual prediction tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense prediction often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. The ``content'' features are aligned with image crop\nrepresentations to improve local discriminability, while ``context'' features\nlearn to retain the spatial correlations under the guidance of vision\nfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIP\nsignificantly outperforms existing methods across multiple open-vocabulary\ndense prediction tasks, including object detection and semantic segmentation.\nCode is available at magenta{https://github.com/xiaomoguhz/DeCLIP}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a385281cbf675203fbb7df/MpXgXINxg1RmSFnfeqXqU.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a385281cbf675203fbb7df",
      "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
      "fullname": "Junjie Wang",
      "name": "xiaomoguhzz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09568",
      "authors": [
        {
          "_id": "68254419181d43c25d829239",
          "name": "Jiuhai Chen",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923a",
          "name": "Zhiyang Xu",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923b",
          "name": "Xichen Pan",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923c",
          "name": "Yushi Hu",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923d",
          "name": "Can Qin",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923e",
          "name": "Tom Goldstein",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923f",
          "name": "Lifu Huang",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829240",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829241",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829242",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829243",
          "name": "Le Xue",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829244",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829245",
          "name": "Ran Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T17:11:07.000Z",
      "submittedOnDailyAt": "2025-05-15T00:07:05.564Z",
      "title": "BLIP3-o: 완전 결합 개방의 통합 다 모델의 가족 - 아키텍처, 훈련 및 데이터 세트",
      "submittedOnDailyBy": {
        "_id": "6393847e3e30234ae798b7be",
        "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
        "isPro": true,
        "fullname": "JiuhaiChen",
        "user": "jiuhai",
        "type": "user"
      },
      "summary": "최근의 다모델 연구에서, 이미지 이해와 생성의 통합이 주목을 받고 있습니다. 이미지 이해의 설계 선택은 광범위하게 연구되어 있지만, 이미지 생성과의 통합 프레임워크의 최적의 모델 구조와 훈련 드리프트는 조사가 부족합니다. 자동 복원 모델과 확산 모델이 고품질의 생성과 scalability에 강한 가능성을 보여주는 데서 영감을 받아, 이러한 모델의 통합 모델 설정에서의 사용에 대한 연구를 진행했습니다. 이러한 조사를 기반으로, 확산 채널 Formaner를 사용하여 CLIP 이미지 특징을 의미적으로 풍부하게 생성하는 새로운 접근 방식을 제안했습니다. 이 접근 방식은 고가의 훈련 효율과 생성 품질의 향상을 실현합니다. 또한, 통합 모델의 순차 학습 전략(이미지 이해의 학습을 먼저 수행한 후, 이미지 생성의 학습을 수행)은 이미지 이해의 능력을 유지하면서 강력한 이미지 생성 능력을 개발하는 실용적인 장점을 나타냅니다. 마지막으로, GPT-4o를 다양한 시나리오, 물체, 인간의 손짓 등 포함하는 캡처 세트로, 이미지 생성을 위한 고품질의 인스톰션 훈련 데이터 세트 BLIP3o-60k를 구축했습니다. 이 새로운 모델 설계, 훈련 드리프트, 데이터 세트를 기반으로, BLIP3-o라는 가장 선진적인 통합 모델 시스템을 개발했습니다. BLIP3-o는 이미지 이해와 생성의 두 가지 테스트 벤치마크에서 가장 선진적인 성능을 달성합니다. 향후 연구를 지원하기 위해, 모델의 코드, 모델 가중치, 훈련 스크립트, 사전 학습 데이터 세트와 인스톰션 훈련 데이터 세트를 완전히 오픈 소스로 만들었습니다.",
      "upvotes": 13,
      "discussionId": "6825441a181d43c25d82927a",
      "ai_keywords": [
        "autoregressive models",
        "diffusion models",
        "semantically rich CLIP image features",
        "diffusion transformer",
        "VAE-based representations",
        "sequential pretraining strategy",
        "image understanding",
        "image generation",
        "instruction-tuning dataset",
        "GPT-4o",
        "state-of-the-art unified multimodal models"
      ]
    },
    "publishedAt": "2025-05-14T13:11:07.000Z",
    "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset",
    "summary": "Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09568.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6393847e3e30234ae798b7be",
      "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
      "fullname": "JiuhaiChen",
      "name": "jiuhai",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 27
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09343",
      "authors": [
        {
          "_id": "682578ca1b93095c061429ff",
          "name": "Chenggang Zhao",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a00",
          "name": "Chengqi Deng",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a01",
          "name": "Chong Ruan",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a02",
          "name": "Damai Dai",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a03",
          "name": "Huazuo Gao",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a04",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a05",
          "name": "Liyue Zhang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a06",
          "name": "Panpan Huang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a07",
          "name": "Shangyan Zhou",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a08",
          "name": "Shirong Ma",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a09",
          "name": "Wenfeng Liang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0a",
          "name": "Ying He",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0b",
          "name": "Yuqing Wang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0c",
          "name": "Yuxuan Liu",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0d",
          "name": "Y. X. Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T12:39:03.000Z",
      "submittedOnDailyAt": "2025-05-15T05:22:39.526Z",
      "title": "DeepSeek-V3의 디피사이언스브یم3의 인사이트: AI 아키텍처의 스케일링 문제와 하드웨어의 반성",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "대언어 모델(LLMs)의 급격한 확장은 현재의 하드웨어 아키텍처에 대한 중요한 한계를 드러내고 있습니다. 이 한계는 메모리 용량, 계산 효율성, 간접 연결 브랜드웨이에 포함됩니다. DeepSeek-V3는 2,048대의 NVIDIA H800 GPU를 사용하여 훈련되었으며, 하드웨어에 관련된 모델의 공동 창작을 효과적으로 대처할 수 있는 방법을 보여주고, 확장의 비용 효율적인 훈련과 추론을 가능하게 합니다. 본 논문에서는 DeepSeek-V3/R1 모델 아키텍처와 그 AI 인프라 구조를 자세히 분석하며, Multi-head Latent Attention(MLA)로 메모리 효율적인 통신, Mixture of Experts(MoE) 아키텍처로 계산 통신의 보완, FP8 혼합 정밀도 훈련으로 하드웨어 능력을 최대한 활용, Multi-Plane 네트워크 토폴로지로 클러스터 수준의 네트워크 오버헤드 최소화 등 주요 혁신을 특징으로 합니다. DeepSeek-V3 개발 기간 동안의 하드웨어 한계에 기반하여, 학계와 산업界的 많은 이들이 향후 하드웨어의 방향성을 논의하고, 정밀한 저정밀 계산 유닛, 확장과 축소의 결합, 저레이턴 시 통신 팩토리의 혁신 등을 검토합니다. 이러한 시각은 AI 작업로드가 증가하는 요구에 대응하기 위해 하드웨어와 모델의 공동 창작의 중요성을 명확히 하고, 다음 세대의 AI 시스템의 혁신적인 계획을 제공합니다.",
      "upvotes": 10,
      "discussionId": "682578cb1b93095c06142a55",
      "ai_keywords": [
        "Multi-head Latent Attention (MLA)",
        "Mixture of Experts (MoE)",
        "FP8 mixed-precision training",
        "Multi-Plane Network Topology",
        "low-precision computation units",
        "scale-up and scale-out convergence",
        "low-latency communication fabrics"
      ]
    },
    "publishedAt": "2025-05-14T08:39:03.000Z",
    "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures",
    "summary": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09343.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 862
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.08455",
      "authors": [
        {
          "_id": "6824176351679cbc704daa88",
          "user": {
            "_id": "6483b3d52193a1768c00c5ff",
            "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
            "isPro": false,
            "fullname": "Pritam Sarkar",
            "user": "pritamqu",
            "type": "user"
          },
          "name": "Pritam Sarkar",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-14T04:09:09.283Z",
          "hidden": false
        },
        {
          "_id": "6824176351679cbc704daa89",
          "name": "Ali Etemad",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-13T11:35:58.000Z",
      "submittedOnDailyAt": "2025-05-15T04:58:35.875Z",
      "title": "VCRBench: 대규모 비디오 언어 모델의 긴 문맥因果 추론 능력의 검토",
      "submittedOnDailyBy": {
        "_id": "6483b3d52193a1768c00c5ff",
        "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
        "isPro": false,
        "fullname": "Pritam Sarkar",
        "user": "pritamqu",
        "type": "user"
      },
      "summary": "최근의 이미지 이해의 발전에 따라, 대규모 이미지 언어 모델(LVLMs)은 이미지에 기반한 원인적 추론을 수행할 수 있는 능력이, 시각적으로 기반된 목표 주도 설정에서의 원인적 추론 평가에 대한 관련 벤치마크가 없기 때문에, 주로 조사가 진행되지 않았습니다. 이를 채워주기 위해, 새로운 벤치마크 VCRBench를 도입합니다. VCRBench는 간단한 일상적인 활동의 순서가 결정적으로 셔플된 프로세스 이미지로 구성되었으며, 각샷이 원인적 이벤트를 감지하는 방식으로, LVLMs가 특정 목표를 달성하기 위해 수행되는 이벤트를 인식하고 이유를 제공하며, 올바른 순서를 인식할 수 있는지 테스트합니다. 또한, 벤치마크는 언어적인 짧은 요약을 사용하지 않고, 복수 선택이나 이진 QA 형식으로 설계되어, 개방형 QA 평가의 문제를 피하도록 설계되었습니다. LVLMs의 VCRBench에서 가장 최근의 평가에 따르면, 이러한 모델은 이미지 기반의 긴 문장의 원인적 추론에 어려움을 겪고 있으며, 이는 직접적인 시각적 관찰에서 긴 거리의 원인적 의존 관계를 모델링하는 어려움에 의한 것입니다. 이를 도입하기 위한 간단한 단계로, 인식-理由 분해(RRD)라는 모듈화된 접근 방식을 제안합니다. RRD는 이미지 기반의 원인적 추론을 이미지 인식과 원인적 추론의 두 가지 서브 태스크로 분해합니다. VCRBench에서 수행된 실험 결과에 따르면, RRD는 VCRBench의 정확도를 크게 향상시켰으며, 최대 25.2%의 향상을 나타냅니다. 마지막으로, 상세한 분석에 따르면, 예를 들어, LVLMs는 복잡한 이미지 기반의 긴 문장의 원인적 추론 태스크에서 주로 언어 지식에 의존하는 등, 흥미로운 해시가 얻어졌습니다.",
      "upvotes": 2,
      "discussionId": "6824176551679cbc704daafb",
      "projectPage": "https://pritamsarkar.com/VCRBench/",
      "githubRepo": "https://github.com/pritamqu/VCRBench",
      "ai_keywords": [
        "Large Video Language Models (LVLMs)",
        "causal reasoning",
        "benchmarks",
        "procedural videos",
        "causal dependencies",
        "video-based long-form causal reasoning",
        "VCRBench",
        "video recognition",
        "Recognition-Reasoning Decomposition (RRD)"
      ]
    },
    "publishedAt": "2025-05-13T07:35:58.000Z",
    "title": "VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\n  Video Language Models",
    "summary": "Despite recent advances in video understanding, the capabilities of Large\nVideo Language Models (LVLMs) to perform video-based causal reasoning remains\nunderexplored, largely due to the absence of relevant and dedicated benchmarks\nfor evaluating causal reasoning in visually grounded and goal-driven settings.\nTo fill this gap, we introduce a novel benchmark named Video-based long-form\nCausal Reasoning (VCRBench). We create VCRBench using procedural videos of\nsimple everyday activities, where the steps are deliberately shuffled with each\nclip capturing a key causal event, to test whether LVLMs can identify, reason\nabout, and correctly sequence the events needed to accomplish a specific goal.\nMoreover, the benchmark is carefully designed to prevent LVLMs from exploiting\nlinguistic shortcuts, as seen in multiple-choice or binary QA formats, while\nalso avoiding the challenges associated with evaluating open-ended QA. Our\nevaluation of state-of-the-art LVLMs on VCRBench suggests that these models\nstruggle with video-based long-form causal reasoning, primarily due to their\ndifficulty in modeling long-range causal dependencies directly from visual\nobservations. As a simple step toward enabling such capabilities, we propose\nRecognition-Reasoning Decomposition (RRD), a modular approach that breaks\nvideo-based causal reasoning into two sub-tasks of video recognition and causal\nreasoning. Our experiments on VCRBench show that RRD significantly boosts\naccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis\nreveals interesting insights, for instance, that LVLMs primarily rely on\nlanguage knowledge for complex video-based long-form causal reasoning tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08455.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6483b3d52193a1768c00c5ff",
      "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
      "fullname": "Pritam Sarkar",
      "name": "pritamqu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]