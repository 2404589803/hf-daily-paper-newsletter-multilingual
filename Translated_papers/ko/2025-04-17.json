[
  {
    "paper": {
      "id": "2504.10514",
      "authors": [
        {
          "_id": "67ffedb8b0c26d6ec0b608cf",
          "name": "Yijun Liang",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d0",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d1",
          "user": {
            "_id": "64a8121e35fab7cd04c30ed0",
            "avatarUrl": "/avatars/48849b84703158772f1022932331b143.svg",
            "isPro": false,
            "fullname": "Chenrui Fan",
            "user": "Fcr09",
            "type": "user"
          },
          "name": "Chenrui Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:05:05.662Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d2",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d3",
          "name": "Dang Nguyen",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d4",
          "user": {
            "_id": "63f546e0fcf95ecac2b0ee3e",
            "avatarUrl": "/avatars/02a401bcff91cc473d9946bbb771a985.svg",
            "isPro": false,
            "fullname": "Kwesi Cobbina",
            "user": "kweCobi",
            "type": "user"
          },
          "name": "Kwesi Cobbina",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:36.225Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d5",
          "user": {
            "_id": "639d4b8d860db464ae35c3ab",
            "avatarUrl": "/avatars/ec0fa3e91593a03fc9fb611e66b30553.svg",
            "isPro": false,
            "fullname": "Shweta Bhardwaj",
            "user": "shweta12",
            "type": "user"
          },
          "name": "Shweta Bhardwaj",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:29.680Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d6",
          "user": {
            "_id": "6393847e3e30234ae798b7be",
            "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
            "isPro": true,
            "fullname": "JiuhaiChen",
            "user": "jiuhai",
            "type": "user"
          },
          "name": "Jiuhai Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:22.786Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d7",
          "name": "Fuxiao Liu",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d8",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:05:07.396Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T16:36:26.000Z",
      "submittedOnDailyAt": "2025-04-17T00:58:33.032Z",
      "title": "ColorBench: VLMs는 색의 세계를 보는 것과 이해할 수 있는지? 색감의 예측, 이유론과 강건성 평가기준",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "색조는 인간이 인식하는 데 중요한 역할을 하며, 일반적으로 시각적 추론에서 중요한 지침을 제공합니다. 그러나 시각 언어 모델(VLMs)이 인간처럼 색을 인식하고 이해하고 사용할 수 있는지는 명확하지 않습니다. 본 논문에서는 색 인식 능력을 평가하기 위한 새로운 벤치마크인 \"ColorBench\"를 소개합니다. 이 벤치마크는 색 인식, 이유의 성립, 그리고 색 변환에 대한 강건성을 포함하여 색 이해 능력을 평가하기 위해 상세히 제작되었습니다. 다양한 테스트 시나리오를 선택하고 현실적인 애플리케이션에 기반하여 수집한 ColorBench는 이러한 모델이 색을 어떻게 인식하고 색 기반의 코드를 통해 의미를 추론하고 색 변환에 대한 일관된 성능을 유지하는지를 평가합니다. 32개의 VLMs를 구성하는 다양한 언어 모델과 시각 인코더로 구성되어, 세부적인 평가를 수행하였으며, 본 논문에서는 다음과 같은 새로운 발견을 밝힙니다: (i) 스케일링 라운드(큰 모델이 좋은)은 ColorBench에서도 유지되며, 언어 모델이 시각 인코더보다 중요한 역할을 수행하고 있습니다. (ii) 그러나 모델 간의 성능 간격은 상대적으로 작으며, 기존의 VLMs가 색 이해를 크게 버티고 있음을 보여주고 있습니다. (iii) CoT 추론은 색 이해의 정확성과 강건성을 향상시키지만, 이는 시각 센티티 작업입니다. (iv) VLMs은 ColorBench에서 색 기반의 코드를 사용하지만, 이러한 코드는 모델을 잘못 안내하는 경우가도 있습니다. 이러한 발견은 현재의 VLMs의 중요한 한계를 명확히 하고, 색 이해의 향상이 필요함을 요구합니다. ColorBench는 다양한 AI의 인간 수준의 색 이해 연구를 촉진하기 위한 기초적인 도구로役立ちます.",
      "upvotes": 18,
      "discussionId": "67ffedbeb0c26d6ec0b60a5b",
      "projectPage": "https://huggingface.co/datasets/umd-zhou-lab/ColorBench",
      "githubRepo": "https://github.com/tianyi-lab/ColorBench",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "color understanding",
        "color perception",
        "color-based cues",
        "color transformations",
        "scaling law",
        "language model",
        "vision encoder",
        "CoT reasoning",
        "multimodal AI"
      ]
    },
    "publishedAt": "2025-04-10T12:36:26.000Z",
    "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
    "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10514.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12240",
      "authors": [
        {
          "_id": "680057b49031335df49732fc",
          "user": {
            "_id": "64970d3d9c3b29dca8633f87",
            "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
            "isPro": false,
            "fullname": "JunhaoZhuang",
            "user": "JunhaoZhuang",
            "type": "user"
          },
          "name": "Junhao Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:04:09.222Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732fd",
          "user": {
            "_id": "66837d3c48edefb453b0640a",
            "avatarUrl": "/avatars/b16385eaa612578728e2c6460a76b38f.svg",
            "isPro": false,
            "fullname": "Lingen Li",
            "user": "l-li",
            "type": "user"
          },
          "name": "Lingen Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:09:48.492Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732fe",
          "user": {
            "_id": "62d4577bc85b0fcf7fde39bb",
            "avatarUrl": "/avatars/a3a5729e33ae89ce9ba408830db3c835.svg",
            "isPro": false,
            "fullname": "Xuan Ju",
            "user": "juxuan27",
            "type": "user"
          },
          "name": "Xuan Ju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:09:55.870Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732ff",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "680057b49031335df4973300",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "680057b49031335df4973301",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:12:47.014Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
      ],
      "publishedAt": "2025-04-16T16:45:19.000Z",
      "submittedOnDailyAt": "2025-04-17T00:32:24.205Z",
      "title": "코브라: 광범위한 참조를 이용한 효율적인 선화 색칠법",
      "submittedOnDailyBy": {
        "_id": "64970d3d9c3b29dca8633f87",
        "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
        "isPro": false,
        "fullname": "JunhaoZhuang",
        "user": "JunhaoZhuang",
        "type": "user"
      },
      "summary": "コミック制作業界では、고정밀도, 효율성, 맥락의 일관성, 유연한 제어를 요구하는 기준에 따라 선화의 색칠이 필요합니다. 한 페이지의 만화에는 다양한 인물, 물체, 배경이 포함되므로, 색칠 과정이 복잡해집니다. 이미지 생성을 위한 확산 모델의 발전은 있지만, 선화의 색칠에 대해서는 확산 모델의 적용이 제한되어 있습니다. 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의 적용에는 확산 모델의",
      "upvotes": 16,
      "discussionId": "680057b89031335df497343e",
      "projectPage": "https://zhuang2002.github.io/Cobra/",
      "githubRepo": "https://github.com/zhuang2002/Cobra",
      "ai_keywords": [
        "diffusion models",
        "line art colorization",
        "contextual image guidance",
        "color hints",
        "Causal Sparse DiT architecture",
        "positional encodings",
        "causal sparse attention",
        "Key-Value Cache",
        "long-context references",
        "color identity consistency"
      ]
    },
    "publishedAt": "2025-04-16T12:45:19.000Z",
    "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
    "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64970d3d9c3b29dca8633f87",
      "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
      "fullname": "JunhaoZhuang",
      "name": "JunhaoZhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12285",
      "authors": [
        {
          "_id": "68006e6e175c8dce4ec17f7a",
          "user": {
            "_id": "613f07f40153aafa379775f2",
            "avatarUrl": "/avatars/3965175b320d753d9a5ccb0c7d9298a4.svg",
            "isPro": false,
            "fullname": "Shuming Ma",
            "user": "shumingma",
            "type": "user"
          },
          "name": "Shuming Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:13:17.676Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7b",
          "user": {
            "_id": "63f71771d36951307fcb4dcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
            "isPro": false,
            "fullname": "Hongyu Wang",
            "user": "hongyuw",
            "type": "user"
          },
          "name": "Hongyu Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:14:45.597Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7c",
          "user": {
            "_id": "632bd2f72d6a805eeb4bc601",
            "avatarUrl": "/avatars/6e1533e8a599f3068290aa69ac82cab7.svg",
            "isPro": false,
            "fullname": "HUANG SHAOHAN",
            "user": "buaahsh",
            "type": "user"
          },
          "name": "Shaohan Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:14:59.750Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7d",
          "user": {
            "_id": "64abbcff6cadc7aca584f71b",
            "avatarUrl": "/avatars/fc6e85ad4a8befd133a37b411712c648.svg",
            "isPro": false,
            "fullname": "Xingxing Zhang",
            "user": "THU-CHUNXIA",
            "type": "user"
          },
          "name": "Xingxing Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:15:06.359Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7e",
          "name": "Ying Hu",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7f",
          "name": "Ting Song",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f80",
          "name": "Yan Xia",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f81",
          "user": {
            "_id": "6368c512fbfe97c16a40baba",
            "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
            "isPro": false,
            "fullname": "Furu Wei",
            "user": "thegenerality",
            "type": "user"
          },
          "name": "Furu Wei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:15:42.250Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T17:51:43.000Z",
      "submittedOnDailyAt": "2025-04-17T01:29:56.744Z",
      "title": "BitNet b1.58 2B4T 기술보고서",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "비트넷 b1.58 2B4T, 최초의 오픈 소스, 원생 1 비트의 대형 언어 모델(LLM)을 소개합니다. 이 모델은 2억 파라미터 규모로, 4조 토큰의 코퍼스에 의해 훈련되었습니다. 모델은 언어 이해, 수학적 추론, 코딩 능력, 대화 능력의 벤치마크를 공개하고 엄격한 평가에 직면했습니다. 우리의 결과를 통해, 비트넷 b1.58 2B4T는 같은 크기의 선도적인 오픈 웨이트, 전체 정확도 LLM과 같은 성능을 달성하고, 계산 효율성의 큰 우위를 제공하고 있습니다. 특히 메모리 사용량, 에너지 소비량, 결정 시간은 크게 감소합니다. 또한 연구와 adop을 촉진하기 위해, 모델의 가중치는 Hugging Face에서 공개되고, 두 가지 GPU와 CPU 아키텍처의 오픈 소스 추론 구현도 제공됩니다.",
      "upvotes": 12,
      "discussionId": "68006e70175c8dce4ec17fc0",
      "ai_keywords": [
        "BitNet b1.58 2B4T",
        "Large Language Model (LLM)",
        "1-bit Large Language Model",
        "Train",
        "Corpus",
        "4 trillion tokens",
        "Benchmarks",
        "Language understanding",
        "Mathematical reasoning",
        "Coding proficiency",
        "Conversational ability"
      ]
    },
    "publishedAt": "2025-04-16T13:51:43.000Z",
    "title": "BitNet b1.58 2B4T Technical Report",
    "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12285.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10326",
      "authors": [
        {
          "_id": "67ff772061373fdf16ce1d38",
          "user": {
            "_id": "66486ba1640bc89c93bcc8a2",
            "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
            "isPro": false,
            "fullname": "Yangshen Deng",
            "user": "YangshenDeng",
            "type": "user"
          },
          "name": "Yangshen Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T11:57:58.302Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d39",
          "name": "Zhengxin You",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3a",
          "name": "Long Xiang",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3b",
          "user": {
            "_id": "66d6be0bddf54fd90923c727",
            "avatarUrl": "/avatars/7bb82c8c339db944d79d47b3b9b35aa8.svg",
            "isPro": false,
            "fullname": "Li",
            "user": "Qilong00",
            "type": "user"
          },
          "name": "Qilong Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:16.665Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3c",
          "user": {
            "_id": "66efe50667c4ce2c9024cd45",
            "avatarUrl": "/avatars/7b7f650953c371f08a5beecc500b6a43.svg",
            "isPro": false,
            "fullname": "peiqiyuan",
            "user": "YuanPeiqi",
            "type": "user"
          },
          "name": "Peiqi Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:22.829Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3d",
          "name": "Zhaoyang Hong",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3e",
          "user": {
            "_id": "66d6c339b61dd11022907252",
            "avatarUrl": "/avatars/d2ed3cc003e94b2e5204ce0f8a481dcf.svg",
            "isPro": false,
            "fullname": "Yitao Zheng",
            "user": "FeTieTer",
            "type": "user"
          },
          "name": "Yitao Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:36.808Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3f",
          "name": "Wanting Li",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d40",
          "user": {
            "_id": "671a7bce2b10d343bab18637",
            "avatarUrl": "/avatars/af1c10a59236d953b42e67d3955eecc4.svg",
            "isPro": false,
            "fullname": "runzhong",
            "user": "runzhongli",
            "type": "user"
          },
          "name": "Runzhong Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:17:02.031Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d41",
          "user": {
            "_id": "63898b61ec1f539adc0f4da2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674248167280-63898b61ec1f539adc0f4da2.jpeg",
            "isPro": false,
            "fullname": "Haotian Liu",
            "user": "liuhaotian",
            "type": "user"
          },
          "name": "Haotian Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:17:25.728Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d42",
          "name": "Kyriakos Mouratidis",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d43",
          "name": "Man Lung Yiu",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d44",
          "name": "Huan Li",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d45",
          "name": "Qiaomu Shen",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d46",
          "name": "Rui Mao",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d47",
          "user": {
            "_id": "66b02a2642c34e7a212133c0",
            "avatarUrl": "/avatars/737a69b095e8c427ecd08f870b173635.svg",
            "isPro": false,
            "fullname": "Bo Tang",
            "user": "BO1022",
            "type": "user"
          },
          "name": "Bo Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:18:01.460Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T15:34:26.000Z",
      "submittedOnDailyAt": "2025-04-17T06:00:20.705Z",
      "title": "AlayaDB: 긴 문맥 LLM 추론의 효율적이고 효과적인 데이터 기반",
      "submittedOnDailyBy": {
        "_id": "66486ba1640bc89c93bcc8a2",
        "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
        "isPro": false,
        "fullname": "Yangshen Deng",
        "user": "YangshenDeng",
        "type": "user"
      },
      "summary": "アライアDB는 대용량 언어 모델(LLMs)의 긴 문맥 추론을 효율적으로하고 효과적으로 수행하기 위해, 原生地に 구축한 최신 벡터 데이터베이스 시스템입니다. 특히, LLM 추론 시스템에서 KV 캐시와 注意 계산을 분리하여 새로운 벡터 데이터베이스 시스템에 통합됩니다. 모델 아키텍처 제공자(MaaS)에 대해, 현재의 대체 솔루션(예: KV 캐시의 분리, 검색 기반의 희소 注意)과 비교하여, 다양한 작업로드에서 다양한 서비스 수준 목표(SLOs)에 대해, 경량한 장비 자원 사용량과 고품질의 생성을 제공합니다. 阿里云의 핵심은 LLM 추론의 注意 계산과 캐시 관리를 쿼리 처리 프로세스에抽象화하고, 자연스러운 쿼리 연산자로 성능을 최적화합니다. 본 논문에서, 업계 파트너로부터 3개의 사례 연구와 LLM 추론 벤치마크의 확장된 실험 결과를 통해, 阿里云의 효율성을 입증합니다.",
      "upvotes": 12,
      "discussionId": "67ff772261373fdf16ce1d93",
      "ai_keywords": [
        "vector database",
        "long-context inference",
        "Large Language Models (LLMs)",
        "KV cache",
        "attention computation",
        "Model as a Service (MaaS)",
        "Service Level Objectives (SLOs)",
        "KV cache disaggregation",
        "retrieval-based sparse attention",
        "query processing procedure",
        "native query optimizer",
        "LLM inference benchmarks"
      ]
    },
    "publishedAt": "2025-04-14T11:34:26.000Z",
    "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
    "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10326.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66486ba1640bc89c93bcc8a2",
      "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
      "fullname": "Yangshen Deng",
      "name": "YangshenDeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09081",
      "authors": [
        {
          "_id": "67fdbfcccaa65039e8c3d8ae",
          "user": {
            "_id": "67dcd93f73e2178fe917b893",
            "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
            "isPro": false,
            "fullname": "Prabhat Pandey",
            "user": "panprabh",
            "type": "user"
          },
          "name": "Prabhat Pandey",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:52.370Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8af",
          "name": "Rupak Vignesh Swaminathan",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b0",
          "user": {
            "_id": "66279810009f1bfdc6bf71bf",
            "avatarUrl": "/avatars/97f561c91b92e1abb4fe6f0b5c688126.svg",
            "isPro": false,
            "fullname": "Girish",
            "user": "vijaygirish2001",
            "type": "user"
          },
          "name": "K V Vijay Girish",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:49.809Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b1",
          "user": {
            "_id": "66367dfb2d6b86ff193dbbe0",
            "avatarUrl": "/avatars/0a0c230bb5fc81a28a166691146cf807.svg",
            "isPro": false,
            "fullname": "Arunasish Sen",
            "user": "svinxz",
            "type": "user"
          },
          "name": "Arunasish Sen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T11:58:01.767Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b2",
          "name": "Jian Xie",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b3",
          "name": "Grant P. Strimel",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b4",
          "name": "Andreas Schwarz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-12T04:45:48.000Z",
      "submittedOnDailyAt": "2025-04-17T07:32:43.212Z",
      "title": "SIFT-50M: 음성 명령에 대한 대형 다국어 데이터 세트에서의 미세 조정",
      "submittedOnDailyBy": {
        "_id": "67dcd93f73e2178fe917b893",
        "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
        "isPro": false,
        "fullname": "Prabhat Pandey",
        "user": "panprabh",
        "type": "user"
      },
      "summary": "SIFT（스peech Instruction Fine-tuning）를 소개합니다. SIFT-50M은 인스트럭션 fine-tuning과 스peech-텍스트 Large Language Models (LLMs)의 사전학습에 적합한 50M개의 데이터셋입니다. SIFT-50M은 공개적으로 사용할 수 있는 스peech 코퍼스로 구축되었으며, 총 14,000시간의 스peech를 포함하며, LLMs와 상업용 전문 모델을 활용하고 있습니다. 이 데이터셋은 5언어로 확장되었으며, 다양한 스peech 이해와 제어 가능한 스peech 생성 인스트럭션을 포함합니다. SIFT-50M을 사용하여 SIFT-LLM를 훈련시키고, 인스트럭션 따라行为的 벤치마크에서 현재의 스peech-텍스트 LLMs를 초월하고 기본적인 스peech 태스크에서 우수한 성능을 발휘했습니다. 또한, 더进一步的 연구를 위해 EvalSIFT를 소개합니다. EvalSIFT는 스peech-텍스트 LLMs의 인스트럭션 따라行为的 능력을 평가하기 위해 특별히 설계된 벤치마크 데이터셋입니다.",
      "upvotes": 9,
      "discussionId": "67fdbfe0caa65039e8c3de4b",
      "ai_keywords": [
        "SIFT (Speech Instruction Fine-Tuning)",
        "speech-text large language models (LLMs)",
        "instruction fine-tuning",
        "pre-training",
        "speech corpora",
        "off-the-shelf expert models",
        "speech understanding",
        "controllable speech generation",
        "SIFT-LLM",
        "instruction-following benchmarks",
        "foundational speech tasks",
        "EvalSIFT"
      ]
    },
    "publishedAt": "2025-04-12T00:45:48.000Z",
    "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning",
    "summary": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09081.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67dcd93f73e2178fe917b893",
      "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
      "fullname": "Prabhat Pandey",
      "name": "panprabh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10483",
      "authors": [
        {
          "_id": "67fdd3e3913c97aa32f94e9b",
          "user": {
            "_id": "641480554b1701c01cdb36c4",
            "avatarUrl": "/avatars/f1f6b294e0236d76a68c099164c81f36.svg",
            "isPro": false,
            "fullname": "Xingjian Leng",
            "user": "xingjianleng",
            "type": "user"
          },
          "name": "Xingjian Leng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:36.449Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9c",
          "name": "Jaskirat Singh",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9d",
          "user": {
            "_id": "6752870ec63bc5b670b1b27e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6752870ec63bc5b670b1b27e/3CdHxnyTKbGup-1V67nEV.jpeg",
            "isPro": false,
            "fullname": "Yunzhong Hou",
            "user": "yunzhong-hou",
            "type": "user"
          },
          "name": "Yunzhong Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:11.946Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9e",
          "user": {
            "_id": "63bf533f4a2beec65568f813",
            "avatarUrl": "/avatars/bba583653b5cada8dd4a3ff2281e9dec.svg",
            "isPro": false,
            "fullname": "Xing",
            "user": "Zhenchang",
            "type": "user"
          },
          "name": "Zhenchang Xing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:21.190Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9f",
          "user": {
            "_id": "6596422646624a86ff3b3bda",
            "avatarUrl": "/avatars/216e12b77e45ac5f1fa20932f5745411.svg",
            "isPro": false,
            "fullname": "Saining Xie",
            "user": "sainx",
            "type": "user"
          },
          "name": "Saining Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:39.700Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94ea0",
          "user": {
            "_id": "666351ebd86c026caa135e5c",
            "avatarUrl": "/avatars/50a37f7e999f660c69f518b71577eb7d.svg",
            "isPro": false,
            "fullname": "Liang Zheng",
            "user": "liangzheng06",
            "type": "user"
          },
          "name": "Liang Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:56.041Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:59:53.000Z",
      "submittedOnDailyAt": "2025-04-17T05:08:17.990Z",
      "title": "REPA-E: 잠재적 디퓨저를 활용한 종단부터 종단으로의 튜닝에서 VAE를 해방하는 방법\n트랜스포머스\n\n(注意：这里将“Transformers”直接翻译为“트랜스포머스”，因为“트랜스포머스”是Transformers在韩语中的常见翻译。如果需要更正式或学术性的翻译，可以考虑使用“트랜스포머 모델”或“트랜스포머 기술”。)",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "이 논문에서는 기본적인 문제를 다루고 있습니다: \"潜在的な 디퓨션 모델과 분산 오토인코더(VAE) 토크나이저를 통일하여 끝에서 끝까지 한꺼번에 훈련할 수 있을까요?\" 전통적인 심층 학습의 지식은 가능한 한 끝에서 끝까지 한꺼번에 훈련이 좋습니다. 그러나潜在的な 디퓨션 트랜스포머의 경우, 표준적인 디퓨션 손실을 사용하여 VAE와 디퓨션 모델을 끝에서 끝까지 한꺼번에 훈련하는 것은 최종적인 성능이 떨어지는 것을 확인합니다. 우리는 디퓨션 손실이 유효하지 않다는 것을 보여주고, 끝에서 끝까지 훈련을 가능하게 하기 위해 표현 조정(REPA) 손실을 사용하는 것이 가능한 것을 보여주었습니다. REPA-E의 간단한 훈련 프로세스는 REPA와 베지아의 훈련 프로세스에 비해 디퓨션 모델의 훈련 속도를 17배 이상, 45배 이상 빠르게 할 수 있습니다. 흥미롭게도, REPA-E에서 끝에서 끝까지 훈련은 VAE 자체도 개선하고, 잠재 공간 구조와 다운 스트림 생성 성능도 향상시킵니다. 최종적인 성능에 있어서, 우리의 접근 방식은 새로운 최단거리로 됩니다. ImageNet 256 x 256에서, 클래스 플레이드 GUI드가 없는 경우와 있는 경우, FID가 1.26과 1.83을 달성합니다. 코드는 https://end2end-diffusion.github.io에 제공됩니다.",
      "upvotes": 3,
      "discussionId": "67fdd3e5913c97aa32f94ee3",
      "projectPage": "https://end2end-diffusion.github.io/",
      "githubRepo": "https://github.com/End2End-Diffusion/REPA-E",
      "ai_keywords": [
        "latent diffusion models",
        "variational auto-encoder (VAE) tokenizer",
        "end-to-end training",
        "diffusion-loss",
        "representation-alignment (REPA) loss",
        "diffusion model training",
        "VAE",
        "latent space structure",
        "downstream generation performance",
        "FID",
        "ImageNet 256 x 256"
      ]
    },
    "publishedAt": "2025-04-14T13:59:53.000Z",
    "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers",
    "summary": "In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10483.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 820
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11952",
      "authors": [
        {
          "_id": "6800646e0679d4ec4b9d01a7",
          "user": {
            "_id": "645c60dd7d655680b57ddbff",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
            "isPro": true,
            "fullname": "Ram Kadiyala",
            "user": "1024m",
            "type": "user"
          },
          "name": "Ram Mohan Rao Kadiyala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:04:05.627Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01a8",
          "user": {
            "_id": "63da8ba3f03c3d71ef32408c",
            "avatarUrl": "/avatars/1b2e6f3ea2bac5ab35dbd53edb7f8cf2.svg",
            "isPro": false,
            "fullname": "Siddartha Pullakhandam",
            "user": "Siddartha10",
            "type": "user"
          },
          "name": "Siddartha Pullakhandam",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:18:54.938Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01a9",
          "name": "Kanwal Mehreen",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01aa",
          "user": {
            "_id": "618c1ad1c74578e0a4a4d074",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618c1ad1c74578e0a4a4d074/8u_AkeHt4d6xtQ8hzaffU.jpeg",
            "isPro": true,
            "fullname": "Drishti Sharma",
            "user": "DrishtiSharma",
            "type": "user"
          },
          "name": "Drishti Sharma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:19:22.573Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ab",
          "name": "Siddhant Gupta",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ac",
          "user": {
            "_id": "66c578770a22b2f9ab575847",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c578770a22b2f9ab575847/-zfNho1DR3yZHDazq669-.png",
            "isPro": false,
            "fullname": "Jebish Purbey",
            "user": "jebish7",
            "type": "user"
          },
          "name": "Jebish Purbey",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:00.066Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ad",
          "user": {
            "_id": "653d84f13fc9c706fa755d03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653d84f13fc9c706fa755d03/F_jYbeuLLM9EX8hKXcbHD.png",
            "isPro": false,
            "fullname": "Ashay Srivastava",
            "user": "ashay-sriv",
            "type": "user"
          },
          "name": "Ashay Srivastava",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:15.588Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ae",
          "user": {
            "_id": "669a745a4bbe8ad52ee287cf",
            "avatarUrl": "/avatars/245644aa638b45a17ff71124bd5bbe0f.svg",
            "isPro": false,
            "fullname": "Subhasya Tippareddy",
            "user": "subhasyar",
            "type": "user"
          },
          "name": "Subhasya TippaReddy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:23.946Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01af",
          "user": {
            "_id": "669a7383c9111326dc596f5e",
            "avatarUrl": "/avatars/8bbd307fd4bb2d7055b2c8fc9140dc81.svg",
            "isPro": false,
            "fullname": "Arvind Reddy Bobbili",
            "user": "Arvindreddy",
            "type": "user"
          },
          "name": "Arvind Reddy Bobbili",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:32.804Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b0",
          "name": "Suraj Telugara Chandrashekhar",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b1",
          "user": {
            "_id": "668db0bfb09a05f3d7cc796f",
            "avatarUrl": "/avatars/969c511cca4d129b99eca6252a468385.svg",
            "isPro": false,
            "fullname": "Modabbir Adeeb",
            "user": "moda10",
            "type": "user"
          },
          "name": "Modabbir Adeeb",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:46.461Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b2",
          "user": {
            "_id": "641c3337f0b71a9743629985",
            "avatarUrl": "/avatars/820b124887f173c263a675728baf99c6.svg",
            "isPro": false,
            "fullname": "Srinadh Vura",
            "user": "SriV",
            "type": "user"
          },
          "name": "Srinadh Vura",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:53.315Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b3",
          "name": "Hamza Farooq",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T10:29:30.000Z",
      "submittedOnDailyAt": "2025-04-17T00:47:10.039Z",
      "title": "로바스트な 핀기어디텍스의 AI 생성 텍스트 검출",
      "submittedOnDailyBy": {
        "_id": "645c60dd7d655680b57ddbff",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
        "isPro": true,
        "fullname": "Ram Kadiyala",
        "user": "1024m",
        "type": "user"
      },
      "summary": "理想な マシン 生成 内容 の 検出 システム は 、いくつかの ジェネレーター 上でも 良く 動作 する べき で ある 。 既存 の システム は 、 短い テキスト で AI 生成 内容 を 正確 に 識別 する こと が 難しい 。 また 、 全て の テキスト は 人間 か LLM が 完全 に 著手 した もの では ない ので 、 これら の 部分ケース に 焦点 を 当てて います 。 我々 の 論文 では 、 トークンクラス 分類 の 任務 に 向けて 構築 された モデル の セット を 紹介 し 、 これら の モデル は 、 見た こと の ない ドメイン 、 見た こと の ない ジェネレーター 、 非母語者 の テキスト 、 そして 対抗的 な 入力 に 対して も 良く 動作 しました 。 また 、 我々 は 、 23 言語 で の ショートポップ の LLM が 共著 した ほとんど 全て の テキスト を 2.4M 以上 の データセット を 紹介 しました 。 また 、 各 ドメイン と ジェネレーター の それぞれ の テキスト に おける モデル の 性能 を 示し 、 対抗的 な 手法 に 対する 性能 比較 、 入力 テキスト の 長さ 、 生成 テキスト と 元の 人間 著手 テキスト の 特徴 を 比較 した 結果 も 追加 しました 。",
      "upvotes": 2,
      "discussionId": "680064710679d4ec4b9d0224",
      "ai_keywords": [
        "token classification",
        "adversarial inputs"
      ]
    },
    "publishedAt": "2025-04-16T06:29:30.000Z",
    "title": "Robust and Fine-Grained Detection of AI Generated Texts",
    "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645c60dd7d655680b57ddbff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
      "fullname": "Ram Kadiyala",
      "name": "1024m",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11092",
      "authors": [
        {
          "_id": "6800a43e1fd95d7dc21d6b83",
          "user": {
            "_id": "63e367d3fae035bdc4c347fc",
            "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
            "isPro": false,
            "fullname": "Jiaxin Huang",
            "user": "JaceyH919",
            "type": "user"
          },
          "name": "Jiaxin Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:03:59.476Z",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b84",
          "name": "Sheng Miao",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b85",
          "name": "BangBnag Yang",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b86",
          "name": "Yuewen Ma",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b87",
          "name": "Yiyi Liao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T11:38:14.000Z",
      "submittedOnDailyAt": "2025-04-17T07:32:38.400Z",
      "title": "Vivid4D: 4D 재구성을 개선하기 위한 비디오 기반의 단원 카메라 비디오\n\n(注意：虽然要求不添加额外文本，但为了确保翻译的准确性和专业性，我在翻译时尽量保持了原文的技术术语和格式。)",
      "submittedOnDailyBy": {
        "_id": "63e367d3fae035bdc4c347fc",
        "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
        "isPro": false,
        "fullname": "Jiaxin Huang",
        "user": "JaceyH919",
        "type": "user"
      },
      "summary": "4D 동적인 화면을 단순하게 촬영한 단목 비디오로부터 재구성하는 것은 가치가 있지만, 매우 어려워서입니다. 시간 슬라이드의 각 시간점은 단일의 관점에서 볼 수 있습니다. 우리는 새로운 접근법인 Vivid4D를 소개합니다. 이 방법은 관찰 관점을 강화하고 4D 단목 비디오 합성을 향상시키기 위해 목표를 세웁니다. 기존 방법에서 다른 점은 단목의 깊이 우선을 사용하여 관찰 관점을 강화하고 다른 관점의 비디오를 합성합니다. 이 방법은 새로운 관점으로 비디오를 이동시키는 것처럼 비디오의 컷 편집 작업으로 재구성됩니다. 이를 위해 합성된 마스크를 사용하여 무동작 웹 비디오로 비디오 컷 편집 모델을 훈련합니다. 이는 공간적 및 시간적 연속성을 보장하는 데 사용됩니다. 또한 단목의 깊이 우선의 부정확성을 줄이기 위해 관찰 관점을 강화하는 반복적인 접근법과 강력한 재구성 손실을 도입합니다. 실험은 우리의 방법이 단목의 4D 화면의 재구성과 완성을 효과적으로 향상시키는 것을 보여줍니다.",
      "upvotes": 2,
      "discussionId": "6800a4441fd95d7dc21d6d46",
      "projectPage": "https://xdimlab.github.io/Vivid4D/",
      "ai_keywords": [
        "Vivid4D",
        "4D monocular video synthesis",
        "view augmentation",
        "video inpainting",
        "monocular depth priors",
        "unposed web videos",
        "synthetically generated masks",
        "iterative view augmentation strategy",
        "robust reconstruction loss"
      ]
    },
    "publishedAt": "2025-04-15T07:38:14.000Z",
    "title": "Vivid4D: Improving 4D Reconstruction from Monocular Video by Video\n  Inpainting",
    "summary": "Reconstructing 4D dynamic scenes from casually captured monocular videos is\nvaluable but highly challenging, as each timestamp is observed from a single\nviewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular\nvideo synthesis by augmenting observation views - synthesizing multi-view\nvideos from a monocular input. Unlike existing methods that either solely\nleverage geometric priors for supervision or use generative priors while\noverlooking geometry, we integrate both. This reformulates view augmentation as\na video inpainting task, where observed views are warped into new viewpoints\nbased on monocular depth priors. To achieve this, we train a video inpainting\nmodel on unposed web videos with synthetically generated masks that mimic\nwarping occlusions, ensuring spatially and temporally consistent completion of\nmissing regions. To further mitigate inaccuracies in monocular depth priors, we\nintroduce an iterative view augmentation strategy and a robust reconstruction\nloss. Experiments demonstrate that our method effectively improves monocular 4D\nscene reconstruction and completion.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11092.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e367d3fae035bdc4c347fc",
      "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
      "fullname": "Jiaxin Huang",
      "name": "JaceyH919",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11536",
      "authors": [
        {
          "_id": "6800cc7159e20f50cc282e87",
          "name": "Jiazhan Feng",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e88",
          "name": "Shijue Huang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e89",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8a",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8b",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8c",
          "name": "Baoquan Zhong",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8d",
          "name": "Chengquan Jiang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8e",
          "name": "Jinxin Chi",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8f",
          "name": "Wanjun Zhong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T18:10:22.000Z",
      "submittedOnDailyAt": "2025-04-17T08:10:39.383Z",
      "title": "ReTool: 전략적인 도구 사용에 대한 강화 학습",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "리스닝 모델(예: DeepSeek R1)은 강화학습(RL)을 사용하여 훈련된 모델로, 자연어의 논리적 논리에 능숙하지만, 구조화된 문제 해결에 적합하지 않습니다. 예를 들어, 기하학적 논리, 간단한 계산, 복잡한 방정식 해법과 같은 경우, 계산 도구처럼 코드 인터프리터(CI)가 뛰어납니다. 이를 메우기 위해, ReTool을 제안합니다. ReTool은 학습을 통해 통합된 도구를 사용하는 긴 문장 논리적 논리를 강화하고, 두 가지 주요 기능들을 가지고 있습니다. 1. 자연어 논리적 논리 처리 중 실시간 코드 실행의 동적 간격, 2. 자동화된 RL 패러다임에서, 실시간 코드 실행을 포함하는 다턴 정책 로울아웃을 허용하고, 모델이 어떤 도구를 호출하는 방법을 학습시키는 것입니다. ReTool은 합성적인 냉각 데이터의 생성을 시작하고, 코드 추가의 긴 문장 논리적 논리 트래스 생성하고, 기본 모델의 미세 조정을 수행합니다. 다음으로, RL 훈련은 작업의 결과를 보상으로, 모델의 도구 사용 전략을 연속적으로 개선하고, 인간의 사전 지식을 제외하고, 최적의 도구 호출 패턴을 자동으로 발견할 수 있습니다. 엄격한 MATH OLYMPIAD 벤치마크의 AIME에서의 실험으로, ReTool의 우수한 성능이 나타납니다: 우리의 32B 모델은 400트레이닝 스텝에서 67%의 정확도를 달성하며, 텍스트 기반의 RL 기반 라인(40% 정확도, 1080 스텝)를 뛰어넘으며, 효율성과 성능에도 뛰어납니다. 특히, ReTool-32B는 확장된 설정에서 72.5%의 정확도를 달성하며, OpenAI의 o1-preview을 27.9% 초과합니다.进一步的分析表明，发现了类似于代码自动调整的行为，模型能够自动学习适应性的工具使用，这被称为“啊哈”时刻。这些发现展示了基于结果的工具整合的可能性，并为复杂的数学逻辑推理的进步提供了新的视角。",
      "upvotes": 0,
      "discussionId": "6800cc7359e20f50cc282f43",
      "ai_keywords": [
        "reinforcement learning",
        "dynamic interleaving",
        "real-time code execution",
        "natural language reasoning processes",
        "automated RL paradigm",
        "policy rollouts",
        "multi-turn real-time code execution",
        "synthetic cold-start data generation",
        "code-augmented long-form reasoning traces",
        "fine-tuning",
        "RL training",
        "task outcomes as rewards",
        "autonomous discovery",
        "optimal tool invocation patterns",
        "MATH Olympiad benchmark",
        "AIME",
        "accuracy",
        "training steps",
        "OpenAI's o1-preview",
        "code self-correction",
        "adaptive tool use",
        "hybrid neuro-symbolic systems"
      ]
    },
    "publishedAt": "2025-04-15T14:10:22.000Z",
    "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
    "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11536.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6669
    },
    "isAuthorParticipating": false
  }
]