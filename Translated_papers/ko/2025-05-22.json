[
  {
    "paper": {
      "id": "2505.15277",
      "authors": [
        {
          "_id": "682e854551706f69070aca6b",
          "user": {
            "_id": "64c8f4cec547ed5243ebd0a8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
            "isPro": false,
            "fullname": "Hyungjoo Chae",
            "user": "hyungjoochae",
            "type": "user"
          },
          "name": "Hyungjoo Chae",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:37.301Z",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6c",
          "user": {
            "_id": "646a0897c37ca1e12308b026",
            "avatarUrl": "/avatars/6d720a9e366db9bec15c8c10878c0c75.svg",
            "isPro": false,
            "fullname": "Sunghwan Kim",
            "user": "KimSHine",
            "type": "user"
          },
          "name": "Sunghwan Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:32.322Z",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6d",
          "name": "Junhee Cho",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6e",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6f",
          "name": "Seungjun Moon",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca70",
          "name": "Gyeom Hwangbo",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca71",
          "user": {
            "_id": "6683b8680b72be136701de35",
            "avatarUrl": "/avatars/0c135e570b16b81ee2fb81ad65b01ba8.svg",
            "isPro": false,
            "fullname": "Dongha Lim",
            "user": "donghalim",
            "type": "user"
          },
          "name": "Dongha Lim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:34.741Z",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca72",
          "name": "Minjin Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca73",
          "name": "Yeonjun Hwang",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca74",
          "name": "Minju Gwak",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca75",
          "name": "Dongwook Choi",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca76",
          "name": "Minseok Kang",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca77",
          "name": "Gwanhoon Im",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca78",
          "name": "ByeongUng Cho",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca79",
          "name": "Hyojun Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7a",
          "name": "Jun Hee Han",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7b",
          "name": "Taeyoon Kwon",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7c",
          "name": "Minju Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7d",
          "name": "Beong-woo Kwak",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7e",
          "name": "Dongjin Kang",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7f",
          "name": "Jinyoung Yeo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64c8f4cec547ed5243ebd0a8/hXqaaoJTvW35xMW1lPVv0.png"
      ],
      "publishedAt": "2025-05-21T08:56:55.000Z",
      "submittedOnDailyAt": "2025-05-22T00:31:53.858Z",
      "title": "웹-슈퍼드: 웹 계정 관리 도구를 강화하기 위한 PRM의 발전",
      "submittedOnDailyBy": {
        "_id": "64c8f4cec547ed5243ebd0a8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
        "isPro": false,
        "fullname": "Hyungjoo Chae",
        "user": "hyungjoochae",
        "type": "user"
      },
      "summary": "웹 맵핑은 주로 반복적으로 수행되는 실시간 작업의 자동화를 목적으로 특화된 분야이며, 일반적인 다모달 대 언어 모델(MLLM)의 태스크보다 장기적인 순차적인 결정을 요구하기 때문에 더 어려워집니다. 그러나 웹 맵핑의 훈련 및 테스트 시에 사용할 수 있는 전문적인 보상 모델이 아직 존재하지 않았습니다. 속도와 비용 효율성에 대한 중요성을 고려한 기존 연구에서는 MLLM을 보상 모델로 사용했지만, 실제적인 도입에는 큰 제약이 있었습니다. 이에 대해, 본 논문에서는 첫 번째 프로세스 보상 모델(PRM)인 웹 셰퍼드(Web-Shepherd)를 제안합니다. 웹 셰퍼드는 웹 맵핑 프로세스 단계별로 평가할 수 있습니다. 이를 위해 먼저 웹 PRM 컬렉션을 구축합니다. 웹 PRM 컬렉션은 40K 단계 수준의 선호 페어와 다양한 영역 및 난이도 레벨을 포함하는 큰 데이터 세트입니다. 다음으로 웹 보상 벤치마크(WebRewardBench)를 소개합니다. 웹 보상 벤치마크는 PRM의 평가에 사용할 첫 번째 메타 평가 벤치마크입니다. 실험에서 웹 보상 벤치마크에서 웹 셰퍼드가 GPT-4o를 사용했을 때 약 30점의 정확도를 향상시킬 수 있었습니다. 또한 정책으로 GPT-4o-mini를 사용하며, 웹 셰퍼드를 테스트할 때 웹 아레나-lite에서 GPT-4o-mini를 테스트하고 사용했을 때 10.9점의 성능 향상과 10배의 비용 절감이 이루어졌습니다. 모델, 데이터 세트, 코드는 공개적으로 링크로 사용 가능합니다.",
      "upvotes": 75,
      "discussionId": "682e854951706f69070acbf0",
      "githubRepo": "https://github.com/kyle8581/Web-Shepherd",
      "ai_summary": "The paper introduces Web-Shepherd, a process reward model for web navigation, which improves accuracy and cost-effectiveness in step-level trajectory assessment compared to existing multimodal large language models.",
      "ai_keywords": [
        "multimodal large language model",
        "process reward model",
        "web navigation",
        "webPRM collection",
        "webrewardbench",
        "long-horizon sequential decision making",
        "preference pairs",
        "annotated checklists",
        "step-level assessment",
        "webarena-lite",
        "policy",
        "verifier"
      ]
    },
    "publishedAt": "2025-05-21T04:56:55.000Z",
    "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents",
    "summary": "Web navigation is a unique domain that can automate many repetitive real-life\ntasks and is challenging as it requires long-horizon sequential decision making\nbeyond typical multimodal large language model (MLLM) tasks. Yet, specialized\nreward models for web navigation that can be utilized during both training and\ntest-time have been absent until now. Despite the importance of speed and\ncost-effectiveness, prior works have utilized MLLMs as reward models, which\nposes significant constraints for real-world deployment. To address this, in\nthis work, we propose the first process reward model (PRM) called Web-Shepherd\nwhich could assess web navigation trajectories in a step-level. To achieve\nthis, we first construct the WebPRM Collection, a large-scale dataset with 40K\nstep-level preference pairs and annotated checklists spanning diverse domains\nand difficulty levels. Next, we also introduce the WebRewardBench, the first\nmeta-evaluation benchmark for evaluating PRMs. In our experiments, we observe\nthat our Web-Shepherd achieves about 30 points better accuracy compared to\nusing GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by\nusing GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve\n10.9 points better performance, in 10 less cost compared to using GPT-4o-mini\nas the verifier. Our model, dataset, and code are publicly available at LINK.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64c8f4cec547ed5243ebd0a8/hXqaaoJTvW35xMW1lPVv0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15277.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64c8f4cec547ed5243ebd0a8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
      "fullname": "Hyungjoo Chae",
      "name": "hyungjoochae",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14302",
      "authors": [
        {
          "_id": "682e887e866a44f6a81409b1",
          "user": {
            "_id": "64aea082704210bf815e7551",
            "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
            "isPro": false,
            "fullname": "Mengzhao Chen",
            "user": "ChenMnZ",
            "type": "user"
          },
          "name": "Mengzhao Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:29.873Z",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b2",
          "name": "Chaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b3",
          "name": "Jing Liu",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b4",
          "name": "Yutao Zeng",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b5",
          "name": "Zeyue Xue",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b6",
          "name": "Zhiheng Liu",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b7",
          "name": "Yunshui Li",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b8",
          "name": "Jin Ma",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b9",
          "name": "Jie Huang",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409ba",
          "name": "Xun Zhou",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409bb",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T12:54:43.000Z",
      "submittedOnDailyAt": "2025-05-22T00:44:46.277Z",
      "title": "확률적 인식 훈련의 스케일링 법칙",
      "submittedOnDailyBy": {
        "_id": "64aea082704210bf815e7551",
        "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
        "isPro": false,
        "fullname": "Mengzhao Chen",
        "user": "ChenMnZ",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는 계산량과 메모리 리소스의 큰 요구를 동반하며, 도입에 문제가 생깁니다. Quantization-aware training(QAT)은 모델의 정확도를 감소시키면서 성능을 유지함으로써 이러한 문제를 해결합니다. 그러나, QAT의 스케일링 행동, 특히 4비트 정확도(W4A4)에서는 이해가 약한 상태입니다. 현재의 QAT 스케일링 방법은 학습 토큰의 수와 양자화 거칠기 등 중요한 요인을 무시하고, 적용 범위가 좁아지고 있습니다. 본 논문에서는 모델 크기, 학습 데이터량과 양자화 그룹 크기를 구성 요소로, QAT의 양자화 오류를 측정하는 스케일링 방법을 제안합니다. 268회의 QAT 실험을 통해 모델 크기가 증가함에 따라 양자화 오류가 감소하고, 학습 토큰의 증가와 거친 양자화 거칠기에 의해 증가하는 것을 보여줍니다. W4A4 양자화 오류의 원인을 특정하기 위해, 이를 가중치와 활성 성분으로 분해합니다. 두 성분은 W4A4 양자화 오류의 전체적인 추세를 따라하지만, 다른 민감성을 가지고 있습니다. 특히, 가중치 양자화 오류는 학습 토큰의 증가에 의해 급격히 증가합니다. 진한 분석에서는 FC2 계층에서 활성 성분의 양자화 오류, 이는 아웃라이어로 인한 것임을, W4A4 QAT 양자화 오류의 주요한 꼴neck을 보여주고 있습니다. 이 꼴neck을 해결하기 위해, 혼합 정밀도 양자화를 적용하고, 가중치와 활성 성분의 양자화 오류가 유사한 수준에 도달하는 것을 보여줍니다. 또한, 학습 데이터가 증가함에 따라, 가중치 양자화 오류는 활성 성분의 양자화 오류를 초과하고, 이러한 경우에도 가중치 양자화 오류의 감소는 중요하다는 것을 보여주고 있습니다. 이러한 발견은 QAT의 연구와 개발의 향상에 연결된 주요 전망을 제공합니다.",
      "upvotes": 46,
      "discussionId": "682e887e866a44f6a81409f0",
      "ai_summary": "A unified scaling law for quantization-aware training (QAT) identifies key factors affecting quantization error, leading to improvements through mixed-precision quantization.",
      "ai_keywords": [
        "quantization-aware training",
        "QAT",
        "quantization error",
        "model size",
        "training tokens",
        "quantization granularity",
        "weight quantization",
        "activation quantization",
        "mixed-precision quantization",
        "FC2 layer"
      ]
    },
    "publishedAt": "2025-05-20T08:54:43.000Z",
    "title": "Scaling Law for Quantization-Aware Training",
    "summary": "Large language models (LLMs) demand substantial computational and memory\nresources, creating deployment challenges. Quantization-aware training (QAT)\naddresses these challenges by reducing model precision while maintaining\nperformance. However, the scaling behavior of QAT, especially at 4-bit\nprecision (W4A4), is not well understood. Existing QAT scaling laws often\nignore key factors such as the number of training tokens and quantization\ngranularity, which limits their applicability. This paper proposes a unified\nscaling law for QAT that models quantization error as a function of model size,\ntraining data volume, and quantization group size. Through 268 QAT experiments,\nwe show that quantization error decreases as model size increases, but rises\nwith more training tokens and coarser quantization granularity. To identify the\nsources of W4A4 quantization error, we decompose it into weight and activation\ncomponents. Both components follow the overall trend of W4A4 quantization\nerror, but with different sensitivities. Specifically, weight quantization\nerror increases more rapidly with more training tokens. Further analysis shows\nthat the activation quantization error in the FC2 layer, caused by outliers, is\nthe primary bottleneck of W4A4 QAT quantization error. By applying\nmixed-precision quantization to address this bottleneck, we demonstrate that\nweight and activation quantization errors can converge to similar levels.\nAdditionally, with more training data, weight quantization error eventually\nexceeds activation quantization error, suggesting that reducing weight\nquantization error is also important in such scenarios. These findings offer\nkey insights for improving QAT research and development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14302.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64aea082704210bf815e7551",
      "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
      "fullname": "Mengzhao Chen",
      "name": "ChenMnZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14231",
      "authors": [
        {
          "_id": "682db25d265177367e35d5b1",
          "name": "Sule Bai",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b2",
          "name": "Mingxing Li",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b3",
          "name": "Yong Liu",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b4",
          "name": "Jing Tang",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b5",
          "name": "Haoji Zhang",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b6",
          "name": "Lei Sun",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b7",
          "user": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "name": "Xiangxiang Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T12:27:07.959Z",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b8",
          "name": "Yansong Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T11:40:43.000Z",
      "submittedOnDailyAt": "2025-05-22T00:13:45.780Z",
      "title": "UniVG-R1: 강화학습에 의한 논리적인 일반적인 시각화 조드닝",
      "submittedOnDailyBy": {
        "_id": "6513a0f14f1682e4407758a9",
        "avatarUrl": "/avatars/b2a6886114492944cfa235363817565f.svg",
        "isPro": false,
        "fullname": "Mingxing Li",
        "user": "MingxingLi",
        "type": "user"
      },
      "summary": "전통적인 시각 위치화 방법들은 주로 단일 이미지 환경에서 수행되고, 간단한 텍스트 참조를 사용합니다. 그러나 이러한 방법을 실제 환경에서 사용 가능한, 복잡한 지침을 포함하는 여러 이미지에 적용하기 위해 큰 도전을 제기하고 있습니다. 이는 여러 모드의 복잡한 맥락을 처리하는 고급 추론 능력의 부족으로 인해 발생합니다. 본 연구는 더 실용적인 일반적인 위치화 작업을 해결하기 위해 UniVG-R1을 제안합니다. 이는 강화 학습(RL)과 냉동 시작 데이터 증강을 통해 추론 능력이 향상된 추론 지침을 위한 다중 모드 대 언어 모델(MLLM)입니다. 구체적으로, 우리는 먼저 상세한 추론 연결을 포함하는 고품질 Chain-of-Thought(CoT) 위치화 데이터셋을 구축하고, 이 데이터셋을 통해 모델이 정확한 추론 경로를 향하도록 지도 학습을 통해 학습시킵니다. 이어서, 규칙 기반의 강화 학습을 수행하여 모델이 정확한 추론 연결을 인식하도록 유도하고, 이를 통해 추론 능력을 향상시킵니다. 또한, RL 훈련 진행 시 발생하는 난이도 편향을 인식하고, 난이도感知의 가중치 조정 전략을 제안하여 성능을 더욱 향상시킵니다. 실험 결과는 UniVG-R1의 유효성을 입증하며, MIG-Bench에서 9.1%의 향상을 달성하였고, 가장 최신의 성능을 달성했습니다. 또한, 우리의 모델은 네 개의 이미지와 비디오 추론 위치화 테스트 데이터베이스에서 평균 23.4%의 0-shot 성능 향상을 나타내었습니다. 프로젝트 페이지는 https://amap-ml.github.io/UniVG-R1-page/에서 방문할 수 있습니다.",
      "upvotes": 39,
      "discussionId": "682db25e265177367e35d638",
      "projectPage": "https://amap-ml.github.io/UniVG-R1-page/",
      "githubRepo": "https://github.com/AMAP-ML/UniVG-R1",
      "ai_summary": "UniVG-R1, a reasoning-guided multimodal large language model, enhances visual grounding by leveraging reinforcement learning and a difficulty-aware strategy, achieving state-of-the-art results and strong generalizability.",
      "ai_keywords": [
        "multimodal large language model",
        "reasoning guided",
        "reinforcement learning",
        "cold-start data",
        "Chain-of-Thought dataset",
        "supervised fine-tuning",
        "rule-based reinforcement learning",
        "difficulty bias",
        "difficulty-aware weight adjustment"
      ]
    },
    "publishedAt": "2025-05-20T07:40:43.000Z",
    "title": "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement\n  Learning",
    "summary": "Traditional visual grounding methods primarily focus on single-image\nscenarios with simple textual references. However, extending these methods to\nreal-world scenarios that involve implicit and complex instructions,\nparticularly in conjunction with multiple images, poses significant challenges,\nwhich is mainly due to the lack of advanced reasoning ability across diverse\nmulti-modal contexts. In this work, we aim to address the more practical\nuniversal grounding task, and propose UniVG-R1, a reasoning guided multimodal\nlarge language model (MLLM) for universal visual grounding, which enhances\nreasoning capabilities through reinforcement learning (RL) combined with\ncold-start data. Specifically, we first construct a high-quality\nChain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning\nchains, to guide the model towards correct reasoning paths via supervised\nfine-tuning. Subsequently, we perform rule-based reinforcement learning to\nencourage the model to identify correct reasoning chains, thereby incentivizing\nits reasoning capabilities. In addition, we identify a difficulty bias arising\nfrom the prevalence of easy samples as RL training progresses, and we propose a\ndifficulty-aware weight adjustment strategy to further strengthen the\nperformance. Experimental results demonstrate the effectiveness of UniVG-R1,\nwhich achieves state-of-the-art performance on MIG-Bench with a 9.1%\nimprovement over the previous method. Furthermore, our model exhibits strong\ngeneralizability, achieving an average improvement of 23.4% in zero-shot\nperformance across four image and video reasoning grounding benchmarks. The\nproject page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14231.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6513a0f14f1682e4407758a9",
      "avatarUrl": "/avatars/b2a6886114492944cfa235363817565f.svg",
      "fullname": "Mingxing Li",
      "name": "MingxingLi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15809",
      "authors": [
        {
          "_id": "682e7e061d7637a25846bf52",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf53",
          "user": {
            "_id": "64e357dd825f4133e7427bf8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e357dd825f4133e7427bf8/HwaWhINrzkbXG6SHG2oyf.jpeg",
            "isPro": false,
            "fullname": "tyfeld",
            "user": "tyfeld",
            "type": "user"
          },
          "name": "Ye Tian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:11.786Z",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf54",
          "name": "Bowen Li",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf55",
          "user": {
            "_id": "653e5d31ffd60206c8b64bb5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653e5d31ffd60206c8b64bb5/JrfOSCunFSW39vdW7E59y.png",
            "isPro": false,
            "fullname": "Xinchen Zhang",
            "user": "comin",
            "type": "user"
          },
          "name": "Xinchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:14.114Z",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf56",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf57",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf58",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:59:05.000Z",
      "submittedOnDailyAt": "2025-05-22T00:24:15.122Z",
      "title": "MMaDA: 모노모달 라지리 디퓨옹션 라ン치어드 모댓",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "MMaDA는 텍스트 추론, 다 모델 이해, 텍스트에서 이미지 생성 등 다양한 분야에서 높은 성능을 달성하기 위해 설계된 새로운 클래스의 다 모델 디피유전기 기반 모델입니다. 이 접근 방식은 세 가지 혁신적인 키 포인트를 특징으로 합니다: (i) MMaDA는 공통적인 확률 표현과 모델 의존성 없는 설계를 채택하여 통일된 디피유전기 아키텍처를 사용하며, 모델 의존성 특정 컴포넌트의 필요성을 줄입니다. 이 아키텍처는 서로 다른 데이터 타입 간의 일관된 인터랙션과 처리를 보장합니다. (ii) 모델의 최종의 강화 학습(RL) 단계의 차가 시작 훈련을 촉진하기 위해, 텍스트와 시각 영역의 이유 과정의 통합을 위해, 혼합 라ングチェーン 오프숍(CoT)의 통합 형식을 채택하여 혼합 라ングチェーン 오프숍(CoT)의 미세 조정 전략을 구현합니다. 이 전략은 텍스트와 시각 영역의 이유 과정의 통합을 통해 최종의 RL 단계의 차가 시작 훈련을 촉진하고, 모델이 복잡한 태스크를 초기부터 처리할 능력이 향상됩니다. (iii) UniGRPO라는, 이유와 생성의 두 가지 태스크의 후 학습을 통일적으로 수행하기 위해 특화된 정책 그레이디언트 기반의 RL 알고리즘을 제안합니다. 다양한 보상 모델링을 활용하여, UniGRPO는 이유와 생성의 두 가지 태스크의 후 학습을 통일적으로 수행하고, 일관된 성능 향상을 보장합니다. 실험 결과를 통해, MMaDA-8B는 통일된 다 모델 기반 모델로서 강한 일반화 능력을 보여주고, LLaMA-3-7B와 Qwen2-7B보다 텍스트 추론에서 높은 성능을 보여주고, Show-o와 SEED-X보다 다 모델 이해에서 높은 성능을 보여주고, SDXL과 Janus보다 텍스트에서 이미지 생성에서 높은 성능을 보여주었습니다. 이러한 성과는 MMaDA가 통일된 디피유전기 아키텍처의 사전 학습과 후 학습 사이에 간극을 통합하는 효과성을 보여주고, 향후 연구와 개발에 대한 일관된 프레임워크를 제공합니다. 우리는 다음 URL에서 코드와 학습된 모델을 공개합니다: https://github.com/Gen-Verse/MMaDA",
      "upvotes": 35,
      "discussionId": "682e7e0a1d7637a25846c03b",
      "projectPage": "https://huggingface.co/spaces/Gen-Verse/MMaDA",
      "githubRepo": "https://github.com/Gen-Verse/MMaDA",
      "ai_summary": "MMaDA, a multimodal diffusion foundation model, achieves superior performance through a unified architecture, mixed long chain-of-thought fine-tuning, and a unified policy-gradient-based RL algorithm.",
      "ai_keywords": [
        "multimodal diffusion foundation models",
        "unified diffusion architecture",
        "modality-agnostic design",
        "mixed long chain-of-thought fine-tuning",
        "cold-start training",
        "reinforcement learning",
        "UniGRPO",
        "policy-gradient-based RL algorithm",
        "diversified reward modeling",
        "generalization capabilities",
        "textual reasoning",
        "multimodal understanding",
        "text-to-image generation"
      ]
    },
    "publishedAt": "2025-05-21T13:59:05.000Z",
    "title": "MMaDA: Multimodal Large Diffusion Language Models",
    "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15809.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13909",
      "authors": [
        {
          "_id": "682d525994ed89a9b2aec35d",
          "user": {
            "_id": "661b9ac57cfb7bcb3057a578",
            "avatarUrl": "/avatars/f8afaa8eaad3a1e5963a4feebec3f7ab.svg",
            "isPro": false,
            "fullname": "Yanheng He",
            "user": "henryhe0123",
            "type": "user"
          },
          "name": "Yanheng He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:46.371Z",
          "hidden": false
        },
        {
          "_id": "682d525994ed89a9b2aec35e",
          "user": {
            "_id": "663f5e959e6f865ec6d4fb62",
            "avatarUrl": "/avatars/eb0a908562b2c57335ae8bb949220430.svg",
            "isPro": false,
            "fullname": "Jiahe Jin",
            "user": "zizi-0123",
            "type": "user"
          },
          "name": "Jiahe Jin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:43.438Z",
          "hidden": false
        },
        {
          "_id": "682d525994ed89a9b2aec35f",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T04:20:18.000Z",
      "submittedOnDailyAt": "2025-05-22T00:48:35.836Z",
      "title": "컴퓨터 사용에 대한 효율적인 아웃로버 훈련",
      "submittedOnDailyBy": {
        "_id": "663f5e959e6f865ec6d4fb62",
        "avatarUrl": "/avatars/eb0a908562b2c57335ae8bb949220430.svg",
        "isPro": false,
        "fullname": "Jiahe Jin",
        "user": "zizi-0123",
        "type": "user"
      },
      "summary": "고품질의 타로크 데이터의 확장은 인간처럼 사용하는 컴퓨터 사용 에이전트의 개발에 장기적으로 중요한 후盾였습니다. 우리는 인간示范에 의존을 줄이기 위해 효율적인 에이전트의 훈련 프레임워크인 PC Agent-E를 소개합니다. 처음에는 312건의 인간 설명된 컴퓨터 사용 타로크 데이터만 가지고 있었지만, Claude 3.7 Sonnet을 사용하여 다양한 행동 결정을 합성하고 데이터의 질을 발전시켰습니다. 이러한 확장된 타로크 데이터에 훈련된 PC Agent-E 모델은 WindowsAgentArena-V2에서 확장된 사고를 가진 강력한 Claude 3.7 Sonnet을 초월하여 141%의 상대적인 향상을 달성했습니다. 또한, PC Agent-E는 OSWorld에서 다양한 운영체제에 대한 강력한 일반화 성능을 보여주었습니다. 우리의 발견은少量의 고품질의 타로크 데이터에서 강력한 컴퓨터 사용 능력을 자극할 수 있다는 것을 보여주고 있습니다.",
      "upvotes": 25,
      "discussionId": "682d525a94ed89a9b2aec397",
      "githubRepo": "https://github.com/GAIR-NLP/PC-Agent-E",
      "ai_summary": "PC Agent-E framework improves data efficiency and achieves superior performance on human-like computer use tasks through enhanced trajectory synthesis and training.",
      "ai_keywords": [
        "agent training framework",
        "human-annotated trajectories",
        "action decisions",
        "Claude 3.7 Sonnet",
        "WindowsAgentArena-V2",
        "OSWorld",
        "generalizability"
      ]
    },
    "publishedAt": "2025-05-20T00:20:18.000Z",
    "title": "Efficient Agent Training for Computer Use",
    "summary": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13909.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "663f5e959e6f865ec6d4fb62",
      "avatarUrl": "/avatars/eb0a908562b2c57335ae8bb949220430.svg",
      "fullname": "Jiahe Jin",
      "name": "zizi-0123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15045",
      "authors": [
        {
          "_id": "682e9672d28d9650c90db133",
          "user": {
            "_id": "638f1803c67af472d317a922",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638f1803c67af472d317a922/9BMVXqHa-AsdZPmBprcbd.jpeg",
            "isPro": false,
            "fullname": "siyue zhang",
            "user": "siyue",
            "type": "user"
          },
          "name": "Siyue Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:04.722Z",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db134",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:02.079Z",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db135",
          "user": {
            "_id": "65457e29bd25cef7d118122c",
            "avatarUrl": "/avatars/67777b3f4584b8ba92f12e95dbf93482.svg",
            "isPro": false,
            "fullname": "Liyuan Geng",
            "user": "LYGeng",
            "type": "user"
          },
          "name": "Liyuan Geng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:59.203Z",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db136",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db137",
          "name": "Anh Tuan Luu",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db138",
          "name": "Chen Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638f1803c67af472d317a922/2-30it6-2JiegJOHJEfOy.png"
      ],
      "publishedAt": "2025-05-21T02:59:14.000Z",
      "submittedOnDailyAt": "2025-05-22T01:48:05.988Z",
      "title": "확산 언어 모델 vs. 자동회귀 언어 모델: 언어 모델의 확산과 자동회귀 언어 모델의 비교 - 텍스트 삽입의 관점에서의 비교\n\n(注意: 原文中的“言語モデルの拡散と自動帰り言語モデルの比較 - 文字埋め込みの視点”在翻译时被理解为“언어 모델의 확산과 자동회귀 언어 모델의 비교 - 텍스트 삽입의 관점에서의 비교”，以保持原文的专业性和准确性。)",
      "submittedOnDailyBy": {
        "_id": "638f1803c67af472d317a922",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638f1803c67af472d317a922/9BMVXqHa-AsdZPmBprcbd.jpeg",
        "isPro": false,
        "fullname": "siyue zhang",
        "user": "siyue",
        "type": "user"
      },
      "summary": "대 언어 모델(LLM) 기반의 덧입기 모델은 대규모의 사전 학습과 후 학습을 통해 문서 검색과 같은 일반적인 텍스트 덧입기 작업에서 BERT나 T5 기반의 모델을 초월하게 되었습니다. 그러나 LLM 덧입기의 기본적인 한계는 사전 학습 시 자동 회귀를 위한 단방향 注意 기능이며, 이는 텍스트 덧입기 작업의 양방향 특성에 적절히 대응하지 못하는 점입니다. 이에 대해 우리는 단방향의 注意 기능과 다른 고유의 양방향의 아키텍처를 통해 최근의 성공을 보여주는 것을 모티브로, 덧입기 작업에 덧입기 언어 모델을 도입하는 것을 제안합니다. 우리는 초기의 체계적인 연구를 수행했으며, 긴 문서 검색에서 LLM 기반의 덧입기 모델보다 20%의 개선, 논리적인 검색에서 8%의 개선, 지시에 따라 검색에서 2%의 개선을 달성했으며, 전통적인 텍스트 덧입기 벤치마크에서 상대적인 성능을 보여주었습니다. 우리의 분석은 긴 문서나 복잡한 문서의 글로벌 컨텍스트의 덧입기에 양방향의 注意 기능의 중요성을 증명하고 있습니다.",
      "upvotes": 24,
      "discussionId": "682e9673d28d9650c90db154",
      "ai_summary": "Diffusion language models outperform large language model embeddings in text retrieval tasks due to their bidirectional architecture.",
      "ai_keywords": [
        "large language model (LLM)",
        "diffusion language models",
        "unidirectional attention",
        "bidirectional attention",
        "document retrieval",
        "reasoning-intensive retrieval",
        "instruction-following retrieval",
        "text embedding benchmarks"
      ]
    },
    "publishedAt": "2025-05-20T22:59:14.000Z",
    "title": "Diffusion vs. Autoregressive Language Models: A Text Embedding\n  Perspective",
    "summary": "Large language model (LLM)-based embedding models, benefiting from large\nscale pre-training and post-training, have begun to surpass BERT and T5-based\nmodels on general-purpose text embedding tasks such as document retrieval.\nHowever, a fundamental limitation of LLM embeddings lies in the unidirectional\nattention used during autoregressive pre-training, which misaligns with the\nbidirectional nature of text embedding tasks. To this end, We propose adopting\ndiffusion language models for text embeddings, motivated by their inherent\nbidirectional architecture and recent success in matching or surpassing LLMs\nespecially on reasoning tasks. We present the first systematic study of the\ndiffusion language embedding model, which outperforms the LLM-based embedding\nmodel by 20% on long-document retrieval, 8% on reasoning-intensive retrieval,\n2% on instruction-following retrieval, and achieve competitive performance on\ntraditional text embedding benchmarks. Our analysis verifies that bidirectional\nattention is crucial for encoding global context in long and complex text.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638f1803c67af472d317a922/2-30it6-2JiegJOHJEfOy.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15045.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638f1803c67af472d317a922",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638f1803c67af472d317a922/9BMVXqHa-AsdZPmBprcbd.jpeg",
      "fullname": "siyue zhang",
      "name": "siyue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15612",
      "authors": [
        {
          "_id": "682e97bf9c1b77a503087f4f",
          "user": {
            "_id": "6458af46f4d212d780bd7c68",
            "avatarUrl": "/avatars/832fd34bcc041b0b7b551873a459fc3c.svg",
            "isPro": false,
            "fullname": "Wei Liu",
            "user": "PeterV09",
            "type": "user"
          },
          "name": "Wei Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:57.023Z",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f50",
          "name": "Ruochen Zhou",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f51",
          "name": "Yiyun Deng",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f52",
          "name": "Yuzhen Huang",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f53",
          "name": "Junteng Liu",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f54",
          "user": {
            "_id": "63081e15a670ed10f9d44229",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
            "isPro": true,
            "fullname": "Yuntian Deng",
            "user": "yuntian-deng",
            "type": "user"
          },
          "name": "Yuntian Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:54.170Z",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f55",
          "name": "Yizhe Zhang",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f56",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T15:03:26.000Z",
      "submittedOnDailyAt": "2025-05-22T01:49:54.677Z",
      "title": "학습, 길이에 따라 대응보정 학습을 적용하며 효율적으로 이유를 설명하는 방법을 배워라.",
      "submittedOnDailyBy": {
        "_id": "6458af46f4d212d780bd7c68",
        "avatarUrl": "/avatars/832fd34bcc041b0b7b551873a459fc3c.svg",
        "isPro": false,
        "fullname": "Wei Liu",
        "user": "PeterV09",
        "type": "user"
      },
      "summary": "대논리 모델（LRMs）는 강화학습（RL）을 통해 복잡한 문제를 해결하는 데 놀라운 능력을 보여주고 있습니다. 특히, 긴 논리 트래스 생성에 있어서. 그러나, 이러한 확장된 출력은 더 큰 불필요한 정보로 가득 차 있으며, LRMs의 효율성을 제한하고 있습니다. 본 논문에서는, RL 기반의 접근법을 통해 논리의 효율화를 촉진하는 방법을 조사합니다. 특히, 길이 기반의 보상 씹핑을 통해, 여러 효율적인 논리 방법을 공식화하는 통일된 프레임워크를 제안합니다. 이 프레임워크에 따라, 길이 기반 스텝 보상 씹핑 방법（LASER）을 제안합니다. LASER는 스텝 함수를 보상으로 하고, 목표 길이에 의해 제어됩니다. LASER는 이전의 방법을 초과하여, 성능과 효율의 팍로어피티 모뎀의 우수한 균형을 실현합니다. 다음으로, LASER는 두 가지 중요한 직관에 기반하여 발전시키며: （1）모델의 논리 행동은 학습 중 진화하며 보상의 규정도 적응적이고 동적으로 필요합니다; （2） 짧은 또는 긴 사고 연속（CoT）을 일관적으로 장려하는 것이 아니라, 길이 기반의 보상 씹핑은 어려움에 대한 것이 되어, 간단한 질문에서 긴 CoT을 더 엄격하게 평가하는 것이 필요합니다. 이 접근법은 빠른 및 느린 사고의 조합을 촉진하고 전체적인 트레이드오프를 개선하는 것을 기대합니다. 그 결과, LASER-D（동적 및 어려움에 대한）라고 불립니다. DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-32B에 대한 실험은, 우리 접근법이 논리 성능과 답변 길이의 효율성을 크게 향상시키는 것을 보여주었습니다. 예를 들어, LASER-D와 그의 변체는 AIME2024에서 +6.1의 개선을 거두며, 토큰 사용량을 63% 줄였습니다. 진한 분석은, 우리 RL 기반의 압축이 불필요한 「자각」을 줄이는 대신, 간결한 논리 패턴을 생성하는 것을 보여줍니다. 자원은, https://github.com/hkust-nlp/Laser 에 있습니다.",
      "upvotes": 20,
      "discussionId": "682e97bf9c1b77a503087f81",
      "githubRepo": "https://github.com/hkust-nlp/Laser",
      "ai_summary": "RL-based reward shaping methods, particularly LASER-D, enhance reasoning efficiency and performance in large reasoning models by dynamically adapting to difficulty and reducing redundancy.",
      "ai_keywords": [
        "reinforcement learning",
        "long reasoning traces",
        "length-based reward shaping",
        "LASER",
        "LASER-D",
        "reasoning behavior",
        "target length",
        "step function",
        "adaptive",
        "dynamic",
        "difficulty-aware",
        "chains of thought",
        "self-reflections"
      ]
    },
    "publishedAt": "2025-05-21T11:03:26.000Z",
    "title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping",
    "summary": "Large Reasoning Models (LRMs) have shown remarkable capabilities in solving\ncomplex problems through reinforcement learning (RL), particularly by\ngenerating long reasoning traces. However, these extended outputs often exhibit\nsubstantial redundancy, which limits the efficiency of LRMs. In this paper, we\ninvestigate RL-based approaches to promote reasoning efficiency. Specifically,\nwe first present a unified framework that formulates various efficient\nreasoning methods through the lens of length-based reward shaping. Building on\nthis perspective, we propose a novel Length-bAsed StEp Reward shaping method\n(LASER), which employs a step function as the reward, controlled by a target\nlength. LASER surpasses previous methods, achieving a superior Pareto-optimal\nbalance between performance and efficiency. Next, we further extend LASER based\non two key intuitions: (1) The reasoning behavior of the model evolves during\ntraining, necessitating reward specifications that are also adaptive and\ndynamic; (2) Rather than uniformly encouraging shorter or longer chains of\nthought (CoT), we posit that length-based reward shaping should be\ndifficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.\nThis approach is expected to facilitate a combination of fast and slow\nthinking, leading to a better overall tradeoff. The resulting method is termed\nLASER-D (Dynamic and Difficulty-aware). Experiments on\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and\nDeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both\nreasoning performance and response length efficiency. For instance, LASER-D and\nits variant achieve a +6.1 improvement on AIME2024 while reducing token usage\nby 63%. Further analysis reveals our RL-based compression produces more concise\nreasoning patterns with less redundant \"self-reflections\". Resources are at\nhttps://github.com/hkust-nlp/Laser.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15612.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6458af46f4d212d780bd7c68",
      "avatarUrl": "/avatars/832fd34bcc041b0b7b551873a459fc3c.svg",
      "fullname": "Wei Liu",
      "name": "PeterV09",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15400",
      "authors": [
        {
          "_id": "682ec5f6b16c79c271c8559e",
          "user": {
            "_id": "67f33b43ccb05db5f0190cf6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/c-UeSyHfO7QchwNXW9mgu.png",
            "isPro": false,
            "fullname": "ZhangXiaoyun",
            "user": "DadaCloud01",
            "type": "user"
          },
          "name": "Xiaoyun Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:16.038Z",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c8559f",
          "user": {
            "_id": "64872abb3fb9bc8f4994e014",
            "avatarUrl": "/avatars/9743b5a2b6f9b9990aa1cb06bd6eb5c6.svg",
            "isPro": false,
            "fullname": "Jingqing Ruan",
            "user": "Amanda2023",
            "type": "user"
          },
          "name": "Jingqing Ruan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:09.457Z",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a0",
          "user": {
            "_id": "663bc46ae14047f71026c2b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663bc46ae14047f71026c2b3/lo4zFH0aIkFUQSNuzJW4I.jpeg",
            "isPro": false,
            "fullname": "Maxing",
            "user": "Machine981",
            "type": "user"
          },
          "name": "Xing Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:12.313Z",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a1",
          "user": {
            "_id": "682ecaa39dd5ef944657bc16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/MGiOAd4-_0HpBfmXWsNmY.png",
            "isPro": false,
            "fullname": "ZHUYAWEN",
            "user": "Yaawennn",
            "type": "user"
          },
          "name": "Yawen Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T08:47:18.660Z",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a2",
          "name": "Haodong Zhao",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a3",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a4",
          "name": "Jiansong Chen",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a5",
          "name": "Ke Zeng",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a6",
          "name": "Xunliang Cai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T11:41:39.000Z",
      "submittedOnDailyAt": "2025-05-22T05:06:49.326Z",
      "title": "어느 때로 기억할 수 있는 적절한 시기: 효율적인 이유에 대한 적응적인 기억 시뮬레이션의 모드 전환\n\n이 번역은 전문성과 정확성을 유지하였습니다.",
      "submittedOnDailyBy": {
        "_id": "64872abb3fb9bc8f4994e014",
        "avatarUrl": "/avatars/9743b5a2b6f9b9990aa1cb06bd6eb5c6.svg",
        "isPro": false,
        "fullname": "Jingqing Ruan",
        "user": "Amanda2023",
        "type": "user"
      },
      "summary": "대논리 모형（LRMs）는 긴 논리 체인에서 놀라운 성능을 달성하지만, 특히 간단한 태스크에서 불필요한 논리로 인한 과도한 계산 오버헤드가 발생합니다. 본 논문에서는 LRMs의 Long-Thinking와 No-Thinking 모우드를 모두 시스템적으로 평가하고, \"내부 자기복원 기능\" 현상을 밝혀냅니다. 이 관점에서, 적응적 자기복원 논리（ASRR） 프레임워크를 제안하여 불필요한 논리를 억제하고 잠재적인 복원을 가능하게 합니다. 정확도에 대한 길이 상금 조정을 도입하여, ASRR은 문제의 난이도에 따라 논리의 노력을 적절하게 분배하고, 성능 부담을 최소화하면서 높은 효율성을 달성합니다. 많은 벤치마크와 모델의 실험 결과를 통해, GRPO와 비교하여, ASRR은 최대 32.5%（1.5B） 또는 25.7%（7B）의 논리 영역을 줄였으며, 정확도 손실은 최소화（pass@1의 1.2%와 0.6%）, 안전 벤치마크에서 무害율을 크게 향상시켰습니다（최대 +21.7%）. 우리의 결과는 LRM에서 효율적이고 적응적이고 더 안전한 논리를 가능하게 하는 ASRR의 잠재력을 밝혀냅니다.",
      "upvotes": 15,
      "discussionId": "682ec5f7b16c79c271c855df",
      "ai_summary": "ASRR framework optimizes reasoning efficiency in large models by suppressing redundant information processing without significantly impacting performance or safety.",
      "ai_keywords": [
        "Large reasoning models",
        "LRMs",
        "Long-Thinking modes",
        "No-Thinking modes",
        "Internal Self-Recovery Mechanism",
        "Adaptive Self-Recovery Reasoning",
        "accuracy-aware length reward regulation",
        "reasoning budget",
        "pass@1",
        "harmless rates",
        "safety benchmarks",
        "efficiency",
        "adaptive reasoning",
        "safety"
      ]
    },
    "publishedAt": "2025-05-21T07:41:39.000Z",
    "title": "When to Continue Thinking: Adaptive Thinking Mode Switching for\n  Efficient Reasoning",
    "summary": "Large reasoning models (LRMs) achieve remarkable performance via long\nreasoning chains, but often incur excessive computational overhead due to\nredundant reasoning, especially on simple tasks. In this work, we\nsystematically quantify the upper bounds of LRMs under both Long-Thinking and\nNo-Thinking modes, and uncover the phenomenon of \"Internal Self-Recovery\nMechanism\" where models implicitly supplement reasoning during answer\ngeneration. Building on this insight, we propose Adaptive Self-Recovery\nReasoning (ASRR), a framework that suppresses unnecessary reasoning and enables\nimplicit recovery. By introducing accuracy-aware length reward regulation, ASRR\nadaptively allocates reasoning effort according to problem difficulty,\nachieving high efficiency with negligible performance sacrifice. Experiments\nacross multiple benchmarks and models show that, compared with GRPO, ASRR\nreduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal\naccuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates\non safety benchmarks (up to +21.7%). Our results highlight the potential of\nASRR for enabling efficient, adaptive, and safer reasoning in LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15400.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64872abb3fb9bc8f4994e014",
      "avatarUrl": "/avatars/9743b5a2b6f9b9990aa1cb06bd6eb5c6.svg",
      "fullname": "Jingqing Ruan",
      "name": "Amanda2023",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15210",
      "authors": [
        {
          "_id": "682e84d83291b134b3370184",
          "name": "Jie Ma",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370185",
          "user": {
            "_id": "653d0136d6f7982a7a52054d",
            "avatarUrl": "/avatars/5d47d022dd5954a4c3d243a7f0292d32.svg",
            "isPro": false,
            "fullname": "QUNING",
            "user": "stillqu",
            "type": "user"
          },
          "name": "Ning Qu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:55.205Z",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370186",
          "name": "Zhitao Gao",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370187",
          "name": "Rui Xing",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370188",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370189",
          "name": "Hongbin Pei",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018a",
          "name": "Jiang Xie",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018b",
          "name": "Linyun Song",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018c",
          "name": "Pinghui Wang",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018d",
          "name": "Jing Tao",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018e",
          "name": "Zhou Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T07:38:45.000Z",
      "submittedOnDailyAt": "2025-05-22T00:31:54.508Z",
      "title": "「처음의 신뢰성 검토: 지식 그래프의 대규모 언어 모델의 신뢰할 수 있는 이유의 정당성」",
      "submittedOnDailyBy": {
        "_id": "67a7099286a55d5569acb213",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rYISkvTtyraUdbgSsNfpC.png",
        "isPro": false,
        "fullname": "JieMa",
        "user": "JamesMile",
        "type": "user"
      },
      "summary": "知識 그래프 기반의 검색 증강 생성은 대규모 언어 모델(LLMs)에 의한 지식 부족 및 정보의 만료로 인한 혼란을 줄이는 것을 목표로 하고 있습니다. 그러나 현재의 방법들은 지식 그래프(KGs)에 포함되는 놀라운 지식들을 충분히 활용할 수 없으며, 특히 구조적 정보나 명시적 또는 은닉한 제약에 대해如此。 이러한 제약들은 LLMs의 이유의 정확성을 높일 수 있으며, 응답 생성의 신뢰성을 향상시킬 수 있습니다. 이러한 점을 목표로, 우리는 신뢰할 수 있는 이유의 틀과 \"놀라운 것을 초월하여 검토하는\" (DP)라는 이름을 붙인, KGs에 포함되는 놀라운 것을 충분히 활용하는 프레임워크를 제안합니다. 구체적으로는, DP는 지도 학습과 Kahneman-Tversky 최적화의 조합으로 구조적인 놀라운 것을 LLMs에 통합하는 발전적인 지식 훈련 전략을 채택하고, 관계 패스의 정확성을 향상시킵니다. 또한, 우리의 프레임워크는 추출된 제약 놀라운에 기반한 검증된 이유의 반성 전략을 사용하여 응답 생성의 신뢰성을 보장합니다. 3개의 벤치마크 데이터셋에서 확장된 실험에 따라, DP는 새로운 최단거리 성능을 달성했으며, 특히 ComplexWebQuestions 데이터셋에서 Hit@1의 13%의 향상을 달성하며, 고信任도의 응답을 생성합니다. 또한 다양한 분석을 수행하여, 그 유연성과 실용성을 확인했습니다. 코드는 https://github.com/reml-group/Deliberation-on-Priors에 접근할 수 있습니다.",
      "upvotes": 13,
      "discussionId": "682e84d93291b134b33701cb",
      "githubRepo": "https://github.com/reml-group/Deliberation-on-Priors",
      "ai_summary": "The Deliberation over Priors framework enhances the trustworthiness of LLMs by integrating structural and constraint priors from knowledge graphs through knowledge distillation and reasoning introspection.",
      "ai_keywords": [
        "knowledge graph-based retrieval-augmented generation",
        "Large Language Models (LLMs)",
        "knowledge graphs (KGs)",
        "structural information",
        "explicit constraints",
        "implicit constraints",
        "trustworthworthy reasoning framework",
        "Deliberation over Priors (DP)",
        "progressive knowledge distillation",
        "supervised fine-tuning",
        "Kahneman-Tversky optimization",
        "relation path generation",
        "reasoning-introspection strategy",
        "ComplexWebQuestions dataset",
        "Hit@1 improvement"
      ]
    },
    "publishedAt": "2025-05-21T03:38:45.000Z",
    "title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models\n  on Knowledge Graphs",
    "summary": "Knowledge graph-based retrieval-augmented generation seeks to mitigate\nhallucinations in Large Language Models (LLMs) caused by insufficient or\noutdated knowledge. However, existing methods often fail to fully exploit the\nprior knowledge embedded in knowledge graphs (KGs), particularly their\nstructural information and explicit or implicit constraints. The former can\nenhance the faithfulness of LLMs' reasoning, while the latter can improve the\nreliability of response generation. Motivated by these, we propose a\ntrustworthy reasoning framework, termed Deliberation over Priors (DP), which\nsufficiently utilizes the priors contained in KGs. Specifically, DP adopts a\nprogressive knowledge distillation strategy that integrates structural priors\ninto LLMs through a combination of supervised fine-tuning and Kahneman-Tversky\noptimization, thereby improving the faithfulness of relation path generation.\nFurthermore, our framework employs a reasoning-introspection strategy, which\nguides LLMs to perform refined reasoning verification based on extracted\nconstraint priors, ensuring the reliability of response generation. Extensive\nexperiments on three benchmark datasets demonstrate that DP achieves new\nstate-of-the-art performance, especially a Hit@1 improvement of 13% on the\nComplexWebQuestions dataset, and generates highly trustworthy responses. We\nalso conduct various analyses to verify its flexibility and practicality. The\ncode is available at https://github.com/reml-group/Deliberation-on-Priors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15210.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a7099286a55d5569acb213",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rYISkvTtyraUdbgSsNfpC.png",
      "fullname": "JieMa",
      "name": "JamesMile",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15779",
      "authors": [
        {
          "_id": "682e9d84a0db46260ccc15e1",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e2",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e3",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e4",
          "name": "Mingliang Zhai",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e5",
          "name": "Yifan Chang",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e6",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:31:49.000Z",
      "submittedOnDailyAt": "2025-05-22T02:37:20.764Z",
      "title": "IA-T2I: 인터넷 어쩌구 텍스트 투 이라이즈 생성",
      "submittedOnDailyBy": {
        "_id": "6794cd79b72b1721ea69f4f2",
        "avatarUrl": "/avatars/4e4fb9e9e127a0c031131ace705687cd.svg",
        "isPro": false,
        "fullname": "Ming Li",
        "user": "afdsafas",
        "type": "user"
      },
      "summary": "현재의 텍스트에서 이미지 생성(T2I) 모델은 안정적인 성과를 거뒀지만, 문맥에 포함된 지식이 불확실할 때 문제가 발생합니다. 예를 들어, 2월에 릴리즈된 T2I 모델은 4월에 상영할 영화의 적절한 포스터를 생성하는 데 어려움을 겪습니다. 이는 모델이 캐릭터 디자인이나 스타일이 불확실하다는 이유입니다. 이러한 문제를 해결하기 위해, 우리는 텍스트에서 이미지 생성(IA-T2I) 프레임워크를 제안합니다. 이 프레임워크는 문맥에 포함된 불확실한 지식을 명확히 하기 위해, 텍스트 프로ン톰에 기반하여 참조 이미지를 제공하여 T2I 모델을 강화합니다. 특히, 활성 검색 모듈은 문맥에 따라 참조 이미지가 필요할지 결정하고, 계층적인 이미지 선택 모듈은 이미지 검색 엔진에서 반환되는 최적의 이미지를 감지하고, 반사 모듈은 생성된 이미지를 지속적으로 평가하여 텍스트 프로ン톰에 충실하게 개선합니다. 프레임워크의 성능을 평가하기 위해, 텍스트 프로ン톰에는 3가지 불확실한 지식을 포함하는 데이터 세트「Img-Ref-T2I」를 준비했습니다. 이 데이터 세트는 (1) 알고 있지만 희귀한 것, (2) 모르는 것, (3) 불확실한 것입니다. 또한, 복잡한 프로ン톰을 생성하고, GPT-4o를 사용하여 선호 평가를 수행하였으며, 이는 인간 선호 평가의 정확도와 유사한 정확도를 나타냅니다. 실험 결과를 통해, 우리의 프레임워크의 효과와 GPT-4o의 인간 평가에서 약 30% 이상 높은 성능을 보여주었습니다.",
      "upvotes": 12,
      "discussionId": "682e9d85a0db46260ccc161f",
      "ai_summary": "An Internet-Augmented text-to-image generation framework improves uncertain text prompt handling by integrating reference images, enhancing image quality and fidelity.",
      "ai_keywords": [
        "text-to-image generation",
        "IA-T2I framework",
        "reference images",
        "active retrieval module",
        "hierarchical image selection module",
        "self-reflection mechanism",
        "Img-Ref-T2I dataset",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-05-21T13:31:49.000Z",
    "title": "IA-T2I: Internet-Augmented Text-to-Image Generation",
    "summary": "Current text-to-image (T2I) generation models achieve promising results, but\nthey fail on the scenarios where the knowledge implied in the text prompt is\nuncertain. For example, a T2I model released in February would struggle to\ngenerate a suitable poster for a movie premiering in April, because the\ncharacter designs and styles are uncertain to the model. To solve this problem,\nwe propose an Internet-Augmented text-to-image generation (IA-T2I) framework to\ncompel T2I models clear about such uncertain knowledge by providing them with\nreference images. Specifically, an active retrieval module is designed to\ndetermine whether a reference image is needed based on the given text prompt; a\nhierarchical image selection module is introduced to find the most suitable\nimage returned by an image search engine to enhance the T2I model; a\nself-reflection mechanism is presented to continuously evaluate and refine the\ngenerated image to ensure faithful alignment with the text prompt. To evaluate\nthe proposed framework's performance, we collect a dataset named Img-Ref-T2I,\nwhere text prompts include three types of uncertain knowledge: (1) known but\nrare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt\nto guide GPT-4o in making preference evaluation, which has been shown to have\nan evaluation accuracy similar to that of human preference evaluation.\nExperimental results demonstrate the effectiveness of our framework,\noutperforming GPT-4o by about 30% in human evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15779.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6794cd79b72b1721ea69f4f2",
      "avatarUrl": "/avatars/4e4fb9e9e127a0c031131ace705687cd.svg",
      "fullname": "Ming Li",
      "name": "afdsafas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14357",
      "authors": [
        {
          "_id": "682d2b9ba006b93dbe79a333",
          "user": {
            "_id": "66b06fe4ce2224a4d066cc0a",
            "avatarUrl": "/avatars/3f89007c5c17e57e623a10d82637b5fc.svg",
            "isPro": false,
            "fullname": "knightnemo",
            "user": "knightnemo",
            "type": "user"
          },
          "name": "Siqiao Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T18:04:10.245Z",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a334",
          "user": {
            "_id": "643b866bff50448bcfc7d1d1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "manchery",
            "type": "user"
          },
          "name": "Jialong Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-22T05:44:25.954Z",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a335",
          "name": "Qixing Zhou",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a336",
          "name": "Shangchen Miao",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a337",
          "name": "Mingsheng Long",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T13:41:45.000Z",
      "submittedOnDailyAt": "2025-05-22T00:17:33.386Z",
      "title": "Vid2World: 비디오 디피유션 모뎀을 제작하고 상호작용 세계 모뎀에 대해 이야기합니다.",
      "submittedOnDailyBy": {
        "_id": "66b06fe4ce2224a4d066cc0a",
        "avatarUrl": "/avatars/3f89007c5c17e57e623a10d82637b5fc.svg",
        "isPro": false,
        "fullname": "knightnemo",
        "user": "knightnemo",
        "type": "user"
      },
      "summary": "세계 모델은 역사 관찰과 행동 시퀀스를 기반으로 예측을 수행하며, 순차적 결정의 데이터 효율 향상에 큰 기대를 보여줍니다. 그러나 현재의 세계 모델은 엄격한 도메인 전문적인 훈련이 필요하고, 낮은 품질의 간략한 예측을 출력하며, 복잡한 환경에서의 적용성을 제한하고 있습니다. 대규모 인터넷 데이터셋으로 훈련된 비디오 디퓨전 모델은 다양한 리알 웨어의 동작을 감지하는 고품질의 비디오의 생성에 뛰어난 능력을 보여주고 있습니다. 본 논문에서는 Vid2World라는 일반적인 접근 방식을 제시하고, 비디오 디퓨전 모델을 상호작용 모델에 활용하여 전달합니다. 빈도를 메우기 위해, Vid2World는 자동 회귀적인 생성을 가능하게 하는 비디오 디퓨전 모델의 아키텍처와 훈련 목적을 생성하고, 원인화를 수행합니다. 또한, 원인화 행동 가이드 구조를 도입하고, 상호작용 모델의 행동 제어 가능도를 향상시킵니다. 기계어 조작과 게임 시뮬레이션 분야에서 확장된 실험은 우리의 방법이 고성능의 비디오 디퓨전 모델을 상호작용 모델에 재활용하는 손실 적인 효과적인 접근 방식을 제공함을 보여줍니다.",
      "upvotes": 12,
      "discussionId": "682d2b9da006b93dbe79a3af",
      "projectPage": "https://knightnemo.github.io/vid2world/",
      "ai_summary": "Vid2World repurposes pre-trained video diffusion models into interactive world models via causalization and action guidance, enhancing action controllability and scalability in complex environments.",
      "ai_keywords": [
        "world models",
        "transitions",
        "history observation",
        "action sequences",
        "data efficiency",
        "sequential decision making",
        "low-fidelity",
        "coarse predictions",
        "video diffusion models",
        "internet-scale datasets",
        "high-quality videos",
        "real-world dynamics",
        "autoregressive generation",
        "causal action guidance",
        "robot manipulation",
        "game simulation"
      ]
    },
    "publishedAt": "2025-05-20T09:41:45.000Z",
    "title": "Vid2World: Crafting Video Diffusion Models to Interactive World Models",
    "summary": "World models, which predict transitions based on history observation and\naction sequences, have shown great promise in improving data efficiency for\nsequential decision making. However, existing world models often require\nextensive domain-specific training and still produce low-fidelity, coarse\npredictions, limiting their applicability in complex environments. In contrast,\nvideo diffusion models trained on large, internet-scale datasets have\ndemonstrated impressive capabilities in generating high-quality videos that\ncapture diverse real-world dynamics. In this work, we present Vid2World, a\ngeneral approach for leveraging and transferring pre-trained video diffusion\nmodels into interactive world models. To bridge the gap, Vid2World performs\ncasualization of a pre-trained video diffusion model by crafting its\narchitecture and training objective to enable autoregressive generation.\nFurthermore, it introduces a causal action guidance mechanism to enhance action\ncontrollability in the resulting interactive world model. Extensive experiments\nin robot manipulation and game simulation domains show that our method offers a\nscalable and effective approach for repurposing highly capable video diffusion\nmodels to interactive world models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14357.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b06fe4ce2224a4d066cc0a",
      "avatarUrl": "/avatars/3f89007c5c17e57e623a10d82637b5fc.svg",
      "fullname": "knightnemo",
      "name": "knightnemo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15146",
      "authors": [
        {
          "_id": "682e980fb16c79c271babcbd",
          "user": {
            "_id": "6301d6455e305a35cb0846a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
            "isPro": true,
            "fullname": "Lanxiang Hu",
            "user": "Snyhlxde",
            "type": "user"
          },
          "name": "Lanxiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:51.281Z",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcbe",
          "name": "Mingjia Huo",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcbf",
          "user": {
            "_id": "653200c5d0f5a9e537e76695",
            "avatarUrl": "/avatars/d1beb984d91908c03e0daeb77589a475.svg",
            "isPro": false,
            "fullname": "Yuxuan Zhang",
            "user": "Yuxuan13",
            "type": "user"
          },
          "name": "Yuxuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:48.879Z",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc0",
          "name": "Haoyang Yu",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc1",
          "name": "Eric P. Xing",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc2",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc3",
          "name": "Tajana Rosing",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc4",
          "name": "Haojian Jin",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc5",
          "name": "Hao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T06:02:55.000Z",
      "submittedOnDailyAt": "2025-05-22T01:52:33.947Z",
      "title": "lmgame-Bench: 게임을 플레이하는 LLM의 실력은 얼마나 되나요?",
      "submittedOnDailyBy": {
        "_id": "6301d6455e305a35cb0846a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
        "isPro": true,
        "fullname": "Lanxiang Hu",
        "user": "Snyhlxde",
        "type": "user"
      },
      "summary": "게임을 실행하는 데는 시각적 인식, 기억력, 계획 능력이 필요합니다. 이러한 능력은 현대의 대규모 언어 모델(LLM) 에이전트가 이해해야 할 능력입니다. LLM을 게임에 직접 적용하여 효과적인 평가를 수행할 수 있는지 연구하고, LLM을 게임에 직접 적용하여 효과적인 평가를 수행할 수 없다고 발견했습니다. 이유는 3가지로 나옵니다: 취약한 시각적 인식, 프로ンプト의 민감성, 데이터의 컨텍스트화. lmgame-Bench를 소개합니다. lmgame-Bench는 플랫폼 게임, 퍼즐 게임, 나로틱 게임을 제공하며, Gym 스타일 API를 사용하며, 가벼운 시각적 인식과 기억력 표준을 결합하여 프로ンプト의 불확실성을 안정화하고 컨텍스트화 문제를 해결합니다. 13가지의 첨단 모델을 포함하여, lmgame-Bench는 도전적이며, 모델을 구분할 수 있습니다. 상관 분석에 따르면 각 게임은 고유한 능력의 조합을 검증하며, 각 게임에서 다른 방식으로 검증됩니다. 흥미로운 점은 lmgame-Bench의 한 게임에서 강화학습을 수행하면, 새로운 게임으로의 전파와 외부 계획 태스크로의 전파가 가능합니다. 평가 코드는 https://github.com/lmgame-org/GamingAgent/lmgame-bench에 공개되어 있습니다.",
      "upvotes": 11,
      "discussionId": "682e9810b16c79c271babd61",
      "projectPage": "https://lmgame.org/",
      "ai_summary": "lmgame-Bench evaluates large language models using games with diverse challenges, demonstrating unique capability blends and transfer learning potential.",
      "ai_keywords": [
        "LLMs",
        "large language model agents",
        "brittle vision perception",
        "prompt sensitivity",
        "data contamination",
        "Gym-style API",
        "perception scaffolds",
        "memory scaffolds",
        "reinforcement learning",
        "transfer learning"
      ]
    },
    "publishedAt": "2025-05-21T02:02:55.000Z",
    "title": "lmgame-Bench: How Good are LLMs at Playing Games?",
    "summary": "Playing video games requires perception, memory, and planning, exactly the\nfaculties modern large language model (LLM) agents are expected to master. We\nstudy the major challenges in using popular video games to evaluate modern LLMs\nand find that directly dropping LLMs into games cannot make an effective\nevaluation, for three reasons -- brittle vision perception, prompt sensitivity,\nand potential data contamination. We introduce lmgame-Bench to turn games into\nreliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and\nnarrative games delivered through a unified Gym-style API and paired with\nlightweight perception and memory scaffolds, and is designed to stabilize\nprompt variance and remove contamination. Across 13 leading models, we show\nlmgame-Bench is challenging while still separating models well. Correlation\nanalysis shows that every game probes a unique blend of capabilities often\ntested in isolation elsewhere. More interestingly, performing reinforcement\nlearning on a single game from lmgame-Bench transfers both to unseen games and\nto external planning tasks. Our evaluation code is available at\nhttps://github.com/lmgame-org/GamingAgent/lmgame-bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15146.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6301d6455e305a35cb0846a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
      "fullname": "Lanxiang Hu",
      "name": "Snyhlxde",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15765",
      "authors": [
        {
          "_id": "682e85aa7b41e70cf11758b6",
          "name": "Kaizhi Zheng",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758b7",
          "name": "Ruijian Zhang",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758b8",
          "name": "Jing Gu",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758b9",
          "name": "Jie Yang",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758ba",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/LlIYiX21uaVLjqWu6lAQI.mp4"
      ],
      "publishedAt": "2025-05-21T17:10:47.000Z",
      "submittedOnDailyAt": "2025-05-22T00:36:41.981Z",
      "title": "3D 마을은 한 장의 사진만으로 만들 수 있습니다.",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "3DTown는 단일 위상 이미지로부터 현실적이고 일관된 3D 풍경을 합성하기 위해 훈련없이 설계된 프레임워크입니다. 우리의 방법는 두 가지 원칙에 기반합니다: 지역 기반 생성은 이미지에서 3D의 일치와 해상도를 향상시키고, 공간 인지 3D inpainting은 전체 풍경의 일관성과 고품질의 3D 생성을 보장합니다. 입력 이미지를 겹치는 지역으로 분해하고, 각 지역은 사전 훈련된 3D 물체 생성기로 생성되며, 이후 마스킹된 정렬된 흐름 inpainting 프로세스를 통해 결측의 3D 구조를 채우되 구조적 연속성을 유지합니다. 이 모듈 설계는 해상도 장애를 극복하고 공간 구조를 보존할 수 있으며, 3D 감독 또는 미세 조정이 필요하지 않습니다. 다양한 풍경에서 광범위한 실험을 통해 3DTown은 기하학적 질, 공간 일관성, 그리고 테xture의 일관성을 포함하여 최신 기준과 비교하여 우수한 성능을 보입니다. 우리의 결과는 단일 이미지에서 원칙적이고 훈련없이 고품질의 3D 도시 생성이 가능함을 입증합니다.",
      "upvotes": 9,
      "discussionId": "682e85b17b41e70cf1175aa0",
      "ai_summary": "A training-free framework named 3DTown generates realistic 3D scenes from a single top-down image using region-based generation and spatial-aware 3D inpainting techniques.",
      "ai_keywords": [
        "3D generative models",
        "3DTown",
        "region-based generation",
        "spatial-aware 3D inpainting",
        "masked rectified flow",
        "Trellis",
        "Hunyuan3D-2",
        "TripoSG"
      ]
    },
    "publishedAt": "2025-05-21T13:10:47.000Z",
    "title": "Constructing a 3D Town from a Single Image",
    "summary": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view\ndata, or labor-intensive modeling. Therefore, a lightweight alternative,\ngenerating complex 3D scenes from a single top-down image, plays an essential\nrole in real-world applications. While recent 3D generative models have\nachieved remarkable results at the object level, their extension to full-scene\ngeneration often leads to inconsistent geometry, layout hallucinations, and\nlow-quality meshes. In this work, we introduce 3DTown, a training-free\nframework designed to synthesize realistic and coherent 3D scenes from a single\ntop-down view. Our method is grounded in two principles: region-based\ngeneration to improve image-to-3D alignment and resolution, and spatial-aware\n3D inpainting to ensure global scene coherence and high-quality geometry\ngeneration. Specifically, we decompose the input image into overlapping regions\nand generate each using a pretrained 3D object generator, followed by a masked\nrectified flow inpainting process that fills in missing geometry while\nmaintaining structural continuity. This modular design allows us to overcome\nresolution bottlenecks and preserve spatial structure without requiring 3D\nsupervision or fine-tuning. Extensive experiments across diverse scenes show\nthat 3DTown outperforms state-of-the-art baselines, including Trellis,\nHunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and\ntexture fidelity. Our results demonstrate that high-quality 3D town generation\nis achievable from a single image using a principled, training-free approach.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/LlIYiX21uaVLjqWu6lAQI.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15765.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15404",
      "authors": [
        {
          "_id": "682e8b3de3d1137730d0517b",
          "name": "Zhexin Zhang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517c",
          "name": "Xian Qi Loye",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517d",
          "name": "Victor Shea-Jay Huang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517e",
          "user": {
            "_id": "65d859a3661492b25c46a117",
            "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
            "isPro": false,
            "fullname": "junxiao yang",
            "user": "yangjunxiao2021",
            "type": "user"
          },
          "name": "Junxiao Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:24.874Z",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517f",
          "name": "Qi Zhu",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05180",
          "name": "Shiyao Cui",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05181",
          "name": "Fei Mi",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05182",
          "name": "Lifeng Shang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05183",
          "name": "Yingkang Wang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05184",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05185",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T11:45:29.000Z",
      "submittedOnDailyAt": "2025-05-22T00:57:01.326Z",
      "title": "어떤 방식으로 대규모 논리 추론 모델의 안전성을 향상시킬 수 있는지: 실험적인 연구",
      "submittedOnDailyBy": {
        "_id": "61b58aa0d65058ce70beb98c",
        "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
        "isPro": false,
        "fullname": "Zhexin Zhang",
        "user": "nonstopfor",
        "type": "user"
      },
      "summary": "대논리 모형（LRMs）는 수학이나 프로그래밍 등 논리적인 문제를 해결하는 데에 놀라운 성공을 거두고 있습니다. 그러나 이러한 향상된 논리 능력은 안전성 향상에 반드시 대응하지는 않습니다. 또한 몇몇 경우에는 안전성을 악화시킬 수 있습니다. 이로 인해 중요한 연구 과제가 생겼습니다: LRMs의 안전성을 향상시키는 방법은 무엇일까요? 본 논문에서는 Supervised Fine-Tuning（SFT）을 통해 LRMs의 안전성을 향상시키는 방법에 대한 상세한 실험 연구를 수행합니다. 우리의 조사는 DeepSeek-R1에서 직접 안전한 답변을 도출하는 것이 안전성의 큰 향상에 반드시 대응하지는 않는다는 놀라운 것을 발견했습니다. 이 현상을 분석하여 이 원인을 3가지의 하이브리드 형식으로 특징적으로 식별했습니다. 그 후, 데이터 디스틸 프로세스에서 이러한 문제를 명시적으로 해결하는 것이 안전성의 큰 향상에 연결되는 것을 보여주었습니다. 다음으로, 긴 또는 복잡한 논리 프로세스가 안전성 달성에 필요할 수 있는지 조사했습니다. 흥미롭게도, 짧은 또는 템플릿 기반의 논리 프로세스로 비교적 안전성 성능을 달성할 수 있으며, 더 복잡한 논리 체인보다 모델이 학습하는 데에 더욱 간단합니다. 이러한 발견은 논리가 안전성을 보장하는 역할을 더욱 깊게 생각하도록 촉발합니다. 마지막으로, 안전성 미세 조정에 Magic Data를 섞는 것이 안전성과 과도한 거부 사이의 균형을 이루는 데 도움을 줍니다. 전체적으로, 우리의 실험 연구는 LRMs의 안전성을 향상시키는 방법에 대해 더 전도적인 시각을 제공하려고 합니다. 본 논문에서 사용된 코드와 데이터는 https://github.com/thu-coai/LRM-Safety-Study에 공개되어 있습니다.",
      "upvotes": 8,
      "discussionId": "682e8b3fe3d1137730d051f5",
      "ai_summary": "The study investigates methods to enhance the safety of Large Reasoning Models (LRMs) through Supervised Fine-Tuning (SFT), finding that explicit addressing of failure patterns and use of simpler reasoning processes can improve safety without requiring complex reasoning chains or excessive data.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRMs",
        "Supervised Fine-Tuning",
        "SFT",
        "DeepSeek-R1",
        "data distillation",
        "reasoning process",
        "safety improvements",
        "over-refusal",
        "math reasoning"
      ]
    },
    "publishedAt": "2025-05-21T07:45:29.000Z",
    "title": "How Should We Enhance the Safety of Large Reasoning Models: An Empirical\n  Study",
    "summary": "Large Reasoning Models (LRMs) have achieved remarkable success on\nreasoning-intensive tasks such as mathematics and programming. However, their\nenhanced reasoning capabilities do not necessarily translate to improved safety\nperformance-and in some cases, may even degrade it. This raises an important\nresearch question: how can we enhance the safety of LRMs? In this paper, we\npresent a comprehensive empirical study on how to enhance the safety of LRMs\nthrough Supervised Fine-Tuning (SFT). Our investigation begins with an\nunexpected observation: directly distilling safe responses from DeepSeek-R1\nfails to significantly enhance safety. We analyze this phenomenon and identify\nthree key failure patterns that contribute to it. We then demonstrate that\nexplicitly addressing these issues during the data distillation process can\nlead to substantial safety improvements. Next, we explore whether a long and\ncomplex reasoning process is necessary for achieving safety. Interestingly, we\nfind that simply using short or template-based reasoning process can attain\ncomparable safety performance-and are significantly easier for models to learn\nthan more intricate reasoning chains. These findings prompt a deeper reflection\non the role of reasoning in ensuring safety. Finally, we find that mixing math\nreasoning data during safety fine-tuning is helpful to balance safety and\nover-refusal. Overall, we hope our empirical study could provide a more\nholistic picture on enhancing the safety of LRMs. The code and data used in our\nexperiments are released in https://github.com/thu-coai/LRM-Safety-Study.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15404.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b58aa0d65058ce70beb98c",
      "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
      "fullname": "Zhexin Zhang",
      "name": "nonstopfor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15817",
      "authors": [
        {
          "_id": "682ec132fbbbc6e3a91b106e",
          "user": {
            "_id": "6623ea65b642e29cdf90a1b4",
            "avatarUrl": "/avatars/e32e90574c1162b2be87ed78604e3e4d.svg",
            "isPro": true,
            "fullname": "TongZheng",
            "user": "TongZheng1999",
            "type": "user"
          },
          "name": "Tong Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:27.903Z",
          "hidden": false
        },
        {
          "_id": "682ec132fbbbc6e3a91b106f",
          "name": "Lichang Chen",
          "hidden": false
        },
        {
          "_id": "682ec132fbbbc6e3a91b1070",
          "name": "Simeng Han",
          "hidden": false
        },
        {
          "_id": "682ec132fbbbc6e3a91b1071",
          "name": "R. Thomas McCoy",
          "hidden": false
        },
        {
          "_id": "682ec132fbbbc6e3a91b1072",
          "name": "Heng Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:59:54.000Z",
      "submittedOnDailyAt": "2025-05-22T06:15:33.221Z",
      "title": "논리적 이유를 통해 이유를 이해하는 과정에 의존하는 사고의 혼杂를 통제하여 명확한 논리적 사고를 유지하는 방법론을 학습하고 이해합니다.",
      "submittedOnDailyBy": {
        "_id": "6623ea65b642e29cdf90a1b4",
        "avatarUrl": "/avatars/e32e90574c1162b2be87ed78604e3e4d.svg",
        "isPro": true,
        "fullname": "TongZheng",
        "user": "TongZheng1999",
        "type": "user"
      },
      "summary": "인간은 자연스럽게 많은 이유의 모형으로 학습하고 논리적인 문제를 해결합니다. 예를 들어, 자연어, 코드, 심볼릭 로직 등 다양한 표현 형식을 사용합니다. 대조적으로, 현재 많은 LLM 기반의 접근 방식은 훈련 시 단일 이유의 모형으로 사용합니다. 일반적으로는 자연어입니다. 그러나 일부 방법은 추론 시 모형 선택이나 확장을 시도하지만, 훈련 프로세스는 모형과 관련이 없으며, 모형 간의 협력을 제한합니다. 이러한 결점을 보완하기 위해, 우리는 Thought-Mixture(MoT)를 제안합니다. MoT는 LLM이 자연어, 코드, 추가된 심볼릭 모형, truth-table(로그기 케이스를 체계적으로 나열하여 자연어 논리 추론의 주요 실패 모드를 일부 완화하는)의 3가지 보충 모형을 가로지르는 이유를 제공하는 프레임워크입니다. MoT는 2단계 설계를 사용합니다. 1) 자동적인 MoT 훈련, 모형 간의 과거를 필터링하여 생성된 이유로 학습합니다. 2) MoT 추론, 3가지 모형의 협력을 최대한 활용하여 더 좋은 예측을 생성합니다. FOLIO와 ProofWriter 등 논리적인 추론 벤치마크에 실험을 수행했으며, 우리의 MoT 프레임워크는 단일 모형의 chain-of-thought 접근 방식과 비교하여 평균 정확도의 상한에서 +11.7pp의 효과를 확인했습니다.进一步的分析表明，MoT框架在训练和推理阶段都带来了利益，特别是对困难的逻辑推理问题有效，展示了模态之间的互补性，克服了 truth-table 逻辑推理的主要瓶颈。",
      "upvotes": 7,
      "discussionId": "682ec133fbbbc6e3a91b10c1",
      "githubRepo": "https://github.com/zhengkid/Truth_Table_Logical_Reasoning",
      "ai_summary": "A Mixture-of-Thought framework enables LLMs to reason across natural language, code, and symbolic logic, improving accuracy on logical reasoning tasks compared to single-modality approaches.",
      "ai_keywords": [
        "LMM-based approaches",
        "reasoning modality",
        "natural language",
        "code",
        "symbolic logic",
        "modality-blind",
        "self-evolving MoT training",
        "MoT inference",
        "FOLIO",
        "ProofWriter",
        "truth-table",
        "logical reasoning"
      ]
    },
    "publishedAt": "2025-05-21T13:59:54.000Z",
    "title": "Learning to Reason via Mixture-of-Thought for Logical Reasoning",
    "summary": "Human beings naturally utilize multiple reasoning modalities to learn and\nsolve logical problems, i.e., different representational formats such as\nnatural language, code, and symbolic logic. In contrast, most existing\nLLM-based approaches operate with a single reasoning modality during training,\ntypically natural language. Although some methods explored modality selection\nor augmentation at inference time, the training process remains modality-blind,\nlimiting synergy among modalities. To fill in this gap, we propose\nMixture-of-Thought (MoT), a framework that enables LLMs to reason across three\ncomplementary modalities: natural language, code, and a newly introduced\nsymbolic modality, truth-table, which systematically enumerates logical cases\nand partially mitigates key failure modes in natural language reasoning. MoT\nadopts a two-phase design: (1) self-evolving MoT training, which jointly learns\nfrom filtered, self-generated rationales across modalities; and (2) MoT\ninference, which fully leverages the synergy of three modalities to produce\nbetter predictions. Experiments on logical reasoning benchmarks including FOLIO\nand ProofWriter demonstrate that our MoT framework consistently and\nsignificantly outperforms strong LLM baselines with single-modality\nchain-of-thought approaches, achieving up to +11.7pp average accuracy gain.\nFurther analyses show that our MoT framework benefits both training and\ninference stages; that it is particularly effective on harder logical reasoning\nproblems; and that different modalities contribute complementary strengths,\nwith truth-table reasoning helping to overcome key bottlenecks in natural\nlanguage inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15817.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6623ea65b642e29cdf90a1b4",
      "avatarUrl": "/avatars/e32e90574c1162b2be87ed78604e3e4d.svg",
      "fullname": "TongZheng",
      "name": "TongZheng1999",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15781",
      "authors": [
        {
          "_id": "682ea1129c1b77a5030b31f2",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "682ea1129c1b77a5030b31f3",
          "name": "Runpeng Yu",
          "hidden": false
        },
        {
          "_id": "682ea1129c1b77a5030b31f4",
          "name": "Gongfan Fang",
          "hidden": false
        },
        {
          "_id": "682ea1129c1b77a5030b31f5",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:32:10.000Z",
      "submittedOnDailyAt": "2025-05-22T02:29:25.873Z",
      "title": "dKV-Cache: dKV Cache는 Difference Language Model을 위한 캐시입니다.",
      "submittedOnDailyBy": {
        "_id": "64396ebc21221ac7411852b3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
        "isPro": false,
        "fullname": "Xinyin Ma",
        "user": "horseee",
        "type": "user"
      },
      "summary": "Diffusion Language Models (DLMs)은 자동복원 언어 모델로 기대되는 좋은 선택입니다. 그러나, DLMs는 오랜 기간 동안 추론 속도의 한계를 겪고 있습니다. 핵심적인 문제점은 비자동복원 아키텍처와 양방향적 注意가, 디코딩을 가속화하기 위해 사용하는 키 밸류 캐시를 사용할 수 없기 때문입니다. 이러한 블록 포인트를 해결하기 위해, DLMs의 확산 프로세스에 유사한 KV-cache 기계를 제안하였습니다. 우리의 접근법은 확산 프로세스 중 서로 다른 토큰이 서로 다른 표현적인 동적을 가지고 있음을 관찰한 것에 기반합니다. 따라서, 키와 밸류의 상태에 대한 지연된 캐시 전략을 제안하였습니다. 그리고 단계별로 키와 밸류를 캐시하기 위한 2개의 보간 변체를 설계하였습니다. (1) dKV-Cache-Decode는 거의 무손실의 가속을 제공하며, 긴 시퀀스에서도 성능을 향상시키고, 현재의 DLMs가 추론 시에는 컨텍스트 정보를 과소地利용하고 있음을 보여줍니다. (2) dKV-Cache-Greedy는 캐시의 생명주기를 줄인 진격적인 캐시를 구현하였으며, 두차원 시간 복잡도를 통해 고속화를 실현하였지만, 성능 저하를 허용합니다. 최종적으로, dKV-Cache는 추론 시 2-10배의 가속을 실현하였으며, AR와 DLMs 사이의 차이를 크게 좁폽니다. 우리의 dKV-Cache는 일반적인 언어 이해, 수학, 코드 생성의 벤치마크에서 평가되었으며, 가속을 제공하였습니다. 실험은 현재의 DLMs에서 트레인링 필요없이도 캐시를 사용할 수 있음을 보여주었습니다.",
      "upvotes": 7,
      "discussionId": "682ea1139c1b77a5030b322a",
      "ai_summary": "A KV-cache-like mechanism, delayed KV-Cache, accelerates diffusion language models' inference without significantly degrading performance.",
      "ai_keywords": [
        "Diffusion Language Models",
        "KV-cache",
        "non-autoregressive architecture",
        "bidirectional attention",
        "key-value states",
        "token representation dynamics",
        "dKV-Cache-Decode",
        "dKV-Cache-Greedy",
        "speedup",
        "autoregressive models"
      ]
    },
    "publishedAt": "2025-05-21T13:32:10.000Z",
    "title": "dKV-Cache: The Cache for Diffusion Language Models",
    "summary": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15781.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64396ebc21221ac7411852b3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
      "fullname": "Xinyin Ma",
      "name": "horseee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15776",
      "authors": [
        {
          "_id": "682eb95671d01f3fc74f0f8c",
          "user": {
            "_id": "644a41fcd9a3ae8341055179",
            "avatarUrl": "/avatars/21b35abdc60a34589443b5879901eb46.svg",
            "isPro": false,
            "fullname": "Changtai Zhu",
            "user": "BeastyZ",
            "type": "user"
          },
          "name": "Changtai Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:31.485Z",
          "hidden": false
        },
        {
          "_id": "682eb95671d01f3fc74f0f8d",
          "name": "Siyin Wang",
          "hidden": false
        },
        {
          "_id": "682eb95671d01f3fc74f0f8e",
          "name": "Ruijun Feng",
          "hidden": false
        },
        {
          "_id": "682eb95671d01f3fc74f0f8f",
          "name": "Kai Song",
          "hidden": false
        },
        {
          "_id": "682eb95671d01f3fc74f0f90",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:27:42.000Z",
      "submittedOnDailyAt": "2025-05-22T04:51:57.147Z",
      "title": "ConvSearch-R1: 강화학습에 의한 논리화의 활용을 통한 대화 검색의 검색어 재구성 향상",
      "submittedOnDailyBy": {
        "_id": "64c3c631e77ea9f28111172a",
        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
        "isPro": false,
        "fullname": "Siyin Wang",
        "user": "sinwang",
        "type": "user"
      },
      "summary": "회화 검색 시스템은, 맥락 의존성을 가진 쿼리를 효과적으로 처리하는 것이 필요합니다. 이러한 쿼리는 다양한 불확실성, 결欠, 코어 페러렌스(코어 페러렌스)를 포함합니다. 대화 쿼리 리포롯(CQR)는 이러한挑戦를 해결하기 위해, 이러한 쿼리를 단일 엔드 프레임에서 사용하는 검색 장치에 적합한 형식으로 변환하여 도움을 제공합니다. 그러나 현재의 CQR 접근 방식은 두 가지 중요한 제약으로 결점이 있습니다: 높은 외부의 인간 어노테이션 및 대규모 언어 모델로부터의 비용 높은 외부의 슈퍼바이저 의존성, 그리고 리포롯 모델과 하류의 검색 장치 사이의 불충분한 대응성입니다. 우리는 첫 번째 자동 구동 프레임워크 ConvSearch-R1을 소개합니다. 이 방법은 강화 학습을 사용하여 검색 신호를 통해 리포롯을 직접 최적화하여 외부의 리포롯 슈퍼바이저 의존성을 완전히 제거합니다. 우리의 새로운 2단계 접근 방식은, 검색 가이드를 통한 자동 학습을 통해 코드 시작 문제를 해결하기 위한 자동 구동 정책의 웜업과, 특별히 Rank-in-Incidence 보상 시드링 기법을 적용한 검색 가이드를 통한 강화 학습을 조합합니다. 이는 일반적인 검색 메트릭의 희박성 문제를 해결할 수 있습니다. TopiOCQA와 QReCC 데이터 세트에서 확장된 실험에 따라, ConvSearch-R1은 이전의 최상급 방법보다 크게 뛰어넘고, 어려운 TopiOCQA 데이터 세트에서 10% 이상의 개선을 달성하며, 외부의 슈퍼바이저를 사용하지 않는 작은 3B 파라미터 모델을 사용합니다.",
      "upvotes": 7,
      "discussionId": "682eb95771d01f3fc74f0fdd",
      "githubRepo": "https://github.com/BeastyZ/ConvSearch-R1",
      "ai_summary": "ConvSearch-R1 uses reinforcement learning and self-distillation to improve conversational query reformulation without relying on external supervision, outperforming state-of-the-art methods.",
      "ai_keywords": [
        "Conversational Query Reformulation",
        "CQR",
        "reinforcement learning",
        "self-distillation",
        "cold-start problem",
        "rank-incentive reward shaping",
        "retrieval signals",
        "TopiOCQA",
        "QReCC"
      ]
    },
    "publishedAt": "2025-05-21T13:27:42.000Z",
    "title": "ConvSearch-R1: Enhancing Query Reformulation for Conversational Search\n  with Reasoning via Reinforcement Learning",
    "summary": "Conversational search systems require effective handling of context-dependent\nqueries that often contain ambiguity, omission, and coreference. Conversational\nQuery Reformulation (CQR) addresses this challenge by transforming these\nqueries into self-contained forms suitable for off-the-shelf retrievers.\nHowever, existing CQR approaches suffer from two critical constraints: high\ndependency on costly external supervision from human annotations or large\nlanguage models, and insufficient alignment between the rewriting model and\ndownstream retrievers. We present ConvSearch-R1, the first self-driven\nframework that completely eliminates dependency on external rewrite supervision\nby leveraging reinforcement learning to optimize reformulation directly through\nretrieval signals. Our novel two-stage approach combines Self-Driven Policy\nWarm-Up to address the cold-start problem through retrieval-guided\nself-distillation, followed by Retrieval-Guided Reinforcement Learning with a\nspecially designed rank-incentive reward shaping mechanism that addresses the\nsparsity issue in conventional retrieval metrics. Extensive experiments on\nTopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly\noutperforms previous state-of-the-art methods, achieving over 10% improvement\non the challenging TopiOCQA dataset while using smaller 3B parameter models\nwithout any external supervision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3c631e77ea9f28111172a",
      "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
      "fullname": "Siyin Wang",
      "name": "sinwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15656",
      "authors": [
        {
          "_id": "682e8add58a17fe0e9ec02e6",
          "name": "Zhexin Zhang",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02e7",
          "name": "Yuhao Sun",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02e8",
          "user": {
            "_id": "65d859a3661492b25c46a117",
            "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
            "isPro": false,
            "fullname": "junxiao yang",
            "user": "yangjunxiao2021",
            "type": "user"
          },
          "name": "Junxiao Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:27.527Z",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02e9",
          "name": "Shiyao Cui",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02ea",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02eb",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T15:32:14.000Z",
      "submittedOnDailyAt": "2025-05-22T00:55:08.348Z",
      "title": "주의를 기울이세요: Open-Source LLMs의 微調節(fine-tuning) 시 주의를 기울이세요. 당신의 微調節(fine-tuning) 데이터는 비밀로 도둑질될 가능성이 있습니다.",
      "submittedOnDailyBy": {
        "_id": "61b58aa0d65058ce70beb98c",
        "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
        "isPro": false,
        "fullname": "Zhexin Zhang",
        "user": "nonstopfor",
        "type": "user"
      },
      "summary": "오픈소스 대형 언어 모델(LLMs)에 전문 데이터를 사용하여 미세 조정하는 것은 다운 스트림 개발자가 특정 작업에 적합한 LLMs를 얻는 표준적인 실천입니다. 놀라울 정도로 이 실천에 동반된 새로운 혹은 우려되는 위험이 발견되었습니다: 오픈소스 LLMs의 제작자는 간단한 백도어 트레이닝을 통해 전문의 다운 스트림 핑치닝 데이터를 추출할 수 있습니다. 우리의 세부적인 실험은 4가지의 전문 데이터에 사용된 오픈소스 모델(3B부터 32B 파라미터)과 2가지 다운 스트림 데이터 세트를 통해 수행되었으며, 추출 성능이 향상되는 것을 보여주었습니다: 실제적인 설정에서 5,000 샘플의 전체 데이터 중 76.3%의 다운 스트림 핑치닝 데이터(쿼리)가 완벽하게 추출될 수 있으며, ideale한 설정에서 성공률이 94.9%에 도달할 수 있습니다. 또한, 우리는 검출기 기반의 방어 전략을 조사하였으나, 개선된 공격에 견디는 것을 확인했습니다. 전체적으로, 핑치닝의 신규 인식된 데이터 브레이크 위험에 대한 긴급감을 강조하고, 이러한 우려되는 위험에 대처하기 위한 발전을 촉구하는 추적 연구의 필요성을 바랍니다. 우리 실험에 사용된 코드와 데이터는 https://github.com/thu-coai/Backdoor-Data-Extraction에 공개되어 있습니다.",
      "upvotes": 7,
      "discussionId": "682e8ade58a17fe0e9ec0348",
      "ai_summary": "There is a newly identified risk that creators of open-source LLMs can extract fine-tuning data from downstream models through backdoor training, even with black-box access.",
      "ai_keywords": [
        "Large Language Models",
        "fine-tuning",
        "open-source",
        "black-box access",
        "backdoor training",
        "data extraction",
        "data breach"
      ]
    },
    "publishedAt": "2025-05-21T11:32:14.000Z",
    "title": "Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data\n  Could Be Secretly Stolen!",
    "summary": "Fine-tuning on open-source Large Language Models (LLMs) with proprietary data\nis now a standard practice for downstream developers to obtain task-specific\nLLMs. Surprisingly, we reveal a new and concerning risk along with the\npractice: the creator of the open-source LLMs can later extract the private\ndownstream fine-tuning data through simple backdoor training, only requiring\nblack-box access to the fine-tuned downstream model. Our comprehensive\nexperiments, across 4 popularly used open-source models with 3B to 32B\nparameters and 2 downstream datasets, suggest that the extraction performance\ncan be strikingly high: in practical settings, as much as 76.3% downstream\nfine-tuning data (queries) out of a total 5,000 samples can be perfectly\nextracted, and the success rate can increase to 94.9% in more ideal settings.\nWe also explore a detection-based defense strategy but find it can be bypassed\nwith improved attack. Overall, we highlight the emergency of this newly\nidentified data breaching risk in fine-tuning, and we hope that more follow-up\nresearch could push the progress of addressing this concerning risk. The code\nand data used in our experiments are released at\nhttps://github.com/thu-coai/Backdoor-Data-Extraction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b58aa0d65058ce70beb98c",
      "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
      "fullname": "Zhexin Zhang",
      "name": "nonstopfor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13934",
      "authors": [
        {
          "_id": "682e8369b16c79c271b4db81",
          "user": {
            "_id": "643b866bff50448bcfc7d1d1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "manchery",
            "type": "user"
          },
          "name": "Jialong Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-22T05:44:24.633Z",
          "hidden": false
        },
        {
          "_id": "682e8369b16c79c271b4db82",
          "name": "Shaofeng Yin",
          "hidden": false
        },
        {
          "_id": "682e8369b16c79c271b4db83",
          "name": "Ningya Feng",
          "hidden": false
        },
        {
          "_id": "682e8369b16c79c271b4db84",
          "name": "Mingsheng Long",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643b866bff50448bcfc7d1d1/OXSeIgpwt_mjJJqBffXyA.png"
      ],
      "publishedAt": "2025-05-20T05:02:53.000Z",
      "submittedOnDailyAt": "2025-05-22T00:24:21.597Z",
      "title": "RLVR-World: 강화학습을 이용한 세계 모델의 훈련",
      "submittedOnDailyBy": {
        "_id": "643b866bff50448bcfc7d1d1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "manchery",
        "type": "user"
      },
      "summary": "세계 모델은 행동에 대한 상태 이동을 예측하고 다양한 모달리티에서 개발되어 있습니다. 그러나 최대 가능성 확률 평가(MLE)과 같은 표준적인 훈련 목적 함수는 세계 모델의 특정한 태스크의 목적에 맞지 않습니다. 예를 들어, 이동 예측의 평가 지표로 정확도나 시각적 품질과 같은 경우입니다. 본 논문에서는 RLVR(확인 가능한 보상을 사용한 강화 학습)을 활용한 통일된 프레임워크인 RLVR-World를 소개합니다. 이 프레임워크는 세계 모델을 이러한 지표에 직접 최적화하는 것을 목표로 합니다. RLVR-World는 언어 모델과 이미지 모델 모두에서 게임, 웹 네비게이션, 로봇 조작 등 다양한 분야에서 성능을 크게 향상시킬 수 있음을 보여주었습니다. 우리의 연구는 언어 모델의 최근 발전을 초월하여 RLVR은 생성 모델의 실용성을 넓게 향상시키는 후 훈련 패러다임의 가능성을 보여주고 있습니다.",
      "upvotes": 7,
      "discussionId": "682e836bb16c79c271b4dc78",
      "projectPage": "https://thuml.github.io/RLVR-World/",
      "githubRepo": "https://github.com/thuml/RLVR-World",
      "ai_summary": "RLVR-World uses reinforcement learning with verifiable rewards to optimize world models for task-specific metrics, achieving improved performance across language and video domains.",
      "ai_keywords": [
        "world models",
        "maximum likelihood estimation",
        "transition prediction",
        "reinforcement learning",
        "verifiable rewards",
        "autoregressive prediction",
        "tokenized sequences",
        "text games",
        "web navigation",
        "robot manipulation"
      ]
    },
    "publishedAt": "2025-05-20T01:02:53.000Z",
    "title": "RLVR-World: Training World Models with Reinforcement Learning",
    "summary": "World models predict state transitions in response to actions and are\nincreasingly developed across diverse modalities. However, standard training\nobjectives such as maximum likelihood estimation (MLE) often misalign with\ntask-specific goals of world models, i.e., transition prediction metrics like\naccuracy or perceptual quality. In this paper, we present RLVR-World, a unified\nframework that leverages reinforcement learning with verifiable rewards (RLVR)\nto directly optimize world models for such metrics. Despite formulating world\nmodeling as autoregressive prediction of tokenized sequences, RLVR-World\nevaluates metrics of decoded predictions as verifiable rewards. We demonstrate\nsubstantial performance gains on both language- and video-based world models\nacross domains, including text games, web navigation, and robot manipulation.\nOur work indicates that, beyond recent advances in reasoning language models,\nRLVR offers a promising post-training paradigm for enhancing the utility of\ngenerative models more broadly.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643b866bff50448bcfc7d1d1/OXSeIgpwt_mjJJqBffXyA.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13934.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643b866bff50448bcfc7d1d1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
      "fullname": "Jialong Wu",
      "name": "manchery",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13529",
      "authors": [
        {
          "_id": "682e8c441ffd3af3cb2f27d3",
          "user": {
            "_id": "65d859a3661492b25c46a117",
            "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
            "isPro": false,
            "fullname": "junxiao yang",
            "user": "yangjunxiao2021",
            "type": "user"
          },
          "name": "Junxiao Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:19.492Z",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d4",
          "name": "Jinzhe Tu",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d5",
          "name": "Haoran Liu",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d6",
          "name": "Xiaoce Wang",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d7",
          "name": "Chujie Zheng",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d8",
          "name": "Zhexin Zhang",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d9",
          "name": "Shiyao Cui",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27da",
          "name": "Caishun Chen",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27db",
          "name": "Tiantian He",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27dc",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27dd",
          "name": "Yew-Soon Ong",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27de",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T07:27:34.000Z",
      "submittedOnDailyAt": "2025-05-22T01:02:19.522Z",
      "title": "BARREL:境界에 관한 사실적인 신뢰성 있는 LRMs의 이유론",
      "submittedOnDailyBy": {
        "_id": "65d859a3661492b25c46a117",
        "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
        "isPro": false,
        "fullname": "junxiao yang",
        "user": "yangjunxiao2021",
        "type": "user"
      },
      "summary": "최근의 대규모 추론 모델(LRMs)의 발전은 수학과 논리 추론의 뛰어난 능력을 보여주고 있습니다. 그러나 현재의 LRMs은 무知를 인정하거나 \"모르겠다\"와 같은 대답을 줄 수 없고, 대신 과도한 자신감으로 잘못된 답을 출력하고 있습니다. 이로 인해 실제 신뢰성에 대한 우려가 생겼습니다. 본 연구에서는 과도하게 생각하여 잘못된 답을 출력하는 비정상적인 추론 패턴을 두 가지 식별하고, 이러한 문제를 해결하기 위해 새로운 프레임워크인 BARREL을 제안합니다. 이 프레임워크는 단순하고 경계에 대한 인식을 가진 사실적인 추론을 촉진하는 것을 목표로 합니다. 실험 결과를 통해 BARREL 훈련은 DeepSeek-R1-Distill-Llama-8B의 신뢰성을 39.33%에서 61.48%까지 높였으며, 동시에 R1에서 생성된 추론 데이터로 미세 조정된 모델과 동일한 정확도를 달성했습니다. 이러한 결과를 통해 우리의 실험 연구는 신뢰성과 사실적인 System 2 LRMs의 개발에 더 많은 가능성을 지닌다는 것을 보여주고 있습니다.",
      "upvotes": 7,
      "discussionId": "682e8c451ffd3af3cb2f281f",
      "ai_summary": "A novel framework, BARREL, addresses overconfidence in Large Reasoning Models by promoting concise and factual reasoning, significantly improving their reliability.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRMs",
        "overthinking",
        "last-minute guessing",
        "second-thought spiraling",
        "boundary-aware",
        "DeepSeek",
        "R1-Distill-Llama-8B",
        "reliable System 2 LRMs"
      ]
    },
    "publishedAt": "2025-05-18T03:27:34.000Z",
    "title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs",
    "summary": "Recent advances in Large Reasoning Models (LRMs) have shown impressive\ncapabilities in mathematical and logical reasoning. However, current LRMs\nrarely admit ignorance or respond with \"I don't know\". Instead, they often\nproduce incorrect answers while showing undue confidence, raising concerns\nabout their factual reliability. In this work, we identify two pathological\nreasoning patterns characterized by overthinking that contribute to the\noverconfident and incorrect answers: last-minute guessing and second-thought\nspiraling. To address these issues, we propose BARREL-a novel framework that\npromotes concise and boundary-aware factual reasoning. Our experiments show\nthat BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B\nfrom 39.33% to 61.48%, while still achieving accuracy comparable to models\nfinetuned on reasoning data generated by R1. These results demonstrate that our\npilot study is inspiring to build more reliable and factual System 2 LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13529.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d859a3661492b25c46a117",
      "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
      "fullname": "junxiao yang",
      "name": "yangjunxiao2021",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15778",
      "authors": [
        {
          "_id": "682e8b59a5b1e59c6978645a",
          "name": "Zhen Zhang",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645b",
          "name": "Xuehai He",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645c",
          "name": "Weixiang Yan",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645d",
          "name": "Ao Shen",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645e",
          "name": "Chenyang Zhao",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645f",
          "name": "Shuohang Wang",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c69786460",
          "name": "Yelong Shen",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c69786461",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:29:15.000Z",
      "submittedOnDailyAt": "2025-05-22T00:56:40.217Z",
      "title": "소프트 사고: 뉴스 개념 공간에서 LLMs의 논리론의 잠재력을 해방합니다.",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "인간은 일반적으로 추상적이고 유동적인 개념을 통해 생각하는 것이 많지만, 단순한 언어 토큰의 고정점이 아니라 이를 사용합니다. 현재의 논리론 모델은 인간 언어의 경계 내에서 논리론을 수행하는 데 제한되어 있으며, 이때는 고정점을 나타내는 단일 토큰을 처리합니다. 이 단일성 제약은 이러한 논리론 모델의 표현력과 한계를 제한하고, 일반적으로 논리론의 경로의 불완전한 탐색을 불러옵니다. 표준의 Chain-of-Thought (CoT) 방법은 하나의 토큰을 샘플링하는 데 의존합니다.\n\n본 논문에서는 \"소프트 사고\"라는 무학습 방법을 소개합니다. 이것은 인간처럼 \"소프트\" 논리론을 모방하며, 연속적인 개념 공간에서 소프트한 추상적인 개념 토큰을 생성합니다. 이 개념 토큰은 토큰埋め込み의 확률적 가중치의 혼합으로 생성되며, 이 토큰埋め込み가 형성하는 연속적인 개념 공간을 형성합니다. 이로 인해, 평활한 이동과 풍부한 표현이 가능해지고, 단일성 제약을 초월할 수 있습니다. 본질적으로, 생성된 각 개념 토큰은 연관된 단일 토큰으로부터의 여러 의미를 내포하고, 정확한 답에 효과적으로 맞추기 위해 다양한 논리론의 경로를 잠재적으로 탐색합니다.\n\n다양한 수학 및 코딩 벤치마크에서의 실험적 평가는 \"소프트 사고\"의 유효성과 효율성을 일관적으로 보여주며, 표준의 CoT과 비교하여, PATH@1의 정확도를 2.48 점 높였으며, 동시에 토큰 사용량을 22.4% 줄일 수 있음을 입증했습니다. 질적 분석은 \"소프트 사고\"의 출력이 고차원으로 해석 가능하고 읽기성이 높은 것을 밝혀, 언어 기반의 단일성의 고유한 한계를 깨는 가능성을 보여줍니다. 코드는 https://github.com/eric-ai-lab/Soft-Thinking 에 제공됩니다.",
      "upvotes": 6,
      "discussionId": "682e8b5aa5b1e59c697864ce",
      "projectPage": "https://soft-thinking.github.io",
      "githubRepo": "https://github.com/eric-ai-lab/Soft-Thinking",
      "ai_summary": "Soft Thinking, a training-free method, enhances reasoning by generating soft, abstract concept tokens in a continuous space, improving accuracy and efficiency in mathematical and coding benchmarks.",
      "ai_keywords": [
        "Soft Thinking",
        "continuous concept space",
        "token embeddings",
        "Chain-of-Thought",
        "pass@1 accuracy"
      ]
    },
    "publishedAt": "2025-05-21T13:29:15.000Z",
    "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous\n  Concept Space",
    "summary": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15778.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14827",
      "authors": [
        {
          "_id": "682ea25b2c417303938ae8fc",
          "name": "Yufan Zhuang",
          "hidden": false
        },
        {
          "_id": "682ea25b2c417303938ae8fd",
          "name": "Liyuan Liu",
          "hidden": false
        },
        {
          "_id": "682ea25b2c417303938ae8fe",
          "name": "Chandan Singh",
          "hidden": false
        },
        {
          "_id": "682ea25b2c417303938ae8ff",
          "name": "Jingbo Shang",
          "hidden": false
        },
        {
          "_id": "682ea25b2c417303938ae900",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6438ccbb3b46237de3d052e8/6rJB7g-l02Jn_iSD0C7ZJ.png"
      ],
      "publishedAt": "2025-05-20T18:41:46.000Z",
      "submittedOnDailyAt": "2025-05-22T02:35:44.464Z",
      "title": "텍스트 생성을 토큰 샘플링보다 더 발전시키세요.",
      "submittedOnDailyBy": {
        "_id": "6438ccbb3b46237de3d052e8",
        "avatarUrl": "/avatars/baa624d417b0b905e82127dc66346478.svg",
        "isPro": true,
        "fullname": "Yufan Zhuang",
        "user": "yzhuang",
        "type": "user"
      },
      "summary": "표준의 자동 회귀 생성에서, LLM는 다음 토큰 분포를 예측하고, 이 분포를 샘플링하여 샘플링된 토큰을 제외한 나머지를 버리고, 샘플링된 토큰만 새로운 입력으로 전달합니다. 이 분포의 풍부한 정보를 보존하기 위해, 자동 리브ク션 생성의 훈련 무제한 메소드인 \"Mixture of Inputs (MoI)\"를 제안합니다. 표준 패러다임에 따라 토큰을 생성한 후, 생성된 토큰과 이전에 버린 토큰 분포를 혼합하여 새로운 입력을 구축합니다. 특히, 토큰 분포를 사전 분포로, 샘플링된 토큰을 관찰로, 일반적인 one-hot 벡터를 연속적인 후验 분포의 기대값으로 대체하는 베이지안 추정법을 사용합니다. MoI는 모델이 전체 생성 프로세스에서 풍부한 내부 표현을 유지할 수 있으며, 문장 품질과 추론 능력를 향상시킬 수 있습니다. 수리 계산, 코드 생성, 디피레벨 QA 태스크 등에서, MoI는 QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, DAPO-Qwen-32B 등 여러 모델에서 성능을 향상시키고, 추가적인 훈련이나 계산 오버헤드 없이, 가벼운 것으로 만듭니다.",
      "upvotes": 4,
      "discussionId": "682ea25c2c417303938ae926",
      "projectPage": "https://github.com/EvanZhuang/mixinputs",
      "githubRepo": "https://github.com/EvanZhuang/mixinputs",
      "ai_summary": "Mixture of Inputs (MoI), a training-free method, enhances autoregressive generation by maintaining a richer internal representation, improving text quality and reasoning capabilities in mathematical reasoning, code generation, and PhD-level QA tasks.",
      "ai_keywords": [
        "Mixture of Inputs (MoI)",
        "autoregressive generation",
        "token distribution",
        "Bayesian estimation",
        "posterior expectation"
      ]
    },
    "publishedAt": "2025-05-20T14:41:46.000Z",
    "title": "Text Generation Beyond Discrete Token Sampling",
    "summary": "In standard autoregressive generation, an LLM predicts the next-token\ndistribution, samples a discrete token, and then discards the distribution,\npassing only the sampled token as new input. To preserve this distribution's\nrich information, we propose Mixture of Inputs (MoI), a training-free method\nfor autoregressive generation. After generating a token following the standard\nparadigm, we construct a new input that blends the generated discrete token\nwith the previously discarded token distribution. Specifically, we employ a\nBayesian estimation method that treats the token distribution as the prior, the\nsampled token as the observation, and replaces the conventional one-hot vector\nwith the continuous posterior expectation as the new model input. MoI allows\nthe model to maintain a richer internal representation throughout the\ngeneration process, resulting in improved text quality and reasoning\ncapabilities. On mathematical reasoning, code generation, and PhD-level QA\ntasks, MoI consistently improves performance across multiple models including\nQwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional\ntraining and negligible computational overhead.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6438ccbb3b46237de3d052e8/6rJB7g-l02Jn_iSD0C7ZJ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14827.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6438ccbb3b46237de3d052e8",
      "avatarUrl": "/avatars/baa624d417b0b905e82127dc66346478.svg",
      "fullname": "Yufan Zhuang",
      "name": "yzhuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.12650",
      "authors": [
        {
          "_id": "682c057b36dd2dbca3931150",
          "user": {
            "_id": "64ca1b79ec9a33183aa3bd29",
            "avatarUrl": "/avatars/cafeafeccced927aac986575338a804c.svg",
            "isPro": false,
            "fullname": "yang",
            "user": "yaotianvector",
            "type": "user"
          },
          "name": "Yaotian Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:18:15.239Z",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931151",
          "user": {
            "_id": "6552f1ad5d55ccb20e9142a0",
            "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
            "isPro": false,
            "fullname": "Ivan Tang",
            "user": "IvanTang",
            "type": "user"
          },
          "name": "Yiwen Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:18:12.714Z",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931152",
          "name": "Yizhe Chen",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931153",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931154",
          "name": "Jiangjie Qiu",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931155",
          "name": "Hao Xiong",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931156",
          "name": "Haoyu Yin",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931157",
          "name": "Zhiyao Luo",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931158",
          "name": "Yifei Zhang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931159",
          "name": "Sijia Tao",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115a",
          "name": "Wentao Li",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115b",
          "name": "Qinghua Zhang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115c",
          "name": "Yuqiang Li",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115d",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115e",
          "name": "Bin Zhao",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115f",
          "name": "Xiaonan Wang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931160",
          "name": "Fei Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T03:04:50.000Z",
      "submittedOnDailyAt": "2025-05-22T00:29:51.182Z",
      "title": "AutoMat: 현미경으로부터의 자동화 시스템 구조 재구성을 가능하게 하는 어게네틱 도구의 사용",
      "submittedOnDailyBy": {
        "_id": "6552f1ad5d55ccb20e9142a0",
        "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
        "isPro": false,
        "fullname": "Ivan Tang",
        "user": "IvanTang",
        "type": "user"
      },
      "summary": "기계학습에 기반한 원자간 상호작용력과 힘장은 정확한 원자 구조에 의존하지만, 이러한 데이터는 실험적으로 밝혀진 유한한 결정의 유용성으로 인해 희귀합니다. 전자현미경의 원자분해능 측정은 구조 데이터의 잠재적 자원으로 작용하지만, 이러한 이미지가 시뮬레이션에 적합한 형식으로 변환하는 것은 노동이 풍부하고 오차가 많은 절차로 되어 있으며, 모델의 훈련과 검증에 붕어높이 역할을 합니다. 우리는 AutoMat라는 시작부터 끝까지의 에이전트를 돕는 파이프라인을 소개합니다. 이는 스캔트랜스포션 전자현미경(STEM) 이미지를 자동적으로 원자결정 구조로 변환하고 그 물리적 특성을 예측하는 것입니다. AutoMat는 패턴 적응 디노이즈, 물리적 가이드 템플릿 검색, 대칭성에 대한 원자 재구성, MatterSim을 통해 고속 리라ク션 및 특성 예측, 그리고 모든 단계의 협업적인 운영을 조합하여 구성됩니다. 우리는 이 작업에专用한 STEM2Mat-Bench를 제안하고, 격자 RMSD, 형성 에너지 MAE, 구조 매칭의 성공률로 성능을 평가합니다. 외부 도구를 운영하는 것을 통해, AutoMat은 이 분야에서 텍스트만 가지는 LLM을 초월할 수 있으며, 우선 루프의 이유로 파이프라인 전체를 실현합니다. 450개 이상의 구조 샘플을 초과하는 규모적인 실험에서는, AutoMat은 현재 다양한 규모의 언어 모델과 도구를 크게 초월합니다. 이러한 결과는 AutoMat와 STEM2Mat-Bench 모두를 증명하고, 재료 과학에서 현미경과 원자의 시뮬레이션을 연결하는 새로운 단계를 시작하는 키 단계를 나타냅니다. 코드와 데이터셋은 https://github.com/yyt-2378/AutoMat과 https://huggingface.co/datasets/yaotianvector/STEM2Mat에서 공개되어 있습니다.",
      "upvotes": 4,
      "discussionId": "682c057d36dd2dbca3931206",
      "ai_summary": "AutoMat, an agent-assisted pipeline, transforms atomic-resolution STEM images into simulation-ready atomic crystal structures and predicts their properties, overcoming the bottleneck in data availability and processing.",
      "ai_keywords": [
        "end-to-end pipeline",
        "agent-assisted",
        "scanning transmission electron microscopy (STEM)",
        "pattern-adaptive denoising",
        "physics-guided template retrieval",
        "symmetry-aware atomic reconstruction",
        "fast relaxation",
        "property prediction",
        "MatterSim",
        "STEM2Mat-Bench",
        "lattice RMSD",
        "formation energy MAE",
        "structure-matching success rate",
        "multimodal large language models"
      ]
    },
    "publishedAt": "2025-05-18T23:04:50.000Z",
    "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from\n  Microscopy via Agentic Tool Use",
    "summary": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12650.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6552f1ad5d55ccb20e9142a0",
      "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
      "fullname": "Ivan Tang",
      "name": "IvanTang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15816",
      "authors": [
        {
          "_id": "682e8304e9980508c9a2fc73",
          "name": "Penghao Wu",
          "hidden": false
        },
        {
          "_id": "682e8304e9980508c9a2fc74",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "682e8304e9980508c9a2fc75",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:59:52.000Z",
      "submittedOnDailyAt": "2025-05-22T00:23:40.991Z",
      "title": "스트리밍 라인 윷엿 오워크 디렉션 인 LMM",
      "submittedOnDailyBy": {
        "_id": "64101f81b27543634e377fc1",
        "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
        "isPro": false,
        "fullname": "Penghao Wu",
        "user": "craigwu",
        "type": "user"
      },
      "summary": "대규모 다모달 모델은 다모달 태스크에서 뛰어난 성능을 보입니다が, 시각 토큰에 대한 과도한 계산으로 인해 계산적 문제를 크게 발생합니다. 토큰 축약法是 토큰 수준의冗長성을 중점으로 하지만, 우리는 계산 수준의冗長성을 시각 토큰에 대한 것을 식별하고 정보 손실을 제거하기 위해 연구를 진행합니다. 우리의 주요 견해는, 사전 학습된 시각 인코더에서 나온 시각 토큰은 LMM에서만 디코더만 필요하여 모든 중량적인 연산(예: self-attention, FFNs)이 필요하지 않습니다. 적절하게 설계하면 더 가벼운 처리가 가능합니다. 우리는 시각 관련 계산冗長성을 발견하고 이를 줄이기 위한 실험을 설계했습니다. 우리의 발견에 기반하여, ProxyV라는 새로운 접근법을 제안합니다. ProxyV는 계산 부담을 줄이면서 효율을 향상시키고, 더 温和한 효율 향상의 경우도 明顯한 효과를 얻을 수 있습니다. 또한, ProxyV의 유연성은 토큰 축약법과 조합하여 더 높은 효율을 얻을 수 있음을 보여줍니다. 코드는 이 URL에서 공개됩니다.",
      "upvotes": 2,
      "discussionId": "682e8305e9980508c9a2fca1",
      "projectPage": "https://penghao-wu.github.io/ProxyV/",
      "githubRepo": "https://github.com/penghao-wu/ProxyV",
      "ai_summary": "ProxyV alleviates computational burdens in large multimodal models by using proxy vision tokens, enhancing efficiency without sacrificing performance.",
      "ai_keywords": [
        "multimodal models",
        "computation-level redundancy",
        "vision tokens",
        "pretrained vision encoder",
        "decoder-only LMMs",
        "self-attention",
        "FFNs",
        "proxy vision tokens",
        "ProxyV"
      ]
    },
    "publishedAt": "2025-05-21T13:59:52.000Z",
    "title": "Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM",
    "summary": "Large multimodal models excel in multimodal tasks but face significant\ncomputational challenges due to excessive computation on visual tokens. Unlike\ntoken reduction methods that focus on token-level redundancy, we identify and\nstudy the computation-level redundancy on vision tokens to ensure no\ninformation loss. Our key insight is that vision tokens from the pretrained\nvision encoder do not necessarily require all the heavy operations (e.g.,\nself-attention, FFNs) in decoder-only LMMs and could be processed more lightly\nwith proper designs. We designed a series of experiments to discover and\nprogressively squeeze out the vision-related computation redundancy. Based on\nour findings, we propose ProxyV, a novel approach that utilizes proxy vision\ntokens to alleviate the computational burden on original vision tokens. ProxyV\nenhances efficiency without compromising performance and can even yield notable\nperformance gains in scenarios with more moderate efficiency improvements.\nFurthermore, the flexibility of ProxyV is demonstrated through its combination\nwith token reduction methods to boost efficiency further. The code will be made\npublic at this https://github.com/penghao-wu/ProxyV URL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15816.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64101f81b27543634e377fc1",
      "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
      "fullname": "Penghao Wu",
      "name": "craigwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15791",
      "authors": [
        {
          "_id": "682e8e9f1d7637a2584b6b8c",
          "name": "Fengyuan Dai",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b8d",
          "name": "Zifeng Zhuang",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b8e",
          "name": "Yufei Huang",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b8f",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b90",
          "name": "Bangyan Liao",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b91",
          "name": "Donglin Wang",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b92",
          "name": "Fajie Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:44:37.000Z",
      "submittedOnDailyAt": "2025-05-22T01:10:45.389Z",
      "title": "VARD: 가치기준의 RL를 활용한 확산모형의 효율적인 덴시조정",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "Diffusion 모델은 여러 분야에서 강력한 생성 도구로 등장하지만, 특정한 원하는 특성을 구현하기 위해 플레이트 체인 모델을 조정하는 것은 어려워집니다. 강화 학습(RL)은 좋은 결정을 제공하지만, 현재의 방법들은 안정적이고 효율적인 미세 조정을 동시에 달성하고, 미분 불가능한 보상을 지원하는 것이 어렵습니다. 또한, 희소한 보상의 관계는 중간 단계에서 충분한 시각을 제공하지 않고, 생성물 품질이 일반적으로 저조한 결과를 초래하는 경우가 많습니다. 이러한 제한을 해결하기 위해, 전체적인 확산 프로세스에서 밀집적이고 미분 가능한 신호가 필요합니다. 따라서, 우리는 보상의 기대값을 중간 상태로부터 예측하는 새로운 접근법을 제안합니다. 이 가치 함수는 전체적인 생성 프로세스에서 밀집적인 시각을 제공하기 위해 KL 정규화를 사용합니다. 우리의 방법은 학습 모델에 가깝게 만들면서, 역전파를 통해 효과적이고 안정적인 학습을 가능하게 합니다. 실험 결과를 통해, 우리의 접근법은 더 좋은 경로 가이드, 학습 효과의 향상, 복잡한, 미분 불가능한 보상 함수를 최적화한 확산 모델의 RL 적용 범위를 확장할 수 있음을 보여줍니다.",
      "upvotes": 2,
      "discussionId": "682e8ea11d7637a2584b6c46",
      "ai_summary": "VARD introduces a value function based reinforcement learning approach to enhance diffusion models with dense and differentiable supervision, improving training efficiency and handling non-differentiable rewards.",
      "ai_keywords": [
        "diffusion models",
        "reinforcement learning",
        "stable fine-tuning",
        "non-differentiable rewards",
        "sparse rewards",
        "value function",
        "dense supervision",
        "KL regularization",
        "backpropagation",
        "trajectory guidance",
        "training efficiency"
      ]
    },
    "publishedAt": "2025-05-21T13:44:37.000Z",
    "title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with\n  Value-based RL",
    "summary": "Diffusion models have emerged as powerful generative tools across various\ndomains, yet tailoring pre-trained models to exhibit specific desirable\nproperties remains challenging. While reinforcement learning (RL) offers a\npromising solution,current methods struggle to simultaneously achieve stable,\nefficient fine-tuning and support non-differentiable rewards. Furthermore,\ntheir reliance on sparse rewards provides inadequate supervision during\nintermediate steps, often resulting in suboptimal generation quality. To\naddress these limitations, dense and differentiable signals are required\nthroughout the diffusion process. Hence, we propose VAlue-based Reinforced\nDiffusion (VARD): a novel approach that first learns a value function\npredicting expection of rewards from intermediate states, and subsequently uses\nthis value function with KL regularization to provide dense supervision\nthroughout the generation process. Our method maintains proximity to the\npretrained model while enabling effective and stable training via\nbackpropagation. Experimental results demonstrate that our approach facilitates\nbetter trajectory guidance, improves training efficiency and extends the\napplicability of RL to diffusion models optimized for complex,\nnon-differentiable reward functions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15791.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15047",
      "authors": [
        {
          "_id": "682e8a7651706f69070c40d3",
          "name": "Yingming Pu",
          "hidden": false
        },
        {
          "_id": "682e8a7651706f69070c40d4",
          "name": "Tao Lin",
          "hidden": false
        },
        {
          "_id": "682e8a7651706f69070c40d5",
          "name": "Hongyu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T03:09:39.000Z",
      "submittedOnDailyAt": "2025-05-22T00:58:05.428Z",
      "title": "PiFlow: 원리 개념을 지닌 과학의 발견을 다효설의 협업으로 실현\n\n(注意：虽然要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，这里提供了一个更符合韩语表达习惯的版本。)",
      "submittedOnDailyBy": {
        "_id": "63d0c7d16b985b0f25d00a22",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d0c7d16b985b0f25d00a22/6bNSM4WhVsEndIVx4Mb8U.png",
        "isPro": false,
        "fullname": "Mellen Y. Pu",
        "user": "Mellen",
        "type": "user"
      },
      "summary": "대 언어 모델(LLM) 기반의 다 에이전트 시스템(MAS)은 과학의 발견에 뚜렷한 가능성을 보여주고 있습니다. 그러나 현재의 접근 방식은 무의미한 작업 흐름을 사용하여 과학의 발견을 자동화하고 있습니다. 이는 무목적의 가설 제안과 가설과 증거의 일관성 부족으로 인해 체계적인 불확실성을 감소시키는 데 방해됩니다. 이러한 제한을 극복하기 위해 체계적인 불확실성의 감소가 근본적으로 필요합니다. 우리는 PiFlow라는 정보 이론적인 프레임워크를 소개하며, 과학의 발견을 원칙(예: 과학의 법칙)에 따라 유도된 구조화된 불확실성 감소 문제를 처리하는 방법을 제안합니다. 3가지 다른 과학 분야에서 평가한 결과, 우리의 방법은 발견의 효율을 크게 향상시키고, 특성값의 곡선 아래 면적(AUC)은 탐색 단계에 대해 73.55% 증가하였으며, 베이지 에이전트 시스템과 비교하여 해의 질은 94.06% 향상되었습니다. 전체적으로, PiFlow는 고효율적인 자동화된 과학의 발견에 있어 새로운 패러다임의 변화를 촉진하고, 더 강건하고 가속화된 AI 주도 연구를 위한 길을 열어줍니다. 코드는 https://github.com/amair-lab/PiFlow{GitHub}에서 공개되어 있습니다.",
      "upvotes": 2,
      "discussionId": "682e8a7751706f69070c4121",
      "githubRepo": "https://github.com/amair-lab/PiFlow",
      "ai_summary": "PiFlow, an information-theoretical framework, improves automated scientific discovery by systematically reducing uncertainty and enhancing solution quality across various scientific domains.",
      "ai_keywords": [
        "Large Language Model",
        "multi-agent systems",
        "automated scientific discovery",
        "predefined workflows",
        "scientific discovery",
        "information-theoretical framework",
        "uncertainty reduction",
        "hypothesis",
        "evidence",
        "Area Under the Curve",
        "exploration steps",
        "solution quality",
        "Plug-and-Play method"
      ]
    },
    "publishedAt": "2025-05-20T23:09:39.000Z",
    "title": "PiFlow: Principle-aware Scientific Discovery with Multi-Agent\n  Collaboration",
    "summary": "Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate\nremarkable potential for scientific discovery. Existing approaches, however,\noften automate scientific discovery using predefined workflows that lack\nrationality constraints. This often leads to aimless hypothesizing and a\nfailure to consistently link hypotheses with evidence, thereby hindering\nsystematic uncertainty reduction. Overcoming these limitations fundamentally\nrequires systematic uncertainty reduction. We introduce PiFlow, an\ninformation-theoretical framework, treating automated scientific discovery as a\nstructured uncertainty reduction problem guided by principles (e.g., scientific\nlaws). In evaluations across three distinct scientific domains -- discovering\nnanomaterial structures, bio-molecules, and superconductor candidates with\ntargeted properties -- our method significantly improves discovery efficiency,\nreflected by a 73.55\\% increase in the Area Under the Curve (AUC) of property\nvalues versus exploration steps, and enhances solution quality by 94.06\\%\ncompared to a vanilla agent system. Overall, PiFlow serves as a\nPlug-and-Play method, establishing a novel paradigm shift in highly efficient\nautomated scientific discovery, paving the way for more robust and accelerated\nAI-driven research. Code is publicly available at our\nhttps://github.com/amair-lab/PiFlow{GitHub}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15047.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d0c7d16b985b0f25d00a22",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d0c7d16b985b0f25d00a22/6bNSM4WhVsEndIVx4Mb8U.png",
      "fullname": "Mellen Y. Pu",
      "name": "Mellen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15034",
      "authors": [
        {
          "_id": "682ecc348573bac82a974c8b",
          "user": {
            "_id": "6801fbe4cae7a6623f66ef64",
            "avatarUrl": "/avatars/383a72cc890946d89a185b8482d1b828.svg",
            "isPro": false,
            "fullname": "Kaiwen Zha",
            "user": "sunshinekevin",
            "type": "user"
          },
          "name": "Kaiwen Zha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:06.379Z",
          "hidden": false
        },
        {
          "_id": "682ecc348573bac82a974c8c",
          "name": "Zhengqi Gao",
          "hidden": false
        },
        {
          "_id": "682ecc348573bac82a974c8d",
          "name": "Maohao Shen",
          "hidden": false
        },
        {
          "_id": "682ecc348573bac82a974c8e",
          "name": "Zhang-Wei Hong",
          "hidden": false
        },
        {
          "_id": "682ecc348573bac82a974c8f",
          "name": "Duane S. Boning",
          "hidden": false
        },
        {
          "_id": "682ecc348573bac82a974c90",
          "name": "Dina Katabi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T02:43:15.000Z",
      "submittedOnDailyAt": "2025-05-22T06:45:08.638Z",
      "title": "RL Tango: 언어의 이유론을 강화하는 생성자와 증명자의 함께 강화\n\n(Note: The original text \"RL Tango: 言語の理由論を強化する生成器と証明者の共に強化\" is a direct translation of the provided English text. The Korean translation maintains the same structure and meaning as the English version.)",
      "submittedOnDailyBy": {
        "_id": "6801fbe4cae7a6623f66ef64",
        "avatarUrl": "/avatars/383a72cc890946d89a185b8482d1b828.svg",
        "isPro": false,
        "fullname": "Kaiwen Zha",
        "user": "sunshinekevin",
        "type": "user"
      },
      "summary": "강화 학습(RL)은 최근에 등장한 대규모 언어 모델(LLM)의 성능 향상을 위한 효과적인 접근 방식입니다. LLM 제너레이터는 인식기(보상 모델)에 의해 가이드된 정책으로 작동합니다. 그러나 현재의 LLM의 RL 후처리 방법은 고정(룰 기반 또는 학습된停止) 또는 규범적으로 학습된 인식기를 주로 사용합니다. 이러한 설계는 보상 해킹에 취약하고, 학습 분포를 초과하여 확장하는 것이 어렵습니다. 이러한 제한을 극복하기 위해, 우리는 Tango라는 새로운 프레임워크를 제안하고 있습니다. Tango는 RL을 사용하여 LLM 제너레이터와 인식기를 교차 학습하여 동시에 학습하는 것입니다. Tango의 핵심 혁신은 생성적인 프로세스 수준의 LLM 인식기이며, RL을 사용하여 학습되며 제너레이터와 함께 진화하는 것입니다. 중요한 점은 인식기는 결과 수준의 인식정확성 보상에 기반하여 학습되며, 과정 수준의 명확한 설명이 필요하지 않아서, 이 생성적인 RL 학습된 인식기는 확실한 또는 SFT 학습된 인식기보다 강건성과 확장성이 향상되어 제너레이터와 상호강화를 촉진하고 있습니다. 확장된 실험은 7B/8B 규모 모델 중 가장 先端한 결과를 달성했습니다: 제너레이터는 5개의 대회 수준의 수학 벤치마크와 4개의 어려운 외응역 사유 태스크에서 가장 先端한 성능을 달성했으며, 인식기는 ProcessBench 데이터 세트에서 가장 先端한 성능을 달성하고 있습니다. 특히, 두 컴포넌트는 가장 어려운 수학 사유 문제를 위해 큰 향상을 나타냅니다. 코드는 https://github.com/kaiwenzha/rl-tango에 있습니다.",
      "upvotes": 2,
      "discussionId": "682ecc358573bac82a974cd9",
      "githubRepo": "https://github.com/kaiwenzha/rl-tango",
      "ai_summary": "Tango is an RL framework that concurrently trains a generative LLM and a RL-trained verifier, achieving superior robustness and generalization in math benchmarks and out-of-domain reasoning tasks.",
      "ai_keywords": [
        "Reinforcement learning (RL)",
        "large language models (LLMs)",
        "policy",
        "verifier",
        "reward model",
        "reward hacking",
        "supervised fine-tuning (SFT)",
        "generative",
        "process-level",
        "verification correctness rewards",
        "mutual reinforcement",
        "ProcessBench dataset",
        "mathematical reasoning"
      ]
    },
    "publishedAt": "2025-05-20T22:43:15.000Z",
    "title": "RL Tango: Reinforcing Generator and Verifier Together for Language\n  Reasoning",
    "summary": "Reinforcement learning (RL) has recently emerged as a compelling approach for\nenhancing the reasoning capabilities of large language models (LLMs), where an\nLLM generator serves as a policy guided by a verifier (reward model). However,\ncurrent RL post-training methods for LLMs typically use verifiers that are\nfixed (rule-based or frozen pretrained) or trained discriminatively via\nsupervised fine-tuning (SFT). Such designs are susceptible to reward hacking\nand generalize poorly beyond their training distributions. To overcome these\nlimitations, we propose Tango, a novel framework that uses RL to concurrently\ntrain both an LLM generator and a verifier in an interleaved manner. A central\ninnovation of Tango is its generative, process-level LLM verifier, which is\ntrained via RL and co-evolves with the generator. Importantly, the verifier is\ntrained solely based on outcome-level verification correctness rewards without\nrequiring explicit process-level annotations. This generative RL-trained\nverifier exhibits improved robustness and superior generalization compared to\ndeterministic or SFT-trained verifiers, fostering effective mutual\nreinforcement with the generator. Extensive experiments demonstrate that both\ncomponents of Tango achieve state-of-the-art results among 7B/8B-scale models:\nthe generator attains best-in-class performance across five competition-level\nmath benchmarks and four challenging out-of-domain reasoning tasks, while the\nverifier leads on the ProcessBench dataset. Remarkably, both components exhibit\nparticularly substantial improvements on the most difficult mathematical\nreasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15034.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6801fbe4cae7a6623f66ef64",
      "avatarUrl": "/avatars/383a72cc890946d89a185b8482d1b828.svg",
      "fullname": "Kaiwen Zha",
      "name": "sunshinekevin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14157",
      "authors": [
        {
          "_id": "682ee9d0498a5aaed2692828",
          "user": {
            "_id": "615313b0793ef66b3324da1f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg",
            "isPro": false,
            "fullname": "Pittawat Taveekitworachai",
            "user": "pittawat",
            "type": "user"
          },
          "name": "Pittawat Taveekitworachai",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-22T09:09:37.412Z",
          "hidden": false
        },
        {
          "_id": "682ee9d0498a5aaed2692829",
          "name": "Potsawee Manakul",
          "hidden": false
        },
        {
          "_id": "682ee9d0498a5aaed269282a",
          "name": "Sarana Nutanong",
          "hidden": false
        },
        {
          "_id": "682ee9d0498a5aaed269282b",
          "user": {
            "_id": "62d192c2d50433c35eb1b48e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d192c2d50433c35eb1b48e/VjmDu8GOIuLuQNBQdQLLS.png",
            "isPro": false,
            "fullname": "Kunat Pipatanakul",
            "user": "kunato",
            "type": "user"
          },
          "name": "Kunat Pipatanakul",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-22T09:09:37.412Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T10:05:11.000Z",
      "submittedOnDailyAt": "2025-05-22T07:40:05.896Z",
      "title": "전처리 프로노프펫 공학에 의한 강화조정",
      "submittedOnDailyBy": {
        "_id": "615313b0793ef66b3324da1f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg",
        "isPro": false,
        "fullname": "Pittawat Taveekitworachai",
        "user": "pittawat",
        "type": "user"
      },
      "summary": "이 논문은 강화 조정(RFT)의 背景에서 선두 프로ン퓰트 공학(pPE)를 조사하고 있습니다. 언어 모델(LMs)은 보상 신호에 따라 성능을 최대화하는 행동을 나타내는 것을 장려받습니다. 기존의 RFT 연구는 알고리즘, 보상의 형태, 데이터의 필터링에 집중하였지만, 선두 프로ン퓰트의 설계—학습 중 추가되는 질문 앞에 주어지는 지시—는 조사가 부족했습니다. pPE의 차이 중 어느 하나가 LMs를 RFT 후 다른 행동을 구현할 수 있는지 조사하고 있습니다. 추론 시 프로ン퓰트 공학(iPE)에 의해 영감을 받은 것으로, reasonning, planning, code-based reasoning, knowledge recall, null-example utilization의 5가지 대표적인 iPE 전략을 pPE의 대응 전략로 번역하였습니다. Qwen2.5-7B을 사용하여 각 pPE 전략을 실험하고 in-domain 과 out-of-domain 벤치마크(예: AIME2024, HumanEval+, GPQA-Diamond)에서 성능을 평가하였습니다. 결과적으로, 모든 pPE 훈련 모델은 iPE 프로ン퓰트에 의해 받은 것보다 뛰어난 성능을 보였으며, null-example pPE 전략이 평균 성능의 가장 큰 향상과 AIME2024, GPQA-Diamond에서 최고의 향상을 기록했습니다. 또한, 행동 분류 프레임워크를 적용하여, 서로 다른 pPE 전략이 결과 모델에 다른 행동 스타일을 부여하는 것을 보여주었습니다. 이러한 발견은 pPE가 RFT의 강력한 조사 부족한 축으로 자리잡을 수 있음을 나타냅니다.",
      "upvotes": 1,
      "discussionId": "682ee9d1498a5aaed2692895",
      "ai_summary": "Prior prompt engineering is investigated as a means to guide language models to internalize distinct behaviors through reinforcement fine-tuning, showing significant performance gains over inference-time prompt engineering.",
      "ai_keywords": [
        "prompt engineering",
        "reinforcement fine-tuning",
        "language models",
        "reward signals",
        "step-by-step reasoning",
        "Qwen2.5-7B",
        "AIME2024",
        "HumanEval+",
        "GPQA-Diamond",
        "behavior-classification framework"
      ]
    },
    "publishedAt": "2025-05-20T06:05:11.000Z",
    "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning",
    "summary": "This paper investigates prior prompt engineering (pPE) in the context of\nreinforcement fine-tuning (RFT), where language models (LMs) are incentivized\nto exhibit behaviors that maximize performance through reward signals. While\nexisting RFT research has primarily focused on algorithms, reward shaping, and\ndata curation, the design of the prior prompt--the instructions prepended to\nqueries during training to elicit behaviors such as step-by-step\nreasoning--remains underexplored. We investigate whether different pPE\napproaches can guide LMs to internalize distinct behaviors after RFT. Inspired\nby inference-time prompt engineering (iPE), we translate five representative\niPE strategies--reasoning, planning, code-based reasoning, knowledge recall,\nand null-example utilization--into corresponding pPE approaches. We experiment\nwith Qwen2.5-7B using each of the pPE approaches, then evaluate performance on\nin-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and\nGPQA-Diamond). Our results show that all pPE-trained models surpass their\niPE-prompted counterparts, with the null-example pPE approach achieving the\nlargest average performance gain and the highest improvement on AIME2024 and\nGPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by\nadapting a behavior-classification framework, we demonstrate that different pPE\nstrategies instill distinct behavioral styles in the resulting models. These\nfindings position pPE as a powerful yet understudied axis for RFT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14157.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "615313b0793ef66b3324da1f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg",
      "fullname": "Pittawat Taveekitworachai",
      "name": "pittawat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14990",
      "authors": [
        {
          "_id": "682eb25eb92a286a35d681fb",
          "name": "Ishika Agarwal",
          "hidden": false
        },
        {
          "_id": "682eb25eb92a286a35d681fc",
          "name": "Nimet Beyza Bozdag",
          "hidden": false
        },
        {
          "_id": "682eb25eb92a286a35d681fd",
          "name": "Dilek Hakkani-Tür",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T00:31:13.000Z",
      "submittedOnDailyAt": "2025-05-22T03:43:27.386Z",
      "title": "언어특유지식: 모델은 X에서 영어보다 더 잘 알고 있는가?",
      "submittedOnDailyBy": {
        "_id": "6391e4e984afa726d66180b9",
        "avatarUrl": "/avatars/e437e2820745b522a868b8da27d9a11f.svg",
        "isPro": false,
        "fullname": "Ishika Agarwal",
        "user": "ishikaa",
        "type": "user"
      },
      "summary": "코드 스위치는 동일한 단어, 생각, 변환으로 다른 언어를 교환하는 현상입니다. 우리가 코드 스위치하는 이유는 특정 토픽이나 분야에서 하나의 언어로 대화하는 것이 더 자유롭게 느껴질 때입니다. 지식 밀도 높은 언어 모델의 증가에 따라, 우리는 다음 자연스러운 문제를 생각합니다: 모델은 특정 토픽에서 특정 언어 X에서 더 많은 지식을 가질 수 있을까요? 더욱 중요한 것은 언어를 바꿔推理를 수행하여推理의 효율을 향상시킬 수 있을까요? 언어 고유의 지식(LSK)라는 용어를 사용하여 이 현상을 표현합니다. 민족 문화는 다양한 언어와 함께 개발되는 경우가 많기 때문에, 문화 고유의 데이터 세트(문화나 사회 행동 규범에 대한 지식을 포함)를 사용합니다. 영어 외의 언어로 체인 오브 시인 추론을 사용했을 때, 언어 모델은 더 좋은 결과를 내는 것을 발견했습니다. 또한 이전 연구와 함께 의미 유사성과 표현 유사성과는 다를 수 있음을 보여주고, 문화 고유의 문장이 상대적으로 많을 때, 특정 지식이 특정의 「전문」 언어에 제한되어 할 수 있다는 가정을 합니다. 초기의 결과를 통해 격려받아, 언어 고유의 지식을 검출하는 간단한 메소드 로직을 설계하고, LSKExtractor라는 이름으로 붙였습니다. 이 모듈을 사용하여 추론 시 그 지식을 사용합니다. 모델이나 데이터 세트의 다양한 종류에 대해 평균적으로 정확도가 10% 이상 향상되었습니다. 우리의 연구는 언어 모델의 오픈 소스 개발에 기여하고, 그 모델이 사용될 문화와 언어 컨텍스트에 따라 더 많은 문화를 포함하는 것을 실현합니다.",
      "upvotes": 0,
      "discussionId": "682eb25fb92a286a35d6822d",
      "githubRepo": "https://github.com/agarwalishika/LSKExtractor",
      "ai_summary": "Models perform better in reasoning and accuracy when using language-specific knowledge and chain-of-thought reasoning in certain languages, including low-resource ones, compared to others.",
      "ai_keywords": [
        "code-switching",
        "Language Specific Knowledge (LSK)",
        "culture-specific datasets",
        "chain-of-thought reasoning",
        "culturally specific texts",
        "LSKExtractor"
      ]
    },
    "publishedAt": "2025-05-20T20:31:13.000Z",
    "title": "Language Specific Knowledge: Do Models Know Better in X than in English?",
    "summary": "Code-switching is a common phenomenon of alternating between different\nlanguages in the same utterance, thought, or conversation. We posit that humans\ncode-switch because they feel more comfortable talking about certain topics and\ndomains in one language than another. With the rise of knowledge-intensive\nlanguage models, we ask ourselves the next, natural question: Could models hold\nmore knowledge on some topics in some language X? More importantly, could we\nimprove reasoning by changing the language that reasoning is performed in? We\ncoin the term Language Specific Knowledge (LSK) to represent this phenomenon.\nAs ethnic cultures tend to develop alongside different languages, we employ\nculture-specific datasets (that contain knowledge about cultural and social\nbehavioral norms). We find that language models can perform better when using\nchain-of-thought reasoning in some languages other than English, sometimes even\nbetter in low-resource languages. Paired with previous works showing that\nsemantic similarity does not equate to representational similarity, we\nhypothesize that culturally specific texts occur more abundantly in\ncorresponding languages, enabling specific knowledge to occur only in specific\n\"expert\" languages. Motivated by our initial results, we design a simple\nmethodology called LSKExtractor to benchmark the language-specific knowledge\npresent in a language model and, then, exploit it during inference. We show our\nresults on various models and datasets, showing an average relative improvement\nof 10% in accuracy. Our research contributes to the open-source development of\nlanguage models that are inclusive and more aligned with the cultural and\nlinguistic contexts in which they are deployed.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6391e4e984afa726d66180b9",
      "avatarUrl": "/avatars/e437e2820745b522a868b8da27d9a11f.svg",
      "fullname": "Ishika Agarwal",
      "name": "ishikaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0
    },
    "isAuthorParticipating": false
  }
]