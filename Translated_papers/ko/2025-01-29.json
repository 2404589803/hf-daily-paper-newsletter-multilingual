[
  {
    "paper": {
      "id": "2501.17161",
      "authors": [
        {
          "_id": "6799b39b15f4661561c22968",
          "name": "Tianzhe Chu",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c22969",
          "name": "Yuexiang Zhai",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296a",
          "name": "Jihan Yang",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296b",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296c",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296d",
          "name": "Dale Schuurmans",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296e",
          "name": "Quoc V. Le",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296f",
          "name": "Sergey Levine",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c22970",
          "name": "Yi Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T18:59:44.000Z",
      "title": "SFT 기억, RL 일반화: 기초 모델의 후 테스트 비교 연구",
      "summary": "서버스 피트닝(SFT)와 강화학습(RL)은 기초 모델에 대한 광범위하게 사용된 후 훈련 기술이지만, 이들이 모델의 일반화 능력을 향상시키는 데에 대한 명확한 논의가 부족합니다. 본 논문에서는 SFT와 RL의 일반화와 기억 과정의 차이를 조사하고, 문맥 기반의 규칙 변체와 시각 변체를 중심으로 집중하고 있습니다. 또한, GeneralPoints(일반화 포인트)라는 산술 추론 카드 게임과 V-IRL(실세계 지도 환경)을 사용하여, SFT와 RL로 훈련된 모델이 문맥적 및 시각적인未见 변체들에 어떻게 일반화하는지 평가하고 있습니다. 이러한 결과를 통해, RL은 특히 결과 기반의 보상을 사용하여 훈련된 경우, 규칙 기반의 문맥적 및 시각적인 변체들에도 일반화할 수 있음을 보여줍니다. 반면, SFT는 훈련 데이터를 기억하고, 분포 외의 시나리오에 대한 일반화가 어려운 것을 보여줍니다. 진행된 분석에서, RL은 모델의 잠재적인 시각 인식 능력을 향상시키고, 시각 영역에서의 일반화를 촉진하는 것을 명확히 알 수 있었습니다. RL의 뛰어난 일반화 능력에도 불구하고, SFT는 효과적인 RL 훈련에서 중요한 역할을 하는 것을 보여주고 있습니다. SFT는 모델의 출력 형식을 안정화하고, 다음 단계의 RL에서 성능의 향상을 실현할 수 있음을 보여줍니다. 이러한 발견은 복잡한 다모달 태스크에서 일반화 가능한 지식을 얻는 RL의 능력을 보여줍니다.",
      "upvotes": 11,
      "discussionId": "6799b39d15f4661561c229e6"
    },
    "publishedAt": "2025-01-28T23:50:56.664Z",
    "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17161.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.17116",
      "authors": [
        {
          "_id": "6799b367d30dc065a2d51592",
          "name": "Ruizhe Wang",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51593",
          "name": "Yeyun Gong",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51594",
          "name": "Xiao Liu",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51595",
          "name": "Guoshuai Zhao",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51596",
          "name": "Ziyue Yang",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51597",
          "name": "Baining Guo",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51598",
          "name": "Zhengjun Zha",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51599",
          "user": {
            "_id": "653feb7ccf1f9c88f4928910",
            "avatarUrl": "/avatars/23a6a6818116683ea9485e1470a0062f.svg",
            "isPro": false,
            "fullname": "Peng Cheng",
            "user": "cp5555",
            "type": "user"
          },
          "name": "Peng Cheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-29T04:49:44.372Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T18:04:50.000Z",
      "title": "FP4 데이터형을 사용하여 대규모 언어 모델의 훈련을 최적화하는 방법",
      "summary": "대 언어 모형(LLMs)의 훈련에 있어서 계산 요구가 증가함에 따라 효율적인 방법을 필요로 합니다. Quantized training은 저 비트 계산을 가능하게 하여 이러한 비용들을 줄이는 좋은 해결책으로 소개되어 있습니다. FP8의 정확도를 구현할 수 있다는 가능성을 보여주지만, FP4의 사용은 큰 양화 오류와 제한된 표현력으로 문제를 초래하고 있습니다. 본 논문에서는 LLMs의 첫 FP4 훈련 프레임워크를 소개하고, 다음과 같은 두 가지 핵심 인нова션으로 이러한 문제를 해결합니다: 미분 가능한 양화 에스터와 활성화의 파괴를 방지하기 위한 이상값의 캡과 보정 전략. 안정성을 보장하기 위해 프레임워크는 혼합 정밀도 훈련 시나프스와 벡터 웨이즈의 양화를 조합하여 있습니다. 실험 결과를 통해 우리의 FP4 프레임워크는 BF16과 FP8과 같은 정확도를 달성하며 최소한의 손실을 가집니다. 13B 파라미터의 LLMs를 100B 토큰으로 훈련할 수 있습니다. 다음 세대의 하드웨어가 FP4를 지원하는 경우, 우리의 프레임워크는 효율적인 초 저 정밀도 훈련의 기초로 될 것입니다.",
      "upvotes": 9,
      "discussionId": "6799b368d30dc065a2d515bf"
    },
    "publishedAt": "2025-01-28T23:50:12.472Z",
    "title": "Optimizing Large Language Model Training Using FP4 Quantization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17116.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16975",
      "authors": [
        {
          "_id": "6799b345a66ae6b357bef986",
          "name": "Hongzhi Huang",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef987",
          "name": "Defa Zhu",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef988",
          "name": "Banggu Wu",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef989",
          "name": "Yutao Zeng",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef98a",
          "name": "Ya Wang",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef98b",
          "name": "Qiyang Min",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef98c",
          "name": "Xun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T14:15:42.000Z",
      "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling\n\n단어 단위로 과도하게 분할된 Transformer: 어휘 사전은 일반적으로 확장이 가치가 있는 것",
      "summary": "토큰화는 대규모 언어 모델(LLMs)의 기본적인 구성 요소이며, 그 영향은 완전히 조사되지 않았습니다. 본 논문에서는, 입력과 출력의 토큰 집합을 독립적으로 설정하고 언어 모델링의 성능을 향상시키기 위해 새로운 프레임워크인 \"Over-Tokenized Transformers\"를 소개합니다. 특히, 우리의 접근법은 입력 토큰 집합을 확장하고 다그램 토큰을 사용하여 스케일링을 실현합니다. 상세한 실험을 통해 입력 토큰 집합의 크기와 훈련 손실 사이의 로그선형 관계가 밝혀졌으며, 모델 크기와 관계없이 큰 입력 토큰 집합은 모델의 성능을 일관되게 향상시키는 것을 보여줍니다. 큰 입력 토큰 집합을 사용함으로써 추가적인 비용 없이 기본 모델의 두 배 크기와 같은 성능을 달성할 수 있습니다. 우리의 발견은 토큰화의 중요성을 스케일링 법칙에 강조하고, 토큰화기의 설계에 실질적인 지침을 제공하며, 더 효율적이고 강력한 LLMs를 위한 길을 열어줍니다.",
      "upvotes": 6,
      "discussionId": "6799b346a66ae6b357bef9e3"
    },
    "publishedAt": "2025-01-28T23:49:26.959Z",
    "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16975.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16764",
      "authors": [
        {
          "_id": "6799aa5a311dbfe3c96724cd",
          "name": "Chenguo Lin",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724ce",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724cf",
          "name": "Bangbang Yang",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724d0",
          "name": "Zeming Li",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724d1",
          "name": "Yadong Mu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T07:38:59.000Z",
      "title": "DiffSplat: 이미지 디퓨션 모델을 사용하여 스케일러블 Gaussian Splat 생성에 대한 재활용",
      "summary": "최근 3D 콘텐츠 생성의 발전은 고품질의 3D 데이터셋의 한계와 2D 다각형 생성의 불확실성에 문제점이 있습니다. 우리는 큰 규모의 텍스트를 이미지로 확장하는 모델을 활용하여 3D 가우시안 스플릿을 원生地生成하는 새로운 3D 생성 프레임워크인 'DiffSplat'을 소개합니다. 이 모델은 이전 3D 생성 모델과 달리, 웹 크기의 2D 선행 모델을 효과적으로 활용하면서, 통일된 모델로 3D의 일관성을 유지합니다. 훈련을 시작하기 위해, 가벼운 재구성 모델을 제안하고, scalable 데이터셋의 편집에 대응하는 다각형의 가우시안 스플릿 그리드를 즉시 생성할 수 있습니다. 이 그리드에 대한 일반적인 확산 손실과 함께, 3D의 일관성을 촉진하는 3D 렌더링 손실을 추가합니다. 이미지 확산 모델과의 호흡이 좋기 때문에, 이미지 생성을 위한 많은 기술이 3D 영역에서 쉽게 적용될 수 있습니다. 확장된 실험은 DiffSplat의 텍스트와 이미지에 의한 생성 작업 및 그 하류 애플리케이션에서 뛰어난 성능을 명확히 보여주었습니다. 세부적인 실험은 각 중요한 설계 선택의 효과성을 증명하고, 후 기술에 대한 통찰을 제공했습니다.",
      "upvotes": 4,
      "discussionId": "6799aa5c311dbfe3c9672542"
    },
    "publishedAt": "2025-01-29T01:12:02.839Z",
    "title": "DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/654866e8cd0a5621395f8287/TFJMeKzXxMLOnq8NH8ltZ.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/654866e8cd0a5621395f8287/6kn1RLEUsUV-W6S0Taylo.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16764.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "654866e8cd0a5621395f8287",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654866e8cd0a5621395f8287/4Bccwd1ehn-Ee4T1rId5S.jpeg",
      "fullname": "Panwang Pan",
      "name": "paulpanwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16496",
      "authors": [
        {
          "_id": "6799b2fbfe3c29ec219d7d99",
          "name": "Lee Sharkey",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9a",
          "user": {
            "_id": "64ad563f4beffa272de6efac",
            "avatarUrl": "/avatars/f1a4902a95830cc3936058449626f8e4.svg",
            "isPro": false,
            "fullname": "Bilal Chughtai",
            "user": "bilalchughtai",
            "type": "user"
          },
          "name": "Bilal Chughtai",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-29T04:47:56.702Z",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9b",
          "name": "Joshua Batson",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9c",
          "name": "Jack Lindsey",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9d",
          "name": "Jeff Wu",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9e",
          "name": "Lucius Bushnaq",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9f",
          "name": "Nicholas Goldowsky-Dill",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da0",
          "name": "Stefan Heimersheim",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da1",
          "name": "Alejandro Ortega",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da2",
          "name": "Joseph Bloom",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da3",
          "name": "Stella Biderman",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da4",
          "name": "Adria Garriga-Alonso",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da5",
          "name": "Arthur Conmy",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da6",
          "name": "Neel Nanda",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da7",
          "name": "Jessica Rumbelow",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da8",
          "name": "Martin Wattenberg",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da9",
          "name": "Nandi Schoots",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7daa",
          "name": "Joseph Miller",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dab",
          "name": "Eric J. Michaud",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dac",
          "name": "Stephen Casper",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dad",
          "name": "Max Tegmark",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dae",
          "name": "William Saunders",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7daf",
          "name": "David Bau",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db0",
          "name": "Eric Todd",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db1",
          "name": "Atticus Geiger",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db2",
          "name": "Mor Geva",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db3",
          "name": "Jesse Hoogland",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db4",
          "name": "Daniel Murfet",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db5",
          "name": "Tom McGrath",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T20:57:18.000Z",
      "title": "기계의 설명의 개방적인 문제",
      "summary": "기계적인 해석성은 신경 네트워크의 계산적 구조를 이해하는 데에 의존하여 구체적인 과학적 및 공학적 목표를 달성하기 위해 노력하고 있습니다. 이 분야의 발전은 AI 시스템의 행동에 대한 높은 신뢰성을 제공하며, 뇌의 특성에 대한 깊은 과학적 문제를 조명하는 것을 약속하고 있습니다. 최근의 발전에도 불구하고, 이 분야에는 많은 개방적인 문제를 가지고 있으며, 이러한 문제를 해결하기 전까지 많은 과학적 및 실용적인 이익을 실현할 수 없습니다: 우리 방식은 개념적 및 실용적인 개선이 필요하며, 깊은 통찰을 드러낼 수 있습니다; 우리는 특정 목표에 대한 우리의 방식의 최적 적용 방법을 명확히 해야 합니다; 그리고 이 분야는 우리 작업에 영향을 미칠거나 영향을 받는 사회 기술적 문제를 직면해야 합니다. 이 선진적인 리뷰에서는 기계적인 해석성의 현재의 한계와 이 분야가 우선적으로 다루어야 하는 개방적인 문제를 논의하고 있습니다.",
      "upvotes": 4,
      "discussionId": "6799b2fcfe3c29ec219d7dca"
    },
    "publishedAt": "2025-01-28T23:48:30.888Z",
    "title": "Open Problems in Mechanistic Interpretability",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16496.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.15747",
      "authors": [
        {
          "_id": "6799946c18cb282841d42639",
          "name": "Sankalp KJ",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263a",
          "name": "Ashutosh Kumar",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263b",
          "user": {
            "_id": "66707b60405252abeefd4c50",
            "avatarUrl": "/avatars/ee2728f115376e234e96820b8b376849.svg",
            "isPro": false,
            "fullname": "Laxmaan Balaji",
            "user": "laxmaanb",
            "type": "user"
          },
          "name": "Laxmaan Balaji",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-29T02:37:34.240Z",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263c",
          "name": "Nikunj Kotecha",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263d",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263e",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-29T08:55:14.312Z",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263f",
          "name": "Sreyoshi Bhaduri",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T03:19:03.000Z",
      "title": "IndicMMLU-Pro: 아시아诸国的大规模语言模型多任务语言理解基准测试",
      "summary": "인디사브콘티니언의 15억 이상의 인구가 아는 인디언어는 풍부한 문화유산, 언어다양성과 복잡한 구조로 인해 자연어 처리(NLP) 연구에 특별한 문제를 제공하며, 기회도 제공되고 있습니다. IndicMMLU-Pro는 인디언어의 대언어 모델(LLMs)을 평가하기 위한 세부적인 벤치마크이며, MMLU Pro(마스イ브스 멀티 태스크 언어 이해) 프레임워크에 기반하여 설계되었습니다. Handi, Bengali, Gujarati, Marathi, Kannada, Punjabi, Tamil, Telugu, Urdu 등 주요 언어를 대상으로, 인디사브콘티니언의 언어다양성에 의한 특별한 문제를 해결하기 위해 목적입니다. 이 벤치마크는 언어 이해, 논리론리, 생성 등 광범위한 태스크에서 인디언어의 복잡한 분위기를 극대화하여 자세히捉える 것을 목표로 합니다. IndicMMLU-Pro는 인디언어의 AI 연구를 위한 피드백을 제공하며, 정확성, 효율성과 문화적 민감성을 가진 모델의 개발을 뒷받침하기 위해 표준화된 평가 프레임워크를 제공합니다. 이 논문은 벤치마크의 설계 원칙, 태스크 테크놀로지, 데이터 수집 방법과 최신의 다언어 모델의 기준 결과들을 보여줍니다.",
      "upvotes": 3,
      "discussionId": "6799946e18cb282841d426d6"
    },
    "publishedAt": "2025-01-28T21:38:17.182Z",
    "title": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15747.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.16372",
      "authors": [
        {
          "_id": "67999c3dc1e34886f90320ee",
          "name": "J. Pablo Muñoz",
          "hidden": false
        },
        {
          "_id": "67999c3dc1e34886f90320ef",
          "name": "Jinjie Yuan",
          "hidden": false
        },
        {
          "_id": "67999c3dc1e34886f90320f0",
          "name": "Nilesh Jain",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-23T02:14:08.000Z",
      "title": "저 순위 어댑터와 뉴럴 아키텍처 탐색이 LLM 압축에 합류합니다.",
      "summary": "LLM의 급격한 확장은 微調節와 部署에 필요한 계산 컴퓨팅 리소스에 대한 중대한 문제를 背負하고 있습니다. Low-Rank Adapters의 최근의 진보는 이러한 모델의 파라미터 효율적인 微調節(PEFT)의 효과를 보여줍니다. 본 기록 논문에서는, Low-Rank 표현과 Neural Architecture Search(NAS) 방법, 특히 Weight-Shared Super Network와의 협조를 중심으로 혁신적인 접근을 자세히 논의합니다. 이러한 방법을 통합함으로써, 대형 사전 학습 모델의 압축과 微調節에 대한 강력한 해결책이 개발됩니다. 우리의 분석은 이러한 조합된 전략의 민주화의 가능성에 특히 강조하고, 리소스 제한 환경에서 部署를 통해 더욱 접근 가능한 LLM의 사용에 대한 촉진 결과를 보여줍니다. 결과적으로 얻은 모델은 메모리 플럿프린트를 줄이고, 추론 시간을 줄이며, LLM의 실용적이고 스케일러블한 애플리케이션의 실현에 연결됩니다. 모델과 코드는 https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning 에서 이용 가능합니다.",
      "upvotes": 2,
      "discussionId": "67999c3dc1e34886f9032140"
    },
    "publishedAt": "2025-01-28T22:11:04.472Z",
    "title": "Low-Rank Adapters Meet Neural Architecture Search for LLM Compression",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16372.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.17117",
      "authors": [
        {
          "_id": "6799e5f9121155210e4fa48c",
          "name": "Thibaud Leteno",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa48d",
          "name": "Irina Proskurina",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa48e",
          "name": "Antoine Gourru",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa48f",
          "name": "Julien Velcin",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa490",
          "name": "Charlotte Laclau",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa491",
          "name": "Guillaume Metzler",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa492",
          "name": "Christophe Gravier",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T18:07:30.000Z",
      "title": "모라럴알리먼트의 평가에 대한 프랑스어 데이터셋 \"모라럴노리마르\"",
      "summary": "언어 모델을 인간적인 가치관과 일치시키는 것은 중요합니다, 특히 일상 생활에서 더욱 많이 포함되는 경우 매우 중요합니다. 모델은 일반적으로 사용자의 취향에 맞추어 사용될 수 있지만, 이는 동시에 현실적인 사회의 가치관과 행동을 일치시키는 것과 같은 중요성을 가집니다. 영어와 중국어에서 큰 진보가 있었지만, 프랑스어에서는 이 분야에 약간의 주의가 제공되지 않았습니다. 이로 인해 프랑스어의 Large Language Models (LLMs)가 어떻게 가치관을 다루는지 이해하는 것이 불가능해졌습니다. 이를 보완하기 위해, 우리는 「모라의 이야기」라는 프랑스어 데이터 세트를 소개합니다. 이것은 Moral Stories를 번역한 것입니다, 프랑스어의本国어자에게 도움을 받아 문법의 정확성과 프랑스 문화의 맥락에 맞는 것을 확인합니다. 또한 데이터 세트 내의 가치관의 注記을 바탕으로 프랑스의 가치관을 일치시키는 것을 확인합니다. 「모라의 이야기」는 다양한 사회 상황들을 포함하고 있습니다. 그 중에는 기회의 차이, 관계 속에서의 진실성 표현, 혹은 동물에 대한 책임 등이 포함됩니다. 미래의 연구를 촉진하기 위해, 우리는 프랑스어 데이터와 영어 데이터의 다언어 모델의 조화 방법과 그 조화 방법의 강건성을 위해 초기 실험을 진행합니다. 모델은 일반적으로 사용자의 취향을 최적화하는 데에 의해 가치관 및 불윤이 데이터를 쉽게 영향을 받습니다.",
      "upvotes": 0,
      "discussionId": "6799e5fb121155210e4fa500"
    },
    "publishedAt": "2025-01-29T03:32:09.927Z",
    "title": "Histoires Morales: A French Dataset for Assessing Moral Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17117.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "629a3dbcd496c6dcdebf41cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655113762275-629a3dbcd496c6dcdebf41cc.jpeg",
      "fullname": "Irina Proskurina",
      "name": "iproskurina",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]