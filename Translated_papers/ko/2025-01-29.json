[
  {
    "paper": {
      "id": "2501.17161",
      "authors": [
        {
          "_id": "6799b39b15f4661561c22968",
          "name": "Tianzhe Chu",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c22969",
          "name": "Yuexiang Zhai",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296a",
          "name": "Jihan Yang",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296b",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296c",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296d",
          "name": "Dale Schuurmans",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296e",
          "name": "Quoc V. Le",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296f",
          "name": "Sergey Levine",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c22970",
          "name": "Yi Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T18:59:44.000Z",
      "title": "SFT 메모리, RL 일반화: 기초 모델의 보완 후의 비교적 연구",
      "summary": "초보 학습(SFT)와 강화 학습(RL)은 기초 모델의 훈련 후의 기술로 광범위하게 사용되고 있으나, 이들이 모델의 일반화 능력을 향상시키는 역할을 명확하지 않습니다. 본 논문에서는 SFT와 RL의 일반화와 기억 과정의 차이를 조사하고, 문맥 기반의 규칙 변체와 시각 변체를 중심으로 집중합니다. 우리는 GeneralPoints(일반적인 점)이라는 산술 추론 카드 게임과 V-IRL(실세계의 네비게이션 환경)을 통해 SFT와 RL로 훈련된 모델이 문맥적이고 시각적인未见 변체에 어떻게 일반화하는지 평가합니다. 우리는 RL, 특히 결과 기반의 보상을 사용하여 훈련된 경우, 규칙 기반의 문맥적이고 시각적인 변체에서도 일반화함을 보여줍니다. SFT와 비교하여, 훈련 데이터를 기억하는 경향이 있으며, 분포 외의 시나리오에서 일반화하는 것이 어렵습니다. 진보적인 분석에서, RL은 모델의 시각 인식 능력을 향상시키고 시각 영역에서의 일반화에 촉진하는 것을 보여줍니다. RL의 우수한 일반화성을 가지는 데도, SFT는 효과적인 RL 훈련에 있어서 중요하며, SFT는 모델의 출력 형식을 안정시키고 후속의 RL이 성능 향상을 달성하는 것을 가능하게 합니다. 이러한 발견은 복잡한 다모달 태스크에서 일반화 가능한 지식을 학습하는 RL의 능력을 보여주고 있습니다.",
      "upvotes": 11,
      "discussionId": "6799b39d15f4661561c229e6"
    },
    "publishedAt": "2025-01-28T23:50:56.664Z",
    "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17161.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.17116",
      "authors": [
        {
          "_id": "6799b367d30dc065a2d51592",
          "name": "Ruizhe Wang",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51593",
          "name": "Yeyun Gong",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51594",
          "name": "Xiao Liu",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51595",
          "name": "Guoshuai Zhao",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51596",
          "name": "Ziyue Yang",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51597",
          "name": "Baining Guo",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51598",
          "name": "Zhengjun Zha",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51599",
          "user": {
            "_id": "653feb7ccf1f9c88f4928910",
            "avatarUrl": "/avatars/23a6a6818116683ea9485e1470a0062f.svg",
            "isPro": false,
            "fullname": "Peng Cheng",
            "user": "cp5555",
            "type": "user"
          },
          "name": "Peng Cheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-29T04:49:44.372Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T18:04:50.000Z",
      "title": "FP4 데이터형을 사용하여 대규모 언어 모델의 훈련을 최적화하는 방법",
      "summary": "LLM의 학습에 따른 증가하는 계산부담에 대해 더 효율적인 방법을 필요로 합니다. Quantized training은 낮은 비트 연산을 가능하게 하여 이러한 비용을 줄이는 데에 바람직한 해결책으로 자리잡습니다. FP8 정밀도는 실행의 가능성을 보여주지만, FP4의 활용은 큰 양식 오류와 제한된 표현 능력으로 인해 어려움을 초래하고 있습니다. 본 논문에서는 LLM의 첫 번째 FP4 학습 프레임워크를 통해 이러한 문제를 해결하기 위해 두 가지 주요 혁신을 제안합니다: 미분 가능한 양식 평가기를 사용한 가중치 업데이트와 이상값의 클랩과 보정 전략으로 활성화를 보호합니다. 안정성을 보장하기 위해 프레임워크는 혼합 정밀도 학습 시너지와 벡터 위즈 양식을 조합하여 있습니다. 실험 결과에 따르면 우리의 FP4 프레임워크는 최소한의 손실로 BF16과 FP8과 같은 동일한 정확도를 달성할 수 있으며, 13B 파라미터의 LLM을 100B 토큰으로 학습할 수 있습니다. 다음 세대의 하드웨어가 FP4를 지원할 경우, 우리의 프레임워크는 효율적인 초저정 정밀도 학습의 기초를 제공합니다.",
      "upvotes": 9,
      "discussionId": "6799b368d30dc065a2d515bf"
    },
    "publishedAt": "2025-01-28T23:50:12.472Z",
    "title": "Optimizing Large Language Model Training Using FP4 Quantization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17116.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16975",
      "authors": [
        {
          "_id": "6799b345a66ae6b357bef986",
          "name": "Hongzhi Huang",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef987",
          "name": "Defa Zhu",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef988",
          "name": "Banggu Wu",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef989",
          "name": "Yutao Zeng",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef98a",
          "name": "Ya Wang",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef98b",
          "name": "Qiyang Min",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef98c",
          "name": "Xun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T14:15:42.000Z",
      "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling\n\n이 번역은 전문성과 정확성을 유지하며 제공됩니다.",
      "summary": "토크나이징은 대규모 언어 모델(LLMs)의 기본적인 구성 요소이며, 그 영향은 완전히 조사되지 않았습니다. 본 논문에서는 입력과 출력의 단어 벡터를 분리하고 언어 모델링의 성능을 향상시키기 위해 새로운 프레임워크인 \"Over-Tokenized Transformers\"를 소개합니다. 특히, 우리의 접근 방식은 입력의 단어 벡터를 확장하고 다그램 토큰을 활용합니다. 확장된 실험을 통해 입력의 단어 벡터의 크기와 훈련 손실 사이에 로그 리너 관계가 밝혀졌으며, 이러한 크기는 모델의 크기와 상관없이 모델의 성능을 일관되게 향상시키는 것을 보여주었습니다. 큰 입력의 단어 벡터를 사용함으로써 추가적인 비용 없이 기본 모델의 2배 크기와 같은 성능을 달성했습니다. 우리의 발견은 스케일링 법칙의 중요성을 강조하고, 토크나이징의 설계에 실질적인 지침을 제공하며, 더 효율적이고 강력한 LLMs를 위한 길을 열어줍니다.",
      "upvotes": 6,
      "discussionId": "6799b346a66ae6b357bef9e3"
    },
    "publishedAt": "2025-01-28T23:49:26.959Z",
    "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16975.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16764",
      "authors": [
        {
          "_id": "6799aa5a311dbfe3c96724cd",
          "name": "Chenguo Lin",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724ce",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724cf",
          "name": "Bangbang Yang",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724d0",
          "name": "Zeming Li",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724d1",
          "name": "Yadong Mu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T07:38:59.000Z",
      "title": "DiffSplat: 이미지 디퓨션 모델을 스케일러블한 Gaussian Splat 생성에 재활용하는 방법",
      "summary": "최근 3D 콘텐츠 생성의 발전은 제한된 고품질 3D 데이터셋과 2D 다각형 생성의 불확실성에서 떨어졌습니다. 여기서는 새로운 3D 생성 프레임워크인 DiffSplat을 소개합니다. DiffSplat은 대규모 텍스트를 이미지로 확장하는 모델을 활용하여, 3D 가우시안 스플릿을 원生地生成하는 것을 목표로 합니다. 이전의 3D 생성 모델과 달리, 웹 스케일의 2D 프로이저를 효과적으로 활용하면서, 일관된 모델로 3D의 일관성을 유지합니다. 학습을 시작하기 위해, 가벼운 재구성 모델이 제안되어, scalable 데이터셋의 준비에 대해, 여러 각도에 대한 가우시안 스플릿 그리드를 즉시 생성할 수 있습니다. 이러한 그리드에 대한 일반적인 확장 손실과 결합하여, 3D 일관성을 촉진하기 위해 3D 렌더링 손실을 도입합니다. 이미지 확장 모델과의 호흡이 좋으며, 이미지 생성을 위한 많은 기술을 3D 영역에 쉽게 적용할 수 있습니다. 확장된 실험은, DiffSplat의 텍스트와 이미지에 기반한 생성 작업과 그 하류 애플리케이션에서 우수한 성능을 명확히 합니다. 세부적인 연구는 각 중요한 설계 선택의 효과성을 증명하고, 후공을 이해할 수 있습니다.",
      "upvotes": 4,
      "discussionId": "6799aa5c311dbfe3c9672542"
    },
    "publishedAt": "2025-01-29T01:12:02.839Z",
    "title": "DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/654866e8cd0a5621395f8287/TFJMeKzXxMLOnq8NH8ltZ.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/654866e8cd0a5621395f8287/6kn1RLEUsUV-W6S0Taylo.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16764.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "654866e8cd0a5621395f8287",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654866e8cd0a5621395f8287/4Bccwd1ehn-Ee4T1rId5S.jpeg",
      "fullname": "Panwang Pan",
      "name": "paulpanwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16496",
      "authors": [
        {
          "_id": "6799b2fbfe3c29ec219d7d99",
          "name": "Lee Sharkey",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9a",
          "user": {
            "_id": "64ad563f4beffa272de6efac",
            "avatarUrl": "/avatars/f1a4902a95830cc3936058449626f8e4.svg",
            "isPro": false,
            "fullname": "Bilal Chughtai",
            "user": "bilalchughtai",
            "type": "user"
          },
          "name": "Bilal Chughtai",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-29T04:47:56.702Z",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9b",
          "name": "Joshua Batson",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9c",
          "name": "Jack Lindsey",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9d",
          "name": "Jeff Wu",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9e",
          "name": "Lucius Bushnaq",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9f",
          "name": "Nicholas Goldowsky-Dill",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da0",
          "name": "Stefan Heimersheim",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da1",
          "name": "Alejandro Ortega",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da2",
          "name": "Joseph Bloom",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da3",
          "name": "Stella Biderman",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da4",
          "name": "Adria Garriga-Alonso",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da5",
          "name": "Arthur Conmy",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da6",
          "name": "Neel Nanda",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da7",
          "name": "Jessica Rumbelow",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da8",
          "name": "Martin Wattenberg",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da9",
          "name": "Nandi Schoots",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7daa",
          "name": "Joseph Miller",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dab",
          "name": "Eric J. Michaud",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dac",
          "name": "Stephen Casper",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dad",
          "name": "Max Tegmark",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dae",
          "name": "William Saunders",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7daf",
          "name": "David Bau",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db0",
          "name": "Eric Todd",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db1",
          "name": "Atticus Geiger",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db2",
          "name": "Mor Geva",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db3",
          "name": "Jesse Hoogland",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db4",
          "name": "Daniel Murfet",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db5",
          "name": "Tom McGrath",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T20:57:18.000Z",
      "title": "기계의 설명 중의 개방적인 문제",
      "summary": "기계의 설명성은 신경 네트워크의 기능을 이해하고 구체적인 과학적 및 공학적 목표를 달성하기 위해 계산적 구조를 명확히 하는 것을 목표로 합니다. 이 분야의 발전은 AI 시스템의 행동에 대한 신뢰도를 높일 수 있는 것이며, 지능의 본질에 대한 흥미로운 과학적 문제를 조명하는 것을 약속합니다. 최근의 발전에 따라 이 목표를 향해 나아가는 것이 가능합니다. 그러나 이러한 목표를 달성하기 위해 필요한 많은 개방적인 문제를 해결하기 전에 많은 과학적 및 실용적인 이익을 실현할 수 없습니다. 우리 방식은 개념적 및 실용적인 개선이 필요하고, 깊은 이해를 가능하게 해주어야 합니다. 또한 특정 목표를 달성하기 위해 우리 방식이 최적의 방식으로 적용되는 방법을 명확히 해야 합니다. 또한 우리의 작업에 영향을 미칠 뿐만 아니라, 우리의 작업에 영향을 받는 사회 기술적 문제를 대응하는 것이 필요합니다. 이 선진적인 리뷰는 기계의 설명성의 현재의 선두와, 분야가 우선해야 할 개방적인 문제를 논의하고 있습니다.",
      "upvotes": 4,
      "discussionId": "6799b2fcfe3c29ec219d7dca"
    },
    "publishedAt": "2025-01-28T23:48:30.888Z",
    "title": "Open Problems in Mechanistic Interpretability",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16496.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.15747",
      "authors": [
        {
          "_id": "6799946c18cb282841d42639",
          "name": "Sankalp KJ",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263a",
          "name": "Ashutosh Kumar",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263b",
          "user": {
            "_id": "66707b60405252abeefd4c50",
            "avatarUrl": "/avatars/ee2728f115376e234e96820b8b376849.svg",
            "isPro": false,
            "fullname": "Laxmaan Balaji",
            "user": "laxmaanb",
            "type": "user"
          },
          "name": "Laxmaan Balaji",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-29T02:37:34.240Z",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263c",
          "name": "Nikunj Kotecha",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263d",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263e",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-29T08:55:14.312Z",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263f",
          "name": "Sreyoshi Bhaduri",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T03:19:03.000Z",
      "title": "IndicMMLU-Pro: 멀티 태스크 언어 이해에서 Indic 대 언어 모델의 벤치마크\n\n(Note: The original text \"IndicMMLU-Pro: マルチタスク語言理解におけるIndic大語言モデルのベンチマーク\" contains a Japanese character \"マルチタスク\" which was not translated as it was not part of the English text provided. If this is a typo or an error, please let me know.)",
      "summary": "인디아의 15억 이상의 인구가 아는 인디언 언어는 문화의 풍부한 전통, 언어의 다양성과 복잡한 구조로, 자연어 처리(NLP) 연구에 특징적인 문제를 제공하며, 기회를 제공하고 있습니다. IndicMMLU-Pro는 인디언 언어의 광범위한 언어에 대한 대규모 언어 모델(LLMs)의 평가를 위해 설계된 세부적인 벤치마크입니다. Magic Multitask Language Understanding(Massive Multitask Language Understanding) 프레임워크를 기반으로, Handi, Bengali, Gujarati, Marathi, Kannada, Punjabi, Tamil, Telugu, Urdu 등 주요 언어를 대상으로, 인디아의 언어 다양성에 따른 특징적인 문제를 해결합니다. 이 벤치마크는 언어 이해, 논리론리, 생성 등 광범위한 언어 처리의 복잡성을 감지하고, 인디언 언어의 특성을 극대화하여 잘捉える 것입니다. IndicMMLU-Pro는 인디언 언어의 AI 연구를 추진하기 위해 표준화된 평가 프레임워크를 제공하며, 정확한, 효율적인, 문화적으로 민감한 모델의 개발을 촉진합니다. 이 논문은 벤치마크의 설계 원칙, 태스크 기술, 데이터의 수집 방법 등을 설명하고, 가장 최신의 다언어 모델로부터 기준적인 결과를 제공합니다.",
      "upvotes": 3,
      "discussionId": "6799946e18cb282841d426d6"
    },
    "publishedAt": "2025-01-28T21:38:17.182Z",
    "title": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15747.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.16372",
      "authors": [
        {
          "_id": "67999c3dc1e34886f90320ee",
          "name": "J. Pablo Muñoz",
          "hidden": false
        },
        {
          "_id": "67999c3dc1e34886f90320ef",
          "name": "Jinjie Yuan",
          "hidden": false
        },
        {
          "_id": "67999c3dc1e34886f90320f0",
          "name": "Nilesh Jain",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-23T02:14:08.000Z",
      "title": "저 순위 어더퍼터와 뉴럴 아키텍처 검색이 LLM 압축에 합류합니다.",
      "summary": "LLM의 급격한 확장은 微調節와 部署に 필요한 계산 컴퓨팅 리소스에 대한 중요한 문제점을 드러냅니다. 저 레이닝گ 인더 어댑터의 최근 발전은 이러한 모델의 파라미터 효율적인 微調節 (PEFT)에 효과적이다는 것을 보여줍니다. 본 논문에서는 저 레이닝그 표현과 신경 아키텍처 검색 (NAS) 방법, 특히 가중치 공유 슈퍼네트워크와 상호작용을 기반으로 한 혁신적인 접근법을 전적으로 논의합니다. 이러한 방법을 통합하여, 대형 사전 학습 모델의 압축과 微調節의 강력한 해결책을 개발했습니다. 우리의 분석은 이러한 조합의 전략이 LLM의 사용을 민주화하고, 자원 제한 환경에서 部署로 더 접근 가능한 가능성을 밝혀줍니다. 결과적으로 얻은 모델은 메모리 플럿 프린트를 줄이고, 추론 시간을 단축하며, LLM의 실용적이고 scalable한 애플리케이션의 실현을 여닫습니다. 모델과 코드는 아래 URL에서 제공됩니다.\nhttps://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning",
      "upvotes": 2,
      "discussionId": "67999c3dc1e34886f9032140"
    },
    "publishedAt": "2025-01-28T22:11:04.472Z",
    "title": "Low-Rank Adapters Meet Neural Architecture Search for LLM Compression",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16372.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.17117",
      "authors": [
        {
          "_id": "6799e5f9121155210e4fa48c",
          "name": "Thibaud Leteno",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa48d",
          "name": "Irina Proskurina",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa48e",
          "name": "Antoine Gourru",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa48f",
          "name": "Julien Velcin",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa490",
          "name": "Charlotte Laclau",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa491",
          "name": "Guillaume Metzler",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa492",
          "name": "Christophe Gravier",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T18:07:30.000Z",
      "title": "모라럴알리멘트의 평가에 대한 프랑스어 데이터셋 \"모라럴노미티\"",
      "summary": "모델이 일상 생활에 따라 기능이 더 많아져야 하는 가운데, 모델의 언어가 인간적인 가치관에 맞도록 조정하는 것은 중요합니다. 모델은 일반적으로 사용자가 원하는대로 조정될 수 있지만, 현실적인 사회의 윤리 규범과 행동에 맞춰야 할 만큼 중요합니다. 영어나 중국어에서 발전이 큰 반면, 프랑스어에서는 이 분야에 조금의 관심을 가지고 있으며, 프랑스어로 LLM이 모라ル 논리를 어떻게 처리하는지에 대한 이해가 부족합니다. 이러한 부족을 해결하기 위해, 우리는 「Histoires Morales」의 프랑스어 데이터 세트를 소개합니다. 이 데이터 세트는 모라ル 가치의 어노테이션을 포함하며, 프랑스 문화의 맥락에 맞도록 작성되었습니다. 「Histoires Morales」는 기회의 차이, 관계 중의 정직성, 동물에게의 책임 등 다양한 사회 상황들을 커버하고 있습니다. 향후 연구를 위해, 우리는 프랑스어와 영어 데이터의 다언어 모델의 결합 방법과 그 강건성을 위한 초기 실험도 진행할 것입니다. 이러한 실험 결과에 따르면, LLM은 일반적으로 사용자의 선호에 따라 모라ル하거나 불윤리한 데이터에 쉽게 영향을 받습니다.",
      "upvotes": 0,
      "discussionId": "6799e5fb121155210e4fa500"
    },
    "publishedAt": "2025-01-29T03:32:09.927Z",
    "title": "Histoires Morales: A French Dataset for Assessing Moral Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17117.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "629a3dbcd496c6dcdebf41cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655113762275-629a3dbcd496c6dcdebf41cc.jpeg",
      "fullname": "Irina Proskurina",
      "name": "iproskurina",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]