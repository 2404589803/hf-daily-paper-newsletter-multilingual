[
  {
    "paper": {
      "id": "2504.17761",
      "authors": [
        {
          "_id": "680af2df3b93130c9b2b90a7",
          "name": "Shiyu Liu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90a8",
          "name": "Yucheng Han",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90a9",
          "name": "Peng Xing",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90aa",
          "name": "Fukun Yin",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ab",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ac",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:36.757Z",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ad",
          "name": "Jiaqi Liao",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ae",
          "name": "Yingming Wang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90af",
          "name": "Honghao Fu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b0",
          "name": "Chunrui Han",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b1",
          "name": "Guopeng Li",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b2",
          "name": "Yuang Peng",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b3",
          "name": "Quan Sun",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b4",
          "name": "Jingwei Wu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b5",
          "name": "Yan Cai",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b6",
          "name": "Zheng Ge",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b7",
          "name": "Ranchen Ming",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b8",
          "name": "Lei Xia",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b9",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ba",
          "name": "Yibo Zhu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bb",
          "name": "Binxing Jiao",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bc",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bd",
          "user": {
            "_id": "63417332c5565a4b8d43a0d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
            "isPro": false,
            "fullname": "Gang Yu",
            "user": "skicy",
            "type": "user"
          },
          "name": "Gang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:34.650Z",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90be",
          "name": "Daxin Jiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/lRHqqMDr1SxDfhelcO26J.mp4"
      ],
      "publishedAt": "2025-04-24T17:25:12.000Z",
      "submittedOnDailyAt": "2025-04-25T01:12:44.269Z",
      "title": "ステップ1X-편집: 일반적인 이미지 편집의 실용적인 프레임워크",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "최근, 이미지 편집 모델은 놀라운 빠른 발전을 보이고 있습니다. 최근, GPT-4o와 Gemini2 Flash 등 선진의 다중 모델의 공개로, 높은 이미지 편집 능력을 도입되었습니다. 이러한 모델들은 사용자가 요구하는 편집 요구 사항을 충족시키기 위해 뛰어난 능력을 보여주고, 이미지 조작 분야에서 큰 발전을 보여주고 있습니다. 그러나 이러한 폐쇄 소스 모델과 큰 차이점이 있습니다. 따라서, 본 논문에서는 Step1X-Edit라는 가장 선진적인 이미지 편집 모델을 공개하고, 폐쇄 소스 모델과 같은 성능을 제공하도록 시도합니다. 구체적으로는, 참조 이미지와 사용자의 편집 지시를 Multimodal LLM으로 처리하고, 잠재적인 임베딩을 추출하여, 차이화 이미지 디코더와 통합하여 목표 이미지를 얻습니다. 모델의 훈련에는 고품질 데이터셋을 생성하기 위한 데이터 생성 파이프라인을 구축하고, 평가에는 새로운 GEdit-Bench를 개발하여 실제 사용자 지시에 기반한 평가 기준을 사용했습니다. GEdit-Bench에서의 실험 결과를 통해, Step1X-Edit가 현재 오픈 소스 기반의 baseline보다 크게 우위를示し, 선진적인 폐쇄 소스 모델의 성능에 근접하여, 이미지 편집 분야에 큰 기여를 합니다.",
      "upvotes": 38,
      "discussionId": "680af2e13b93130c9b2b9132",
      "githubRepo": "https://github.com/stepfun-ai/Step1X-Edit",
      "ai_keywords": [
        "Multimodal LLM",
        "latent embedding",
        "diffusion image decoder",
        "data generation pipeline",
        "GEdit-Bench",
        "real-world user instructions"
      ]
    },
    "publishedAt": "2025-04-24T13:25:12.000Z",
    "title": "Step1X-Edit: A Practical Framework for General Image Editing",
    "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/lRHqqMDr1SxDfhelcO26J.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17761.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17502",
      "authors": [
        {
          "_id": "680b44fb426b7d5bc2018c75",
          "user": {
            "_id": "631da07f6d6a5870f3d2c375",
            "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
            "isPro": false,
            "fullname": "Aviv Slobodkin",
            "user": "lovodkin93",
            "type": "user"
          },
          "name": "Aviv Slobodkin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-25T08:17:03.471Z",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c76",
          "name": "Hagai Taitelbaum",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c77",
          "name": "Yonatan Bitton",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c78",
          "name": "Brian Gordon",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c79",
          "name": "Michal Sokolik",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7a",
          "name": "Nitzan Bitton Guetta",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7b",
          "name": "Almog Gueta",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7c",
          "name": "Royi Rassin",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7d",
          "name": "Itay Laish",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7e",
          "name": "Dani Lischinski",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7f",
          "name": "Idan Szpektor",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T12:44:51.000Z",
      "submittedOnDailyAt": "2025-04-25T06:50:21.552Z",
      "title": "RefVNLI: 주제를 주도한 문장으로부터 이미지로 변환하는 교환 가능한 평가에 대한 연구",
      "submittedOnDailyBy": {
        "_id": "631da07f6d6a5870f3d2c375",
        "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
        "isPro": false,
        "fullname": "Aviv Slobodkin",
        "user": "lovodkin93",
        "type": "user"
      },
      "summary": "주제 주도의 텍스트에서 이미지(T2I) 생성은 주어진 텍스트 설명과 일치하는 동시에, 참조 이미지에서 시각적인 정체성을 유지하여 이미지를 생성하는 것을 목표로 합니다. 그 광범위한 다운 스트리밍의 적용 범위는 이미지 생성의 확장된 개인화부터, 영상 렌더링에서의 일관된 캐릭터 표현까지 이어갑니다. 그러나 이 분야의 발전은 신뢰성 있는 자동 평가의 부족으로 제한되어 있습니다. 현재의 방법들은 작업의 일부를 평가할 뿐만 아니라, 텍스트의 일치성 또는 주제의 유지를 제외한 다른 측면을 평가할 수 없으며, 이는 인간 판단에 의존하거나, 비용 높은 API 기반 평가에 의존하는 것입니다. 이에 대처하여, 우리는 RefVNLI를 소개합니다. RefVNLI는 하나의 예측에서 텍스트의 일치성과 주제의 유지를 동시에 평가하는 저비용의 메트릭입니다. 비디오 논리 벤치마크에서부터 큰 데이터 세트와 이미지의 변동으로부터 학습되어 있습니다. RefVNLI는 현재의 기준을 초월하거나, 유사한 수준을 달성할 수 있으며, 텍스트의 일치성에는 최대 6.4점의 이득, 주제의 일치성에는 최대 8.5점의 이득을 얻습니다. 또한, 모르는 개념에서도 우수한 성능을 보입니다. 87% 이상의 정확도로 인간들의 취향에 맞습니다.",
      "upvotes": 35,
      "discussionId": "680b44ff426b7d5bc2018d85",
      "ai_keywords": [
        "RefVNLI",
        "video-reasoning benchmarks",
        "image perturbations"
      ]
    },
    "publishedAt": "2025-04-24T08:44:51.000Z",
    "title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image\n  Generation",
    "summary": "Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability --\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering -- progress in this field is\nlimited by the lack of reliable automatic evaluation. Existing methods either\nassess only one aspect of the task (i.e., textual alignment or subject\npreservation), misalign with human judgments, or rely on costly API-based\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\nevaluates both textual alignment and subject preservation in a single\nprediction. Trained on a large-scale dataset derived from video-reasoning\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\nbaselines across multiple benchmarks and subject categories (e.g.,\nAnimal, Object), achieving up to 6.4-point gains in textual\nalignment and 8.5-point gains in subject consistency. It also excels with\nlesser-known concepts, aligning with human preferences at over 87\\% accuracy.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631da07f6d6a5870f3d2c375",
      "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
      "fullname": "Aviv Slobodkin",
      "name": "lovodkin93",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17192",
      "authors": [
        {
          "_id": "680aee7bcf67477f2c00ca53",
          "user": {
            "_id": "64f7bf0c7565a69eb693ad1f",
            "avatarUrl": "/avatars/aba6910aa39a3437a7f0df3f5cd49e6d.svg",
            "isPro": false,
            "fullname": "minju",
            "user": "iaminju",
            "type": "user"
          },
          "name": "Minju Seo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:41.304Z",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca54",
          "user": {
            "_id": "63036b6c5c70c21d0ea79d48",
            "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
            "isPro": false,
            "fullname": "Jinheon Baek",
            "user": "jinheon",
            "type": "user"
          },
          "name": "Jinheon Baek",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:38.982Z",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca55",
          "name": "Seongyun Lee",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca56",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T01:57:01.000Z",
      "submittedOnDailyAt": "2025-04-25T04:17:48.790Z",
      "title": "Paper2Code: 과학 논문에서 코드 자동 생성 (머신 학습)",
      "submittedOnDailyBy": {
        "_id": "6550c4f27bbfce1878f5f280",
        "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
        "isPro": false,
        "fullname": "seongyun_lee",
        "user": "Seongyun",
        "type": "user"
      },
      "summary": "머신러닝 연구의 급속한 발전에 따라, 이에 대응하는 코드 구현이 일반적으로 사용할 수 없기 때문에, 연구자들은 결과를 재현하고 선행 연구에 기반하여 확장하는 데 시간이 오래 걸리고 많은 노력을 필요로 한다. 반면, 최근의 대규모 언어 모델(LLMs)은 과학 논문을 이해하고 고품질의 코드를 생성할 수 있습니다. 이러한 점에 힘입어, PaperCoder라는 다 에이전트 LLM 프레임워크를 통해 기계학습 논문을 기능적인 코드 리포지토리로 변환하는 방법을 제안합니다. PaperCoder는 계획, 분석, 생성의 3단계로 동작합니다. 계획 단계에서는 고レ벨의 맵을 구축하고 시스템 아키텍처를 다이어그램으로 설계하고 파일의 의존관계를 특정하고 설정 파일을 생성합니다. 분석 단계에서는 구현에 특화된 세부 사항을 해석하고 생성 단계에서는 모듈화된, 의존관계를 알고 있는 코드가 생성됩니다. 또한, 각 단계는 전문적인 에이전트 세트로 인스턴스화되어, 파이프라인에서 횡단적으로 효과적으로 협력합니다. 또한, PaperCoder는 기계학습 논문으로부터의 코드 구현을 생성하는 데 대해 모델 기반 평가와 인간 평가를 사용하여 평가하고, 논문의 원 저자로부터의 분배가 사용할 수 있는 경우 그 효과를 보여주는 데 효과적이며, 최근의 PaperBench 벤치마크에서 강력한 베이스라인을 크게 초월하는 강력한 능력을 보여줍니다.",
      "upvotes": 31,
      "discussionId": "680aee7dcf67477f2c00ca96",
      "githubRepo": "https://github.com/going-doer/Paper2Code"
    },
    "publishedAt": "2025-04-23T21:57:01.000Z",
    "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning",
    "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17192.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6550c4f27bbfce1878f5f280",
      "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
      "fullname": "seongyun_lee",
      "name": "Seongyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17432",
      "authors": [
        {
          "_id": "680adfbe464a44cea0b843c1",
          "name": "Tiancheng Gu",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c2",
          "user": {
            "_id": "63e202f352b7578dba448ab5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
            "isPro": false,
            "fullname": "Yang",
            "user": "Kaichengalex",
            "type": "user"
          },
          "name": "Kaicheng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:46.935Z",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c3",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c4",
          "name": "Xingjun Wang",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c5",
          "name": "Yanzhao Zhang",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c6",
          "name": "Dingkun Long",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c7",
          "name": "Yingda Chen",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c8",
          "name": "Weidong Cai",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c9",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T10:51:52.000Z",
      "submittedOnDailyAt": "2025-04-25T01:11:53.967Z",
      "title": "모듈의 벽을 깨고: 다모듈 LLM에 의한 일반적인 내장 학습",
      "submittedOnDailyBy": {
        "_id": "63e202f352b7578dba448ab5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
        "isPro": false,
        "fullname": "Yang",
        "user": "Kaichengalex",
        "type": "user"
      },
      "summary": "대조적인 언어 이미지 이미지(CLIP) 프레임워크는 주로 이미지 문서 검색과 클러스터링에 사용되어 있지만, 3가지의 요인(문서 토큰 트랜잭션, 분리된 이미지 문서 인코딩, 낱말 낱말 행동으로 인한 구조적 결함)으로 인해 효과성은 제한되어 있습니다. 최근의 다 모델 대 언어 모델(MLLM)은 일반화된 시각 언어 이해에 큰 진전을 보였지만, 동적인 다 모델 표현 학습의 가능성은 아직 조사가 부족합니다. 본 논문에서는 MLLM을 사용하여 다양한 하류 태스크에 대한 판단적 표현을 학습하기 위한 새로운 2단계 프레임워크인 Universal Multimodal Embedding(UniME)를 제안합니다. 첫 번째 단계는 강력한 LLM 기반의 교사 모델으로부터 언어 성분의 내장 능력을 높이기 위해 언어적 판단적 지식을 수용합니다. 두 번째 단계는 어려운 부정 예제를 강화한 지시 조정을 도입하여 판단적 표현 학습을 촉진합니다. 특히, 처음에 오류 부정 예제의 오염을 줄이고, 각 배치 내의 각 인스턴스에 대해 여러 가지 어려운 부정 예제를 샘플링하여 모델을 어려운 샘플에 집중하도록 시도합니다. 이 접근법은 판단력을 향상시키면서 하류 태스크의 지시 따라수행 능력도 높입니다. MMEB 벤치마크와 여러 검색 태스크에 대한 확장된 실험을 수행하고, 모든 태스크에서 통일된 성능 향상을 보여주며, 우수한 판단적과 구조적 능력을 보였습니다.",
      "upvotes": 21,
      "discussionId": "680adfbf464a44cea0b8440f",
      "projectPage": "https://garygutc.github.io/UniME/",
      "githubRepo": "https://github.com/deepglint/UniME",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training (CLIP)",
        "Multimodal Large Language Models (MLLMs)",
        "Generalized vision-language understanding",
        "UniME (Universal Multimodal Embedding)",
        "Discriminative representations",
        "Textual discriminative knowledge distillation",
        "LLM-based teacher model",
        "Hard negative enhanced instruction tuning",
        "False negative contamination",
        "Challenging samples",
        "Discriminative power",
        "Instruction-following ability",
        "MMEB benchmark",
        "Short caption retrieval",
        "Long caption retrieval",
        "Compositional retrieval"
      ]
    },
    "publishedAt": "2025-04-24T06:51:52.000Z",
    "title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs",
    "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17432.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17207",
      "authors": [
        {
          "_id": "680af2bf2fa10fbf21684bde",
          "name": "Phillip Y. Lee",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684bdf",
          "name": "Jihyeon Je",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be0",
          "name": "Chanho Park",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be1",
          "name": "Mikaela Angelina Uy",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be2",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be3",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T02:41:34.000Z",
      "submittedOnDailyAt": "2025-04-25T00:59:29.327Z",
      "title": "시점별 이유론을 통한 정신적 이미지의 시뮬레이션 (시점별 이유론을 통한 정신적 이미지 시뮬레이션)",
      "submittedOnDailyBy": {
        "_id": "6342796a0875f2c99cfd313b",
        "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
        "isPro": false,
        "fullname": "Yuseung \"Phillip\" Lee",
        "user": "phillipinseoul",
        "type": "user"
      },
      "summary": "우리는 시각 언어 모델(VLMs)에서 시각 관계 인식을 통해 想象 记忆를 사용한 프레임워크를 제안합니다. 시각 관계 인식은 인간 수준의 시각 이해의 중요한 기준이며, 환경 상호작용과 자율 차량 에이전트의 협력이 필요합니다. VLMs의 공간 인식의 발전에 따라 최근의 연구는 현대의 VLMs가 시각 관계 인식 능력을 크게 부족하고, 중심적인 해석에 강한 편향을 나타내는 것을 밝혀 냈습니다. VLMs와 인간의 시각 인지 사이에 간극을 메우는 데是想象 记忆의 역할을 중시합니다. 인간은 추상화된 표현을 통해 세상을 보는 것이고, 시각의 이동을 촉진합니다. 이를 기반으로 시각 기반 모델(객체 탐색, 분할, 방향 측정)을 활용하여 공간 추상화와 시각 관계 변환을 가능하게 하는 \"추상 시각 관계 변환(APC)\"의 프레임워크를 제안합니다. 합성 이미지와 실상 이미지의 벤치마크에서 실험을 통해 VLMs와 비교하여 프레임워크를 사용한 시각 관계 인식에서 뚜렷한 향상을 보여주고, 공간 인식의 미세 조정 모델과 새로운 시각 포인트 합성 기반의 접근 방식을 더욱 뛰어넘는 것을 보여줍니다.",
      "upvotes": 14,
      "discussionId": "680af2c02fa10fbf21684c1f",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "mental imagery simulation",
        "perspective-taking",
        "visual understanding",
        "environmental interaction",
        "autonomous agents",
        "spatial reasoning",
        "perspective-aware reasoning capabilities",
        "egocentric interpretations",
        "mental imagery",
        "scene abstractions",
        "perspective transformations",
        "object detection",
        "segmentation",
        "orientation estimation",
        "synthetic benchmarks",
        "real-image benchmarks",
        "fine-tuned spatial reasoning models",
        "novel-view-synthesis-based approaches"
      ]
    },
    "publishedAt": "2025-04-23T22:41:34.000Z",
    "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery\n  Simulation",
    "summary": "We present a framework for perspective-aware reasoning in vision-language\nmodels (VLMs) through mental imagery simulation. Perspective-taking, the\nability to perceive an environment or situation from an alternative viewpoint,\nis a key benchmark for human-level visual understanding, essential for\nenvironmental interaction and collaboration with autonomous agents. Despite\nadvancements in spatial reasoning within VLMs, recent research has shown that\nmodern VLMs significantly lack perspective-aware reasoning capabilities and\nexhibit a strong bias toward egocentric interpretations. To bridge the gap\nbetween VLMs and human perception, we focus on the role of mental imagery,\nwhere humans perceive the world through abstracted representations that\nfacilitate perspective shifts. Motivated by this, we propose a framework for\nperspective-aware reasoning, named Abstract Perspective Change (APC), that\neffectively leverages vision foundation models, such as object detection,\nsegmentation, and orientation estimation, to construct scene abstractions and\nenable perspective transformations. Our experiments on synthetic and real-image\nbenchmarks, compared with various VLMs, demonstrate significant improvements in\nperspective-aware reasoning with our framework, further outperforming\nfine-tuned spatial reasoning models and novel-view-synthesis-based approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17207.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6342796a0875f2c99cfd313b",
      "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
      "fullname": "Yuseung \"Phillip\" Lee",
      "name": "phillipinseoul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16511",
      "authors": [
        {
          "_id": "680b2a95c94724c1465c20dd",
          "name": "Fengze Liu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20de",
          "name": "Weidong Zhou",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20df",
          "name": "Binbin Liu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e0",
          "name": "Zhimiao Yu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e1",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e2",
          "name": "Haobin Lin",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e3",
          "name": "Yifeng Yu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e4",
          "name": "Xiaohuan Zhou",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e5",
          "name": "Taifeng Wang",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e6",
          "name": "Yong Cao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T08:36:50.000Z",
      "submittedOnDailyAt": "2025-04-25T04:55:18.773Z",
      "title": "QuaDMix: 질량・다양성 균형을 이루는 데이터 선택에 의한 효율적인 LLM 사전 학습",
      "submittedOnDailyBy": {
        "_id": "668f5875b5b3081d776e4094",
        "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
        "isPro": false,
        "fullname": "Xiaohuan Zhou",
        "user": "XiaohuanZhou",
        "type": "user"
      },
      "summary": "품질과 다양성은 대규모 언어 모델(LLMs)의 훈련 데이터에서 중요한 두 가지 지표이며, 성능 향상에 긍정적인 영향을 미칩니다. 현재의 연구는 이러한 지표를 각각 최적화하고 있지만, 일반적으로 품질 필터링을 수행하고 그 후 데이터의 비율을 조정합니다. 그러나 이러한 접근法是 품질과 다양성의 고유한 트레이드오프를 무시하고 있습니다. 고정된 훈련 배분 가정으로, 각 데이터 포인트의 품질과 데이터 세트 전체의 보완적 효과를 평가하는 것이 중요합니다. 본 논문에서는, QuaDMix라는 통합적인 데이터 선택 프레임워크를 통해 LLM의 사전 훈련에 대해 데이터 분포를 자동적으로 최적화하고 동시에 품질과 다양성을 균형을 이루는 것을 목표로 합니다. 특히, 데이터의 품질을 측정하기 위해 여러 기준을 제안하고, 데이터 포인트를 구분하기 위해 영역 분류를 사용하며 전체의 다양성을 측정합니다. QuaDMix는 이러한 품질과 다양성에 관련된 라벨을 기반으로, 각 데이터 포인트의 샘플링 확률을 결정하기 위한 통합적인 파라미터화 데이터 샘플링 함수를 사용합니다. QuaDMix 프레임워크에 대한 최적화 파라미터의 탐색을 가속화하기 위해, 작은 모델에 대한 컴퓨터 실험을 수행하고 RegMix 미신은 LightGBM을 사용합니다. 다양한 모델과 데이터 세트의 범위에서 실험 결과를 통해, QuaDMix는 평균 7.2%의 성능 향상을 달성했습니다. 이러한 결과를 통해, 독립적인 품질과 다양성 전략에 비해 우수하며, 데이터의 품질과 다양성의 균형의 필요성과 가능성을 명확히 합니다.",
      "upvotes": 13,
      "discussionId": "680b2a97c94724c1465c21a3",
      "ai_keywords": [
        "large language models (LLMs)",
        "QuaDMix",
        "data selection framework",
        "parameterized data sampling function",
        "domain classification",
        "LightGBM",
        "RegMix"
      ]
    },
    "publishedAt": "2025-04-23T04:36:50.000Z",
    "title": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining",
    "summary": "Quality and diversity are two critical metrics for the training data of large\nlanguage models (LLMs), positively impacting performance. Existing studies\noften optimize these metrics separately, typically by first applying quality\nfiltering and then adjusting data proportions. However, these approaches\noverlook the inherent trade-off between quality and diversity, necessitating\ntheir joint consideration. Given a fixed training quota, it is essential to\nevaluate both the quality of each data point and its complementary effect on\nthe overall dataset. In this paper, we introduce a unified data selection\nframework called QuaDMix, which automatically optimizes the data distribution\nfor LLM pretraining while balancing both quality and diversity. Specifically,\nwe first propose multiple criteria to measure data quality and employ domain\nclassification to distinguish data points, thereby measuring overall diversity.\nQuaDMix then employs a unified parameterized data sampling function that\ndetermines the sampling probability of each data point based on these quality\nand diversity related labels. To accelerate the search for the optimal\nparameters involved in the QuaDMix framework, we conduct simulated experiments\non smaller models and use LightGBM for parameters searching, inspired by the\nRegMix method. Our experiments across diverse models and datasets demonstrate\nthat QuaDMix achieves an average performance improvement of 7.2% across\nmultiple benchmarks. These results outperform the independent strategies for\nquality and diversity, highlighting the necessity and ability to balance data\nquality and diversity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16511.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668f5875b5b3081d776e4094",
      "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
      "fullname": "Xiaohuan Zhou",
      "name": "XiaohuanZhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17789",
      "authors": [
        {
          "_id": "680b318bbbebf87944bc9595",
          "name": "Xu Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9596",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9597",
          "name": "Haoyu Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9598",
          "name": "Hao Tang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9599",
          "name": "Chih-Yao Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959a",
          "name": "Jialiang Wang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959b",
          "name": "Kunpeng Li",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959c",
          "name": "Xiaoliang Dai",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959d",
          "name": "Yujun Shi",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959e",
          "name": "Xuan Ju",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959f",
          "name": "Yushi Hu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a0",
          "name": "Artsiom Sanakoyeu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a1",
          "name": "Felix Juefei-Xu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a2",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a3",
          "name": "Junjiao Tian",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a4",
          "name": "Tao Xu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a5",
          "name": "Tingbo Hou",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a6",
          "name": "Yen-Cheng Liu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a7",
          "name": "Zecheng He",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a8",
          "name": "Zijian He",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a9",
          "name": "Matt Feiszli",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95aa",
          "name": "Peizhao Zhang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ab",
          "name": "Peter Vajda",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ac",
          "name": "Sam Tsai",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ad",
          "name": "Yun Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-25T05:28:51.493Z",
      "title": "토큰 셔플： 자동 리턴 모델을 활용한 고해상도 이미지 생성에 관한 연구",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "自動회귀(AR) 모델은 언어 생성에서 장기적으로 지배적이며, 이미지 합성에서도 점점 더 적용되고 있지만, 이 모델은 분산 기반 모델과 비교했을 때 많은 문제를 가지고 있다고 생각됩니다. 주요한 한계점은 AR 모델이 필요하는 이미지 토큰의 수가 많은 것입니다. 이는 훈련 및 추론의 효율성과 이미지의 해상도에 제약을 가합니다. 이를 해결하기 위해, 우리는 토큰 셔플을 제안합니다. 이 방법은 새로운 방법일 수 있지만 간단하고, 트랜스포머의 이미지 토큰의 수를 줄이는 데 사용됩니다. 우리의 주요 아이디어는 멀티모달 대 언어 모델(MLLMs)에서 시각적 어휘의 차원冗長성입니다. 여기에서는 시각적 인코더에서 낮은 차원의 시각 코드는 직접 높은 차원의 언어 어휘에 매핑됩니다. 이를 활용하여, 우리는 두 가지 주요한 작업을 고려합니다: 토큰 셔플과 토큰 엮기. 토큰 셔플은 스펙트럴 차원에 따라 공간적으로 지역적인 토큰을 통합하고, 입력 토큰의 수를 줄입니다. 토큰 엮기는 트랜스포머 블록 이후 추론된 토큰을 연결하고, 출력의 공간적인 순서를 복원합니다. 문맥플랜트와 함께 훈련하는 것은 우리의 전략은 추가 훈련된 문맥 인코더를 필요로 하지 않습니다. 이로 인해, MLLMs은 언어와 이미지의 통합된 다음 토큰 예측 방법으로 매우 높은 해상도의 이미지 합성을 지원할 수 있습니다. 아직 처음으로, 우리는 AR 텍스트에서 이미지 생성의 경계를 2048x2048 해상도까지 추진하고, 만족스러운 생성 성능을 얻었습니다. GenAI-benchmark에서, 우리의 2.7B 모델은 어려운 플랜트에서 전체 스코어 0.77을 달성했으며, AR 모델 LlamaGen의 0.18과 분산 기반 모델 LDM의 0.15를 초과했습니다. 자세한 규모의 인간 평가에서도, 언어 대응성, 시각적 결함, 시각적 외관면에서 우리의 눈에 띄는 이미지 생성 능력이 나타났습니다. 우리는 토큰 셔플이 MLLM 내의 효율적인 고해상도 이미지 생성의 기초적인 설계에役立つ 것을 기대합니다.",
      "upvotes": 4,
      "discussionId": "680b3191bbebf87944bc9739",
      "ai_keywords": [
        "autoregressive (AR) models",
        "image synthesis",
        "diffusion-based models",
        "image tokens",
        "training and inference efficiency",
        "Transformer",
        "dimensional redundancy",
        "visual vocabularies",
        "Multimodal Large Language Models (MLLMs)",
        "visual encoder",
        "high-dimensional language vocabularies",
        "token-shuffle",
        "spatially local tokens",
        "channel dimension",
        "token-unshuffle",
        "spatial arrangement",
        "unified next-token prediction",
        "text-to-image generation",
        "resolution",
        "generation performance",
        "GenAI-benchmark",
        "textual prompts",
        "pretrained text-encoder",
        "text-alignment",
        "visual flaw",
        "visual appearance"
      ]
    },
    "publishedAt": "2025-04-24T13:59:56.000Z",
    "title": "Token-Shuffle: Towards High-Resolution Image Generation with\n  Autoregressive Models",
    "summary": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17789.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17069",
      "authors": [
        {
          "_id": "680b1b33388bb2cfd497ebdb",
          "user": {
            "_id": "62bb84f82ada492aa5775709",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62bb84f82ada492aa5775709/jv4yKL75t8QzHDHLbhBPT.png",
            "isPro": false,
            "fullname": "Rishav Pramanik",
            "user": "rishavpramanik",
            "type": "user"
          },
          "name": "Rishav Pramanik",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:27.724Z",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdc",
          "name": "Antoine Poupon",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdd",
          "name": "Juan A. Rodriguez",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebde",
          "name": "Masih Aminbeidokhti",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdf",
          "name": "David Vazquez",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe0",
          "name": "Christopher Pal",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe1",
          "name": "Zhaozheng Yin",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe2",
          "name": "Marco Pedersoli",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/NUImFkmaqDnfEIBB2l94Q.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/OuC3dzu5XCUb8dxTlVf72.png"
      ],
      "publishedAt": "2025-04-23T19:33:58.000Z",
      "submittedOnDailyAt": "2025-04-25T03:50:09.534Z",
      "title": "단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순한 자동 함수를 사용합니다. 단순",
      "submittedOnDailyBy": {
        "_id": "63a614d264f470027818b066",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
        "isPro": false,
        "fullname": "Juan A. Rodriguez",
        "user": "joanrodai",
        "type": "user"
      },
      "summary": "自動 순차 폼 밋 기반 이미지 생성은 최근에 비교하여 이미지 품질과 스케일러빌리티에서 상대적으로 우수한 결과를 보여주고 있습니다. 이러한 모델들은 시각 언어 모델 내에서 쉽게 통합될 수 있으며, 스케일링도 가능합니다. 그러나 자동 순차 모델은 폼의 생성에 정의된 순서가 필요합니다. 자연스러운 순서는 단어의 지시에 기반한 것입니다. 이는 문장 생성에 의미가 있지만, 이미지 생성에는 고유의 생성 순서가 존재하지 않습니다. 전통적으로는 라스터 스캔 순서(왼쪽 위부터 오른쪽 아래까지)가 자동 순차 이미지 생성 모델을 가이드하고 있습니다. 본 논문에서는 이 순서는 가장 적합하지 않다고 주장하고, 이미지의 내용을因果관계에 따라 존중하지 않습니다: 예를 들어, 일몰의 시각적 설명에 기반하여, 자동 순차 모델은 太陽 앞에 구름을 생성하는 가능성이 있습니다. 그러나 구름의 색은 太陽의 색에 의해 결정되어야 하므로, 반대로 太陽의 색에 의해 결정됩니다. 본 논문에서는 폼의 생성 순서를 임의로 학습시켜 생성 중 폼의 내용과 위치(순서)를 추론할 수 있음을 보여줍니다. 그리고 이러한 추출된 순서를 사용하여, 임의의 순서 모델을 조정하여 더 높은 품질의 이미지를 생성할 수 있음을 보여줍니다. 실험에 따르면, 본 논문에서는 이 새로운 생성 방법을 통해 전통적인 라스터 스캔 접근보다 더 좋은 이미지를 생성할 수 있음을 두 개의 데이터셋에서 보여주고, 동일한 학습 비용과 추가적인 어노테이션이 필요하지 않다고 나타냅니다.",
      "upvotes": 4,
      "discussionId": "680b1b35388bb2cfd497ec76",
      "ai_keywords": [
        "autoregressive patch-based image generation",
        "Vision-Language models",
        "raster-scan order",
        "causality",
        "any-given-order",
        "patch content",
        "patch location",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-04-23T15:33:58.000Z",
    "title": "Distilling semantically aware orders for autoregressive image generation",
    "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/NUImFkmaqDnfEIBB2l94Q.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/OuC3dzu5XCUb8dxTlVf72.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17069.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a614d264f470027818b066",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
      "fullname": "Juan A. Rodriguez",
      "name": "joanrodai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17040",
      "authors": [
        {
          "_id": "680af0c4175842e433ae348e",
          "name": "Zhenhailong Wang",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae348f",
          "name": "Senthil Purushwalkam",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3490",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3491",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3492",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3493",
          "name": "Ran Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T18:38:18.000Z",
      "submittedOnDailyAt": "2025-04-25T06:12:13.135Z",
      "title": "Dynamic Merging and Basic American Merging의 효율적인 VLMs 구현方法",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "DyMU는 비전 언어 모델(VLMs)의 계산 부담을 효율적으로 줄이고 높은 태스크 성능을 유지하기 위한 훈련하지 않는 프레임워크입니다. 우리의 접근 방식은 두 가지 주요 구성 요소로 구성됩니다. 1. 동적 토큰 머린링(DToMe)은 이미지의 복잡성에 기반하여 유사한 토큰을 통합하여 비전 트랜스포머에서 고정 길이의 출력의 부적절성을 해결합니다. 2. 뷰트 토큰 유니메이징(VTU)은 대규모 언어 모델(LLMs)이 기대하는 토큰 시퀀스를 시뮬레이션하여 모든 시퀀스의 注意 동작을 효율적으로 재구성하여 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일 타입의 파일",
      "upvotes": 3,
      "discussionId": "680af0c7175842e433ae3544",
      "ai_keywords": [
        "Dynamic Token Merging (DToMe)",
        "Virtual Token Unmerging (VTU)",
        "vision transformers",
        "token compression",
        "attention dynamics",
        "visual encoders",
        "image complexity",
        "computational costs"
      ]
    },
    "publishedAt": "2025-04-23T14:38:18.000Z",
    "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs",
    "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17040.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16921",
      "authors": [
        {
          "_id": "680b40774d69b6950c4eabda",
          "name": "José Ángel González",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdb",
          "name": "Ian Borrego Obrador",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdc",
          "name": "Álvaro Romo Herrero",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdd",
          "name": "Areg Mikael Sarvazyan",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabde",
          "user": {
            "_id": "60f95c8fda0985b973d59d77",
            "avatarUrl": "/avatars/5606f0191b9f86e6b55f7e5ab6cc8bb6.svg",
            "isPro": false,
            "fullname": "Mara Chinea Rios",
            "user": "mchinea",
            "type": "user"
          },
          "name": "Mara Chinea-Ríos",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-25T07:58:32.606Z",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdf",
          "name": "Angelo Basile",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabe0",
          "name": "Marc Franco-Salvador",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T17:48:25.000Z",
      "submittedOnDailyAt": "2025-04-25T06:28:29.231Z",
      "title": "IberBench: イベリア어의 LLM 평가",
      "submittedOnDailyBy": {
        "_id": "62308d13e4673fe4985d7fc9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655555697880-62308d13e4673fe4985d7fc9.jpeg",
        "isPro": false,
        "fullname": "Areg Mikael Sarvazyan",
        "user": "asarvazyan",
        "type": "user"
      },
      "summary": "대 언어 모델(LLMs)는, 특히 영어 외의 언어에서 고품질의 데이터가 제한되어 있기 때문에, 전반적으로 평가가 어려워진다. 현재의 벤치마크와 리더보드는 영어 중심적이고, 다른 언어에 대한 것은 적은다. 이러한 벤치마크들은 언어의 다양성을 뺏고, 산업적인 가치의 태스크보다 기초적인 자연어 처리(NLP)의 능력을 우선시하고, 고정되어 있는 점이 부족하다. 이러한 점을 인지한 우리들은 IberBench를 소개합니다. IberBench는 Iberian Peninsula와 Ibero-America에서 사용하는 언어의 양쪽의 기본적이고 산업적인 NLP 태스크에서 LLM의 성능을 평가하기 위해 상세하고 확장 가능한 벤치마크입니다. IberBench는 101 데이터 세트를 통합하고, 감정 분석, 독성 검출, 요약 등 22 태스크 카테고리를 커버하고 있습니다. 이 벤치마크는 현재 평가 프로세스의 부족점을 해결하고, 언어의 다양성과 고정된 평가 준비의 부족점을 해결하기 위해, 커뮤니티 주도의 모델과 데이터 세트를 공동체 전문가로 조정하여 연속적인 업데이트를 가능하게 해줍니다. 23개의 LLM(10억부터 140억 파라미터)을 평가하고, 그 강점과 부족점을 실험적인 통찰을 제공합니다. 우리들의 발견은 (i) LLM은 산업적인 태스크에서 기본적인 태스크보다 성능이 나빠서, (ii) Galician과 Basque의 평균적인 성능이 낮고, (iii) 일부 태스크에서 랜덤한 결과를 근접하며, (iv) 다른 태스크에서 랜덤보다 높지만, 공유 태스크 시스템보다 낮은 것을 보여줍니다. IberBench는 데이터 세트의 정규화와 Hosting, LLM의 인크리멘탈 평가, 공개적으로 접근 가능한 리더보드를 포함하는 오픈 소스 구현을 제공합니다.",
      "upvotes": 3,
      "discussionId": "680b407a4d69b6950c4eac96",
      "projectPage": "https://huggingface.co/spaces/iberbench/leaderboard",
      "githubRepo": "https://github.com/IberBench",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "sentiment and emotion analysis",
        "toxicity detection",
        "summarization",
        "IberBench",
        "dataset normalization",
        "incremental evaluation",
        "leaderboard"
      ]
    },
    "publishedAt": "2025-04-23T13:48:25.000Z",
    "title": "IberBench: LLM Evaluation on Iberian Languages",
    "summary": "Large Language Models (LLMs) remain difficult to evaluate comprehensively,\nparticularly for languages other than English, where high-quality data is often\nlimited. Existing benchmarks and leaderboards are predominantly\nEnglish-centric, with only a few addressing other languages. These benchmarks\nfall short in several key areas: they overlook the diversity of language\nvarieties, prioritize fundamental Natural Language Processing (NLP)\ncapabilities over tasks of industrial relevance, and are static. With these\naspects in mind, we present IberBench, a comprehensive and extensible benchmark\ndesigned to assess LLM performance on both fundamental and industry-relevant\nNLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.\nIberBench integrates 101 datasets from evaluation campaigns and recent\nbenchmarks, covering 22 task categories such as sentiment and emotion analysis,\ntoxicity detection, and summarization. The benchmark addresses key limitations\nin current evaluation practices, such as the lack of linguistic diversity and\nstatic evaluation setups by enabling continual updates and community-driven\nmodel and dataset submissions moderated by a committee of experts. We evaluate\n23 LLMs ranging from 100 million to 14 billion parameters and provide empirical\ninsights into their strengths and limitations. Our findings indicate that (i)\nLLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)\nperformance is on average lower for Galician and Basque, (iii) some tasks show\nresults close to random, and (iv) in other tasks LLMs perform above random but\nbelow shared task systems. IberBench offers open-source implementations for the\nentire evaluation pipeline, including dataset normalization and hosting,\nincremental evaluation of LLMs, and a publicly accessible leaderboard.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62308d13e4673fe4985d7fc9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655555697880-62308d13e4673fe4985d7fc9.jpeg",
      "fullname": "Areg Mikael Sarvazyan",
      "name": "asarvazyan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15921",
      "authors": [
        {
          "_id": "680ab2f808464b525df64b07",
          "name": "Jian Hu",
          "hidden": false
        },
        {
          "_id": "680ab2f808464b525df64b08",
          "name": "Dimitrios Korkinof",
          "hidden": false
        },
        {
          "_id": "680ab2f808464b525df64b09",
          "name": "Shaogang Gong",
          "hidden": false
        },
        {
          "_id": "680ab2f808464b525df64b0a",
          "name": "Mariano Beguerisse-Diaz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T14:06:01.000Z",
      "submittedOnDailyAt": "2025-04-25T06:39:24.280Z",
      "title": "ViSMaP: 비 서브젝트 시간장 비디오의 경험적 프로닝에 의한 비 매뉴얼 축소",
      "submittedOnDailyBy": {
        "_id": "65e1b6e9501590df0173cbd3",
        "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
        "isPro": false,
        "fullname": "Jian Hu",
        "user": "lwpyh",
        "type": "user"
      },
      "summary": "ビズマップ（ViSMap）：無マニューバックビデオ要約化を行うメタプロンプティングシステムを紹介します。このシステムは、無マニューバックで1時間のビデオを要約することができます。現在のビデオ理解モデルは、短いビデオや事前にセグメントされたイベントのビデオではよく効果的ですが、長いビデオでは、関連するイベントが稀疏に分布して事前にセグメントされていないため、要約が困難です。また、長ビデオの理解は、複数のステップの訓練を行う必要があり、詳細な注釈が必要ですが、これは高額で時間がかかり、不均一性があるため、これらの問題を解決する必要があります。ビズマップでは、短いビデオ（ここで注釈データが豊富）と長いビデオ（ここではない）の間のギャップをカットします。LLMを使用して、短いビデオから得られるセグメントの説明を用いて、長いビデオの優化されたファクトサマリーを生成します。これらのファクトサマリーは、長ビデオの要約にモデルの訓練データとして使用され、訳注釈の必要性を回避します。特に、メタプロンプティング戦略を採用して、長いビデオのファクトサマリーを反復的に生成し、改良します。この戦略は、短いクリップの説明を用いて要約をガイドします。各イテレーションでは、3つのLLMを順次使用します：クリップの説明からファクトサマリーを生成する、それを評価する、そして、ジェネレーターのプロンプトを最適化する。このイテレーションは、ファクトサマリーの品質がジェネレーターのプロンプトに高度に依存し、ビデオ間でその品質が極端に変化するため必要となります。ビジョンデータセットで広範囲に評価し、ビズマップは、全訓練バックで最先端のモデルと同等の性能を達成し、領域を横断しながら性能を落としません。コードは、論文公表後にリリースされます。",
      "upvotes": 3,
      "discussionId": "680ab2fb08464b525df64bd2",
      "ai_keywords": [
        "LLMs",
        "ViSMap",
        "Unsupervised Video Summarisation",
        "Meta Prompting",
        "long-form video understanding",
        "supervised hierarchical training",
        "pseudo-summaries",
        "short clip descriptions",
        "meta-prompting strategy",
        "generator prompt"
      ]
    },
    "publishedAt": "2025-04-22T10:06:01.000Z",
    "title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting",
    "summary": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e1b6e9501590df0173cbd3",
      "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
      "fullname": "Jian Hu",
      "name": "lwpyh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17601",
      "authors": [
        {
          "_id": "680b2b8c6bd146aa35a48222",
          "user": {
            "_id": "64d496b04ab89be0de7fb1a9",
            "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
            "isPro": false,
            "fullname": "Erik Bergh",
            "user": "erikbergh",
            "type": "user"
          },
          "name": "Erik Bergh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-25T06:39:03.785Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T14:26:42.000Z",
      "submittedOnDailyAt": "2025-04-25T05:00:56.733Z",
      "title": "가우스 가중치付여 선형 변환을 이용한 해석 가능한 비선형 차원 축소 방법",
      "submittedOnDailyBy": {
        "_id": "64d496b04ab89be0de7fb1a9",
        "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
        "isPro": false,
        "fullname": "Erik Bergh",
        "user": "erikbergh",
        "type": "user"
      },
      "summary": "차원 축소 기법은 고차원 데이터의 분석과 시각화에 기초적인 역할을 수행하고 있습니다. t-SNE와 PCA 등 기존 기법은 표현력과 설명성에서 트레이드오프를 보이고 있습니다. 본 논문에서는 이러한 트레이드오프를 해결하기 위한 새로운 접근 방식을 소개하고, 선형 기법의 설명성과 비선형 변환의 표현력을 통합하는 데努비합니다. 제안된 알고리즘은 선형 변환과 가우시안 함수를 사용하여 고차원과 저차원의 공간 사이의 비선형 매핑을 구축합니다. 이 아키텍처는 복잡한 비선형 변환을 가능하게 하되, 선형 기법의 설명성의 장점을 유지하며, 각 변환이 독립적으로 분석될 수 있도록 설계되었습니다. 결과적으로 차원 축소와 동시에 변환된 공간에서 투명한 통찰을 제공합니다. 학습된 변환의 설명 방법도 제시되며, 억제된 차원의 특정성과 공간의 확대 및 축소 방법도 포함됩니다. 이러한 도구는 실용자들이 차원 축소 과정에서 유지되거나 변경된 기하학적 관계를 이해할 수 있게 합니다. 이 알고리즘의 실용적인 효용을 보장하기 위해, 사용자 친화적인 소프트웨어 패키지의 개발을 강조하고, 학술계와 산업界的 도입을 촉진합니다.",
      "upvotes": 1,
      "discussionId": "680b2b8d6bd146aa35a48252",
      "projectPage": "https://github.com/erikbergh/interpretable_dim_reduction/",
      "githubRepo": "https://github.com/erikbergh/interpretable_dim_reduction/",
      "ai_keywords": [
        "t-SNE",
        "PCA",
        "non-linear mapping",
        "Gaussian functions",
        "linear transformations",
        "interpretability",
        "dimensionality reduction",
        "suppressed dimensions",
        "geometric relationships"
      ]
    },
    "publishedAt": "2025-04-24T10:26:42.000Z",
    "title": "Interpretable non-linear dimensionality reduction using gaussian\n  weighted linear transformation",
    "summary": "Dimensionality reduction techniques are fundamental for analyzing and\nvisualizing high-dimensional data. With established methods like t-SNE and PCA\npresenting a trade-off between representational power and interpretability.\nThis paper introduces a novel approach that bridges this gap by combining the\ninterpretability of linear methods with the expressiveness of non-linear\ntransformations. The proposed algorithm constructs a non-linear mapping between\nhigh-dimensional and low-dimensional spaces through a combination of linear\ntransformations, each weighted by Gaussian functions. This architecture enables\ncomplex non-linear transformations while preserving the interpretability\nadvantages of linear methods, as each transformation can be analyzed\nindependently. The resulting model provides both powerful dimensionality\nreduction and transparent insights into the transformed space. Techniques for\ninterpreting the learned transformations are presented, including methods for\nidentifying suppressed dimensions and how space is expanded and contracted.\nThese tools enable practitioners to understand how the algorithm preserves and\nmodifies geometric relationships during dimensionality reduction. To ensure the\npractical utility of this algorithm, the creation of user-friendly software\npackages is emphasized, facilitating its adoption in both academia and\nindustry.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17601.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d496b04ab89be0de7fb1a9",
      "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
      "fullname": "Erik Bergh",
      "name": "erikbergh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17414",
      "authors": [
        {
          "_id": "680b3f12c131c3be24f80ce0",
          "name": "Min Wei",
          "hidden": false
        },
        {
          "_id": "680b3f12c131c3be24f80ce1",
          "name": "Chaohui Yu",
          "hidden": false
        },
        {
          "_id": "680b3f12c131c3be24f80ce2",
          "name": "Jingkai Zhou",
          "hidden": false
        },
        {
          "_id": "680b3f12c131c3be24f80ce3",
          "name": "Fan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T10:12:40.000Z",
      "submittedOnDailyAt": "2025-04-25T06:22:09.592Z",
      "title": "3DV-TON: 분포 모델에 의한 3D 가이드로 일치된 텍스처付 3D 비디오 쇼트라인",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "비디오 챗レン지는 비디오의 옷을 특정 옷으로 대체하는 것입니다. 기존의 방법들은 복잡한 옷 패턴과 다양한 몸짓을 다루는 경우 고품질과 시간적인 일치성을 유지하는 고품질 결과를 생성하는 것이 어려워졌습니다. 우리는 고품질과 시간적인 일치성을 유지하는 새로운 Dif-fusion 기반 프레임워크를 소개합니다. 우리의 접근법은 생성된 동적인 테크스쳐 드래프를 명시적인 프레임 수준의 가이드에 사용하며, 외관 품질과 동작의 일치성을 동시에 중시함으로써 모델이 외관 품질을 우선시하고 동작의 일치성을 잃는 문제를 해결합니다. 이것은 전체 비디오 시퀀스에서 일치하는 옷의 테크스쳐 이동을 직접 참조함으로써 실현됩니다. 제안된 방법은 동적인 3D 가이드를 생성하는 적응적인 플리프 풀을 특징화합니다: 1) 첫 번째 2D 이미지 챗레ン지의 키 프레임을 선택하고, 2) 원본 비디오 포즈와 동기화된 테크스쳐 드래프의 재구성과 애니메이션을 수행합니다. 또한, 동적인 인간과 옷의 이동으로 인한 직접적인 옷 정보의 손실에 의한 아트팩트의 전파를 줄이기 위해 강력한 직사각형 마스크링 전략을 소개합니다. 비디오 챗레ン지의 연구를 위해, 130개의 비디오를 포함하는 다양한 옷 타입과 시나리오의 고해상도 벤치마크 데이터 세트를 소개합니다. 양적인 및 질적인 결과를 통해 우리의 우수한 성능을 보여주고 있습니다. 프로젝트 페이지는 여기 링크에서 있습니다: https://2y7c3.github.io/3DV-TON/",
      "upvotes": 1,
      "discussionId": "680b3f15c131c3be24f80d65",
      "ai_keywords": [
        "diffusion-based framework",
        "animatable textured 3D meshes",
        "frame-level guidance",
        "motion coherence",
        "garment texture movements",
        "adaptive pipeline",
        "keyframe",
        "2D image try-on",
        "textured 3D mesh",
        "synchronized with original video poses",
        "rectangular masking strategy",
        "artifact propagation",
        "HR-VVT",
        "high-resolution benchmark dataset"
      ]
    },
    "publishedAt": "2025-04-24T06:12:40.000Z",
    "title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models",
    "summary": "Video try-on replaces clothing in videos with target garments. Existing\nmethods struggle to generate high-quality and temporally consistent results\nwhen handling complex clothing patterns and diverse body poses. We present\n3DV-TON, a novel diffusion-based framework for generating high-fidelity and\ntemporally consistent video try-on results. Our approach employs generated\nanimatable textured 3D meshes as explicit frame-level guidance, alleviating the\nissue of models over-focusing on appearance fidelity at the expanse of motion\ncoherence. This is achieved by enabling direct reference to consistent garment\ntexture movements throughout video sequences. The proposed method features an\nadaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe\nfor initial 2D image try-on, followed by (2) reconstructing and animating a\ntextured 3D mesh synchronized with original video poses. We further introduce a\nrobust rectangular masking strategy that successfully mitigates artifact\npropagation caused by leaking clothing information during dynamic human and\ngarment movements. To advance video try-on research, we introduce HR-VVT, a\nhigh-resolution benchmark dataset containing 130 videos with diverse clothing\ntypes and scenarios. Quantitative and qualitative results demonstrate our\nsuperior performance over existing methods. The project page is at this link\nhttps://2y7c3.github.io/3DV-TON/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17414.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17343",
      "authors": [
        {
          "_id": "680b24471c5fbd15909bf1f9",
          "name": "Linli Yao",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fa",
          "name": "Yicheng Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fb",
          "name": "Yuancheng Wei",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fc",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fd",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fe",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1ff",
          "name": "Kun Ouyang",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf200",
          "name": "Lean Wang",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf201",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf202",
          "name": "Sida Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf203",
          "name": "Lingpeng Kong",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf204",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf205",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf206",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T07:59:46.000Z",
      "submittedOnDailyAt": "2025-04-25T06:34:54.517Z",
      "title": "타임チャット・オンライン： 스트리밍 영화에서, 80%의 시각 토큰은 자연스럽게冗장입니다.",
      "submittedOnDailyBy": {
        "_id": "655ca347f426a304c6b393a1",
        "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
        "isPro": false,
        "fullname": "Linli Yao",
        "user": "yaolily",
        "type": "user"
      },
      "summary": "온라인 비디오 플랫폼의 급속한 성장, 특히 라이브 스트리밍 서비스로 인해, 실시간으로 비디오를 이해하는 시스템의 급한 요구가 발생했습니다. 이러한 시스템은 연속적인 비디오 스트리밍을 처리하고, 사용자의 요청에 즉시 응답하는 것이 필요합니다. 현재의 Video Large Language Models (VideoLLMs)는 완전히 비디오를 처리하는 데 뛰어나지만, 스트리밍 시나리오에서 비디오의 연속적이고冗長한 프레임을 효율적으로 처리할 수 있는 한계가 있습니다. 시간차트 온라인(TimeChat-Online)를 소개합니다. 이것은 새로운 온라인 VideoLLM이며, 실시간으로 비디오 인터랙티브 모델을 혁신적으로 만들어냅니다. 핵심은 우리의 혁신적인 Differential Token Drop (DTD) 모듈입니다. DTD는 스트리밍 비디오의 시각적冗長성의 기본적인 문제들을 해결합니다. 인간 시각 인식의 Change Blindness 현상으로부터 에너지를 끌어, DTD는 시간적 변화를 유지하면서 프레임 간의静的적冗長한 내용을 필터링합니다. 실험에서, DTD는 StreamingBench에서 82.8%의 비디오 토큰을 줄였으며, 성능을 98% 유지하며, 스트리밍 비디오의 80% 이상의 시각적 내용은 자연스럽게冗長함을 보여주었습니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무언가 가이드가 필요하지 않다는 것을 보여줍니다. 스트리밍 비디오의 무",
      "upvotes": 1,
      "discussionId": "680b244a1c5fbd15909bf2ff",
      "ai_keywords": [
        "Video Large Language Models (VideoLLMs)",
        "Differential Token Drop (DTD)",
        "Change Blindness phenomenon",
        "TimeChat-Online",
        "TimeChat-Online-139K",
        "StreamingBench",
        "OvOBench",
        "Video-MME",
        "MLVU",
        "Proactive Response",
        "real-time video interaction",
        "continuous video streams",
        "user queries",
        "visual redundancy",
        "dense, redundant frames",
        "visual content",
        "video tokens",
        "meaningful temporal changes",
        "static, redundant content",
        "backward-tracing",
        "current-perception",
        "future-responding scenarios"
      ]
    },
    "publishedAt": "2025-04-24T03:59:46.000Z",
    "title": "TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming\n  Videos",
    "summary": "The rapid growth of online video platforms, particularly live streaming\nservices, has created an urgent need for real-time video understanding systems.\nThese systems must process continuous video streams and respond to user queries\ninstantaneously, presenting unique challenges for current Video Large Language\nModels (VideoLLMs). While existing VideoLLMs excel at processing complete\nvideos, they face significant limitations in streaming scenarios due to their\ninability to handle dense, redundant frames efficiently. We introduce\nTimeChat-Online, a novel online VideoLLM that revolutionizes real-time video\ninteraction. At its core lies our innovative Differential Token Drop (DTD)\nmodule, which addresses the fundamental challenge of visual redundancy in\nstreaming videos. Drawing inspiration from human visual perception's Change\nBlindness phenomenon, DTD preserves meaningful temporal changes while filtering\nout static, redundant content between frames. Remarkably, our experiments\ndemonstrate that DTD achieves an 82.8% reduction in video tokens while\nmaintaining 98% performance on StreamingBench, revealing that over 80% of\nvisual content in streaming videos is naturally redundant without requiring\nlanguage guidance. To enable seamless real-time interaction, we present\nTimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse\ninteraction patterns including backward-tracing, current-perception, and\nfuture-responding scenarios. TimeChat-Online's unique Proactive Response\ncapability, naturally achieved through continuous monitoring of video scene\ntransitions via DTD, sets it apart from conventional approaches. Our extensive\nevaluation demonstrates TimeChat-Online's superior performance on streaming\nbenchmarks (StreamingBench and OvOBench) and maintaining competitive results on\nlong-form video tasks such as Video-MME and MLVU.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17343.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655ca347f426a304c6b393a1",
      "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
      "fullname": "Linli Yao",
      "name": "yaolily",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16064",
      "authors": [
        {
          "_id": "680b494c6bd146aa35ab2e1c",
          "name": "Theodoros Kouzelis",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e1d",
          "name": "Efstathios Karypidis",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e1e",
          "name": "Ioannis Kakogeorgiou",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e1f",
          "name": "Spyros Gidaris",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e20",
          "name": "Nikos Komodakis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/677272184d148b904333e874/QkosGJUTQX94tAZPCF46-.png"
      ],
      "publishedAt": "2025-04-22T17:41:42.000Z",
      "submittedOnDailyAt": "2025-04-25T07:32:33.466Z",
      "title": "JOINT 이미지-특징량 합성에 의한 생성 이미지 모델링의 강화",
      "submittedOnDailyBy": {
        "_id": "677272184d148b904333e874",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5dUau7gxLk4Wm1TiiJJri.jpeg",
        "isPro": false,
        "fullname": "Efstathios Karypidis",
        "user": "Sta8is",
        "type": "user"
      },
      "summary": "潜在확산 모듈(LDMs)는 고품질 이미지 생성 분야에서 주도권을 차지하고 있지만, 표현 학습과 생성 모델링의 통합은 어려운 문제입니다. 우리는 딥러닝 모델을 활용하여, 저레벨 이미지 잠재량(변분 오토코더로부터)과 고레벨 의미적 특징(DINO 등 사전 학습된 자동 인식 엔코더로부터)을 함께 모델화하기 위한 새로운 생성 이미지 모델링 프레임워크를 소개합니다. 우리의 잠재적 의미적 딥러닝 접근법은 순수 노이즈로부터 컨텍스트의 이미지 특징 쌍을 생성하는 것을 학습하고, 생성 품질과 학습 효율을 크게 향상시키고, 표준 Diffusion Transformer 아키텍처에 최소한의 변경이 필요합니다. 복잡한 디자이너 목표를 필요로 하지 않는 점에서, 우리의 통합 설계는 학습을 단순화하고, 강력한 새로운 추론 전략인 \"표현 지침\"을 개발합니다. 조건부 및 비조건부로 평가되어, 우리의 방법은 이미지 품질과 학습 수렴 속도에 큰 향상을 가져오고, 표현에 대한 생성 모델링의 새로운 방향을 세팅합니다.",
      "upvotes": 1,
      "discussionId": "680b494e6bd146aa35ab2e97",
      "githubRepo": "https://github.com/zelaki/ReDi",
      "ai_keywords": [
        "latent diffusion models (LDMs)",
        "generative image modeling",
        "diffusion model",
        "low-level image latents",
        "variational autoencoder",
        "high-level semantic features",
        "pretrained self-supervised encoder",
        "DINO",
        "latent-semantic diffusion",
        "coherent image-feature pairs",
        "generative quality",
        "training efficiency",
        "Diffusion Transformer architectures",
        "complex distillation objectives",
        "Representation Guidance",
        "image quality",
        "training convergence speed",
        "representation-aware generative modeling"
      ]
    },
    "publishedAt": "2025-04-22T13:41:42.000Z",
    "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
    "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/677272184d148b904333e874/QkosGJUTQX94tAZPCF46-.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16064.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "677272184d148b904333e874",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5dUau7gxLk4Wm1TiiJJri.jpeg",
      "fullname": "Efstathios Karypidis",
      "name": "Sta8is",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]