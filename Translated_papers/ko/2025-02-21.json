[
  {
    "paper": {
      "id": "2502.14739",
      "authors": [
        {
          "_id": "67b7efc26348a1df80a8ae53",
          "name": "M-A-P Team",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae54",
          "name": "Xinrun Du",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae55",
          "name": "Yifan Yao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae56",
          "name": "Kaijing Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae57",
          "name": "Bingli Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae58",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tianyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:24.002Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae59",
          "name": "Kang Zhu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5a",
          "user": {
            "_id": "6417d9ea8f689506e7148417",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
            "isPro": false,
            "fullname": "minghao",
            "user": "Liam-Liu",
            "type": "user"
          },
          "name": "Minghao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:25.894Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5b",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5c",
          "name": "Xiaolong Jin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5d",
          "name": "Zhenlin Wei",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5e",
          "user": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "isPro": false,
            "fullname": "Chujie Zheng",
            "user": "chujiezheng",
            "type": "user"
          },
          "name": "Chujie Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:34.124Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5f",
          "name": "Kaixing Deng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae60",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae61",
          "name": "Shian Jia",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae62",
          "name": "Sichao Jiang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae63",
          "name": "Yiyan Liao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae64",
          "name": "Rui Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae65",
          "name": "Qinrui Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae66",
          "name": "Sirun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae67",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae68",
          "name": "Yunwen Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae69",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6a",
          "user": {
            "_id": "64de37ee5e192985054be575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
            "isPro": false,
            "fullname": "Yuansheng Ni",
            "user": "yuanshengni",
            "type": "user"
          },
          "name": "Yuansheng Ni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:30.371Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6b",
          "name": "Haoran Que",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6c",
          "user": {
            "_id": "64560618bfdf9c63ce2d658a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
            "isPro": false,
            "fullname": "Mathsion Wong",
            "user": "QiYao-Wang",
            "type": "user"
          },
          "name": "Qiyao Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:28.639Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6d",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6e",
          "name": "Siwei Wu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6f",
          "name": "Tianshun Xing",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae70",
          "name": "Ming Xu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae71",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae72",
          "name": "Zekun Moore Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae73",
          "name": "Junting Zhou",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae74",
          "name": "Yuelin Bai",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae75",
          "name": "Xingyuan Bu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae76",
          "name": "Chenglin Cai",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae77",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae78",
          "name": "Yifan Chen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae79",
          "name": "Chengtuo Cheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7a",
          "name": "Tianhao Cheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7b",
          "name": "Keyi Ding",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7c",
          "name": "Siming Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7d",
          "name": "Yun Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7e",
          "name": "Yaoru Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7f",
          "name": "Yizhe Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae80",
          "name": "Zhaoqun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae81",
          "name": "Tianhao Liang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae82",
          "name": "Chengdong Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae83",
          "name": "Hongquan Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae84",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae85",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae86",
          "user": {
            "_id": "65adda5299c3bd19c74d6a8d",
            "avatarUrl": "/avatars/1ce504b64ab60f375b235ebaf81cafd6.svg",
            "isPro": false,
            "fullname": "PENG ZIFAN",
            "user": "Ziffer",
            "type": "user"
          },
          "name": "Zifan Peng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:20.429Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae87",
          "name": "Qige Qi",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae88",
          "name": "Shi Qiu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae89",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8a",
          "name": "Yizhou Tan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8b",
          "name": "Zili Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8c",
          "name": "Chenqing Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8d",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8e",
          "name": "Yiya Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8f",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae90",
          "name": "Jiajun Xu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae91",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae92",
          "name": "Ruibin Yuan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae93",
          "name": "Yuanhao Yue",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae94",
          "name": "Tianyang Zhan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae95",
          "name": "Chun Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae96",
          "name": "Jingyang Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae97",
          "name": "Xiyue Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae98",
          "name": "Xingjian Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae99",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9a",
          "name": "Yongchi Zhao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9b",
          "name": "Xiangyu Zheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9c",
          "name": "Chenghua Zhong",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9d",
          "name": "Yang Gao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9e",
          "name": "Zhoujun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9f",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea0",
          "user": {
            "_id": "612ee6a7b960e78c6d2319d4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "isPro": false,
            "fullname": "Qian Liu",
            "user": "SivilTaram",
            "type": "user"
          },
          "name": "Qian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:32.399Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea1",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea2",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea3",
          "name": "Junran Peng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea4",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea5",
          "name": "Wenbo Su",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea6",
          "name": "Guoyin Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea7",
          "name": "Shi Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea8",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea9",
          "name": "Min Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeaa",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeab",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeac",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aead",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeae",
          "user": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "isPro": false,
            "fullname": "Jiaheng Liu",
            "user": "CheeryLJH",
            "type": "user"
          },
          "name": "Jiaheng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:22.185Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeaf",
          "name": "Qunshu Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeb0",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeb1",
          "name": "Ge Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T17:05:58.000Z",
      "title": "SuperGPQA: 285 학과별 LLM 평가의 척도화\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "summary": "대 언어 모뎀(LLMs)은 수학, 물리학, 컴퓨터 과학 등主流 학계 분야에서 놀라운 성능을 보여주고 있습니다. 그러나 인간 지식은 200 이상의 전문 분야를 포함하고 있으며, 현재 벤치마크의 범위를 초월하고 있습니다. LLMs는 특히 출력업, 농업, 서비스 관련 분야의 많은 전문 분야에서 능력은 충분히 평가되지 않았습니다. 이러한 결함을 해결하기 위해, 우리는 SuperGPQA라는 285 분야의 대학원 수준의 지식과 추론 능력을 평가하는 엄격한 벤치마크를 소개합니다. 우리 벤치마크는 LLM의 답변과 전문의 피드백에 기반한 반복적인 개선을 통해, 간단하거나 불확실한 질문을 제외한 새로운 인간-LLM 협동 필터링 시스템을 사용합니다. 실험 결과를 통해, 현재의 가장 선진적인 LLMs가 다양한 지식 분야에서 성능에 대한 큰 개선 여지가 있음을 명확히 알 수 있으며, 현재의 모델 능력과 인공지능 사이에 큰 차이를 명확히 합니다. 또한, 우리는 80 이상의 전문 전문가 어노테이터와 상호작용하는 인간-LLM 협동 시스템을 포함하는 대형 어노테이션 프로세스 관리에서, 향후 연구 프로젝트에 대한 유효한 방법론의 가이드라인을 제공하기 위한 구체적인 통찰을 제공합니다.",
      "upvotes": 61,
      "discussionId": "67b7efc66348a1df80a8afc8"
    },
    "publishedAt": "2025-02-20T22:15:33.133Z",
    "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14739.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14786",
      "authors": [
        {
          "_id": "67b7ed0d58f6b70b18dda7b4",
          "name": "Michael Tschannen",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b5",
          "name": "Alexey Gritsenko",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b6",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b7",
          "name": "Muhammad Ferjad Naeem",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b8",
          "name": "Ibrahim Alabdulmohsin",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b9",
          "name": "Nikhil Parthasarathy",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7ba",
          "name": "Talfan Evans",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bb",
          "name": "Lucas Beyer",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bc",
          "name": "Ye Xia",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bd",
          "name": "Basil Mustafa",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7be",
          "name": "Olivier Hénaff",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bf",
          "name": "Jeremiah Harmsen",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7c0",
          "name": "Andreas Steiner",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7c1",
          "name": "Xiaohua Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:08:29.000Z",
      "title": "SigLIP 2: 다언어 비전 언어 인코더로, 문법 이해, 위치화, 밀집 특성량을 향상합니다.",
      "summary": "シグラフィープリミューム2, 새로운 다언어 비지션 언어 인코더의 가족을 소개합니다. 이 모델은 원래의 シグラフィープリミューム의 성공에 기반하여 개발되었습니다. 이 두 번째 세대는 원래의 이미지 텍스트 훈련 객체와 함께 개발된 여러 첨단 기술을 통합한 레시피로 변경되었습니다. 이에는 캡처기 기반의 예측, 자동 전복 로스(self-distillation, 마스크付き 예측), 그리고 온라인 데이터 칼레이션이 포함됩니다. 이러한 변경으로 シグラフィープリミューム2 모델은 シグラフィープリミューム 모델과 비교하여 모든 핵심 기능에 있어 가장 뛰어난 성능을 보입니다. 특히, ゼロショット 클래스 분류, 이미지 텍스트 검색, 비지션 언어 모델(VLM)의 비지션 표현 추출 성능에도 더 뛰어납니다. 또한 새로운 훈련 레시피는 로케션 태스크와 밀접한 예측 태스크에서도 큰 향상을 보입니다. 또한, 여러 크기 대응과 입력의 원생 가로비율을 유지한 모델 버전도 학습되어 있습니다. 또한, 장치 기술을 포함한 다양한 데이터 미크스로 학습하여 여러 언어 이해와 공정성을 향상시켰습니다. 사용자가 추론 비용과 성능을 조정하기 위해, ViT-B(86M), L(303M), So400m(400M), g(1B)의 4가지 크기의 모델 체크포인트를 릴리즈합니다.",
      "upvotes": 50,
      "discussionId": "67b7ed0e58f6b70b18dda7f4"
    },
    "publishedAt": "2025-02-20T22:33:22.039Z",
    "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14786.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14382",
      "authors": [
        {
          "_id": "67b7ed3e58f6b70b18ddb4bc",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4bd",
          "user": {
            "_id": "64ebbae6895a36ab28de811a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ebbae6895a36ab28de811a/gBiaQP4paS4L13eu-yRm7.jpeg",
            "isPro": false,
            "fullname": "Shiyi Cao",
            "user": "eva98",
            "type": "user"
          },
          "name": "Shiyi Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:43.358Z",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4be",
          "name": "Chengkun Cao",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4bf",
          "name": "Xiuyu Li",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c0",
          "name": "Shangyin Tan",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c1",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c2",
          "name": "Jiarong Xing",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c3",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c4",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:18:53.000Z",
      "title": "S*: 코드 생성 시간 스케줄링 테스트",
      "summary": "検査時의 계산량을 증가시키는 것이 LLM의 여러 분야에서 원하는 결과를 보여주고 있지만, 코드 생성에 대한 수학적인 연구는 부족하다. 본 논문에서는, 생성되는 코드의 커버리지와 선택 정확도를 크게 향상시키는 첫 번째 하이브리드 검사 시간 스케일링 프레임워크 S*를 제안합니다. S*는 현재의 병렬 스케일링 패러다임에 순서 스케일링을 추가하여 성능의 한계를 뛰어넘습니다. 또한 새로운 선택 구조를 활용하여 대비적인 입력을 적응적으로 생성하고, 실행 기반의 정보를 조합하여 정확한 해결책을 강하게 특정할 수 있습니다. 12개의 대규모 언어 모델과 대규모 논리 모델을 검증하고, 다음과 같은 결과를 나타냅니다: 1) S*는 모델의 가족과 크기에 따라 일관된 성능 향상을示し, 3B 모델은 GPT-4o-mini를 초월합니다. 2) S*는 논리 모델을 초월할 수 있으며, GPT-4o-mini는 LiveCodeBench에서 o1-preview를 3.7% 초과합니다. 3) S*는 가장 先端의 논리 모델을 더욱 향상시키고, DeepSeek-R1-Distill-Qwen-32B는 LiveCodeBench에서 85.7%를 달성하고, o1 (high)의 88.5%에 가까워집니다. 코드는 https://github.com/NovaSky-AI/SkyThought에서 사용 가능합니다.",
      "upvotes": 29,
      "discussionId": "67b7ed3f58f6b70b18ddb510"
    },
    "publishedAt": "2025-02-20T22:04:42.635Z",
    "title": "S*: Test Time Scaling for Code Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14382.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14258",
      "authors": [
        {
          "_id": "67b7fa96c3f48f8b3fc632fe",
          "name": "Yein Park",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc632ff",
          "name": "Chanwoong Yoon",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63300",
          "name": "Jungwoo Park",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63301",
          "name": "Minbyul Jeong",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63302",
          "name": "Jaewoo Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T04:52:05.000Z",
      "title": "시간은 어디에 놓여있는가? 시간 핸드: 언어 모델이 시간의 정보를 기억하는 장소",
      "summary": "이 문장을 한국어로 번역하면 다음과 같습니다.\n\n「언어 모델이 사실에 대한 추론 능력은 광범위하게 조사되어 있습니다が, 이들이 시간에 따라 변화하는 사실들을 어떻게 처리하는지는 조사가 부족합니다. 시간적 지식 처리를 위한 특수한 어텐션 헤드 'Temporal Heads'를 발견했습니다. 이러한 헤드는 여러 모델에 모두 존재하며, 위치는 다릅니다 그러나 지식의 종류나 상대적인 년도별로 응답이 다릅니다. 이러한 헤드를 비활성화하면 시간적 지식의 특정 능력이 떨어집니다 그러나 시간적 불변성과 문제 해결 능력은 손실되지 않습니다. 또한, 이러한 헤드는 '2004년'의 숫자 조건이나 '…의 년'이라는 문자적 대명詞로도 활성화 됩니다. 이는 이들이 단순한 숫자 표현보다 시간적 차원을 표현하는 것을 보여줍니다. 더욱이, 이러한 헤드의 값을 조정함으로써 시간적 지식의 편집이 가능합니다。」",
      "upvotes": 17,
      "discussionId": "67b7fa9ac3f48f8b3fc63452"
    },
    "publishedAt": "2025-02-20T23:02:42.672Z",
    "title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64587be872b60ae7a3817858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
      "fullname": "Minbyul Jeong",
      "name": "Minbyul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14834",
      "authors": [
        {
          "_id": "67b7f3c4d00e69f10cff219e",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff219f",
          "name": "Yucheng Wang",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a0",
          "name": "Daniel Zhang-Li",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a1",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a2",
          "name": "Jifan Yu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a3",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a4",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a5",
          "name": "Huiqin Liu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a6",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a7",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a8",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:47:36.000Z",
      "title": "LongWriter-V: 시각 언어 모델에서 초장기간과 고품질의 생성을 가능하게 합니다.",
      "summary": "현재의 대규모 비젼 언어 모델(LVLMs)은 128k의 비젼와 텍스트 토큰의 컨텍스트 길이를 처리할 수 있지만, 1,000 단어를 초과하는 코라니티의 출력을 생성하는 것은 어려움입니다. 우리는 주요한 제한은 초감독 학습(SFT) 시 긴 출력 예가 존재하지 않는 것입니다. 이러한 문제를 해결하기 위해, LongWriter-V-22k를 소개합니다. 이는 22,158개의 SFT 데이터셋을 포함하며, 각 예에 여러 입력 이미지, 지시 및 0부터 10,000 단어의 범위의 대응 출력이 있습니다. 입력 이미지에 고정밀도 대응하여 긴 출력을 구현하기 위해, Direct Preference Optimization(DPO)를 SFT 모델에 적용합니다. 긴 출력(예: 3,000 단어)에 대한 인간 피드백을 수집하는 것은 비싸기 때문에, IterDPO를 제안합니다. 이는 긴 출력을 나누고, 기억 기록의 수정을 사용하여 원본 출력과 선호 페어를 형성합니다. 또한, MMLongBench-Write를 개발했습니다. 이는 VLMs의 긴 생성 능력을 평가하기 위한 벤치마크이며, 6개의 태스크를 제공합니다. 우리 7B 파라미터 모델은 LongWriter-V-22k와 IterDPO를 사용하여 훈련되었으며, 이 벤치마크에서 놀라운 성능을 보였으며, GPT-4o와 다른 큰 공개 모델을 초과합니다. 코드와 데이터는 https://github.com/THU-KEG/LongWriter-V에 있습니다.",
      "upvotes": 15,
      "discussionId": "67b7f3c7d00e69f10cff2258"
    },
    "publishedAt": "2025-02-20T22:39:21.551Z",
    "title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/8AYx7CcK4CT6flX3nRDlB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14834.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14768",
      "authors": [
        {
          "_id": "67b7f08c357c2729ac20a81b",
          "name": "Tian Xie",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81c",
          "name": "Zitian Gao",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81d",
          "name": "Qingnan Ren",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81e",
          "name": "Haoming Luo",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81f",
          "name": "Yuqian Hong",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a820",
          "name": "Bryan Dai",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a821",
          "name": "Joey Zhou",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a822",
          "name": "Kai Qiu",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a823",
          "name": "Zhirong Wu",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a824",
          "name": "Chong Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T17:49:26.000Z",
      "title": "Logic-RL: 논리기반의 강화학습에 의한 LLM의 논리론의 해방",
      "summary": "DeepSeek-R1의 성공을 바탕으로, 대규모 논리 모델에서 규칙 기반의 강화학습(RL)의 가능성을 검토합니다. 논리 모델의 동적 과정을 분석하기 위해, 제어 가능한 복잡성과 간단한 답을 확인하는 것이 가능한 Synthetic Logic Puzzles를 학습 데이터로 사용합니다. RL의 효과적이고 안정적인 학습에 중요한 기술적 기여를 합니다: 사고와 답의 과정에 초점을 맞추는 시스템 Prompt, 짧은 답을 보상하는 엄격한 형식 보상 함수, 그리고 안정적인 수렴을 달성하는 간단한 학습 레시피를 제공합니다. 7B 모델은 로직 코퍼스 외의 先進的な 논리 기술 스킬을 개발합니다: 반성, 확인, 요약 등. 특히, 5K의 로직 문제를 학습 후, AIME와 AMC의 어려운 수학 벤치마크에 대한 일반화 능력을 나타냅니다.",
      "upvotes": 14,
      "discussionId": "67b7f08e357c2729ac20a88f"
    },
    "publishedAt": "2025-02-20T22:19:05.902Z",
    "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14768.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14282",
      "authors": [
        {
          "_id": "67b7f5587f4d732dc469270e",
          "name": "Haowei Liu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc469270f",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692710",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692711",
          "name": "Yuyang Wanyan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692712",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692713",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692714",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692715",
          "name": "Chunfeng Yuan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692716",
          "name": "Changsheng Xu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692717",
          "name": "Weiming Hu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692718",
          "name": "Fei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T05:41:55.000Z",
      "title": "PC-Agent: 컴퓨터에 대한 복잡한 작업 자동화를 위한 계층적인 다 에이전트 협업 프레임워크",
      "summary": "マルチランプライムラインモデル(MLLM)에 기반한 GUI 에이전트 분야에서, 스마트폰과 비교하여, PC 시나리오는 교환성 높은 환경의 특징을 가지고 있으며, 애플리케이션 간 복잡한 작업 흐름도 포함됩니다. 이러한 문제를 해결하기 위해, 우리는 PC-Agent라는 하이어러 에이전트 프레임워크를 제안합니다. 특히, 시각적인 관점에서, 현재의 MLLM이 스크린샷의 내용을 무시할 수 있는 능력을 극복하기 위해, 능동적인 시각 인식 모듈(APM)을 설계합니다. 의사결정의 관점에서, 복잡한 사용자 지시와 상호 의존된 서브 태스크를 효과적으로 처리하기 위해, 의사결정 과정을 명령- 서브 태스크- 행동 수준으로 분할하는 휴리스틱적인 다수 에이전트 협업 아키텍처를 제안합니다. 이 아키텍처에서, 명령 분해, 진행 추적, 단계별 의사결정을 위해, Manager, Progress, Decision의 3가지 에이전트가 설치됩니다. 또한, 적시에 오류를 상류로 피드백하고 조정을 가능하게 하기 위해, Reflection 에이전트를 도입합니다. 또한, 25건의 실세계의 복잡한 명령을 포함하는 새로운 벤치마크 PC-Eval을 소개합니다. PC-Eval에서 수행한 실험 결과에 따르면, 우리 PC-Agent는 이전의 최상위 방법 대비 태스크 성공률이 약 32% 증가했습니다. 코드는 공개적으로 사용 가능합니다.",
      "upvotes": 11,
      "discussionId": "67b7f55b7f4d732dc46927c1"
    },
    "publishedAt": "2025-02-20T22:39:48.180Z",
    "title": "PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/feg9OYb4onJJermpjc6nh.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14282.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14499",
      "authors": [
        {
          "_id": "67b7ee1dfedfe971271dcca0",
          "user": {
            "_id": "6114c9fae7a2566ae7d1a1a7",
            "avatarUrl": "/avatars/c71ab1850322fcf5ef239cb8d31cb137.svg",
            "isPro": false,
            "fullname": "Deepak Nathani",
            "user": "dnathani",
            "type": "user"
          },
          "name": "Deepak Nathani",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-21T07:20:46.836Z",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca1",
          "name": "Lovish Madaan",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca2",
          "name": "Nicholas Roberts",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca3",
          "name": "Nikolay Bashlykov",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca4",
          "name": "Ajay Menon",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca5",
          "name": "Vincent Moens",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca6",
          "name": "Amar Budhiraja",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca7",
          "name": "Despoina Magka",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca8",
          "name": "Vladislav Vorotilov",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca9",
          "name": "Gaurav Chaurasia",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccaa",
          "name": "Dieuwke Hupkes",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccab",
          "name": "Ricardo Silveira Cabral",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccac",
          "name": "Tatiana Shavrina",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccad",
          "name": "Jakob Foerster",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccae",
          "name": "Yoram Bachrach",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccaf",
          "name": "William Yang Wang",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccb0",
          "user": {
            "_id": "633e94793a17ab61de8e2b9c",
            "avatarUrl": "/avatars/5f2f58ddeed211393660ada6b135f0d5.svg",
            "isPro": false,
            "fullname": "Roberta Raileanu",
            "user": "rraileanu",
            "type": "user"
          },
          "name": "Roberta Raileanu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-21T03:08:15.471Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T12:28:23.000Z",
      "title": "MLGym: AI 연구 에이전트의 발전을 촉진하는 새로운 프레임워크와 벤치마크",
      "summary": "Meta MLGym와 MLGym-Bench는 새로운 프레임워크와 벤치마크입니다. 이는 LLM 에이전트의 평가와 개발을 위한 인공지능 연구 태스크에서 새로운 프레임워크와 벤치마크입니다. 먼저, 기계 학습(ML) 태스크의 첫 번째 Gym 환경입니다. 이를 통해 이러한 에이전트의 학습을 위한 강화 인지 학습(RL) 알고리즘의 연구가 가능합니다. MLGym-Bench는 컴퓨터 비전, 자연어 처리, 강화 인지 학습, 게임 이론 등 다양한 분야에서 13가지 다양한 개방된 AI 연구 태스크로 구성되어 있습니다. 이러한 태스크를 해결하기 위해, 새로운 아이디어와 가설의 생성, 데이터의 생성과 처리, ML 방법의 구현, 모델의 훈련, 실험의 수행, 결과를 분석, 그리고 이 프로세스를 반복하여 특정 태스크에 대한 개선을 위한 현실적인 AI 연구 기술이 필요합니다. 우리는 Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, Gemini-1.5 Pro 등 다수의 발전된 대규모 언어 모델(LLMs)을 우리의 벤치마크에서 평가하고 있습니다. 우리의 MLGym 프레임워크는 새로운 태스크의 추가, 모델 또는 에이전트의 통합과 평가, 규모 업의 합성 데이터의 생성, AI 연구 태스크에서 에이전트의 학습을 위한 새로운 알고리즘의 개발을 쉽게 수행할 수 있습니다. 우리는 현재의 발전된 모델은 일반적으로 baseline을 개선하기 위해 더 좋은 파라미터를 찾는 데에 집중되어 개선이 가능하지만, 새로운 가설, 알고리즘, 아키텍처, 또는 큰 개선을 생성하지 못했습니다. 우리는 이 프레임워크와 벤치마크를 오픈 소스로 하여, LLM 에이전트의 인공지능 연구 능력의 발전을 촉진하는 미래의 연구를 지원하는 것을 목표로 합니다.",
      "upvotes": 9,
      "discussionId": "67b7ee1ffedfe971271dcd3a"
    },
    "publishedAt": "2025-02-20T22:08:38.225Z",
    "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14499.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14844",
      "authors": [
        {
          "_id": "67b7f5ee8b3dff28b749be78",
          "name": "Rameen Abdal",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be79",
          "name": "Or Patashnik",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7a",
          "name": "Ivan Skorokhodov",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7b",
          "name": "Willi Menapace",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7c",
          "name": "Aliaksandr Siarohin",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7d",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7e",
          "name": "Daniel Cohen-Or",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7f",
          "name": "Kfir Aberman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:53:39.000Z",
      "title": "Dynamic Concepts Personalization from Single Videos\n\nDynamic 개념의 개인화은 단일 비디오에서만 가능합니다.",
      "summary": "개인화 된 텍스트로부터 이미지 모델에서의 진보는 놀라울 정도로 놀라워하지만, 이를 텍스트에서 동영상 모델로 확장하는 것은 특별한 문제를 가지고 있습니다.静的な 개념과 달리, 동영상 모델의 개인화는 동적인 개념을 이해할 수 있습니다. 그리고 그 존재는 외관만 아니라, 움직임을 포함하는 것입니다. 본 논문에서는, 동적인 개념을 사용한 Diffusion Transformers(DiTs) 기반의 생성적인 동영상 모델의 개인화에서 새로운 프레임워크「Set-and-Sequence」를 사용합니다. 우리의 접근 방식은 공간과 시간의 특징을 명시적으로 분리하지 않는 구조 내에서 공간- 시간의 가중 공간에 과제합니다. 이것은 두 가지 주요 단계로 이루어집니다. 먼저, 동영상의 무순열의 프레임을 사용하여 Low-Rank Adaptation(LoRA) 레이어를 미세 조정하고, 외관을 표현하는 identity LoRA 기반으로 시각적인 영향을 학습합니다. 두 번째 단계에서는, identity LoRA가 고정된 상태에서, 그 계수에 Motion Residuals를 추가하여 전체 동영상 시퀀스에서 미세 조정하고, 움직임의 동역학을 이해합니다. 「Set-and-Sequence」 프레임워크는, 동영상 모델의 출력 영역에 동적인 개념을 효과적으로 삽입하고, 전례가 없는 편집 가능성과 조직성을 설정하며, 동적인 개념의 개인화에 새로운 기준을 세봅니다.",
      "upvotes": 8,
      "discussionId": "67b7f5f18b3dff28b749bf45"
    },
    "publishedAt": "2025-02-20T22:41:47.210Z",
    "title": "Dynamic Concepts Personalization from Single Videos",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14844.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14372",
      "authors": [
        {
          "_id": "67b81870cc6b0136b3d84254",
          "user": {
            "_id": "6530a78069751712276d60ed",
            "avatarUrl": "/avatars/2ef4f16d0be557ed60c11d8dcef85f6f.svg",
            "isPro": false,
            "fullname": "Austin He",
            "user": "basil2115",
            "type": "user"
          },
          "name": "Austin Yubo He",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-21T06:30:16.645Z",
          "hidden": false
        },
        {
          "_id": "67b81870cc6b0136b3d84255",
          "name": "Zi-Wen Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:05:34.000Z",
      "title": "强化학습을 활용한 고효율 저무게화 구현을 위한 양자오류보정 코드 연구",
      "summary": "可損失許容量子計算의 스케일러블성은 量子エラー補正コード에 의존되어 기대되어 있습니다. 量子エラー補正과 관련된 효율적인 エラー許容性을 추구하는 과정에서, 중요한 コードパラメーター는 誤りを特定するための 測定の重みです: 高い 測定重みは 実装コストを高め、 エラーを引き起こすことがあるため, コード設計で 測定重みを最適化することが重要です. これは, 量子低密度パリティチェック (qLDPC) コードに対する興味の高まりに基づいています. これらの研究は主に アスインプテクス (大コードの限界) の性質に焦点を当てていました.本研究では, 强化学習 (RL)에 기반한 ステラビラーコードの重み削減の柔軟かつ計算的に効率的なアプローチを紹介します. これにより, 実用的なパラメーターレイムで新しい低重みコードを生成し、現在の最高レベルに比べて大幅に優位を示します. 例えば, 重み6のコードに対して, 現在の結果と比較して 物理キューブのオーバーヘッドを 1〜2オーダーのほど削減でき、近未来の実験に対しては 実用的な範囲に引き込みます. また, RLフレームワークを用いて コードパラメーターの相互作用を調査し、実用的なコーディング戦略の可能性と効率性に新しい見解を提供します.全体として,本研究の結果は, RLが量子コードの発見の重要なばかりで難しい問題を進めることができることを示し、故障許容量子テクノロジーの実用的な実装により速める道を提供します.",
      "upvotes": 7,
      "discussionId": "67b81873cc6b0136b3d8430a"
    },
    "publishedAt": "2025-02-21T01:11:34.971Z",
    "title": "Discovering highly efficient low-weight quantum error-correcting codes with reinforcement learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14372.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6530a78069751712276d60ed",
      "avatarUrl": "/avatars/2ef4f16d0be557ed60c11d8dcef85f6f.svg",
      "fullname": "Austin He",
      "name": "basil2115",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14846",
      "authors": [
        {
          "_id": "67b7f4f1b15c19d57189fc5e",
          "name": "Yue Yang",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc5f",
          "name": "Ajay Patel",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc60",
          "name": "Matt Deitke",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc61",
          "name": "Tanmay Gupta",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc62",
          "name": "Luca Weihs",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc63",
          "name": "Andrew Head",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc64",
          "name": "Mark Yatskar",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc65",
          "name": "Chris Callison-Burch",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc66",
          "name": "Ranjay Krishna",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc67",
          "name": "Aniruddha Kembhavi",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc68",
          "name": "Christopher Clark",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:55:30.000Z",
      "title": "코드 가이드에 따라 합성 모노 모달 데이터 생성을 통한 텍스트 풍부한 이미지 이해의 스케일링",
      "summary": "이미지와 풍부한 텍스트, 예를 들어 차트와 문서,에 대한 추론은 시각 언어 모델(VLMs)의 중요한 응용 분야입니다. 그러나 이러한 영역에서 VLMs는 다양한 텍스트 기반의 시각 언어 데이터의 부족으로 어려움을 겪습니다. 이러한 도전을 해결하기 위해, 우리는 CoSyn, 텍스트 기반의 대형 언어 모델(LLMs)의 코드 생성 능력을 활용하여 합성된 텍스트 기반의 다중 모드 데이터를 자동으로 생성하는 프레임워크를 제시합니다. 목표 영역을 설명하는 입력 텍스트를 제공받으면, CoSyn은 합성된 이미지를 렌더링하기 위한 코드(Python, HTML, LaTeX 등)를 생성하도록 LLM을 촉발합니다. 합성된 이미지의 텍스트 표현으로 된 기본 코드를 통해, CoSyn은again text-only LLM을 통해 고품질의 명령 학습 데이터를 생성할 수 있습니다. CoSyn을 통해, 우리는 400K 이미지와 2.7M 행의 시각 언어 명령 학습 데이터로 구성된 데이터셋을 구축했습니다. 7개의 벤치마크에 대한 종합적인 실험은, 우리의 합성 데이터에 기반한 모델이 경쟁적인 오픈 소스 모델 중 최고 수준의 성능을 달성하며, GPT-4V와 Gemini 1.5 Flash와 같은 비공개 모델을 초월할 수 있음을 보여주었습니다. 또한, CoSyn은 합성된 pointing 데이터를 생성할 수 있으며, 이는 VLMs가 입력 이미지 내의 정보를 고정시키고, 현실 세계 환경에서 작동할 수 있는 다중 모드 에이전트의 개발에 잠재력을 보여줍니다.",
      "upvotes": 5,
      "discussionId": "67b7f4f2b15c19d57189fc95"
    },
    "publishedAt": "2025-02-20T22:38:36.406Z",
    "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12853",
      "authors": [
        {
          "_id": "67b69b6717ccb022c6a95b38",
          "name": "Ruotian Ma",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b39",
          "name": "Peisong Wang",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3a",
          "name": "Cheng Liu",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3b",
          "name": "Xingyan Liu",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3c",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3d",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3e",
          "name": "Xin Zhou",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3f",
          "name": "Nan Du",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b40",
          "name": "Jia Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T13:40:22.000Z",
      "title": "S^2R: 강화학습에 의한 자동 증명과 자동 수정을 위한 LLM 교육\n\n(Note: The translation is provided as requested, without additional explanation or text.)",
      "summary": "최근의 연구는 LLM의 테스트 시 스케일링의 효과성을 보여주고 있습니다. 그러나 현재의 LLM의 깊은 사고 능력을 촉발시키는 방법은 일반적으로 큰 데이터와 훈련의 노력을 필요로 합니다. 반면에, 베이스 모델의 사고 능력을 향상시키는 방법은 명확하지 않습니다. 본 연구에서는 S^2R라는 효율적인 프레임워크를 통해 LLM의 추론을 강화하고 추론 중에서 자동으로 자각증과 보정을 수행하는 모델을 교육하는 방법을 제안합니다. 특히, 처음으로, 조정된 데이터에 의한 규범적 조정 훈련을 통해 반복적인 자각증과 보正的 행동을 초기화합니다. 그 후, 결과 수준과 프로세스 수준의 강화 학습을 통해 자각증과 보正的 스킬을 더욱 강화하고 자원의 요구를 최소화하여 추론 중의 추론 프로세스를 적응적으로 개선할 수 있습니다. 우리의 결과는 3.1k의 자각증과 보正的 행동을 초기화한 것으로, Qwen2.5-math-7B의 정확도는 51.0%에서 81.6%로 상승하며, 같은 양의 긴 코시드 데이터로 훈련된 모델을 초월합니다. 세 개의 베이스 모델을 구성하는 영역 내 및 영역 외 벤치마크에 기반한 확장된 실험과 분석은 S^2R의 효과성을 입증합니다. 우리의 코드와 데이터는 https://github.com/NineAbyss/S2R에 공개되어 있습니다.",
      "upvotes": 4,
      "discussionId": "67b69b6817ccb022c6a95b6e"
    },
    "publishedAt": "2025-02-21T05:00:18.645Z",
    "title": "S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648294b2eb4befee378951c1",
      "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
      "fullname": "Ruotian Ma",
      "name": "vvibt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14669",
      "authors": [
        {
          "_id": "67b7eeddaf9f1b1bd95b878b",
          "name": "Alan Dao",
          "hidden": false
        },
        {
          "_id": "67b7eeddaf9f1b1bd95b878c",
          "name": "Dinh Bach Vu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T16:05:18.000Z",
      "title": "AlphaMaze: GRPO를 기반으로 대규모 언어 모델의 공간 인식 능력을 향상시키기 위한 글로벌 프로젝트 (GRPO)",
      "summary": "대 언어 모델(LLMs)는 언어 처리에 뛰어난 능력을 보여주지만, 실제 시각 공간적인 이유를 필요로 하는 작업에 어려움을 겪는 문제가 있습니다. 본 논문에서는, 일반적인 LLMs에 미로 탐색의 시각적 이유 능력을 추가하기 위한 새로운 2 단계 훈련 프레임워크를 사용합니다. 먼저, 통계적으로 읽은 미로 표현의 컬렉션 데이터 세트에 대한 Supervised Fine Tuning(SFT)을 활용하여, 모델이 단계별 이동 명령을 예측하도록 가르칩니다. 다음으로, DeepSeekR1에서 사용하는 Group Relative Policy Optimization(GRPO)를 도입하여, 쎄프한 보상 함수를 사용하여 모델의 연속적인 결정을 개선하고, 시간 제약을 고려한 행동을 촉구합니다. 합성적으로 생성된 미로에 대한 실험 결과를 통해, 기본 모델은 미로를 탐색할 수 없다고 확인하지만, SFT 훈련된 모델은 86%의 정확도를 달성하고, GRPO의 추가 훈련은 정확도를 93%까지 높입니다. 질적인 분석에 따르면, GRPO는 더 강력한 자기보정의 이유를 촉발시키고, 우리의 접근 방식이 언어 모델과 시각 공간적인 작업 사이의 간격을 메우는 가능성을 밝혀냅니다. 이러한 발견은 로봇공학, 자동 네비게이션, 기타 분야에서 통합된 시각과 연속적인 이유를 필요로 하는 애플리케이션에 대해, 바람직한 의미를 가지고 있습니다.",
      "upvotes": 4,
      "discussionId": "67b7eeddaf9f1b1bd95b87c8"
    },
    "publishedAt": "2025-02-20T22:11:45.130Z",
    "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14377",
      "authors": [
        {
          "_id": "67b7f350357c2729ac216494",
          "name": "Ke Cao",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216495",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216496",
          "name": "Ao Ma",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216497",
          "name": "Jiasong Feng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216498",
          "name": "Zhanjie Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216499",
          "name": "Xuanhua He",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649a",
          "name": "Shanyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649b",
          "name": "Bo Cheng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649c",
          "name": "Dawei Leng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649d",
          "name": "Yuhui Yin",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649e",
          "name": "Jie Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:10:05.000Z",
      "title": "RelaCtrl: Relevance-Guided Efficient Control with Diffusion Transformers",
      "summary": "Diffusion Transformer는 텍스트로부터 이미지 및 텍스트로부터 애니메이션의 생성에 중요한 역할을 수행하고 있으며, 주로 기능적인 스케일라빌리티에 의해 발전하고 있습니다. 그러나 현재의 제어된 Diffusion Transformer의 방법들은 파라미터와 계산 오버헤드가 크고, 드라이버 수준의 제어 정보와의 관련性差이로 인해 자원이 적절하게 배정되지 않고, 효율성이 저하되어 있습니다. 이에 대처하여, 우리는 Relevance-Guided Efficient Controllable Generation 프레임워크인 RelaCtrl를 제안하여 Diffusion Transformer에 효율적인 제어 신호의 형식화를 가능하게 합니다.\n\n먼저, Diffusion Transformer의 각 레이어에 대한 제어 정보의 관련성을 평가하고, \"ControlNet Relevance Score\"를 사용하여 각 제어 레이어가 제거된 경우의 생성 품질과 추론 시의 제어 효과에 대해 평가합니다. 이러한 관련성의 강도를 기반으로, 다음으로 제어 레이어의 위치 디렉토리, 파라미터 스케일 및 모델링 능력에 대해 조정하여 필요한 파라미터와冗餘 계산을 줄이는 것을 목표로 합니다. 또한, 더 높은 효율성을 달성하기 위해, 일반적인 복사 블록에서 사용하는 자동 注意와 FFN을 두차원 셔플 미셔너(TDSM)으로 대체하여 토큰 미셔너와 채널 미셔너의 효율적인 구현을 가능하게 합니다.\n\n定性 및定量적인 실험 결과는 PixArt-delta와 비교하여 파라미터와 계산 복잡도가 15%만 사용되어도, 더 우수한 성능을 발휘하는 것을 보여줍니다. 추가 예시는 https://relactrl.github.io/RelaCtrl/에서 사용 가능합니다.",
      "upvotes": 3,
      "discussionId": "67b7f354357c2729ac216582"
    },
    "publishedAt": "2025-02-20T22:30:51.542Z",
    "title": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14377.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13759",
      "authors": [
        {
          "_id": "67b83a1f26e7d5f7cb0b7c9d",
          "user": {
            "_id": "65407ba7a38390065750233f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
            "isPro": false,
            "fullname": "Zirui Song",
            "user": "Ziruibest",
            "type": "user"
          },
          "name": "Zirui Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:04.247Z",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7c9e",
          "name": "Jingpu Yang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7c9f",
          "name": "Yuan Huang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca0",
          "name": "Jonathan Tonglet",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca1",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca2",
          "name": "Tao Cheng",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca3",
          "name": "Meng Fang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca4",
          "name": "Iryna Gurevych",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca5",
          "name": "Xiuying Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T14:21:25.000Z",
      "title": "지오리렉션에 의한 실제 인간의 게임 플레이 데이터 사용: 큰 데이터 세트와 인간처럼 논리적인 프레임워크",
      "summary": "지오리포지팅, 이미지의 위치를 특정하는 임무, 복잡한 이유를 필요로 하고, 지도, 감시, 문화 보존에 중요합니다. 그러나 현재의 방법은 일반적으로 정확도가 낮은 위치 정보를 생성하며, 해석이 가능한 수준에서 부족합니다. 문제 중 하나는 현재의 지오리포지팅 데이터셋의 질과 규모입니다. 이러한 데이터셋은 일반적으로 작은 규모로 자동적으로 구축되어, 노이즈가 추가된 데이터와 작업의 난이도의 불균형을 통해, 과도하게 표현되는 이미지나, 신뢰할 수 있는 추론에 필요한 충분한 키펫을 갖지 못하는 이미지가 포함됩니다. 이러한 문제를 해결하기 위해, 우리는 세 가지 핵심 구성 요소를 가진 엄격한 지오리포지팅 프레임워크를 소개합니다: GeoComp, 큰 규모의 데이터셋, GeoCoT, 새로운 이유 방법, GeoEval, 평가 지표. 이 프레임워크의 핵심은 GeoComp (지오리포지팅 컴페티션 데이터셋)입니다. 이는 2년 동안 740K 사용자로부터 촬영된 큰 규모의 데이터셋이며, 세계에서 많은 지역을 포함하는 2500만건의 메타데이터와 300만건의 지오태그가 붙은 위치 정보를 포함하고 있으며, 각 위치는 인간 사용자가 수천から数十만 번 뉴스를 달아주었습니다. 이 데이터셋은 다양한 난이도 수준을 제공하여 다양한 분석을 위해 사용될 수 있으며, 현재의 모델의 중요한 약점을 드러냅니다. 이 데이터셋을 기반으로, 우리는 LVMs (Vision Language Models)의 이유 능력을 강화하기 위한 새로운 다단계 이유 프레임워크인 Geographical Chain-of-Thought (GeoCoT)을 제안합니다. GeoCoT은 인간의 지오리포지팅 이유를 모방하는 다단계 프로세스를 통해, 맥락과 공간적 키펫을 통합하여 성능을 향상시킵니다. 마지막으로, GeoEval 지표를 사용하여, GeoCoT이 지오리포지팅의 정확성을 최대 25% 정도 향상시키고, 해석성을 향상시키는 것을 보여줍니다.",
      "upvotes": 1,
      "discussionId": "67b83a2226e7d5f7cb0b7d66"
    },
    "publishedAt": "2025-02-21T03:33:28.852Z",
    "title": "Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13759.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65407ba7a38390065750233f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
      "fullname": "Zirui Song",
      "name": "Ziruibest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14409",
      "authors": [
        {
          "_id": "67b83a20a9fa331061e84ecd",
          "user": {
            "_id": "60a643b9213fe60589b8fdf9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a643b9213fe60589b8fdf9/OOXmW3MkSf88r63tAE6-n.jpeg",
            "isPro": false,
            "fullname": "Dustin Wright",
            "user": "dwright37",
            "type": "user"
          },
          "name": "Dustin Wright",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:02.288Z",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ece",
          "name": "Zain Muhammad Mujahid",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ecf",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ed0",
          "name": "Isabelle Augenstein",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ed1",
          "name": "David Jurgens",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:57:42.000Z",
      "title": "無構造的証拠의 속성화 및 긴 문맥 쿼리에 초점을 맞추는 요약화",
      "summary": "대 언어 모델(LLMs)은 사용자 요청에 따라 매우 긴 컨텍스트로부터 일관된 요약을 생성할 수 있습니다. 증거 스パン의 요약과 정확한 인용이 투명성과 신뢰성을 향상시킬 수 있습니다. 반면에, LLMs는 특정 정보에 대해 주의를 집중시키는 위치에 대한 편향을 보여줍니다. 이는 증거의 인용에 영향을 미칠 수 있습니다. 과거의 연구는 특정 크기(예: 문, 단락, 문서 등)에 대한 증거 인용에 집중했습니다. 그러나 우리는 무구조화된 증거 인용을 포함하는 긴 컨텍스트 쿼리에 기반한 요약을 제안합니다. 현재의 시스템이 컨텍스트로부터 무구조화된 증거를 생성하고 정확한 인용하는 데 어려움을 겪는 것을 보여주고, 증거가 \"중간에 혼란\"되어 있는 것을 보여줍니다. 이를 완화하기 위해, 우리는 새로운 도메인 독립적인 파이프라인으로 생성된 합성 데이터 세트인 \"SUnsET\"(Unstructured Evidence Text with Summaries)를 만들었습니다. 이 데이터 세트는 LLMs가 이 작업에 적용될 때 지원을 사용할 수 있습니다. 5가지의 다른 크기의 LLMs와 4가지의 데이터 세트(이러한 문서의 종류와 길이가 다릅니다)를 교차적으로 조사하고, SUnsET 데이터에 의해 적용된 LLMs는 기초 모델보다 관련성이 높은 사실적으로 일관된 증거를 생성하고, 컨텍스트의 다양한 위치에서 증거를 요약하고, 관련성이 높은 일관된 요약을 생성하는 것을 보여주었습니다.",
      "upvotes": 0,
      "discussionId": "67b83a21a9fa331061e84f36"
    },
    "publishedAt": "2025-02-21T03:33:40.641Z",
    "title": "Unstructured Evidence Attribution for Long Context Query Focused Summarization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14409.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60a643b9213fe60589b8fdf9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a643b9213fe60589b8fdf9/OOXmW3MkSf88r63tAE6-n.jpeg",
      "fullname": "Dustin Wright",
      "name": "dwright37",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]