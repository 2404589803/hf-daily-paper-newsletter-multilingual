[
  {
    "paper": {
      "id": "2505.04620",
      "authors": [
        {
          "_id": "681c6c1817fc8222eff39a1a",
          "user": {
            "_id": "647773a1168cb428e00e9a8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
            "isPro": false,
            "fullname": "Hao Fei",
            "user": "scofield7419",
            "type": "user"
          },
          "name": "Hao Fei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:07.591Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1b",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1c",
          "user": {
            "_id": "67bc247b593452cc18965cb1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/EA3kTYaaff0Hr7-dGiOOj.png",
            "isPro": false,
            "fullname": "JUNCHENG LI",
            "user": "JunchengLi",
            "type": "user"
          },
          "name": "Juncheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:36:52.461Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1d",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:36:59.117Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1e",
          "name": "Qingshan Xu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1f",
          "name": "Bobo Li",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a20",
          "user": {
            "_id": "64c139d867eff857ea51caa8",
            "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
            "isPro": false,
            "fullname": "Shengqiong Wu",
            "user": "ChocoWu",
            "type": "user"
          },
          "name": "Shengqiong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:39.333Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a21",
          "user": {
            "_id": "64ff369d9abcc85a5519b33e",
            "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
            "isPro": false,
            "fullname": "Yaoting Wang",
            "user": "Gh0stAR",
            "type": "user"
          },
          "name": "Yaoting Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:37:27.790Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a22",
          "user": {
            "_id": "67e906836c7216f5bf91f70c",
            "avatarUrl": "/avatars/9c7f34d5b1d41ad7231d2733a399abb3.svg",
            "isPro": false,
            "fullname": "junbao.zhou",
            "user": "junbaozhou",
            "type": "user"
          },
          "name": "Junbao Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:37:34.046Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a23",
          "user": {
            "_id": "65a28e129acab19980226731",
            "avatarUrl": "/avatars/abc3828f807efc4e03837b0eae063f98.svg",
            "isPro": false,
            "fullname": "Jiahao Meng",
            "user": "marinero4972",
            "type": "user"
          },
          "name": "Jiahao Meng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:37:40.200Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a24",
          "user": {
            "_id": "656724074f6ec72017754d33",
            "avatarUrl": "/avatars/e61de248f6f53719b2375077340dd033.svg",
            "isPro": false,
            "fullname": "QingyuShi",
            "user": "QingyuShi",
            "type": "user"
          },
          "name": "Qingyu Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:34.088Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a25",
          "name": "Zhiyuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a26",
          "name": "Liangtao Shi",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a27",
          "user": {
            "_id": "648ef24dc92367eecac0f4bd",
            "avatarUrl": "/avatars/38f1afd6b52efeee3aa41cc80225d788.svg",
            "isPro": false,
            "fullname": "Minghe Gao",
            "user": "gmh5811",
            "type": "user"
          },
          "name": "Minghe Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:38:16.554Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a28",
          "user": {
            "_id": "6241b95cfee9374a2598ecfe",
            "avatarUrl": "/avatars/196669df1689a5872fc18b271e80fdc1.svg",
            "isPro": false,
            "fullname": "Zhang Daoan",
            "user": "hazard",
            "type": "user"
          },
          "name": "Daoan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:38:28.567Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a29",
          "name": "Zhiqi Ge",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2a",
          "name": "Weiming Wu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2b",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2c",
          "name": "Kaihang Pan",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2d",
          "user": {
            "_id": "662917afda1cae6cbb50cd00",
            "avatarUrl": "/avatars/aa66de6cef6665c5d67071d82bac35c4.svg",
            "isPro": false,
            "fullname": "Yaobo Ye",
            "user": "superyyb",
            "type": "user"
          },
          "name": "Yaobo Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:55:06.463Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2e",
          "user": {
            "_id": "6391e41f2e73987364e6bcb2",
            "avatarUrl": "/avatars/d09a9ee329bb8c3a9e2929d67d24e97d.svg",
            "isPro": false,
            "fullname": "Haobo Yuan",
            "user": "HarborYuan",
            "type": "user"
          },
          "name": "Haobo Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:39:11.094Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2f",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a30",
          "user": {
            "_id": "6816d98fc075e49c1b15928e",
            "avatarUrl": "/avatars/6b24d047fc25075bedb3e74f78981bc0.svg",
            "isPro": false,
            "fullname": "Tianjie Ju",
            "user": "jometeorieNUS",
            "type": "user"
          },
          "name": "Tianjie Ju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:53:06.930Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a31",
          "name": "Zixiang Meng",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a32",
          "name": "Shilin Xu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a33",
          "name": "Liyu Jia",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a34",
          "name": "Wentao Hu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a35",
          "user": {
            "_id": "64ad1c0bad6218d51a07b54e",
            "avatarUrl": "/avatars/0f84d9a51c6ca9bcef44de2d7c707d9b.svg",
            "isPro": false,
            "fullname": "LUO MENG",
            "user": "Eureka-Leo",
            "type": "user"
          },
          "name": "Meng Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:37.235Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a36",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a37",
          "name": "Tat-Seng Chua",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a38",
          "user": {
            "_id": "67eaa070b9fa8908e151fd7d",
            "avatarUrl": "/avatars/1fe2fd678d2e71099a83a9bcb9ab517e.svg",
            "isPro": false,
            "fullname": "shuicheng yan",
            "user": "shuicheng",
            "type": "user"
          },
          "name": "Shuicheng Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:52:30.205Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a39",
          "name": "Hanwang Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/TqzNcdmo0rwc-0JkEc0-i.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HBVHtWBiagedRLXD-lWZc.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/X02B6xk6CZDywtG8tGliK.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Xnc89DOmtr5j2hST5Um17.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Awl6jj9MX38cMRkgXpxHp.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HT_9Y1ponvvqUREjMTMBB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/sG3FBiQCx3EONF25cEbED.png"
      ],
      "publishedAt": "2025-05-07T17:59:32.000Z",
      "submittedOnDailyAt": "2025-05-09T01:19:17.510Z",
      "title": "멀티모달 젖기 계획：일반 수준과 일반 벤치마크",
      "submittedOnDailyBy": {
        "_id": "647773a1168cb428e00e9a8f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
        "isPro": false,
        "fullname": "Hao Fei",
        "user": "scofield7419",
        "type": "user"
      },
      "summary": "다모뎅대언어모델(MLLM)은 LLM의 첨단 능력을 통해 급격히 성장하고 있습니다. 지난주 전문가와는 달리 현재의 MLLM은 다모뎅ジャンプ파라다이움에 대한 발전을 보이고 있습니다. 초기에는 여러 모델을 이해하는 데만 제한되어 있었지만, 이 모델들은 동시에 이해하고 모델을 생성하는 것이 가능해지고, 그 능력은 coarse-grained에서 fine-grained 다모뎅모델 이해로, 특정한 모델에서 임의의 모델으로 확장되었습니다. 모델의 성능을 평가하기 위해 여러 벤치마크가 존재하지만, 중요한 문제가 있습니다: 각 태스크에서 높은 성능이 MLLM의 강력한 능력을 보여주는 것이 가능한가? 이러한 모델이 실제 AI에 가까워지는지 확인하는가? 이 프로젝트에서는 General-Level라는 평가 프레임워크를 소개하고, MLLM의 성능과 일반성을 5스케일로 정의하여 MLLM의 비교와 현재 시스템의 발전을 평가하는 방법을 제공합니다. 이 프레임워크의 핵심은 simplicity이라는 개념입니다. 이는 모델이 이해와 생성, 또는 여러 모델에서의 능력의 일관성을 측정합니다. 이를 위해 General-Bench를 제공하며, 기술, 모델, 형식, 능력의 광범위한 범위를 포함합니다. 현재의 100개 이상의 가장 선진된 MLLM을 포함하는 평가 결과를 제공하여 일반적인 모델의 능력 순위를 보여주고, 실제 AI에 이르는 도전을 명확히 합니다. 이 프로젝트는 다음 세대의 다모뎅기반 모델의 연구를 열어, AGI의 실현을 촉진하는 강력한 인프라를 제공합니다. 프로젝트 페이지: https://generalist.top/",
      "upvotes": 42,
      "discussionId": "681c6c1d17fc8222eff39b45",
      "projectPage": "https://generalist.top/",
      "githubRepo": "https://github.com/path2generalist/General-Level",
      "ai_keywords": [
        "Multimodal Large Language Model (MLLM)",
        "Multimodal Generalist",
        "multimodal understanding",
        "comprehension",
        "generation",
        "General-Level",
        "Synergy",
        "General-Bench",
        "AGI (Artificial General Intelligence)"
      ]
    },
    "publishedAt": "2025-05-07T13:59:32.000Z",
    "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
    "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/TqzNcdmo0rwc-0JkEc0-i.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HBVHtWBiagedRLXD-lWZc.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/X02B6xk6CZDywtG8tGliK.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Xnc89DOmtr5j2hST5Um17.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Awl6jj9MX38cMRkgXpxHp.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HT_9Y1ponvvqUREjMTMBB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/sG3FBiQCx3EONF25cEbED.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04620.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "647773a1168cb428e00e9a8f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
      "fullname": "Hao Fei",
      "name": "scofield7419",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05470",
      "authors": [
        {
          "_id": "681d9829edf34a77aab565eb",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ec",
          "user": {
            "_id": "6553316bf151de82f6a23e1d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6553316bf151de82f6a23e1d/GTBkSj4Fa3OoyM6Muz_Sc.jpeg",
            "isPro": false,
            "fullname": "Gongye Liu",
            "user": "liuhuohuo",
            "type": "user"
          },
          "name": "Gongye Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:41:47.403Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ed",
          "name": "Jiajun Liang",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ee",
          "user": {
            "_id": "64d71083a787c9bc7b9f1238",
            "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
            "isPro": false,
            "fullname": "Yangguang Li",
            "user": "Lp256",
            "type": "user"
          },
          "name": "Yangguang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:43:44.697Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ef",
          "user": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "isPro": false,
            "fullname": "Jiaheng Liu",
            "user": "CheeryLJH",
            "type": "user"
          },
          "name": "Jiaheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:45:07.297Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f0",
          "user": {
            "_id": "60e272ca6c78a8c122b12127",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
            "isPro": false,
            "fullname": "Xintao Wang",
            "user": "Xintao",
            "type": "user"
          },
          "name": "Xintao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:42:27.977Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f1",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f2",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:43:01.366Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f3",
          "name": "Wanli Ouyang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T17:58:45.000Z",
      "submittedOnDailyAt": "2025-05-09T05:45:53.355Z",
      "title": "Flow-GRPO: 온라인 RL를 이용한 흐름 매칭 모델의 훈련",
      "submittedOnDailyBy": {
        "_id": "64d71083a787c9bc7b9f1238",
        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
        "isPro": false,
        "fullname": "Yangguang Li",
        "user": "Lp256",
        "type": "user"
      },
      "summary": "Flow-GRPO를 제안합니다. 이것은 첫 번째로 유용한 방법입니다. 우리 접근법은 두 가지 핵심 전략을 사용합니다.\n\n(1) ODE-to-SDE 변환: 결정적인 일반 미분 방정식(ODE)을 시간 단계 모두에서 원래 모델의 경계 분포를 매칭하는 등가의 랜덤 미분 방정식(SDE)으로 변환합니다. 이로 인해 RL 검색의 통계적 샘플링이 가능합니다.\n\n(2) Denoising Reduction 전략: 원의 추론 시간 단계 수를 유지하면서 딕노이징을 줄이면 샘플링 효율을 크게 향상시킬 수 있습니다.\n\n실험적으로, Flow-GRPO는 복잡한 조직에서도 효과적이며, 복잡한 조직에서는 RL 조정된 SD3.5는 근사적으로 객체 수, 공간 관계, 작은 속성을 생성하고 GenEval의 정확도를 63%에서 95%까지 올립니다. 시각화 텍스트 렌더링에서 정확도는 59%에서 92%까지 올립니다. Flow-GRPO는 인간의 취향의 일치에도 큰 효과를 발휘합니다. 특히, 보상 힙킹은 발생하지 않았습니다. 이는 이미지의 품질이나 다양성을 감소시키지 않고 보상이 증가하지 않았음을 의미하며, 둘 다 실험에서 안정적이었습니다.",
      "upvotes": 26,
      "discussionId": "681d982aedf34a77aab56635",
      "ai_keywords": [
        "Flow-GRPO",
        "reinforcement learning (RL)",
        "flow matching models",
        "ODE-to-SDE conversion",
        "Ordinary Differential Equation (ODE)",
        "Stochastic Differential Equation (SDE)",
        "Denoising Reduction strategy",
        "GenEval accuracy",
        "text-to-image tasks",
        "SD3.5",
        "visual text rendering",
        "human preference alignment",
        "reward hacking"
      ]
    },
    "publishedAt": "2025-05-08T13:58:45.000Z",
    "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
    "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05470.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64d71083a787c9bc7b9f1238",
      "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
      "fullname": "Yangguang Li",
      "name": "Lp256",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04921",
      "authors": [
        {
          "_id": "681dbb9988ca86d430f1d0d2",
          "user": {
            "_id": "62fdb01bc1588e1d4c6c1a7c",
            "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
            "isPro": false,
            "fullname": "Yunxin Li",
            "user": "YunxinLi",
            "type": "user"
          },
          "name": "Yunxin Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:05:42.761Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d3",
          "user": {
            "_id": "64380ae1819f3ab20d17431b",
            "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
            "isPro": false,
            "fullname": "ZhenyuLiu",
            "user": "foggyforest",
            "type": "user"
          },
          "name": "Zhenyu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T10:08:42.493Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d4",
          "user": {
            "_id": "67ecc6a08647cfa1775a9fda",
            "avatarUrl": "/avatars/bb15abd7a3d2c51380b0b1f819ef76e2.svg",
            "isPro": false,
            "fullname": "Zitao Li",
            "user": "TerenceL-TL",
            "type": "user"
          },
          "name": "Zitao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T08:33:26.702Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d5",
          "name": "Xuanyu Zhang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d6",
          "user": {
            "_id": "639c379cdb7c5f35004066cb",
            "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
            "isPro": false,
            "fullname": "Zhenran Xu",
            "user": "imryanxu",
            "type": "user"
          },
          "name": "Zhenran Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T10:08:40.339Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d7",
          "name": "Xinyu Chen",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d8",
          "name": "Haoyuan Shi",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d9",
          "name": "Shenyuan Jiang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0da",
          "name": "Xintong Wang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0db",
          "name": "Jifang Wang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0dc",
          "name": "Shouzheng Huang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0dd",
          "name": "Xinping Zhao",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0de",
          "name": "Borui Jiang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0df",
          "name": "Lanqing Hong",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e0",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e1",
          "name": "Zhuotao Tian",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e2",
          "name": "Baoxing Huai",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e3",
          "name": "Wenhan Luo",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e4",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e5",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e6",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e7",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T03:35:23.000Z",
      "submittedOnDailyAt": "2025-05-09T06:54:36.013Z",
      "title": "知觉、理由、思考、计划：大规模多模态模型的理由论调查",
      "submittedOnDailyBy": {
        "_id": "64380ae1819f3ab20d17431b",
        "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
        "isPro": false,
        "fullname": "ZhenyuLiu",
        "user": "foggyforest",
        "type": "user"
      },
      "summary": "이론이 지능의 중심에 위치하며, 결정력, 결론추출력, 그리고 영역을 초월하는 일반화 능력이 형성되어 있습니다. 인공지능 분야에서 시스템이 개방적이고 불확실한, 다양한 환경에서 작동하도록 개발될 때, 이론이 강력한, 적응적인 행동을 가능하게 하는 데 중요해집니다. 대규모 다형 이론 모델(LMRMs)은 텍스트, 이미지, 음성, 영상 등 다양한 모델을 통합하여 복잡한 이론 능력을 지원하며, 전체적인 인식, 정밀한 이해, 깊은 이론을 목표로 등장했습니다. 연구가 진행될수록, 다형 이론은 모듈화된 인식을 주도하는 파이프라인에서부터, 연속적인 언어 중심적인 프레임워크로 급격히 발전했습니다. 이 과정에서 명령 조정과 강화학습이 모델의 이론을 개선했지만, 모든 유형의 일반화, 이론의 깊이, 그리고 출력 행동에 대해서는 큰 문제를 남겨 놨습니다. 이러한 문제를 해결하기 위해, 우리는 필드의 변화된 설계 철학과 새로운 능력에 반영된 4단계 개발 로마트에 기반하여, 상세한 구조화된 다형 이론 연구 조사를 제공합니다. 먼저, 특정한 태스크에 기반한 모듈의 초기 노력을 조사하고, 이론이 표현, 어레이, 융합의 단계에서 숨겨져 있는 것처럼 엮여 있었습니다. 다음으로, 최근의 접근을 살펴보며, 이론을 다형 LLMs에 통합하고, Multimodal Chain-of-Thought(MCoT) 및 다형 강화학습 등 발전이 복잡한, 더 구조화된 이론 체인을 가능하게 합니다. 마지막으로, OpenAI O3와 O4-mini의 어려운 벤치마크와 실험 사례를 기반으로, 본질적인 대규모 다형 이론 모델(N-LMRMs)의 개념적인 방향을 논의하고, 복잡한, 현실적인 환경에서 scalable한, 출력의, 적응적인 이론과 계획을 지원하는 것을 목표로 합니다.",
      "upvotes": 21,
      "discussionId": "681dbb9b88ca86d430f1d183",
      "projectPage": "https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models",
      "githubRepo": "https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models",
      "ai_keywords": [
        "Large Multimodal Reasoning Models (LMRMs)",
        "multimodal reasoning",
        "Cross-modal understanding",
        "task-specific modules",
        "representation",
        "alignment",
        "fusion",
        "Multimodal Chain-of-Thought (MCoT)",
        "multimodal reinforcement learning",
        "native large multimodal reasoning models (N-LMRMs)",
        "scalable",
        "agentic",
        "adaptive reasoning",
        "planning"
      ]
    },
    "publishedAt": "2025-05-07T23:35:23.000Z",
    "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models",
    "summary": "Reasoning lies at the heart of intelligence, shaping the ability to make\ndecisions, draw conclusions, and generalize across domains. In artificial\nintelligence, as systems increasingly operate in open, uncertain, and\nmultimodal environments, reasoning becomes essential for enabling robust and\nadaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a\npromising paradigm, integrating modalities such as text, images, audio, and\nvideo to support complex reasoning capabilities and aiming to achieve\ncomprehensive perception, precise understanding, and deep reasoning. As\nresearch advances, multimodal reasoning has rapidly evolved from modular,\nperception-driven pipelines to unified, language-centric frameworks that offer\nmore coherent cross-modal understanding. While instruction tuning and\nreinforcement learning have improved model reasoning, significant challenges\nremain in omni-modal generalization, reasoning depth, and agentic behavior. To\naddress these issues, we present a comprehensive and structured survey of\nmultimodal reasoning research, organized around a four-stage developmental\nroadmap that reflects the field's shifting design philosophies and emerging\ncapabilities. First, we review early efforts based on task-specific modules,\nwhere reasoning was implicitly embedded across stages of representation,\nalignment, and fusion. Next, we examine recent approaches that unify reasoning\ninto multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)\nand multimodal reinforcement learning enabling richer and more structured\nreasoning chains. Finally, drawing on empirical insights from challenging\nbenchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the\nconceptual direction of native large multimodal reasoning models (N-LMRMs),\nwhich aim to support scalable, agentic, and adaptive reasoning and planning in\ncomplex, real-world environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64380ae1819f3ab20d17431b",
      "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
      "fullname": "ZhenyuLiu",
      "name": "foggyforest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02847",
      "authors": [
        {
          "_id": "681d7031e9969eecfcb4eb81",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb82",
          "user": {
            "_id": "648294b2eb4befee378951c1",
            "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
            "isPro": false,
            "fullname": "Ruotian Ma",
            "user": "vvibt",
            "type": "user"
          },
          "name": "Ruotian Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:30.886Z",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb83",
          "name": "Qingxuan Jiang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb84",
          "name": "Peisong Wang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb85",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb86",
          "name": "Zheng Xie",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb87",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb88",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb89",
          "name": "Fanghua Ye",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8a",
          "name": "Jian Li",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8b",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8c",
          "user": {
            "_id": "67485743561b1e6f9579389f",
            "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
            "isPro": false,
            "fullname": "Zhaopeng Tu",
            "user": "zptu",
            "type": "user"
          },
          "name": "Zhaopeng Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:28.468Z",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8d",
          "name": "Xiaolong Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T19:06:10.000Z",
      "submittedOnDailyAt": "2025-05-09T01:37:10.548Z",
      "title": "신텐토 에이전트를 재판관으로: 대규모 언어 모델의 고급 사회 인지를 평가하기 위한 연구\n\n(注意：虽然要求不添加额外文本，但为了确保翻译的准确性和专业性，这里提供了一个更加正式和精确的翻译版本。)",
      "submittedOnDailyBy": {
        "_id": "648294b2eb4befee378951c1",
        "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
        "isPro": false,
        "fullname": "Ruotian Ma",
        "user": "vvibt",
        "type": "user"
      },
      "summary": "대 언어 모델(LLM)가 인간을 이해하는 데, 그 뿐만 아니라 문장을 이해하는 데 얼마나 이해를 하는지에 대한 평가는 개방적인 도전입니다. 이를 해결하기 위해, 우리는 \"Sentient Agent as a Judge\" (SAGE)라는 자동 평가 프레임워크를 소개합니다. 이 프레임워크는 LLM의 고차원 사회적 인지를 측정하기 위한 것입니다. SAGE는 인간처럼 감정 변화를 모방하고 내면의 생각을 모방하여, 여러 차례의 대화를 통해 모델을 더욱 현실적으로 평가하는 것을 목표로 합니다. 각 차례에서, 이 에이전트는 (i) 감정이 어떻게 변화하고 (ii) 어떻게 느끼고 (iii) 어떻게 응답하는지를 근거로, 숫자적인 감정궤도와 해석 가능한 내면의 생각을 제공합니다. 100개 대화의 스케너에서의 실험은 최종적인 Sentient 감정 점수가 Barrett-Lennard Relationship Inventory (BLRI)의 점수와 유타ن 수준의 에마팀을 강한 상관관계를 나타내며, 심리적인 신뢰성을 증명합니다. 또한, 18개의 상업 모델과 오픈 소스 모델을 커버하는 공개 산체 실험실도 구축되었으며, 가장 최신 시스템(GPT-4o-Latest, Gemini2.5-Pro)과 초기 모델 사이에 큰 차이(최대 4배)를 밝혀줍니다. 이는 일반적인 실험실(예: Arena)에서 반영되지 않는 오류입니다. SAGE는 실제로 에마팀이나 사회적으로 우수한 언어 에이전트의 발전을 추적하는 데 필요한 원칙적이고 확장 가능한, 해석 가능한 도구로 제공됩니다.",
      "upvotes": 15,
      "discussionId": "681d7033e9969eecfcb4ec2d",
      "githubRepo": "https://github.com/Tencent/digitalhuman/tree/main/SAGE",
      "ai_keywords": [
        "Sentient Agent as a Judge (SAGE)",
        "higher-order social cognition",
        "emotional changes",
        "inner thoughts",
        "multi-turn conversations",
        "numerical emotion trajectory",
        "Barrett-Lennard Relationship Inventory (BLRI)",
        "utterance-level empathy metrics",
        "psychological fidelity",
        "Sentient Leaderboard",
        "empathetic",
        "socially adept language agents"
      ]
    },
    "publishedAt": "2025-05-01T15:06:10.000Z",
    "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models",
    "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02847.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "648294b2eb4befee378951c1",
      "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
      "fullname": "Ruotian Ma",
      "name": "vvibt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05315",
      "authors": [
        {
          "_id": "681d7ccb572e742b3f42d1f3",
          "user": {
            "_id": "6602869253a0518b2a98cafd",
            "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
            "isPro": false,
            "fullname": "Yuhui Xu",
            "user": "yuhuixu",
            "type": "user"
          },
          "name": "Yuhui Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:04.644Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f4",
          "user": {
            "_id": "63a3ff69f91ad3ea5703841d",
            "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
            "isPro": false,
            "fullname": "Hanze Dong",
            "user": "hendrydong",
            "type": "user"
          },
          "name": "Hanze Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:10.829Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f5",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f6",
          "user": {
            "_id": "65f84fd980481173afd91233",
            "avatarUrl": "/avatars/6ac7bd6beba24d1476c5179b88c9e3fa.svg",
            "isPro": false,
            "fullname": "Doyen",
            "user": "doyensahoo",
            "type": "user"
          },
          "name": "Doyen Sahoo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:18.676Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f7",
          "user": {
            "_id": "61f9d3b54ac99e8a1bae85f4",
            "avatarUrl": "/avatars/ac47d13204dd22452e4bc46e280842d5.svg",
            "isPro": false,
            "fullname": "JunnanLi",
            "user": "JunnanLi",
            "type": "user"
          },
          "name": "Junnan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:32.272Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f8",
          "user": {
            "_id": "649dbcc4e0fff1ed099dc80a",
            "avatarUrl": "/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg",
            "isPro": false,
            "fullname": "Caiming Xiong",
            "user": "cxiong",
            "type": "user"
          },
          "name": "Caiming Xiong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:38.430Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T15:01:06.000Z",
      "submittedOnDailyAt": "2025-05-09T02:31:21.542Z",
      "title": "스케일러블 컨티뉴스 스코프스 바이 엘레스포ン드 리지니징",
      "submittedOnDailyBy": {
        "_id": "6602869253a0518b2a98cafd",
        "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
        "isPro": false,
        "fullname": "Yuhui Xu",
        "user": "yuhuixu",
        "type": "user"
      },
      "summary": "대논리 모델(LRMs)은 복잡한 태스크에서 긴 추론 체인(CoT)을 생성함으로써 놀라운 진전을 달성했습니다. 그러나 제한없는 출력 길이는 실제 세계적인 구현에서 토큰, 라틴어, 또는 컴퓨팅의 추론 시의 방식이 엄격하게 제한되어 있을 때 큰 문제를 겪습니다. 우리는 스케일러블한 긴 추론의 새로운 프레임워크를 제안합니다. 이것은 독립된 이유로 분배된 방식이며, '생각하기'와 '해결책'의 두 단계에 명확히 구분하여 이유를 나누는 것입니다. 테스트 시에 Elastic Reasoning은 해결책 섹션의 완전성을 우선시하고, 긴장된 리소스 제약 하에서 신뢰성을 크게 향상시킵니다. 트랜크된 생각 과정에 대해 강건한 모델을 훈련하기 위해, GRPO에 통합된 가벼운 방식 제약 로우스틱스 구조를 도입합니다. 이 구조는 생각 과정이 짧아지면 가장 적절한 이유를 제시하도록 모델을 교육하고, 새로운 방식 제약에 효과적으로 일반화하며 추가적인 훈련이 필요하지 않습니다. 수학(AIME, MATH500)과 프로그래밍(LiveCodeBench, Codeforces)의 벤치마크에서 실험 결과, Elastic Reasoning은 엄격한 방식 제약 하에서도 강건하게 동작하고, 베이스 메소드보다 크게 낮은 훈련 비용을 요구합니다. 특히, 우리의 접근법은 제한없는 설정에서도 더 간결하고 효율적인 이유를 생성합니다. Elastic Reasoning은 스케일러블한 제어 가능한 이유의 급한 문제에 원리적이고 실용적인 해결책을 제공합니다.",
      "upvotes": 13,
      "discussionId": "681d7ccc572e742b3f42d21a",
      "ai_keywords": [
        "Large reasoning models (LRMs)",
        "chain of thought (CoT)",
        "inference-time budgets",
        "tokens",
        "latency",
        "compute",
        "Elastic Reasoning",
        "scalable chain of thoughts",
        "thinking phase",
        "solution phase",
        "independently allocated budgets",
        "completeness of solution segments",
        "reliability",
        "resource constraints",
        "lightweight budget-constrained rollout strategy",
        "GRPO",
        "adaptive reasoning",
        "unseen budget constraints",
        "mathematical benchmarks (AIME, MATH500)",
        "programming benchmarks (LiveCodeBench, Codeforces)",
        "unconstrained settings",
        "principled solution"
      ]
    },
    "publishedAt": "2025-05-08T11:01:06.000Z",
    "title": "Scalable Chain of Thoughts via Elastic Reasoning",
    "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602869253a0518b2a98cafd",
      "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
      "fullname": "Yuhui Xu",
      "name": "yuhuixu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05071",
      "authors": [
        {
          "_id": "681da6375f701833274a0d21",
          "user": {
            "_id": "6621e591c50869c1e91a1639",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621e591c50869c1e91a1639/L_PoEn2BRAJcnWZX-JebR.jpeg",
            "isPro": false,
            "fullname": "Chunyu Xie",
            "user": "xiechunyu",
            "type": "user"
          },
          "name": "Chunyu Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:03.296Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d22",
          "user": {
            "_id": "5e49e8cf37cb5b49818287ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e49e8cf37cb5b49818287ae/IV9b5Z70NhgmBNfAlc_co.jpeg",
            "isPro": false,
            "fullname": "Bin Wang",
            "user": "binwang",
            "type": "user"
          },
          "name": "Bin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:33.968Z",
          "hidden": true
        },
        {
          "_id": "681da6375f701833274a0d23",
          "user": {
            "_id": "632c098b456c31252774e7c5",
            "avatarUrl": "/avatars/e3720d2fcb69d93c8d5aa5f50aab5f0e.svg",
            "isPro": false,
            "fullname": "kong",
            "user": "fanjing",
            "type": "user"
          },
          "name": "Fanjing Kong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:42.881Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d24",
          "user": {
            "_id": "65b793b374698ba5a815bf4f",
            "avatarUrl": "/avatars/44a7e694a5089dbc773018111270ac26.svg",
            "isPro": false,
            "fullname": "Jincheng Li",
            "user": "jinchenglijc",
            "type": "user"
          },
          "name": "Jincheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:50.538Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d25",
          "user": {
            "_id": "659b8576999b82db2ad8a398",
            "avatarUrl": "/avatars/2ec7663e25e4a0238819818e69d9a5bd.svg",
            "isPro": false,
            "fullname": "Liang",
            "user": "DaweiLiang",
            "type": "user"
          },
          "name": "Dawei Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:56.445Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d26",
          "name": "Gengshen Zhang",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d27",
          "user": {
            "_id": "649935abbe8fd92c27ab1ed8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
            "isPro": false,
            "fullname": "David Leon",
            "user": "DavidLeon",
            "type": "user"
          },
          "name": "Dawei Leng",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-09T07:00:10.249Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d28",
          "name": "Yuhui Yin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T09:06:53.000Z",
      "submittedOnDailyAt": "2025-05-09T05:27:38.509Z",
      "title": "FG-CLIP: 세부화된 시각적 및 문자적 어레이멘트",
      "submittedOnDailyBy": {
        "_id": "649935abbe8fd92c27ab1ed8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
        "isPro": false,
        "fullname": "David Leon",
        "user": "DavidLeon",
        "type": "user"
      },
      "summary": "Contrastive Language-Image Pre-training (CLIP)는 이미지-텍스트 검색과 0샷 분류 등 다양한 태스크에 뛰어난 성능을 발휘하지만, 핵심 문장의 짧은 캡션을 중심으로 동작하여 미세한 이해에 적합하지 않습니다. 이러한 문제를 해결하기 위해, 우리는 Fine-Grained CLIP (FG-CLIP)를 제안합니다. FG-CLIP는 세 가지 핵심 인нова션을 통해 미세한 이해를 강화합니다. 먼저, 대규모 다양화 모델을 활용하여 16억 개의 긴 캡션-이미지 쌍을 생성하고, 글로벌 수준의 의미적 세부 사항을 파악할 수 있습니다. 다음으로, 1200만 장의 이미지와 4000만 쌍의 지역특성의 bounding box를 포함하는 고품질 데이터셋을 구축하여, 세부적인 캡션과 맥락을 가진 정밀한 표현을 보장합니다. 마지막으로, 1000만 쌍의 어려운 미세한 부정 샘플을 추가하여 모델이 의미적 미묘한 차이를 구분하는 능력을 향상시킵니다. 이러한 데이터에 대응하는 훈련 방법은 또한 미세하게 설계되었습니다. 확장된 실험은 FG-CLIP이 미세한 이해, 오픈 박스 대상물 검출, 이미지-텍스트 검색, 그리고 일반적인 다양화 벤치마크에서 원래의 CLIP이나 다른 최전단 방법의 성능을 초과하는 성능을 보여주었습니다. 이러한 결과를 통해, FG-CLIP이 미세한 이미지 세부 사항을 파악하고 전체 모델 성능을 향상시키는 효과가 명확히 드러났습니다. 관련 데이터, 코드, 모델은 https://github.com/360CVGroup/FG-CLIP에 제공됩니다.",
      "upvotes": 8,
      "discussionId": "681da6385f701833274a0d8a",
      "githubRepo": "https://github.com/360CVGroup/FG-CLIP",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training (CLIP)",
        "image-text retrieval",
        "zero-shot classification",
        "fine-grained understanding",
        "coarse-grained short captions",
        "multimodal models",
        "1.6 billion long caption-image pairs",
        "high-quality dataset",
        "12 million images",
        "40 million region-specific bounding boxes",
        "detailed captions",
        "10 million hard fine-grained negative samples",
        "fine-grained understanding",
        "open-vocabulary object detection",
        "general multimodal benchmarks",
        "FG-CLIP"
      ]
    },
    "publishedAt": "2025-05-08T05:06:53.000Z",
    "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment",
    "summary": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05071.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649935abbe8fd92c27ab1ed8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
      "fullname": "David Leon",
      "name": "DavidLeon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05474",
      "authors": [
        {
          "_id": "681d7b5ae27a030c96a28bde",
          "user": {
            "_id": "672392c4a4c4381cefc06416",
            "avatarUrl": "/avatars/8ee84a7e3e91e5d13074bc3c407ff75d.svg",
            "isPro": false,
            "fullname": "Wen Beichen",
            "user": "wenbc21",
            "type": "user"
          },
          "name": "Beichen Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:55.900Z",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28bdf",
          "user": {
            "_id": "63f47b5321eb234ab739e91a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
            "isPro": false,
            "fullname": "Haozhe Xie",
            "user": "hzxie",
            "type": "user"
          },
          "name": "Haozhe Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:02.341Z",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be0",
          "user": {
            "_id": "62fc8cf7ee999004b5a8b982",
            "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
            "isPro": false,
            "fullname": "Zhaoxi Chen",
            "user": "FrozenBurning",
            "type": "user"
          },
          "name": "Zhaoxi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:08.543Z",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be1",
          "name": "Fangzhou Hong",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be2",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:27.473Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/PqIosa0nVWamlNxCmd6w1.webp"
      ],
      "publishedAt": "2025-05-08T17:59:54.000Z",
      "submittedOnDailyAt": "2025-05-09T02:21:54.023Z",
      "title": "3D 스키 메커니즘 생성: 개요",
      "submittedOnDailyBy": {
        "_id": "63f47b5321eb234ab739e91a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
        "isPro": false,
        "fullname": "Haozhe Xie",
        "user": "hzxie",
        "type": "user"
      },
      "summary": "3Dシーン生成는, 満喫メディア, ロボット工学, 自動運転, 具象AIなど의アプリケーションでは, 공간적으로 구조付け, 語意的に意味を持つ, 写実的な環境の合成を目的としています。早期のプロセス順序に基づく方法はスケーラブルでしたが, 多様性が限られていました。深層生成モデル(例: GAN, ディフュージョンモデル)と3D表現(例: NeRF, 3Dガウス)の最近の進展は, 実世界のシーン分布の学習を可能にし, 忠実度, 多様性, 視点の一致性を向上させました。最近の進歩では, ディフュージョンモデルなどの機能を通じて, 3Dシーン合成と写実性を結びつけるために, 生成を画像またはビデオ合成の問題として再構成しました。この調査は, 最先端のアプローチのシステマティックな概要を提供し, プロセス順序生成, ニューラル3Dベース生成, 画像ベース生成, ビデオベース生成の4つのパラダイムに組み立てています。技術的な基礎, トレードオフ, 代表的な結果を分析し, 通常のデータセット, 評価プロトコル, ダウンストリームアプリケーションを紹介します。最後に, 生成能力, 3D表現, データと注釈, 評価の関連する主要な課題を議論し, 高い忠実度, 物理的知識を持つおよび相互作用生成, ユニットポーション認識生成モデルなどの有望な方向を示します。このレビューは, 生成AI, 3Dビジョン, 具象知能の交差点での最近の進歩を組み立て, 生成AI, 3Dビジョン, 具象知能の交差点での有望な方向を明らかにします。進歩を追跡するために, 最新のプロジェクトページを維持しています: https://github.com/hzxie/Awesome-3D-Scene-Generation.",
      "upvotes": 7,
      "discussionId": "681d7b5be27a030c96a28c29",
      "githubRepo": "https://github.com/hzxie/Awesome-3D-Scene-Generation",
      "ai_keywords": [
        "deep generative models",
        "GANs",
        "diffusion models",
        "NeRF",
        "3D Gaussians",
        "procedural generation",
        "neural 3D-based generation",
        "image-based generation",
        "video-based generation"
      ]
    },
    "publishedAt": "2025-05-08T13:59:54.000Z",
    "title": "3D Scene Generation: A Survey",
    "summary": "3D scene generation seeks to synthesize spatially structured, semantically\nmeaningful, and photorealistic environments for applications such as immersive\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\nprocedural rules offered scalability but limited diversity. Recent advances in\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\ndistributions, improving fidelity, diversity, and view consistency. Recent\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\nreframing generation as image or video synthesis problems. This survey provides\na systematic overview of state-of-the-art approaches, organizing them into four\nparadigms: procedural generation, neural 3D-based generation, image-based\ngeneration, and video-based generation. We analyze their technical foundations,\ntrade-offs, and representative results, and review commonly used datasets,\nevaluation protocols, and downstream applications. We conclude by discussing\nkey challenges in generation capacity, 3D representation, data and annotations,\nand evaluation, and outline promising directions including higher fidelity,\nphysics-aware and interactive generation, and unified perception-generation\nmodels. This review organizes recent advances in 3D scene generation and\nhighlights promising directions at the intersection of generative AI, 3D\nvision, and embodied intelligence. To track ongoing developments, we maintain\nan up-to-date project page:\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/PqIosa0nVWamlNxCmd6w1.webp"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05474.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f47b5321eb234ab739e91a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
      "fullname": "Haozhe Xie",
      "name": "hzxie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05327",
      "authors": [
        {
          "_id": "681d95bc11abe59dc97e4c5a",
          "user": {
            "_id": "647e99d9becb41a272970ca4",
            "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
            "isPro": false,
            "fullname": "Ann",
            "user": "yyxsghx",
            "type": "user"
          },
          "name": "Yixin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:18.753Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5b",
          "user": {
            "_id": "670740744341dcee459fb990",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/66UkZvrAk7fQr5YCylEFk.png",
            "isPro": false,
            "fullname": "Qingxiu Dong",
            "user": "Rsy24",
            "type": "user"
          },
          "name": "Qingxiu Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:38.844Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5c",
          "user": {
            "_id": "655ca347f426a304c6b393a1",
            "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
            "isPro": false,
            "fullname": "Linli Yao",
            "user": "yaolily",
            "type": "user"
          },
          "name": "Linli Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:47.561Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5d",
          "user": {
            "_id": "654cca3fe1b4cd6d40d5a7ae",
            "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
            "isPro": false,
            "fullname": "Fangwei Zhu",
            "user": "soliz1998",
            "type": "user"
          },
          "name": "Fangwei Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:53.796Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5e",
          "name": "Zhifang Sui",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T15:17:37.000Z",
      "submittedOnDailyAt": "2025-05-09T07:13:58.961Z",
      "title": "ICon: 컨텍스트 내의 자동 데이터 선택의 기여\n\n이 번역은 전문성과 정확성을 유지하였습니다.",
      "submittedOnDailyBy": {
        "_id": "647e99d9becb41a272970ca4",
        "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
        "isPro": false,
        "fullname": "Ann",
        "user": "yyxsghx",
        "type": "user"
      },
      "summary": "데이터 선택은 대규모 언어 모델(LLMs)의 성능 향상과 훈련 비용 감소에 있어서 중요합니다. 그러나 현재의 자동화된 선택 방법은 계산적으로 고가한 경사 기반 측정이나 수동 설계된 휴리스틱에 의존하고 있으며, 이는 데이터의 고유한 특성을 완전히 활용할 수 없습니다. 본 논문에서는 In-context Learning for Contribution Measurement(ICon)이라는 새로운 경사 없는 방법을 제안합니다. 이 방법은 In-context Learning(ICL)의 잠재적인 미세 조정의 특성을 활용하여, 경사 계산이나 수동 프로젝트 매뉴얼의 설계를 필요로 하지 않는 방식으로 샘플의 기여도를 평가할 수 있습니다. ICon은 경사 기반의 방법보다 계산적으로 효율적인 대체로 되고, 휴리스틱 기반의 접근 방식에서 인간의 추론 편향을 줄일 수 있습니다. ICon은 3개의 구성 요소로 구성되며, ICL에 의한 잠재적인 학습을 통해 성능의 변화를 평가하여 고 기여도의 데이터를 특정합니다. 3개의 LLMs와 12개의 벤치마크, 5개의 페어 세트를 사용한 확장된 실험에서 ICon의 효과성을 보여주었습니다. 특히, LLaMA3.1-8B에서 ICon으로 선택된 데이터의 15%를 사용하여 훈련된 모델은 전체 데이터 세트보다 5.42%포인트 높게 나섰으며, 일반적인 선택 방법에 대한 가장 좋은 성능을 2.06%포인트 초과했습니다. 또한, ICon으로 선택된 고 기여도의 샘플에 대한 분석을 수행하고, 이들은 다양한 태스크와 적절한 난이도 레벨을 가지고 있으며, 이들만이 가장 어려운 일로 할 수 없는 것을 보여주었습니다.",
      "upvotes": 7,
      "discussionId": "681d95bd11abe59dc97e4c87",
      "ai_keywords": [
        "in-context learning (ICL)",
        "implicit fine-tuning",
        "In-context Learning for Contribution Measurement (ICon)"
      ]
    },
    "publishedAt": "2025-05-08T11:17:37.000Z",
    "title": "ICon: In-Context Contribution for Automatic Data Selection",
    "summary": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05327.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647e99d9becb41a272970ca4",
      "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
      "fullname": "Ann",
      "name": "yyxsghx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03981",
      "authors": [
        {
          "_id": "681d7ee755699177c7fb636a",
          "user": {
            "_id": "617e7729129c9e67703ffe61",
            "avatarUrl": "/avatars/f47ee9f2f0e2b1075bebf3682ee2f817.svg",
            "isPro": false,
            "fullname": "qianchu liu",
            "user": "qianchu",
            "type": "user"
          },
          "name": "Qianchu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:18.501Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636b",
          "user": {
            "_id": "6234c11b7d5de9839bc44163",
            "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
            "isPro": false,
            "fullname": "Sheng Zhang",
            "user": "shengz",
            "type": "user"
          },
          "name": "Sheng Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:22.974Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636c",
          "user": {
            "_id": "64b8e41d52b7353d8c6dd38f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IAItP4FvD6JX9s1jwnQwF.png",
            "isPro": false,
            "fullname": "Guanghui Qin",
            "user": "hiaoxui",
            "type": "user"
          },
          "name": "Guanghui Qin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:24.320Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636d",
          "name": "Timothy Ossowski",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636e",
          "name": "Yu Gu",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636f",
          "name": "Ying Jin",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6370",
          "user": {
            "_id": "627bd86f7e62b4bf5c367108",
            "avatarUrl": "/avatars/4e87eea02d51680ebac7992dfe527e07.svg",
            "isPro": false,
            "fullname": "Sid Kiblawi",
            "user": "sidkiblawi",
            "type": "user"
          },
          "name": "Sid Kiblawi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:37.798Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6371",
          "user": {
            "_id": "65a13da85dce70a3025b7534",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zSPDBEGIULEYN4P7JCdyC.png",
            "isPro": false,
            "fullname": "Sam Preston",
            "user": "RustyArchimedes",
            "type": "user"
          },
          "name": "Sam Preston",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:43.654Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6372",
          "name": "Mu Wei",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6373",
          "user": {
            "_id": "6797f24ded1557b14d708541",
            "avatarUrl": "/avatars/d69ac80a9a500764766ce9ac7d549cc2.svg",
            "isPro": false,
            "fullname": "Paul Vozila",
            "user": "Paulvozila",
            "type": "user"
          },
          "name": "Paul Vozila",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:01.673Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6374",
          "user": {
            "_id": "5e5870466bc35159a08ca572",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e5870466bc35159a08ca572/pT6gEEs8RLRJGeM-faNWj.jpeg",
            "isPro": false,
            "fullname": "Tristan Naumann",
            "user": "tnaumann",
            "type": "user"
          },
          "name": "Tristan Naumann",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:15.476Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6375",
          "user": {
            "_id": "664d07456083f276c4feb1a4",
            "avatarUrl": "/avatars/1bfa6d8f82e9223b47630cefd79d7d0e.svg",
            "isPro": false,
            "fullname": "Hoifung Poon",
            "user": "hoifung",
            "type": "user"
          },
          "name": "Hoifung Poon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:22.468Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T21:08:27.000Z",
      "submittedOnDailyAt": "2025-05-09T02:41:23.534Z",
      "title": "X-Reasoner: 다양한 모델과 분야에서 일반적인 논리론에 대한 방향\n\n(注意：虽然您要求不添加任何解释或额外的文本，但为了确保翻译的准确性和专业性，我在翻译时考虑了句子的结构和语境，以确保翻译结果既符合原文的意思，又符合韩语的表达习惯。)",
      "submittedOnDailyBy": {
        "_id": "6234c11b7d5de9839bc44163",
        "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
        "isPro": false,
        "fullname": "Sheng Zhang",
        "user": "shengz",
        "type": "user"
      },
      "summary": "최근의 プロプライティモデル（例：o3）는 강력한 다 모델 논리 능력으로 발전하고 있습니다. 그러나 현재의 오픈 소스 연구들은 주로 문단만 대상으로 논리 모델을 훈련하고, 평가는 주로 수학적 및 일반 분야의 태스크에 제한되어 있습니다. 따라서 문단 입력과 일반 분야를 초과하는 논리 능력을 효과적으로 확장하는 방법이 명확히 밝혀지지 않았습니다. 본 논문에서는 기본적인 연구 문제를 조사하고 있습니다: 논리는 모델과 영역 모두에서 일반화 가능한가요? 우리의 발견은 긍정적인 답변을 지지하고 있습니다: 일반 분야의 문단 기반 후 프로그래밍은 이러한 강력한 일반화 가능한 논리를 가능하게 합니다. 이러한 발견을 활용하여, 우리는 X-Reasoner를 소개합니다. X-Reasoner는 일반 분야의 문단만을 대상으로 한 후 프로그래밍 된 시각 언어 모델이며, 두 단계 접근을 사용하여 일반화 가능한 논리를 실현합니다: 첫 번째 SUBJECT 경험 학습 최종 훈련 단계와, 보상을 확인 가능한 강화 학습 최종 훈련 단계입니다. 실험은 X-Reasoner는 시각 다 모델과 영역 외의 논리 능력으로 성공적으로 전이하고, 일반적인 및 의료적인 벤치마크에서 현재의 최선 모델을 초과하는 것을 보여주고 있습니다 (그림 1). 또한, X-Reasoner의 특수 영역의 성능은 영역 고유의 문단 데이터에 의한 지속적인 훈련으로 더욱 향상됩니다. 이에 따라, 우리는 X-Reasoner-Med를 소개합니다. X-Reasoner-Med는 훈련된 일반 분야 모델과 시각 다 모델을 초과하여, 훈련된 훈련 데이터에 의해 새로운 최선 모델을 달성합니다.",
      "upvotes": 5,
      "discussionId": "681d7ee855699177c7fb63b7",
      "projectPage": "https://github.com/microsoft/x-reasoner",
      "ai_keywords": [
        "multimodal reasoning",
        "vision-language model",
        "post-training",
        "long chain-of-thoughts",
        "reinforcement learning",
        "verifiable rewards",
        "X-Reasoner",
        "out-of-domain settings",
        "X-Reasoner-Med"
      ]
    },
    "publishedAt": "2025-05-06T17:08:27.000Z",
    "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains",
    "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03981.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6234c11b7d5de9839bc44163",
      "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
      "fullname": "Sheng Zhang",
      "name": "shengz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05467",
      "authors": [
        {
          "_id": "681d95b5c7ae5f65b0e55ff9",
          "user": {
            "_id": "63fee47352441fe3e87b5088",
            "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
            "isPro": false,
            "fullname": "WANG HAIBO",
            "user": "WHB139426",
            "type": "user"
          },
          "name": "Haibo Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:20.829Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffa",
          "name": "Bo Feng",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffb",
          "user": {
            "_id": "66b5295f83425904fa7a1a6a",
            "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
            "isPro": false,
            "fullname": "Zhengfeng Lai",
            "user": "jefflai",
            "type": "user"
          },
          "name": "Zhengfeng Lai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:45.264Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffc",
          "name": "Mingze Xu",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffd",
          "user": {
            "_id": "67fa856547b40f55b7ff3ce5",
            "avatarUrl": "/avatars/745937497772e9b533ba7940d758d30d.svg",
            "isPro": false,
            "fullname": "Shiyu Li",
            "user": "ShiyuLi",
            "type": "user"
          },
          "name": "Shiyu Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:01:08.366Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffe",
          "name": "Weifeng Ge",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55fff",
          "user": {
            "_id": "66fc2377516eaf950d4b8209",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mcUxUxXy18Gv9KvCW23s0.png",
            "isPro": false,
            "fullname": "Afshin Dehghan",
            "user": "afshindn",
            "type": "user"
          },
          "name": "Afshin Dehghan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:01:21.716Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e56000",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e56001",
          "name": "Ping Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T17:57:40.000Z",
      "submittedOnDailyAt": "2025-05-09T04:13:07.003Z",
      "title": "스트리밍 브릿지: 지역 비디오 대 언어 모델을 능동적인 스트리밍 보조로 변환하기",
      "submittedOnDailyBy": {
        "_id": "63fee47352441fe3e87b5088",
        "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
        "isPro": false,
        "fullname": "WANG HAIBO",
        "user": "WHB139426",
        "type": "user"
      },
      "summary": "StreamBridge는 간단하고 효과적인 프레임워크입니다. 이 프레임워크는 온라인의 Video-LLMs를 쉽게 변환하여 유동적 모델로 만드는 것을 실현합니다. StreamBridge는 기존 모델을 온라인 시나리오에 적용할 때 발생하는 두 가지 주요 문제를 해결합니다. (1) 여러턴의 시간 단위로 이해할 수 있는 능력의 제한과 (2) 동적인 응답 구조의 부족입니다. 특히, StreamBridge는 (1) 메모리 버퍼와 원주 감쇠 합성 전략을 조합하여, 긴 시퀀스의 여러턴의 상호작용을 지원합니다. (2) 분리된 가벼운 활성 모델을 사용하며, 기존 Video-LLMs에 쉽게 통합될 수 있도록 설계되어 있으며, 지속적인 동적인 응답을 가능하게 합니다. 또한, StreamBridge가 더 강력한 지원을 제공하기 위해, 유동적 비디오 이해에 적합한 대규모 데이터 세트인 Stream-IT를 구축했습니다. 이 데이터 세트는 무작위의 비디오-텍스트 시퀀스와 다양한 지시 형식을 특징으로 합니다. 확장된 실험에 따라, StreamBridge는 온라인의 Video-LLMs의 유동적 이해 능력을 크게 향상시켰으며, GPT-4o와 Gemini 1.5 Pro와 같은 프로피티 모델을 초과하는 성능을 보여주었습니다. 동시에, 표준적인 비디오 이해 벤치마크에서 상대적으로 높은 성능을 보였습니다.",
      "upvotes": 4,
      "discussionId": "681d95b6c7ae5f65b0e5606c",
      "ai_keywords": [
        "Video-LLMs",
        "streaming-capable models",
        "multi-turn real-time understanding",
        "proactive response mechanisms",
        "memory buffer",
        "round-decayed compression strategy",
        "long-context multi-turn interactions",
        "decoupled activation model",
        "Stream-IT",
        "interleaved video-text sequences",
        "standard video understanding benchmarks",
        "GPT-4o",
        "Gemini 1.5 Pro"
      ]
    },
    "publishedAt": "2025-05-08T13:57:40.000Z",
    "title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant",
    "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05467.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fee47352441fe3e87b5088",
      "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
      "fullname": "WANG HAIBO",
      "name": "WHB139426",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05469",
      "authors": [
        {
          "_id": "681db3a8a9286b53a51dc77b",
          "user": {
            "_id": "672403d5f328a3e6638331ee",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/TXr0SKWI-z-6FvUXTNWXT.jpeg",
            "isPro": false,
            "fullname": "Ava Pun",
            "user": "AvaLovelace",
            "type": "user"
          },
          "name": "Ava Pun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:33.422Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77c",
          "user": {
            "_id": "645d34ecce72244df7b29317",
            "avatarUrl": "/avatars/1248933d9f89a15e67086325a8322d5e.svg",
            "isPro": false,
            "fullname": "Kangle Deng",
            "user": "kangled",
            "type": "user"
          },
          "name": "Kangle Deng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:39.736Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77d",
          "user": {
            "_id": "658b307b75ddc76f9dc747ca",
            "avatarUrl": "/avatars/fc5393dc0bb33a8c0fea3a6f79640386.svg",
            "isPro": false,
            "fullname": "Ruixuan Liu",
            "user": "RLCMU",
            "type": "user"
          },
          "name": "Ruixuan Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:45.442Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77e",
          "user": {
            "_id": "6337151b0267ebcf02640eb6",
            "avatarUrl": "/avatars/14a723cafc5587043bdfb19304fc202d.svg",
            "isPro": false,
            "fullname": "Deva Ramanan",
            "user": "devakramanan",
            "type": "user"
          },
          "name": "Deva Ramanan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:52.637Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77f",
          "name": "Changliu Liu",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc780",
          "user": {
            "_id": "63a0acc32fabbbb899952a2b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671474335794-noauth.jpeg",
            "isPro": false,
            "fullname": "Jun-Yan Zhu",
            "user": "junyanz",
            "type": "user"
          },
          "name": "Jun-Yan Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:07:07.095Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T17:58:18.000Z",
      "submittedOnDailyAt": "2025-05-09T06:30:13.597Z",
      "title": "텍스트로부터 생성되는 물리적으로 안정적이고 구축 가능한 레기오디자인",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "レゴGPT는 문장 피드로부터 물리적으로 안정된 레ゴ 블랙 모델을 생성할 수 있는 첫 번째 접근을 소개합니다. 이를 달성하기 위해, 레ゴ 디자인의 큰 규모의 물리적으로 안정된 데이터셋을 구축하고, 관련된 캡처와 함께 다음 토큰 예측을 통해 자동 복원 크기의 큰 언어 모델을 훈련합니다. 결과의 디자인의 안정성을 향상시키기 위해, 자동 복원 추론 시 효율적인 유효성 체크와 물리적 지식에 기반한 로바ック을 사용하며, 물리 법칙과 조립 제약을 사용하여 불가행한 토큰 예측을 제거합니다. 실험에 따르면, 레ゴGPT는 입력의 문장 피드에 따라 가까운 안정된, 다양한, 아름다운 레ゴ 디자인을 생성합니다. 또한, 레ゴ 디자인을 사람이나 로봇 팔로 자동적으로 조립할 수 있는 텍스트 생성 방법을 개발합니다. 그리고 새로운 데이터셋 「StableText2Lego」을 릴리즈합니다. 이 데이터셋은 47,000 이상의 레ゴ 구조를 포함하며, 28,000가지 이상의 3D 오브젝트를 포함하고, 세부적인 캡처와 함께 프로젝트 웹 사이트에서 코드와 모델을 공개합니다.",
      "upvotes": 3,
      "discussionId": "681db3aca9286b53a51dc875",
      "ai_keywords": [
        "autoregressive large language model",
        "next-token prediction",
        "validity check",
        "physics-aware rollback",
        "autoregressive inference",
        "physics laws",
        "assembly constraints",
        "text-based LEGO texturing method",
        "automatic assembly",
        "robotic arms"
      ]
    },
    "publishedAt": "2025-05-08T13:58:18.000Z",
    "title": "Generating Physically Stable and Buildable LEGO Designs from Text",
    "summary": "We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05469.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6796
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.05408",
      "authors": [
        {
          "_id": "681daba2e3775056736651ce",
          "user": {
            "_id": "61424bf4f0d914a5f606a823",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
            "isPro": false,
            "fullname": "Yong Zheng-Xin",
            "user": "yongzx",
            "type": "user"
          },
          "name": "Zheng-Xin Yong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:20:45.630Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651cf",
          "name": "M. Farid Adilazuarda",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d0",
          "user": {
            "_id": "6509feb92257a3afbaeecfea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6509feb92257a3afbaeecfea/a_UbA-2WtZeLTf0ugVzSh.jpeg",
            "isPro": false,
            "fullname": "Jonibek Mansurov",
            "user": "MJonibek",
            "type": "user"
          },
          "name": "Jonibek Mansurov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:03:39.843Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d1",
          "name": "Ruochen Zhang",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d2",
          "user": {
            "_id": "5f1eb362eec0ad2a071ad6e2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
            "isPro": false,
            "fullname": "Niklas Muennighoff",
            "user": "Muennighoff",
            "type": "user"
          },
          "name": "Niklas Muennighoff",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:00.900Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d3",
          "name": "Carsten Eickhoff",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d4",
          "user": {
            "_id": "5f5c4b20e56d546cd6233098",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
            "isPro": false,
            "fullname": "Genta Indra Winata",
            "user": "gentaiscool",
            "type": "user"
          },
          "name": "Genta Indra Winata",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:13.878Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d5",
          "user": {
            "_id": "6544e43b12da508864c38f96",
            "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
            "isPro": false,
            "fullname": "Julia Kreutzer",
            "user": "JuliaKreutzerCohere",
            "type": "user"
          },
          "name": "Julia Kreutzer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:29.257Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d6",
          "name": "Stephen H. Bach",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d7",
          "name": "Alham Fikri Aji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T16:50:06.000Z",
      "submittedOnDailyAt": "2025-05-09T05:46:57.523Z",
      "title": "クロス언어 논리화는 검증 시 스케일링을 통해 실현됩니다.",
      "submittedOnDailyBy": {
        "_id": "61424bf4f0d914a5f606a823",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
        "isPro": false,
        "fullname": "Yong Zheng-Xin",
        "user": "yongzx",
        "type": "user"
      },
      "summary": "대 언어 모델의 논리 능력은 주로 영어에서 연구되고 있으며, 언어의 다양성과 프리트레이닝 모델의 활용에 따라 확장되어 왔습니다. 본 연구에서는 영어의 긴 Chain-of-Thoughts(CoTs) 논리 조정이 다른 언어에서 얼마나 일반화될 수 있는지 조사합니다. 우선, 영어 중심의 논리 언어 모델(RLM)의 추론 계산을 확장하여, 수학적인 논리와 다양한 언어(특히 저 리소스 언어)에서 높은 성능을 나타냅니다. 또한 영어 중심의 RLM의 CoTs는 자연스럽게 영어가 주요 부분을 차지하지만, 참조된 비영어 입력에 대한 논리 처리 시, 참조와 함께 생각하는 패턴이 연속되어 나타납니다. 또한 긴 CoT 논리를 제어하는 효과적인 전략을 발견하여, 모델은 자원 풍부한 언어에서 더 효율적으로 논리 처리할 수 있습니다. 마지막으로, 특히 과학・기술・수학(STEM) 분야에서 문화적인 일반적인 지식으로의 논리 일반화가 불량하고, 영어에서도 이러한 현상이 관찰됩니다. 전체적으로, 영어 논리 테스트 시 언어 간 일반화의 가능성, 구조, 한계점을 제시하고, 영어 중심의 RLM은 자원 풍부한 언어에서 논리 처리를 권장하며, 저 리소스 언어와 외래 상황에서의 논리 개선을 위해 발전해야 하는 것을 결론짓습니다.",
      "upvotes": 3,
      "discussionId": "681daba2e3775056736651f9",
      "ai_keywords": [
        "reasoning language models (RLMs)",
        "long chain-of-thoughts (CoTs)",
        "multilingual mathematical reasoning",
        "low-resource languages",
        "quote-and-think pattern",
        "scaling up inference compute",
        "high-resource languages",
        "out-of-domain reasoning generalization",
        "STEM",
        "cultural commonsense knowledge",
        "crosslingual generalization",
        "test-time scaling"
      ]
    },
    "publishedAt": "2025-05-08T12:50:06.000Z",
    "title": "Crosslingual Reasoning through Test-Time Scaling",
    "summary": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05408.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61424bf4f0d914a5f606a823",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
      "fullname": "Yong Zheng-Xin",
      "name": "yongzx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.19314",
      "authors": [
        {
          "_id": "681d89e6d025518b321f67ce",
          "user": {
            "_id": "6673cf668d570d59b83511cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
            "isPro": false,
            "fullname": "Peilin Zhou",
            "user": "PALIN2018",
            "type": "user"
          },
          "name": "Peilin Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:01:42.666Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67cf",
          "name": "Bruce Leon",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d0",
          "user": {
            "_id": "64489ca21d52a633c8f55aba",
            "avatarUrl": "/avatars/5199a5e93161c61d14ec13f79dd8c2c5.svg",
            "isPro": false,
            "fullname": "Xiang Ying",
            "user": "MindYing",
            "type": "user"
          },
          "name": "Xiang Ying",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-09T04:51:51.507Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d1",
          "name": "Can Zhang",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d2",
          "name": "Yifan Shao",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d3",
          "user": {
            "_id": "636dfa6193d9a0c987d41b73",
            "avatarUrl": "/avatars/14396c8beb376b0d3c27a23fadaeb15e.svg",
            "isPro": false,
            "fullname": "Qichen YE",
            "user": "yeeeqichen99",
            "type": "user"
          },
          "name": "Qichen Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:02:33.873Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d4",
          "name": "Dading Chong",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d5",
          "user": {
            "_id": "66bc683f432c73a183ef787c",
            "avatarUrl": "/avatars/9505a1e6131093a91d0454e50bcbba00.svg",
            "isPro": false,
            "fullname": "Zhiling Jin",
            "user": "HawkFaust",
            "type": "user"
          },
          "name": "Zhiling Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:02:48.180Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d6",
          "name": "Chenxuan Xie",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d7",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d8",
          "name": "Yuxin Gu",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d9",
          "name": "Sixin Hong",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67da",
          "name": "Jing Ren",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67db",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67dc",
          "name": "Chao Liu",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67dd",
          "name": "Yining Hua",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/NJsFys1PlInj9W3E7pD8t.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/gJJ8b6rc6njgCz7Fepbbd.png"
      ],
      "publishedAt": "2025-04-27T17:32:43.000Z",
      "submittedOnDailyAt": "2025-05-09T03:24:22.739Z",
      "title": "Broadway Competition-ZH: Large Language Model's Chinese Broadway Benchmark",
      "submittedOnDailyBy": {
        "_id": "6673cf668d570d59b83511cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
        "isPro": false,
        "fullname": "Peilin Zhou",
        "user": "PALIN2018",
        "type": "user"
      },
      "summary": "이 프로그램의 응답은 사용자가 요구하는 대로 지정된 언어로 번역된 결과를 반환합니다. 이 경우, 영어를 일본어로 번역하면 다음과 같습니다:\n\n「LLM이 도구 사용 에이전트로 진화하는 때, 웹을 브라우저로 시간별로 탐색하는 능력은 그 논리적 및 검색 능력을 평가하는 데 중요한 지표가 됩니다. 현재의 벤치마크의 예로, BrowseComp는 주로 영어를 중심으로 foccus하고 있으며, 다른 주요 정보 생태계의 언어적, 인프라 구조적, 그리고 강제 편집 관계의 복잡성을 무시하고 있습니다. 이를 채우기 위해, BrowseComp-ZH를 소개합니다. BrowseComp-ZH는 중국 웹에서 LLM 에이전트의 평가를 전반적으로 수행하기 위해 개발된 고난이도 벤치마크입니다. BrowseComp-ZH는 11가지 분야를 확장한 289개의 다단계 질문으로 이루어져 있으며, 각 질문은 짧은, 객관적인, 그리고 간단히 확인 가능한 답변을 역공학하여 있습니다 (예: 날짜, 숫자, 고유명사 등). 두 단계의 품질 관리 프로토콜을 적용하고, 높은 질의의 난이도와 답변의 다양성을 추구합니다. 이 제안의 BrowseComp-ZH에서, 20개 이상의 최첨단 언어 모델과 에이전트 검색 시스템을 벤치마크했습니다. 대부분의 모델은 강력한 컨버사션 및 검색 능력이 있지만, 엄격한 훈련을 받습니다: 다수의 모델의 정확도는 10% 미만이고, 일부는 20%를 초과합니다. 가장 우수한 시스템인 OpenAI의 DeepResearch는 그대로 42.9%를 달성합니다. 이러한 결과를 통해, BrowseComp-ZH의 큰 난이도를 보여주며, 성공에 필요한 것은 유효한 검색 전략뿐만 아니라 복잡한 논리적 및 정보의 통합을 요구하는 능력임을 보여줍니다. 이 데이터셋, 구축 가이드라인, 벤치마크 결과를 https://github.com/PALIN2018/BrowseComp-ZH에서 공개합니다.」",
      "upvotes": 3,
      "discussionId": "681d89e7d025518b321f6807",
      "githubRepo": "https://github.com/PALIN2018/BrowseComp-ZH",
      "ai_keywords": [
        "tool-using agents",
        "multihop questions",
        "information ecosystems",
        "quality control protocol",
        "reasoning and information reconciliation"
      ]
    },
    "publishedAt": "2025-04-27T13:32:43.000Z",
    "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language\n  Models in Chinese",
    "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/NJsFys1PlInj9W3E7pD8t.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/gJJ8b6rc6njgCz7Fepbbd.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19314.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6673cf668d570d59b83511cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
      "fullname": "Peilin Zhou",
      "name": "PALIN2018",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05288",
      "authors": [
        {
          "_id": "681d9dd229119d666079b275",
          "user": {
            "_id": "63a3170f8c0c89dcae316858",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
            "isPro": false,
            "fullname": "Ahmed Abdelreheem",
            "user": "Samir55",
            "type": "user"
          },
          "name": "Ahmed Abdelreheem",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:06.667Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b276",
          "user": {
            "_id": "66eae6491cd3794eb4cd1992",
            "avatarUrl": "/avatars/5802de373ccc815f68b98b320aa787bf.svg",
            "isPro": false,
            "fullname": "Filippo Aleotti",
            "user": "Filippo8",
            "type": "user"
          },
          "name": "Filippo Aleotti",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:46.129Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b277",
          "user": {
            "_id": "63166685f5e32157c51fe616",
            "avatarUrl": "/avatars/5b7d8b0e54339d2dc982676af9e4f4fe.svg",
            "isPro": false,
            "fullname": "Jamie Watson",
            "user": "Aileron",
            "type": "user"
          },
          "name": "Jamie Watson",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:52.606Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b278",
          "user": {
            "_id": "6703aece547acbd64c531b72",
            "avatarUrl": "/avatars/8042f99c6eb2c9b61be8c9b950818b2f.svg",
            "isPro": false,
            "fullname": "Zawar Qureshi",
            "user": "zuluquebec",
            "type": "user"
          },
          "name": "Zawar Qureshi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:58.663Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b279",
          "user": {
            "_id": "6577f3bacfb2207f11e847bb",
            "avatarUrl": "/avatars/825998cfebc47d8106f633be5ad10964.svg",
            "isPro": false,
            "fullname": "Abdelrahman Eldesokey",
            "user": "abdo-eldesokey",
            "type": "user"
          },
          "name": "Abdelrahman Eldesokey",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:05:05.828Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27a",
          "name": "Peter Wonka",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27b",
          "name": "Gabriel Brostow",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27c",
          "user": {
            "_id": "67f517dcbd75b1099bba2857",
            "avatarUrl": "/avatars/a1a25d7972b1857f8bb49bc9efc02ded.svg",
            "isPro": false,
            "fullname": "Sara Vicente",
            "user": "svicente",
            "type": "user"
          },
          "name": "Sara Vicente",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:05:21.990Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27d",
          "name": "Guillermo Garcia-Hernando",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T14:29:11.000Z",
      "submittedOnDailyAt": "2025-05-09T04:50:18.526Z",
      "title": "PlaceIt3D: 3D Realistic 3D Scheme Guided Object Placement\n\n(注意：原文中的“リアル3Dスキームでの言語ガイドされたオブジェクト配置”在翻译时，考虑到“语言指导”可能在技术文档中指的是通过语言描述来指导操作，因此翻译为“3D Realistic 3D Scheme Guided Object Placement”。如果原文中的“言語ガイドされた”特指某种特定的语言指导方式，请提供更多上下文以便更准确地翻译。)",
      "submittedOnDailyBy": {
        "_id": "63a3170f8c0c89dcae316858",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
        "isPro": false,
        "fullname": "Ahmed Abdelreheem",
        "user": "Samir55",
        "type": "user"
      },
      "summary": "우리는 실제 3D 화면에서 언어를 안내한 물체 배치라는 새로운 과제를 소개합니다. 우리의 모델은 3D 화면의 포인트 클라우드, 3D 자산, 그리고 3D 자산을 배치해야 하는 곳을 광범위하게 설명하는 텍스트 Prompt를 입력받습니다. 여기서의 과제는 Prompt를 존중하는 3D 자산의 유효한 배치를 찾는 것입니다. 3D 화면에서의 다른 언어를 안내한 위치화 작업과 비교하여, 이 작업은 특정한 도전을 가지고 있습니다: 다중 유효한 솔루션을 가지고因而는 모호하고, 3D 기하학적 관계와 자유 공간을 고려하는 것이 필요합니다. 우리는 이 과제를 시작하기 위해 새로운 벤치마크와 평가 프로토콜을 제안합니다. 또한, 이 과제에 대한 3D LLM의 훈련을 위한 새로운 데이터셋을 제공하며, 첫 번째 비 trivia적인 기준을 제공하는 방법도 소개합니다. 우리는 이 도전적인 과제와 새로운 벤치마크가 일반적인 3D LLM 모델을 평가하고 비교하는 벤치마크 세트의 일부가 될 수 있다고 믿습니다.",
      "upvotes": 2,
      "discussionId": "681d9dd529119d666079b348",
      "projectPage": "https://nianticlabs.github.io/placeit3d/",
      "githubRepo": "https://github.com/nianticlabs/placeit3d",
      "ai_keywords": [
        "point cloud",
        "3D asset",
        "textual prompt",
        "3D LLMs",
        "bounding",
        "grounding",
        "3D geometric relationships",
        "free space",
        "evaluation protocol",
        "benchmark"
      ]
    },
    "publishedAt": "2025-05-08T10:29:11.000Z",
    "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
    "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05288.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a3170f8c0c89dcae316858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
      "fullname": "Ahmed Abdelreheem",
      "name": "Samir55",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03422",
      "authors": [
        {
          "_id": "681db58b1f1c39ba8fbe0162",
          "user": {
            "_id": "681db120007a2d4056d25c70",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
            "isPro": false,
            "fullname": "yepeng liu",
            "user": "pengliu123",
            "type": "user"
          },
          "name": "Yepeng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T08:33:21.362Z",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0163",
          "name": "Wenpeng Lai",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0164",
          "name": "Zhou Zhao",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0165",
          "name": "Yuxuan Xiong",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0166",
          "name": "Jinchi Zhu",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0167",
          "name": "Jun Cheng",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0168",
          "name": "Yongchao Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T10:59:23.000Z",
      "submittedOnDailyAt": "2025-05-09T08:38:34.519Z",
      "title": "LiftFeat: 3D 기어메트릭에 관련된 국부특징 매칭\n\n(注意：原文中的“ギィエメトリー”在韩语中没有直接对应的词汇，因此保留了原文的英文缩写“GI”以保持专业性和准确性。)",
      "submittedOnDailyBy": {
        "_id": "681db120007a2d4056d25c70",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
        "isPro": false,
        "fullname": "yepeng liu",
        "user": "pengliu123",
        "type": "user"
      },
      "summary": "robust하고 효율적인 지역특징량 매칭은 로봇의 SLAM과 시각적 위치지정 등 응용 분야에서 중요한 역할을 합니다. 발전이 있었지만, 조명이 급격히 변하는, 낮은 에지인 장소, 또는 재현되는 패턴이 있는 경우, 로바ス트하고 차별 가능한 시각적 특징량의 추출은 매우 어려워집니다. 본 논문에서는 새로운 가벼운 네트워크를 제안합니다. 이것은 3D 기모트리특징량을 집중하여 원의 디시카르트의 로바ス트성을 높임으로써 LiftFeat라고 불립니다. 특히, 먼저 학습된 단일카메라의 깊이 추정 모델을 사용하여 예측된 표면정규선을 생성하고, 이를 기반으로 예측된 표면정규선에 기반한 3D 기모트리특징량을 추출합니다. 다음으로, 3D 기모트리에 대한 특징량 업로드 모듈을 설계하고 표면정규선특징량과 원의 2D 디시카르트특징량을 융합합니다. 이러한 3D 기모트리특징량의 통합으로 2D특징량의 식별력을 극대화합니다. 상대적인 자세 추정, 호모그래피 추정, 시각적 위치지정 실험 결과로부터, 우리의 LiftFeat는 가벼운 최전단 방법과 비교하여 뛰어넘는 것을 보여줍니다. 코드는 다음 URL에서 공개됩니다: https://github.com/lyp-deeplearning/LiftFeat.",
      "upvotes": 2,
      "discussionId": "681db58d1f1c39ba8fbe01fb",
      "githubRepo": "https://github.com/lyp-deeplearning/LiftFeat",
      "ai_keywords": [
        "monocular depth estimation model",
        "pseudo surface normal label",
        "3D geometric feature",
        "surface normal feature",
        "2D descriptor feature",
        "3D geometry-aware feature lifting module",
        "relative pose estimation",
        "homography estimation"
      ]
    },
    "publishedAt": "2025-05-06T06:59:23.000Z",
    "title": "LiftFeat: 3D Geometry-Aware Local Feature Matching",
    "summary": "Robust and efficient local feature matching plays a crucial role in\napplications such as SLAM and visual localization for robotics. Despite great\nprogress, it is still very challenging to extract robust and discriminative\nvisual features in scenarios with drastic lighting changes, low texture areas,\nor repetitive patterns. In this paper, we propose a new lightweight network\ncalled LiftFeat, which lifts the robustness of raw descriptor by\naggregating 3D geometric feature. Specifically, we first adopt a pre-trained\nmonocular depth estimation model to generate pseudo surface normal label,\nsupervising the extraction of 3D geometric feature in terms of predicted\nsurface normal. We then design a 3D geometry-aware feature lifting module to\nfuse surface normal feature with raw 2D descriptor feature. Integrating such 3D\ngeometric feature enhances the discriminative ability of 2D feature description\nin extreme conditions. Extensive experimental results on relative pose\nestimation, homography estimation, and visual localization tasks, demonstrate\nthat our LiftFeat outperforms some lightweight state-of-the-art methods. Code\nwill be released at : https://github.com/lyp-deeplearning/LiftFeat.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03422.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "681db120007a2d4056d25c70",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
      "fullname": "yepeng liu",
      "name": "pengliu123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.04955",
      "authors": [
        {
          "_id": "681dc1b7965798cccfeab83c",
          "user": {
            "_id": "654cca3fe1b4cd6d40d5a7ae",
            "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
            "isPro": false,
            "fullname": "Fangwei Zhu",
            "user": "soliz1998",
            "type": "user"
          },
          "name": "Fangwei Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:07:17.372Z",
          "hidden": false
        },
        {
          "_id": "681dc1b7965798cccfeab83d",
          "user": {
            "_id": "656873f33fd0bf1f82558695",
            "avatarUrl": "/avatars/7a085da2e2a91d7f41988501a573ebf9.svg",
            "isPro": false,
            "fullname": "PEIYI, WANG",
            "user": "peiyiwang89",
            "type": "user"
          },
          "name": "Peiyi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:07:23.247Z",
          "hidden": false
        },
        {
          "_id": "681dc1b7965798cccfeab83e",
          "name": "Zhifang Sui",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/654cca3fe1b4cd6d40d5a7ae/omgv1F-oog_5ohzEFDbns.png"
      ],
      "publishedAt": "2025-05-08T05:32:36.000Z",
      "submittedOnDailyAt": "2025-05-09T07:21:04.391Z",
      "title": "Chain-of-Thought Tokens는 컴퓨터 프로그램의 변수입니다.",
      "submittedOnDailyBy": {
        "_id": "654cca3fe1b4cd6d40d5a7ae",
        "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
        "isPro": false,
        "fullname": "Fangwei Zhu",
        "user": "soliz1998",
        "type": "user"
      },
      "summary": "Chain-of-thoughts (CoT)은 최종적인 답을 도출하기 전에 중간적인 단계를 생성하는 대규모 언어 모델(LLMs)을 필요로 하며, 복잡한 논리적인 구조를 가진 문제를 효과적으로 해결하는 데 효과적이라는 사실이 입증되어 있습니다. 그러나 CoT의 내부 구조는 거의 불명확합니다. 본 논문에서는 LLMs에서 CoT 토큰의 역할을 실험적으로 연구합니다: 다자리 곱셈과 동적 계획법 두 가지 조합 태스크를 다루며, CoT은 이러한 문제를 해결하기 위해 필수적이지만, 중간적인 결과를 저장하는 데만 비교적 성능을 달성할 수 있다는 것을 발견했습니다. 또한, 중간적인 결과를 대체적인 잠재적인 형식으로 저장하는 것은 모델의 성능에 영향을 미치지 않습니다. 또한, CoT 내에서 값을 랜덤하게 간섭시켰을 때, 그 후의 CoT 토큰과 최종적인 답이 대응하여 변화하는 것을 발견했습니다. 이러한 발견은 CoT 토큰이 컴퓨터 프로그래밍의 변수와 같은 역할을 할 수 있다는 가능성을 보여줍니다. 또한, 무의식한 간단한 풀과 토큰 간의 계산량의 제한 등 잠재적인 결점을 포함합니다. 코드와 데이터는 https://github.com/solitaryzero/CoTs_are_Variables에 공개되어 있습니다.",
      "upvotes": 0,
      "discussionId": "681dc1b8965798cccfeab86d",
      "ai_keywords": [
        "chain-of-thought (CoT)",
        "large language models (LLMs)",
        "intermediate steps",
        "reasoning tasks",
        "inner mechanism",
        "CoT tokens",
        "compositional tasks",
        "multi-digit multiplication",
        "dynamic programming",
        "intermediate results",
        "latent form",
        "variables",
        "unintended shortcuts",
        "computational complexity limits"
      ]
    },
    "publishedAt": "2025-05-08T01:32:36.000Z",
    "title": "Chain-of-Thought Tokens are Computer Program Variables",
    "summary": "Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/654cca3fe1b4cd6d40d5a7ae/omgv1F-oog_5ohzEFDbns.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654cca3fe1b4cd6d40d5a7ae",
      "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
      "fullname": "Fangwei Zhu",
      "name": "soliz1998",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]